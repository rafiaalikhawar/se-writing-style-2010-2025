statistical analysis of large sets of models nder babur eindhoven university of technology mb eindhoven the netherlands o.babur tue.nl abstract many applications in model driven engineering involve processing multiple models e.g.
for comparing and merging of model variants into a common domain model.
despite many sophisticated techniques for model comparison little attention has been given to the initial data analysis and ltering activities.
these are hard to ignore especially in the case of a large dataset possibly with outliers and sub groupings.
we would like to develop a generic approach for model comparison and analysis for large datasets using techniques from information retrieval natural language processing and machine learning.
we are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis repository management and model searching scenarios.
ccs concepts computing methodologies !cluster analysis software and its engineering !model driven software engineering software reverse engineering keywords model driven engineering model comparison vector space model clustering .
introduction model driven engineering mde promotes the use of models and metamodels as rst class artefacts to tackle the complexity of software systems .
as mde is applied for larger problems the complexity size and variety of models increase.
with respect to model size the issue of scalability for models has been pointed out by kolovos et al.
as a limiting factor.
however scalability with respect to model variety and multiplicity i.e.
dealing with a large number of di erent models is also an important issue and has been diagnosed by klint et al.
as an interesting aspect to explore.
there are many approaches to fundamental operapermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
isbn .
.
.
.
such as model comparison applied to problems such as model merging however those mainly focus on pairwise and deep comparison of models for a very small number of models.
rubin et al.
further discuss the inadequacy of pairwise comparison for multiple models and propose an n way model merging algorithm.
indeed many problems in mde involve processing a large number of models.
some examples are domain model recovery from several candidate models and family mining for software product lines spl from model variants .
to make it concrete consider the case where a common model is reverse engineered out of several candidate model variants.
we argue that as the number and variety of input models get larger the initial data analysis and ltering step gets more relevant.
this in turn calls for a need to inspect the dataset for an overview identify potential relations such as proximities cluster formations and outliers.
this information can be used for ltering noisy data for grouping models or even for determining the order of processing for a pairwise model merging or spl generation algorithm see for a discussion on how pairwise comparison order a ects the outcome of merging multiple models .
having set the scene we move to our main objectives.
the purpose of this study is to answer the following questions rq1.
how can we represent models for large scale comparative analysis?
rq2.
how can we analyse and compare a large set of models as a whole avoiding pairwise comparison?
.
related work only a few comparison techniques consider multiple input models without pairwise comparisons such as n way merging .
concept mining uses nlp to cluster concepts.
another technique builds domain ontologies as the intersection of graphs of apis but does not focus on the statistical dimension of the problem.
a similar technique is applied speci cally for business process models using process footprints and lacks the genericness of our approach.
clustering is considered in the software engineering community mostly within a single body of code or model .
a very recent approach which we encountered after publishing our early work is presented by basciani et al.
.
they share most of our objectives and some of the techniques we use i.e.
vector representation and clustering though focusing on repository management.
bislimovska et al.
also propose information retrieval techniques for indexing and searching webml models .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
.
method for model clustering in this section we outline our approach after a brief but necessary summary of the underlying techniques.
.
preliminaries information retrieval deals with e ectively indexing analyzing and searching various forms of content including natural language text documents.
as a rst step for document retrieval in general documents are collected and indexed via some unit of representation.
index construction can be implemented using the vector space model vsm with the following major components a vector representation of occurrence of the vocabulary in a document named term frequency zones e.g.
author or title weighting schemes such as inverse document frequency idf and zone weights natural language processing nlp techniques for handling compound terms detecting synonyms and semantically related words.
the vsm allows transforming each document into an ndimensional vector thus resulting in an m nmatrix for mdocuments.
over the vsm document similarity can be de ned as the distance e.g.
euclidean or cosine between vectors.
these can be used for identifying similar groups of documents in the vector space.
this unsupervised machine learning ml technique is called clustering.
among many clustering methods there is a major distinction between at clustering where a at cluster labelling is performed and hierarchical clustering where a hierarchy of proximities is produced.
speci cally hierarchical agglomerative clustering hac outputs a nested tree structure called dendrogram suitable for visualization and manual inspection.
finally n grams are used in computational linguistics to build probabilistic models of natural language text e.g.
for estimating the next word given a sequence or comparing text collections based on their n gram pro les.
in essence n grams represent a linear encoding of structural context.
.
conceptual overview set of models metamodel features nlp tokeniza8on matching scheme weigh8ng scheme vsm distance calcula8on clustering dendrogram automated extrac8on inferred clusters extrac8on scheme filtering synonym detec8on ... data selec8on filtering clone detec8on ... classifica8on analysis ... repository management domain analysis ... n grams metrics ... manual inspec8on figure overview of our approach.
in this section we describe our approach for the statistical analysis of models.
an overview of the conceptual framework is given in figure .
the base framework is inspired by ir based and statistical techniques for comparing documents as summarized in section .
.
as input to our framework we obtain a set of models of the same type e.g.
ecoremetamodels in our case studies.
the major steps of the work ow are outlined in the following paragraphs.
feature extraction.
the approach starts with the metamodel based extraction of features from the models.
we use features as the general term for any piece of information to be used statistical analysis.
currently the features supported by the framework are are typed identi ers of model elements e.g.
class attribute names metrics e.g.
number of attributes for a class and an n grams of those for capturing the structural relations between model elements e.g.
a bigram for n encoding two classes with an association in between .
the extraction can be a simple one based on the underlying graph or a domain speci c one exploiting the semantics of the model e.g.
special handling of inheritance .
feature comparison and nlp techniques.
the framework has several parameters on using nlp techniques for comparing model identi ers and schemes for comparing and weighting features.
nlp techniques tokenization stemming synonym checking most notably using wordnet1 to name a few are essential for handling real datasets as model identi ers will typically have compound names typos acronyms synonyms and so on.
for comparing features the framework similarly o ers a set of matching schemes such as checking types of identi ers e.g.
class state vs. attribute state or just ignoring them choosing an ngram similarity method e.g.
simple vs. maximum similar subsequence etc.
other options are the weighting of different types of features e.g.
classes have higher weight than attributes idf weighting and context weight for n grams.
computing the vector space model.
here each model consisting of a set of features is compared against the maximal set of features collected from all models.
the result is a matrix similar to the term frequency matrix in ir where each model or model fragment if aiming for lower granularity of extraction e.g.
for detecting clones is represented by a vector in a high dimensional vector space.
consequently we reduce the model similarity problem into a distance measurement of the corresponding vectors.
distance measurement and clustering.
the framework allows several parameters for distance measures among vectors such as euclidean and cosine distance di erent clustering algorithms such as k means and hac and further clustering speci c parameters such as linkage.
the clusters in the dataset can be automatically computed to be used directly e.g.
for the purpose of data selection and ltering.
also particularly for hac the resulting dendrogram can be visualized and inspected manually for giving an insight into the dataset.
the framework is planned to be further extended for other types of statistical analysis such as classi cation and multidimentional scaling.
quantitative evaluation of the parameter space.
the framework natively supports quantitative evaluation of the selected parameter space e.g.
di erent distance measures and provides a range of external cluster validity measures based on a reference clustering such as f measure and purity.
clustering is done for permutation of all the values in the parameter space and the resulting accuracies are displayed in the form of box plots for inspection.
see section .
for an application in one of our case studies.
.
progress and evaluation so far we have implemented our approach in java for the feature extraction nlp and vsm construction and r2for the statistical algorithms.
the following subsections outline the use cases and evaluation from our three publications.
.
proof of concept on a synthetic dataset in we rst introduced our approach and evaluated it on a synthetic dataset as a preliminary proof of concept.
we used a model mutation framework to synthetically generate populations of models to be used for comparison mutated instances with small modi cations such as addition or removal of a model element.
the goal was to emulate a model population evolved from a common model starting with a single model recursively generating mutated o springs.
we eventually formed a tree with a branch for each mutation and a depth level for each generation.
for evaluation we got the youngest generation of models as the input and performed hierarchical clustering trying to reconstruct a hierarchical similarity scheme resembling the original tree.
indeed we obtained a dendrogram of the models with an obvious resemblance to the original tree to be interpreted as as an approximate reconstruction of the evolution.
.
evaluation on public datasets we extended our initial work in terms of nlp features and case studies on public datasets in .
the nlp features are already given in section .
.
for the evaluation of the framework we performed two case studies.
in the rst case study we searched github3for ecore metamodels for state machines and extracted the top results according to github s best match criteria.
the goal was to test our approach on a dataset of a single domain with possible duplicates outliers and subdomains.
we thus considered domain analysis and model searching exploration scenarios e.g.
for reuse in the sense of traversing search results and nding the desired metamodels .
out of this search we obtained a somewhat heterogeneous dataset and applied clustering.
figure shows the resulting dendrogram that we qualitatively evaluated.
five clusters were manually identi ed coloured and encircled corresponding to basic nite state machines fsm models with controllers triggers hierarchical state machines and so on.
some items were considered outliers e.g.
a model with identi ers in french and one about train behaviour.
.
.
.
.
.
.
figure dendrogram of github top search results.
the second case study we used a subset of the ecore metamodels in atlanmod metamodel zoo4.
we selected a subset of metamodels from di erent domains mostly as labelled in the repository .
the aim was to evaluate our approach on a large dataset with multiple domains.
the domains were chosen to be in a wide range clustering was to show the groups subgroups in the dataset.
we thus considered domain analysis and repository management scenarios.
the resulting dendrogram is omitted here due to space limitations.
inspecting the dendrogram we were able to manually identify clusters roughly corresponding to separate domains with varying degree of accuracy.
to name a few we identi ed small clusters for multi agent and build tool models clusters of petri net and state machine models in sibling branches and clusters of word and excel models in close sub clusters.
we employed an external measure of validity and obtained an f0 5score of as the accuracy.
.
n grams for structural comparison in recent work we have improved the clustering framework by automating the cluster extraction employing ngrams to incorporate the structural information in the models and nally introducing quantitative evaluation of clustering with di erent parameters.
we have developed a technique to encode model structure linearly as n grams and evaluated its accuracy quantitatively in two case studies.
in the rst one we have aimed to measure the accuracy of n grams for relatively small datasets using up to trigrams n .
we have extracted random small subsets from the dataset in clustered each one using unigrams bigrams and trigrams and reported the cumulative average f0 5values.
we have observed n grams do not universally improve accuracy over unigrams higher ndoes not lead to monotonically higher accuracy yet n grams with n on average perform better than with n .
.
.
.
.
.
.
.
regular cutf0.
.
.
.
.
.
.
.7f0.
unigram bigramdynamic cut unigram bigram figure unigram vs bigram f0 5measures for two di erent cluster extraction methods.
with the rst case study giving us some insight into the accuracy of n grams we have turned to cluster the whole model dataset.
we have restricted the upper bound for n grams to bigrams and found out that bigrams led to an increase in clustering accuracy compared to unigrams.
the results are given in a boxplot of the f0 5measures with di erent parameter permutations for unigrams and bigrams in figure .
it is evident that bigrams considerably improve the worst case mean and median.
ecore .
future work we plan to extend the prototype and publish it as an open framework after a process of modularizing documenting the code and providing a simple gui.
another important improvement is support for parallel multicore execution and possibly high performance computing in order for our approach to be applicable for larger datasets.
furthermore we are looking for di erent datasets both from public and industrial domains.
the current datasets we use are obtained from model repositories atlanmod zoo and crawled from github.
while looking at other repositories e.g.
of uml models we are considering another option as well reverse engineering models out of curated code base corpus already used for source code analysis research.
more importantly we are in the process of obtaining datasets of real metamodels and models from a number of industry partners that extensively use mde or spl approaches and are interested in our framework.
while our research is inspired by the problems in domain analysis and repository management we intend to use our approach for di erent scenarios.
the most notable of these is model clone detection which we have already initiated a study for feasibility.
it seems to be a natural continuation of our study changing the granularity of the extraction e.g.
extract a vector per class and perform a similar analysis to obtain clusters of model fragments.
this analysis would probably involve other types of features such as metrics as well.
further scenarios include pattern detection model classi cation querying and co evolution analysis.
a nal path to mention as future work is the extension for di erent types of models.
these not only include structural models such as uml class diagrams or feature models but also behavioural models such as statecharts and simulink models.
though the latter would involve model semantics e.g.
comparing traces and be more challenging.
.
conclusion in this paper we have stated the problem of comparing large sets of models for detecting relations such as groupings and outliers among them.
to solve this problem we have presented a generic approach using techniques from ir nlp and ml.
pointing to our previous work we have evaluated our approach using public datasets of models.
the current evaluation results elevate our con dence in our approach and we outline many potential improvements and application scenarios for it.
we aim to further validate our approach for real use cases in industrial context.
.