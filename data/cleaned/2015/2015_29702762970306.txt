automatically recommending code reviewers based on their expertise an empirical comparison christoph hannebauer michael patalas sebastian st nkel y volker gruhn paluno the ruhr institute for software technology university of duisburg essen germany first name.last name paluno.uni due.de ysebastian.stuenkel stud.uni due.de abstract code reviews are an essential part of quality assurance in free libre and open source software floss projects.
however finding a suitable reviewer can be di cult and delayed or forgotten reviews are the consequence.
automating reviewer selection with suitable algorithms can mitigate this problem.
we compare empirically six algorithms based on modification expertise and two algorithms based on review expertise on four major floss projects.
our results indicate that the algorithms based on review expertise yield better recommendations than those based on modification expertise.
the algorithm weighted review count wrc recommends at least one out of five reviewers correctly in to of all cases which is one of the best results achieved in the comparison.
ccs concepts software and its engineering !software defect analysis programming teams open source model information systems !
decision support systems keywords code reviewer recommendation code reviews expertise metrics issue tracker open source patches recommendation system .
introduction a common practice for quality assurance qa in software development projects are code reviews.
in a pre commit review policy every patch is first reviewed by one or more reviewers before it is allowed to be merged into the main codebase stored in a version control system vcs .
this is common for free libre and open source software floss projects.
floss projects may have a large community with a many reviewers so it can be di cult to decide who should review which patch especially for newcomers.
problems with reviewer assignment in floss projects can defer the acceptance of patches by to days on average .
sometimes submitted patches receive no review at all and therefore are not integrated into the application .
finding reviewers is generally a problem in software development projects .
there is extensive research on bug triaging which is the problem of assigning developers to a specified issue and reducing the human e ort involved .
this is an important problem in practice and floss projects use tools to support bug triaging .
there is also research on the review practices in floss projects in general .
more recently research emerged on tools to support reviewer assignments .
these tools rely on algorithms that evaluate the project s review history and calculate thereview expertise of all potential reviewers for submitted patches as the foundation for the recommendation.
in contrast sethanandha et al.
propose that the ideal reviewer would be the person with the greatest modification expertise for the code that the submitted patch modifies .
there is research based on practical observations and empirical data for algorithms to measure the modification expertise of individual developers for a given part of the source code .
however these algorithms have not yet been evaluated for their suitability to recommend reviewers.
reviewer recommendation tools are only useful if they recommend competent reviewers for submitted patches.
this depends on the employed algorithms and on the type of data that these algorithms use for their recommendations.
in this paper we compare the prediction performance of the algorithms file path similarity fps and weighted review count wrc both of which use review expertise for their decision against six algorithms based on modification expertise.
our study is the first to evaluate algorithms based on modification expertise for reviewer recommendation and wrc has not been evaluated at all.
the evaluation uses historical data from the same three floss projects as in a previous study to ensure internal validity and added firefox as a fourth floss project for external validity.
.
related work this section describes existing algorithms to measure expertise.
a reviewer acquires review expertise when reviewing code.
a developer acquires modification expertise when modifying code.
.
review expertise jeong et al.
first suggested how reviews of submitted patches could be predicted automatically.
they proposed bayesian networks to predict the outcome of the review and the actual reviewer of patches.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
ase september singapore singapore acm.
... .
balachandran developed the two reviewer recommendation algorithms review bot and revhistreco.
if a patch for some files is about to be reviewed review bot assigns scores to all previous authors and reviewers of the same lines of code that the patch modifies.
revhistreco is similar but operates on file history instead of line history.
the algorithms use a time prioritization parameter to decrease the impact of older modifications and patches.
the developers with the highest scores are recommended as reviewers.
balachandran compares the two algorithms on two projects with and1676 reviews.
review bot had a better prediction performance than revhistreco.
both review bot as well as revhistreco use both review expertise as well as modification expertise.
another approach was presented by thongtanunam et al.
.
they described the recommendation algorithm file path similarity fps .
the main idea of fps that files with similar file paths are closely related by their function.
it assumes that in most large projects the directory structure is well organized.
the algorithm assigns scores to reviewers of files that have similar file paths as the files of a given review request.
reviewers with the highest accumulated score are recommended as reviewers for the given request.
like review bot the algorithm contains a time prioritization factor to prefer more current over past reviews.
in the following we define fps in mathematical terms according to the original definition in thongtanunam et al.
s paper.
let f r and ddenote the set of all files all reviews and all developers respectively.
let files r!p f be the function that maps a review to the set of its reviewed files.
fps uses a function commonpath f f!nthat counts the number of common directory levels from left to right in the paths of two given files.
if the two files are identical the count is one higher than for two files in the same directory because of the identical file name.
similarly length f!ncounts the number of directory levels including the file name in a file s path or in a di erent formulation length f commonpath f f .
the definition of the functions similarity f f!nandreviewsimilarity r r!rshall be the following similarity f1 f27!commonpath f1 f2 max length f1 length f2 reviewsimilarity r s7!p f12files r f22files s similarity f1 f2 jfiles r j jfiles s j the function reviewers shall map each review to the set of developers that participated in the review.
let r1 rndenote all reviews inrin chronological order.
for a developer dand a review rx let drxbfri2rj0 i x d2reviewers ri g the set of all reviews that happened before rxin which the developer dparticipated as a reviewer.
then fpsscore r d!rgives a prediction of each developer s expertise for each review according to the following calculation fpsscore rx d7!x ri2drxreviewsimilarity rx ri x i the time prioritization factor ensures that older reviews have less influence on the resulting score than newer reviews.
for there is no time prioritization and older reviews have the same weight as newer reviews.
thongtanunam et al.
compared the prediction performance of fps to balachandran s review bot.
both algorithms calculatedrecommendations for historical reviews of the three floss projects android open source project aosp openstack and qt.
the algorithms used five di erent time prioritizations and .
the number of recommended reviewers was limited to k and 5for each review.
then they compared each set of recommended reviewers with the actual historical reviewers in each review.
a recommendation was considered correct if at least one of the krecommended reviewers was one of the actual reviewers.
the results answer two research questions first for all considered combinations of projects and values of kand fps performed better for greater values of .
consequently 1outperformed all other tested values of .
for review bot had little influence less than 5percentage points with the exception of aosp where 2andk 1is an extreme outlier.
since this outlier is not discussed in the text and even contradicted with even this outlier is likely just a misprint in the table.
second fps significantly outperforms review bot except for some configurations in qt especially for low values of and number of recommended reviewers.
for k 5and fps predicts .
.
and .
correctly for aosp openstack and qt respectively.
for k review bot predicts with a project specific optimal value for .
.
and .
correctly for aosp openstack and qt respectively.
in a more recent study thongtanunam et al.
further improved their prediction algorithm and added the floss project libreo ce to their evaluation.
they used a more sophisticated version of the commonpath function.
this more sophisticated version does not only compare the longest common prefix of two paths but also three other types of path similarity metrics and combines them with borda count .
the improved algorithm correctly predicts and with k 5for aosp openstack qt and libreo ce respectively.
this is a change of and percentage points.
the software forge service github o ers features to support social coding .
jiang et al.
analyzed reviewer recommendation algorithms that use social relationships between developers and reviewers among other properties for recommendations.
the best algorithm in their evaluation uses support vector machines svms as machine learning algorithm for recommendations.
the results cannot be directly compared to the other approaches including ours because of their specialization on github features that are not available in other floss projects.
zanjani et al.
combine three metrics to calculate review expertise.
for each combination of reviewer and file each metric yields a value between 0and1.
these three values are added to a composed review expertise.
yu et al.
presented an approach based on information retrieval.
their algorithm consists of two parts.
the first part tries to find similar reviews using textual semantics.
the expertise score of a developer is based on the number of comments in this reviews.
the second part uses comment relations to calculate the quantity of common interests between developers.
both values are added together to predict relevant reviewers.
the algorithm weighted review count wrc is also part of the comparison and uses review expertise.
it is defined and discussed in section .
.
.
modification expertise this section discusses research on metrics to evaluate the expertise a software developer has about a software artifact.
the existing metrics di er on one hand in the source of the measured data and on the other hand in the algorithms used to calculate the resulting expertise.
the metrics were not originally designed for reviewer 100recommendation instead common goals of these expertise metrics are finding a mentor for new team members and finding an expert for specific questions on a part of the application.
each unique algorithm we will evaluate for reviewer recommendation in this study is highlighted in boldface.
the most simple expertise calculation algorithm is the line rule only the last editor of a module is assumed to have expertise with it.
the name of this algorithm derives from a vcs that stored the author of a commit in line of the commit message.
because of its simplicity various research uses the line rule.
in early research on expertise in the software development domain mcdonald and ackerman used three types of sources to identify experts who may be software developers or support representatives first they asked all users to create profiles about themselves.
second they counted the number of changes each developer made on a module using vcs logs.
third they indexed the descriptions of issues for which support representatives were responsible using data from an issue tracker.
this allowed their tool expertise recommender to recommend a set of experts given a problem description.
the recommendation list is ordered by the date of the last change to the module and the last editor appears first in the list.
this is a straightforward extension to the line rule which considers only the authors of the latest change.
expertise recommender also takes authors of previous changes into account although they are considered to have a lower expertise.
however user profiles are often outdated .
the number of changes to a module is only a very rough metric of users modification expertise.
for example if the development team switches to the continuous integration ci development method the number of logged changes to the modules will increase due to the regular commits to the vcs .
this introduces a bias to the metric as early seldom committing developers are considered less experienced than later developers with a high commit frequency.
nevertheless other research also uses the number of changes to an artifact as a metric to quantify the developers expertises of the artifact.
balachandran s review bot described in section .
also falls into this category.
as other early research of this type mockus and herbsleb define experience atoms eas to be elementary units of experience .
they distinguish between several categories of experience where expertise with a specific module is only one example category.
to calculate the ea a developer has acquired they mine vcs logs.
developers acquire one ea for each commit in each relevant category.
examples of categories are the module that the commit changes and the technology that the commit makes use of.
they have also developed a tool called expertise browser to visualize the eas.
expertise browser allows its user to find experts for example the developers with the highest number of eas for a module.
as another example robles et al.
devised a method to visualize changes to the core group of developers in a floss project at each point in time.
their calculation also counts the number of commits for each developer to the vcs.
for each time period a fraction of developers with the most commits are considered core developers.
qualitative analyses have shown to to be reasonable fractions of commits to be considered core developers.
thus the calculation s output is a project wide decision whether a specific developer was a core developer at a specific point in time.
the visualization techniques itself are not relevant for our study and will not be discussed here.minto s and murphy s emergent expertise locator also calculates the number of times each developer has edited each file and stores this information in a file authorship matrix.
this matrix is used as part of a calculation to find developers that share expertise and therefore are assumed to be part of an emerging team.
the emergent expertise locator can then propose prospective colleagues to a developer to help in the formation of such a team.
however the calculation step used to find a developer s expertise with a file still only counts the number of edits.
schuler and zimmermann propose to count not only the number of changes directly to a module as modification expertise but also the number of commits with code that call the module.
this is no modification expertise but usage expertise.
however they provide no algorithm that aggregates the measured modification and usage expertises into a single metric.
as modification expertise appears more adequate than usage expertise to recommend reviewers formodifications of a module their approach does not directly contribute an additional algorithm for reviewer recommendation.
g rba et al.
approximated code ownership using vcs logs which is specifically a list of file revisions with an author name and the number of removed and added lines for each file.
they developed two algorithms that execute consecutively for the code ownership approximation.
the first algorithm approximates the size of each code file as the file size is not directly available in their type of vcs log.
for each author the second algorithm approximates the fraction of lines in the file that this author owns i.e.
whom the author has edited last.
ownership maps visualize these ownership data.
they also showed how to identify patterns in the ownership map.
alonso et al.
assigned each source code file to a category based on its file path.
their tool expertise cloud measures a developer s expertise in a category as the number of changes to files in the category.
thus this algorithm is a variant of counting the number of changes on a module instead of the file level.
it stands out in that it also specifies how to find out the changed modules from vcs logs.
however their example project the apache http server project clearly documented its source code layout which implies a classification scheme for the project s files.
it is not immediately clear how to classify files in other projects which have less clear documentation on their source code layout let alone an automatic classification without manual configuration.
their approach also includes a visualization of expertise in tag clouds hence the name expertise cloud.
the degree of knowledge dok defined by fritz et al.
aggregates two metrics on di erent data sources the degree ofinterest and the degree of authorship doa .
first the how much a developer uses a module.
the schuler s and zimmermann s idea of usage expertise as it accounts not only calls to the module but also other interactions such as opening a module s source code in the integrated development environment ide .
second the doa measures how much of the module s source code a developer has created .
similarly to g rba et al.
s code ownership approximation a developer s doa decreases when other developers change the module.
fritz et al.
determined concrete weightings for aggregating the doa components and the a single dok value via linear regression on experiment data.
in multiple experiments they found that the weightings depend on the project but they also gave general weightings that best fit the aggregated experiment data.
acquiring the data to calculate the a plugin on each developer s computer.
this impedes its usage in our evaluation in three ways.
first it disallows using data before the plugin was installed.
second additional e ort is necessary to let the developers in the evaluated floss projects 101use the plugin as the plugin needs support and floss developers need to be convinced to cooperate.
third ensuring the plugin s usage requires a closed experimental setting.
therefore we used only the doa for the comparison.
anvik and murphy compared three di erent algorithms to determine experts.
the algorithms require that the issues have already been closed and are therefore not suitable for reviewer prediction.
however two of the algorithms evaluate vcs logs to find experts and one uses data from the issue tracker.
in this regard the comparison resembles the comparison in this paper as it also compares recommendation systems based on modification expertise with recommendation systems based on expertise acquired when using an issue tracker.
anvik and murphy found the two algorithms based on vcs logs to have an average precision of and and an average recall of and .
the algorithm based on bug reports shows results between the two vcs based algorithms with an average precision of and an average recall of .
thus no algorithm is better for all use cases instead there is a trade o between precision and recall.
furthermore the number of recommended experts di ers between algorithms and their published data show a strong association between the average number of recommended experts and average recall.
this suggests that the choice between the two sources of data may have little or no influence at all on the measurement of expertise.
.
expertise explorer each expertise algorithm presented in section .
works on a dataset from a development tool most commonly the vcs logs and calculates the expertise for a given code artifact for a given developer.
some expertise algorithms like ea and distinguish between di erent types of expertise but even these algorithms have an explicit variant to calculate modification expertise.
this modification expertise may or may not be di erent from the review expertise that the algorithms presented in section .
approximate.
if they are di erent these two types of expertise are not distinct review expertise implies an understanding of the changes to an artifact which is helpful or even necessary to create code .
thus when gathering review expertise a developer also gathers modification expertise.
the reverse is also true an expert creator of an artifact s code may anticipate side e ects of a change and more generally knows the context of a code artifact like its intended purpose its callers and callees and its technical constraints.
these skills obviously also help when reviewing code.
this study uses the platform expertise explorer that we have developed for expertise calculations and the more specific purposes of this study.
expertise explorer is publicly available under the terms of the gnu general public license gpl and therefore anybody can reproduce our results or use it for similar calculations or comparisons.
we also provide a lab package with the source and result data used in this study .
this section provides exact definitions for ambiguous terms like review and reviewer.
a description of the technical aspects of expertise explorer relevant for our evaluation of the historical data follows.
the section eventually provides an alternative calculation method for fps that fits to expertise explorer s data structure.
.
data model for the review process the evaluation prerequires an exact definition of what a review is and when the algorithms must calculate their predictions of reviewers.
if the calculation takes place later then the algorithms may obviously use more data and may come to di erent predictions.
the review model includes the concepts of issues patch submissions reissue reviewgroup of recommended reviewerspatch submissionissue patch1 n prediction1 npatch review1 ndate author list of files reviewer date algorithmup to five recommended reviewersfigure erd of the core data structures used in the evaluation views and the main vcs.
figure depicts the former three and their relationships as an entity relationship diagram erd .
an issue is a change request to the floss project s application either caused by a fault in the software or in case of a feature request by a change to the specification.
in either case only changes to the source code implement this change request.
for our definition documentation files and build scripts also count as source code if they also reside in the vcs because they can be di cult to distinguish and the review process is the same.
thus any issue in the floss project s issue tracker counts as issue.
proposed changes to the source code manifest as patch submissions.
each patch submission has an author occurs at a specific point of time and modifies one or more files in the vcs.
a patch submission belongs to one specific issue and each issue may have any number of patch submissions.
each patch submission may receive any number of reviews.
each review is performed by a reviewer at a specific point of time and belongs to one specific patch submission.
the files that a patch submission modifies also belong to its corresponding reviews.
obviously a review can occur only after its corresponding patch submission.
note that although a review is actually positive or negative this is not relevant for the model.
what exactly counts as a review depends on the issue tracker and will be discussed separately for each floss project in our evaluation.
eventually some patch submissions are merged into the main vcs at a specific point in time.
issues without patch submissions or reviews are discarded for the evaluation.
for the remaining issues all algorithms calculate up to fiverecommended reviewers for the issue when the earliest patch is submitted.
if the algorithms were used in a recommendation tool in practice this is when the floss project has to decide who should review the patch.
for later patch submissions the algorithms will not calculate new recommendations one reviewer often reviews all patch submissions of an issue and therefore a reviewer recommendation tool does not need to recommend new reviewers in these cases.
thus there is exactly one group of up to five recommended reviewers for each algorithm and for each of the remaining issues.
all reviewers who review any patch submission of an issue at any point in time are considered actual reviewers for the issue.
an algorithm may use data from all events that have occurred before the time of calculation i.e.
when the earliest patch to an issue is submitted.
for wrc and fps this includes all reviews on patch submissions in other issues.
reviews in the issue under analysis cannot have occurred before the first patch submission and therefore are not included.
there can be multiple reviews for each issue so the periods between first and last review of two issues may overlap.
in these cases wrc and fps take early reviews of an issue into consideration while they do not take later reviews of the same issue into consideration because these later reviews happen only after the patch submission that triggered the calculation.
102the six algorithms described in section .
use data from the main vcs.
that means that they can use patch submissions only if and only after they have been merged into the main vcs.
in contrast to wrc and fps they take the author of a patch submission into account.
.
data processing expertise explorer needs two sets of data as input.
these sets of data may or may not have the same origin depending on the project under analysis.
first expertise explorer needs authorship data as stored in a vcs log.
with these data expertise explorer can execute the vcs based reviewer recommendation algorithms described in section .
.
second expertise explorer needs the review history.
with these data expertise explorer can execute the issue tracker based reviewer recommendation algorithms.
the review history is also needed to compare the computed reviewer recommendations with actual reviewer recommendations which will be used for the evaluation.
the vcs history data may be substituted by the issue tracker history data if the latter contain enough authorship information especially the number of added and removed lines and whether files are modified or newly created.
a database stores for each project authorship data expertise values for each combination of file and contributor and the algorithm comparison results described in section .
.
in a first import step all authorship data are imported into the database.
next expertise explorer iterates chronologically through a list of activities in the project s issue tracker specifically reviews and patch submissions.
for each first patch submission expertise explorer calculates all algorithms expertise values for all combinations of contributors and submitted files using review and authorship data that occurred before the data of this first patch submission.
thus the expertise values correspond to the values that a reviewer recommendation tool would have had access to at the date and time of the first patch submission to the issue.
afterwards expertise explorer calculates for each algorithm the five contributors with the highest expertise for the patch.
if the patch contains multiple files this involves an algorithm specific aggregation of expertise values.
for most algorithms expertise values are simply added.
note that adding and the arithmetic mean yield the same reviewer recommendations as the order of recommended reviewers is the same for both types of aggregations.
for line rule and expertise recommender the maximum expertise value is used instead of the sum as this yields the latest editor of any of the reviewed files.
for each review the algorithms based on review expertise update their data to include the review.
furthermore the reviewer is stored as an actual reviewer for the issue.
afterwards another module of expertise explorer iterates over the list of issues in the database.
for each issue it compares the up to five recommended reviewers for each algorithm with the set of actual reviewers for the issue to find out which algorithms had correctly recommended reviewers and which had not.
.
alternative algorithm for fps thongtanunam et al.
provided an algorithm in pseudo code for fps which directly uses the formulas presented in section .
.
because of expertise explorer s database structure expertise explorer cannot directly use this algorithm.
however we will show in this section that there is an equivalent definition for fps s core that expertise explorer can use.
using the definitions of section .
we define a function wrc f d r!rthat calculates the value weighted review count wrc an intermediate value that specifies the review experience a developer has with a specific file at the time of a specific review.remembering that r1 rnare all reviews in chronological order wrc has the following definition wrc f d rx7!x ri2drx f2files ri x i 1jfiles ri j expertise explorer treats wrc like any other algorithm and calculates its value for all developers and all files.
using the precalculated values for wrc allows a di erent calculation of fps using the following equivalence fpsscore rx d x ri2drxreviewsimilarity rx ri x i x ri2drxp f12files rx f22files ri similarity f1 f2 jfiles rx j jfiles ri j x i x f12files rx jfiles rx j 1x ri2drx f22files ri similarity f1 f2 jfiles ri j x i x f12files rx jfiles rx j 1x f22f ri2drx f22files ri similarity f1 f2 jfiles ri j x i x f12files rx f22fsimilarity f1 f2 jfiles rx jx ri2drx f22files ri jfiles ri j x i x f12files rx f22fsimilarity f1 f2 wrc f2 d rx jfiles rx j this alternative algorithm to calculate fps has the advantage over the original variant that the values for wrc can be calculated before knowing which files belong to rx.
thus expertise explorer pre calculates all current values for wrc.
when the floss project needs a reviewer recommendation for a set of files files rx the alternative algorithm can provide the recommendation more quickly.
.
empirical ev aluation we evaluated the prediction performance of the six algorithms based on modification expertise described in section .
in particular line rule number of changes expertise recommender code ownership expertise cloud and doa.
we compared these algorithms to fps developed by thongtanunam et al.
described in section .
.
we used the original definition of commonpath and not the more sophisticated version developed in their more recent publication because calculating fps values for all our data takes multiple weeks even if there are no problems and it came to our attention only recently.
furthermore the more sophisticated version s results di er from the original version by only about and percentage points for the three floss projects aosp openstack and qt which is less than the di erences we expected between the other algorithms.
furthermore we used only as parameter as this was the optimum tested value for fps as determined by thongtanunam et al.
.
in their more recent study thongtanunam et al.
also used only .
we also included wrc as defined in section .
in the comparison because expertise explorer had to calculate its values for technical reasons anyway.
we did not include review bot in the comparison as it had already been shown to be inferior to fps and di erently to all other considered algorithms review bot requires much more input data specifically a line based modification history.
103we evaluated the algorithms on data from four di erent projects firefox aosp openstack and qt.
thongtanunam et al.
also used aosp openstack and qt which allows us to compare results.
we added firefox to increase external validity as it uses a di erent issue tracker than the other three floss projects.
firefox uses bugzilla to keep track of reported software bugs aosp openstack and qt use gerrit code review.
table summarizes the characteristics of our study data.
the study period includes a training phase of one year.
only prediction results after the training phase were part of the further analysis.
this ensured that the algorithms could access historical data of at least one year for each recommendation.
the numbers in parentheses refer to the properties excluding the data from the training phase.
.
firefox mozilla is the parent project of firefox and hosts the infrastructure for firefox and its other subprojects.
part of this infrastructure is mozilla s issue tracker bugzilla which happens to be a mozilla project itself.
mozilla tracks the issues of all mozilla projects in one bugzilla instance which will be referred to as just bugzilla in the following.
mozilla employs a policy that requires an issue in bugzilla for every change to the code before it is committed to the mozilla codebase.
furthermore every change needs positive review before it is committed even if the author is the module s maintainer or someone else with review rights.
in these latter cases author and reviewer need to be di erent.
the author of the change attaches a patch in di format to the issue and requests the review.
reviewers indicate the result of their review with a flag on the reviewed attachments.
usually one review from an eligible reviewer su ces before committing although there are exceptions.
the vcs log of mozilla s mercurial hg repository for firefox is one of two data sources used for the comparison.
they contain all information the vcs based recommendation algorithms need to compute expertise values.
in particular hg logs not only the committer of a patch but also its author.
expertise explorer reads hg logs to find this information.
the vcs log used for the analysis contains all changes committed between and .
on firefox migrated from the vcs concurrent versions system cvs to hg therefore no older vcs logs were used.
the second data source of the comparison is the history data of mozilla s issue tracker bugzilla .
one component for the download and analysis of bugzilla s history data of the firefox project was zhou s and mockus s download and data transformation scripts for mozilla.
the applicable output of the scripts is a comma separated values csv file with one activity in bugzilla per line.
reviews are one of those activities.
the csv file contains the following information for each review who was the reviewer?
when did the review take place?
the csv file does not say which files were reviewed but only an identification number for an issue s attachment containing the patch as di file.
to retrieve this missing information expertise explorer has a component attachmentcrawler that downloads the di files of each reviewed patch to find out which files the patch changes.
expertise explorer parses the csv file and filters for relevant reviews such that each included activity fulfills the following constraints each relevant activity contains one of the flags review or review as these indicate completed reviews .
the analysis discards architecture reviews indicated by superreview as these are not in the focus of this work.
the review took place between and as the vcs logs span this time frame.
reviews must be for files of the firefox project.
all mozilla projects use bugzilla and are therefore present in the csv file.
the reviewed file names must match a file in the vcs log to fulfill this condition.
in some cases it is still ambiguous to which mozilla subproject an issue belongs and it is up to subjective decisions.
a clear distinction between firefox issues and issues of other mozilla projects is therefore not always possible.
.
aosp openstack and qt for the gerrit based projects aosp openstack and qt we used the data obtained from hamasaki et al.
analogously to thongtanunam et al.
.
however we used the newest available data which are newer than thongtanunam et al.
s. the newer data cover the period up to the end of instead of only to the beginning and middle of so we considered times as many reviews in total.
the gerrit data contained some inconsistencies that we had to take care of.
first the format of the data is di erent to the description .
in the latest dataset there are unfortunately no flags which indicate who performed the review and when.
however these flags are essential for our studies because these data are used both as the basis for wrc and fps as well as for the evaluation for all algorithms.
however it is possible to extract the reviews from auto generated message texts.
messages containing one of the following seven text patterns designate reviews code review code review looks good to me approved i would prefer that you didn t do not merge and do not submit .
a further problem concerns the qt data.
from onwards only the last patch submission of an issue is stored in the data.
since patch submissions contain the list of modified files we could not identify on which files a review was made except for reviews of the last patch.
as this problem a ects only the two algorithms wrc and fps that are based on review expertise the data are biased in favor of the other six algorithms based on modification expertise.
thus we have limited the observation period for qt to issues occurring before .
expertise explorer needs two sets of data one for authorship and one for review history data.
both datasets can be generated from hamasaki et al.
s javascript object notation json formatted files .
the first set of data could have been obtained from the project s vcs but we chose to generate it from hamasaki et al.
s json formatted files.
one advantage is that these files use consistent names for authors and reviewers.
we wrote a python script that expects hamasaki et al.
s json file as input and creates an authorship data file.
the script iterates over all json elements wherein an element represents an issue and for every issue over all messages.
only issues with at least one merge message and one file are considered in the further processing.
the script recognizes merge messages based on the following text patterns successfully merged into successfully cherry and successfully pushed .
we interpret the merge message date as the date that the referenced patch has been merged into the vcs.
to generate the review history data we wrote another python script.
like the first script it iterates over all messages of every issue.
it uses the above mentioned text patterns to identify a review message.
similar to the first script we use the date of a review message as the review date the author of the message as the reviewer and the files of the referenced patch as the reviewed files.
as a result the script generates one line in a csv for every review.
additionally the script creates one result line for every patch submission.
104table characteristics of study data the values in parentheses exclude training data firefox aosp openstack qt study period including training year number of contributors number of reviewers number of issues number of reviews number of files in vcs arithmetic mean of reviews per issue .
.
.
.
.
results analogously to the previous research on reviewer recommendation accuracy is the percentage of correct recommendations on the total number of recommendations.
a recommendation is considered correct if the intersection between the set of recommended reviewers and the set of actual reviewers on the same issue is not empty.
again analogously to previous research we did not use the concepts of recall and precision because we let all algorithms recommend five reviewers if possible and so there is no trade o between recall and precision.
figure shows the aggregated top accuracy of the eight recommendation algorithms in the four evaluated floss projects over the course of the study period excluding the training phase.
table shows all top top and top prediction performances at the end of the study period.
.
discussion the main research goal of this study was an empirical comparison of reviewer recommendation algorithms.
two of the compared algorithms used review expertise for their recommendations and six algorithms used modification expertise.
for every evaluated floss project the algorithm with the highest accuracy was an algorithm based on review expertise.
surprisingly in two out of four floss projects the algorithm wrc outperformed fps.
furthermore wrc performs close to the optimum even for aosp and openstack where it is not the best algorithm.
hence wrc is a reliable algorithm.
in contrast fps seems to be more sensitive to the characteristics of the floss project as its top accuracy is only for qt.
in addition to its good prediction performance wrc is a simple algorithm especially compared to other algorithms with good prediction performance like fps and expertise cloud it requires less input data per recommendation it is easier to implement and its value can be computed more quickly.
the definition in section .
immediately shows that wrc is simpler than fps as the value of wrc is used in the calculation of fps.
like fps expertise cloud evaluates expertise not only directly on the reviewed files but also on files with similar paths.
as a consequence calculating a value for expertise cloud or fps relies on data for a large proportion of files in the floss project.
in large floss projects like the ones evaluated in this study this can involve querying hundreds of thousands of values.
fps and expertise cloud therefore have a worse computational performance than the other algorithms including wrc.
during the evaluation in this study fps and expertise cloud needed at least one order of magnitude more time for the computation than the other algorithms.
.
.
reviewer base set as described above algorithms based on review expertise usually perform better in terms of accuracy than those based on modifica tion expertise.
however it has to be taken into account that the former algorithms have an implicit advantage over the latter they automatically exclude developers without review rights.
as shown in table the number of contributors in a floss project exceeds the number of reviewers by a factor of .
to .
.
by design the algorithms based on review expertise can only recommend reviewers who had already performed a review in the past and therefore have a higher chance to currently have review rights.
algorithms based on modification expertise may therefore recommend developers who never perform reviews as they lack review rights.
if the algorithms would filter out reviewers without review rights before recommendation this might increase accuracy especially for algorithms based on modification expertise and thereby turn the tide.
.
.
the most competent reviewer although it may seem paradoxical first it may actually be undesirable in a floss project that each patch receives its review from the most competent reviewer.
this may be the case if some members of the floss project are the most competent reviewers for so many patches that they cannot review all of them.
it might be better if less competent but also less busy reviewers perform some of the reviews as the disadvantages of long delays may at some point be greater than the disadvantages of slightly worse feedback in the reviews.
as another reason some very competent reviewers might be even more competent developers.
if this is the case in a floss project good developers might have higher value than good reviewers.
these two reasons appear more likely when considering that a small group of about .
to .
of all contributors are the core developers of a floss project who contribute of the code .
whether those trade o s between reviewer competency and reviewer cost are necessary depends on the specific floss project.
algorithms based on review expertise may have an advantage in this case as their recommendations do not directly depend on competency but on who reviewed patches in the past which already reflects current practice in the floss project and the trade o s that their members decided about.
.
.
vicious circles if a floss project extensively uses a reviewer recommendation tool algorithms based on review expertise base their decisions on data that they themselves influenced they recommend reviewers these reviewers review the patch and then they assume that these reviewers are well suited to review those types of patches because they did it in the past.
this induces the danger of a vicious circle where some unsuitable reviewers are recommended more and more often for some kind of patches because of prior unsuitable recommendations.
this problem has also been discussed for other types of recommender systems .
recommendation algorithms based on modification expertise do not su er from this problem by design as they do not take review expertise into account.
105line rule expertise recommendernumber of changes doawrc code ownershipfps expertise cloud accuracy in number of issues a firefox020406080 accuracy in number of issues b aosp accuracy in number of issues c openstack020406080 accuracy in number of issues d qt figure project results excluding data from the training phase .
.
variation between projects the prediction performance varies stronger between projects for some algorithms than for others.
especially the line rule and its close relative expertise recommender have a top accuracy of only .
and .
in firefox respectively but their accuracy for aosp openstack and qt is .
to .
and .
to .
respectively.
aosp openstack and qt seem to follow a policy similar to the line rule without a reviewer recommendation system.
simple metrics like the line rule and expertise recommender do not need a specialized reviewer recommendation system as the usual development environment already contains a vcs client that can display the vcs log which immediately shows the experts according to line rule and expertise recommender.
since the line rule and expertise recommender are very simple metrics of modification expertise that have been shown to be inferior to metrics like doa a reviewer recommendation tool may supply useful additional information to select reviewers for a patch.
.
.
variation between algorithms an algorithm either makes a correct or an incorrect prediction for each issue so the results have a binomial distribution.
as thealgorithms worked on the same data we used a paired test to check whether the di erences are statistically significant a mcnemar test with continuity correction confirms highly significant p di erences for most pairs of algorithms with exception of expertise recommender and doa in firefox and doa and number of changes in aosp which are not significant at and between expertise recommender and number of changes in firefox p .
these results can be reproduced with the lab package .
as visible in the graphs in figure the algorithms number of changes and doa generally have very similar accuracies.
in qt and aosp code ownership is also very similar to number of changes and doa.
having large datasets the di erences are still highly significant except for the cases mentioned in the previous paragraph.
even where the di erences are statistically highly significant they are often very small for example there are only out of recommendations for qt for which doa and number of changes have di erences in correctness i.e.
one of the algorithms recommends a correct reviewer while the other does not.
this implies that decreasing the expertise value when other authors change a file which is the main di erence between doa and number of changes in regard to reviewer recommendation has little influence on the overall results.
106table top k accuracy of the algorithms in firefox aosp openstack and qt in excluding training data firefox aosp openstack qt top top top top top top top top top top top top line rule .
.
.
.
.
.
.
.
.
.
.
.
expertise recommender .
.
.
.
.
.
.
.
.
.
.
.
number of changes .
.
.
.
.
.
.
.
.
.
.
.
code ownership .
.
.
.
.
.
.
.
.
.
.
.
expertise cloud .
.
.
.
.
.
.
.
.
.
.
.
degree of authorship .
.
.
.
.
.
.
.
.
.
.
.
weighted review count .
.
.
.
.
.
.
.
.
.
.
.
file path similarity .
.
.
.
.
.
.
.
.
.
.
.
.
.
time sensitivity figure shows that some algorithms in some floss projects gain prediction performance over the months while others lose prediction performance.
yet others have a more or less stable prediction performance.
possible causes are changes to the floss projects review policies for example if each patch requires more reviewers it will be easier for the algorithms to recommend one of them correctly.
if a floss project grows it will be harder to recommend a reviewer correctly because the number of selectable reviewers grows.
however these changes can only a ect all algorithms at once and do not explain di erences between algorithms.
it is possible to distinguish three di erent categories of algorithms first line rule and expertise recommender are time local they use only data for their recommendations that were generated shortly before their execution.
second code ownership and doa are forgetful algorithms.
these algorithms consider all historical data for their recommendations but older data have less influence and expertise decreases during phases of inactivity.
for doa and code ownership an author s expertise decreases when other authors modify files that the author has expertise with.
for wrc and fps are also forgetful but this is not the case in this study.
third are time global algorithms like expertise cloud number of changes as well as wrc and fps with .
they use all historic data for their recommendation and all data have the same value independently to when they were generated.
forgetful and time global algorithms may increase their prediction performance over time as they can base their recommendations on a larger dataset.
however time global algorithms may have trouble with changes in the project because they always take old data into consideration that are based on obsolete and now invalid processes.
for example two reviewers that were active for two years will both be recommended with equal probability even if one of them was active only five years ago and then left the project and the other is still active.
this e ect decreases their prediction performance over time and may also a ect forgetful algorithms that do not properly forget obsolete data.
conversely if we observe a changing prediction performance in time local algorithms this must be an e ect of the project as a whole.
for firefox both time local algorithms stabilize on a slow loss of predictive performance.
a possible cause for the loss is firefox s growth.
consequently other algorithms like expertise cloud and fps also slowly loose predictive performance.
wrc is more or less stable though as the benefits of its growing database cancels out the losses through firefox s growth.
expertise cloud and fps both use path similarities which lets them access much more data for each prediction.
these additional data possibly include obsolete data and therefore make them more vulnerable to changes in the project structure.in this study fps and therefore also wrc uses the time prioritization factor so e ectively data do not age.
this time prioritization turned out to be the best value for fps in previous studies .
however the next lower tested value was so the optimum may be between 8and1.
in fact 8seems like a very low value for large floss projects.
firefox as an example has .
reviews per day.
this means that expertise gained through a review has only times its original value after one day.
reasonable values for should therefore be much closer to 1or the time prioritization factor should not have exponential impact.
the exact optimum should be the subject of future research as a forgetful fps could have a higher long term prediction performance than a time global.
.
.
training phase the algorithms prediction performance becomes steady only after some time because of two e ects.
first accuracy is an average and therefore varies while the total number of recommendations is low.
second the algorithms use past reviews and modifications for their recommendations and so they cannot correctly recommend reviewers if the number of past reviews or modifications is still low.
in this study the algorithms are granted a training phase of one year which is enough to mitigate the second e ect.
however the algorithms have di erent required training phases.
figure shows aggregated accuracies for firefox including the training phase.
although wrc has the best prediction performance after the whole study period of six years it takes about issues of training phase to overtake fps.
apparently algorithms like fps and expertise cloud have a shorter required training phase than algorithms like wrc.
wrc considers only expertise with the file itself while fps and expertise cloud can induce from other related files if the information about a requested file is insu cient.
.
threats to v alidity this section discusses threats that endanger the validity of our findings.
this includes threats to internal validity including one threat to construct validity and a threat to external validity.
.
internal validity each algorithm s accuracy is the fraction of issues in which the algorithm recommends reviewers that turned out to be actual reviewers in reality.
however actual reviewers may in some cases not be competent reviewers.
this is a threat to construct validity.
in an extreme case an algorithm may recommend reviewers more competent than the actual reviewers are and have a low accuracy because of the di erences between recommended and actual reviewers.
nevertheless the self selection of reviewers ensures that actual reviewers are at least a good approximation to competent reviewers.
accuracy in number of issuesline rule expertise recommender number of changes code ownershipdoa expertise cloud wrc fps figure firefox results with focus on the training phase issues with negative numbers designate issues of the training phase furthermore even if a recommendation tool fails to recommend more competent reviewers than a manual selection does it is still faster which is a benefit by itself .
besides section .
.
argues that recommending the most competent reviewer can be a pitfall.
comparison to actual reviewers has also been used in all previous empirical research of this type that we know of .
as a considerable special case of the previous threat the data indicate that aosp uses the line rule to select reviewers see section .
.
.
this and other selection strategies that resemble evaluated algorithms introduce a systematic bias to the comparison.
in these cases a good prediction performance only indicates that the algorithm is currently already used but not that it is necessarily better than other algorithms.
however some algorithms are consistently better than others among all evaluated projects with their diverse possible reviewer selection strategies which mitigates this problem.
as discussed in section .
.
if a floss project grows over time prediction performance decreases.
the observed results therefore depend on the length of observation.
as shown in table each of the four evaluated floss projects had a di erent study period.
this threat impairs comparisons between the four floss projects.
since all algorithms are evaluated within the same time period for each individual floss project this is no threat to comparisons of di erent algorithms within each floss project.
the data used for the evaluation contain measurement artifacts.
for example someone might have accidentally flagged a patch in firefox twice in which case this reviewer would have twice been awarded review expertise.
we included consistency checks to filter out corrupt data items.
for example we filtered out reviews that a single reviewer performed within two seconds which not only prevents errors like the initial example but also duplicated review entries due to failures of the floss project s databases.
the exact dates of commits to firefox s vcs are especially vulnerable to measurement errors.
these dates stem from the developers machines.
the vcs respects time zones but these may be incorrectly configured on the developers machines.
indeed the time and date on the developers machines may be completely wrong in some cases.
we filtered out issues with reviews that happened before their corresponding patch was submitted.
we also filtered issues withobviously wrong time stamps like when some activities happened outside of our study period.
despite our e orts to eliminate measurement artifacts a complete check of the data was not possible and therefore measurement artifacts may still remain.
when firefox migrated their vcs from cvs to hg they imported all existing source code in a single commit with the author hg mozilla.com .
the algorithms assumed an enormous modification expertise for hg mozilla.com for some time after the migration.
the training phase however allowed to recover from these types of problems.
to test whether hg mozilla.com biased the results we also evaluated results after filtering out all issues for which one of the algorithms had recommended hg mozilla.com as a reviewer.
this had only minor impact on the results and especially did not change the order of algorithms in terms of prediction performance.
.
external validity section .
.
discusses how the prediction performance varies between the four evaluated floss projects.
while there is variation between projects most results apply universally to all four evaluated floss projects.
this indicates that the results also apply to many other floss projects and are in fact to a large extent independent from the specific floss project.
there may be exceptions though especially for those algorithms that vary stronger already between the four evaluated projects like the line rule.
.
conclusion and future work submitted patches to floss projects must usually undergo a code review before they are accepted.
problems with reviewer assignment cause delayed and forgotten patches.
reviewer recommendation systems may help with reviewer assignment problems but only if they accurately recommend competent reviewers i.e.
have a good prediction performance.
this study compared the prediction performance of eight reviewer recommendation algorithms using historical data from four large floss projects.
six algorithms are metrics of modification expertise that have been described in literature but had not been used for reviewer recommendation before.
the algorithm fps was included as a comparison reviewer recommendation algorithm.
the eighth algorithm wrc is a pre stage to fps but had not yet been considered as a recommendation algorithm.
on the bottom line wrc achieves the best results with the additional advantage of its simplicity.
the data set used in our evaluation is more than an order of magnitude larger than those used in previous studies e.g.
.
our results show that calculating reviewer recommendations is possible in large floss projects.
we have also implemented a proof ofconcept realization of a reviewer recommender using wrc as part of a larger system .
our findings raise questions open for future research can a hybrid of existing algorithms combine the advantages of those algorithms?
can additional information like the current group of core reviewers improve the algorithms performance?
can a better time prioritization improve the prediction performance of fps?
do the tested algorithms really recommend competent reviewers or only common reviewers?
when should an algorithm make a trade o between competency and workload to relieve competent but busy reviewers?
furthermore when is a review necessary at all and which patches should undergo a more rigorous review ?
.