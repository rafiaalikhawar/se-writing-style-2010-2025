parallel data race detection for task parallel programs with locks adarsh yoga department of computer science rutgers university piscataway nj usa adarsh.yoga cs.rutgers.edusantosh nagarakatte department of computer science rutgers university piscataway nj usa santosh.nagarakatte cs.rutgers.eduaarti gupta department of computer science princeton university princeton nj usa aartig cs.princeton.edu abstract programming with tasks is a promising approach to write performance portable parallel code.
in this model the programmer explicitly specifies tasks and the task parallel runtime employs work stealing to distribute tasks among threads.
similar to multithreaded programs task parallel programs can also exhibit data races.
unfortunately prior data race detectors for task parallel programs either run the program serially or do not handle locks and or detect races only in the schedule observed by the analysis.
this paper proposes ptracer a parallel on the fly data race detector for task parallel programs that use locks.
ptracer detects data races not only in the observed schedule but also those that can happen in other schedules which are permutations of the memory operations in the observed schedule for a given input.
it accomplishes the above goal by leveraging the dynamic execution graph of a task parallel execution to determine whether two accesses can happen in parallel and by maintaining constant amount of access history metadata with each distinct set of locks held for each shared memory location.
to detect data races beyond the observed schedule in programs with branches sensitive to scheduling decisions we propose static compiler instrumentation that records memory accesses that will be executed in the other path with simple branches.
ptracer has performance overheads similar to the state of theart race detector for task parallel programs spd3 while detecting more races in programs with locks.
ccs concepts software and its engineering !dynamic analysis software testing and debugging software verification keywords data races intel tbb fork join programs .
introduction task parallelism is an effective abstraction to write performance portable code.
in a task parallel programming environment the protask t1task t2task t3s1lock l1 t c unlock l1 if t true lock l1 y x unlock l1 else y x lock l1 c true y x unlock l1 x y c false spawn t2spawn t3 syncs3s2x y and c are shared memory locationsl1 is a lock variablefigure an example task parallel program that uses locks.
there are three tasks t1 t2 andt3.
they access three shared memory variables x y andc.
there are three regions of code without any task management constructs.
they are labeled s1 s2 ands3.
the program has a data race on shared memory location y. grammer specifies the tasks and the work stealing runtime distributes these tasks to the threads.
a task parallel program can provide scalable speedups when the program is executed on a machine with different core thread count as the runtime dynamically balances the load between the threads.
task parallel frameworks like cilk intel threading building blocks tbb habanero java x10 and the java fork join framework have become mainstream.
given the promise of performance portable code there are initiatives in teaching parallelism through task parallel programming models .
task parallel programs can have data races similar to multithreaded programs.
a program exhibits a data race when there are multiple accesses to a shared memory location at least one of them is a write and there is no ordering between these accesses.
in the absence of locks a data race occurs when these accesses are not ordered by task management spawn sync constructs.
in the presence of locks a data race occurs when two parallel accesses one of which is a write are not protected by a common lock.
similar to multithreaded programs data races in task parallel programs are usually indicators of program errors.
the behavior of the program is dependent on the memory model in the presence of data races.
this is the author s version of the work.
it is posted here for your personal use.
not for redistribution.
the definitive version was published in the following publication fse november seattle wa usa c acm.
... artifact evaluated by fse 833instrumentedprogramstatic analysis and instrumentationparallel dynamic detection of racescheck feasibility of branchesinput task parallel programreport an apparent race in another trace with branch condition inverted possible apparentracesreport an apparent race with the operations in the current tracesmt formula for branch feasibilitycheck validity of smt formulatrue apparent racesinputsfigure workflow of our parallel data race detection algorithm for task parallel programs that use locks.
further it can also cause non deterministic execution.
figure illustrates an example task parallel program with a data race.
in the terminology of netzer and miller data races can be classified into two categories apparent races and feasible races.
data races that appear to occur in an execution of a program primarily considering the parallel constructs but without taking the actual computation into account are termed apparent races.
data races that occur taking into account the computation synchronization and parallel constructs are termed feasible races.
an apparent race may not be a feasible race in some scenarios where operations in critical sections influence branches.
depending on how critical sections are scheduled the computation itself may change when the tasks are scheduled in a different order.
however every apparent race is also a feasible race for a class of task parallel programs called abelian programs which have commutative critical sections.
identifying feasible races is a hard problem .
detectors that aim to detect feasible races need to perform interleaving exploration which is practically infeasible.
hence we focus on the detection of apparent races in this paper as task parallel programs have structured parallelism.
data race detectors can be classified into three categories based on their detection abilities per program per input and per schedule detectors.
per program detectors detect possible races for all inputs and schedules for a given program.
although per program detectors are appealing in theory they can report a large number of false positives in practice because of approximations in the underlying static analysis .
per input detectors detect possible races in various schedules for a given input when the programs do not use locks .
finally per schedule detectors detect data races in the observed schedule.
they typically need to be coupled with interleaving exploration to detect races that can occur in other schedules for the same input.
further data race detectors can also be classified into offline on the fly or hybrid detectors depending on whether the race is detected with a postmortem analysis during program execution or a combination of them respectively.
our goal is to detect data races in task parallel programs with the following three objectives.
first the detector should detect races in the presence of locks because frameworks like intel tbb and cilk provide various lock implementations.
second the detector should detect races in the observed schedule and also possible races in other schedules for a given input to either minimize or obviate the need for interleaving exploration.
third the detector should use multiple cores available on modern processors to enable its usage with long running programs.
although data race detection is a well studied topic for multithreaded programs existing detectors do not satisfy our goals.
fasttrack is a vector clock based detector that detects races in schedules that follow the observed happens before ordering in multithreaded programs.
when we repurpose fasttrack for tasks fasttrack s vector clock metadata with each shared memory locationis proportional to the number of tasks which makes it impractical for task parallel programs that create a large number of tasks.
our implementation of fasttrack aborted with out of memory errors with many applications see section .
prior research has also investigated numerous techniques to detect races in task parallel programs .sp bags esp bags and spd3 leverage the series parallel structure of a task parallel execution to detect races per input for task parallel programs that do not use locks.
spd3 with support for isolated blocks and all sets detect data races per schedule in task parallel programs.
both these approaches can detect races per input when the program has commutative critical sections i.e.
abelian programs .
however spd3 does not support locks and all sets runs the detector serially.
in this paper we explore if it is possible to combine spd3 andall sets to attain our objectives.
inspired by spd3 and all sets we propose an onthe fly dynamic data race detector ptracer for task parallel programs with locks.
ptracer uses the dynamic program structure tree dpst representation from spd3 to determine whether two accesses can happen in parallel.
it borrows the idea of tracking the set of locks held before an access with each entry in the metadata space from all sets .
the key challenge is in maintaining appropriate metadata when there are multiple readers and writers to a given shared memory location to enable effective and efficient detection of races.
further branch statements in a task parallel program can be influenced by scheduling decisions when the program uses locks.
such branches are called schedule sensitive branches ssb .
in the presence of ssbs the dynamic trace observed by the analysis may not have memory operations from all schedules e.g.
from the path not taken at the branch statement .
we explore if it is possible to detect races that happen in other schedules in the presence of ssbs.
to accomplish our objectives we designed ptracer with three components a static analysis and instrumentation component to instrument shared memory accesses and identify accesses in the other path with simple branch statements a parallel dynamic race detection component that extends spd3 to handle locks and a diagnosis component that classifies some of the races reported by the parallel dynamic analysis as infeasible if the branch is not schedule sensitive.
figure provides an overview of the various components in ptracer .
the static analysis and instrumentation component of ptracer has two goals instrument shared memory accesses and identify accesses that can be executed in the other path at a branch statement.
ptracer uses a compiler pass to instrument the program with calls to the dynamic race detection library.
the compiler pass identifies branch statements and records the memory accesses executed in the other path at a simple branch statement with calls to the race detection library.
when the branching structure involves loops or 834has nested branches the compiler pass informs the user that the dynamic race detection will be restricted to the detection of apparent races in the observed schedule and schedules that are permutations of memory operations in the observed schedule.
the dynamic data race detection component of ptracer constructs the dpst as the program executes and performs dynamic data race detection when the race detection library calls execute.
hence the race detection happens in parallel.
ptracer maintains two reads and two writes with each distinct set of locks held before an access to a shared memory location in the metadata space for each shared memory location.
maintaining information about two reads and a single write is sufficient when accesses are performed without holding any locks.
when there are multiple writes to a shared memory location with the same set of locks held ptracer maintains two write operations so that all other parallel write operations to the same location are in the subtree under the least common ancestor lca of the two writes in the dpst.
any future write or a read operation that may execute in parallel with any of the not maintained accesses will execute in parallel with at least one of the two writes maintained in the metadata space.
the dynamic data race detector reports a race when two accesses are not protected by a common lock.
the races reported by the dynamic race detector are either true apparent races that involve operations in the observed schedule or possible apparent races that involve races between operations from the observed schedule and operations in the not taken path at a branch statement.
finally the diagnosis component of ptracer filters out possible races by checking the schedule sensitivity of the branch resulting in the race.
the diagnosis component runs the program again to obtain a detailed trace.
it encodes the detailed trace and the inverted branch condition of the schedule sensitive branch involved with the possible apparent race as a first order logic formula and checks its satisfiability similar to prior work .
if the formula is satisfiable then the diagnosis component reports the race as an apparent race to the user.
ptracer detects apparent races for a given input for abelian programs similar to all sets .
when there are multiple races involving a shared memory location ptracer reports a single race to the user.
ptracer detects apparent races in the observed schedule and in schedules that are permutation of the memory operations in the observed schedule for non abelian programs.
some of these races may not be feasible if the computation in the program forbids them.
even the infeasible apparent races are likely program errors as task parallel programs typically have structured communication.
the race detection involving operations from non taken paths is best effort i.e.
it can miss races with ssbs due to the limitations of our static analysis.
our prototype detector ptracer detects data races in intel tbb programs that use locks.
the prototype detects all races in our test suite which has unit tests with locks without false positives and without requiring interleaving exploration.
in contrast fasttrack misses many races and spd3 reports false positives as it does not handle locks.
ptracer is usable with long running applications and has performance overhead similar to spd3 .
.
background this section provides background on dynamic program structure tree dpst representation of a task parallel execution to identify parallel accesses.
as we use the dpst and build on spd3 we also provide a brief background on the spd3 data race detector for task parallel programs.
f11s1f12a2a3s2s3figure dpst for the sample task parallel program in figure after it has executed all statements.
there are three step nodes s1 s2 ands3 in the dpst which are depicted by nodes with two circles and are executed by tasks t1 t2 and t3respectively.
the step nodes s2ands3can occur in parallel because lca s2 s3 isf12 and the left child of f12 is an async node and it is an ancestor of s2.
the step nodes s1ands2cannot occur in parallel aslca s1 s2 is a finish node f11 and its left child that is also an ancestor of s1is not an async node.
.
dynamic program structure tree the execution of a task parallel program results in a series parallel execution graph.
the series parallel execution graph can be used to determine whether two accesses by different tasks can logically execute in parallel .
the graph can be used to find apparent races in other schedules when these programs do not use locks.
however the program has to be executed serially which results in performance overheads.
to address the problem of serial execution with data race detection raman et al.
proposed an approach to check whether two accesses can logically execute in parallel using an ordered tree called the dynamic program structure tree dpst .
the dpst captures the dynamic parent child relationship between tasks which enables parallel race detection with spd3 .
the dpst consists of three types of nodes a step nodes b finish nodes and c async nodes.
a step node in the dpst represents the maximal sequence of instructions without any task spawn for task creation and sync join statements.
all computation and memory accesses occur in the step nodes.
hence every memory access has a corresponding step node associated with it.
further the step nodes are always leaf nodes in the dpst.
the async nodes capture the spawning of a task by a parent task.
the descendants of an async node execute asynchronously with the remainder of the parent task.
a finish node is created when a task spawns a child task and waits for the child and its descendants to complete.
a finish node is the parent of all async finish and step nodes directly executed by its children or their descendants.
a node s children in the dpst are ordered left to right to reflect the left to right sequencing of computation in their parent task.
the dpst s construction ensures that all internal nodes are either async or finish nodes.
the path from a node to the root and the leftto right ordering of siblings in the dpst do not change even when nodes are added to the dpst during execution.
the construction of the dpst ensures that two step nodes s1ands2 assumings1is to the left ofs2 can execute in parallel if the least common ancestor ofs1ands2 i.e.
lca s1 s2 in the dpst has an immediate childathat is an async node and is also an ancestor of s1.
consider the example task parallel program in figure with three tasks t1 t2 and t3.
figure presents the dpst after all tasks and instructions in the program in figure have executed.
there are three step nodes s1 s2 ands3.
there are two finish nodes f11 835that corresponds to the implicit finish with the main task and f12 that corresponds to the collection of tasks t1andt2followed by a sync statement.
the step nodes s2ands3can occur in parallel since the lca s2 s3 isf12 and its left child is an async node.
in contrast step nodes s1ands2cannot occur in parallel since the lca s1 s2 isf11 and its left child that is also an ancestor of s1iss1 which is not an async node.
similarly step nodes s1and s3cannot occur in parallel.
.
spd3 race detector any dynamic race detector needs to determine if two accesses at least one of them is a write can happen in parallel and track accesses to the same location.
spd3 uses the dpst to determine if two accesses can occur in parallel.
it maintains shadow memory for each memory location that tracks tasks that have accessed the same location.
rather than maintaining information about every access to a shared memory location by tasks spd3 maintains a total of two reads r1andr2 and a write w1 with every shared memory location in shadow memory.
it is sufficient to maintain information about one write as all other writes should either occur in series or constitute a data race.
however there can be multiple readers and it is necessary to maintain information about them.
when there are multiple parallel readers to a location spd3 stores two reads r1andr2such that the subtree under lca r1 r2 in the dpst includes other reads.
in contrast to vector clock based detectors spd3 maintains constant number of access history entries with each monitored shared memory location irrespective of the number of tasks.
spd3 detects races for a given input by examining a single trace provided the task parallel program does not use locks.
next we describe how to handle locks and propose a technique to detect races not only in the current trace but also in other schedules for a given input.
.
approach our goal is to design an on the fly data race detector for task parallel programs with the following attributes runs in parallel handles programs that use locks and detects data races that occur in different schedules for a given input by examining a single trace.
we do not need to store long traces with an on the fly detector.
a parallel data race detector reduces performance overheads by leveraging multi cores.
handling locks enables us to detect races in applications written with frameworks such as intel tbb and cilk that support locks.
detecting races that can happen in other schedules by examining a single schedule for a given input either minimizes or obviates in the best case the need for interleaving exploration.
we are primarily focused on detecting apparent races which are races that appear to occur taking into account the parallel constructs in the program .
in the presence of critical sections some of these apparent races may not be feasible when the actual computation performed by the task is considered along with the parallel and synchronization constructs.
detecting feasible races typically requires interleaving exploration and covering all interleavings is not possible in practice.
however every apparent race is a feasible race for a class of programs i.e.
abelian programs with commutative critical sections .
when the program contains non commutative critical sections we investigate if it possible to detect races that can happen in other schedules to minimize the need for interleaving exploration.
in such scenarios our goal is to detect apparent races that can happen in other schedules which perform the same shared memory accesses task t1task t2task t3s1lock l1 t c unlock l1 if t true lock l1 y x unlock l1 recordrd x recordwr y else y x recordwr y l1 recordrd x l1 lock l1 c true y x unlock l1 x y c false spawn t2spawn t3 syncs3s2x y and c are shared memory locationsl1 is a lock variablefigure the task parallel program in figure instrumented with recordrd andrecordwr instrumentation calls using static analysis.
there is a recordwr y in the if block that is executed when cis true in task t2because shared memory locationyis written in the else branch without any lock acquisitions and releases from the start of the else block.
similarly there is recordwr y l1 in the else block because memory location yis written in the if branch after acquiring lock l1and before releasing any lock from the beginning of the if block.
but possibly in a different order for a given input.
this guarantee is similar in spirit to the guarantees aimed by predictive testing techniques for multithreaded programs .
in the presence of critical sections the branch statements in the program can also be influenced by the scheduling of critical sections.
such branches are called schedule sensitive branches ssbs .
a schedule observed by the dynamic analysis may not contain memory accesses from the not taken path of a schedule sensitive branch.
we propose a static instrumentation technique to record memory accesses that will be executed in the not taken branch.
this approach enables us to detect apparent races that can occur in different schedules of a program with ssbs for the same input without requiring interleaving exploration.
to filter false positives when the branch is not schedule sensitive we encode the trace and the inverted branch condition as a first order logic formula and check its satisfiability.
three components of our proposed detector.
our proposed detector ptracer consists of three components to accomplish the above goals static analysis and instrumentation component to instrument the program with calls to the dynamic race detection library to construct the dpst record shared memory accesses in the presence of branches and to detect races a parallel dynamic analysis component that executes when the task parallel program executes and detects races and a diagnosis component that checks the feasibility of the reported races involving memory accesses from the not taken path at a branch statement.
figure illustrates the three components of ptracer which we describe in detail below.
.
static instrumentation component the static analysis and instrumentation component has three objectives add instrumentation to identify task management constructs to build the dpst at runtime add instrumentation to identify shared memory accesses and lock operations to perform 836procedure dataracedetector l s a ls ah metadata l for all p2ah do ifp ls ls then ifa rd dmhp s p w then report write read race between p w 1ands end if ifa rd dmhp s p w then report write read race between p w 2ands end if ifa wr dmhp s p w then report write write race between p w 1ands end if ifa wr dmhp s p w then report write write race between p w 2ands end if ifa wr dmhp s p r then report read write race between p r1ands end if ifa wr dmhp s p r then report read write race between p r2ands end if end if ifp ls lsthen .update the metadata for lockset ls ifa wr then if dmhp s p w dmhp s p w then p w s p w null end if ifdmhp s p w dmhp s p w then lca12 lca p w p w lca1s lca p w s lca2s lca p w s iflca1s dpstlca12 lca2s dpstlca12then p w s end if end if end if ifa rdthen if dmhp s p r dmhp s p r then p r1 s p r2 null end if ifdmhp s p r dmhp s p r then lca12 lca p r1 p r lca1s lca p r1 s lca2s lca p r2 s iflca1s dpstlca12 lca2s dpstlca12then p r1 s end if end if end if end if end for ifls 2metadata l then create a new lockset lsand add a new entry for l end if return end procedure figure algorithm to check for a data race on memory access to locationlby the step node swith access type aand lockset ls.metadata l function returns the access history in shadow memory associated with locationl.
the predicate dmhp dynamic may happen in parallel is used to determine if two accesses are parallel.
the predicate dmhp si sj returns true if the step nodes siandsjin the dpst can execute in parallel.
lca si sj returns the least common ancestor node of siandsj.
dynamic race detection and add instrumentation to identify shared memory operations that are possibly executed in the other path in the presence of a schedule sensitive branch statement whichis inspired by a similar attempt for property driven pruning with dynamic partial order reduction .
the static analysis and instrumentation phase can be performed either on the source code or within the compiler.
we perform this phase within the compiler for mainly two reasons a the compiler already performs various analyses to identify thread local accesses and the branch statements and b the instrumentation can be performed on optimized code which reduces the performance overhead of race detection.
the compiler pass that adds calls to the race detection library to identify task management constructs and shared memory accesses is straightforward which we omit for space constraints.
the interesting aspect is the addition of instrumentation to identify memory accesses performed in both the taken and the not taken paths in the presence of schedule sensitive branches.
our algorithm to record memory accesses is described below.
first the compiler pass identifies conditional and unconditional branch statements and their corresponding join statements.
second the compiler pass identifies the set of memory operations performed the set of locks acquired and released before every memory operation in the taken and the not taken branch from the beginning of the branch for each branch statement and its corresponding join statement.
if the branch condition does not dominate either the memory access or the lock variable then we need to perform additional work to make these accesses visible in the other branch.
in such scenarios we add the backward static program slice of the memory access and or the lock variable in the other branch.
finally the compiler inserts calls to the runtime library to record the memory operation performed in the other path.
if a memory access ais written in the else block of a schedule sensitive branch with lalocks acquired and lrlocks released from the beginning of the else block till memory operation a then the compiler introduces a runtime call recordwr a la lr in the if block.
figure shows the additional recordwr andrecordrd instrumentation in the if branch and the else branch.
in the if branch the additional recordwr andrecordrd instrumentation correspond to memory accesses in the else branch.
this instrumentation enables us to detect data races that can occur with operations in the else branch even when the trace observed during the dynamic analysis contains only operations from the if branch.
limitations.
we primarily focus on simple branch statements that are not nested and are not part of loops to record memory accesses from the not taken path.
our static analysis informs the user about the presence of non nested branches and branches that are part of loops.
in the presence of such branches our framework still detects apparent races that occur in other schedules whose memory operations are a permutation of the memory operations in the observed trace.
the taken and not taken paths at a branch can include function calls provided it is non recursive without nestedbranches and loops.
we chose this design point to avoid false apparent races.
however our race detector will miss some races given these limitations.
.
parallel dynamic data race detector ptracer detects data races when the program executes the library calls introduced by the static instrumentation component.
as tasks execute in parallel the race detection also happens in parallel.
the dynamic race detector component of ptracer maintains two pieces of information at runtime the dpst and the metadata.
the dpst is constructed at runtime and queried to determine if two accesses can occur in parallel.
the metadata is maintained with each shared memory location that provides information about prior accesses by various tasks.
837metadata design.
a naive approach to detect apparent races in the observed schedule and other schedules involving the same memory operations would maintain a list of accesses performed by various tasks with each shared memory location.
as each access to a shared memory location can occur with different sets of locks held the access history should also maintain information about the set of locks held lockset before performing a memory access.
however such an approach would make the metadata proportional to the number of dynamic memory accesses and is infeasible in practice.
our contribution is in designing a dynamic data race detection algorithm that maintains a constant number of access history entries which is independent of the number of tasks and the number of dynamic memory accesses while handling locks.
our metadata for each shared memory location contains four access history entries step nodes of two reads r1andr2 and two writes w1andw2 for each distinct set of locks held before the access.
although the size of the metadata is proportional to the number of distinct sets of locks held for each memory location we observe in practice that each shared memory location is accessed with similar sets of locks.
in summary the access history with each shared memory location can be conceptually viewed as an array of data nodes where each data node contains the unique lockset and step nodes corresponding to two reads r1andr2 and two writes w1andw2 .
metadata checks on a shared memory access.
figure provides the algorithm for checking the metadata on a shared memory access.
the algorithm iterates over all access history entries corresponding to each lockset in the metadata space.
first the algorithm checks if the intersection of the lockset of the current access and each lockset in the metadata space is empty.
if the intersection is empty two accesses have been performed without a common lock andptracer reports a race if two accesses can occur in parallel and at least one of them is a write.
updating the read metadata.
after the check the algorithm in figure updates the metadata corresponding to the appropriate lockset.
if the current access is a read access then the metadata is updated similar to spd3 except that the access history entries for a particular lockset are updated.
if the current read access is in series with both the reads r1andr2 corresponding to the current lockset then r1is set to the current access and r2is set to null a unique empty value .
when there are multiple readers that can execute in parallel i.e.
two existing readers r1andr2in the metadata space and current access ptracer maintains two reads in the metadata space such that the subtree under lca r1 r2 includes all reads similar to spd3 .
the key insight is that any future access that can have a data race with the not stored reads will also have a data race with r1and orr2.
updating the write metadata for a lockset.
updating the metadata in the presence of write operations and locksets requires some thought in comparison to spd3 .
when writes are performed without locks any two parallel writes is a data race.
when tasks use locks two writes can happen in parallel but may be protected by the same lock.
hence they do not constitute a data race.
when ptracer sees multiple parallel writes current access w1andw2 in the metadata space with the same lockset it needs to identify two writes to maintain in the metadata space.
similar to multiple parallel reads ptracer maintains two writes in the metadata space such that the subtree under lca w1 w2 includes all writes.
any future access that can race with one of the not stored writes will also race with at least one of w1orw2.
maintaining only two reads and two writes with each distinct set of locks enables ptracer to detect apparent races both in the observed schedule and other .
t1 s1 x .
t1 s1 y .
t1 s1 c false .
t1 spawn t2 .
t1 spawn t3 .
t3 s3 lock l1 .
t3 s3 c true .
t3 s3 y x .
t3 s3 unlock l1 .
t2 s2 lock l1 .
t2 s2 t c .
t2 s2 unlock l1 .
t2 s2 if t true .
t2 s2 lock l1 .
t2 s2 y x two writes to y with lock set l1 by step nodes s3 and s2.
.
t2 s2 unlock l1 .
t2 s2 recordrd x .
t2 s2 recordwr y write write race for y based on its access in the other branch as s2 and s3 occur in parallel19.
t1 sync timeobserved tracemetadata for yfigure illustration of race detection with a concrete trace of the program in figure .
the observed trace provides the instruction executed the task and the step node performing the operation observed in the trace.
the metadata in shadow memory for the shared memory variable yis also shown.
the metadata for a shared memory location is a list of access histories with each lockset.
there are four access histories for each lockset w1 w2 r1 r2 .
on time step yis written by step node s1in task t1 without holding any lock.
we create a new entry for the empty lockset and update the access history corresponding to the write by s1i.e.
s1 null null null .
similarly when yis written with lockset l1 by step node s3in task t3at time step the race detector checks if this access results in a race according to existing history and updates the metadata of y with a new lockset l1and the current write.
it is important to note that when yis written by step node s2 in task t2at time step there is already a write to yin the metadata space with lockset l1.
since this write by s2can occur in parallel with the existing write we maintain both writes in the metadata for y. finally the record instrumentation enables us to find races that would occur when the not taken branch is executed on a different schedule.
schedules for the same input.
figure illustrates the metadata for shared memory variables checks and the metadata update actions performed after each statement in the observed trace.
.
diagnosis phase the parallel data race detection algorithm described above reports two kinds of apparent races which we call true races and possible races.
a true race is a data race that occurs between the operations of the observed trace without involving memory operations from the recordwr andrecordrd instrumentation.
a possible race is a data race that involves shared memory operations from the otherbranch instrumentation.
if the branch is not schedule sensitive then these possible races will never manifest for a given input.
hence we propose a diagnosis phase to identify whether the branch is schedule sensitive when our parallel race detection algorithm reports a possible race.
we divide the diagnosis phase into two components execution trace generator that generates per task execution traces for the reported possible race and a constraint generator that checks if the branch statement responsible for the reported possible race is schedule sensitive by transforming the trace into a first order logic 838a trace representationper task trace canonicalized trace1.
x wx .
.
y wy .
.
c false wc .
false .
spawn t2 spawnt2 .
.
spawn t3 spawnt3 .
.
sync synct2 t3 .6t1 per task trace canonicalized trace1.
lock l1 ll1 .
.
c true wc .
true .
y x rx .
wy .
x .
.
unlock l1 ul1 .4t3per task trace canonicalized trace1.
lock l1 ll1 .
.
t c rc .
wt .
c .
.
unlock l1 ul1 .
.
if t true rt .
.
lock l2 ll2 .
.
y x rx .
wy .
x .
.
unlock l2 ul2 .
negated branch check t .
!
truet2b constraints per task order constraints w1.
w1.
w1.
l2.
r2.
w2.
u2.
r2.
l2.
r2.
w2.
u2.
l3.
w3.
r3.
w3.
u3.
synchronization constraints u3.
l2.
u2.
l3.
read write constraints c2.
false w1.
r2.
w3.
w1.
r2.
w3.
c2.
true w3.
r2.
w1.
w3.
r2.
w1.
t2.
c2.
w2.
r2.
spawn sync constraints w1.
l2.
w1.
l3.
negated branch check constraint t2.
!
truefigure a concrete per task traces and canonicalized trace representation.
for every statement in the per task trace we create an order variable.
the order variable for a write operation performed by statement jin task tiis represented by wi j. similarly the order variables for the read lock and unlock operations performed in statement jby task tiis represented by ri j li j andui j respectively.
the elements within the same task are ordered.
hence we havew1 w w .
there are two value variables c2 andt2 representing symbolic read operations that directly or indirectly influence the branch condition.
the read write constraints connect the value variables and the order variables.
the synchronization and spawn sync constraints further restrict the feasible orderings.
if the negated branch condition is satisfiable with the constraints then the branch is a schedule sensitive branch and the race is reported to the user.
formula and checking its satisfiability using an smt solver.
our constraint generator and schedule sensitivity checker is inspired by prior work .
we enforce an ordering corresponding to the possible race and repurpose prior approaches to a task based context.
per task execution trace generator.
when the dynamic race detector reports a possible race we enforce the schedule corresponding to the possible race.
this step is necessary because the dynamic race detection algorithm does not log the trace as it wants to detect races with a low performance overhead.
we also generate canonicalized per task execution traces.
the canonicalized per task execution trace captures all loads stores synchronization constructs and the branch condition corresponding to the possible race.
to check if the branch is schedule sensitive we need to check if the negation of the branch condition involved in a possible race is satisfiable.
figure a shows the per task trace and the canonicalized trace for an execution of the program in figure .
constraint generator.
using the per task traces we construct a first order logic formula to check for the satisfiability of the negated branch condition.
inspired by prior work we create two types of variables order variables andvalue variables .
the order variables are used to encode the position of the operation in the per task trace.
the value variables are used to symbolically encode the read operations.
since we are specifically interested in checking the satisfiability of the negated branch condition we create valuevariables for only those reads that directly or indirectly influence the branch condition.
we generate a formula that relates these order and value variables using the task parallel execution constraints and checks the negated branch condition.
po sync rw ss br where pois the constraint that encodes the order of execution of operation in a given task sync is the constraint that represents the possible orders of execution among the synchronization statements among tasks rwis the constraint that encodes the data flow between various accesses to the same location ssis the constraint that represents the parent child relationship between tasks and br is the constraint representing the negated branch condition.
satisfiability of implies that the branch involved in a possible race is a schedule sensitive branch and the reported race is indeed feasible.
generating the constraints.
the per task order constraints po ensure that the operations within the same task are ordered.
the synchronization constraints sync order the lock and unlock operations performed by the tasks on the same lock.
let li m ui n andlj p uj qbe two lock unlock order variables from two different tasks i6 j on the same lock.
there are two possible orderings in this scenario.
when the lock statement from task iis executed first thenlj pis executed only after the unlock statement ui n which results in an ordering constraint ui n lj p. otherwise the lock statement from task jis executed first which results in an ordering constraintuj q li m. hence the synchronization constraint is a disjunction of these two constraints.
the read write constraint rw connects the value variable and order variables corresponding to writes to the same location.
for every read operation r our constraint specifies that it reads the value of a particular write wifwhappens before rand every other write happens before wor after r. since we check for the satisfiability of the negated branch condition we consider only those reads and writes that directly or indirectly affect the branch condition.
figure b illustrates the constraints generated for the per task traces in figure a .
in summary if the constraints generated are satisfiable then the branch is a schedule sensitive branch and the not taken path at a branch statement will be executed for the same input.
hence we report all such possible races involved with schedule sensitive branches to the user.
.
implementation this section describes the metadata encoding and the implementation optimizations that we use to reduce the performance overhead of data race detection.
.
metadata organization the metadata for each shared memory location is stored in shadow memory.
we implement shadow memory using a two level lookup trie data structure as it provides the ability to shadow every address in memory efficiently.
a trie is a page table like structure where each level is accessed using few bits from the address whose metadata is being looked up.
ptracer maps a bit virtual address space using a two level trie .
the first level trie mappings are allocated at program initialization and the second level entries are allocated on demand when a memory location is touched for the first time which reduces the memory overhead.
metadata encoding.
the metadata associated with a shared memory location is a list of four access history entries for each lockset.
maintaining linked data structures in the shadow space increases the performance overhead.
in practice we observe that 839most shared memory locations are accessed with a small number of locksets.
hence we accelerate the common case i.e.
accesses with few locksets by organizing the entry in shadow memory as a constant sized array of data nodes.
each data node represents the access history for a given lockset.
if a shared memory location is accessed with more locksets than the constant sized array of data nodes then we resort back to the slower list representation for that shared memory location.
each data node contains five bit values a bit value encoding the lockset two bit values for representing the step nodes performing the reads r1andr2 and two bit values for representing the step nodes performing the writes w1andw2.
these implementation techniques enabled us to successfully run parallel data race detection on long running programs.
.
optimizations we observed three major opportunities for reducing the performance overhead in our implementation choosing appropriate data structures for the dpst identifying redundant checks and identifying redundant lca queries on the dpst.
we describe these optimizations below.
overlay dpst in a linear array.
rather than building the dpst using a linked n ary tree data structure we optimize the layout of the dpst by overlaying the tree in a linear array of nodes.
we maintain parent child relationship in such an overlay by maintaining the index of the parent node with each child node.
we achieve better locality avoid pointer chasing code and avoid the cost of frequent dynamic allocations by overlaying the dpst in a linear array of nodes.
this representation reduces the overhead of a single lca query when compared to the linked data structure because it eliminated several pointer indirections in the traversal of the dpst.
access caching.
we observed that there were multiple accesses to the same location with the same lockset from a given step node.
however we were not able to prove that they are redundant accesses through static analysis.
we observe that when there are multiple accesses of the same type with the same lockset in a step node then it is sufficient to perform the check once and store the metadata for only one access.
we reduce the overhead of metadata checking and propagation by caching accesses performed in the task and not performing the check on accesses that have an entry in the cache with the same lockset and access type.
lca caching.
lca queries are expensive even after optimizing the layout of the dpst because each query can traverse a large number of nodes.
moreover our data race detection algorithm performs lca queries on each access to check and propagate metadata.
we observe that even when previously unseen addresses are being checked there are opportunities to cache lca queries.
lca queries check whether two step nodes can occur in parallel.
when these step nodes have been previously accessed in a lca query it is not necessary to perform the query again as the series parallel relationship between the tasks does not change as nodes are being added to the graph.
hence we cache the frequently performed lca queries to reduce the overhead resulting from the repeated traversals of the dpst.
these optimizations not only reduced the overhead of our implementation but also reduced the overhead of spd3 the baseline that we compare against in our evaluation.
.
experimental ev aluation this section describes our prototype implementation optimizations benchmarks and experimental evaluation to measure the effectiveness and the performance overhead of data race detection.table we report the number of dynamic shared memory accesses the number memory accesses due to record instrumentation to capture operations from the not taken path at a branch statement the number of least common ancestor queries the percentage of unique lca queries and the percentage of accesses that hit in the access cache for each benchmark.
we use m for million in the table.
benchmark no.
of accessesno.
of other branch accessesno.
of lcaspercent.
of unique lcaspercent.
of access cache hits blackscholes 140m 253m .
.
bodytrack .48m .97m .
.
fluidanimate .49m .08m .
.
streamcluster 257m 854m .
.
swaptions 301m 924m .
.
convexhull .07m .11m .
.
delrefine 153m 328m .
deltriang 20m .36m .
karatsuba 115m 152m .
.
kmeans 118m 147m .
.
nearestneigh 76m 134m .
.
raycast 128m 655m .
.
sort .74m .14m .
.
prototype.
our prototype ptracer is designed for c programs that use intel threading building blocks tbb for task parallelism.
it includes a compiler intermediate representation instrumenter a race detection library that performs runtime race detection an execution trace generator and a constraint generator.
the instrumenter is implemented as a compiler pass in clang llvm .
.
it inserts calls to the race detection library at shared memory accesses synchronization statements and task management statements.
we use the demangled name of the library calls to identify synchronization and task management statements.
the instrumenter also identifies accesses performed in both the taken and not taken paths of a branch statement.
the race detection library is written in c .
the runtime library builds the dpst performs metadata propagation and checks for data races as described in figure .
the constraint generator is written in python.
it parses the per task traces creates order and value variables and constructs the first order logic formula to check the feasibility of a schedule sensitive branch.
ptracer uses z3 to check the satisfiability of the generated formula.
our tool is open source .
benchmarks.
we evaluate the performance overheads of our prototype with thirteen tbb applications which include five tbbbased applications from parsec five geometry and graphics applications from the problem based benchmark suite pbbs and three applications from the structured parallel programming book .
the pbbs applications were originally implemented using cilk .
we translated these applications to use intel tbb for task parallelism.
table lists the applications used and their important features.
evaluation environment.
the experiments were performed on a .00ghz four core intel x86 i7 processor with gb of memory running bit ubuntu .
.
.
each benchmark was executed five times and the reported performance overhead is calculated by taking the average of the five executions.
we use geometric mean to report average slowdown in our evaluation.
8400x5x10xslowdownptracer spd3 blackscholesbodytrackfluidanimate streamclusterswaptions convexhulldelrefine deltriang karatsubakmeans nearestneighraycastsort geo.mean11x 11x 13x11x 12xfigure execution time slowdown of ptracer and spd3 when compared to a baseline without any instrumentation.
0x5x10xslowdownptracer with other branch ptracer without other branch blackscholesbodytrackfluidanimate streamclusterswaptionsconvexhulldelrefine deltriang karatsubakmeans nearestneighraycastsort geo.mean11x 11x 13x13x 12x12x figure execution time slowdown of ptracer with and without data race detection with record instrumention from the not taken path of a branch statement.
effectiveness in detecting data races.
to test the effectiveness of our prototype in detecting races we have built a test suite of unit tests that include racy and non racy programs with and without locks and schedule sensitive branches.
there were races in total in the test suite.
ptracer successfully detects all the races in the racy suite without any false positives.
in contrast spd3 detected races reported false positives and missed races.
we ran spd3 multiple times to detect races in the presence of schedule sensitive branches.
there were races that were not detected by spd3 even after performing multiple executions.
performance overhead in comparison to spd3 .figure reports the performance overhead of ptracer andspd3 when compared to a baseline without any instrumentation.
we use lca caching by default even with spd3 .
there are two bars for each benchmark smaller bars are better as it reports overheads .
the average performance overhead of ptracer is .
.
although the average overhead is .
four applications streamcluster swaptions delrefine andraycast have overhead greater than .
among them streamcluster andswaptions have large number of race detection checks and they perform a large number of lca queries.
applications delrefine andraycast have relatively fewer accesses but have no locality in their lca queries and race checks see table .
the performance overhead of spd3 is .
.spd3 maintains constant metadata for every shared memory location and does not detect data races in programs that use locks.
ptracer has similar overheads when compared to spd3 and detects more races in the presence of locks and schedule sensitive branches.
performance overhead in comparison to fasttrack.
we compared the performance overhead of our implementation of fasttrack for tasks when compared to a baseline without any instrumentation.
fasttrack aborted with out of memory errors with three applica0x5x10xslowdownptracer with access caching ptracer without access caching blackscholesbodytrackfluidanimate streamclusterswaptionsconvexhulldelrefine deltriang karatsubakmeans nearestneighraycastsort geo.mean11x14x11x14x 13x14x 12x 11x 12x12xfigure execution time slowdown of ptracer with and without access caching.
0x5x10xslowdownptracer with lca caching ptracer without lca caching blackscholesbodytrackfluidanimate streamclusterswaptionsconvexhulldelrefine deltriang karatsubakmeans nearestneighraycastsort geo.mean11x15x11x14x 13x15x 11x 13x 12x12x figure execution time slowdown of ptracer with and without lca caching.
tions streamcluster delrefine andraycast .
these applications create many tasks and have a large number of shared accesses that prevent the use of optimized version of vector clocks.
the average performance overhead of fasttrack for other applications which did not abort with out of memory errors is .
performance impact of record instrumentation from the nottaken path.
figure reports the performance overhead of ptracer with and without the record instrumentation to capture operations from both the paths of a branch statement.
the record instrumentation has increased the overhead of data race detection from .
to .
.
with a nominal increase in performance overhead the record instrumentation enables detection of races in the presence of schedule sensitive branches.
performance benefits with access caching.
figure reports the effect of the access caching optimization on the performance overhead of ptracer .
the access caching optimization reduces the average performance overhead from .
without access caching 8410x5x10xslowdown1 thread threads threads blackscholesbodytrackfluidanimate streamclusterswaptionsconvexhulldelrefine deltriang karatsubakmeans nearestneighraycastsort geo.mean11x11x11x12x11x11x 14x13x13x 13x12x12xfigure execution time slowdown of ptracer when executed with and threads.
to .
.
most applications benefit from access caching.
three applications fluidanimate convexhull and sort have significant reduction in overhead because a large fraction of the accesses hit in the access cache and do not perform the costly data race check and the metadata update.
applications bodytrack delrefine deltriang andraycast do not benefit from access caching since most locations are accessed once in a given step node.
performance benefits with lca caching.
figure reports the performance overhead of ptracer with and without lca caching when compared to a baseline without any instrumentation.
on average lca caching reduces the performance overhead from .
without lca caching to .
.
all applications except raycast see a significant reduction in overheads as there are fewer unique lca queries.
application raycast does not benefit much from lca caching because of the large number of unique lca queries.
performance overhead for different number of threads.
figure reports the performance overhead of ptracer when executed by restricting the task parallel runtime to use and threads.
the average overhead when executed with and threads is .
.
and .
respectively.
the average overhead is almost constant with increasing core count which indicates that our approach scales well when the program is executed on machines with larger number of processors.
.
related work data race detection in multithreaded programs.
there is a large body of work on dynamic data race detection in thread based parallel programs .
fasttrack represents the state of the art in dynamic data race detection for threaded programs.
fasttrack detects data races in the given execution by tracking happens before relations between shared memory accesses.
the overhead of race detection with fasttrack can be high when executed on programs that create many threads as the metadata is proportional to the number of threads.
further fasttrack only detects races that occur in a given schedule.
eraser uses a lockset based approach and checks for errors in locking discipline.
lockset based approaches often have lower performance overhead when compared to happens before based approaches but can report false positives.
there are dynamic approaches that attempt to reduce the overhead of race detection through sampling.
but these approaches often miss data races while trying to reduce the overhead.
there are several proposals for static race detection .
while static detection approaches are appealing as they have no runtime performance overhead they can produce a large number of false positives.predictive testing for threaded programs.
there are also numerous approaches that attempt to detect races and other concurrency errors feasible in a different schedule derived from a trace of a multithreaded program .
predictive testing ideally can detect feasible races for a given input as long as all operations that can occur in different thread schedules occur in the observed trace.
however most predictive testing techniques bound the instruction window up to 4k instructions to make it practical.
ptracer provides guarantees similar to an ideal predictive testing by leveraging the structure of the task parallel execution and by maintaining appropriate metadata.
further it detects races that can occur in other schedules in the presence of schedule sensitive branches using record instrumentation.
data race detection in task parallel programs.
the approach proposed by mellor crummey et al.
and nondeterminator were seminal in proposing the detection of apparent data races in task parallel programs using the series parallel execution graph.
subsequently these techniques have been enhanced to handle locks to handle task graphs in habanero java and to detect races without serial execution with spd3 .
our proposed research uses the dpst representation in spd3 and is inspired by the access histories in the all sets algorithm for cilk .
determinacy checkers.
in the absence of synchronization data race freedom ensures determinism .
even in deterministic programs there can be a large number of schedules for different inputs.
there are proposals that memoize past schedules and limit the execution to a set of input covering schedules .
tardis checks for determinism by maintaining a log of accesses and identifying conflicting accesses between tasks.
in contrast our approach detects data races both in the presence and absence of synchronization operations.
.
conclusion this paper addresses the problem of detecting apparent data races in task parallel programs with a parallel detector that handles locks.
the key insight is to leverage the execution graph of a task parallel program to determine if accesses can occur in parallel and design metadata that tracks a constant number of access histories for each lockset held before an access to a shared memory location.ptracer uses static analysis and instrumentation to identify operations that can happen in the not taken path in the presence of schedule sensitive branches and detects apparent races that can occur in other schedules for a given input.
in summary ptracer is a data race detector for task parallel programs that a runs in parallel b detects data races when the program uses locks maintains per location metadata that is independent of the number of dynamic accesses and detects apparent races not only in the observed schedule but also in other schedules for a given input.
.