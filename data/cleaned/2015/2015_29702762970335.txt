greedy combinatorial test case generation using unsatisfiable cores akihisa y amada university of innsbruck austria akihisa.yamada uibk.ac.atarmin biere johannes kepler university austria biere jku.at cyrille artho takashi kitamura eun hye choi national institute of advanced industrial science and technology aist japan c.artho t.kitamura e.choi aist.go.jp abstract combinatorial testing aims at covering the interactions of parameters in a system under test while some combinations may be forbidden by given constraints forbidden tuples .
in this paper we illustrate that such forbidden tuples correspond to unsatis able cores a widely understood notion in the sat solving community.
based on this observation we propose a technique to detect forbidden tuples lazily during a greedy test case generation which signi cantly reduces the number of required sat solving calls.
we further reduce the amount of time spent in sat solving by essentially ignoring constraints while constructing each test case but then amending it to obtain a test case that satis es the constraints again using unsatis able cores.
finally to complement a disturbance due to ignoring constraints we implement an e cient approximative sat checking function in the sat solver lingeling .
through experiments we verify that our approach signi cantly improves the e ciency of constraint handling in our greedy combinatorial testing algorithm.
ccs concepts software and its engineering !software testing and debugging keywords combinatorial testing test case generation sat solving .
introduction combinatorial testing cf.
aims at ensuring the quality of software testing by focusing on the interactions of parameters in a system under test sut while at the same time reducing the number of test cases that has to be executed.
it has been shown empirically that a significant number of defects can be detected by t way testing which tests all t way combinations of parameters at least once where tis a relatively small number.
constraint handling mentioned already by tatsumi in the late 80s remains as a challenging research topic in combinatorial testing .
to illustrate the concept we take a simple web application example which is expected to work in various environments listed as follows parameter values cpu intel amd os windows linux mac browser ie firefox safari combinatorial testing aims at covering all combinations of values but not all of them are necessarily executable e. g. we have the following constraints .
ie is available only for windows.
.
safari is available only for mac.
.
mac does not support amd cpus.
thus one must take care of such combinations which cannot be executed called forbidden tuples .
in the above example there are six forbidden tuples flinux ieg flinux safarig famd macg etc.
there is substantial work on combinatorial testing taking constraints and forbidden tuples into account includingmeta heuristic approaches sat based approaches and greedy approaches which is further categorized into one test at a time otat approaches and in parameter order generalized ipog approaches .
meta heuristic approaches and sat based approaches often generate test suites that are smaller than the greedy approaches although they usually require more computation time cf.
.
thus these approaches are preferred in case the cost of test execution is high.
on the other hand there are practical needs for quickly generating a test suite of reasonable size while the size is not the primary concern.
for instance if test execution is automated one might better start executing the test cases instead of waiting for a sophisticated algorithm to nd a smaller test suite.
also in the phase of test modeling one might want to check how test cases look like for an un nished test model not expecting a highly optimized test suite that would be bene cial if it were to be executed.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
ase september singapore singapore acm.
... .
in the ipog based approach yu et al.
proposed an e cient constraint handling mechanism which made the approach practical in the presence of complex constraints.
in more recent work yu et al.
signi cantly improved the e ciency of the approach by developing a dedicated analysis of minimal forbidden tuples .
in the otat approach on the other hand such a significant progress towards e cient constraint handling has not yet been made .
there is both theoretical and practical interest in this approach an ideal greedy otat algorithm ignoring constraints is shown to deliver a test suite whose size is logarithmic to the number of parameters in the input sut model .
a similar result is known for the more feasible density algorithm of this category.
in addition the nature of generating one test at a time can be bene cial since one can start test execution before the entire test suite has been generated.
in this paper we introduce an e cient constraint handling technique for the otat algorithms.
the rst challenge for e cient test suite generation is how to e ciently detect forbidden tuples.
to this end we exploit the information of unsatis able cores a notion widely understood in the sat community .
in essence we point out that every forbidden tuple corresponds to an unsatis able core more precisely the failed assumptions in it.
using failed assumptions which ipasir1 compliant sat solvers can provide we propose a technique to lazily detect forbidden tuples during greedy test case construction.
the second challenge is that due to the nature of otat algorithms still a larger number of sat solving calls is needed.
we show that most of these sat solving calls are not required to guarantee the termination of the algorithm but are needed to ensure that the given constraints are satis ed by the generated test cases.
we introduce a new technique to amend a test case turn a test case that possibly violates the constraints into one that satis es the constraints again using failed assumptions.
then we show that we can omit most of the sat solving calls without affecting the correctness of the overall algorithm.
finally this omission of sat solving makes the greedy test case generation heuristic to be approximative i. e. it can make a wrong choice that will later be amended.
hence we propose reducing the chance of making such wrong choices by exploiting the internal reasoning of sat solvers.
we added a new api function in the sat solver lingeling that instantly checks satis ability but allows for the third answer unknown .
we experimentally show that this technique pays o in terms of the sizes of generated test suites with a mild computational overhead.
in principle the proposed constraint handling method is applicable to any algorithms that comply the otat framework of bryce et al.
.
we implement the method in our base otat algorithm that is inspired by pict and aetg and experimentally show that the proposed constraint handling method delivers a signi cant improvement to the e ciency of the algorithm.
this paper is organized as follows section de nes combinatorial testing formally and section describes the usage of embedded sat solvers.
section shows the greedy base variant of our algorithm where we observe that forbidden tuples correspond to unsatis able cores as described 1ipasir is the new incremental sat solver interface used in the sat race competition .in section .
section uses unsatis able cores to amend test cases and section uses an extension of our sat solver lingeling to optimize this algorithm.
section gives on overview of related work.
the results of our experiments are shown in section and section concludes.
.
combinatorial testing we de ne several notions for combinatorial testing.
first we de ne a model of a system under test sut .
de nition .
ansut model is a triplehp v iof a nite set pofparameters a familyv fvpgp2pthat assigns each p2pa nite setvpofvalues and a boolean formula called the sut constraint whose atoms are pairshp viofp2pandv2vp.
hereafter instead of hp viwe writep vor evenvif no confusion arises.
a valid test case is a choice of values for parameters that satisfy the sut constraint.
de nition test cases .given suthp v i atest case is a mapping p!svthat satis es the domain constraint p 2vpfor everyp2p.
a test case is called valid if it satis es more precisely the following assignmentb satis es .
b p v true if p v false otherwise we call a set of valid test cases a test suite .
example .
consider the web application mentioned in the introduction.
the sut model hp v ifor this example consists of the following parameters and values p fcpu os browserg vcpu fintel amdg vos fwindows linux macg vbrowser fie firefox safarig following convention the size of this model is denoted as meaning that there is one parameter with two values and two with three values.
the constraint in the introduction are expressed by the following sut constraint ie windows safari mac mac amd the following table shows a test suite for the sut model consisting of seven valid test cases.
no.
cpu os browser 1amd windows ie 2intel windows firefox 3intel linux firefox 4intel windows ie 5intel mac safari 6amd linux firefox 7intel mac firefox the observation supporting combinatorial testing is that faults are caused by the interaction of values of a few parameters.
such interactions are formalized as follows.
615de nition tuples .given suthp v i aparameter tuple is a subset pof parameters and a value tuple over is a mapping !svthat satis es the domain constraint.
here is denoted by dom .
we identify a tuple with the following set fp vj p vis de nedg a test case is also a tuple s. t. dom p. de nition covering tests .we say that a test case covers a tuple i i. e. value choices in meet .
a tuple is possible i a valid test case covers it otherwise it is forbidden .
given a set tof tuples we say that a test suite ist covering i every 2tis either forbidden or covered by some .
the covering test problem is to nd at covering test suite.
the terms t way ort wise testing and covering arrays cf.
refer to a subclass of the covering test problems where tis the set of all value tuples of size t. the number tis called the strength of combinatorial testing.
example .
the sut model of example has tuples of size two where six out of them are forbidden.
the test suite in example covers all the possible tuples and thus is a way covering test suite for the sut model.
.
sat solving satis ability sat solvers are tools that given a boolean formula in conjunctive normal form cnf decide whether it is possible to instantiate the variables in the formula such that the formula evaluates to true.
more formally consider a boolean formula over a set xof variables.
an assignment is a mapping x !
ftrue falseg.
it satis es a formula i evaluates to true after replacing every variable xin by x .
a formula is satis able if it can be satis ed by some assignment and is unsatis able otherwise.
when sat solvers conclude unsatis ability they are typically able to output a minimal unsatis able core which is de ned as follows.
here we consider a cnf also as a set of clauses.
de nition .
anunsatis able core of a cnf is a subset of which is unsatis able.
an unsatis able core is minimal if any proper subset of is satis able.
.
cdcl the dpll algorithm with con ict driven clause learning cdcl is a de facto standard architecture of sat solvers.
the cdcl approach constitutes a backtrack based search algorithm which e ciently scans the search space of possible assignments for the variables of a given formula.
its basic procedure is to repeat choosing a variable and assigning a truth value for it decision and simplifying the formula based on decisions using unit propagation .
during this procedure it may detect a con ict of some combinations of decisions and propagated assignments.
then a cause of the con ict a set of decisions that derives it is analyzed and a clause is learned to avoid the same con ict later on.
after backtracking the learned clause forces the assignment of one of its variables to be ipped which might trigger further propagation.
during this procedure the algorithmalso checks whether all variables have been assigned and no more propagations are pending.
in this case it terminates indicating that the formula is satis able.
.
incremental sat solving and failed assumptions incremental sat solving facilitates checking satis ability for a series of closely related formulas.
it is particularly important in the context of bounded model checking .
state of the art sat solvers like lingeling implement the assumption based incremental algorithm as pioneered by the highly in uential sat solver minisat .
incremental sat solvers remember the current state and do not just exit after checking the satis ability of one input formula.
besides asserting clauses that will be valid in the later satis ability checks incremental sat solvers accept assumption literals which are used as forced decision and are only valid during the next incremental satis ability check thus abandoned in later checks.
when an incremental sat solver derives unsatis ability it is particularly important to know which assumption literals are a cause of unsatis ability in other words constitute an unsatis able core.
such literals are called failed assumptions .
the interface of an incremental sat solver is expressed in an object oriented notation as follows which is also compatible with the ipasir interface.
class solverf literal newvar void assert clausec void assume literall bool check assignment model refers to a solution if exists listhliteralifailed assumptions g .
base greedy algorithm before introducing constraint handling we introduce our base otat algorithm without constraint handling which is shown as algorithm .
the algorithm works as follows to generate one test case the rst step picks up a parameter tuple that has most uncovered tuples and xes those parameters to cover one of the uncovered tuples lines .
the second step greedily chooses a parameter and a value so that the number of newly covered tuples is maximized lines .
it may happen that xing any single parameter value will not increase the number of covered tuples but xing more than two will.
in such a case we apply the rst step again to x multiple parameters lines .
the rst step mimics that of pict while the second step is similar to aetg .
the main di erence from aetg is that we search all un xed parameters for a value that maximizes the number of newly covered tuples while aetg randomly chooses a parameter to be xed and searches only for the best value for the parameter.
here we do some clever computation in order to e ciently search all parameters.
due to this di erence our algorithm is deterministic for tie breaking we use the deterministic random number generator of the standard c library and produces a fairly small test suite without requiring multiple test case generation runs.
616algorithm basic greedy test case generation input an sut modelhp v iand a settof tuples output at covering test suite 1whilet6 do choose 2ts.
t.dom contains most uncovered tuples as in pict cover at least this tuple while there are an un xed parameter pand a value vs. t. fp vgcovers some uncovered tuples do choose such pandvthat maximize the number of covered tuples fp vg ifthere is an uncovered tuple that may be covered by xing more than two parameters in then choose such as in pict go to line fix un xed parameters in to arbitrary values f g add the new test case remove from tthe tuples covered by .
naive constraint handling now we present a variant of algorithm with naive constraint handling using incremental sat solving as already proposed by cohen et al.
.
consider an sut model hp v i. to represent a test case as a sat formula we introduce a boolean variable p vfor eachp2pandv2vp denoting p v. since p must be uniquely de ned we impose the following constraint unique p2p x v2vpp v in the following algorithms tool is assumed to be a sat solver instance on which unique is asserted.
algorithm shows our rst algorithm called naive which however utilizes incremental sat solving.
it works as follows before generating test cases it rst removes all forbidden tuples in the set tof target tuples that has to be covered lines .
then whenever it chooses a tuple or value it checks if the choice does not violate the constraint with already xed values in lines and .
the use of incremental sat solving reduces the total cost of sat solving .
nevertheless as we will see in the experimental section algorithm is not e cient for large scale test models due to the large number of required sat solving calls.
hence in the next section we try to improve e ciency by reducing the number of sat solving calls.
.
forbidden tuples as cores the most time consuming part of algorithm consists of lines where all forbidden tuples are removed a priori .
note that for t way testing of an sut model of size gk the number of tuples in t that is the number of sat solving calls needed in this phase sums up to o gtkt .
hence as the rst improvement to this naive algorithm we propose to remove forbidden tuples lazily .
the key observation is that a forbidden tuple corresponds to the set of failed assumptions in an unsatis able core.
2in order to encode the above formula into a cnf we use theladder encoding for at most one constraints.algorithm naive treatment of constraints 1function possible foreachv2 dotool assume v return tool check 4foreach 2tdo remove forbidden tuples if possible thent tnf g 6whilet6 do main loop choose 2tas in pict cover at least this tuple while there existpandv2vps.
t. fp vgcovers new tuples and possible fp vg do choose such best pandv fp vg ifthere is 2ts.
t.possible then choose such as in pict go to line at this point possible true is ensured.
fix un xed parameters in according to tool model f g add the new test case remove from tthe tuples covered by example .
consider the sut model of example and suppose that in a test case generation procedure browser has been xed to safari andoshas been xed to mac.
note that no con ict will arise at this point.
next consider xing cpu toamd .
this choice raises unsatis ability in the possible call in line of algorithm .
the corresponding minimum unsatis able core is either of the following mac amd mac amd safari amd safari mac mac amd the set of failed assumptions in isfmac amdg this indicates that one cannot x ostomac andcpu toamd in a test case i. e. fmac amdgis a forbidden tuple.
similarly core indicates thatfsafari amdgis a forbidden tuple.
in either case we detect a forbidden tuple.
now we introduce algorithm called lazy which omits to remove the forbidden tuples a priori but lazily removes them when sat checks show unsatis ability.
algorithm lazy removal of forbidden tuples 1function possible foreachv2 dotool assume v iftool check then return true else 5t f 02tjtool failed assumptions 0g return false 7while there is 2ts.
t.possible do choose 2tas in pict do lines of algorithm where possible is replaced by possible function possible checks if the tuple or equivalently partial test case is possible line .
if it is not the case then the sat solver provides a set of failed assumptions such asfmac amdgin example .
we now know that tuples containing the failed assumptions are forbidden hence we remove such tuples from t line .
617the correctness of algorithm i. e. that it terminates and generates a t covering test suite is easily proven.
proposition .
algorithm is correct.
proof.
in every iteration of the main loop the rst chosen in line is either removed or covered by the newly added test case.
hence the algorithm terminates as jtjstrictly decreases in each iteration.
clearly all tuples in tare either forbidden or covered by the output test suite.
in many examples the lazy removal of forbidden tuples signi cantly improves the runtime of the algorithm.
in certain cases however this approach still su ers from an excessive large number of sat solving calls see also the experimental section .
this phenomenon is caused by the second occurrence of pict like value choices line in algorithm if the constraint is so strict that the current values in are not compatible with any other remaining tuples in t then at line we have to perform sat checks for all the remaining tuples in t. hence in the next section we consider to even omit these sat checks.
.
cores for amending test cases the termination argument of proposition shows that only the rst sat check in every iteration is crucial to achieve the termination of the algorithm.
the remaining sat checks are only necessary to ensure that intermediate value choices in never contradict the sut constraint .
however need not always respect it su ces if the nal test cases added to the output in line satisfy .
hence in all iterations we can omit the sat checks except for the rst one.
unfortunately then the resulting test case is not guaranteed to be valid in line anymore.
to solve this problem we propose a technique to amend such an invalid test case and turn it into a valid one again using the information of failed assumptions.
in general if an incremental sat solver detects unsatis ability then one of the failed assumptions must be removed in order to satisfy the formula.
in our application this means that some of the value assignments must be abandoned.
by repeatedly removing failed assumptions until the formula becomes satis able we can derive a test case that satis es the sut constraint.
example .
consider again the sut model of example and the following invalid test case fsafari mac amdg possible will return false with a set of failed assumptions e. g. fmac amdg.
this indicates that at least either mac oramd must be removed from the test case.
thus consider removing e. g. amd fsafari macg possible returns true with a satisfying assignment fsafari mac intelg which is a valid test case.
it is important to properly choose which value assignment to remove.
note that even the termination of the algorithm cannot be ensured if one removes the rst assumptions that are made to cover the rst tuple in line .for this choice we propose to remove the failed assumption that corresponds to the most recently chosen value assignment.
the underlying observation for this choice is that later value assignments are decided depending on earlier choices the earlier the value assignment is chosen the more in uential it is to the coverage of the test case.
this algorithm which we call amend is shown in algorithm .
algorithm amending test cases 1function amend make a valid test case while possible do identifyp v2tool failed assumptions that is the most recently xed nfp vg fix un xed parameters in according to tool model return 7while there is 2ts.
t.possible do choose 2tas in pict do lines of algorithm ignoring constraints amend f g add the new test case remove from tthe tuples covered by proposition .
algorithm is correct.
proof.
the crucial point is that the assumptions made in line to cover the rst tuple will not be removed.
since the satis ability of is ensured any unsatis able core that is found later must contain a failed assumption that is added by later greedy choice.
in the worst case all the choices but may be removed but still tstrictly decreases by each iteration.
as we see in the above proof algorithm is correct however it may happen that many value choices in a test case which are chosen to cover as many tuples as possible are eventually abandoned.
this may in some particular cases result in preferable randomness that eventually yields a smaller test suite but in general disturbs the greedy heuristic and results in a larger test suite especially when the considered sut constraint is so strict that most value choices violate the constraint.
note also that the notion of strictness does not directly correspond to the size or complexity of the constraint.
.
a voiding wrong choices finally we reduce the chance of making wrong choices by extending the sat solver lingeling with the following method bool imply literall method imply l addslas an assumption literal just like assume l but it additionally performs the unit propagation.
if the unit propagation causes a con ict then the method returns false and the sat solver goes into the state where it derives unsatis ability.
otherwise lis added as an assumption literal.
using this method we implement a function maybe which is an approximate variant of possible .maybe tests if is possible by dispatching the imply method on the backend sat solver but will not perform the actual satis ability check.
that is this function may fail to detect a con ict even if is actually impossible.
however if it detects a con ict then is indeed impossible and the lazy removal of forbidden tuples section is performed.
the overall algorithm imply which leverages the new imply method is presented in algorithm .
algorithm avoiding wrong choices 1function maybe foreachv2 do if tool imply v then t f jtool failed assumptions g return false return true 7while there is 2ts.
t.possible do choose 2tas in pict do lines of algorithm where possible is replaced by maybe amend f g add the new test case remove from tthe tuples covered by since maybe is only approximate it is not ensured that the test case is valid after all values are xed.
thus the application of the amend function from the previous section is a crucial step line .
proposition .
algorithm is correct.
proof.
the reasoning is the same as proposition .
the estimation quality of maybe depends on how many clauses have been learned by the sat solver.
at the beginning of test suite generation it may often fail to detect con icts but at this stage disturbance by amend should be small since most tuples are yet to be covered and any test case will cover some of them.
as more test cases are generated maybe becomes more precise.
.
related work here we recall previous work towards e cient constraint handling in combinatorial testing and remark a few other uses of sat solving for test case generation.
.
pict pict is a well known combinatorial testing tool based on the otat approach.
for constraint handling it precomputes allforbidden tuples regardless of t and uses this information when greedily constructing a test case .
this approach is quite fast if the sut constraint is weak i. e. only few tuples are forbidden.
however it becomes intractable if the constraint is so strict that a great number of tuples are forbidden.
note again that the strictness of constraint does not correspond to the complexity or the size of the constraint.
on the other hand the e ciency of our approach is stable from the strictness of the sut constraint.
.
aetg cohen et al.
pioneered the use of incremental sat solving for constraint handling in their otat algorithm aetg .
all their algorithms however remove forbidden tuples before the actual test case generation phase as in our naive algorithm.
hence their aetg variants would also bene t from our ideas.
we also did not follow their may and must analysis which was introduced to prevent the back end sat solver from encountering unsatis ability.
in our usage deriving unsatis ability is the key to detecting forbidden tuples and we have no reason to prevent it.
.
acts acts is another successful combinatorial testing tool which is based on the ipog algorithm .
yu et al.
improved the constraint handling of the ipog algorithm using the notion of minimum forbidden tuples mfts i. e. the forbidden tuples whose proper subsets are not forbidden.
their rst algorithm precomputes all mfts and uses this information during the test case generation.
moreover to relax the cost of computing all mfts which is signi cant if the constraint is complex they further introduced an algorithm using necessary forbidden tuples nfts that computes forbidden tuples when it becomes necessary.
their work largely inspired us although we did not choose ipog as our base greedy algorithm.
the notion of mfts is somewhat related to minimal unsatis able cores i. e. unsatis able cores whose proper subsets are satis able.
the idea of the nft algorithm is also visible in our lazy algorithm.
however while minimal unsatis able cores are naturally obtained by cdcl sat solving computing mfts is a hard optimization problem since one has to further minimize unsatis able cores in terms of the failed assumptions.
.
icpl in the context of software product lines spls johansen et al.
introduced the icpl algorithm for t way test suite generation for spls.
icpl also incorporates sat solvers and take a di erent approach to reduce the cost of precomputing forbidden tuples.
their algorithm generates t way test suite by generating t0 way test suites for t0 t .
using the information of t0 way forbidden tuples they proposed an algorithm to e ciently compute t0 way forbidden tuples.
compared to their work our approach does not require a particular phase for computing forbidden tuples since they are provided for free by incremental sat solvers as failed assumptions.
we leave it for future work to experimentally compare our tool with icpl these tools assume di erent input formats.
.
constraints in other approaches of course there are e orts towards constraint handling in non greedy algorithms.
the sat based approach encodes entire test suite generation as a sat formula and hence constraints can be naturally encoded .
also the use of sat solving has been proposed for the simulated annealingbased tool casa .
lin et al.
recently introduced the two mode meta heuristic approach in combinatorial testing and their tool tca produces notably small test suites.
the primary concern of these approaches are not on execution time but on the size of test suites.
indeed casa tca and the sat based test suite optimization function in 619x1x10x100 naive lazy amend imply2 way x1x10x100x1000 naive lazy amend imply3 wayfigure comparison of the runtime of algorithms .
the ratio of the runtime over the best among all algorithms for each benchmark are plotted.
x1x1.05x1.1x1.15x1.2x1.
naive lazy amend imply2 way x1x1.05x1.1x1.15x1.2x1.
naive lazy amend imply3 way figure comparison of algorithms in terms of the sizes of the generated test suites.
calot do not output a test suite even when they internally have one they try to optimize it as far as possible until certain termination conditions are met.
nevertheless we observe some cases where our work may provide a bene t to these approaches e. g. calot previously employed acts for constructing an initial test suite for optimization which is now done e ciently inside calot .tca also employs a greedy algorithm for the initial test suite and one of its two modes is a greedy mode where we expect our technique can improve its e ciency.
farchi et al.
proposed using unsatis able cores in the test modeling phase of combinatorial testing.
their test modeling tool focus tests if tuples are possible or forbidden as expected.
if a tuple is unexpectedly forbidden then the tool analyzes the unsatis able core and indicates which clauses of the sut constraint forbid the tuple.
thus users can e ectively model an intended the sut constraint.
our use of unsatis able cores makes a good contrast farchi et al.
consider assumed value choices are correct and xes the clauses in an unsatis able core while we consider the clauses are correct and x the value choices.
.
sat solvers in model based testing the model based testing mbt cf.
considers more elaborated sut models compared to combinatorial testing namely states of suts are considered.
mbt tools aim at generating sequences of test cases which ensure a certain path coverage criterion.
the use of incremental sat solving in mbt is also proposed .
we expect that it is also interesting to use unsatis able cores to improve such sat based mbt tools e. g. it might be able to e ciently detect forbidden paths .
we leave it for future work to explore to this direction.
.
experiments we implemented algorithms in our tool calot and conducted experiments to investigate the following research questions.
rq1 how e cient is the lazy algorithm compared to the naive one?
how does it a ect the sizes of test suites?
rq2 how e cient is the amend algorithm compared to lazy ?
how does it a ect the sizes of test suites?
rq3 how much does the imply algorithm improve the sizes of test suites compared to amend ?
how does it a ect the e ciency of the algorithm?
rq4 how does the imply algorithm compare with other greedy test case generation tools?
as the benchmark set we collected the following the benchmarks from cohen et al.
the industry applications from segall et al.
the two industry applications from yu et al.
the industry applications from kitamura et al.
and two applications from our industry collaborators.
the experiments were run on a laptop with a .59ghz intel core i5 4310u processor and 4gb of ram running windows .
figure compares our four algorithms in terms of runtime for generating both way and way test suites and figure compares the sizes of the generated test suites.
3one of our examples is available at .aist.go.jp t.kitamura dl .
620x1x10x100x1000 wins timeoutspict 3acts 1imply way x1x10x100x1000 wins timeoutspict 4acts 1imply wayfigure comparing relative execution times with other tools.
x1x1.1x1.2x1.3x1.4x1.
wins timeoutspict 3acts 1imply way x1x1.1x1.2x1.3x1.4x1.
wins timeoutspict 4acts 1imply way figure comparing test suite sizes with other tools.
the box plots show the distribution of the data the box encompasses the rst and third quartile of the distribution with the middle line denoting the median value of the data.
the whiskers are drawn at .
times the interquartile range to the data point closest to .
times the distance between the median and the lower upper quartile.
points outside that range are considered outliers and shown as a dot.
to measure the validity of our claims we also report thep values in the wilcoxon signed rank test .
rq1 naive vs. lazy from figure we observe that lazy signi cantly improves the average execution time over naive .
the signi cance of the di erence is p for both and way cases.
as explained in section however there are a few examples in way case where the runtime does not improve.
in terms of the sizes of test suites from figure we observe only minor di erence in way case where lazy can be slightly worse than naive with p .
rq2 lazy vs. amend here we measure the improvement due to amending invalid choices see section .
in figure we observe further improvement in the e ciency of the amend algorithm over lazy .
the signi cance is p for both and way cases.
on the other hand the sizes of generated test suites get noticeably worse figure with p .rq3 amend vs. imply now we measure the improvement due to the new imply method see section .
our nal algorithm imply improves the sizes of test suites over the previous amend algorithm with signi cance p and apparently recovers to a similar result to lazy .
on the other hand the overhead in runtime over amend is noticeable p .
rq4 comparison with other greedy tools finally we compare our algorithms with the otat based greedy tool pict version .
and the ipog based greedy toolacts version .
.
the results are shown in figures and also summarised in table .
the scatter plots in figure compare individual data points between two settings.
a data point above below the diagonal implies a higher lower value for the baseline algorithm the naive algorithm.
for a few benchmarks pict and acts do not respond within a reasonable time so we set seconds as timeout.
in case of timeout we assumed seconds as the runtime.
we do not know the size of the output test suite in these cases hence figure excludes the case of timeouts in the box plots.
instead the number of timeouts are reported.
the gures also report the number of wins i. e. how often the tool achieved the best result among others.
when counting wins ties are counted as a win for all tied tools.
when comparing the size of test suites from figure we clearly observe that our algorithm outperforms others.
the 621table detailed comparison with other tools.
for each categories the average avr.
geometric mean g.m.
and the number of wins are reported.
fields with cannot be computed due to timeouts.
way way imply pict acts imply pict acts source size time size time size time size time size time size time cohen et al.
35avr.
.
.
.
.
.
.
.
.
.
.
.
.
g.m.
.
.
.
.
.
.
.
.
.
.
.
.
wins segall et al.
20avr.
.
.
.
.
.
.
.
.
.
.
.
.
g.m.
.
.
.
.
.
.
.
.
.
.
.
.
wins yu et al.
2avr.
.
.
.
.
.
.
.
.
.
.
g.m.
.
.
.
.
.
.
.
.
.
.
wins kitamura et al.
18avr.
.
.
.
.
.
.
.
.
.
.
.
g.m.
.
.
.
.
.
.
.
.
.
.
.
wins company a .
.
.
.
.
.
.
.
.
.
.
company b .
.
.
.
.
.
.
.
.1s1s10s100s1000s .1s 1s 10s 100s2 way naive pict acts imply .1s1s10s100s1000s .1s 1s 10s 100s 1000s 10000s3 way naive pict acts imply figure comparing runtime with other tools.
the vertical axis presents the runtime of each algorithm while the horizontal axis presents the runtime the naive algorithm took for the same benchmark.
signi cance is p for both and way and for both pict andacts .
when comparing runtime from figure one might think that our imply is faster than the other tools.
however looking more detail in figure and table we see that the other tools perform quite well for many benchmarks.
in particular pict is fast for some way examples and acts is remarkably fast for many way examples from cohen et al.
.
we conjecture that the e ciency of these tools depends on the weakness of the constraints.
.
conclusion in this paper we have developed constraint handling techniques for one test at a time otat combinatorial test case generation algorithms using the information of unsatis able cores.
we implemented the proposed constraint handling methods in the otat algorithm of our test case generation tool calot .
through experiments we veri ed that our techniques signi cantly improve the e ciency of test case gen eration without sacri cing the size of generated test suites compared to constraint handling utilizing incremental sat solvers naively.
we also compared our tool with the greedy test case generation tools pict and acts and observed that our tool perform well in terms of both e ciency and the size of generated test suite which are usually a trade o with each other.
limitation and future work.
although calot performed well in our experiments if one ignores constraints the ipog algorithm of acts is remarkably faster than our base algorithm for large sut models and higher strength cases.
this is explained by the fact that our base algorithm always has to compute the best parameters and values to x although this e ort often results in a smaller test suite.
hence we could only observe in some industry examples that the minimum forbidden tuple computation of yu et al.
may become a signi cant overhead.
we leave it for future work to implement our constraint handling approach in the ipog algorithm to fairly compare our 622constraint handling and the minimum forbidden tuple approach.
in principle our constraint handling method can be immediately generalized to the otat framework by bryce et al.
.
we leave it for future work to develop such a framework where one can plug in their own otat heuristics such as the aetg orpict heuristics without being concerned about constraint handling.
.