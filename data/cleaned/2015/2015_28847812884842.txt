cross supervised synthesis of web crawlers adi omari technion omari cs.technion.ac.ilsharon shoham academic college of tel aviv ya f fo sharon.shoham gmail.comeran y ahav technion yahave cs.technion.ac.il abstract a web crawler is a program that automatically and systematically tracks the links of a website and extracts information from its pages.
due to the different formats of websites the crawling scheme fordifferent sites can differ dramatically.
manually customizing a crawler for each specific site is time consuming and error prone.
furthermore because sites periodically change their format and presenta tion crawling schemes have to be manually updated and adjusted.
in this paper we present a technique for automatic synthesis of web crawlers from examples.
the main idea is to use hand crafted possibly partial crawlers for some websites as the basis for crawl ing other sites that contain the same kind of information.
technically we use the data on one site to identify data on another site.
we then use the identified data to learn the website structure andsynthesize an appropriate extraction scheme.
we iterate this process as synthesized extraction schemes result in additional data to be used for re learning the website structure.
we implemented ourapproach and automatically synthesized 30crawlers for websites from nine different categories books tvs conferences universi ties cameras phones movies songs and hotels.
categories and subject descriptors d. .
automatic programming i. .
program synthesis introduction a web crawler is a program that automatically and systematically tracks the links of a website and extracts information from its pages.
one of the challenges of modern crawlers is to extract complex structured information from different websites where the information on each site may be represented and rendered in a differentmanner and where each data item may have multiple attributes.
for example price comparison sites use custom crawlers for gathering information about products and their prices across theweb.
these crawlers have to extract the structured information de scribing products and their prices from sites with different formats and representations.
the differences between sites often force a programmer to create a customized crawler for each site a taskthat is time consuming and error prone.
furthermore websites permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others thanacm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c 2016 acm.
isbn .
.
.
.
eventually change their format and presentation therefore the crawling schemes have to be manually maintained and adjusted.
goal the goal of this work is to automatically synthesize webcrawlers for a family of websites that contain the same kind of information but may significantly differ on layout and formatting.
we assume that the programmer provides one or more hand craftedweb crawlers for some of the sites in the family and would like to automatically generate crawlers for other sites in the family.
for example given a family of four websites of online book stores eachcontaining tens of thousands of books and a hand crafted crawlerfor one of them we automatically generate crawlers for the other three.
note that our goal is not only to extract data from web sites but to synthesize the programs that extract the data.
existing techniques our work is related to wrapper induction .
the goal of wrapper induction is to automatically generate extraction rules for a website based on the regularity of pages inside the site.
our main idea is to try and leverage a similar regularity across multiple sites.
however because different sites may signifi cantly differ on their layout we have to capture this regularity at a more abstract level.
towards that end we define an abstract logical representation of a website that allows us to identify commonality even in the face of different formatting details.
in contrast to supervised techniques which require labeled examples and unsupervised techniques that frequently require manual annotation of the extracted data our approach uses cross supervision where the learned extraction rules of one site are used to produce labeled examples for learning the extraction rules in another site.
our technique uses xpath a widely used web documents query language along with regular expressions.
this makes ourresulting extraction schemes human readable and easy to modify when needed.
there has been some work on the problem of xpath robustness to site changes trying to pick the most robust xpath query for extracting a particular piece of information.
while robustness is a desirable property our ability to efficiently synthesize a crawler circumvents this challenge as a crawler can beregenerated whenever a site changes.
our approach cross supervised learning of crawling schemes we present a technique for automatically synthesizing data extractingcrawlers.
our technique is based on two observations i sites with similar content have data overlaps and ii in a given site information with similar semantics is usually located in nodes with asimilar location in the document tree.
using these observations we synthesize data extracting crawlers for a group of sites sharing the same type of information.
starting from one or more hand crafted crawlers which provide a relatively small initial set of crawled data we use an iterative approach todiscover data instances in new websites and extrapolate data ex2016 ieee acm 38th ieee international conference on software engineering traction schemes which are in turn used to extract new data.
we refer to this process as cross supervised learning as data from one web site is repeatedly used to guide synthesis in other sites.
our crawlers extract data describing different attributes of items.
we introduce the notion of a container to maintain relationships between different attributes that refer to the same item.
we use containers which are automatically selected without any prior knowledge of the structure of the website to handle pages with multipleitems and to filter out irrelevant data.
this allows us to synthesizeextraction schemes from positive examples only.
our approach is scalable and practical we used cross supervision to synthesize crawlers for several product review websites e.g.
tvexp.com weppir.com camexp.com and phonesum.com.
main contributions the contributions of this paper are a framework for automatic synthesis of web crawlers.
the main idea is to use hand crafted crawlers for a number of websites as the basis for crawling other sites that contain thesame kind of information.
a new cross supervised crawler synthesis algorithm that ex trapolates crawling schemes from one web site to another.the algorithm handles pages with multiple items and synthesizes crawlers using only positive examples.
an implementation and evaluation of our approach automatically synthesizing 30crawlers for websites from nine different categories books tvs conferences universities cam eras phones movies songs and hotels.
the crawlers thatwe synthesize are real crawlers that were used to crawl morethan 000webpages over all categories.
2o v e r v i e w .
motivating example consider a price comparison service for books which crawls book seller websites and provides a list of sellers and correspondingprices for each book.
examples of such book seller sites include barnesandnoble.com b n blackwell.co.uk blackwell a n d abebooks.com abe .
each of these sites lists a wide collection of books typically presented in template generated webpages feeding from a database.
since these pages are template generated they present structured information for each book in a format that is repeated across books.
by recognizing this repetitive structure for a given site one can synthesize a data extraction query and use it to automatically extract the entire book collection.
while the format within a single site is typically stable the formats between sites differ considerably.
fig.
shows a small and simplified fragment of the page structure on b n and blackwell in html.
fig.
shows part of the tree representation of the corresponding sites as well as of abe whered1 ... d 4denote different pages.
due to the differences in structure the data ex traction query can differ dramatically.
for example in blackwell and abe each of the pages d d3 d4 presents a single book whereas in b n the paged1shows a list of several books.
the goal of this work is to automatically synthesize crawlers for new sites based on some existing hand crafted crawler s .
for example given a crawler for the blackwell site our technique synthesizes a crawler for b n website.
the synthesized crawler is depicted in fig.
.
we show that this can be done despite the signif icant differences between the sites blackwell a n d b n i nt e r m s of html structure.
we note that the examples that we present in this section are abbreviated and simplified.
for example the real dom tree for the b n page we show here contains around nodes.
the structure of the full trees and the xpaths required for processing them are more involved than what is shown here.barnesandnoble.com ol class result set box li class result box .. div class details below axis a href ... data bntrack title 9781628718980 class title through the looking glass a a href .. data bntrack contributor 9781628718980 class contributor david winston busch a ... div class price format a href ... data bntrack paperback format span class format paperback span span class price .
span a div div ... li li class result box .. div class details below axis a href ... data bntrack title 9780071633604 class title alice s adventures in wonderland a a href .. data bntrack contributor 9780071633604 class contributor lewis carroll a ... div class price format a href ... data bntrack paperback format span class format paperback span span class price .
span a div div ... li ol blackwell.com div id product biblio h1 through the looking glass h1 a class link type1 href jsp a lewis carroll david winston busch a div class price info align center span class price .
span div div figure fragments of webpages with the similar attribute values for a book on two different book shopping sites.
.
cross supervised learning of crawling schemes our main observation is that despite the significant differences in the concrete layout of websites the pages of websites that exhibitthe same product category often share the same logical structure they present similar attributes for each product.
for example eachof the pages of fig.
presents the same important attributes aboutthe book including its title author name andprice .
moreover there is a large number of shared products between these websites.
the book through the looking glass is one such example for b n and blackwell .
our technique exploits data overlaps across sites in order to learn the concrete structure of a new website sbased on other websites.
specifically we identify in sconcrete data extracted from other sites and as such learn the structure in which this data is representedins.
we then use multiple examples of the structure in which the data appears in sin order to generalize and get an extraction query fors.
this enables our algorithm to extrapolate a crawler for s. we do not require a precise match of data across sites as our technique also handles noisy data.
for example prices do not have 3691class myspider crawlspider name barnesandnoble allowed domains start urls ?store allproducts keyword java programming rules rule linkextractor allow s .
callback parse item follow true def parse item self response sel selector response rows sel.xpath body div div section div ol result set box li div div for r in rows item booksitem item r.xpath a .extract item r.xpath a .extract item r.xpath div a span .extract yield item figure crawler for java books from barnes noble.
to be identical any number can be a match.
crawling schemes a crawler such as the one of fig.
contains some boilerplate code defining the crawler class and its operations.
however the essence of the crawler is its crawling scheme.f o r example in fig.
the crawling scheme is highlighted in boldface.
a crawling scheme is defined with respect to a set of semantic groups called attributes which define the types of data to be extracted.
in the books example the attributes are book title author andprice.
given a set of attributes a crawling scheme consists of the following two components i a data extraction query that defineshow to obtain values of the attributes for each item listed on thesite.
ii a starting point url and a url filtering pattern which let the crawler locate relevant pages and filter out irrelevant pages without downloading and processing them.
our crawlers use xpath as a query language for data extraction.
xpath is a query language for selecting nodes from an xmldocument which is based on the tree representation of the xmldocument and provides the ability to navigate around the tree se lecting nodes by describing their path from the document tree root node.
for example fig.
and fig.
show the crawling schemes for crawling books from blackwell and b n respectively where the data extraction query is expressed using xpaths.
two level data extraction schemes we assume that the data extraction query has two levels the first level query is an xpath describing an item container.
intuitively a container is a sub tree that contains all the attribute values we would like to extract de fined more formally in sec.
.
for example in fig.
the xpath body div ... describes a container of book attributes on blackwell pages.
the second level queries contain an extraction xpath for values of each individual attribute.
these xpaths are relative to the rootof the container.
for example div h1 i nf i g .4i su s e dt op i c k the node that has type h1 heading containing the book title.iterative synthesis of crawling schemes our approach considers a set of websites and a set of attributes to extract.
to bootstrap thesynthesis process the user is required to provide the set of websitesfor which crawler synthesis is desired as well as a crawling scheme for at least one of these sites.
alternatively the user can provide multiple partial crawling schemes for different sites that togethercover all the different item attributes.
the synthesis process starts by invoking the provided extraction scheme s on the corresponding sites to obtain an initial set of val ues for each one of the attributes.
these values are then used to locate nodes that contain attribute values in the document trees of webpages of new sites.
the nodes that contain attribute values re veal the structure of pages of the corresponding websites.
in par ticular smallest subtrees that exhibit all the attributes amount to containers.
this allows for synthesis of data extraction schemes for new websites.
the newly learned extraction schemes are usedto extract more values and add them to the set of values of each attribute possibly allowing for additional websites to be handled.
this process is repeated until complete extraction schemes are ob tained for all websites or until no additional values are extracted.
in our example the algorithm starts with the data extraction scheme for blackwell see fig.
provided by a user.
it extracts from d2author x title x a n d price as values of the book title author and price attributes respectively see fig.
.
these values are identified in d1 b n within the subtree of the left most node represented by body ... ol li ... div which then points to the latter node as a possible container.
additional values taken from d3and other pages in blackwell identify additional nodes in the b n tree as attribute and container nodes.
note that author x is also found in another subtree in d1.h o w ever there are no instances of the remaining attributes in that subtree therefore the subtree is not considered a container and thecorresponding node is treated as noise.
by identifying the commonality between the identified containers and between nodes of the same attribute a data extraction schemefor b n is synthesized see below .
in the next iteration the new data scheme is used to extract from b n the values author z title z andprice as additional values for book title author and price respectively that did not exist in blackwell .
the new values are located in abe seed4in fig.
allowing to learn an extraction scheme for abeas well.
xpath synthesis for two level queries our approach synthesizes a two level extraction scheme for each website from a set of attributenodes and candidate containers identified in its webpages.
the twolevel query structure is reflected also in the synthesis process of the extraction scheme.
technically we use a two phase approach tosynthesize the extraction scheme.
in each site we first generate an xpath query for the containers.
we then filter the attribute nodes keeping only those reachable from containers that agree with thecontainer xpath and generate xpaths for their extraction relatively to the container nodes.
to generate an xpath query for a set of nodes e.g.
for the set of containers we consider the concrete xpath of each node this is an xpath that extracts exactly this node.
we unify these concrete xpaths by a greedy algorithm that aims to find the most concrete most strict xpath query that agrees with a majority of the concretexpaths.
keeping the unified xpath as concrete as possible prevents the addition of noise to the extraction scheme.
the generated xpaths for b n are depicted in fig.
.
in this example unification is trivial since the xpaths are identical.
how370figure example dom trees container body div table tr td table tr td title div h1 author div a price div div span url pattern .
jsp id .
figure crawling scheme for blackwell .
container body div div section div ol li div div title a author a price div a span url pattern s .
figure crawling scheme for b n .
ever if for example each of the container nodes labeled div ind1 had different id s the idfeature would have been removed during unification.
note that even if the subtree that contains the noisy instance of author x ind1had been identified as a candidate container e.g.
if it had contained values of the other attributes itwould have been discarded during the unification.
url pattern synthesis in order to synthesize a url pattern for the crawling scheme of a new site we extend the iterative technique used for synthesis of data extraction schemes in each iteration of the algorithm for each website we identify a set of pages of interest as pages that contain attribute data.
we filter these pages in accor dance with the filtering of container and attribute nodes.
we thenunify the urls of remaining pages similarly to xpath unification.
fig.
depicts the url pattern generated by our approach for b n .
this pattern identifies webpages in b n that present a list of books these are the pages whose structure conforms with the syn thesized extraction scheme.
note that b n also presents the same books in a separate page each but such pages require a different crawling scheme.
preliminaries in this section we define some terms that will later be used to describe our approach.
.
logical structure of webpages each webpage implements some logical structure .
following we use relations as a logical description of data which is independent of its concrete representation.
a relational specification is a set of relations where each relation is defined by a set of column names and a set of values for the columns.
a tuplet c1 d1 c2 d2 ... maps a set of columns c1 c2 ... to values.
a relationris a set of tuples t1 t2 ... such that the columns of everyt t rare the same.
for example b n blackwell and abedescribed in section implement a relational description of a list of books where each book has a title an author and a price.
then book title author and price are columns and the set of books is modeled as a relation with these columns where each tuple is a book item.
data items attributes and instances we refer to each tuple of a relationras adata item.
the columns of a relation rare called attributes denoted att.
each attribute defines a class of data sharing semantic similarities such as meaning and or extraction scheme.
the value of attribute a attin some tuple of ris also called aninstance ofa.
the set of all values of all attributes is denoted v. each attribute ais associated with an equivalence relation a that determines if two values are equivalent or not as instances of a. the notion of equivalence may differ between different at tributes.
by default if not specified by the user we use the bag of words representation of each value d denotedw d and use jaccard similarity function j d d2 with a threshold of .
as an equivalence indicator between values d1andd2 d1 ad2iffj d1 d2 .5wherej d1 d2 w d1 w d2 w d1 w d2 .
.
concrete layout of webpages technically webpages are documents with structured data such as xml or html documents.
the concrete layout of the webpageimplements its logical structure where attribute instances are presented as nodes in the dom tree.
xml documents as dom trees a well formed xml document describing a webpage of some website can be represented by a dom tree.
a dom tree is a labeled ordered tree with a set ofnodesnand a labeling function that labels each node with a set of node features not to be confused with data attributes wheresome of the features might be unspecified.
common node featuresinclude tag class andid.
371for example fig.
depicts part of the tree representation of pages of b n blackwell and abe.
a node labeled by a class title is a node whose tag isa class istitle a n d id is unspecified.
node descriptors anode descriptor is an expression xin some language defining a set of nodes in the dom tree.
we use expr to denote the set of node descriptors.
for a node descriptor x expr and a webpage p w ed e fi n e llbracketx rrbracketpto be the set of nodes described byxfromp.w h e npis clear from the context we omit it from the notation.
a node descriptor is concrete if it represents exactly one node.
we sometimes also refer to node descriptors as extraction schemes.
in this work we use xpath as a specification language for node descriptors.
.
xpath as a data extraction language xpath is a query language for traversing xml documents.
xpath expressions xpaths in short are used to select nodes from the dom tree representation of an xml document.
an xpath expression is a sequence of instructions x x1...x k. each instructionxidefines how to obtain the next set of nodes given the set of nodes selected by the prefix x1...x i where the empty sequence selects the root node only.
roughly speaking each instruc tionx iconsists of i axis defining where to look relatively to the current nodes at children descendants parent siblings ii node filters describing which tag to look for these can be all text comment etc.
and iii predicates that can restrict the selected nodes further for example by referring to values of additional node features e.g.
class that should be matched.
for example the xpath div a selects all nodes that follow a sequence of nodes that can start anywhere in the dom tree and has to consist of a node with tag div followed by some node whose features are unspecified and is fol lowed by a node with tag a andclass link type1 .
the crawler synthesis problem in this section we formulate the crawler synthesis problem.
a crawler for a website can be divided into two parts a page crawler and a data extractor .
the page crawler is responsible for grabbing the pages of the site that contain relevant information.
the data extractor is responsible for extracting data of interest from each page.
logical structure of interest our work considers websites whose data containing webpages share the following logical structure each webpage describes one main relation denoted data.
as such data items are tuples of the data relation.
further the set attof attributes consists of the columns of the data relation.
note that different concrete layouts can implement this simple logical structure.
for example if we consider a webpage that ex hibits a list of books then the concrete layout can first group books by author and for each author list the books or it can avoid the partition based on authors.
further some websites will present eachbook in a separate webpage whereas others will list several books in the same page.
even for websites that are structured similarly by the former parameters the mapping of attribute instances to nodesin the dom tree can vary significantly.
page crawlers a page crawler for a website sis given by a url pattern denoted u s which identifies the set of webpages of interest.
these are the webpages of the website that contain data of the relevant kind.
we denote by p s the set of webpages whose url matchesu s .
data extractors recall that we consider webpages where instances of different attributes are grouped into tuples of some relation de noted data.
we are guided by the observation that data in suchwebpages is typically stored in subtrees where each subtree con tains instances of all attributes for some data item i.e.
tuple of thedata relation .
we refer to the roots of such subtrees as containers containers a node in the dom tree whose subtree contains all the entries of a single data item i.e.
a single tuple of data is called a container.
note that any ancestor of a container is also a container.
we therefore also define the notion of a best container to be a container such that none of its predecessors is a container.
depending on the concrete layout of the webpage a best container might correspond to an element in a list or in another data structure.
it might also be the root of a webpage if each webpage presents only onedata item.
for example in the tree d 1depicted in fig.
both of the nodes selected by body ... div are containers and as such so are their ancestors including the root.however the latter are not best containers since they include strictsubtrees that are also containers.
attribute nodes a node in the dom tree that holds an instance of an attribute a attis called ana attribute node or simply an attribute node whenais clear from the context or does not matter.
data extractors a data extractor for the relation data over columns attin some website scan be described by a pair container f wherecontainer expr is a node descriptor representing containers andf att arrowhookleft expr is a possibly partial function that maps each attribute name to a node descriptor with the meaning that this descriptor represents the attribute nodes relatively to the container node i.e.
the attribute descriptor considers the container node asthe root.
the data extractor is partial iffis partial.
ifcontainer is empty it is interpreted as a node descriptor that extracts the rootof the page.
if container is empty andfis undefined for every attribute we say that the data extractor is empty .
examples of data extractors appear in fig.
and fig.
.
crawler synthesis the crawler synthesis problem w.r.t.
a set att of attributes is defined as follows.
its input is a set sof websites where each website s sis associated with a data extractor denotede s over att.e s might be partial or even empty.
the desired output is a page crawler along with a complete data extrac tor for everys s. data extractor synthesis in this section we focus on synthesizing data extractors as a first step towards synthesizing crawlers.
we temporarily assume that the page crawler is given i.e.
for each website we have the setof webpages of interest and present our approach for synthesizingdata extractors.
we will remove this assumption later and also address synthesis of the page crawler using similar techniques.
the input to the data extractor synthesis is therefore a set sof websites where each website s sis associated with a set of webpages denoted p s and with a data extractor denoted e s which might be partial or even empty.
the goal is to synthesize a complete data extractor for every s s. the main challenge in synthesizing a data extractor is identifying the mapping between the logical structure of a webpage and its concrete layout as a dom tree.
the key to understanding this mapping amounts toidentifying the container nodes in the dom tree that contain all the attributes of a single data item tuple .
once this mapping is learnt the next step is to capture it by synthesizing extraction schemes inthe form of xpaths.
the data extractor synthesis algorithm is first described using the generic notion of node descriptors.
in section .
we then instanti ate it for the case where node descriptors are provided by xpaths.
before we describe our algorithm we review its main ingredi372ents.
in the following we use n p to denote the set of nodes in the dom tree of a webpage p p s .
knowledge base of data across websites our synthesizer maintains a knowledge base o att 2vwhich consists of a set of observed instances for each attribute a att.
these are instances collected across different websites from s. they enable the synthesizer to locate potential a attribute nodes in webpages for which the data extractor ofais unspecified.
data to node mapping per website in addition to the global knowledge base for each website s sour synthesizer maintains i a setncont p n p of candidate container nodes for each webpagep p s and ii a set na p n p of candidate attribute nodes for each webpage p p s and attributea att.
deriving extraction schemes per website the synthesis algorithm iteratively updates the container and attribute node sets for each webpage inp s and attempts to generate a data extractor e s expr att arrowhookleft expr forsby generating node descriptors for the set of containers and for each of the attributes.
the extractionscheme is shared by all webpages of the website.
the updates of the sets and the attempts to generate node descriptors from the setsare interleaved as one can affect the other on the one hand node descriptors are generated in an attempt to represent the sets on the other hand once descriptors are generated elements of the sets thatdo not conform to them are removed.
while attribute instances are used to identify attribute nodes across different websites the synthesis of node descriptors is performedfor each website separately and independently of others while con sidering all of the webpages associated with the website .
.
algorithm algorithm presents our data extractor synthesis algorithm.
the algorithm is iterative where each iteration consists of two phases phase data extraction for knowledge base extension.
initially the setso a of instances of all attributes a attare empty.
in each iteration we use yet un crawled extraction schemes to extract attribute nodes in all webpages of all websites and extend the setso a for every attribute abased on the content of the extracted nodes.
at the first iteration input extraction schemes are used.
inlater iterations we use newly learnt extraction schemes generatedin phase of the previous iteration.
phase synthesis of data extractors.
for every website s sfor which the extraction scheme is not yet satisfactory we attempt to generate an extraction scheme by performing the following steps locating attribute nodes per page we traverse all webpages p p s and for each attribute awe use the instances o a collected in phase from this iteration and previous ones to identify potentiala attribute nodes in p. technically for every p p s we iterate on all n n p and use the default or user specified equivalence relation ato decide whether ncontains data that matches the attribute instances in o a .i fs o nis added tona p .
locating container nodes per page in every webpage p p s we locate potential container nodes and collect them in ncont p .
a container is expected to contain instances of all attributes att.
however since our knowledge of the attribute instances is incomplete we need to also consider subsets of att.
in each webpage we define the best set of attributes to be the set of all attributes whose instances appear in it.
potential containers are nodes whose subtree contains attribute nodes of the best set of attributes andno strict subtree contains nodes of the same set of attributes.
the latter ensures that the container is best.
technically for every node n n p we compute the set of reachable attributes a att such that ana attribute node in n a p is reachable from n. nodesnwhose set is best and no other node reachable from nhas the same set of reachable attributes are collected in ncont p .
for each container node nc ncont p we also maintain its support t h e number of attribute nodes reachable from it.
generating container descriptor we consider the concrete node descriptor of every container node nc ncont p in every webpagep p s .
we unify the concrete node descriptors across all webpages into a single node descriptor and use it to updatee s relying on the observation that containers are typically elements of some data structure and are therefore accessed similarly.
filtering attribute nodes based on container descriptor we filter the setsn cont p of containers in all webpages to keep only containers that match the unified node descriptor and accordingly filter the setsna p of attribute nodes in all webpages to contain only nodes that are reachable from the filtered sets of containers.
this step enables us to automatically distinguish the nodeswe are interested in from others that accidentally contain attribute instances without any a priori knowledge.
generating attribute descriptors for each attribute a att we consider the concrete node descriptors of all the nodes in thefiltered setsn a p of all webpages p p s where the concrete node descriptor of nis computed relatively to the container node whose subtree contains n. for each attribute a we find a unified node descriptor for these concrete node descriptors and use it to updatee s .
again we use the observation that containers are structured similarly and therefore attribute data within them is accessed similarly.
remark.
for a successful application of our algorithm at leastone extraction scheme should be provided for every attribute.
ourapproach is also applicable if a user provides a set of annotated webpages instead of a set of initial extraction expressions.
section describes a running example of our algorithm.
node descriptor unification node descriptors for the container and attributes are generated by unifying concrete node descriptors of the nodes inn cont p andna p respectively.
roughly speaking the purpose of the unification is to derive a node descriptor that is general enough to describe as many of the concrete node descriptors as possible but also as concrete as possible in order tointroduce as little noise as possible.
concreteness of a node descriptorxis measured by an abstraction score denoted abs x .
the node descriptor unification algorithm is parametric in the abstraction score.
in section .
we provide a definition of this score when the node descriptors are given by xpaths.
d efinition .
.
for a setxof concrete node descriptors and a weight function support that associates each x xwith its support the unification problem aims to find a node descriptor xg s.t.
.support x x llbracketx rrbracket llbracketxg rrbracket support x i.e.
xgcaptures at least of the total support of the node descriptors in x. .
abs x g is minimal.
in container descriptor unification step the given node descriptors represent container nodes.
the support of each descriptor represents the number of attribute nodes reachable from the container.
in attribute descriptors unification step the given descriptors represent attribute nodes for some attribute all of whichare reachable from a set of containers of interest.
the attribute node descriptors are relative to the container nodes.
.
implementation using sequential xpaths in order to complete the description of our data extractor synthesizer we describe how the ingredients of algorithm are implemented when node descriptors are given by xpaths.
specifically our approach uses sequential xpaths 373algorithm data extractor synthesizer input set of attributes att input set of websites s input am a p e s expr att arrowhookleft expr mapping a website s to a data extractor e s which consists of a possibly empty container descriptor as well as a possibly partial mapping of attributes to node descriptors o while there is change in ooredo data extraction phase foreach s ss.t.e s is uncrawled do o o extractinstances att p s e s o synthesis phase foreach s ss.t.e s is incomplete do locate attribute nodes foreach p p s do foreach a attdo na p findattnodes n p a o a locate container nodes foreach p p s do bestattset a att na p foreach n n p do reachatt a att n reach n n na p support n reach n a att n na p ncont p candidates n n p reachatt bestattset foreach n candidates do foreach n children n do ifn candidates then ncont p ncont p n break generate container descriptor exprs relativeexpr p emptyexpr n support p p s n ncont p containerexpr unifyexpr exprs filterattributenodes generate attribute descriptors foreach a attdo exprs relativeexpr p containerexpr n p p s n na p attexpr unifyexpr exprs e s containerexpr attexpr return e sequential xpaths apath in the dom tree is a sequence of nodesn1 ... n k where for every i k there is an edge fromnitoni .
such a path can naturally be encoded using an xpath xs x1...x kwhere eachxistarts with .x1may start with rather than i f does not necessarily start at the root of the tree.
further each xiuses node filters and predicates to describe the features of ni.
therefore xican be described via equalitiesf1 v1 ... f m vm such thatfj f w h e r efis the set of node features used.
we consider f tag class id for simplicity but our approach is not limited to these features.
a feature might be unspecified for ni in which case no corresponding equality will be included in xi.
for example let be the left most path in d2 .
then xs body ... td div h1 .xs can also be described as a sequence tag body ... tag td tag div tag h1 .
we refer to xpaths of the above form as sequential .
the xpaths that our approach generates as node descriptors are all sequential.
concrete xpaths each nodenin the dom tree can be uniquely described by the unique path denoted n leading from the root ton.
the xpath xs n is a sequential xpath such that llbracketxs n rrbracket n a n d llbracketxs n rrbracketis minimal i.e.
every other sequential xpath that also describes n describes a superset of llbracketxs n rrbracket .
we therefore refer to xs n as the concrete xpath ofn denoted xs n with abuse of notation.
if we include in fthe position of a node among its siblings as an additional node feature and encode it by an xpath instruction using sibling predicates then we will have llbracketxs n rrbracket n .
agreement of sequential xpaths we observe that for sequential xpaths checking if a node nmatches a node descriptor xg i.e.
n llbracketxg rrbracket can be done by checking if the concrete xpath xs n agrees with the xpath xg where agreement is defined as follows.
definition .
.
letx x1...x kandxg xg ...xg mbe sequential xpaths.
the instruction xiagrees with instruction xg i if whenever some feature is specified in xi it either has the same value inxg ior it is unspecified in xgi.
the xpathxagrees with the xpathxgifm k and for every i m xiagrees withxg i. for example body ... td div h1 agrees with both body ... td div h1 a n d body ... td div .
node descriptor unification via xpath unification we now describe our solution to the node descriptor unification problem in the setting of sequential xpaths.
we first define the abstraction score abstraction score for a sequential xpath instruction xiwe define spec xi to be the subset of features whose value is specified in xi andunspec x i f spec x i is the set of unspecified features inxi.
we define the abstraction score of xito be the number of features in unspec xi t h a ti s a b s xi unspec x i .
for a sequential xpath x x1...x k we define abs x to be the sum of abs x i .
greedy algorithm for unification algorithm presents our unification algorithm.
we use the observation that for sequential xpaths the condition llbracketx rrbracket llbracketxg rrbracketthat appears in item of the unification problem see definition .
can be reduced to checking if the xpathxagrees with the xpath xg.
letxbe a weighted set of sequential xpaths with a weight function support that associates each xpath in xwith its support.
letts support x denote the total support of xpaths in x. the unification algorithm selects kto be the length of the longest xpath inx.
it then constructs a unified xpath xg xg ... xg m top down from i tok possibly stopping at i m k .
intuitively in each step the algorithm tries to select the most concrete instruction whose support is high enough.
note that there is a tradeoff between the high support requirement and the highconcreteness requirement.
we use the threshold as a way to balancethese measures.
at iterationiof the algorithm x i 1is the restriction of xto the xpaths whose prefix agrees with the prefix xg ... xg i 1ofxg computed so far initially x0 x .
we inspect the i th instructions of all xpaths in xi .
the corresponding set of instructions is denoted byii xi x xi .
the support of an instruction xbw.r.t.iiissupport x xi xiagrees withxb .
to select the most concrete instruction whose support is high enough we consider a predefined order on sets of feature value pairs where sets that are considered more concrete i.e.
more specified precede sets considered more abstract .
technically we consider only feature value sets where each feature has a unique value.
the order on such sets used in the algorithm is defined suchthat if b b2 thenb1precedesb2.
in particular we make sure that sets where all features are specified are first in that order.
for every setbof feature value pairs ordered by the predefined order we consider the instruction xbthat is specified exactly on 374the features inb a sd e fi n e db y b. if its support exceeds w es e t xg itoxbandxito x xi xiagrees withxb .
otherwise xbis not yet satisfactory and the search continues with the next b. there is always a bfor which the support of the xbexceeds the threshold for instance the last set bis always the empty set with xb which agrees with all the concrete xpaths in xi .
if at some iteration ii i.e.
the xpaths in xi 1are all of length iand therefore there is no next instruction to discover the algorithm terminates.
otherwise it terminates when i k. example .given the following concrete xpaths as an input cx1 div span a cx2 div span a cx3 div span a the unification starts with x0 cx1 cx2 cx3 andi .t o selectxg recall that the algorithm first considers the most specific feature value sets in order to find the most specific instruction .
in our example it starts from b1 tag div class note for whichxb1 div .
however cx3is the only xpath inx0which agrees with xb1.
therefore it has support of .
we use a threshold of .
thus the support ofxb1is insufficient.
the algorithm skips to the next option obtaining xb2 div .
this instruction is as specific as xb1and has a sufficient support of it agrees withcx1andcx2 .
therefore for i the algorithm selectsxg xb2andx1 cx1 cx2 .f o ri the algorithm selectsxg span as the most specific instruction which also has support of both cx1andcx3fromx1agree with it .
fori the algorithm selects xg aas none of the more specific instructions a or a has a support greater than .
the resulting unified xpath is x div span a .
algorithm top down xpath unification input setxof sequential xpaths input support function support x n input threshold ts support x k m a x x x x x0 x foreach i ... k do ii xi x xi ifii then i i break foreach b fin decreasing order of b do support b findsupport xb xi i support ifsupport b tsthen xg i xb xi x xi xiagrees with xb break return xg ... xg i crawler synthesis in this section we complete the description of our crawler synthesizer.
to do so we describe the synthesis of a page crawler for eachwebsites.
recall that a page crawler corresponds to a url pattern u s which defines the webpages of interest.
the synthesis of a page crawler is intertwined with the data extractor synthesis anduses similar unification techniques to generate the url pattern.initialization we assume that each website s sis given by a main webpage p main s .
initially the set p s of webpages of s is the set of all webpages obtained by following links in pmain s and recursively following links in the resulting pages where the traversed links are selected based on some heuristic function which determines which links are more likely to lead to relevant pages.
iterations we apply the data extractor synthesis algorithm of section using the sets p s .
at the end of phase of each iteration we updateu s using the steps described below.
at the beginning of phase of the subsequent iteration we then update p s to the set of webpages whose urls conform with u s .
filtering webpage sets based on the observation that relevant webpages of a website shave a similar structure we keep inp s only webpages that contain container and attribute nodes that match the generated e s and are reachable from pmain s via such webpages.
generating url patterns for each webpage p p s we consider its url.
we unify the urls into u s by a variation of algorithm which views a url as a sequence of instructions similarly to a sequential xpath.
evaluation in this section we evaluate the effectiveness of our approach.
weused it to synthesize data extracting web crawlers for real world websites containing structured data of different categories.
our experiments focus on two different aspects i the ability to suc cessfully synthesize web crawlers and ii the performance of the resulting web crawlers.
.
experimental settings we have implemented our tool in c .
all experiments ran on a machine with a quad core cpu and 32gb memory.
our experiments were run on different websites related to nine different categories books tvs conferences universities cameras phones movies songs and hotels.
for each category we selected a group of known sites which appear in the first page of google search results.
the sites in each category have a different structure but they share at least some of their instances which makes our approach applicable.
the complexity of the data extracted from different categories is also different.
for instance a movie has four attributes title genre director and list of actors.
for a book the set of attributes consists of title author andprice while the attribute set of a camera consists of the name andprice only.
in each category we used one manually written crawler and automatically synthe sized the others for the books category we also experimented with 3partial extraction schemes one for each attribute .
to synthesize the web crawlers our tool processed over webpages from the30different sites.
to evaluate the effectiveness of our tool we consider 4aspects of synthesized crawlers i crawling scheme completeness ii url filtering iii container extraction and iv attributes extraction.
.
experiments and results crawling scheme completeness a complete crawling scheme defines extraction queries for all of the data attributes.
the completeness of the synthesized crawling schemes is an indicator for the success of our approach in synthesizing crawlers.
to measure completeness we calculated for each category the average number ofattributes covered by the schemes divided by the number of attributes of the category.
the results are reported in fig.
left .
the results show that the resulting extraction schemes are mostlycomplete with a few missing attribute extraction queries.
375figure results crawling scheme completeness left url filtering middle and attribute extraction right for each category.
figure attribute extraction precision and recall and crawling scheme completeness as a function of the threshold of jac card similarity used to define equivalence between instances.
url filtering the ability to locate pages containing data is an important aspect of a crawler s performance.
to evaluate the url filtering performance of the synthesized crawlers we measure therecall andprecision of the synthesized url pattern for each site recall rel sol rel precision rel sol sol to do so we have manually generated two sets of urls for eachsite one containing urls for pages that contain relevant data comprising the rel set ground truth and another denoted irr contains a mixture of irrelevant urls from the same site.
solcontains the urls from rel irr that match the synthesized url pattern for the site i.e.
the urls accepted by the synthesised url pattern .
a good performing url filtering pattern should match allthe urls from rel and should not match any from irr.t h e a v erage recall and precision scores of the sites of each category arecalculated and reported in fig.
middle .
container extraction to check the correctness of the synthesized container extraction query we have manually reviewed the resulting container xpaths against the html sources of the relevant webpages for each site to verify that each extracted container con tains exactly one data item.
we found that the containers always contained no more than one item.
however in a few cameras and songs websites the container query was too specific and did notextract some of the containers this happened in tables containingclass odd in some rows and class even in others which affected the recall scores of attribute extraction.
attributes extraction we calculate the recall and precision see equation of the extraction query for each attribute.
techni cally for each category of sites we have manually written extraction queries for each attribute in every one of the category relatedsites.
for each attribute a we used these extraction queries to extract the instances of afrom a set of sample pages from each site.
the extracted instances are collected in rel.
we have also applied the synthesized extraction queries as a concatenation of the con tainer xpath and attribute xpath to extract instances of afrom the same pages into sol.
for each site the precision and recall are calculated according to equation .
the average over sites of thesame category recall and precision scores of all attributes of each category are reported in fig.
right .
equivalence relation to evaluate the effect of the threshold used in the equivalence relation a on the synthesized crawlers we have measured the average completeness as well as the average recall and precision scores of attribute extraction as a function of the threshold.
the results appear in fig.
.
remark.
the reported attribute extraction recalls in fig.
and fig.
are computed based on queries for which synthesis succeeded missing queries affect only completeness and not recall .
.
discussion the completeness of the synthesized extraction schemes is highly dependent upon the ability to identify instances in pages of some site by comparison to instances gathered from other sites.
for mostcategories completeness is high.
for the conferences category however completeness is low.
this is due to the use of acronyms in conference names e.g.
icse in some sites vs. full names e.g.
in ternational conference on software engineering in others whichmakes it hard for our syntax based equivalence relation to identify matches.
this could be improved by using semantic equivalence relations such as esa or w2v .
as for the quality of the resulting extraction schemes and url filtering patterns most of the categories have perfect recall .
however some have a slightly lower recall due to our attempt to keep the synthesized xpaths or regular expressions for url fil tering as concrete as possible while having majority agreement.
this design choice makes our method tolerant to small noises in the identified data instances and prevents such noises from caus ing drifting without negative examples.
yet in some cases the resulting xpaths are too specific and result in a sub optimal recall.
for precision most categories have good scores while a few have lower scores.
loss of precision can be attributed to the majoritybased unification and the lack of negative examples.
for the books category for instance the synthesized extraction xpath of price for some sites is too general since they list multiple price instances original price discount amount and new price .
all are listed in 376the same parent container with the author and book title and are therefore not filtered by the container hence affecting xpath unifi cation.
this could be improved with user guidance.
the results in fig.
reflect the tradeoff between precision and crawling scheme completeness.
a more strict equivalence relation with higher threshold leads to a better precision but has negative effect on the scheme completeness whereas the use of a forgiv ing equivalence relation with lower threshold severely affects theprecision.
we use a threshold of .5as a balanced threshold value.
according to our findings the attribute queries suffer from a lowrecall for both low and high threshold values.
in low threshold it is due to wrong queries that extract wrong nodes e.g.
menu nodes without including attribute nodes.
for higher threshold values the tool identified less instances of attribute nodes sometimesonly one leading to a lower quality generalization.
real world use case we used our crawler synthesis process as a basis for data extraction for several product reviews websites.
for instance tvexp.com weppir.com camexp.com and phonesum.com extract product names and specifications specs using our ap proach.
we manually added another layer of specs scoring and created comparison sites for product specs.
these websites have a continually updated database with over products.
related work in this section we briefly survey closely related work.
while therehas been a lot of past work on various aspects of mining and dataextraction our technique has the following unique combination of features i works across multiple websites ii synthesizes both the extraction xpath queries and the url pattern iii is auto matic and does not require user interaction iv works with only positive examples v does not require an external database and vi synthesizes a working crawler.
data mining and wrapper induction our work is related to data mining and wrapper induction.
in contrast to supervised techniques e.g.
our approach only requires an initial crawler or partial crawling scheme and requires no tagged examples.
flashextract allows end users to give examples via aninteraction model to extract various fields and to link them using special constructs.
it then applies an inductive synthesis algorithm to synthesize the intended data extraction program from the givenexamples.
in contrast our starting point is a crawler for one ormore sites which we then extrapolate from.
further our technique only requires positive examples obtained by bootstrapping our knowledge base by crawling other sites .
unsupervised extraction techniques have been proposed.
several works propose methods that use repeated pattern mining to discover data records while use tree edit distance as the basis for recordrecognition and extraction in a single given page.
these methodsrequire manual annotation of the extracted data or rely on knowledge bases .
roadrunner uses similarities and differences between webpages to discover data extraction pattern.
sim ilarities are used to cluster similar pages together and dissimilarities between pages in the same cluster are used to identify relevant structures.
other information extraction techniques rely on textual or use visual features of the document for data extraction.
clustvx renders the webpage in contemporary web browser for processing all visual styling information.
visual and structuralfeatures are then used as similarity metric to cluster webpage elements.
tag paths of the clustered webpages are then used to derive extraction rules.
in contrast our approach does not use visualstyling but relies on similar content between the different sites.hao et al.
present a method for data extraction from a group of sites.
their method is based on a classifier that is trained on aseed site using a set of predefined feature types.
the classifier isthen used as a base for identification and extraction of attribute instances in unseen sites.
in contrast our goal is to synthesize xpaths that are human readable editable and efficient.
further with thelack of an attribute grouping mechanism such as our notion of container the method cannot handle pages with multiple data items.
program synthesis several works on automatic synthesis of programs were recently proposed aiming for automating repetitive programming tasks.
programming by example for instance is a technique used in for synthesizing a programby asking the user to demonstrate actions on concrete examples.
inspired by these works our approach automatically synthesizes data extracting web crawlers.
however we require no user interaction.
semantic annotation many works in this area attempt to automatically annotate webpages with semantic meta data.
seeker is a platform for large scale text analysis and an application written on the platform called semtag that performs automated semantic tagging of large corpora.
ciravegna et al.
propose a methodology based on adaptive information extractionand implement it in a tool called armadillo .
the learning process is seeded by a user defined lexicon or an external data source.
in contrast to these works our approach does not require externalknowledge base and works by bootstrapping its knowledge base.
other aspects of web crawling there are a lot of works dealing with different aspects of web crawlers.
jiang et al.
and jung et al.
deal with deep web related issues like the problem of discovering webpages that cannot be reached by traditional web crawlers mostly because they are results of a query submitted toa dynamic form and they are not reachable via direct links fromother pages.
some other works like address the problem of efficient navigation of website pages to reach pages of specific type by training a decision model and using it do decide which links tofollow in each step.
our paper focuses on the different problem of data extraction and is complementary to these techniques.
conclusion we presented an automatic synthesis of data extracting web crawlersby extrapolating existing crawlers for the same category of data from other websites.
our technique relies only on data overlapsbetween the websites and not on their concrete representation.
as such we manage to handle significantly different websites.
technically we automatically label data in one site based on others andsynthesize a crawler from the labeled data.
unlike techniques that synthesize crawlers from user provided annotated data we cannot assume that all annotations are correct hence some of the examplesmight be false positives and we cannot assume that unannotated data is noise hence we have no negative examples .
we overcome these difficulties by a notion of containers that filters the labeling.
we have implemented our approach and used it to automatically synthesize 30crawlers for websites in nine different product categories.
we used the synthesized crawlers to crawl more than12 webpages over all categories.
in addition we used our method to build crawlers for real product reviews websites.