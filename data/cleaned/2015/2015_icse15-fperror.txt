a genetic algorithm for detecting significant floating point inaccuracies daming zou y ran wang y yingfei xiong y lu zhang y zhendong suz hong mei y key laboratory of high confidence software technologies peking university moe china yinstitute of software school of eecs peking university china zdepartment of computer science university of california davis usa fzoudm lilianwangran xiongyf zhanglucs meih g pku.edu.cn su ucdavis.edu abstract it is well known that using floating point numbers may inevitably result in inaccurate results and sometimes even cause serious software failures.
safety critical software often has strict requirements on the upper bound of inaccuracy and a crucial task in testing is to check whether significant inaccuracies may be produced.
the main existing approach to the floating point inaccuracy problem is error analysis which produces an upper bound of inaccuracies that may occur.
however a high upper bound does not guarantee the existence of inaccuracy defects nor does it give developers any concrete test inputs for debugging.
in this paper we propose the first metaheuristic search based approach to automatically generating test inputs that aim to trigger significant inaccuracies in floating point programs.
our approach is based on the following two insights with fpdebug a recently proposed dynamic analysis approach we can build a reliable fitness function to guide the search two main factors the scales of exponents and the bit formations of significands may have significant impact on the accuracy of the output but in largely different ways.
we have implemented and evaluated our approach over real world floating point functions.
the results show that our approach can detect significant inaccuracies in the subjects.
i. i ntroduction inaccuracy caused by floating point numbers is a well known problem in software development.
in critical software systems disastrous results may be caused by floating point inaccuracy.
an example well cited in the literature is the failure of a patriot missile to intercept an incoming missile in the first persian gulf war due to the accumulated floating point errors during the continuous tracking and guidance.
this failure caused the loss of lives and around injuries.
floating point numbers are usually less inaccurate because they use finite number of digits to represent a real number.
for example when we represent .
as a single precision floating point number because .
cannot be represented in finite digits in a binary fraction the value is in fact .
.
one consequence of the we sincerely acknowledge dr. hao zhong at shanghai jiaotong university for his useful and helpful feedback on an early draft of this paper and xinrui he at peking university for her help on setting up the experiments.
the authors from peking university are supported by the national basic research program of china under grant no.
2014cb347701 and the national natural science foundation of china under grant no.
.
zhendong su is partially supported by united states nsf grants and .
yingfei xiong is the corresponding author.inaccuracy in representation is the rounding error e.g.
when adding a small number to a large number significant digits in the small number may be rounded off due to the limited digits to represent the final result.
accumulated rounding errors during the computation may result in significant inaccuracy in the output.
because of the importance of ensuring accuracy in floatingpoint calculation several approaches have been proposed to detect floating point inaccuracies.
most approaches use static analysis based on interval arithmetic or affine arithmetic trying to determine the possible range of errors in the result of the computation.
however we observe several limitations in reporting the ranges of errors due to the limitations of static analysis the computed range is often an over approximation of the actual error and the difference is often very large.
even in the latest approach the computed upper bound is usually several orders of magnitude larger than the actual error and is sometimes infinite.
as a result even if a large upper bound is reported it is still unknown whether or not a problem in accuracy exists.
to debug an inaccuracy problem it would be more convenient to have an input that triggers the problem so that developers can follow the execution to discover the root cause of the problem.
however as reported by bao and zhang there may be only a small portion of inputs among all possible inputs causing significant inaccuracies in the output.
thus it would be quite difficult for developers to obtain such an input manually.
to overcome these problems this paper proposes a metaheuristic search based approach that aims to generate a test input for a program to maximize the error of the output.
to the best of our knowledge our approach is the first metaheuristic test generation approach aiming to detect floatingpoint inaccuracies.
existing test generation approaches for floating point programs focus on maximizing path coverage rather than output errors.
our approach is based on the following two insights.
first with fpdebug an approach recently proposed by benz et al.
we can obtain the likely error occurred in the output of a particular concrete execution.
thus we can build a fitness function around this error to guide the search.
second there are two main factors of the inputtable i ieee floating point representation sign exponent significand single precision double precision floating point numbers that may affect the error of the output the scales of exponent and the bit formation of significand but their relations to accuracy exhibit different characteristics.
these factors could be exploited for designing efficient search algorithms.
in summary we make the following major contributions we perform an empirical analysis to uncover the relations between different factors and the accuracy of output.
the results suggest that both the scales of the exponents and the bit formation of significands may substantially affect accuracy.
while only exponents in a small interval lead to significant inaccuracies a large portion of significands may lead to significant inaccuracies.
we design a novel genetic algorithm locality sensitive genetic algorithm lsga based on the results of the empirical analysis.
our basic idea is to evolve the exponent to hit the small interval while randomly generate the significand as the large portion of bit formation is easy to hit.
the fitness function is built upon the output of fpdebug .
we evaluate our approach by a sanity check on six classic examples of stable and unstable algorithms and a series of experiments on floating point functions selected from the latest version of the gnu scientific library.
our experiments compare three search algorithms including our own a standard genetic algorithm and a random search algorithm.
our results reveal that our algorithm exhibits absolute superiority over the other two algorithms and our approach is able to find extremely large inaccuracies in widely used real world scientific functions.
the rest of the paper is organized as follows.
section ii introduces some background knowledge while section iii further motivates our research.
section iv presents the empirical analysis and our algorithm design.
section v reports the two evaluations.
section vi discusses the main limitations and possible future research.
section vii discusses related work and section viii concludes.
ii.
b ackground a. format of floating point numbers according to ieee standard a floating point number contains three parts sign exponent and significand.
table i depicts the numbers of bits of the three parts in either a single precision number or a double precision number.
let us denote the sign as s the value of the exponent as e and the value of the significand as f. if all bits of the exponent are this floating number is one of the special values ornan wherenan indicates errors in computation suchas division by zero.
otherwise the value of a floating point number is depicted by formula s f 2e suppose that the significand is in the form of b0b1 bn the value of the significand f is defined by formula f pn i 0bi 2i if all bits of eare pn i 0bi 2i otherwise suppose that the exponent is in the form of b0b1 bn the value of the exponent e is defined by formula e n 1x i 02ibi n b. genetic algorithm a genetic algorithm ga is a metaheuristic search technique that simulates the process of natural selection for solving optimization problems.
in a ga each candidate solution is called an individual and has a set of properties that can be represented in a binary form.
there is also a fitness function f to evaluate how close an individual is to an optimal solution.
the process of ga typically starts from a set of individuals randomly selected or pre defined to form the first generation.
the population size is problem dependent.
when to create the next generation all the individuals in the current generation are put into a selection pool in which each individual has a probability which is dependent on the fitness of the individual to be a parent for generating individuals in the next generation.
to create an individual in the next generation a pair of parent solutions are selected from the selection pool.
then two types of operations i.e.
crossover and mutation are used to create a child of the two parents.
the generated children optionally plus the individuals from the previous generation form the next generation.
the selection creation loop is repeated until reaching a termination condition such as finding a good enough solution reaching the maximum number of generations and reaching the time limit.
there are several key components in designing a genetic algorithm such as the selection method i.e.
how to select individuals for reproduction crossover operator i.e.
how to produce a child from two parents mutation operator i.e.
how to mutate a child and initial population i.e.
how the first generation is populated .
iii.
m otivating example example.
to motivate our research let us consider function f x defined in the following code snippet.
f x is a carefully constructed example to demonstrate the problem of floatingpoint inaccuracy containing several major operations on floating point numbers addition subtraction division and a common code pattern adding up many numbers in a loop .f x should always returns a constant number in real arithmetic.
float f float x f int i n float y z y z n if x x x for i i n i y y x y y z return .125f x y .875f g as the loop on lines and will be executed ntimes the value of ybefore executing line should be equal to n jxj .
to make the analysis easy we set nto8192 .
as a result the value of yafter executing line should be equal to jxj.
therefore f x should always return .
however due to the accumulation of rounding errors in the loop the value of yafter executing line cannot be exactly jxj.
this error will be further magnified on line .
as a result f x may produce significant inaccuracy for some inputx.
the most inaccurate output is .
.
range of problematic inputs.
the range of xforf x to produce significant inaccuracies is not large.
the reason is that when the value of jxjbecomes larger e.g.
times larger than .
the value of f x will be mainly decided by jxj not the accumulated rounding error.
in fact all the cases of inaccuracy over .
occur when xis between .
and .
.
note that when the value of xis very close to f x cannot produce very large inaccuracy.
the reason is that a very smalljxj e.g.
smaller than .
would not produce a large enough rounding error when executing the loop only times.
considering that the number of possible values of xis huge when testing f x generating tests to trigger significant inaccuracies of f x should be difficult.
in fact if we want to trigger an inaccuracy that is close to the largest inaccuracy .
e.g.
the absolute error is larger than .
the range ofjxjis much smaller i.e.
between .
and .
.
note that existing coverage based test generation cannot help here since any value of xcan achieve branch coverage.
factors affecting accuracy.
this example demonstrates that there are two distinctive factors that may affect the error of the output and the impact of one factor may be very different from the other.
the first factor is the scale of the exponent in the input.
since the scale of a floating number is mainly determined by its exponent only when the exponent falls into a small range can the input trigger a large error.
the second factor affecting the accuracy of the output is the bit formation of the significand.
due to the round to even policy adopted by floating point numbers large inaccuracies appear only when the accumulated rounding errors do not cancel each other.
this requires that the formation of bits in the significand exhibits certain patterns.
iv.
approach a. problem definition given a program using floating point numbers denoted as p withminput parameters denoted as i1 i2 ... im wedeem the problem of detecting floating point inaccuracy in p as a search problem.
the search space of this search problem is the space represented by all the possible combinations of the values of the minput parameters.
the aim of the search problem is to find a particular input i.e.
a combination of the values of the minput parameters that maximize the error of the output.
to solve this search problem we need a criterion to determine whether one input would lead to a more inaccurate output than another input.
search algorithms will use this criterion to guide the searching process.
unlike coverage which is widely used as the criterion in search based test generation it is not straightforward to set up the criterion needed in our approach.
fortunately benz et al.
recently proposed a dynamic analysis technique which dynamically increase the precision of the floating point numbers to calculate both the likely absolute error and the likely relative error of the output of a program for an arbitrary input.1following the common practice we use relative error to measure the inaccuracy of the output.
let us denote the real result as rand the actual output asa.
the relative error is defined as jr aj jrj.
with this basic framework different metaheuristic search techniques can be adopted for detecting significant inaccuracies.
however it is not easy to discover significant inaccuracies through searching.
first as our motivating example and an existing study have shown often only a very small portion among all possible floating point numbers may lead to serious inaccuracies.
when there is more than one input parameter for a floating point function the probability of hitting a large error becomes very low.
second fpdebug has a slow down of several hundreds of times such that we could test only a relatively small number of input values during the search process.
as a result it is critical to design an effective search algorithm that hits large inaccuracies quickly.
to design such an algorithm we perform a small empirical analysis to understand the relation between the accuracy of the output and different factors.
b. empirical analysis in section iii we have seen that there are two main factors that may affect the accuracy of the output each corresponding to one of the three main components of a floating point numbers.
these two factors can be exploited to design an effective search algorithm.
to further understand the relation between the two factors and the accuracy of the output we performed a small empirical analysis.
analysis setup.
in our analysis we randomly chose four functions from the gnu scientific library gsl .
more description of gsl can be found in section v. the four functions are bessel k0 ci erf and legendre q1.
function bessel k0 computes the cylindrical besssel function ci computes the cosine integral erf computes the gauss error function and legendre q1 computes the legendre function.
we deliberately 1these errors are likely errors because increasing the precision does not guarantee more accurate result in all cases .
g882 g1005 g1012 g882 g1005 g1011 g882 g1005 g1010 g882 g1005 g1009 g882 g1005 g1008 g882 g1005 g1007 g882 g1005 g1006 g882 g1005 g1005 g882 g1005 g1004 g882 g1013 g1004 g1005 g1004 g1012 g1006 g1005 g1010 g1007 g1006 g1008 g1008 g1007 g1006 g1009 g1008 g1004 g1010 g1008 g1012 g1011 g1009 g1010 g1012 g1010 g1008 g1013 g1011 g1006 g1005 g1004 g1012 g1004 g1005 g1005 g1012 g1012 g1005 g1006 g1013 g1010 g1005 g1008 g1004 g1008 g1005 g1009 g1005 g1006 g1005 g1010 g1006 g1004 g1005 g1011 g1006 g1012 g1005 g1012 g1007 g1010 g1005 g1013 g1008 g1008 g62 g381 g336 g894 g90 g286 g367 g258 g410 g349 g448 g286 g3 g28 g396 g396 g381 g396 g895 g28 g454 g393 g381 g374 g286 g374 g410 g3 g381 g296 g3 g47 g374 g393 g437 g410fig.
erf at significand 0x34873b27b23c6 fig.
erf at exponent chose functions with one floating point input parameter to simplify our analysis.
we invoked these programs in fpdebug with different inputs and monitored how the relative errors of output change based on the change of input.
to better understand the two factors individually we fixed one factor and changed the other.
first we fixed significand and changed exponent.
for each program we randomly generated three significands.
for each significand we combined it with every possible exponent allowed by the double precision format to form an input to a subject program and then we executed the program in fpdebug to get the error for each input.
second we fixed exponent and changed significand.
we randomly generated two exponents for each program.
however because of the slow down from fpdebug it is not possible to test the whole space of significand in double precision.
as a result we generated random significands and tested each pair of significand and exponent.
in all tests we set the sign bit to .
results.
fig.
shows different relative errors at different exponents for function erf when the significand is fixed at 0x34873b27b23c6.
fig.
shows different relative errors at different significands for erf when the exponent is fixed at .
note that in the x axes of both figures we interpret both the exponents and significands as unsigned integers and report their integer values.
we shall use the two figures as examples to illustrate our results and the experiments on all functions exhibit very similar patterns.
first we observe that both the exponents and the significands have a great impact on the relative errors.
in all functions by changing the significands we could achieve a difference of orders of magnitude and by changing the exponents we could achieve a difference of orders of magnitude.
second we observe that the exponents that invoke largerelative errors stay in a small interval of the axis while the significands that invoke large errors distribute evenly on the axis.
this result confirms our analysis about the two factors in section iii.
how exponents affect accuracy is decided by their scales while how significands affect accuracy is decided by their bit formations.
third as we can see from the figures the portion of exponents that invoke large relative errors is usually very small.
in the worst case there is no more than .
of the exponents invoking errors that are within two orders of magnitude difference of the largest error.
on the other hand the portion of significands that invoke large relative errors is usually large.
even in the worst case we still have more than of the significands invoking errors that are within two orders of magnitude difference of the largest error.
note that this result is consistent with the existing study although a large portion of significands may invoke large errors the probability of a random input invoking a large error is still very small as both the significand and the exponent need to invoke large errors.
fourth there usually exist exponents that are near the interval of large errors and lead to errors higher than the average.
as shown in fig.
there is small burst of error near the exponent of before the large burst near .
though the errors from the small burst are still much smaller than the largest error they are higher than most other errors.
fifth we observe that the small interval of exponents leading to large errors is likely to be near the value which is just the median of all double precision exponents.
this indicates that the exponents around the median may have higher probability to invoke large relative errors.
the third observation suggests that the key to design an effective search algorithm is to hit the small interval of the exponents that lead to large errors which is much more difficult than hitting a significand that lead to a large error.
the fourth and fifth observations indicate possible strategies towards this problem.
the next section explains how we design our algorithm based on these observations.
c. our genetic algorithm as revealed by the fourth observation exponents near the small interval of large relative errors may also invoke a relatively high error.
though it is difficult to hit the small interval of large errors it is much easier to hit an exponent near it and gradually evolve the exponent to hit the small interval.
based on the fifth observation exponents invoking large errors often appear near the median of all possible exponents so searching around the median might be more effective than searching in other places.
based on these ideas we design a genetic algorithm named locality sensitive genetic algorithm lsga .
a high level outline of our algorithm is shown in algorithm .
let us first consider programs with one floating point input parameter.
our genetic algorithm first randomly generates a set of exponents line .
this generation process tries to make the generated exponents evenly distributed in the space of all exponents algorithm outline of lsga population generateinitialpopulation fori ndo input select population input mutate input population.add input return maxerror population but favors exponents around the median.
these exponents are combined with randomly generated significands and signs to form the initial population.
during every iteration we pick an individual with a high relative error line mutate it line and put it back to the population line without removing the original one from the population.
the mutation process adds a random number to the exponent regenerates its significand randomly and flips its sign bit with a probability.
this process repeats until a predefined number of iterations have been reached and the result with the highest relative error is returned line .
programs with multiple floating point input parameters are processed in a similar way but each time we deal with a set of floating point numbers rather than one number.
the algorithm always randomly generates the significand because as revealed by our empirical analysis a randomly generated significand already has a high probability to invoke a large error.
to avoid further complication for our algorithm we use random search for the significands.
we regenerate the significands at every mutation to increase the diversity of the population.
our algorithm also drops the cross over operation in the standard genetic algorithm because we do not find sensible operation to combine two exponents based on their scales.
as a matter of fact this coincides with the design of many existing genetic algorithms which favor mutation over cross over .
we can see that there are three main components of the algorithm generate an initial population select and mutate.
in the following we explain each in detail.
initial population.
we generate nexponents as the initial population based on a uniform distribution but the exponents in the interval has a probability five times as high as other exponents.
number median is the median value of all possible exponents being for doubleprecision and for single precision.
number tis equal to 2dk 2ein our current implementation where kis the number of digits in the exponent part.
to implement this distribution we separate the space of exponents into nintervals where the lengths of the intervals within are five times as small as those outside .
for programs with a single floating point input parameter we randomly generate an exponent within each interval and get ninputs.
for programs with multiple floating point input parameters we randomly pick an interval and generate an exponent within the interval for each parameter.
we repeat the generation ntimes fornsets of parameters.
for each generated exponent we randomly generate the sign bit and the significand to form a floating point number.
then we run the program in fpdebug with each set of input parameters to get the relative error of its output.
selection method.
in the selection step we would like to favor the inputs that lead to a larger error.
a standard selection method for this purpose is roulette wheel selection where the probability of selecting individual iisfi pn j 1fj where fiis the fitness in our case relative error of individual iand nis the total number of individuals.
however this method is not suitable for our case because the probability of hitting a large error is small.
if we have not hit a large error it is likely that in our population there is only a few number of individuals whose errors are slightly larger than the others.
roulette wheel selection cannot select those slightly better individuals because their number is too small.
on the other hand if we really encounter an individual who has several orders of magnitude larger error than other individuals it is unlikely we will choose any other individual.
to overcome this problem we choose the group based rank selection which is more suitable to our case based on existing studies .
we first group the population by their relative errors.
every two individuals in a group have a difference of no more than two orders of magnitude in relative error.
then we select a group by their rank.
we first select the group with highest relative errors with a probability of p. if the first group is not selected we select the group with the second highest relative error with probability of p and etc.
after we have selected a group we pick a random individual from the group.
based on our experience with a few small programs we set p .
mutation operation.
as mentioned before given a floatingpoint number our mutation operation adds a value v can be positive or negative to its exponent flip its sign bit with a probabilityq and randomly regenerate its significand.
after mutation we run the program with the mutated input in fpdebug to get the relative error on the output.
the valuevis decided by a normal distribution n where 2is the length of the interval in the initial population which this exponent falls in.
this setting ensures that the change to the exponent is always small and is even smaller around the median because the initial population is already condensed around the median.
to ensure the sign bit is not frequently changed we set q as0 1in our current implementation.
number of iterations.
since the maximum population size in our algorithm is confined by the number of times that fpdebug can be executed within a time frame it is important to properly balance between the number of initial population denoted as n and the number of mutations denoted as m .
we currently setn min our implementation.
this setting is different from common genetic algorithms where n m. however since our main task is to hit the small interval of exponents that lead to large errors it is important to have a large initial populationtable ii sanity check results newton inv root poly exp cos lsga .8e .2e .1e .7e .0e .2e bgrt .7e .5e .3e .7e .1e .2e to cover the whole space.
v. e valuation a. sanity check we first evaluated our technique on a set of collected test subjects from related work .
these subjects consist of six classic examples of stable and unstable implementations as shown in table ii where the first two programs are stable and the rest are unstable.
stable implementations are likely to produce accurate results than unstable ones.
our technique is able to confirm those stable implementations and find relevant inputs that lead to large errors for the unstable ones.
for each of the two stable programs in the set the maximal relative error detected by our approach is smaller than1 .
for each of the four unstable programs in the set the maximal relative error detected by our approach is larger than .
this result provides a preliminary evidence on the feasibility and usefulness of our approach.
interestingly in parallel to our work chiang et al.
also explore search algorithms for inputs that cause large errors and in their experiments a guided random search algorithm bgrt works best.
we also compared our technique with bgrt on the six subjects.
as we can see from table ii on none of the subjects bgrt outperforms lsga and bgrt also cannot distinguish stable and unstable subjects.
one possible explanation is that guided random research cannot exploit the benefit of the fourth observation.
consequently though bgrt found relatively large errors it could not hit the largest one.
b. experiment overview and research questions to further understand the performance of our approach on real world programs we performed an experimental study using functions from the gnu scientific library.
in our study we experimentally compared our approaches with two standard search techniques a random search rand and a standard genetic algorithm std serving as the control techniques.
to compare the three techniques a limit should be set to terminate the search.
based on our testing executing programs in fpdebug occupies more than .
of the execution time in all three search algorithms.
for convenience we set a limit on the number of times that fpdebug can be invoked by each algorithm.
then we compared the effectiveness of the three techniques on each function under this limit.
in general our experimental study aims to investigate the following two research questions.
the first research question rq1 is concerned with which of the three technique tends to find larger inaccuracies for each experimented function.
the second research question rq2 concerns whether our approach is able to detect potential accuracy problems in practice.table iii subjects with different parameters total para para para para c. experimental setup we conducted our experimental study on a virtual machine running the ubuntu .
.
hosted on a pc with a .3ghz intel pentium i5 2410m cpu and 6gb memory.
subjects our subjects are also chosen from the latest version i.e.
version .
of the gnu scientific library gsl 2as subjects.
gsl is an open source numerical library for c and c programmers.
the library provides a wide range of mathematical routines such as random number generators special functions and least squares fitting.
gsl has been used in previous studies on analyzing programs with floating point numbers.
gsl has in total functions involving intensive floating point computations.
from these functions whose total size is 48k lines of code we chose all the functions with all inputs being floating point numbers and the output being also a floating point number.
there are also three functions where fpdebug is reported to not work properly so we removed the three functions.
as a result we used functions in our experimental study.
each of the functions has up to parameters.
table iii depicts the numbers of functions having and parameters respectively.
all the parameters and the return values of the functions are of double precision.
control techniques as mentioned earlier we use a random search and a standard genetic algorithm as control techniques.
the random search randomly generates every bit of the input parameters at each iteration and returns the maximum relative error in all iterations.
the standard genetic algorithm is designed by configuring the classic framework for genetic algorithm using standard operations .
more concretely the algorithm starts from nrandomly generated inputs as initial generation.
for each generation the algorithms repeatn 2reproduction iterations to produce nchildren for the next generation with each iteration producing two children.
in every reproduction iteration the algorithm picks two individuals from the current generation based on roulette wheel selection.
the weight of each individual is the logarithm of the relative error.
as recommended by the framework computing logarithms is suitable for largely different fitnesses.
then each pair of parameters at the same position in the two individuals are crossed over with a probability c. the crossover is performed by treating the numbers as bit vectors and exchange the first i bits of the two numbers with a randomly generated i. finally every bit of the two individuals are flipped with probability m. after a new generation is produced the process starts with the new generation.
finally the individual with the largest error in all generations is returned.
following the recommendation in existing papers we set n c m .
experimental procedure to answer the first research question for each of the subject function we used each of the three search techniques to calculate the maximum relative error that the technique can find.
the limit of times for invoking fpdebug is which are approximately equal to seconds.
in other words random search will generate inputs the standard genetic algorithm will have generations with individuals each and our algorithm will have initial individuals and mutations.
there are mainly two reasons for us to use a relatively short iteration limit.
of course more experiments are needed to further study the practical iteration limit for each of the techniques.
first all the subject functions are library functions which are building blocks for real world software.
as a result the execution time of one test input for a real world program can be hundreds or even thousands of times of that for a subject function in our experimental study.
thus a small iteration limit for our subjects may be equivalent to a long time execution for real world programs.
in other words only techniques that can achieve satisfactory results on our subjects in a short iteration limit would have more practical value for real world programs.
second as the number of subject functions in our experimental study is large a relatively small iteration limit would help us control our experimental procedure.
note that each technique may need to be executed many times against each subject function due to calibration.
the second research question is difficult to answer because it is difficult to decide whether a large error is a problem or not.
large errors may be fundamentally inevitable in many computations and are not considered problems.
interestingly many gsl functions report an estimated absolute error for each execution.
using this information we conservatively deem a large relative error as a potential problem when the actual absolute error is times larger than the estimated one as the large error is probably unexpected by the developer and may cause serious consequences.
for the generated test inputs that trigger an error larger than we invoke the associated functions with these inputs in fpdebug and compare the estimated absolute error with the actual error reported by fpdebug.
when the actual error is times larger than the estimated one we consider it a potential problem.
d. threats to validity the main threat to the internal validity lies in the possible faults in our implementation.
to reduce this threat we reviewed all our source code before conducting the experiments.
note that as benz et al.
have made their tool publicly downloadable we implemented the three techniques by directly invoking their tool helping us further reduce this threat.
the main threat to external validity is concerned with the representativeness of our subjects.
to reduce this threat we used a large number of widely used functions which have also been used in previous studies as subjects in our study.
conducting more experiments using more real world programs as subjects would help us further reduce these threats.table iv maximum inaccuracies detected total rand std lsga tied table v sign test on inaccuracy detection n n n p lsga vs. rand .14e lsga vs. std .46e std vs. rand .52e the main threat to construct validity is the limit of times that fpdebug can be invoked.
to reduce this threat we used a short limit to make the experimented techniques applicable for real world programs whose execution time may be much larger than that of our subjects.
note that a technique that can detect inaccuracy in a short time would become more effective or at least as effective when used under a long time limit.
e. experimental results in this subsection we present the experimental results for the two research questions.
all our experimental data are online.
rq1 effectiveness of inaccuracy detection given a subject denoted as s and a technique denoted as t if the maximum relative error returned by the technique is larger than both the two maximum relative errors returned by the other two techniques we deem that tis the best technique fors.
therefore for each of three techniques we count the number of subjects for which the technique is the best.
for some subjects no single technique is superior to both the other two we deem that no technique is the best and denoted this situation as a tie.
the result of this comparison is depicted in table iv.
as we can see from the table lsga finds the maximum errors in the majority of the subjects while random search finds the maximum errors in the least number of subjects.
this results indicates that lsga are more effective than the other two algorithms while standard genetic algorithm is better than random search.
to further confirm whether the differences between the three techniques are statistically significant we perform the sign test on each pair of techniques.
the result of our sign test on inaccuracy detection is depicted in table v. from the table we can see that the differences between each pair of techniques are significant as pis much smaller than .
the usual threshold for significant difference.
in particular lsga has a very smallpwhen compared with the other two techniques which indicate that lsga has absolute superiority over the other two techniques.
the sign test only considers which technique performs better for each subject but does not consider whether the differences of relative errors found by different techniques are significant.
if the relative errors found by two techniques are very close both techniques are usable in practice.
we deem that the difference between the two relative errors e1ande2is significant if vi significantly larger inaccuracies detected left right lsga vs. rand lsga vs. std std vs. rand e1 e2ore2 e1is larger than .
then we calculate for each pair of techniques how many significantly larger errors one techniques found over the other.
the result is shown in table vi.
the left column shows how many significantly larger inaccuracies the left technique found than the right.
similarly the right column shows how many significantly larger inaccuracies the right technique found than the left.
as we can see from the table lsga found significantly larger errors than the other two techniques in a large number of subjects while the other two techniques found significantly larger errors than lsga only in rare cases.
although random search performed the worst among the three techniques the above result also suggests that random search still found relatively large errors on some subjects.
to further understand why this happened we analyze a sample function where random search returns a large error.
this function is gsl sf bessel j1 which solves spherical bessel function j1 x sin x x2 cos x x for this function random search reaches the maximum relative error of .089913e a fairly large error.
the implementation code of this function is listed below int gsl sf bessel j1 e const double x gsl sf result result f double ax fabs x check pointer result if x .
f result val .
result err .
return gsl success g else if ax .
gsl dbl min f underflow error result g else if ax .
f const double y x x const double c1 .
.
const double c2 .
.
const double c3 .
.
const double c4 .
.
const double c5 .
.
const double sum .
y c1 y c2 y c3 y c4 y c5 result val x .
sum result err .
gsl dbl epsilon fabs result val return gsl success g elsef gsl sf result cos result gsl sf result sin result const int stat cos gsl sf cos e x cos result const int stat sin gsl sf sin e x sin result const double cos x cos result.val const double sin x sin result.val result val sin x x cos x x result err fabs sin result.err x fabs cos result.err fabs x result err .
gsl dbl epsilon fabs sin x x x fabs cos x x result err .
gsl dbl epsilon fabs result val return gsl error select 2 stat cos stat sin g g in the above code gsl dbl min is about .22507e .
this function divides the input space into four segments and returns a constant number line an underflow error line the value calculated by series expansion lines and the value calculated by standard library functions lines .
since the program has no loop the only possibility of producing such a large error is to subtract two similar numbers known as cancellation .
cancellation can happen at addition and subtraction which exist on line and line .
in the case of line because the path condition line is jxj y and eachciproduced between lines and will be largely different on scale and we will not subtract two similar values.
on the other hand large error may be triggered on line e.g.
whenxis a large number and cos x happens to be near zero.
as a result the probability of producing a large error for a random input is decided by the probability of executing line and the probability of producing a large error when line is executed.
we obtain the former by analyzing the path condition and the latter by sampling.
the path condition of line is jxj and of all double precision floating point numbers fall in this range.
we then randomly created test inputs satisfying jxj and of them generated a relative error larger than .
putting the two probabilities together .
of random inputs will trigger an extremely large error.
it is very easy for random search to locate an input within this range.
since there were test inputs created in our experiment there is a probability of .
to trigger a large error.
the analysis of this sample function explains why random search can find large errors in some cases there exist subjects for which very significant inaccuracies can be easily triggered by chance.
nevertheless our results also suggest most programs do not belong to this category and metaheuristic search based approaches would be useful in locating large errors in these programs.
rq2 ability to identify potential problems in the previous experiment our algorithm generated test inputs for functions where the relative error is larger than .
we invoked these functions with the generated test inputs and functions returned estimated absolute errors together with the result.
by comparing the actual and estimated absolute errors we found functions that have potential problems of inaccuracy.
this result again outperforms the other two algorithms significantly where the std found seven potential problems and rand found five.
the detailed result about the functions is shown in table vii.
each line is a potential problem our approach identifies.
the first column lists the function names the secondtable vii functions with potential bugs name relative errorestimated absolute errorreported absolute error airy aideriv .54e .04e .35e airy aideriv scaled .54e .04e .35e clausen .52e .37e .31e eta .58e .27e .71e exprel .85e .44e .41e gamma .07e .94e .05e synchrotron .35e .47e .07e synchrotron .67e .39e .86e zeta .58e .41e .19e zetam1 .42e .51e .42e bessel knu .08e .33e .05e bessel knu scaled .08e .66e .05e beta .21e .91e .04e ellint e .92e .58e .14e ellint f .79e .86e .64e gamma incq .36e .88e .25e hyperg 0f1 .80e .08e .33e hyperg 2f0 .35e .20e .19e column lists the relative errors reported by fpdebug the third column lists the estimated absolute errors reported by the subject functions and the last column lists the absolute errors reported by fpdebug.
we can see that in most cases the actual errors are many orders of magnitudes larger than the estimated ones indicating potentially serious problems in practice.
by searching the gsl repository we found that none of problems in the functions has been reported as bugs while there exist bug reports reporting relative errors in a few orders of magnitude.
we have submitted bug reports for the identified functions but have not received any feedback yet possibly due to the fact that there is only one programmer actively maintaining gsl now .
vi.
l imitations and future work first our approach relies on fpdebug to detect the accuracy of the output and fpdebug detects the accuracy through promoting the precision of floating point numbers.
for example if the original program uses single precision numbers fpdebug will use double precision numbers to perform the same computation and compare the results to get the relative error.
in theory this approach may not always produce the correct result because precision specific treatments may be used in programs.
for example the original program may predict some large error for the precision it used and add to the result a pre defined value to compensate the error or the original program may use bit operators to accelerate the computation which usually works only for a certain precision.
raising precision on these programs may not lead to more accurate results.
as a result the inaccuracies reported by our approach are only indications of potential accuracy problems and are not guaranteed to be bugs.
however this probably is not a problem in practice.
first programmers are able to identify the false positives easily as they know whether any precision specific treatment is used in their programs.
second precision specific treatments are not very commonly used inpractice.
as a matter of fact precision adjustment has been used in different approaches and no problem is reported as far as we know.
second as our approach is based on testing we cannot guarantee the inaccuracy detected by our approach to be always the maximum inaccuracy the program under test can produce.
similarly when there are several inputs in the input domain of the program under test to trigger significant inaccuracy our approach may find only one of them.
note that when there are several independent inaccuracy related faults it is very likely that these faults lead to multiple unrelated inaccuracyinducing inputs.
in fact as we do not know the maximum inaccuracy of the program under test we also do not know how close the inaccuracy produced by our approach is to the maximum inaccuracy.
in future work we need to establish some benchmarks of maximum inaccuracy through exhaustive search and use the maximum inaccuracy to evaluate our approach.
furthermore we should also investigate the injection of inaccuracy faults to create subject programs with controllable inaccuracy.
third although fpdebug provided by benz et al.
needs to instrument the binary code of the program under test our approach itself is in principle a black box approach.
that is to say our approach does not analyze the code of the program under test but completely relies on the results produced by fpdebug.
this strategy should be suitable for programs with simple control structures like the function discussed in section iii.
but there are also programs with both complex control structures and intensive floating point computations.
in future work we plan to investigate approaches that can also utilize information e.g.
coverage information obtained from code analysis.
one possible benefit of using coverage information is that as collecting coverage information is much cheaper than executing benz et al.
s tool using coverage information to avoid always executing benz et al.
s tool could make our approach more efficient.
another possible benefit of using coverage information is that coverage information may guide our approach to explore more paths to find more than one independent inaccuracy related faults.
fourth our approach is currently applicable to only floatingpoint parameters.
if the function has other type of parameters the user has to specify pre defined values for them.
in future work we plan to investigate approaches that can also deal with other types of parameters.
one possible way is to consider suitable representations of other types of parameters in our genetic algorithm.
thus our genetic algorithm can be naturally extended to these types.
another possible way is to use different heuristics e.g.
a heuristic based on coverage for other types to consider that they may play different roles from floating point numbers in programs with intensive floating point calculation.
finally it would be interesting to compare our approach with static analysis approaches that give an upper bound of errors.
however it is hard to be done at the current stage because we do not know the maximum possible errors of our subjects.
furthermore to the best of our knowledge there exists no nontrivial benchmark with precious bounds of errors.
when astatic analysis produces a large upper bound while our approach finds only a small error we cannot decide which approach performs better.
in future work benchmarks of floating point error can be developed so that different approaches can be compared.
vii.
r elated work static analysis.
many approaches have been proposed to statically analyze the possible upper bound of errors.
these approaches are typically built upon interval arithmetic or affine arithmetic .
interval arithmetic presents each number as a pair of a lower bound and a upper bound of values and replaces basic arithmetic operations as operations between intervals.
for example adding two intervals resulting in another interval presenting the maximally possible range of the result .
affine arithmetic enhances interval arithmetic by distinguishing errors coming from different sources.
in affine arithmetic each number is represented as an affine v x1 x2 wherevis the precise value xiis the error coming from a source and iis a symbol representing the source.
by differentiating errors by their sources affine arithmetic can produce better result in operations such as n n where the operands contain errors from the same sources.
typical static analysis approaches replace the numbers and operations in the target program in their interval affine form and use standard program analysis techniques such as symbolic execution and data flow analysis to obtain the possible errors on the output.
for example putot et al.
present a static analysis that relies on abstract interpretation by interval form.
goubault and martel also propose an approach based on affine arithmetic to analyze nontrivial numerical computations.
however these approaches usually consider the possible errors for the whole input space and cannot identify which input could produce a large error.
furthermore due to the nature of static analysis a large interval on the output does not guarantee the existence of a large error.
static verification.
another branch of approaches try to verify the precision of floating point programs.
given a property about the floating point accuracy these approaches try to verify automatically or interactively whether the property holds in a program.
boldo et al.
build support for floatingpoint c programs in coq allowing one to easily build proofs for floating point c program.
ayad and march e propose the use of multiple provers to try to automatically generate proofs of floating point properties.
darulova and kuncak propose a type system to guarantee the precision of floatingpoint programs.
however the verification based approaches suffer from the same problem as static analysis failing to prove a properties does not necessarily implies the existence of a large error and no input could be provided for further debugging.
precision tuning.
fpdebug the key component enabling our approach dynamically analyzes the program by performing all floating point computation side by side in higher precision.
the difference between the standard result and the result inhigh precision is the error of the result.
bao and zhang proposes to reduce the cost of detection by not computing the precise error but marking and tracking potentially inaccurate values.
based on similar ideas of tuning precision lam et al.
rubio gonz alez and schkufza et al.
propose approaches that automatically reduce the precision of floatingpoint computation to enhance performance with an acceptable loss of precision.
these approaches serve as evidences that changing precision is a feasible technique for various purposes.
external errors.
our approach focuses on internal errors which is about how much inaccuracy may be introduced during the computation of the program.
external errors are errors from the sources outside the scope of the program.
such external errors in the input may be magnified during the computation of the program and result in significant inaccurate output.
different approaches have been proposed to analyze how robust a program is under an inaccurate input.
recent work include static verification of robustness by chaudhuri et al.
dynamic analysis by tang et al.
and dynamic sampling by bao et al.
.
however these approaches cannot be used for internal errors as they are concerned with the execution of the subject program but not the precise output.
test generation for floating point programs.
test input generation is an important research topic and has been approached in different angles .
a typical approach is to use symbolic execution to explore different paths and generate test inputs by solving path constraints.
however constraints with floating point operations are usually difficult to solve.
miller and spooner first propose the use of search based techniques instead of symbolic execution to generate test input data.
recently bagnara et al.
propose to use several search heuristics to enhance constraint solving in concolic testing for floating point programs.
however their goal of test generation is to increase path coverage but not to detect floating point inaccuracies.
as far as we are aware our approach is the first test generation approach to the detection of floating point inaccuracies.
besides test input test oracle generation is also an important problem in test automation.
recently zhang et al.
propose to infer metamorphic relations for regression testing.
however their approach only works for regression testing but not for initial testing.
viii.
c onclusion in this paper we have shown that with the recent advance in the dynamic analysis of floating point errors it has become possible for search based test generation aiming at maximizing the likely errors.
by exploiting the statistical characteristics of large floating point inaccuracies we have designed a specialized genetic algorithm in order to efficiently search for large inaccuracies in numerical programs.
our experimental results demonstrate that our approach is able to find many large inaccuracies in a widely used library indicating the proneness of numerical programs to large inaccuracies in practice.