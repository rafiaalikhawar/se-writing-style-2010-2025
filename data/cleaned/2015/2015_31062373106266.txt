descry reproducing system level concurrency failures tingting yu university of kentucky lexington ky tyu cs.uky.edutarannum s. zaman university of kentucky lexington ky tarannum.zaman uky.educhao wang university of southern california los angeles ca wang626 usc.edu abstract concurrent systems may fail in the field due to various elusive faults such as race conditions.
reproducing such failures is hard because concurrency failures at the system level often involve multiple processes or event handlers e.g.
software signals which cannot be handled by existing tools for reproducing intra process thread level failures detailed field data such as user input file content and interleaving schedule may not be available to developers and the debugging environment may differ from the deployed environment which further complicates failure reproduction.
to address these problems we present descry the first fully automated tool for reproducing system level concurrency failures based only on default log messages collected from the field.
descry uses a combination of static anddynamic analysis techniques together with symbolic execution to synthesize both the failure inducing data input and the interleaving schedule and leverages them to deterministically replay the failed execution using existing virtual platforms.
we have evaluated descry on realworld multi process linux applications with a total of lines of code to demonstrate both its effectiveness and its efficiency in reproducing failures that no other tool can reproduce.
ccs concepts software and its engineering software testing and debugging keywords multi process applications debugging concurrency failures failure reproduction acm reference format tingting yu tarannum s. zaman and chao wang.
.
descry reproducing system level concurrency failures .
in proceedings of esec fse paderborn germany september pages.
introduction the ever increasing parallelism in computer systems has made software more prone to concurrency failures causing problems not only during the development but also after deployment.
when concurrency failures occur in a deployed system developers often have to diagnose them in a different debugging environment to identify the root causes.
toward this end an important step is to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany association for computing machinery.
acm isbn .
.
.
.
the failure in a timely manner.
however this is challenging due to the limited data generated by production runs.
typically field data are transferred from customers to developers.
however if they belong to different organizations customers may not be willing to share their inputs and file contents involved in the failed execution .
there are pre deployment debugging techniques which leverage fine grained logging for deterministic record andreplay although effective in testing they are ill suited for deployment because of the often unbearable performance overhead.
thus for the purpose of reproducing failures in production runs we have to assume that the only available field data are default log messages generated by the unmodified application.
under this assumption we propose descry debugging systemlevel concu rrencyfailures the first fully automated tool for reproducing failures using only default logs collected from the field.
this is a challenging task because in practice log messages are often sparsely printed such that there may be thousands of program paths leading to the same message.
descry focuses on inter process bugs where multiple operating system components e.g.
processes software signals and interrupts incorrectly shared resources.
they differ from intra process thread level bugs which are the focus of many prior work on reproducing concurrency failures.
the difference is that an intra process thread level concurrency bug often corrupts only volatile memory within a process whereas an interprocess system level concurrency bug is more dangerous since it corrupts the persistent storage and other system wide resources thus potentially crashing the entire system.
as laadan et al.
noted more than of the race conditions reported in popular linux distributions were process level races which could not be reproduced by existing thread level debugging tools .
ideally reproducing a system level concurrency failure requires the availability of all input data the entire interleaved execution and the same execution environment as in the deployed system.
however as we have mentioned earlier they do not exist in practice.
therefore descry only assumes the existence of the source code of processes under debugging puds and default logs generated by the failed execution.
internally descry leverages a combination of static and dynamic program analysis techniques as well as symbolic execution to compute the failure inducing data input and interleaving schedule.
as such it shifts the operational cost from the customer side to the developer side thus avoiding the overhead of fine grained logging and heavy weight code instrumentation in production runs this is the differentiating feature of descry compared to existing techniques.
figure provides an overview of descry.
first it leverages the logs to identify processes that are relevant to the observed failure i.e.
puds .
next it uses static program analysis to connect the log messages with statements in the source code of the puds i.e.
logging points that print these messages.
in the third step descry uses symbolic execution to generate failure inducing data inputs of the puds which steer the execution through these logging points.
esec fse september paderborn germany tingting yu tarannum s. zaman and chao wang p1 p2 ... fieldlogs p1 p2 d file test inputs schedulelog analyzerpudidentifierinputgeneratorschedulegeneratorin housep1 p2 ...goal lists tr ace figure the overview of our descry framework.
to improve scalability it limits the program state space using summaries of logging points and skipping irrelevant code.
descry also constructs a prediction model based on the partial order of observed inter process operations to compute a failure inducing interleaving schedule.
finally with the new data inputs and interleaving schedule descry controls the puds to deterministically replay the failure.
descry has been implemented as a software tool using the llvm compiler front end the klee symbolic virtual machine and the simics virtual platform for deterministic replay.
the tool can directly handle multi process applications written in c c .
to evaluate descry we conducted experiments on popular linux applications with a total of lines of code and known concurrency failures.
the logs used for our evaluation are real and generated by running the applications with their default production settings.
our experimental results show that descry can successfully reproduce of the known failures which is significant because no other tool can reproduce this many failures.
furthermore the two remaining failures could have been successfully reproduced by descry if limitations in the implementation of the underlying klee were to be removed.
finally the time taken by descry to compute the failure inducing data inputs and interleaving schedule is typically a few minutes indicating that the proposed method is efficient for practical use.
in summary this paper makes the following contributions we propose descry the first fully automated method for deterministic reproducing failures of multi process applications using only default logs generated by these applications.
we implement descry and conduct an empirical study to demonstrate its effectiveness and efficiency on real world applications.
the remainder of this paper is organized as follows.
first we use examples to illustrate the main technical challenges and our corresponding solutions in section .
then we present our detailed algorithms in sections and .
next we present our experimental evaluation in section .
finally we review related work in section and give our conclusions in section .
motivation and background in this section we use examples to illustrate the challenges in reproducing concurrency failures and then formally define our problem.
.
motivating example figure shows a real race condition between two linux applications updatedb and tail .updatedb is a script that spawns multiple application processes e.g.
by executing mv a.txt.backup a.txt to recover a file.
the race bugzilla occurs when updatedbmodifies the file name in the database and tail displays the file content to standard output.
for example while the command tail f a.txt is monitoring the file a.txt to output its last lines a user may execute updatedb to spawn mv a.txt.backup a.txt .
as a result the tail program finds that a.txt is missing.
the reason is that updatedb fails to ensure that mv a.txt.backup a.txt is executed atomically.
by atomically we mean that there should be no point in time when a.txt does not exist.
however in the actual implementation mvfirst unlinks the target file line if it has multiple hard links e.g.
created by ln .
iftail runs concurrently with mv and accesses the target file line after unlink line but before rename line it will find the target file missing thereby triggering the error tail no such file or directory.
.
this bug can be fixed by removing the condition check that triggers the unlink line of figure .
however in practice when reporting the bug to developers the customer only submitted the default log messages produced by the application as shown in the second column of figure .
such log messages are fairly common in practice but not really helpful to developers who try to reproduce the failure in the debugging environment.
first this bug involves a total of processes e.g.
updatedb mv frcode spawned by the updatedb together with the tail despite the fact that only mvand tail need to be analyzed for diagnosing the failure.
second there are a total of messages displayed in the default log file of which messages are from tail alone manually sifting through all these log messages would be time consuming.
more importantly no information of the failure inducing data inputs or interleaving schedule is provided.
the first challenge in reproducing such failure is to identify the processes that are responsible for the failure.
for instance among the processes involved in the erroneous execution only the mv and tail processes are actually relevant.
therefore we need a method to quickly weed out the irrelevant processes.
moreover the failing process might not be the process that contains the bug.
for example although tail triggers the failure that contains the bug the buggy code is actually in mv.
to decide whether a process is buggy we need to match the output messages back to the source code that print these messages.
however since many log messages are irrelevant processing all log messages would be inefficient.
the second challenge is that most of the time concurrency failures are triggered by specific combinations of data inputs.
since the total number of possible combinations can be astronomically large it is extremely difficult for random stress testing to trigger these failures using randomly generating inputs.
for the mvand tail example in figure even if the file name a.txt is known from the log messages it may not be part of the failure inducing inputs unless the file also has hard links to other files.
695descry reproducing system level concurrency failures esec fse september paderborn germany .
main arc argv .
if dest is dir c1 .
free new dest .
else .
ok movefile source dest x .
return ok .
.
movefile char src name char dst name ... .
if x backup type !
no backups ... c2 .
if rename dst name dst backup !
c3 .
error errno cannot backup s dst name lp1 .
return false .
.
else if !
s isdir dst sb .
st mode ... c4 x pre serve links dst sb.st nlink .
printf hard links s n dst name lp2 msg1 .
if unlink dst name !
c5 .
error errno cannot remove s dst name lp3 .
return false .
.
.
if !
s isdir src mode c6 .
emit src name dst name ... .
.if x move mode c7 .
rename src name dst name .
emit char src char dst ... .
printf s s quote n src quote n dst lp4 msg2 .
if backup dst name c8 .
printf backup s backup dst name lp5 .
.
int main .
if forever f option c9 .
tail forever inotify wd f ... .
.
void tail forever inotify int wd struct file f ... .
while .
if follow mode follow name c10 .
recheck f false .
.
.void recheck f ... .
int fd open f name o rdonly .
if fd c11 .
error errno s lp6 msg99 .
else .
printf output s f msg lp7 msg1 msg2 ... .
figure code snippet showing the race condition caused by interleaved execution of mv top and tail bottom .
the third challenge in reproducing system level concurrency failures is the need to analyze the interleaving schedule across multiple processes.
in figure for example an inter process operation is the system call open made by tail which must occur after the system call unlink but before the system call rename made by mv.
enforcing such execution order requires modeling of the systemlevel happens before relations and controlling the kernel scheduler which cannot be accomplished by existing methods focused only on intra process thread level race conditions.
.
problem statement we define the failure production problem as follows.
given the source code of a set of processes under debugging puds and default logs generated by these puds in a failed execution compute the data inputs for these puds and their interleaving schedule such that the failure can be deterministically reproduced.
loggoal listpathconditiondata inputupdatedb frcode ...mv frcode ... ...mv hard links a.txt mv a.txt.backup a.txt list1 lp2 lp4 !c1 !c2 !c3 c4 !c5 c6 c7 !c8.
mva.txt.backupa.txttailtail output hello1 tail output hello2 tail ...tail nosuch fileordirectory list1 lp7 lp6 1stiteration c9 c10 !c11 c112nditeration c9 c10 !c11.
tail f a.txtfigure logs path conditions and inputs for mvandtail.
we assume that a concurrent system consists of a set of processes p1.
.
.pm and a set of software signals s1.
.
.sn .
each process may create multiple threads but for ease of presentation we focus only on the process level concurrency in this work while assuming each process has one thread.
a failing process pfis a process that behaves erroneously manifested by the failure messages it prints.
a failure point is a program statement that prints a failure message.
logs are sequences of normal or error messages that the application prints to files and consoles.
in this context each entry in the log is a log message .
to be realistic we assume log messages are uninterpreted plaintext strings.
each program statement capable of printing a log message is called a logging point lp .
there are two types of logging points with respect to the failure relevant logging points rlps and irrelevant logging points ilps .rlps are the ones that may print log messages relevant to the failure ilps are the ones that can never print log messages relevant to the failure.
therefore only rlps need be used to synthesize the failureinducing data input.
for example in the mvprogram of figure lp2 andlp4arerlps since they may print log messages for the failed execution whereas lp1 lp3 and lp5areilps since they can never print log messages relevant to the failed execution.
we assume that logs are generated by applications during production runs and when a failure occurs they are transferred from the customer to the developer.
this is a realistic assumption because logging for critical events is a common and important software engineering practice .
there are quantitative evidence that logging is pervasive during software development and is actively maintained by developers.
a system level concurrency fault occurs when multiple processes signals or interrupts access a system wide resource e.g.
file device without proper synchronization .
such resources are often accessed through system calls.
thus handling systemlevel concurrency fault requires the modeling of read write effects and synchronization operations involving system calls.
for example the lstat system call on file freads the metadata of f. the clone system call creates a new process inode under the proc directory write .
synchronization operations control process interactions through kernel process scheduler.
common process level synchronization primitives include fork wait exit pipe and signal .
the second column of figure shows an example interleaving schedule for running the mvandtail processes.
each event in the schedule is either a shared resource access r denotes read and w denotes write or a synchronization operation.
details of shared resource and event modeling can be found in prior work .
however note that system call modeling in descry is different from that in symbolic execution tools e.g.
klee .
for example klee models the system calls to generate the required program constraints for achieving high coverage whereas in descry the systems calls are modeled as concurrency events.
696esec fse september paderborn germany tingting yu tarannum s. zaman and chao wang arbitrary scheduleeventspermutatedschedule1.
mv stat a.txt r a.txt .
mv stat a.txt .
mv lstat a.txt.backup r a.txt.backup .
mv lstat a.txt.backup .
mv lstat a.txt r a.txt .
mv lstat a.txt .
mv stat a.txt r a.txt .
mv stat a.txt .
mv access a.txt r a.txt .
mv access a.txt .
mv unlink a.txt w a.txt .
mv unlink a.txt .
mv rename a.txt.backup a.txt w a.txt.backup w a.txt .
tail open a.txt r only .
tail open a.txt r only r a.txt .
mv rename a.txt.backup a.txt figure example execution trace for mvandtail descry algorithm inputs pall lo s outputs t s begin puds identifypud pall foreachpi puds lpi getlpoints lo s pi gi computegoals lpi pi endfor while time t i me max foreachpi puds ifexecuteconcrete t pi does not lead to l gi t guidedsymbolicexecution gi pi endif e getexecutiontrace t s schedulegeneration e ifreplay t s is successful return t s endif endfor .
endwhile end figure the overall algorithm of descry.
the descry approach the overall algorithm of descry is shown in figure .
the input of this algorithm includes the set of running processes pallwhen the failure occurs as well as the logs.
the output is a tuple t s where tis the failure inducing data input and sis the failure inducing interleaving schedule.
in the remainder of this section we explain each step of the algorithm using the example of figure .
details of the algorithm are described in sections .
given palland the logs we first identify the failing process pf palland the subset l pallof processes potentially interacting with pf line .
processes in l pf are called the processes under debugging puds .
for example in figure among all processes spawned by updatedb andtail only mvandtail are the puds.
next we invoke getlpoints to obtain the logging points line .
to handle large data and improve the scalability of subsequent procedures for input and schedule generations we first remove therepeated log messages and then map the remaining messages to logging points in the program.
next we use the subroutine computegoals line to connect logging points of each pud to form a set of logging sequences denoted by gi which subsequently will be explored during symbolic execution.
in the example of figure descry would map all messages to their logging points msg1ofmvis mapped to lp2 msg2ofmv is mapped to lp4 msg1tomsg98oftail are mapped to lp7 and msg99oftail is mapped to lp6 which is the failure point.
all other logging points in the program are considered to be irrelevant.
in the end mvandtail each has a goal list lp2 lp4 formvand lp7 lp6 fortail as shown in column of figure .
next descry computes the failure inducing data input t for each individual pud to exercise at least one of its goal lists in gi line .
this is accomplished by a customized symbolic executionprocedure on each pud that uses the logging sequences in gias guidance.
to reduce the computational overhead we propose three optimization techniques the seeding of concrete inputs the pruning of irrelevant states and the prioritization of program paths.
these optimizations are made possible by leveraging results of our static log analysis see section .
column of figure shows the path conditions computed by our symbolic execution procedure.
they are used to compute the data input of mv a.txt anda.txt.backup where a.txt has a hard symbolic link .
however symbolic execution fails to compute the data input of tail because the first path connecting lp7tolp6is infeasible.
therefore descry relaxes the goals in tail by replacing the failure point lp6with a failure predicate at line to allow the exploration of more paths.
the rationale is that the diverged goal may still be reached through a different path if the control flow of the pud is changed under a different event interleaving schedule.
when the goal list is changed to lp6 for example our symbolic execution procedure is able to compute the desired input f a.txt .
finally we compute the failure inducing interleaving schedule s lines .
toward this end descry first executes all puds concretely under their new inputs while following an arbitrary interleaving schedule.
if the resulting trace denoted by e triggers the failure we are done.
otherwise we systematically explore alternative interleavings of the inter process operations in ein order to trigger the failure.
if no such interleaving exists descry backtracks and uses symbolic execution to compute a set of new data inputs until the failure inducing schedule is successfully reproduced lines .
for example assume the trace in figure the first column was generated by mvandtail in figure following an arbitrary schedule.
since it does not trigger the failure descry systematically permutes events of the trace to generate a failure inducing schedule.
after swapping the two events mv rename tail open we have found the failure inducing event interleaving schedule.
note that the data inputs generated from different puds may have different names but need to point to the same shared resources.
for example tail may generate an input file called b.txt whose name is different from a.txt generated from mv.
in this case the two file names must be unified to reproduce the failure.
descry records a list of system calls that access the data input for each pud.
if both lists in the pair of puds are non empty the data input is a shared resource between the puds and thus the file names are unified.
static analysis of log messages the first step is to identify relevant processes puds from pall since there can be tens or hundreds of active processes when the failure occurs in pf not all of which are puds.
we consider a process pas a pud only in one of the following scenarios paccepts input with the same type as the failed process pf e.g.
mvandtail both accept files as input pfandpare different instances of the same program e.g.
multiple bash processes running concurrently pfandpare different processes spawned by an application e.g.
a mutt mail client process tries to open an ms word attachment process pis a software signal within the process pf.
697descry reproducing system level concurrency failures esec fse september paderborn germany the last three scenarios are automatically identified by descry.
we assume active processes are captured by system built in tools such as the linux auditd daemon .
the first scenario may require user intervention e.g.
to specify the type of input of a process if it has not yet been specified before.
this is the only manual step potentially needed by descry.
in practice there is no technical difficulty in this because developers who use descry should know what type of input files strings or numbers that a process accepts.
for the example in figure descry would examine the system log directory var log that contains process names to identify active processes shown in column of figure where tail is the failed process.
since the type of input accepted by mvis the same as that of tail i.e.
file they are selected as the puds.
note that under the default system environment var log record system logs that do not involve program inputs from the user space.
as such descry does not assume these inputs are available.
.
identifying the logging points the next step is to identify the logging points both rlps and ilps that print normal and error messages.
this is extremely challenging for real applications because they often use customized logging facilities as opposed to simple printf statements.
for example in figure the error call at line does not contain the string no such file or directory observed in the error message but calls an error handling function named strerrno to construct and then print the string.
because of this reason mapping log messages to logging points alone is a challenging problem and has been studied by the systems community extensively e.g.
sherlog .
however unlike sherlog which relies on the user to specify log patterns before it can identify the logging points descry does it automatically by performing an inter procedural static analysis of each pud and its libraries e.g.
strerror.c to match log messages against the source code.
it first identifies the set of program statements inside puds and libraries involving the printing apis denoted by fp such as printf andsprintf and records the set strof format strings passed as parameters to functions in fp.
a format string is determined by extracting the string within the quotes of the api while excluding the the format specifiers e.g.
s and escape characters.
for instance hard links at line of figure is a format string passed to printf .
for each log message in the logs if its substring is identical to a string str str the corresponding function in fpis identified as a logging point.
if the failure point fp fpexists in a library file descry traces fpback to the pud source code through the data and control flow edges of the inter procedural cfg the source code location is then identified as a logging point e.g.
line of figure .
it is possible that the logging points are over approximate although it does not occur often.
if a log message comes from a dynamically assembled string as opposed to a string constant it will not be identified statically.
the subsequent symbolic execution will have to be an un guided search.
.
constructing the goal lists we construct the goal lists by connecting relevant logging points of the pud in a log hierarchy graph lhg which describes the partial order of the logging points.
a lhg is a directed acyclic graph where nodes correspond to logging points and the hierarchy levels correspond to the order in which they appear in the log the entrylm1 1lm1 2lm1 n...lm2 1lm2 ... entrylm1 1lm1 2lm1 n...lm2 1lm2 2post dominated by an irrelevant point ...figure examples for illustrating the log hierarchy graph lhg .
first log message is at the top level.
for each message id m the set of logging points associated with mis lm i where lm iis the logging point for mat the i th program statement.
each logging point lm iindicates an unique program statement.
however a logging point may associate with multiple log messages having different ids but the same format strings due to loop iterations.
in the example of figure there are messages mapped to lp7.
descry considers such messages to be the same and thus unifies their message ids.
specifically these messages are renamed tomsgabecause they have the same format strings.
in other words we mitigate the cost of goal list construction by ignoring loops.
although in theory it may lead to unsuccessful failure reproduction e.g.
when a failure can only be triggered after going through a loop a fixed number of times such cases are rare in practice as shown in our experimental evaluation.
furthermore we mitigate the problem during our subsequent symbolic execution step by iteratively increasing the number of loop iterations to eliminate unsuccessful failure reproduction should it appear.
a goal list denoted by pp connects the logging points in the lhg.
figure left shows an example lhg where each vertical path corresponds to a goal list.
since enumerating all goal lists takes o sk where sis the number of logging points associated with each log message and kis the length it can become too expensive in practice.
fortunately not all goal lists correspond to a feasible execution.
in descry we use a lightweight static program analysis to prune away the infeasible goal lists.
toward this end we decompose the problem of searching for a complete lhg path into subproblems of searching for segments of the path.
more specifically descry iterates through all lhg nodes at two neighboring lhg levels iandj and for each edge lmi i lmj j checks if lmj jis reachable from lmi iand their pre and post dominators do not contain irrelevant logging points.
if the first condition is not met we remove lmi i lmj j because paths traversing this edge are infeasible.
if the second condition is not met we remove the node lmj jand all its associated incoming and outgoing edges and we do not involve lmj jin the next iteration.
this is because paths traversing lmj jcontain irrelevant points and thus must not have been exercised for triggering the failure.
only when both conditions are met lmj jis considered as a new goal after lmi i. applying our new method to the example of figure right will remove the edge lm1 lm2 because lm2 2is not reachable from lm1 the first condition .
it will also remove lm1 2and the edge entry lm1 because lm1 2dominates an irrelevant log message the second condition .
thus the final goal lists will be and .
698esec fse september paderborn germany tingting yu tarannum s. zaman and chao wang guided symbolic execution we propose a new symbolic execution procedure to compute failureinducing data inputs of all puds in a multi process application.
this step differs from prior work on testing concurrent software using symbolic execution in that our method is designed for multi process applications whereas prior works all focus on a single process.
internally we leverage klee to conduct a goal directed exploration of the puds to traverse program statements specified in the goal list.
as shown in line of figure our algorithm takes each pud piand the set of goal lists gias input and returns t as output.
here t is the data input that forces pito go through a goal list in l gi.
let oal lbe the current goal and stateset be the set of program states of pi.
at each step of the symbolic execution of pi we select a state si stateset that is more likely to reach oal.
if no state in stateset can reach oal we check if oalis a program statement in a loop.
if oalis in a loop we increase the number of loop iterations by a fixed number n in our experiments and try again until reaching the loop bound lmax.
this will increase our chance of reaching the goal.
if oalcannot be reached in this way we backtrack to the previous goal in l and search for a path to the new goal.
if backtracking is repeated many times eventually it may move back to the first goal indicating that the current goal list cannot be exercised.
in this case we choose another l giand try again.
upon reaching the final goal in l we traverse the corresponding program path in pito compute the path condition pc which is a symbolic expression of the input condition under which this program path will be executed.
we compute the data input t by solving the path condition using an smt solver.
the main problem in this log guided symbolic execution is to make the computation efficient by exploring the more promising program paths.
toward this end we propose several techniques.
first we statically analyze the source code of each pud to prune away basic blocks that do not lead to the goals they correspond to not only the irrelevant logging points section .
but also the related non logging program statements.
second we skip computationally expensive constraint solver calls unless the program path traverses some previously unexplored system calls.
in addition to these optimizations we prioritize the path exploration based on the estimated distance between current program state and the next goal to increase the likelihood of reaching it sooner.
finally for efficiency reasons if the program path does not traverse any previously unexplored system call descry heuristically avoids generating the data input because it will be less useful for failure reproduction.
we next explain how the next state is selected at each step how concrete configuration options are leveraged to avoid generating a large number of invalid inputs and how concrete data inputs are computed for other puds to further avoid expensive constraint solving.
.
selecting the next state at each step of the symbolic execution of process pi we need to prioritize the exploration by selecting the most promising next state.
internally descry estimates the distance between each state siand oalbefore selecting the most promising one.
the distance is defined as the number of instructions to be executed from si to oaland is computed by statically traversing the control flowgraph of the pud.
if multiple states have the same distance to oal descry would favor the one through which some previously unexplored system calls can be invoked.
as such the search strategy significantly differs from prior state prioritization techniques which do not attempt to maximize the exploration of logging points or previously unexplored system calls.
furthermore to avoid computing the distance more than once we cache the result into a map so it can be queried in subsequent symbolic execution steps.
this is relevant because the distance between two nodes was often queried multiple times e.g.
due to backtracking and without caching there would be re computations.
our search strategy does not require the distance computation to be precise a less accurate estimation will not affect the correctness of the algorithm.
for example we use approximation to handle external libraries with missing source code.
the potential imprecision caused by such approximation will be eliminated during the subsequent dynamic analysis step which concretely executes the puds for interleaving schedule generation and replay.
.
seeding concrete configurations and inputs during symbolic execution the symbolic variables of each pican be either user inputs or configuration options.
for many linux applications configuration options or configuration parameters are treated as a special type of inputs.
these options are often specified in a configuration file read by the program.
treating configuration options or configuration files as symbolic inputs which is the standard approach in symbolic execution can be inefficient because it may take a large amount of time and memory to explore all possible configuration options most of which are invalid e.g.
modules.server ?
.
using concrete options can simplify the constraint solving and thus improve the scalability of symbolic execution.
therefore we propose a heuristic method for identifying configuration options that are more relevant to the logging points.
specifically we employ a technique called static program chopping which computes the intersection of forward and backward slicing.
in program chopping two points of interest source s and target t are chosen and the chop is defined as all statements that could transmit effect of executing stot.
as such chopping reveals the ways in which one program point may affect another program point.
descry takes the process pand the goal list las input.
it considers the read points of each configuration option c cas chopping sources sc and logging points in the goal list l l are chopping targets t l .
for each cand l it applies the static chopping algorithm to compute the chop set cscwith respect to cand l. if cscis not empty the configuration option cis potentially relevant to l in which case descry adds cto the relevant configuration option set c l. when exploring a goal list guidedsymbolicexecution directly takes the concrete configuration options in c las input as opposed to treating them as symbolic values.
given npuds where n descry in general may need to invoke the guidedsymbolicexecution routine ntimes.
to reduce the computational cost it heuristically seeds the concrete input data generated by symbolic execution from one pud pto another pud p ifp accepts the same type of input data.
in this case rather than invoking guidedsymbolicexecution onp we check if we can use the input data dfrom pas the concrete input to p .
ifdallows p to reach its goal list then no expensive constraint solving is needed.
699descry reproducing system level concurrency failures esec fse september paderborn germany interleaving schedule generation we propose a predictive dynamic analysis method to generate the failure inducing interleaving schedule when given a set of puds and their corresponding data inputs.
at the high level we first execute the application consisting of all puds under an arbitrary schedule to generate the initial execution.
since the schedule is defined by the order of inter process events system calls it is represented by the sequence of these events.
if the initial execution does not trigger the failure we systematically generate alternative interleavings of these events.
our method for generating alternative interleavings relies on a predictive constraint model constructed from the initial execution trace.
.
constraint model the constraint model captures the partial order relation of the system call events appeared in the initial execution.
for any two events eiandej we say that ei ejifeimust happen before ej.
if two events do not follow any must happen before order they can be flipped to create a new schedule.
specifically we employ the following order relations program order ei ejwhen eioccurs before ejin the same process thread.
fork return order ei ejwhen eiis the fork that starts the child process pj and ejis the return of pj.
wait exit order ei ejwhen eiis the wait that blocks a parent process and ejis the exit that terminates the child process.
pipe read order ei ejwhen eiis a stream write to a pipe and ejis the corresponding stream read.
signal order ei ejwhen eiis an event of the process before it enables a software signal s and ejis in s. we represent the constraint model as a partial order graph pog denoted by v e where vis the set of nodes corresponding to the inter process events and eis the set of edges between the nodes.
each edge ei ej erepresents a must happen before relation between eiandej.
to generate the failure inducing schedule we systematically permute events in the initial execution by flipping the order of system calls involving shared resource accesses while respecting the order relations.
specifically each time we pick an event pair and flip their order to generate a new interleaving schedule offline and then check if the schedule is feasible by replaying the puds under the new schedule.
.
generating new schedules our new schedule generation algorithm takes the partial order graph as input and returns a set of interleaving schedules as output.
internally descry analyzes two puds at a time e.g.
the failing processpfand a pud pichosen by descry to pair with pf.
this is an approximation that is sufficient for handling all bugs encountered in our experiments where all the bugs involve only two processes .
it is consistent with a prior study which found most real world bugs involve two threads or processes.
there are two challenging problems in generating new schedules which event pair to pick and flip as there can be many event pairs and how to ensure the new schedule is not only feasible but also more likely to trigger the failure.
to solve the first problem descry prioritizes event pairs where at least one of the two events is a write system call and is closer ep1p2exitwaitforkexece1e6e2e3e4e5e7e8... failurefigure an example of schedule generation.
to the failure point based on our observation that events closer to the failure point are more likely to be relevant.
descry starts from events in pfand selects the event efclosest to the failure point.
next it selects an event eithat is close to but does not have order relations with ef.
the event pair ei ef is called a suspicious event pair.
for each suspicious event pair descry generates a new interleaving schedule by flipping the order of the two events.
to avoid obviously infeasible schedules which is the second problem mentioned above descry ensures that the new schedule is consistent with the order relations captured by the pog.
figure shows an example trace where e4 e8 and e3 e4 are two event pairs in the two processes p1andp2 respectively.
the solid edges represent the order relations.
here descry would consider flipping e4 e8 first because it has a higher priority e8is closer to the failure point .
however the two events have a musthappen before relation because of fork return and therefore cannot be flipped.
in contrast the suspicious event pair e3 e4 can be flipped and thus descry would try to schedule e3after e4.
however there are multiple ways of executing e3after e4.
if we simply execute e3immediately after e4 it would violate the partial order relation of e3ande5.
therefore during the reordering our algorithm moves not only the candidate event but also events depending on the candidate event.
in figure this corresponds to moving both e3ande5after e4 which leads to the new interleaving schedule e1 e2 e4 e3 e5 e6 e7 e8 .
in the replay phase descry executes the puds with the newly generated data inputs while controlling the system call events to follow the newly generated interleaving schedule.
if the execution does not match the permuted events which means the execution has taken a different branch from what is expected descry skips the execution since it is an infeasible schedule and proceeds to generate another interleaving schedule this amounts to identifying and flipping another suspicious event pair.
if descry cannot find any failure inducing interleaving schedule after permuting all suspicious event pairs it will backtrack and invoke the symbolic execution procedure again to generate another set of data inputs.
experiments we have implemented descry in a software tool built upon a number of open source platforms.
specifically our static program analysis for mapping log messages to program statements was implemented in llvm our log guided symbolic execution was implemented using klee and our interleaving schedule generator was implemented using the simics virtual platform .
to evaluate descry we consider two research questions rq1 how effective is descry in reproducing real world concurrency failures in multi process applications based only on the default logs generated by these applications?
rq2 how efficient is descry in computing the failure inducing data inputs as well as the event interleaving schedules?
700esec fse september paderborn germany tingting yu tarannum s. zaman and chao wang table benchmarks and descriptions of the failures prog.
loc bug id proc observed failure mv bugzilla another process terminates file is missing rm bugzilla rm terminates directory not empty pxz 370bugzilla file permission mode is modified chmod gnu file permission mode is modified ln debian ln terminates file doesn t exist mkdir debian file permission mode is modified mknod debian file permission mode is modified mkfifo debian file permission mode is modified tail1 changelog output not updated after attached process exits tail2 changelog incorrect output lines after a delivery of signal cp1 changelog file permission mode is modified cp2 changelog directory create fails directory exists sort sig changelog program terminates unlink failed ps bugzilla error message print grep n aaa itself strace1 bugzilla the process been tracked is not detatched strace2 sig bugzilla hang logrotate debian file access permission denied bash debian corrupted history file tcsh debian corrupted history file bzip2 debian file permission mode is modified gzip debian file permission mode is modified lighttpd lighttpd http timeout lighttpd lighttpd error response with an empty body apache apache server shutdown command is ignored .
benchmarks and evaluation metrics all our benchmarks are real linux applications with known concurrency failures due to incorrectly shared resources between processes and or signal handlers.
they are identified by searches of open source repositories such as gnu bugzilla and debian.
there are program versions from unique applications among which applicaitons were from linux coreutils.
searches of these opensource repositories were conducted by research assistants students who are not involved in the descry project to minimize bias.
furthermore the root causes of these failures were unknown to us until we finished running and analyzing the results of descry.
table shows the statistics of each benchmark including the name the number of non comment lines of code the bug id the total number of processes and a short description of the symptom.
in this table the benchmarks are divided into two categories separated by the double horizontal lines.
benchmarks in the first category are from the gnu coreutils.
benchmarks in the second category are from other popular linux applications.
evaluation metrics.
we measure both the effectiveness and the efficiency .
to measure the effectiveness we check whether a known failure can be successfully reproduced within the time limit.
to measure the efficiency of descry we analyze the failure reproduction time by measuring the time spent on pud identification log analysis input generation and schedule generation respectively.
our experiments were conducted on a computer with an intel core i5 .
ghz cpu gb ram and ubuntu .
linux.
we used the most recent version of klee built from llvm .
.
we set a two hour time limit for all four techniques.
to control for variance due to randomization we ran each of the three descry techniques five times to compute the average.
.
experimental results and analysis table summarizes the results of applying descry to the benchmarks.
column shows the benchmark name.
column shows the number of log messages.
column shows the number of relevant puds.
columns show the result of our log analysis including the number of relevant logging points rlp the number of irrelevant logging points ilp and the number of goal lists gl .
column shows the failure inducing data input.
columns show the result of our schedule generation including the total number of explored happens before relations hb system calls sc and system level resources sv .
in the latter two columns numbers in the parenthesis correspond to the events of interest i.e.
relevant system calls and shared resources .
column shows the total number of loop iterations in the main algorithm it is the summation of the number of data inputs generated and the number of times that goal relaxation occurred numbers marked with indicates that goal relaxation occurred .
column shows the total execution time of descry.
column shows the result of failure reproduction where y means it succeeded and no means it failed.
finally column shows the root cause of the failure where events are system calls in the current process whereas system calls marked with are from other processes.
for example in mv with bug id the buggy process causes another process to terminate early due to a missing file when the atomicity of unlink andrename is broken by the write operation of another process.
rq1 effectiveness in reproducing failures.
descry succeeded in reproducing of the failures.
we repeated each experiment five times and found that descry consistently succeeded in generating the failure inducing data inputs and interleaving schedules to reach all failure points of these failures.
therefore the results indicate that descry is effective in reproducing system level concurrency failures.
for the two cases where descry failed the failures were all due to limitations of klee in modeling the file system not limitations of the algorithms in descry.
specifically the failures require the program inputs to be hierarchical directories but klee models the file system as a flattened system where symbolic files have pathnames such as a b and c without any hierarchy .
fixing this problem in klee would have allowed descry to handle these two failures without any modification.
descry is the only tool that can automatically reproduce the known failures.
existing tools such as racepro and simracer may appear to be similar but cannot really solve the same problem due to the following limitations.
first they require the user to provide concrete data inputs which cannot be satisfied in practice.
second their search for erroneous interleaving schedules is not guided by logs.
since prior work has shown that simracer outperformed racepro due to limitations of racepro such as replay divergence in this study we compare descry only to simracer.
simracer does not have the capability of generating new data inputs so we had to feed random inputs to simracer.
as shown in table simracer reproduced only eight out of the failures reproduced by descry.
within descry we also evaluated different search strategies.
by default descry uses our log directed symbolic execution to compute data inputs.
another option is to use the search strategy provided by klee denoted descry df s where the symbolic execution is not guided by the logging points.
this controlled experiment allows us to evaluate the performance of our new input generation algorithm.
the third option is descry as which uses the logging points to guide symbolic execution but does not use our new schedule generation algorithm.
instead descry asrelies on active testing techniques .
this controlled experiment allows us to evaluate the performance of our new schedule generation algorithm.
as shown in table descry df s reproduced only of the failures reproduced by descry indicating that our new logging points guided symbolic execution procedure in descry is significantly more effective than existing techniques.
descry as 701descry reproducing system level concurrency failures esec fse september paderborn germany table results of applying descry to all benchmark applications.
prog.
msg puds log analysis data input generated schedule generation total time replay bug description rlp ilp gl hb sc sviterations min success mv 6a b.lnk y unlink rename .
.
.
stat unlink .
.
.
stat rename rm ?
no openat fstat .
.
.
unlink openat .
.
.
unlink fstat pxz z t a y umask chmod .
.
.
symlink umask .
.
.
symlink chmod chmod ?
no stat fchmodat .
.
.
symlink stat .
.
.
symlink fchmodat ln s f a b y stat unlink .
.
.
unlink stat .
.
.
unlink unlink mkdir m a y mkdir chmod .
.
.
symlink mkdir .
.
.
symlink chmod mknod m a y mkdir chmod .
.
.
symlink mknod .
.
.
symlink chmod mkfifo m a y mkdir chmod .
.
.
symlink mkdir .
.
.
symlink chmod tail1 s f a pid y write .
.
.
read exit read write .
.
.
exit tail2 f a y read stat .
.
.
write read .
.
.
write stat cp1 p dir1 dir2 y mkdir stat .
.
.
fchmod mkdir .
.
.
fchmod stat cp2 r dir1 dir2 y stat mkdir .
.
.
mkdir stat .
.
.
mkdir mkdir sort sig n r a y read unlink .
.
.
unlink read .
.
.
unlink unlink ps u y read execve execve read strace1 f a y stat write fork fork stat write strace2 sig 2a.fork y wait execve execve wait logrotate f a y open chown .
.
.
fchmod open .
.
.
fchmod chown bash c aa history w y write .
.
.
write .
.
.
write write tcsh c aa y write .
.
.
write .
.
.
write write bzip2 d a y close chmod .
.
.
write close .
.
.
write chmod gzip d a y close chmod .
.
.
symlink close .
.
.
symlink chmod lighttpd 19mod.cgi y waitpid exit exit waitpid lighttpd 20mod.cgi y waitpid close .
.
.
waitpid close .
.
.
waitpid waitpid apache k start y signal sigpromask sigpromask signal table comparing the success rate of different methods.
prog.
descry success simracer success descry df s success descry assuccess mv y no no y rm no no no no pxz y no no y chmod no no no no ln y no no y mkdir y no no y mknod y no no y mkfifo y no no y tail1 y no no y tail2 y no no y cp1 y no no y cp2 y y y y sort sig y no no y ps y y y y strace1 y y no y strace2 sig y no no y logrotate y y no y bash y no no y tcsh y no no y bzip2 y y y y gzip y y y y lighttpd y y no y lighttpd y y no y apache y no no y total reproduced all failures.
however as will be discussed in the results of rq2 it took significantly longer time than descry.
rq2 efficiency in reproducing failures.
for all successful cases descry reproduced the failures in less than minutes.
on average each benchmark took only a couple of minutes indicating that descry is efficient for practical use.
furthermore on average .
of the time was spent on generating the failure inducing data inputs while log analysis and schedule generation took only .
and .
of the time respectively.
figure compares the total time in minutes taken by simracer descry df s descry as and descry.
when a bar reaches the top of the vertical axis it means the corresponding method failed to reproduce the failure in the two hour time limit.
among the eight failures successfully reproduced by simracer simracer was times faster than descry on average.
unfortunately there are other failures that simracer could not reproduce but descry could.
compared to descry the method descry df s was .
to times slower on the individual benchmarks both can handle and .
times slower on average and the method descry aswas .
to times slower on the individual benchmarks both can handle and times slower on average .
these results demonstrate the effectiveness of figure comparing the time taken by different methods.
our new symbolic execution and predictive schedule generation algorithms.
.
case study we present two representative examples where descry succeeded in reproducing the system level concurrency failures to demonstrate why it is helpful to developers.
bash bash.
when executing multiple bash shells concurrently the shell history file may be corrupted e.g.
when one bash process p1opens .bash history using open fd o wronly before writing to it and another bash process p2also opens the file for writing.
the failed execution is p1 open p2 open p1 write p2 write p1 close p2 close which produced only the log message writing to the history file .
therefore manually inspecting the source code ofbash to figure out the root cause of the failure would be extremely difficult.
in contrast descry was able to automatically reproduce the failure using the log message.
specifically it produced a program input containing the command line option c i.e.
to execute a new command and the bash command aa history w as well as the failure inducing event interleaving schedule.
lighttpd cgi.
when the cgi program writes an http response tostdout the server cannot complete the transaction.
under normal 702esec fse september paderborn germany tingting yu tarannum s. zaman and chao wang execution the lighttpd process calls waitpid to wait for the cgi process to terminate.
once the cgi terminates the lighttpd receives the fdevent hup signal and the httptransaction successfully terminates.
however when the waitpid in the lighttpd process happens after the cgi process terminates exit lighttpd does the cleanup work and removes the pipe s fd.
in this case lighttpd never gets the fdevent hup event and thus the connection cannot be closed causing a timeout of the http transaction.
this race is caused by thewaitpid andexitcalls on the shared resource proc pid which corresponds to the following log messages benchmarking localhost completed requests ... completed requests completed requests apr poll the timeout specified has expired total of requests completed for lighttpd descry first identified the repeated messages i.e.
lines a list of items printed by the same logging point in lighttpd and located the relevant logging points.
then it used static chopping to identify that the configuration option mod cgi is relevant to the logging points.
this option is specified in the configuration file.
next it generated an input start using guided symbolic execution.
finally given the configuration file and a regular cgi process descry generated a failure inducing interleaving schedule by swapping the order of waitpid and exit.
.
discussion symbolic execution.
one limitation of descry is that it relies on symbolic execution which is known to only scale up to mediumsized applications as in klee although if suitably defined well tuned and well engineered symbolic execution can scale up to larger applications .
quality of logs.
the effectiveness of descry depends on the quality of logs in general it performs better when given more detailed logs.
fortunately research has shown that logging is pervasive in software development practice .
during our experiments the size of the logs ranges from one message to hundreds of messages column of table indicating its effectiveness even in the absence of detailed logs.
server and storage applications tend to produce significantly more detailed logs .
there is only one case tcsh where our log guided input generation algorithm did not outperform the dfs based algorithm of klee.
upon further examination we found it is because the logging points were not under control flow points that lead to the observed failure.
in other words these log messages are too general to provide any useful information of the erroneous execution.
it is worth noting that logs used by descry were generated by the applications under their default production setting i.e verbosity level .
in such case the logging overhead is less than in the eleven applications from the linux coreutils and .
.
in other nine applications.
related work fault detection.
the focus of our work is reproducing failures using only default logs generated by the failed applications.
however there is related work on detecting faults in concurrent systems such as the time of check to time of use tocttou bugs signal races and order violations .
these techniques focus only on fault detection using existing data inputs e.g.
by executingtarget programs in specialized environments.
none of them can automatically generate new data inputs to reproduce system level failures.
similarly racepro is a tool for detecting process level races but it assumes the failure inducing data input already exists.
in contrast descry relies only on default logs generated in the deployment environment.
log and core dump analysis.
there are tools for analyzing failure information in core dumps and heap data .
core dumps were also used to guide the search for failure inducing program inputs .
jin et al.
use various runtime information of the failed execution to guide the generation of program inputs.
sherlog analyzes the program s source code by leveraging coarse grained logs generated by the program to infer paths that may lead to the failure.
however these techniques focus on sequential programs only.
furthermore since they rely on core dumps they cannot handle non crashing failures that do not lead to core dumps.
symbolic execution.
symbolic execution has been widely used in software testing to help increase code coverage and reveals bugs for both sequential and concurrent software.
zamfir et al.
leverage symbolic execution to generate failure inducing execution paths and the corresponding interleaving schedules.
however their method only handles thread level concurrency failures.
symbolic execution has also been combined with other techniques for testing and debugging multithreaded software .
for example guo et al.
use static analysis to identify program paths that do not lead to any failure and prune them away during symbolic execution.
huang et al.
compute feasible thread schedules using a constraint solver by combining constraints from thread paths and constraints from the memory model.
again none of these techniques handle multi process systems.
fault localization.
existing methods for localizing concurrency bugs often rely on analyzing a large set of passed and failed test runs to narrow down the root cause .
for example park et al.
localize anomalous data access patterns associated with a program s pass fail results based on statistical analysis.
weeratunge et al.
diagnose heisenbugs by comparing core dumps of failing and passing runs.
however these methods all assume that developers already have the failure inducing data inputs together with a large set of passing and failing execution traces.
in contrast our method does not make such assumptions.
instead it relies only on the default logs of failed applications.
conclusions we have presented descry the first fully automated software tool for reproducing concurrency failures of multi process applications using only default logs from these applications.
descry leverages new static program analysis to identify the logging points new symbolic execution strategy to generate data inputs and new predictive dynamic analysis to generate failure inducing interleaving schedules.
we have evaluated descry on a large number of widely used linux applications and showed that it successfully reproduced real failures that could no be reproduced by any other tool.
therefore it is a useful addition to the application developer s toolbox for debugging multi process concurrent software systems.