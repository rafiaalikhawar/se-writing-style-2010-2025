tricorder building a program analysis ecosystem caitlin sadowski jeffrey van gogh ciera jaspan emma s derberg collin winter supertri jvg ciera emso collinwinter google.com google inc. abstract static analysis tools help developers find bugs improve code readability and ensure consistent style across a project.
however these tools can be difficult to smoothly integrate with each other and into the developer workflow particularly when scaling to large codebases.
we present tricorder a program analysis platform aimed at building a data driven ecosystem around program analysis.
we present a set of guiding principles for our program analysis tools and a scalable architecture for an analysis platform implementing these principles.
we include an empirical in situ evaluation of the tool as it is used by developers across google that shows the usefulness and impact of the platform.
index terms program analysis static analysis i. i ntroduction static analysis tools provide a promising way to find bugs in programs before they occur in production systems.
developers can run analyzers on their source code to find issues before even checking in the code.
in spite of the rich vein of research on static analysis tools these tools are often not used effectively in practice.
high false positive rates confusing output and poor integration into the developers workflow all contribute to the lack of use in everyday development activities .
in addition to finding bugs tools must take into account the high demands on a developer s time .
any interruption generated by an automated tool forces a developer to contextswitch away from her primary objective .
successful static analysis tools add high value while minimizing the distractions for already busy software engineers.
our past experience with static analysis tools also showed that many of them are not scalable to a codebase of google s size.
analyses cannot presume that they have access to the entire source repository or all the compilation results there is simply too much data for a single machine.
therefore analysesmust be shardable and able to run as part of a distributed computation with only partial information.
the sharded analysis must be extremely fast and provide results within a couple of minutes.
in our previous experiences at google no existing analysis platform could scale in this way.
we also found that existing platforms and tools were not extensible enough.
google has many specialized frameworks and languages and an ideal system would provide static analyses for all of them.
our ideal system would let domain experts write their own analyses without having to bear the cost of building or maintaining an entire end to end pipeline.
for example a team writing c libraries can write checks to make sure developers use those libraries correctly without worry ing about the problems inherent in running a large production system.
after experimenting with a variety of commercial and opensource program analysis tools at google see section ii for more details we repeatedly had problems with tool scalability or usability.
building off of these experiences we wanted to create a static analysis platform that would be widely and actively used by developers to fix problems in their code without prompting from a small group of advocates or management.
integrate smoothly into the existing developer workflow.
scale to the size of an industrial codebase.
empower developers even non analysis experts to write and deploy their own static analyses.
in this paper we present t ricorder a program analysis platform aimed at building a data driven ecosystem around static analysis.
t ricorder integrates static analysis into the workflow of developers at google provides a feedback loop between developers and analyzer writers and simplifies fixing issues discovered by analysis tools.
to accomplish this tricorder leverages a microservices architecture to scale to google s codebase and generates some analysis results each day.
a small team of people maintain t ricorder and the ecosystem around it in addition to working on an open source version of the platform .
t ricorder s plugin model allows contributors from teams across the company to become part of the program analysis community at google.
the contributions of this paper include a set of guiding principles that have resulted in a successful widely used program analysis platform at google.
section iii a scalable architecture for a program analysis platform.
this platform builds a program analysis ecosystem through workflow integration supporting contributors responding to feedback and automatic fixes.
section iv an empirical in situ validation of the usefulness of the platform based upon developers responses to the analyses in their normal workflow.
section v ii.
b ackground a. development workflow at google most engineers1work in an extremely large codebase where most software development occurs at head.
every workday at google engineers perform more than 800k 1some exceptions to this development setup exist for example chrome and android have independent open source repositories and developer tools.
ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee icse florence italybuilds run 100m test cases produce 2pb of build outputs and send 30k changelist snapshots patch diffs for review.
a large codebase has many benefits including ease of code reuse and the ability to do large scale refactorings atomically.
because code reuse is common most code depends upon a core set of libraries and making changes to these base libraries may impact many projects.
to ensure that changes do not break other projects google has a strong testing culture backed by continuous testing infrastructure .
google engineers use a standardized distributed build system to produce hermetic builds from source code .
a team of dedicated engineers maintains this infrastructure centrally providing a common location to insert analysis tools.
because google engineers use the same distributed build environment they may use their choice of editor.
editor choices include but are not limited to eclipse intellij emacs and vim .
as part of a strong code review culture every new patch called a changelist is reviewed by someone other than the author before being checked in.
engineers perform these reviews using an internal code review tool similar to gerrit .
this tool provides the ability to comment on lines of code reply to existing comments upload new snapshots of the code being reviewed author and approve the changelist reviewer .
instead of using a separate qa process busy engineers test and analyze!
their own code.
this means that analysis results must target engineers and it must be easy for those engineers to run and respond to analyzers.
because most code being shipped is server code the cost of pushing a new version is very low making it relatively easy to fix bugs after code has shipped.
b. program analysis at google several attempts have been made to integrate program analysis tools into the google development workflow.
findbugs in particular has a long history of experimentation at google along with other analysis tools such as coverity klocwork and fault prediction .
all of these tools have largely fallen out of use due to problems with workflow integration scaling and false positives.
some tools displayed results too late making developers less likely to fix problems after they had submitted their code.
others displayed results too early while developers were still experimenting with their code in the editor.
editor based tools also hit scaling problems their latency requirements for interactive use were not able to keep up with the size of the codebase.
nearly all tools had to be run as a distinct step and were difficult to integrate with the standard compiler toolchains.
we have repeatedly found that when developers have to navigate to a dashboard or run a standalone command line tool analysis usage drops off.
even when developers ran these tools they often produced high false positive rates and inactionable results .
these experiences match prior research showing why developers do not use static analysis tools .
in the end very few developers used any of the tools we previously experimented with and even for the analysis that got the most traction findbugs the command line tool was used by only developers in and by of those only once .
we did previously show findbugs results in code review but this attempt ran into scaling problems resulting in stale or delayed results and produced many results developers were uninterested in addressing.
in contrast t ricorder has seen success as a part of the standard developer workflow.
iii.
g oogle philosophy on program analysis a. no false positives no may be a bit of an overstatement but we severely limit the number of false positives we allow analyses to produce.
false positives are bad for both usability and adoption .
there is a disconnect around what exactly the term false positive means.
to an analysis writer a false positive is an incorrect report produced by their analysis tool.
however to a developer a false positive is any report that they did not want to see .
we prefer to use the term effective false positive to capture the developer s perspective.
we define an effective false positive as any report from the tool where a user chooses not to take action to resolve the report.
as an example of this some google developers use static annotation checking systems e.g.
for data race detection .
when an annotation checking tool correctly reports an issue it could mean that either there is a bug in the source code e.g.
a variable is not actually protected by a lock or that the code is actually fine but the set of annotations is not exhaustive enough for the tool.
typically in program analysis research the latter is not considered a false positive the developer needs to supply additional information to the tool.
however some developers consider such issues to be false positives since they do not represent a bug in the code .
in contrast we have found that if an analysis incorrectly reports a bug but making the suggested fix would improve code readability this is not considered a false positive.
readability and documentation analyses are frequently accepted by developers especially if they come with a suggested improvement.
it is also notworthy that some analyses may have false positives in theory but not in practice.
for example an analysis may have false positives only when a program is constructed in an unusual way but in practice such a program is never seen.
such an analysis might have theoretical false positives but in an environment with a strictly enforced style guide it would effectively have zero false positives.
the bottom line is that developers will decide whether an analysis tool has high impact and what a false positive is.
b. empower users to contribute in a company using a diverse set of languages and custom apis no single team has the domain knowledge to write all needed analyses.
relevant expertise and motivation exists among developers throughout the company and we want to leverage this existing knowledge by empowering developers to contribute their own analyses.
developer contributions both icse florence italyenrich the set of available analyses and make users more responsive to analysis results.
however while these contributors are experts in their domains they may not have the knowledge or skill set to effectively integrate their analyses into the developer workflow.
ideally workflow integration and the boilerplate needed to get an analysis up and running should not be the concern of the analysis writer.
this is where t ricorder comes in as a pluggable program analysis platform supporting analysis contributors throughout the company.
in order to keep the quality of analyzers high we have a contract with analyzer writers about when we may remove their analyzer from t ricorder .
that is we reserve the right to disable analyzers if no one is fixing bugs filed against the analyzer.
resource usage e.g.
cpu disk memory is affecting t ricorder performance.
in this case the analyzer writer needs to start maintaining a standalone service that t ricorder calls out to see section iv a for more details .
the analyzer results are annoying developers see section iv e for how we calculate this .
our experience is that pride in work combined with the threat of disabling an analyzer makes the authors highly motivated to fix problems in their analyzers.
c. make data driven usability improvements responding to feedback is important.
developers build trust with analysis tools and this trust is quickly lost if they do not understand the tool s output .
we also have found by examining bug reports filed against analyzers that many analysis results have confusingly worded messages this is typically an easy problem to fix.
for instance for one analyzer of all bugs filed against the tool from t ricorder were due to misinterpretations of the result wording and were fixed by updating the message text and or linking to additional documentation.
establishing a feedback loop to improve the usability of analysis results significantly increases the utility of analysis tools.
d. workflow integration is key integration into developer workflow is a key aspect in making program analysis tools effective .
if an analysis tool is a standalone binary that developers are expected to run it just will not be run as frequently as intended.
we posit that analyses should be automatically triggered by developer events such as editing code running a build creating updating a changelist or submitting a changelist.
analysis results should be shown before code is checked in because the tradeoffs are different when an engineer has to modify potentially working submitted code.
as one example of this we surveyed developers when sending them changelists to fix an error in their code we were planning to turn on as a compiler error and also when they encountered those errors as compiler errors.
developers were twice as likely to say the error represented a significant bug when encountered as a compiler error.
the importance ofshowing results early matches previous experience with findbugs .
when possible we integrate static analysis into the build .
we support a variety of analyses built on top of the errorprone javac extension and the clang compiler .
these analyses break the build when they find an issue so the effective false positive rate must be essentially zero.
they also cannot significantly slow down compiles so must have overhead.
ideally we only show results that cause builds to fail as we have found warnings shown when building are often ignored.
however build integration is not always practical e.g.
when the false positive rate is too high the analysis is too time consuming or it is only worthwhile to show results on newly edited lines of code.
tricorder introduces an effective place to show warnings.
given that all developers at google use code review tools before submitting changes t ricorder s primary use is to provide analysis results at code review time.
this has the added benefit of enabling peer accountability where the reviewer will see if the author chose to ignore analysis results.
we still enforce a very low effective false positive rate here .
additionally we only display results for most analyses onchanged lines by default this keeps analysis results relevant to the code review at hand.2analyses done at code review time can take longer than analyses that break the build but the results must be available before the review is over.
the mean time for a review of more than one line is greater than hour we typically expect analyses to complete in less than minutes ideally much less as developers may be waiting for results.
there are other potential integration points for program analysis.
many ides include a variety of static analyses.
however most google developers do not use ides or do not use ides for all tasks making ide only integration untenable.
still ide integration is not precluded as an ide can issue rpcs to the t ricorder service.
we also leverage testing to run dynamic analysis tools such as threadsanitizer or addresssanitizer .
these tools typically have no false positives and 10x slowdowns.
t ricorder also shows nightly results from some analyses in google s code search tool as an optional layer.
while most developers do not use this feature it is effective for analyses that have higher false positive rates and have a dedicated cleanup team to sift through the results.
e. project customization not user customization past experiences at google showed that allowing userspecific customization caused discrepancies within and across teams and resulted in declining usage of tools.
we observed teams where a developer abandons a tool they were initially using after discovering teammates were committing new instances of code containing warnings flagged by the tool.
we 2we do show some analyses on all lines by default as an example warnings about unused variables can occur on an unchanged line when the block of code using the variable is deleted by the changelist.
developers have the option of also viewing results for unchanged lines during a review if they want to.
icse florence italyhave worked to eliminate per user customization of analysis results.
to achieve this we got rid of all priority or severity ratings for analysis results.
instead we try to only show high priority severity results and we improve our analysis tools when results are flagged as not important.
in cases where there was some debate as to whether an analysis had useful results we made the analysis optional.
developers still have the ability to explicitly trigger optional analyzers but they will not run by default.
getting rid of priority ratings had several benefits we were able to remove or improve low priority checks that had little benefit.
instead of having developers filter out analyzer results we started getting bug reports about broken analyzers.
for example we discovered that the c linter was also linting objective c files and fixed this issue previously objective c developers had just hidden all linter results.
we dramatically reduced complaints about why certain results appeared or why different views were inconsistent.
we do allow limited customization but the customization is project based rather than user based.
for example a team can choose to run optional analyses by default on all of their code.
we also disable analyzers where they do not apply for example we don t run code style checkers on third party open source code with different code conventions.
iv.
i mplementation a. architecture to efficiently serve analysis results on changelist creation and editing t ricorder leverages a microservices architecture .
thinking in terms of services creates a mindset that encourages scalability and modularity.
in addition t ricorder is designed with the assumption that parts of the system will go down which means analysis workers are designed to be replicated and stateless in order to make the system robust and scalable.
analysis results appear in code review asrobot comments robocomments for short using the code review tool s commenting system.
analysis services implement the same api section iv b .
this api is defined with protocol buffers as a multilanguage serialization protocol and uses a google specific rpc library for communication.
t ricorder includes a series of analyzer worker services written in different languages java c python and go implementing this common language agnostic protocol buffer api.
these services provide a language specific interface for analyzers to implement that abstract away details of handling rpcs analysis writers can implement analyzers in the language that makes the most sense.
t ricorder also includes compiler specific analyzer services providing a way to plug into jscompiler javac and clang.
we additionally have a binary multiplexor linter worker service supporting linters written in arbitrary languages.
tricorder has three stages at which it calls out to analysis services each stage has successively more information.
java analyzer workerchangelist snapshot notifierbuild servicedependencies service analysis driver build listenertargets listenersnapshot listener javac analyzer workerc analyzer worker go analyzer worker python analyzer workerclang analyzer worker jscompiler analyzer workercodereview linter workerfiles deps compilationfig.
overview of the tricorder architecture.
solid boxes correspond to microservices running as jobs and dashed boxes distinguish separate parts of a job.
solid arrows correspond to rpcs being sent within t ricorder while dashed edges refer to rpc to external systems and services.
at the files stage analyzers can access the contents of files that make up the change.
for example linters that check properties like line is over characters can run at this stage.
at the deps short for build dependencies stage analyzers additionally know a list of all the build targets that are affected by the change.
for example an analyzer that reports when a large number of targets are affected can be run at this stage.
finally at the compilation stage analyzers have access to the complete ast for the entire program with fully resolved types and implicit expressions.
dividing the analyses into multiple stages has several practical benefits we can provide faster results for analyses at earlier stages since they do not need to wait for a build.
we can decrease resource usage by rate limiting analyses at costlier stages.
problems with infrastructure we depend on such as the build service will not affect analyses from earlier stages.
the overall architecture of t ricorder is depicted in figure .
the main loop of t ricorder happens in the analysis driver.
the driver calls out to language or compiler specific analyzer workers to run the analyses then sends results as comments to the code review system.
in each worker incoming analysis requests are dispatched to a set of analyzers.
in addition analysis writers can choose to implement their own standalone service with the analyzer worker api.
the analysis driver is divided into separate sections for the different stages of the pipeline the snapshot listener sends requests to files analyzers the targets listener sends requests to deps analyzers and the build listener sends requests to compilation analyzers.
the process is as follows when a new changelist snapshot is generated the icse florence italychangelist snapshot notifier signals to t ricorder via a publisher subscriber model that there is a new snapshot.
this message contains metadata about the snapshot such as the author changelist description and list of files and a source context repository name revision etc that can be used to both read the edited files inside the change and post robot comments about the change later.
when t ricorder receives a snapshot notification it fires off several asynchronous calls tricorder sends analyze requests to the files analyzers.
when it receives results it forwards the results to the code review system.
tricorder makes a request to the dependencies service to calculate which build targets are affected by the change.
the dependencies service notifies t ricorder when it has finished calculating the dependencies and t ricorder makes the following asynchronous calls tricorder sends analyze requests to the deps analyzers complete with the list of dependencies transitively affected by the change.
when t ricorder receives analysis results it forwards them to the code review service.
tricorder requests that the build service starts a build of all targets directly affected by the change.
the build service notifies t ricorder as each independent compilation unit is built.
the builds are instrumented to capture all inputs needed for each compiler invocation in all supported languages.
this set of inputs e.g.
jar files compiler arguments headers etc is preserved for later use by analyzers.
when a message arrives that signals a finished compilation unit t ricorder sends rpcs to the compilation analyzers.
the compiler specific workers replay the compilation using the inputs generated during the build with analysis passes added.
compilation analyzers have access to the ast and all other information provided by the compiler.
when t ricorder receives results from the analyzers they are forwarded to the code review service.
the use of asynchronous communication allows t ricorder to make more efficient use of its machine resources.
earlier analyses can run in parallel to running a build and the compilation units are all analyzed in parallel as well.
even the slowest analyses provide results within a few minutes.
b. plug in model tricorder supports a plug in model across languages.
analyzers may be written in any language currently c java python and go have the best support.
analyzers may analyze any language and there are even a variety of analyzers 3since the google codebase is quite large and projects may have far reaching dependencies this is important to calculate.
for smaller codebases this stage can likely be skipped in favor of building the entire project.focused on the development process and not a programming language section iv c .
all analyzer services implement the analyzer rpc api.
most analyzers are running as part of one of the analyzer workers and implement a language specific interface.
each analyzer must support the following operations getcategory returns the set of categories produced by that analyzer.
the category of an analyzer is a unique human readable name displayed as part of the robocomments it produces.
getstage returns the stage in which this analyzer should run.
analyze takes in information about the change and returns a list of notes.
notes contain key information about the analysis result including the category and optionally subcategory of the analysis result.
the location of the analysis result in the code e.g.
file and range line column within that file.
the error message.
a url with more detailed information about the analyzer and or the message.
an ordered list of fixes.
the notes produced are then posted to google s internal code review tool as robocomments.
since the structured output is flexible they may also be used in additional contexts such as when browsing source code .
a more detailed look at this api is available in the open source version of tricorder shipshape .
shipshape has a different architecture to support the needs of open source projects but the api and design was heavily influenced by t ricorder .
c. analyzers figure shows a selection of analyzers currently running in t ricorder there are currently about with more coming online every month .
six of the analyzers in this table are themselves frameworks.
for example errorprone and clangtidy both find bug patterns based on ast matching for java and c programs respectively.
they each have a variety of individual checks enabled each implemented as a plugin.
another example is the linter analyzer.
this analyzer is comprised of more than individual linters all called via a linter binary multiplexor.
linters can be implemented in any language.
the multiplexor uses a configuration file to determine which linter to send a particular file to based on the file extension and path and how to parse linter output via a regex .
the linter analyzer includes google configured versions of popular external linters such as the java checkstyle linter and the pylint python linter as well as many custom internal linters.
several t ricorder analyzers currently are domainspecific they are targeted at only a portion of the code 4the affectedtargets and builder analyzers are informational and so do not have a please fix option.
icse florence italyanalyzer description stage impl.
lang.analyzed lang.
of pluginsavg.
results dayplease fix usersnot useful users affectedtargets how many targets are affected deps java all an scans android projects for likely bugs comp.
java android autorefaster implementation of refaster comp.
java java builddeprecation identify deprecated build targets deps java build files1689 builder checks if a changelist builds comp.
java all clangtidy bug patterns based on ast matching comp.
c c doccomments errors in javadoc comp.
java java errorprone bug patterns based on ast matching comp.
java java formatter errors in java format strings comp.
java java golint style checks for go programs files go go govet suspicious constructs in go programs files go go javacwarnings curated set of warnings from javac comp.
java java jscompilerwarnings warnings produced by jscompiler comp.
java javascript linter style issues in code files all all unused unused variable detection comp.
java java unuseddeps flag unused dependencies comp.
java build files1419 fig.
of analyzers run in t ricorder .
the fourth and fifth columns report the implmenentation language and the target language respectively.
the sixth column reports the number of plugins for analyzers providing an internal plugin mechanism.
the seventh column shows the average number of results per day.
the final two columns report the number of unique users who clicked on either p lease fix or n otuseful see section iv e in the year .
base.
this includes androidlint which finds both bugs and style violations specifically in android projects as well as validators for several project specific configuration schemas.
several other analyzers another are about metadata relevant to the changelist.
for example one analyzer warns if a changelist needs to be merged with head while another warns if a changelist will transitively affect a large percentage of google s code.
to decide whether an analyzer makes sense to include in tricorder we have criteria for new analyzers drawn from our experience and philosophy on static analysis section iii the warning should be easy to understand and the fix should be clear.
the problem should be obvious and actionable when pointed out.
for example cyclomatic complexity or location based fault prediction does not meet this bar.
the warning should have very few false positives.
developers should feel that we are pointing out an actual issue at least of the time.5to measure this we run analyzers on existing code and manually check a statistically sound sample size of the results.
the warning should be for something that has the potential for significant impact.
we want the warnings to be important enough so that when developers see them they take them seriously and often choose to fix them.
to determine this language focused analyzers are vetted by language experts.
5this false positive threshold matches that used by other analysis platforms such as coverity .
fig.
screenshot of analysis results changelist reviewer view.
in this case there are two results one from the java lint tool configured version of checkstyle and one from errorprone .
reviewers can click on the n otuseful link if they have a problem with the analysis results.
they can also click on p lease fixto indicate that the author should fix the result.
they can also view the attached fix figure .
the warning should occur with a small but noticeable frequency.
there is no point in detecting warnings that never actually occur but if a warning occurs too frequently it s likely that it s not causing any real problems.
we don t want to overwhelm people with too many warnings.
analysis developers can try out new analyses on a small set of whitelisted users first who have volunteered to view experimental results.
some analyses can also be run in a mapreduce over all existing code to check false positive rates before being deployed.
icse florence italyfig.
screenshot of the preview fix view for the errorprone warning from figure d. fixes two common issues with analysis tools are that they may not produce actionable results and that it takes effort for busy developers to fix the issues highlighted.
to address these problems we encourage analysis writers to provide fixes with their analysis results.
these fixes can be both viewed and applied from within the code review tool.
figure shows an example comment produced by t ricorder .
the fix is visible after clicking the show link figure .
note that the fixes here are not tied to a particular ide they are part of each analysis and are language agnostic.
having analyzers supply fixes has several benefits fixes can provide additional clarification for analysis results being able to apply the fix directly means that dealing with analysis results does not entail changing context and tool provided fixes lower the bar to fixing issues in code.
in order to apply fixes we leverage the availability of a system that makes the content of a changelist available for edits.
that is edits like applying fixes can be made directly to the code in the changelist and the changed code will appear in the workspace of the changelist owner.
to implement something similar with gerrit one could leverage existing apis to apply fixes as a patch to the code under review.
e. feedback in order to respond quickly to issues with analyzers we have built in a feedback mechanism that tracks how developers interact with the robocomments t ricorder generates in code reviews.
as seen in figure and figure there are four links that developers can click notuseful gives the developers the opportunity to file a bug about the robocomment.
please fixcreates a review comment asking the author to fix the robocomment and is only available to reviewers.
preview fix show in figure shows a diff view of the suggested fix and is available when a robocomment comes with a fix.
apply fix apply in figure applies the suggested fix to the code and is available only to authors when a robocomment comes with a fix.
this option can only be seen after using p review fix.
we define the not useful rate of an analyzer as notuseful notuseful p lease fix a pply fix analysis writers are expected to check these numbers through a dashboard we provide.
a rate puts the ana lyzer on probation and the analysis writer must show progress toward addressing the issue.
if the rate goes above we may decide to turn the analyzer off immediately in practice we typically work with the analyzer writers to fix the problem instead of immediately disabling an analyzer.
some analyzers only affect a small percentage of developers and so we are not as concerned about a temporary increase in false positives.
nonetheless having a policy in place has proven invaluable in making expectations clear with analyzer writers.
v. r esults a usability we measure the usability of t ricorder through the not useful click rates and numbers of clicks of each type both for t ricorder as a whole and for specific analyzers.
figure shows that developers actively engage with the analyses through clicks.
figure lists the number of unique users in who clicked on either p lease fixor n ot useful at least once for each analyzer.
as can be seen the linter has received p lease fixclicks from over 18k users in .
the number of people who have every clicked n ot useful is substantially lower across all categories.
our click rates show that developers are generally happy with the results from the static analysis tools.
figure shows the week over week across all of the t ricorder analyzers in recent months it is typically at around .
when we remove analyzers that are on probation the number goes down to under .
figure shows a comparison of the not useful rates for several java analyzers.
the checkstyle errorprone and unuseddeps analyses are all fairly stable and have a low not useful rate the data for errorprone has more variance due to the fewer number of findings that it produces.
doccomments is more interesting this analyzer was initially on probation and the developer worked to get the not useful rate under .
however there was a bug introduced in week which resulted in a sharp increase in n otuseful clicks.
the developer was able to use our provided bug reports and click logs to identify the source of the problem the fix was finally released in week .
analysis writers can also investigate the raw numbers of clicks each week.
figure shows the breakdown for the type of clicks for the errorprone analysis.
it is interesting to note how correlated p lease fixis with p review fix we hypothesize that many times a reviewer clicks p lease fix and icse florence italy0 .
.
.
.
.
.
.
.
.
.
weeknot useful rate tricorder t ricorder no probations probationary onlyfig.
not useful click rate for all of t ricorder by week for .
the probationary analyzers are have either been turned off for having a high not useful rate or are being actively improved upon.
while the have a high rate there are few enough of them relative to the rest of the analyzers that they have only minor effect on the total rate.
then the author clicks p review fixto see what the problem was.
the a pply fixclicks are much lower.
based on developer observations we hypothesize that many authors choose to fix the code in their own editor rather than use the code review tool for this purpose especially if they are already addressing other reviewer comments in their editor.
notice that providing a fix has two purposes one is to make it easy for developers to apply the fix but the other is as a further explanation of an analysis result.
clicks are an imperfect measure of analyzer quality many developers report fixing issues before their reviewer sees them so p lease fixcounts are known to be low.
many developers report fixing issues in their own editor rather than via a pply fix so those counts are also low.
developers may ignore findings they do not plan to fix rather than clicking n otuseful .
this may be a signal only of how strongly the developer feels about the issue.
developers may click on n otuseful by accident.
despite these drawbacks clicks have been a good signal for developer annoyance .
our most successful analyzers have not useful rate between .
when a developer clicks on n otuseful a link appears to file a bug in our issue tracking system against the project responsible for that analyzer the bug is pre populated with all the necessary information about the robocomment and gives the developer an opportunity to comment on why they clicked notuseful .
the rate of filing bugs per n otuseful clicks is between depending on the analyzer.
b codebase impact tricorder reduces the number of instances of violations in the codebase over time.
for the clangtidy analyzer we are able to see that showing re0 .
.
.
.
.20not useful rate a checkstyle .
.
.
.
.4not useful rate b doccomments .
.
.
.
.20not useful rate c errorprone .
.
.
.
.20not useful rate d unuseddeps fig.
weekly not useful rate for a selection of java based analyzers in .
sults in code review not only prevents new problems from entering the codebase but also decreases the total number of problems as people learn about the new check.
figure shows the number of occurrences of the clangtidy misc redundant smartptr get check per week this check identifies cases where there is an unneeded .get call on a smart pointer.
the vertical line is when this check began being shown in t ricorder .
after adding the check to t ricorder the number of instances in the codebase decreased dramatically as developers recognized the inappropriate coding pattern stopped making such mistakes in new code and fixed existing occurrences elsewhere in the code.
figure also shows a sampling of trend lines for other clangtidy fixes they all show a similar pattern of drop off or level off after the check is enabled in t ricorder .
the checks which only level off are typically checks with more complicated non local fixes.
c pluggability tricorder is easy to plug into.
we evaluate this by demonstrating that a variety of analyses have successfully plugged in.
as discussed in section iv c more icse florence italy0 weeknumber of clicksnot useful clicks please fix clicks pre view fix clicks apply fix clicksfig.
weekly clicks by type for the errorprone analyzer.
weeknumber of instances in the codebase fig.
instances in the codebase of violations of several clangtidy checks.
the large graph shows the number of violations in the codebase of the check misc redundant smartptr get the smaller graphs show similar trends for other clangtidy checks.
than different analyses are running in t ricorder .
figure shows a selection of t ricorder analyzers these analyzers span a breadth of languages and problems highlighted.
of the analyzers in this table all except formatter builddeprecation builder were contributed by members of more than other teams.
many additional developers contributed plug ins to analyzers such as errorprone clangtidy or the linter.
d scalability tricorder scales to a very large codebase.
to evaluate this we are running t ricorder against all changelist snapshots produced each day at google.
figure shows the average median and max counts of several scaling metrics for t ricorder .
on an average day we run t ricorder on31k snapshots each with an average size of 12files and we report findings for several categories and languages.
in contrast reviewbot was evaluated on 34review requests corresponding to change lists compared to the millions we have used to evaluate t ricorder .
on average each snapshot contains files from one language with a max of 22languages.
in total for one day weaverage median max tricorder runs day 31k 38k 66k findings day 93k 127k 183k builds day 4k 6k 9k analyzer runs day 81k 93k 208k files cl 333k languages cl findings cl 5k please fix cl not useful cl please fix day not useful day fig.
scalability numbers for t ricorder in terms of per day or per changelist cl collected over 90days.
report an average of 93k findings for around categories.
tricorder also runs close to 5k builds per day.
each day reviewers click p lease fixon an average of 716findings from the linters but only 48findings get a not useful click.
an average cl has two please fix clicks and no not useful clicks.
even though we are producing considerably more findings than are clicked on most are not actually shown in the review as they appear on unchanged lines.
vi.
r elated work a previous study investigated why developers do not use static analysis tools to find bugs collecting results from interviews with developers .
many of the conclusions of this study match our experiences with experimenting with various program analysis tools.
t ricorder addresses the main pain points identified by this study.
for example t ricorder continuously ensures that tool output improves and results are understandable by maintaining a tight feedback loop.
t ricorder also enables collaboration around the tool results by code review integration reviewers can suggest or comment on static analysis results.
this study also highlighted the importance of workflow integration a main design point of tricorder .
there is a wide breadth of research on static analysis tools we can only describe a portion of the existing tools here.
findbugs is a heuristic based bug finding for tool that runs on java bytecode.
coverity klocwork and semmle are commercial tools focused on static analysis.
linting tools such as checkstyle java and pylint python are primarily focused on style issues spacing where to break lines etc .
analysis results along with quick fixes are also often surfaced in ides such as eclipse and intellij .
we could surface t ricorder results here too by calling out to our service from within the ide.
many commercial tools also show results in a dashboard format while we do have dashboards we found that this was only useful for analysis writers to help them improve the quality of the analyzers.
6unfortunately we do not have the ability to measure how many results are viewed by developers.
icse florence italyother large companies such as microsoft are actively experimenting with ways to integrate static analysis into developer workflow .
ebay has developed methods for evaluating and comparing the value of different static analysis tools under experimentation .
ibm has also experimented with a service based approach to static analysis .
this work was evaluated through a pilot and surveys with several teams at ibm and they identified several areas for improvement.
t ricorder addresses each of these improvements it is integrated into the developer workflow instead of running in a batch mode it is pluggable and supports analyses written by other teams and it includes a feedback loop to analysis writers to improve the platform and analyzers.
one previously published system called reviewbot showed static analysis results in code review .
reviewbot differs from t ricorder in that reviewers have to explicitly call reviewbot analysis results are not shown to reviewers only three java static analysis tools are currently supported.
reviewbot also added the ability to provide fixes this fix system is completely decoupled from the analysis results themselves.
in contrast t ricorder fixes are produced by analyzers and our mechanism for applying fixes from structured analysis results is language agnostic.
unlike prior work we evaluated t ricorder during the developer workflow not as a survey of results generated and seen by developers outside of their work environment.
this evaluation style allows us to see how developers react in practice rather than in a lab setting.
vii.
d iscussion tricorder has been deployed in production since july .
through implementing launching and monitoring t ricorder we have learned several interesting things about how to make program analysis work.
in section iii we outlined our philosophy both in terms of goals for our system and lessons learned from past experiences.
we now revisit our goals and the presented evaluation.
make data driven usability improvements.
in the end developers will decide whether an analysis tool has high impact and what they consider a false positive to be.
developers do not like false positives.
this is why it is important to listen to feedback from developers and to act on it.
an average of 93k findings per day receive an average of p lease fix and n otuseful clicks.
we pay close attention to these clicks and if needed we put analyzers on probation.
we discuss improvements with analyzer writers and encourage them to improve their analyzer an improvement that may be as simple as updating the wording of results.
empower users to contribute.
tricorder does this by providing a pluggable framework which enables developers to easily contribute analyses within their area of expertise.
in fact the majority of all analyses running in t ricorder are analyses contributed by developers outside the team managing tricorder itself.
workflow integration is key.
as with a previously presented tool from vmware we have found that code re view is an excellent time to show analysis results.
developers receive feedback before changes are checked in and the mechanism for displaying analysis results is uniform no matter which ide or development environment was previously used.
there is also peer accountability as reviewers can see and respond to analysis results reviewers click p lease fix an average of times each week.
project customization not user customization.
based on experience we have found that customization at project level is most successful compared to customization down to user level.
this provides flexibility so that teams of developers can have a joint approach to how the code for a project should be developed and avoids disagreements about analysis results between developers which may lead to results being ignored.
in addition to the above some things should be mentioned about sophistication scalability and fixes.
all of the checks described in figure are relatively simple.
we are not using any control or data flow information pointer analysis wholeprogram analysis abstract interpretation or other similar techniques.
nonetheless they find real problems for developers and provide a good payoff.
that is there is big impact for relatively simple checks.7in general we have been more successful with analyses that provide a suggested fix.
analysis tools should fix bugs not just find them.
there is less confusion about how to address the problem and the ability to automatically apply a fix provided by the tool reduces the need for context switches.
finally to ensure analyses can run even at google s scale program analysis tools should be shardable.
think about an analysis tool as something to map reduce across large sets of programs.
final thoughts.
in this paper we presented a static analysis platform but also our philosophy on how to create such a platform.
it is our goal to encourage analysis writers to consider this philosophy when creating new tools and all the tradeoffs of their tool not just the technically defined false positive rate or the speed of the analysis.
we also encourage other companies even if they have tried program analysis tools before to try again with this philosophy in mind.
while we also failed to use static analysis tools widely for many years there is a large payoff for finally getting it right.