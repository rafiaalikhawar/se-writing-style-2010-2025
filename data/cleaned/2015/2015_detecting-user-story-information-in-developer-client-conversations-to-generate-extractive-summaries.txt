detecting user story information in developer client conversations to generate extractive summaries paige rodeghero siyuan jiang ameer armaly and collin mcmillan department of computer science and engineering university of notre dame notre dame in usa email prodeghe sjiang1 aarmaly cmc nd.edu abstract user stories are descriptions of functionality that a software user needs.
they play an important role in determining which software requirements and bug fixes should be handled and in what order .
developers elicit user stories through meetings with customers.
but user story elicitation is complex and involves many passes to accommodate shifting and unclear customer needs.
the result is that developers must take detailed notes during meetings or risk missing important information.
ideally developers would be freed of the need to take notes themselves and instead speak naturally with their customers.
this paper is a step towards that ideal.
we present a technique for automatically extracting information relevant to user stories from recorded conversations between customers and developers.
we perform a qualitative study to demonstrate that user story information exists in these conversations in a sufficient quantity to extract automatically.
from this we found that roughly .
of these conversations contained user story information.
then we test our technique in a quantitative study to determine the degree to which our technique can extract user story information.
in our experiment our process obtained about .
precision and .
recall on the information.
i. i ntroduction a user story is a description of software functionality from the perspective of the software s user .
user story management is under the umbrella of requirements engineering but a user story is different from a requirement in the traditional sense because a story usually contains no information about how the software should be implemented.
instead the story focuses on user experience including the role function and rationale behind the user s objective in the format as a role i want to function so that i can rationale .
for example instead of a requirement the system shall use a sql database to store recently sold home prices a user story might say as a homebuyer i want to search for recently sold homes so i can estimate prices in my area.
cohn points out that the typical source of user stories is careful analysis of the conversations between programmers and customers.
since user stories do not contain implementation details conversations with customers are an effective place to search for user stories.
developers are advised to take notes during conversations and reread these notes to write user stories by hand.
this process is important because storiesplay a crucial role in agile development where release cycles are often short to accomodate the constantly evolving needs of customers.
the result is that developers are in a constant process of user story elicitation as stories appear mature and are removed .
while there is an emphasis on by hand effort for writing user stories in practice software engineering literature describes automated summarization techniques for knowledge extraction from software artifacts.
notably rastkar et al.
icse built an algorithm for summarizing software artifacts and tested the algorithm on a corpus of bug reports.
that approach was based on earlier work by murray and carenini designed to summarize emails and conversations.
in a nutshell these approaches are machine learning classifiers that are trained to recognize sentences in documents that are likely to contain certain types of information important to the summary.
the performance was considered reasonable by human evaluators at approximately precision and recall see section ii d for a more detailed discussion .
in this paper we automatically extract data for writing user stories from records of conversations between developers and customers.
specifically we perform a qualitative study to test the hypothesis that conversations between developers and customers contain role function and rationale information for user stories and perform a quantitative study to determine the degree to which an existing classification algorithm can be trained to recognize this information in the conversations.
for the qualitative study section iii we recorded approximately hours of spoken conversation conversations total between developers and customers over a period of three weeks at a software development company in the united states with between and employees.
we then transcribed the recorded conversations to text and manually annotated sections of the conversations as containing role function and or rationale information pertaining to user stories.
to ensure we could compare our results with earlier work we also annotated extractive summaries for each conversation see section ii c for details on these summaries .
we found that about .
of the conversations included function information .
discussed rationale but only .
discussed role.
about .
ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
were part of the extractive summaries which was slightly less than the and reported by murray and carenini for the meeting and email corpora respectively and reported by rastkar et al.
for the bug reports.
it is important to note that the murray and carenini meeting study which is the most similar to our study since it also used annotated meeting transcripts has the most similar extractive percentage.
put briefly we found that the conversations included significant function and rationale information but very limited data about the roles.
in the quantitative study section vi we trained a classifier to recognize the function and rationale information as well as the extractive summaries for comparision.
we made numerous modifications from the technique described by rastkar et al.
and murray and carenini to adapt the technique to detect function and rationale data for user stories.
we describe our procedure in detail in section v. we obtained approximately .
precision .
recall for detecting sections of conversations containing function data and .
precision .
recall for rationale data.
for comparison purposes we obtained about .
precision .
recall for extractive summaries.
our long range vision for this research is to design an algorithm that generates user stories by listening to the conversations between developers and customers.
the algorithm could for example be installed into teleconferencing software to automatically create notes about user stories for developers after a meeting.
the technique we present in this paper is a research prototype in that direction to be usable it would need an automated transcriptionist and a natural language generation system to create abstractive user stories from the extractive data we currently can provide.
still it is our view that this paper is a vital early step.
to facilitate reproducibility and assist other researchers we have made our implementation available via an online appendix1 including a virtual machine image with all dependencies installed.
while we cannot release the recordings of the meetings for ethical and privacy reasons we do provide our trained classifier so that it can be tested on other datasets.
to our knowledge no public records of meetings with user story information are available but for comparision purposes we duplicate our quantitative experiments for extractive summaries on the public ami meeting corpus .
ii.
b ackground and rela ted work this section describes supporting technologies for this research as well as related literature.
note that these technologies have been proposed and evaluated in previous work.
we include them here because our work is based on these earlier techniques.
a. user story elicitation user stories play a crucial role in agile development where programming activities are centered around the needs of customers.
in a textbook agile environment developers prodeghe projects userstories elicit user stories through contact with customers prioritize these stories and schedule tasks in release cycles based on the stories.
one characteristic of agile development is a relatively short release cycle that is responsive to changing requirements .
the result is that developers are in a constant process of user story elicitation as stories appear mature and are removed.
this process of elicitation is often messy customers may have difficulty articulating their own needs and developers may miss opportunities to ask clarifying questions or highlight important problems.
cohn reinforces an opinion by robertson and robertson that elicitation is akin to trawling for fish as numerous passes are needed with different tools and techniques in order to catch as many user stories as possible.
nevertheless cohn points out that a key component of user story elicitation is careful analysis of the conversations between programmers and customers.
developers are advised to take notes during conversations and reread these notes to write user stories by hand.
some researchers such as berenbach et al.
are even trying to introduce unique frameworks to better capture and analyze these elicitations.
b. turn based conversation analysis in this paper we use turns instead of sentences as the unit of analysis.
this section defines these terms and outlines our motivation for using turns.
one important characteristic of the work by rastkar et al.
is that it is based on the sentences in the bug reports.
the extractive summaries are a subset of the sentences in the bug reports.
likewise the work by murray and carenini creates summaries from the sentences in emails and transcripts.
for written text and some types of spoken text sentence based analysis is ideal because the boundaries between sentences are clear and written sentences tend to contain cohesive information .
in contrast the preferred unit of analysis for most types of spoken language is the turn as noted by numerous authors in the field of conversation analysis in sociology .
a turn is the unit of speech that occurs when a person speaks in a conversation between other speakers.
in ordinary conversation people take turns speaking and listening to others speak.
turns are different than sentences in that turns are dependent on the context in which the speaker takes a turn and the speaker s own immediate thoughts and reactions.
human factors are present in spoken turns to a higher level than written sentences social rank confusion number of listeners etc.
affect the length order and content of turns.
transcriptionists are tasked with creating written sentences from spoken language.
they will typically divide each turn into multiple sentences marking punctuation including periods and commas.
but this is a highly subjective process since speakers may repeat or rephrase information or fill gaps while thinking with hmms and uhhs.
the listeners in a conversation will typically defer to a speaker to finish a turn even if the turn contains unfinished or run on sentences.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
our view is that role function and rationale information are much more likely to be detectable in turn based analysis than sentence based.
the reason is that in a conversation a customer may struggle to describe for example the function that he or she needs.
the customer may describe a situation in many ways splitting important information over several sentences or partial sentences.
the developers will listen carefully during the customer s turn to allow the customer to finish his or her thought.
in these situations it is far more useful to have the entire turn rather than try to detect a single sentence which contains all the necessary information such a sentence may not exist.
c. summarization and knowledge extraction summarization is the process of creating a short description of a longer artifact .
there are two types of summaries extractive and abstractive.
an extractive summary is a summary created out of pieces of the larger artifact.
sentence selection is a form of extractive summary generation because it picks one sentence out of a document that describes the main points of the document .
a commonly used extractive technique in software engineering research is a tf idf vector space model in which the top nwords are selected from a software artifact to describe that artifact .
in contrast abstractive summaries are synthesized from information inside the artifact without copying that information.
abstractive summarization is typically what humans do as it often involves contextual information or in your own words interpretation.
nevertheless abstractive summarization techniques do exist in the software engineering literature such as work by mcburney et al.
sridhara et al.
moreno et al and buse and weimer .
these techniques are related to our work in that they use summarization techniques to extract knowledge from software artifacts but differ widely in their approach the type of data they are summarizing and the type of artifacts they analyze.
to our knowledge we describe the first technique to automatically extract user story information from developer customer meeting transcripts.
in this paper we treat summarization as a form of knowledge extraction in that a summarizer is attempting to extract and highlight a specific type of information from the larger artifact .
we believe this is similar to but unique from the work done in requirements engineering by clelandhuang et al.
and others.
in a nutshell instead of summarizing a whole document we aim to extract the parts of the document pertaining to the role function and rationale behind user stories.
we apply summarization to the problem of knowledge extraction about user stories.
d. summarizing software artifacts three key publications behind this research are by murray and carenini in and a related advancement on that work by rastkar et al.
published in icse and tse in .murray and carenini describe a technique to produce extractive summaries of email threads and meeting transcripts.
the technique is essentially a machine learning classification problem in which one class of sentences includes the sentences in the extractive summaries and another class includes all other sentences.
generally speaking their approach occurs over three steps compute quantifiable attributes for each sentence in the emails and meetings train a logistic regression classifier to detect sentences that are in manually annotated extractive summaries and conduct a cross validation experiment and compute performance metrics.
the experiment they conducted was a leave one conversation out format in which they trained on all conversations except one and then tested on the remaining conversation.
one major research contribution of their work is that the attributes they computed were generic in the sense that they did not depend on domain specific terminology.
the attributes can be calculated on multi modal conversations instead of a specific type of conversation.
rastkar et al.
demonstrated how to apply murray and carenini s multi modal conversation summarization to software artifacts.
using the same set of attributes rastkar automatically generated extractive summaries for bug reports.
they manually annotated bug reports by marking sentences in the bug reports that belonged to extractive summaries.
they recruited three programmers to annotate each bug report and then used a voting procedure to choose a gold set of annotations for each bug report.
the performance of the classification was precision and recall.
a user study with human experts found that this performance level was acceptable with the experts rating .
.
that the summaries represented the important points of the bug report.
iii.
f ield observ a tions we conducted a series of field observations to create a corpus of meeting records between developers and customers.
this section describes our methodology for collecting these observations.
this section also describes our qualitative analysis of the observations including our research questions for the qualitative study and our annotation procedure.
a. research questions our research objective in this qualitative evaluation is to test the hypothesis that conversations between developers and customers contain role function and rationale information.
this hypothesis stems from existing literature that emphasizes the importance of customer developer conversations in user story elicitation .
specifically it is important that we know whether our dataset contains this information to prepare for our quantitative analysis in section vi.
therefore we ask the following research questions rqs rq 1what percent of the turns in the conversations contain role information function information and rationale information related to user stories?
rq 2what percent of the turns in the conversations belong to extractive summaries of those conversations?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i a comparison of the size of our corpus to related work.
related experiment corpus speech of conversations of turns of sentences murray and carenini enron emails n a murray and carenini ami meetings hours rastkar et al.
bug reports n a this paper devel customer meetings hours approximate.
this is the count of sentences recorded by a transcriptionist.
see section ii b. the purpose of rq 1is twofold.
first while it is widely accepted that the elicitation of user stories should result from these conversations it is possible that not all of the information necessary to write a user story is contained in these conversations.
if the information is not in the conversations then it is not possible to automatically extract it and there is no reason to study it in our quantitative analysis in section vi.
second that quantitative study depends on the turns being annotated as having role function and or rationale information.
answering rq 1provides the opportunity to complete this annotation.
we ask rq 2in order to provide a mechanism for comparing the performance of our classifier section v to previous work.
it is not possible for us to release the records of conversations we collect which means that we will need to use a public dataset to compare our classifier to earlier ones.
however our technique is designed for user story information while the available public datasets have only extractive summaries annotated.
therefore we annotate our dataset with extractive summaries to maintain a consistent baseline.
b. methodology this section describes our methodology for answering our research questions.
we first describe how we created the corpus of meeting records.
second we describe how we annotated that corpus with role function and rationale data as well as extractive summaries.
c. data collection the data that we collect includes recordings of meetings between developers and customers.
to collect this data the first author spent two months working as a software engineering intern at a software development company in the united states with between and employees for privacy purposes the company is anonymous .
during this time she was invited to regular stand up meetings and teleconferences with the client.
these meetings consisted of discussions about progress on current tasks and plans for future tasks.
after obtaining appropriate permission the author observed took notes and recorded audio during each meeting.
in total nine meetings between developers and customers were recorded.
we then hired a professional transcriptionist to create written transcripts of the audio for each meeting.
the transcripts contained an entire meeting s dialogue.
however each meeting contained several conversations each of which had a separate topic with separate people for instance people joining or leaving a conference call as theyare needed for each topic .
therefore we manually separated each transcript into a set conversations.
we understand that this is a subjective process so we used the following criteria to minimize any bias during conversation separation a current speaker s leaves the conversation a new speaker s joins the conversation or a noticeable shift in topic such as switching to a new bug.
after this process was complete we had conversations total over the nine meetings.
our dataset is comparable in size to datasets for related experiments as shown in table i. d. annotation process all of the authors annotated the conversations.
each conversation was manually annotated by at least authors.
we restricted annotation to authors only in order to maintain privacy of the speakers.
the assignment of conversations to annotators was random to ensure an unbiased assignment.
the authors then annotated each of their conversations but were careful not to discuss any annotations with each other to avoid introducing a bias.
every annotation used the following format file the file name of the transcript conversation an id number for the conversation abstractive an in your own words summary extractive a list of turns summarizing the transcript role a list of turns summarizing the role function a list of turns summarizing the function rationale a list of turns summarizing the rationale note that the annotations include an abstractive summary.
we included this summary as an exercise to help the annotators understand the content of the conversation but otherwise the abstractive summary was not used in our experiments.
once annotations were complete we combined them using a voting process to create a final goldset annotation for each conversation.
the voting process was a simple majority we selected turns for the goldset if those turns were selected by at least two of the three annotators for that conversation.
e. threats to v alidity one threat to the validity of these observations is that much of the conversation was over the phone.
this caused some of the conversation to be slightly garbled meaning the transcriber to may have misheard some of the conversation.
although possibly causing some turns to be slightly wrong in our view this threat is minimal since we did not encounter evidence authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
that the errors would significantly change the meaning or context of the conversation.
to minimize this threat we hired a professional transcriptionist with experience handling difficult audio.
another threat with these observations is the subjective nature of manual annotations.
we attempt to mitigate this threat with the voting process of a majority consensus among the set of authors who annotated each conversation.
with this consensus check the final combined annotation is less likely to be biased.
in addition we compute the metric pyramid precision see section vi c which computes precision weighted for the number of votes for each turn.
finally one threat to validity is the source of the data we collected.
it is possible that our dataset is not representative of typical meetings between developers and customers.
we aim to mitigate this threat by recording actual meetings at an active software development company unlike the ami meeting corpus which is simulated .
still it is possible that our partner company is different from other companies in a way that would affect our results.
iv .
f ield observ a tions results in this section we present our answer to each research question as well as our rationale and interpretation of the answers.
these answers are the basis for the quantitative study discussed later in the paper.
a.rq turn information we found that out of the total turns .
of turns contained function information .
of turns contained rationale information and only .
of turns contained role information see figure .
our perception of why role information was missing from conversations is that during these regular meetings the employees already had a understanding of their roles within these projects.
therefore there was no need to identify each participant s role during the meeting in order for the speakers to complete their assigned tasks.
in contrast function and rationale information is present since it is usually unknown before the task is discussed in the meeting.
therefore we only used function and rationale information during the quantitative study.
b.rq extractive summaries for extractive summaries produced from our annotations we found that .
of turns belong to extractive summaries of their respective conversations see figure .
for comparison murray and carenini found .
of their turns to be included within the extractive summaries for the ami meetings and .
of their sentences included in the enron email summaries .
also rastkar et al.
found .
of the bug report turns were included in the extractive summaries .
in our view our findings are consistent with the findings from the ami meeting conversations.
fig.
v enn diagram of the percentages of turns that are included in each category.
the categories are function f rationale a and extractive summary e .
for example .
of turns were marked as including function information only while .
of turns had both function and extractive summary information only.
v. o urapproach our approach is essentially a machine learning classifier in which we classify turns in speech as either having user story information or not.
we build two classifiers for each type of data that we extract two for function information and two for rationale information.
the two classifiers are based on two different algorithms.
we also build two classifiers to create extractive summaries of the conversations for purposes of comparison to related work.
note that because of the very small amount of role information in our dataset see section iv a we do not attempt to extract that type of information.
our approach is inspired by previous work see section ii d but has numerous differences that we describe in this section.
a. data preparation any technique using supervised machine learning will depend on prepared data in the form of labels for the items to be classified.
in our case we labeled every turn in every conversation as containing role function and or rationale data as well as whether the turn belongs to an extractive summary.
we completed this annotation process as part of our field observations in section iii.
we use the same annotations in this section.
this annotation process is depicted in figure we describe our process of obtaining the transcripts in area in section iii c and the annotations in area in section iii d. b. attributes we use a set of attributes in each of classifier that we build figure area .
table ii provides a brief description authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of each attribute.
these attributes are in general similar to the attributes proposed by murray and carenini and later used by rastkar et al.
and due to space limitations we refer readers to those publications for complete details.
however a key distinction is that the unit of analysis in our work is a turn not a sentence as described in section ii b. a few of the attributes are identical such as spau which is the time between the current turn and the following turn.
also a few attributes such as tloc the position of a sentence in its turn are nonsensical when calculated on turns instead of sentences.
but most attributes could be modified slightly for example thisent .
in murray and carenini s paper thisent is the entropy of the current sentence.
in our work it is the entropy of the current turn.
from these classification attributes fall into four categories length structural participants and lexical.
our attributes still fit into these categories.
slen andwc are in the length category cloc ppau spau tpos1 and tpos2 are in the structural category and begauth and dom are in the participant category.
the remaining attributes which are based on probability and entropy of textual data are contained within the lexical category.
we have added two new attributes pent empt and sent empt .
these are when two of the entropy based attributes pent and sent equal zero.
this means that either all previous turns or all subsequent turns respectively have fig.
overview of our approach.
we completed steps and while answering rq 1andrq 2ins section iii.
steps and are described in section v.table ii list of attributes we calculate.
attribute description begauth first participant in convo cent1 cos. of turn and convo.
w sprob cent2 cos. of turn and convo.
w tprob cloc position in convo.
cos1 cos. of convo.
splits w sprob cos2 cos. of convo.
splits w tprob cws rough cluewordscore dom participiant dominance in words mns mean sprob score mnt mean tprob score mxs max sprob score mxt max tprob score pent entropy of convo.
before the turn pent empt no entropy in convo.
before the turn ppau time btwn.
current and prior turn sent entropy of convo.
after the turn sent empt no entropy in convo.
after the turn slen word count in turn globally normalized sms sum of sprob scores smt sum of tprob scores spau time btwn.
current and next turn thisent entropy of current turn tpos1 time from beg.
of convo.
to turn tpos2 time from turn to end of convo.
wc word count in turn not normalized no entropy at all.
these attributes are simpler descriptors of the turns than their numeric counterparts which may provide better classifications for sparser data.
these new attributes still fall under the lexical category.
prior to training with each algorithm we scaled the attribute values to ensure they would be between and to avoid some attributes from dominating the others .
c. adaption for low incidence data one challenge with our dataset is that the incidence of function and rationale information is relatively low.
in section iv a we found that function data was present in .
of turns and rationale data in .
the turns in the groups labeled as having function and or rationale data are a small minority class.
a common problem in supervised machine learning of low incidence data is that the algorithm may predict that everything is in the large class while ignoring the minority class .
we used the smote algorithm to address this problem figure area .
smote works by oversampling the minority class by creating synthetic examples of the minority class and adding those examples to the dataset until the minority class is equal in size to the larger class.
smote has been shown to have generally good performance outperforming duplicative oversampling as well as undersampling of the majority class for many datasets .
d. creating the prediction models we built two prediction models for each of the types of information we extract two for function data two for rationale data and two for extractive summaries.
one model is based on the algorithm support v ector machines with an rbf kernel and the other model is based on the algorithm logistic regression figure area .
we used logistic authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
regression for two reasons first it is the algorithm that murray and carenini and rastkar et al.
used to obtain reasonable performance.
second logistic regression is a probabilistic classifier which means that it returns a probability that each turn belongs to a class.
the advantage to these probabilities is that the size of the predicted class can be set with a cutpoint the top nturns can be selected.
as a contrast we also used the svm rbf algorithm which is a prominent binary classifier.
in section vi we compare the performance of these algorithms on our dataset.
e. reproducibility and implementation to facilitate reproducibility and to assist other researchers we release our prediction models as well as our complete implementation via an online appendix2.
our implementation is built using scikit learn as well as custom scripts to parse the transcripts and calculate the attributes.
these are all available online along with a virtual machine image with all dependencies installed to demonstrate how it is used.
we also release the prediction models that we created as part of the procedure depicted in figure area .
note that these are models trained on the entire corpus of conversations in our dataset they are not the models we use for testing in the cross validation experiments in the next section.
we release these models in lieu of the transcripts since we cannot release the transcripts for privacy reasons.
future researchers can use these models on their own datasets similar to how rastkar et al.tested a model on their own dataset that was created by murray and carenini .
vi.
c ross valida tion experiment this section describes our quantitative study which is a cross validation experiment to evaluate our approach.
we cover our research questions our methodology and metrics for answering those questions and threats to validity.
a. research questions our objective with this quantitative study is to determine the degree to which the classifiers we train are able to extract function and rationale information as well as extractive summaries.
as in section v we do not include role information because our dataset contains so little of it see section iv a .
we pose the following two questions rq 3what is the performance of the best performing configuration of our approach in terms of the metrics in section vi c?
rq 4which attributes are the most informative for the classification task?
we ask rq 3because there are several potential configurations of our approach and we seek the highest performing configuration.
the configuration is the algorithm svm rbf vs logistic regression and the classification threshold for logistic regression.
we ask rq 4for a similar reason.
we prodeghe projects userstories use attributes in our approach and several of these we adapted for turn based analysis instead of sentence based.
some attributes may be more useful for classification than others.
it is beneficial for us to report the usefulness of each attribute because it may be possible for future researchers to simplify the approach by removing some less useful attributes without significantly harming performance.
b. methodology the methodology we follow is depicted in figure .
generally speaking we follow a leave one conversation out procedure.
the typical cross validation process is either a leave one out or an n fold validation.
leave one out usually means train all items in the dataset then test on one.
in ann fold validation nitems are randomly selected as the testing set.
in our case the items in the dataset are turns.
in our view the typical leave one out and n fold validation do not reflect realistic scenarios since the turns in one conversation almost never have any direct affect on the turns in another conversation.
the realistic scenario is that a researcher has mconversations and trains a classifier on those conversations.
then the researcher receives a new conversation and classifies that conversation.
fig.
outline of our cross validation experiment procedure.
note that the procedure is slightly different than the creation of the prediction models shown in figure .
in the experiment we create a new model for each cross validation fold.
in each fold we train on all conversations except one and then test on the one remaining conversation.
with conversations our experiment has folds.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
to emulate that realistic scenario we use a leave one conversation out process in which we set aside one conversation as the test set and create a training set using the remaining conversations figure area .
note that we use the smote procedure section v c on the training set only area to avoid biasing the experiment smote changes the dataset and a bias could occur if we modify testing data.
next we train both svm rbf and logistic regression algorithms area .
we list them together in figure to emphasize that we train both but the algorithms are independent.
they do not share data and we do not attempt to combine the models that they create area .
we use each model to test the conversation that we left out of the training set and compute our performance metrics during that test area .
we repeat this process for every conversation.
c. metrics forrq we calculate the standard machine learning performance metrics precision recall true positive rate tpr and false positive rate fpr .
we also compute pyramid precision which is similar to precision except that it considers the number of human annotators who marked each turn.
in pyramid precision a prediction model is rewarded for predicting turns that have been annotated by more people as belonging to a class.
pyramid precision is useful in cases where a goldset is created by multiple people who may disagree and was used by murray and carenini and rastkar et al.
.
note that logistic regression generates predictions based on a probability threshold turns with probabilities above the threshold are predicted as part of the class.
that is in contrast to svm rbf which provides binary predictions see section v d .
therefore for svm rbf we report the average of the performance metrics over all folds of the cross validation.
but for logistic regression we report the optimal probability threshold.
forrq we calculate the standard metric f score for each attribute.
f score is commonly used in feature selection to determine which attributes are the most informative for a classification task.
we report f score for both svm rbf and logistic regression.
d. threats to v alidity one threat to validity to this study is our data source as mentioned in section iii e. there is a possibility that the data set we use is not representative of typical developercustomer meetings.
however this threat was mitigated by recording several real meetings taking place at an active software development company.
another threat to validity exists in the selection of attributes.
although it is a relatively large set and most were used in previous studies they may not represent all usable attributes for classification tasks.
in addition the conversion of certain attributes from sentence based to turn based may have affected the usefulness of those attributes.
this is handled however by the modeling itself and the analysis performed in section vii b.vii.
c ross valida tion results in this section we present our answer to each research question as well as our rationale and interpretation of the answers.
a.rq best configuration we found the best configuration for classification is logistic regression threshold see table ii though the performance of the algorithms is similar.
logistic regression outperforms svm rbf with the more traditional metrics of precision recall true positive rate tpr and false positive rate fpr .
the tpr and fpr are represented together as the receiver operating characteristic area under the curve auroc score.
however with the pyramid precision pp metric which takes the number of annotators into account svm rbf performs better with two of the three types of information.
table iii metric results using our corpus.
details shown are type classifier threshold where applicable class.
precision prec.
recall rec.
pyramid precision pp and auroc score.
type class.
prec.
rec.
pp auroc e lr .
.
.
.
e svm .
.
.
.
f lr .
.
.
.
f svm .
.
.
.
a lr .
.
.
.
a svm .
.
.
.
with the extractive summary e information the logistic regression model performed the best using traditional metrics as mentioned above with a precision of .
a recall of .
and a auroc score of .
.
in terms of pyramid precision the svm model with rbf kernel outperformed logistic regression with a pp of .
.
although the pp was higher for svm rbf it still did not overcome the logistic regression s precision which we believe means that logistic regression outperformed svm rbf overall.
we observe a similar pattern with function f information.
the logistic regression model performed the best using traditional metrics with a precision of .
a recall of .
and a auroc score of .
.
in terms of pyramid precision the svm model with rbf kernel outperformed logistic regression with a pp of .
though the results are extremely close.
logistic regression outperforms svmrbf overall though the margin is not large enough to justify a strong recommendation either way.
with the rationale a information as the type the logistic regression model outperforms the svm model for all metrics.
this configuration had a precision of .
a recall of .
a pp of .
and a auroc of .
.
with this configuration there are a couple differences to note other than logistic regression having a better pp than svm rbf.
one observation is that this is the only configuration where the pp measures higher than the precision.
another observation authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a logistic regression threshold b svm rbf fig.
f scores of the attributes after running both models on our corpus.
a includes the logistic regression model using extractive summary e function f or rationale a information.
b is the same for the svm model using the rbf kernel.
is that the roc score is highest with this a information than with the e and f information.
we believe all three of these observations are due to the lower incidence of rationale information compared to extractive summary and function information.
with less data to use the traditional prediction becomes more difficult but the metrics that make use of other data that balances them become more useful.
from all these configurations the best overall configuration for the classification task is the logistic regression model especially for the extractive summary information.
as mentioned in section vi c logistic regression uses a probability threshold to determine classification.
this configuration uses a probability threshold of about .
which produces a lower fpr at the cost of a lower tpr.
in terms of the most closely related work section ii d we caution that our data is different transcripts vs. bug reports and emails and that our analysis is turn based rather than sentence based.
still the previous results provide a rough basis for comparision a sanity check .
the murray and carenini ami meeting experiments produced a best case pyramid precision of .
and auroc score of .
.
the murray and carenini email experiments produced a best case pyramid precision of .
and auroc score of .
.
the murray and carenini studies did not report precision and recall but they did mention that they recorded high precision and low recall values.
the rastkar bug report experiments produced a best case pyramid precision of .
an auroc score of .
a precision of .
and a recall of .
.
while these numbers are not directly comparable we do note that high precision and low recall is a common feature of the approaches and that rastkar et al.
reported that this was acceptable during a study with human experts .b.rq best attributes we found of the attributes to consistently be the most informative for this classification task across both models and all three types of turn information.
as shown in figure these attributes tended towards higher f scores see section vi c compared to other attributes.
these attributes are cent1 cent2 slen sms smt spau andwc and are briefly described in table ii.
as explained in section v b these attributes can be separated into four distinct categories length structural participants and lexical.
of these most informative attributes are in length is in structural are in participants and are in lexical.
it is important to note that attributes within the lexical category perform the best in this classification task which we believe is because they are created using the most information.
another important observation is that the attributes representing participant specifics have little to no effect on the classification with our data.
we believe that is because the speech in each conversation is so broken between speakers that it is difficult for the algorithm to use it effectively.
we also found somewhat informative attributes cloc structural cos1 lexical cos2 lexical and cws lexical .
as can be seen in figure these attributes do not get as high f scores as often as the other but they do occasionally score higher than the strictly non informative attributes.
among all the configurations the one containing the highest f scores for the informative attributes is the logistic regression model with the extractive summary information.
this is the same configuration shown in section vii a. it is our view that our most informative set is comparable to the sets produced by the previous studies mentioned in section ii d. all three experiments agree that cent1 cent2 cws slen sms andsmt are informative for classifica57 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
tion.
the murray and carenini ami experiments and email experiments additionally have the mxs andmxt attributes included.
also the murray and carenini ami experiments and the rastkar bug report experiments list the slen2 attribute as informative.
compared to our set almost everyting is in agreement.
we do not include cws slen2 mxs ormxt .
however cws was somewhat informative with our approach andslen2 was not included in our usable attributes to begin with because of our use of the turn conversation system.
viii.
ami e xperiments as mentioned in section i we duplicated our quantitative experiments section vi for extractive summaries on the public ami meeting corpus .
since we cannot ethically release any of our meeting recordings from the active software company these experiments on the ami corpus serve as a reproducible example of our work.
although the ami dataset uses artificially created meetings and conversations it is a popular dataset due to its size and depth of information.
for these experiments we followed the same approach as outlined in figure except that there were many more folds since the api corpus contains more conversations.
however since the ami dataset only includes extractive summaries we do not use function and rationale information for these experiments.
we have made our implementation and this example available via an online appendix see section v e .
using the same metrics we used in our quantitative study see section vi c we found the metric values to be more split between the two models as can be seen in table iv.
for two of the metrics logistic regression outperforms svmrbf with a recall of .
and an auroc score of .
.
the receiver operating characteristic area under the curve auroc score is a combined representation of the true positive rate tpr and the false positive rate fpr .
for the other two metrics the svm model with the rbf kernel outperforms the logistic regression model with a precision of .
and a pyramid precision of .
.
in general both algorithms had generally similar performance so from our view future researchers may choose either algorithm depending on other factors affecting their experiments.
also we observed that the same attributes that were found to be most informative in our study were most informative in terms of f score cent1 cent2 slen sms smt spau andwc.
one may observe that our ami experiments produced a best case pyramid precision of .
and auroc score of .
and the murray and carenini ami experiments produced a best case pyramid precision of .
and auroc score of .
.
as in section vii we caution that these numbers are table iv metric results using the ami meetings corpus.
details shown are type classifier class.
precision prec.
recall rec.
pyramid precision pp and auroc score.
type class.
prec.
rec.
pp auroc e lr .
.
.
.
e svm .
.
.
.576not comparable since the unit of analysis is different turns vs. sentences .
we reiterate that we include this section only to observe trends and to provide a reproducible baseline for our approach.
our intent is that future researchers in the area of summarization and user story generation can build and test their approach on this reproducible baseline of public data to verify that their approach functions similarly to ours.
ix.
d iscussion and conclusion in this paper we advance the state of the art in two key directions.
first we contribute the analysis of actual user story conversations to the software engineering literature.
although we cannot publish the conversations themselves due to privacy concerns previous work in user story analysis used the ami meeting corpus which was artificially created.
in our qualitative study section iv we found that about .
of the turns included function information .
discussed rationale but only .
discussed role.
about .
were part of the extractive summaries.
from these results we have two key findings while function and rationale information are available for analysis in real conversations role information is not and in terms of extractive summary information the artificial ami dataset is a comparably useful source of information as an active meeting dataset.
second we create a novel approach that extracts user story information from transcripts of spoken conversations.
in our quantitative study section vii we obtained approximately .
precision and .
recall for detecting sections of conversations containing function data and .
precision and .
recall for rationale data.
for comparison purposes we obtained about .
precision and .
recall for extractive summaries.
compared to similar previous studies we found our results to be comparable in most cases and better in others most notably with the pyramid precision metric.
with these results this new approach provides an extra option for researchers if they feel their data is either more appropriately broken into turns or simply cannot be represented as full sentences.
acknowledgment we strongly thank karen hooge michalka and dr. lyn spillman from the university of notre dame department of sociology for their advice in preparing the qualitative study in this paper.
we also thank teacher michael dimino of the concord public school district for participating in portions of this research.
we hugely appreciate the active software company for allowing the recording of several of their customer meetings as well as the many employees who allowed the collection of their data.
this work was partially supported by the nsf grants ccf and dge and the office of naval research grant n000141410037.
any opinions findings and conclusions expressed herein are the authors and do not necessarily reflect those of the sponsors.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.