neural augmented static analysis of android communication jinman zhao university of wisconsin madison usa jz cs.wisc.eduaws albarghouthi university of wisconsin madison usa aws cs.wisc.eduvaibhav rastogi university of wisconsin madison usa vrastogi wisc.edu somesh jha university of wisconsin madison usa jha cs.wisc.edudamien octeau google usa docteau google.com abstract we address the problem of discovering communication links between applications in the popular android mobile operating system an important problem for security and privacy in android.
any scalable static analysis in this complex setting is bound to produce an excessive amount of false positives rendering it impractical.
to improve precision we propose to augment static analysis with a trained neural network model that estimates the probability that a communication link truly exists.
we describe a neural network architecture that encodes abstractions of communicating objects in two applications and estimates the probability with which a link indeed exists.
at the heart of our architecture are type directed encoders tde a general framework for elegantly constructing encoders of a compound data type by recursively composing encoders for its constituent types.
we evaluate our approach on a large corpus of android applications and demonstrate that it achieves very high accuracy.
further we conduct thorough interpretability studies to understand the internals of the learned neural networks.
ccs concepts software and its engineering automated static analysis computing methodologies neural networks keywords neural networks type directed encoders android inter component communication acm reference format jinman zhao aws albarghouthi vaibhav rastogi somesh jha and damien octeau.
.
neural augmented static analysis of android communication.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
static analyzermay linksmust linksmachine learning algorithmlearned modellink inference neural network linn type directed encodertype directed encoderclassifieruse learned model to compute probability that a may link is positivesuite of android applicationsfigure overview of our approach introduction in android the popular mobile operating system applications can communicate with each other using an android specific messagepassing system called inter component communication icc .
misuse and abuse of iccmay lead to several serious security vulnerabilities including theft of personal data as well as privilege escalation attacks .
indeed researchers have discovered various such instances a bus application that broadcasts gpslocation to all other applications an sms spying application that is disguised as a tip calculator amongst others.
thus it is important to detect inter component communication can component ccommunicate with component d?
we say that canddhave a link.
with the massive size of android s application market and the complexity of the android ecosystem and its applications a sound and scalable static analysis for link inference is bound to produce an overwhelmingly large number of false positive links.
realistically however a security engineer needs to manually investigate malicious links and so we need to carefully prioritize their attention.
to address false positives in this setting recently octeau et al .
presented primo a hand crafted probabilistic model that assigns probabilities to icclinks inferred by static analysis thus ordering links by how likely they are to be true positives.
we see multiple interrelated problems with primo s methodology first the model is manually crafted constructing the model is a laborious error prone process requiring deep expert domain knowledge in the intricacies of android applications the iccsystem and the static analysis at hand.
second the model is specialized hence changes in the android programming framework which is constantly evolving or the static analysis may render the model obsolete requiring a new expert crafted model.
esec fse november lake buena vista fl usa j. zhao a. albarghouthi v. rastogi s. jha and d. octeau model augmented link inference our high level goal is toautomatically construct a probabilistic model that augments static analysis using minimal domain knowledge .
to achieve this we view the link inference problem through the lens of machine learning.
we make the observation that we can categorize results of a static analysis into must andmay categories links for which we are sure whether they exist or not must links and others for which we are unsure may links .
we then utilize must links to train a machine learning classifier which we then apply to approximate the likelihood of a may link being a true positive.
figure presents an overview of our proposed approach.
link inference neural networks to enable machine learning in our setting we propose a custom neural network architecture targeting the link inference problem which we call link inference neural network linn .
the advantages of using a neural network for this setting and generally in machine learning is to automatically extract useful features of link artifacts specifically the addressing mechanism in icc .
we can train the network to encode the artifacts into real valued vectors that capture relevant features.
this relieves us from the arduous and brittle task of manual feature extraction which requires i expert domain knowledge and ii maintenance in case of changes in android iccor the used static analysis.
the key novelty of linn sis what we call a type directed encoder tde a generic framework for constructing neural networks that encode elements of some type into real valued vectors.
we use tde s to encode abstractions of link artifacts produced by static analysis which are elements of some data type representing an abstract domain .tde sexploit the inherent recursive structure of a data type we can construct an encoder for values of a compound data type by recursively composing encoders for its constituent types.
for instance if we want to encode elements of the type int string then we compose an encoder of intand an encoder of string .
we demonstrate how to construct encoders for a range of types including lists sets product types and strings resulting in a generic approach.
our tdeframework is parameterized by differentiable functions which can be instantiated using different neural network architectures.
depending on how we instantiate our tdeframework we can arrive at different encoders.
at one extreme we can use tde sto simply treat a value of some data type as its serialized value and use a convolutional cnn orrecurrent neural network rnn to encode it at the other extreme we show how tde scan be instantiated as a tree lstm an advanced architecture that maintains the tree like structure of a value of a data type.
this allows us to systematically experiment with various encodings of abstract states computed by static analysis.
contributions we make the following contributions model augmented link inference we formalize and investigate the problem of augmenting a static analysis for android iccwith an automatically learned model that assigns probabilities to inferred links.
link inference nn framework we present a custom neural network architecture link inference neural network linn for augmenting a link inference static analysis .
at the heart of linn saretype directed encoders tde a framework that allowsus to construct encoder neural networks for a compound data type by recursively composing encoders of its component types.
instantiations empirical evaluation we implement our approach using tensorflow and ic3 and present a thorough evaluation on a large corpus of 500android applications from the google play store.
we present multiple instantiations of our tde s ranging in complexity and architecture.
our results demonstrate very high classification accuracy.
significantly our automated technique outperforms primo which relied on 6to9months of manual model engineering.
interpretability study to address the problem of opacity of deep learning we conduct a detailed interpretability investigation to explain the behavior of our models.
our results provide strong evidence that our models learn to mimic the linkresolution logic employed by the android operating system.
android icc overview definitions we now i provide relevant background about android icc and ii formally define intents filters and their static abstractions.
.
icc overview and examples android applications are conceptually collections of components with each component designed to perform a specific task such as display a particular screen to the user or play music files.
applications can leverage the functionality of other applications through the use of a sophisticated message passing system generally referred to as inter component communication icc .
to illustrate say an application developer desires the ability to dial a phone number.
rather than code that functionality from scratch the developer can instead send a message requesting that some other application handle the dialing process.
further such messages are often generic i.e.
not targeted at a specific application.
the same communication mechanism is also used to send messages within an application.
consequently any inter component or inter application program analysis must first begin by computing the icclinks.
intents figure 2a illustrates a simplified iccexample with a ridesharing application rideapp that uses functionality of a map application mapapp and an sms messaging application smsapp .
each application comprises components gray boxes and arrows in the figure represent potential links between components.
iccis primarily accomplished using intents .
intents are messages sent between android components.
an intent can be either explicit orimplicit .
the former specifies the target application and component the latter merely specifies the functionality it requires from its target.
consider for instance the implicit intent in figure 2a it requests the action send send an sms which sends smsdata.
by issuing this intent figure 2b top rideapp is able to send a message without having to worry about how this action is performed and by whom.
intent filters components that wish to receive implicit intents have to declare intent filters filters for short which describe the attributes of the intents that they are willing to receive i.e.
subscribe to intents of those types .
for instance filter in figure 2a specifies that smsapp can handle send andview actions figure 2b bottom shows the code declaring this filter .
therefore when the ride sharing application issues an intent with a send action 343neural augmented static analysis of android communication esec fse november lake buena vista fl usa rideapp ride sharing application smsapp sms applicationmapapp map application searchactivity msgactivityaction viewdatascheme geo action senddatascheme smsmapactivityactions view datascheme geo sendactivityactions send view datascheme smstargetapp rideapptargetcomp searchactivityicc link targetapp rideapptargetcomp msgactivityintent implicit intent explicit intent implicit intent explicit filter filter applicationcomponent a iccexample with three applicationspublic void sendimplicitintent intent intent new intent intent.setaction send msg ... contains phone and msg intent.setdata msg startactivity intent code constructing and starting implicit intent intent filter action android name send action android name view data android scheme sms category android name default intent filter intent filter for a smscomponent b intent for sending an smsand associated filter figure icc example android s intent resolution process matches it with smsapp which offers that functionality.
security and privacy issues arise for instance when malicious applications are installed that intercept sms messages by declaring that they also handle send actions .
.
intents filters and their abstraction we now formalize intents and filters.
we use to denote the set of all strings and to denote an undefined value null .
definition .
intents .
anintent is a pair act cats where act is a string defining the action of e.g.
the string send in figure 2b top or the value .
cats 2 is a set of strings defining categories which provide further information of how to run the intent.
for instance app browser means that the intent should be able to browse the web if an intent is created without providing categories cats is instantiated into the singleton set default .
practically intents also contain the issuing component s name as well as data like a phone number image or email.
we elide these other fields because based on our dataset they provided little information less than of intents filters have fields other than actions and categories and even when they have them the values have few distinct possibilities.
it is however conceivable that as we investigate more applications other fields will become important.
definition .
filters .
afilter fis a tuple acts cats where acts 2 is a set of strings defining actions a filter can perform.
cats 2 is a set of category strings see definition .
.
like intents filters also contain more attributes but for our purposes actions and categories are most relevant.
semantics we will use iandfto denote the sets of all possible intents and filters.
we characterize an android application aby the potentially infinite set of intents ia iand filters fa fit can create i.e.
the collecting semantics of the application.
lety indicating whether a link exists or not .
we use the matching function match i f yto denote an oracle that given an intent and filter f determines whether f can ever match at runtime i.e.
there is a link between them followingthe android intent resolution process.
we refer to octeau et al .
for a formal definition of match .
example .
.
consider the intent and filter described in figure 2b.
the intent carries a send action which matches one of the actions specified in the filter.
the msgin the intent it has an smsdata scheme so that it matches the filter data scheme.
as for the category the android apistartactivity initializes the categories of the intent to the set default .
this then matches the filter s default category.
thus the intent and filter of figure 2b match successfully.
static analysis we assume that we have a domain of abstract intents andabstract filters denoted i andf respectively.
semantically each abstract intent i denotes a potentially infinite set of intents in i and the same analogously holds for abstract filters.
we assume that we have a static analysis that given an application a returns a set of abstract intents and filters i aandf a. these overapproximate the set of possible intents and filters.
formally for every ia there exists an abstract intent i asuch that and analogously for filters.
provided with an abstract intent and filter say from two different apps the static analysis employs an abstract matching function match i f which determines that there must be link between them value there must be no link value or maybe there is a link value .
the more imprecise the abstractions f are the more likely that match fails to provide a definitive must answer.
we employ the ic3 tool for computing abstract intents and filters and the primo tool for abstract matching which reports must ormay results.
an abstract intent computed by ic3 has the same representation as an intent it is a tuple act cats except that strings in actand catsare interpreted as regular expressions.
therefore represents a set of intents one for each combination of strings that matches the regular expressions.
the same holds for filters.
for example an abstract intent can be the tuple .
send default .
this represents an infinite set of intents where the action string has the suffix send .
344esec fse november lake buena vista fl usa j. zhao a. albarghouthi v. rastogi s. jha and d. octeau model augmented link inference we now view link inference as a classification problem.
a probabilistic view of link inference recall that i is the set of abstract intents f is the set of abstract filters and y indicates whether a link exists.
we can construct a classifier a function h i f which given an abstract intent and filter f h f indicates the probability that a link exists the probability that a link does not exist is h f .
in other words h f is used to estimate the conditional probability that y link exists given that the intent is and the filter is f that is h estimates the conditional probability p y f .
there are many classifiers used in practice e.g.
logistic regression .
we focus on neural networks as we can use them to automatically address the non trivial task of encoding intents and filters as vectors of real values a process known as feature extraction .
training via static analysis the two important questions that arise in this context are i how do we obtain training data and ii how do we represent intents and filters in our classifier?
training data we make the observation that the results of a sound static analysis can infer definite labels must information of the intents and filters it is provided.
thus we can i randomly sample applications from the android market and ii use static analysis to construct a training set d i f y d f y1 .
.
.
n f n yn comprised of intents and filters for which we know the label y with absolute certainty because they are in the must category for the static analysis.
to summarize we use static analysis as the oracle that labels the data for us.
representation for a typical machine learning task the input to the classifier is a vector of real values i.e.
the set of features .
our abstract intents for example are elements of type 2 .
so the key question is how can we transform elements of such complex type into a vector of real values?
in we present a general framework for taking some compound data type and training a neural network to encode it as a real valued vector.
we call the technique type directed encoding .
augmenting static analysis once we have trained a model h i f we can compose it with the results of static analysis to construct a quantitative abstract matching function qmatch f match f match f h f match f where when static analysis reports a may result instead of simply returning we return the probability estimated by the model h. link inference neural networks we now present our link inference neural network linn framework and formalize its components.
our goal is to take an abstract intent and filter f representing a may link and estimate the probability that the link is a true positive.
linn sare designed to do that.
alinn is composed of two primary components see figure type directed encoders tde alinn contains two different type directed encoders which are functions mapping an abstract filter tdeclassifier intent tdefilter encodingintent encoding p2 estimated probability abstract intentabstract filterrnrmfigure high level architecture of linn s intent or filter to a vector of real numbers.
essentially the encoder acts as a feature extractor distilling and summarizing relevant parts of an intent filter as a real valued vector.
type directed encoders are compositions of differentiable functions which can be instantiated using various neural network architectures.
classification layers the classification layers take the encoded intent and filter and return a probability that there is a link between intent and filter f. the classification layer we use is a deep neural network multilayer perceptrons mlp .
once we have an encoding of intents and filters in rn we can use any other classification technique e.g.
logistic regression.
however we use neural networks because we can train the encoders and the classifier simultaneously joint training .
before presenting tde sin .
we i present relevant machinelearning background .
and ii describe neural network architectures we can use in tde s .
.
.
for a detailed exposition of neural networks refer to goodfellow et al.
.
.
machine learning basics we begin by defining the machine learning problem we aim to solve and set the notation for the rest of this section.
machine learning problem recall that our training data dis a set i f i yi i whereyiis the label for intent iand filter f i. let hbe a hypothesis space a set of possible classifiers.
a loss function l h z rdefines how farthe prediction of his on some item z i f y. our goal is to minimize the loss function over the training set we thus define the following quantity ld h npn j 1l h j f j yj .
finally we solve argminh hld h which results in a hypothesish hthat minimizes ld h .
this is typically performed using an optimization algorithm like stochastic gradient descent sgd .
notation in what follows we use x y z w .
.
.to denote column vectors in rn.
we will use x y z w .
.
.to denote matrices in rn m where xiwill denote the ithcolumn vector in matrix x. neural networks neural networks are transformations described in the form of matrix operations and pointwise operations.
in general a neural network y x w maps input xto outputywith some vector wcalled the parameters of the neural network which are to be optimized to minimize the loss function.
there are also other parameterizable aspects of a neural network such as the dimensions of input and output which need to be determined before training.
those are called hyperparameters .
345neural augmented static analysis of android communication esec fse november lake buena vista fl usa .
overview of neural architectures we now briefly describe a number of neural network architectures we can use in tde s. multilayer perceptrons mlp multilayer perceptrons are the canonical deep neural networks.
they are compositions of multiple fully connected layers which are functions of the form fc x w b a w x b rm rn where wandbare parameters mis the input dimension nis the output dimension and ais a pointwise activation function .
some common activation functions include sigmoid andrectified linear unit relu respectively x e x andrelu x max x .
convolutional neural networks cnn cnn swere originally developed for image recognition and have recently been successfully applied to natural language processing .cnn sare fast to train compared to rnn s see below and good at capturing shift invariant features inspiring our usage here for encoding strings.
in our setting we use dimensional cnn swith global pooling transforming a sequence of vectors into a single vector i.e.
rn l rk where lis the length of the input and kis the number of kernels .
kernels are functions rn s rofsizesthat are applied to every contiguous sequence of length sof the input.
a global pooling function e.g.
max then summarizes the outputs of each kernel into a single number.
we use cnn sby kim et al .
for encoding natural language where kernels have variable size.
recurrent neural networks rnn rnn sare effective at handling sequential data e.g.
text by recursively applying a neural network unit to each element in the sequence.
an rnn unitis a network mapping an input and hidden state pair at position i 1in the sequence to the hidden state at position i that is rnn unit xi hi hi.
the hidden state hi rncan be viewed as a summary of the sequence seen so far.
given a sequence x rm lof length l we can calculate the final hidden state by applying the rnn unit at each step.
thus we view an rnn as a transformation in rm l rn.
practically we use a long short term memory network lstm anrnn designed to handle long term dependencies in a sequence.
recursive neural networks analogous to rnn s recursive neural networks apply the same unit over recursive structures like trees.
a recursive unit takes input xvassociated with tree node vas well as all the hidden states hc v rn c v from its children c v and computes the hidden state hvof the current node.
intuitively the vector hvsummarizes the subtree rooted at v. one popular recursive architecture is tree lstm .
adapted from lstm s a tree lstm unit can take possibly multiple states from the children of a node in the tree.
for trees with a fixed number kof children per node and or the order of children matters we can have dedicated weights for each child.
this is called a k ary tree lstm unit a function in rn k rn.
for trees with variable number of children per node and or the order of children does not matter we can operate over the sum of children states.
this is called achild sum tree lstm unit a function in rn c v rnfor node v. refer to tai et al.
for details.table types of differentiable functions used in tdes figure and possible practical instantiations encoder type possible differentiable implementations enumenc rltrainable lookup table embedding layer flat l rn rmcnn lstm aggr s rn rmsum child sum tree lstm unit comb rn rm rlsingle layer mlp binary tree lstm unit .
type directed encoders we are ready to describe how we construct type directed encoders tde .
atdetakes an abstract intent or filter and turns it into a realvalued vector of size n. as we shall see however our construction is generic we can take some type and construct a tdefor by recursively constructing encoders for constituents of .
consider e.g.
a product type comprised of an integer and a string.
an encoder for such type takes a pair of an integer and a string and returns a vector in rn.
to construct such an encoder we compose two separate encoders one for integers and one for strings.
formal definition formally an encoder for elements of type is a function rnmapping values of type into vector space rn.
for element a a is called the encoding of aby .
integer nis the dimension of the encoding denoted by dim n. the dimension nof an encoder is a fixed parameter that we have to choose a hyperparameter.
the intuition is that the larger nis the encoder can potentially capture more information but will require more data to train.
encoder construction framework in what follows we begin by describing encoders for base types reals and characters.
then we describe encoders for lists sets product types and sum types.
in our setting of iccanalysis these types appear in for example the intent action field which is a list of characters string or null and filter actions which are sets of lists of characters sets of strings .
the inference rules in figure describe how an encoder can be constructed for some type denoted by .
table lists the parameters used by a tde which are differentiable functions the table also lists possible instantiations.
depending on how we instantiate these functions we arrive at different encodersnote that tde sare compositions of differentiable functions this is important since we can jointly train them along with the classifier in linn s. example .
.
throughout our exposition we shall use the running example of encoding an abstract intent .
we will use l to denote the type of lists of elements in type and s to denote sets of elements in .
we use to denote the singleton type that only contains the undefined value .
with this formalism the type of an abstract intent is l s l .
l is a sum type denoting that an action is either a string list of characters or null.
type s l is that of categories of an abstract intent sets of strings.
the product of action and categories comprises an abstract intent.
following figure the construction proceeds in a bottom up fashion starting with encoders for base types and proceeding upwards.
for instance to construct an encoder for l we first require an encoder for type as defined by the rule e list .
346esec fse november lake buena vista fl usa j. zhao a. albarghouthi v. rastogi s. jha and d. octeau e real x.x renumenc c rn e cat enumenc c rn flat l rn rm e list x.flat map x l rn aggr s rn rm e set x.aggr map x s 1 1 rn 2 2 rmcomb rn rm rl e prod x y .comb 1 x 2 y 1 2 1 1 rn 2 2 rn e sum x.ifx 1then 1 x else 2 x 1 2 cis a categorical type e.g.
characters.
figure rules for type directed encoding.
representations of functions enumenc flat aggr comb are in table .
.
.
encoding base types.
we start with encoders for base types.
numbers since real numbers are already unary vectors we can simply use the identity function x.x to encode elements of r. this is formalized in the rule e real .
the same applies to integers.
categorical types chars we now consider types cwith finitely many values e.g.
ascii characters booleans and user defined enumerated types.
one common encoding is using a lookup table .
suppose there are kpossible values of type c namely a1 a2 .
.
.
ak.
the lookup table is often denoted as a matrix w rn k where n is the dimension of the resulting encoding.
the encoding for value ajis simply the jthcolumn wjof the matrix w. as formalized in rule e cat in figure we encode categorical types using an enumenc function which we implement as a lookup table whose elements ware learned automatically.
example .
.
in our example we need two instances of enumenc enumenc for encoding characters in and enumenc for encoding the null type .
.
.
encoding compound types.
we now switch attention to encoding compound types.
lists as formalized in rule e list to generate an encoder for l we require an encoder for type .
given an encoder rn we can apply it to every element of the list thus resulting in a list of type l rn .
then using the function flat l rn rm we can transform l rn to a vector rm.
as we indicate in table we use acnn or an lstm to learn the function flatduring training.
example .
.
to encode strings l actions we construct strenc x.flat map enumenc x l where enumenc is the encoder we construct for characters and map is the standard list combinator that applies enumenc to every element in list x resulting in a new list.
sets rule e set generates an encoder for type s in an analogous manner to e list .
given an encoder rn we can apply it to every element of the set thus resulting in a set of type s rn .
then using the aggregation function aggr s rn rm we can transform s rn to a vector rm.
as shown in table we can 1one hot encoding is a special case of the lookup table where w ik k.table instantiations of tdeparameters tdeparameters instantiation type enumenc flat aggr comb str rnn l lookup rnn str cnn l lookup cnn typed simple full lookup cnn sum layer perceptron typed tree full lookup cnn tree lstm tree lstm lookup stands for lookup table as described in .
and table .
simply set aggr to be the sum of all vectors in the set or use a child sum tree lstm unit to pool all elements of a set.
note that our treatment of sets differs from lists we do not usecnn sorlstm sto encode sets since unlike lists sets have no ordering on elements to be exploited by a cnn or an lstm .
example .
.
categories of an intent are of type s l .
given an encoder strenc forl we construct the encoder catsenc x.aggr map strenc x s l where map here applies strenc to every element in set x. sum types we now consider sum union types 1 2 where a variable can take values in 1or 2. rule e sum generates an encoder for type 1 2 it assumes that we have two encoders 1and 2 for 1and 2 respectively.
both encoders have to map vectors of the same size n.e sum generates an encoder from 1 2to a vector inrn.
if a variable xis of type 1 it is encoded as the vector 1 x .
alternatively if xis of type 2 it is encoded as the vector 2 x .
example .
.
the action field of an abstract intent is a sum type l we construct an encoder for this as follows actenc x.ifx l then strenc x elseenumenc x l product types we now consider product types of the form 1 2. rule e prod generates an encoder for type 1 2 as with e sum it assumes that we have two encoders 1and 2 one for type 1 and another for 2. additionally we assume we have a function comb rn rm rl.
the encoder for 1 2therefore encodes type 1using 1 encodes type 2using 2 and then summarizes the two vectors produced by 1and 2as a single vector in rl.
this can be performed using an mlp or using a binary tree lstm unit.
essentially we view the the product type as a tree node with children so we can summarize the children using a tree lstm unit.
example .
.
finally to encode an intent we encode the product type of an action and categories a c .comb actenc a catsenc b l s l implementation and evaluation we now describe an implementation of our technique with various instantiations and present a thorough evaluation.
we designed our evaluation to answer the following two questions q1 accuracy arelinn seffective at predicting may links and what are the best instantiations of tde sfor our task?
q2 efficiency what is the runtime performance of model training and link prediction inference ?
we begin by summarizing our findings 347neural augmented static analysis of android communication esec fse november lake buena vista fl usa table choices of hyperparameters hyperparameter choice lookup table dimension cnnkernel sizes kernel counts activation relu pooling max rnn lstm hidden size layer perceptrondimensions activation relu multilayer perceptrondimensions activation relu the best instantiation of tde slabels may links with an f1 score of .
an area under roccurve of .
and a kruskal s of .
.
all these metrics indicate high accuracy.
all instantiations complete training within 20min.
all instantiations take .2ms to label a link except the instantiation using rnn s which takes .2ms per link.
instantiations to answer our research questions we experiment with different instantiations of tde s outlined in table which are designed to vary in model sophistication from simplest to most complex.
in the simplest instantiations str rnn and str cnn we serialize abstract intents and filters as strings l and use rnn sandcnn sfor encoding.
in the more complex instantiations typed simple and typed tree we maintain the fullconstituent types for intents and filters as illustrated in .
.
the typed simple instantiatition uses the simplest choices for aggr andcomb sum of all elements and a single layer neural network respectively.
the typed tree instantiations uses child sum and binary tree lstm units instead.
hyperparameters table summarizes the choice of hyperparameters.
we choose the embedding dimension and the range of kernel sizes following a similar choice as in kim et al .
.
we choose only odd sizes for alignment consideration.
we choose relufor all activations as it is both effective in prediction and efficient in training.
for global pooling in cnn s max is used as we are only interested in the existence of a pattern rather than its frequencies.
the hidden size of rnn sis set close to the output size of cnn s so we could fairly compare the capability of the two architectures.
tree lstm s used in our instantiations require no hyperparameterization as the hidden size is fixed to the output size of the string encoder.
implementation we have implemented our instantiations in python using keras with the tensorflow backend.
we chose cross entropy as our loss function and rmsprop a variation of stochastic gradient descent sgd for optimization.
all experiments were performed on a machine with an intel core i7 .
ghz cpu 32gb memory 1tb ssd and nvidia geforce gtx gpu.
thelinn was trained and tested on the gpu.
.
experimental setup corpus we used the primo corpus consisting of android applications from google play.
for static analysis we used ic3 combined with primo s abstract matching procedure.
this combination provides a definitive must may label to pairs of abstract intents and filters.
.
.
.
.
.
false positive rate0.
.
.
.
.
.0true positive rate roc curve auc .
.
.
.
.
a receiver operating characteristic roc .
.
.
.
.
predicted probability y102103104link count log scale .
.
.
.
.
.
pr y y b distribution of predicted link probabilities figure detailed results for the typed tree instantiation simulating imprecision the challenge with setting up the training and test data is that we do not know the ground truth label of may links.
to work around that we adopt the ingenious approach of octeau et al .
who construct synthetic may links from must links by instilling imprecisions in abstract intents and filters.
this results in a set of may links for which we know the label.
formally to add imprecision to an abstract intent we transform it into aweaker abstract value wsuch that w. intuitively adding imprecisions involves adding regular expressions like .
into e.g.
action strings.
following we use the empirical distribution of imprecision observed in the original abstract intents filters to guide the imprecision simulation process.
specifically we distinguish the different types of imprecision full or partial and by each field and calculate the empirical probability for each of them as they appear in our data.
these probabilities are then used to introduce respective imprecisions in the intents and filters.
note that the introduction of imprecision does not necessarily convert a must link into a may link since even if we weaken an abstract intent the abstract matching process may still detect its label definitively.
sampling and balancing we train the model on a sampled subset of links as the number of links is quadratic in the number of intents and filters and training is costly we have intents and filters leading to over million links .
sampling may result in loss of information however as we show our models perform well.
another important aspect of sampling is balancing.
neural networks are sensitive to a balanced presence of positive and negative training instances.
in our setting links over some particular intent filter could easily fall into one of two extreme cases where most of them are positive intent is vague or most of them are negative intent is specific .
so we carefully sample links balanced between positive and negative labels and among intents filters .
sampling and balancing led to a training set consisting of links negative and positive and a testing set consisting of links negative and positive .
note that the testing set is comprised solely of may links .
.
results q1 accuracy to measure the predictive power of our four instantiations we use a number standard metrics f1 score is the harmonic mean of precision andrecall which gives a balanced measure of a predictor s false negative and falsepositive rates.
when computing f1 the model output is rounded to get a label.
a value of indicates perfect precision recall.
348esec fse november lake buena vista fl usa j. zhao a. albarghouthi v. rastogi s. jha and d. octeau table summary of model evaluations instantiation parameters inference time s link testing testing f1 auc entropy of y pr y y .
pr y .
str rnn .
.
.
.
.
.
str cnn .
.
.
.
.
.
typed simple .
.
.
.
.
.
typed tree .
.
.
.
.
.
receiver operating characteristic roc curve plots the true positive rate against the false positive rate.
it represents a binary classifier s diagnostic performance at different discrimination thresholds.
a perfect model has an area under the curve auc of .
kruskal s is used to measure the correlation between the ranking computed by our model and the ground truth on may links which is useful in our setting as in practice we would use computed probabilities to rank may links in order of likelihood to present them to the user for investigation.
a value of indicates perfect correlation between computed ranking and ground truth.
the metrics for each instantiation are summarized in table .
in addition to the aforementioned metrics we include the number of trainable parameters indicating the fitting power lower is better inference time time to evaluate one instance entropy of predicted probabilities y indicating the power of triaging links lower is better probability of true positives within links with high predicted values pr y y .
higher is better and portion of links with such high predicted values pr y .
higher is better .
we observe that the best instantiation is typed tree.
note however that using tree lstm involves the largest number of trainable parameters.
with the fewest trainable parameters and fastest running time the str cnn model achieves the highest true positive rate within the most highly ranked links.
the str rnn model is worse than others in almost all aspects suggesting that the rnn string encoder struggles to capture useful patterns from intents filters .
our results show that more complex models specifically those that use more parameters or encode more structure tend to perform slightly better.
while simple models are good enough more complex models may still lend significant advantage when we consider market scale analysis link inference has to run on millions of links and even small differences e.g.
a false positive rate difference of just .
between our two best models typed tree and typed simple would translate to thousands of mislabeled links.
figure 5a shows the roccurve over the testing dataset for the typed tree model.
figure 5b shows the distribution of the predicted probabilities of the existence of a link and the probability of a link actually existing given the predicted probability.
the curve depicting the conditional probability would ideally follow y x line.
therefore the model s prediction value is highly correlated to the true probability of being a positive which confirms with the high value we observed.
a higher value at the upper right of the conditional probability curve suggests links highly ranked y .
by our models are highly likely actual positives.
this is particularly important in saving the effort of humans involved in investigating links.
compared to primo our evaluation is over a set of may links only.
this is a strictly more difficult setting than primo s which evaluated over all links containing any imprecision so some of the links are actually must links.
in particular if a link is must not exist a partially imprecise version will also mostly be must not exist .for example a .
c does not match xyz.despite working in a more challenging setting our best instantiations are still able to achieve a kruskal s value .
.
higher than primo .
in fact if we use the same setting as primo our kruskal s reaches .
.
q2 efficiency regarding running time all our instantiations except str rnn are efficient.
as shown in table they take no more than s per link for inference.
the training time is proportional to inference time.
the three best instantiations take no more than 20min to finish epochs of training.
we consider this particularly fast given that we used only average hardware and deep neural network training often lasts several hours or days for many problems.
the only exception is str rnn which is about times slower than the slowest of the other three mainly because of the inefficient unrolling of rnn units over long input strings.
the storage costs or the model size can be measured in the number of trainable parameters.
as shown in table the largest model typed tree consists of about 634k parameters taking up .
mb on disk the smallest model string cnn consists of only about 27k parameters taking up merely 300kb on disk.
in summary even the most complex model does not have significant storage cost.
threats to validity we see two threats to validity of our evaluation.
first to generate labeled may links for testing we synthetically instilled imprecision following the methodology espoused byprimo enabling a head to head comparison with primo .
while the synthetic imprecision follows the empirical imprecision distribution we could imagine that actual may links only exhibit rare pathological imprecisions.
while we cannot discount this possibility we believe it is unlikely.
the ultimate test of our approach is in improving the efficacy of client analyses such as iccta and we intend to investigate the impact of client analyses in future work.
second with neural networks it is often unclear whether they are capturing relevant features or simply getting lucky .
to address this threat we next perform an interpretability investigation.
interpretability investigation we now investigate whether our models are learning the right things and not basing predictions on extraneous artifacts in the data.
an informal way to frame this concern is whether the model s predictions align with human intuition .
this is directly associated with the trust we can place on the model and its predictions .
our efforts towards interpretability are multifold we analyzed the incorrect classifications to understand the reasons for incorrectness .
our analysis indicates that the reasons for incorrectness are understandable and intuitively explained.
.
we analyze several instances to understand what parts of input are important for the predictions .
results show that the model is taking expected parts of the inputs into account.
.
westudied activations of various kernels inside the neural networks to see which inputs activated them the most.
our findings show 349neural augmented static analysis of android communication esec fse november lake buena vista fl usa action null constant categories null actions null constantpop dialog null constantpush dialog .
.
replace dialog .
app 00489869yb964702hupdate view categories null action null constantreplace dialog .
categories null actions categories null action .
categories null actions android.media.ringer mode changed sakurasoft.action.always lock android.intent.action.boot completed categories null action .
login success categories null actions null constantlogin fail null constantcreate payment success .
fatal error .
create payment fail null constantlogin success categories null action app 00489869yb964702hreplace dialog .
categories null actions app 00489869yb964702hlogin fail app 00489869yb964702hcreate payment fail null constantcreate payment success .
fatal error null constantlogin success categories null action com.joboevan.push.message.
.
categories null actions categories null action categories actions categories action .
categories null actions android.intent.action.media button com.ez.addon.music command android.media.audio becoming noisy categories null figure explaining individual instances several recognizable strings which intuitively should have a high influence on classification to be activating the kernels.
.
we used t sne visualizations tounderstand how a model groups similar inputs and what features it may be using .
we found several important patterns boosting our trust in the model.
.
interpreting and explaining predictions of a ml algorithm is a challenging problem and in fact a topic of active research .
our study in this section is thus a best effort analysis.
.
error inspection we present anecdotal insights from our investigation of erroneous classification in the typed simple model.
a number of misclassifications appear where the action or category fields are completely imprecise e.g.
action string is .
or too imprecise to make meaningful deductions.
the model unsurprisingly has difficulty in classifying such cases.
there are also albeit fewer misclassified instances for nearcertain matches.
examples include android.intent.action.view matching against android.intent.action .
ew andandroid.i ntent.action.get c .
ntent matching against android.inten t.action.get content as part of intents and filters .
such misses may be due to imprecision preventing encoders from detecting characteristic patterns e.g.
.
ew is seemingly very different from view .
finally our model predicts a matches for different but very similar strings.
for example consider com.andromo.dev48329.a pp53751.intent.action.feed star .
ing and com.andromo.d .
.app135352.intent.action.feed starting .
the two look similar but have different digits we explore this case more in .
.
overall our model is able to extract and generalize useful patterns.
however it probably has less grasp on the exact meaning of .
so there are failures to identify some highly probable matches.
.
explaining individual instances we would like to explain predictions on individual instances.
our approach is similar to that of the popular system lime we perturb the given instance and study change in prediction of the model treated as a black box due to the perturbation.
in this way we can explain the prediction on the given instance.
we used the str cnn instantiation and perturbed the string representationtable some cnnkernels and their top stimuli conv1d size5 conv1d size5 conv1d size7 segment activation segment activation segment activation .
r .
null .
taviewa .
.
u .
null .
n.view .
.
t .
sulle .
y.view .
of the given intent while keeping the filter string constant.
our perturbation technique is to mask delete a substring from the string.
we perform this perturbation at all locations of the string.
using a mask length of the examples in figure show intents and filters with a heat map overlaid on the intents while keeping the filters uncolored.
all the instances selected here have positive link predictions it is unlikely to perturb a negatively predicted link to result in a positive prediction .
the redder regions indicate a significant difference in prediction when masking around those regions thus indicating that the regions are important for the predictions.
the examples generally show red regions in the action and categories values which implies that those values are important for the prediction.
thus the model is generally making good decisions about what is important.
there are a couple of outliers.
first the second instance does not have the action field reddened.
this is probably because there is no direct match between the action strings in the intent and filter and it may just be the case that the model was learnt from similar examples in the training set.
the other outlier is the third last example we believe that masking any small part of the action still leaves enough useful information which the model can pick up to make a positive prediction.
overall these examples give us confidence that the model has acquired a reasonably correct understanding of intent resolution.
.
studying kernel activations we tested input strings on the str cnn instantiation to find if the cnn kernels are picking up patterns relevant to icclink inference.
we went over all intent strings in our dataset and for each cnn kernel looked for string segments that activate the kernel the most.
some representative kernels and their top activating segments are shown in table .
important segments are noticeable.
for example kernel conv1d size5 gets activated on .
and kernel conv1d size7 gets activated on view .
kernel conv1d size5 is interesting as it seems to capture null an important special value but also captures sulle .
this indicates a single kernel on its own may not be so careful at distinguishing useful vs. less useful but similar looking segments.
apart from such easily interpretable cases many other activations appear for seemingly meaningless strings.
it is likely that in combination they help capture subtle context and when combined in later layers generate useful encodings.
.
visualization of encodings we present a visualization to discover interesting patterns in encodings.
in the interest of space we only discuss intent encodings for the typed simple instantiation.
we use t sne a non linear dimensionality reduction technique frequently used for visualizing high dimensional data in two or three dimensions.
the key aspect of the technique is that similar objects are mapped to nearby points while dissimilar objects are mapped to distant points.
figure shows the encodings for all intents in the training set sub figures 350esec fse november lake buena vista fl usa j. zhao a. albarghouthi v. rastogi s. jha and d. octeau a android.intent.
b imprecise view actions c dev .app .
.feed d default total imprecise and null categories figure intent encodings visualized using t sne highlight different areas .
the size of each point reflects the number of intents sharing the same combination of values.
figure 7a highlights the intents for which the action starts with android.intent .
the top cluster is associated with category andr oid.intent.category.default and the bottom is associated with null categories.
a few different actions comprise the top cluster e.g.
android.intent.action.view android.intent.action.sen d and imprecise versions of these actions.
figure 7b zooms in on the top cluster to show six imprecise versions of the view action one top circle containing android.inte .
.view three circles containing android.intent .
n.view android.intent.
.
n.view and android.intent .
.view and two lower figure circles containing an .
ction.view and a .
action.view .
we can thus see the level of imprecision reflected spatially while the semantic commonality is still well preserved as the points are all close to the big circle of the precise value.
figure 7c highlights actions whose last part starts with feed .
the prefixes are largely different but share the pattern of dev .app where are strings of digits.
our model is thus able to extract the common pattern and generate similar encodings for these intents.
the last example is the effect of three important values of the categories field android.intent.category.default total imprecision .
and .
in figure 7d we highlight three encodings with the view action and the above categories.
the encoding for that of bottom circle is separated from the other two top circle .
this conforms with the fact that total imprecision is the imprecise version of some concrete value which is most likely default rather than the absence of the value for the categories field which means ignoring the field during resolution.
our observations suggest that our intent encoder is able to automatically learn semantic as opposed to merely syntactic artifacts.
related work android analysis the early comdroid system inferred subsets of iccvalues using simple ad hoc static analysis.
epicc constructed a more detailed model of icc.ic3 and bosu et al .
improved on epicc by using better constant propagation.
thesestatic analysis techniques inevitably overapproximate the set of possible iccvalues and can thus potentially benefit from our work.
primo was the first to formalize the icclink resolution process.
we have compared with primo in sections and .
iccsecurity studies range from information flow analysis to vulnerability detection .
these works do not perform a formal iccanalysis and are instead consumers of icc analysis and can potentially benefit from our work.
static analysis alarms z ranking uses a simple counting technique to go through analysis alarms and rank them.
since bugs are rare if the analysis results are mostly safe but then an error is reported then it is likely an error.
tripp et al .
train a classifier using manually supplied labels and feature extraction of static analysis results.
we utilize static analysis to automatically do the labeling and perform feature extraction automatically from abstract values.
koc et al .
use manually labeled data and neural networks to learn code patterns where static analysis loses precision.
our work uses automatically generated must labels and since errors links are not localized in one line or even application we cannot pinpoint specific code locations for training.
merlin infers probabilistic information flow specifications from code and then applies them to discover bugs similar approaches were also proposed by kremenek et al.
and murali et al.
.
we instead augment static analysis with a post processing step to assign probabilities to alarms.
another class of methods uses logical techniques to suppress false alarms e.g.
using abduction or predicate abstraction .
ml for programs there is a plethora of works applying forms of machine learning to big code see allamanis et al .
for a comprehensive survey.
we compare with most related works.
related to tde s parisotto et al .
introduced a tree structured neural network to encode abstract syntax trees for program synthesis.
tde scan potentially be extended in that direction to reason about recursive data types.
allamanis et al .
usecnn sto generate summaries of code.
here we used cnn sto encode list data types.
allamanis et al .
present neural networks that encode and check semantic equivalence of two symbolic formulas.
the task is more strict than link inference.
for our case simple instantiations work reasonably well.
it is also prohibitive to generate all possible intents or filters as they did for boolean formulas.
hellendoorn and devanbu proposed modeling techniques for source code understanding which they test with n grams and rnn s. gu et al.
use deep learning to generate apisequences for a natural language query e.g.
parse xml or play audio .
instead of using deep learning for code understanding we use it to classify imprecise results of static analysis for inter component communication.
raychev et al .
usernn sfor code completion and other techniques for predicting variable names and types .
conclusions we tackled the problem of false positives in static analysis of android communication links.
we augment a static analysis with a post processing step that estimates the probability with which a link is indeed a true positive.
to facilitate machine learning in our setting we propose a custom neural network architecture targeting the link inference problem.
the key novelty is type directed encoders tde a framework for composing neural networks that 351neural augmented static analysis of android communication esec fse november lake buena vista fl usa take elements of some type and encodes them into real valued vectors.
we believe that this technique can be applicable to a wide variety of problems in the context of machine learning applied to programming languages problems.
in the future we want to explore this intriguing idea in other contexts.