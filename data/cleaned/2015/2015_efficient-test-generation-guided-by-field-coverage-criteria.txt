efficient test generation guided by field coverage criteria ariel godio department of software engineering itba buenos aires argentina agodio itba.edu.arvaleria bengolea department of computer science unrc rio cuarto argentina vbengolea dc.exa.unrc.edu.arpablo ponzio department of computer science unrc rio cuarto argentina pponzio dc.exa.unrc.edu.ar nazareno aguirre department of computer science unrc rio cuarto argentina naguirre dc.exa.unrc.edu.armarcelo f. frias department of software engineering itba buenos aires argentina mfrias itba.edu.ar abstract field exhaustive testing is a testing criterion suitable for object oriented code over complex heap allocated data structures.
it requires test suites to contain enough test inputs to cover allfeasible values for the object s fields within a certain scope input size bound .
while previous work shows that fieldexhaustive suites can be automatically generated the generation technique required a formal specification of the inputs that can be subject to sa t based analysis.
moreover the restriction of producing all feasible values for inputs fields makes test generation costly.
in this paper we deal with field coverage as testing criteria that measure the quality of a test suite in terms of coverage and mutation score by examining to what extent the values of inputs fields are covered.
in particular we consider field coverage in combination with test generation based on symbolic execution to produce underapproximations of field exhaustive suites using the symbolic pathfinder tool.
to underapproximate these suites we use transcoping a technique that estimates characteristics of yet to be run analyses for large scopes based on data obtained from analyses performed in small scopes.
this provides us with a suitable condition to prematurely stop the symbolic execution.
as we show transcoping different metrics regarding field coverage allows us to produce significantly smaller suites using a fraction of the generation time.
all this while retaining the effectiveness of field exhaustive suites in terms of test suite quality.
index t erms field exhaustive testing field based testing symbolic execution transcoping i. i ntroduction the functional correctness of software systems i.e.
the degree to which a software system meets the purpose for which it has been built is among the most challenging problems in software engineering .
while many techniques and methodologies have been proposed to guarantee software correctness testing consisting in evaluating the software under analysis by executing it in a number of cases is by far the most widely used product centric technique for software correctness assurance .
an essential part of testing is selecting the cases in which the software under test is going to be run.
this process is very costly inpractice and is performed mostly manually .
at the same time its importance is acknowledged by software development methodologies that strongly encourage writing tests e.g.
agile methods supporting writing tests together with and even before the software that should be tested .
it is well known that tests are inherently incomplete as a measure for guaranteeing functional correctness.
thus testing software appropriately i.e.
attempting to cover as much as possible of software functionality with a finite and limited set of test cases is crucial.
different coverage criteria i.e.
constraints on how a set of tests should exercise software have been proposed and some have been adopted by development models and software certifications e.g.
.
these constraints are in many cases very difficult to comply with making manual test development even more complex and time consuming especially in certain application domains.
this situation has led to the active development of automated test generation techniques a family of approaches that attempt to produce tests automatically.
these techniques vary in their underlying automation approaches including random generation generation based on constraintsolving and generation based on search including evolutionary approaches .
these techniques can also be categorized as white box orblack box .
white box techniques are generally driven by code coverage with tools based e.g.
in symbolic concrete or concolic execution these tend to produce relatively small suites with every test covering exactly one path branch or whatever the unit for coverage is driven by.
black box automated test generation techniques on the other hand generally produce large suites as witnessed by tools based on random generation and constraint solving or exhaustive search .
finally evolutionary computation whose categorization as white box or black box depends on the fitness function used also produces large suites.
this latter observation has some negative implications.
for some tools the size of the 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
produced suites affects the efficiency of the technique e.g.
degradation in test generation using random testing or high cost of suite minimization in evolutionary computation based generation .
and more importantly large test suites imply a high cost of test execution and unsuitability in contexts where suites are continuously and repeatedly executed e.g.
in testdriven development or continuous integration environments .
the above mentioned problems of black box driven test input generation motivated the recent work on a testing criterion called field exhaustive testing .
this black box criterion is particularly well suited for object oriented code over complex heap allocated data structures and requires suites to contain enough test inputs to cover allfeasible values for object fields within a certain input size bound.
while the criterion comes equipped with a technique for automatic generation of fieldexhaustive suites this technique requires a formal specification of the inputs that can be subject to sat based analysis .
moreover the restriction of producing allfeasible values for object fields makes test generation costly and field exhaustive testing difficult to generalize to further testing domains.
in this paper we deal with field coverage as testing criteria that measure test suite quality by examining to what extent the values of inputs fields are covered.
intuitively the approach we propose receives as input a family of sets of pairs field angbracketleftobj1 fieldval angbracketright angbracketleftobj2 fieldval angbracketright ... field angbracketleftobj1 fieldval angbracketright angbracketleftobj2 fieldval angbracketright ... ... each set for instance set field describes a set of pairs to cover.
for example if pair angbracketlefto a angbracketright field then a suite satisfying field coverage must contain a data structure in which objectomust be a part and for which o.field a. in order to determine the pairs covered by a test suite we will look at inputs heap stored data structures as rooted labeled graphs .
the graph root is the receiver object and arcs relating objects are labeled with field names.
in this setting an input is considered redundant if the relations among objects that it establishes have all been covered by previous inputs.
figure presents an example using three different binary trees.
nodes are indexed t0 n0 n1andn2.
above the trees we include the relations between nodes that each tree establishes partitioned according to field names.
the grayed out arcs and pairs between objects correspond to those covered by previous inputs considering a left to right generation order .
we show that this notion generalizes field exhaustive testing withdrawing the need for a sat analyzable formal specification and thus can be combined with any test generation technique to produce smaller test suites.
in particular we consider field coverage in combination with test input generation based on symbolic execution of repok procedures that code data structure representation invariants.
since symbolic execution may be expensive we will produce underapproximations of adequate test suites using the symbolic pathfinder tool.
to underapproximate these suites we use transcoping n0 n1 n1 null n2 null n0 n2 n1 null n2 null t0 n0 t0 n0 n1 n2 null null null nullroot left rightt0 n0 null nullroot left right n0 null n0 null t0 n0 n0 n1 n1 null n0 null n1 null t0 n0 t0 n0 n1 nullnull nullroot left right fig.
.
viewing heap stored data structures as rooted labeled graphs.
a technique that estimates characteristics of yet to be run analyses for large scopes based on data obtained from analyses performed in small scopes.
this imposes a suitable condition to prematurely stop symbolic execution.
as we show transcoping different metrics regarding field coverage helps us produce significantly smaller suites using a fraction of the generation time.
all this while retaining the effectiveness of field exhaustive suites in terms of test suite quality.
we show that this criterion when combined with test generation based on symbolic execution and transcoping improves testing as follows it produces test suites that are well suited for testing programs with complex heap allocated inputs.
significantly improves the time of the corresponding test generation technique when compared to the generation of field exhaustive test suites.
the experiments we will report in section iv show for the treeset subject a speedup of 167x.
it produces test suites whose bug detection ability is comparable to that of field exhaustive suites while improving testing efficiency with its significantly fewer tests.
moreover if we approximate bug detection by measuring branch coverage and mutation score for the treeset subject in section iv we achieve the same coverage and mutation score is slightly smaller .
instead of .
.
the paper is organized as follows.
in section ii we introduce field coverage criteria as a class of coverage criteria and focus on field exhaustive suites and the weaker versions to be discussed in this paper.
in section iii we show that suites satisfying field coverage can be automatically generated using symbolic execution.
in section iv we present experimental results that support the applicability of the presented criteria for bug finding.
finally in section vi we present some concluding remarks on this work.
ii.
f ield coverage testing criteria testing criteria define testing standards that indicate how thoroughly a piece of code is being examined by test cases.
moreover when testing criteria are selected as quality targets they serve as a measure for testing engineers on how to proceed to test a given piece of code.
engineers typically authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
design testing criteria that when met have as a consequence good testing properties.
for instance a suite that fails to execute a certain program statement cannot detect failures originating in that statement statement coverage is a testing criterion that attempts to avoid such omissions by prescribing that one should include test cases that exercise allthe executable statements in the program under test.
notice also that certain criteria help to build test suites able to detect certain classes of errors.
for instance multiple condition coverage takes into consideration all possible combinations of outcomes in conditions within program decision points thus aiming at logical errors or oversights in those points affecting the desired program control flow.
field coverage criteria aim at detecting errors in code operating on data structures as for instance those arising in code that handles heap allocated objects with a complex structure or that operate on heavily constrained memory heaps.
in this section we will provide a definition of the memory heaps that programs under test are going to handle and afterward introduce field coverage criteria.
let c primem arg1 c1 ... argk ck be the method under test declared in class c where arg1 ... argkare the formal parameters for m c1 ... ck their respective types and c primethe return type of m. definition an input memory heap for method m or simply memory heap when the method under analysis is clear from the context is a labeled graph angbracketleftv e l r p ... p k s1 ... s n angbracketrightwhere vis a set of objects and values containing the value null.
objects are typed and a single object may have more than one type.
eis a set of labeled edges between members of v.l the labeling function assigns class field names to edges.
l o1 o2 f o1.f o2.
we assume the labelling function respects the heap typing.
r vis the receiver object from class c. p1 ... p k vare the actual parameters for m. s1 ... s n vare the static objects values declared in class c. before defining field coverage criteria we need to introduce some notation about the semantics we will use.
intuitively the semantics for a class cis given by the set of those objects from the memory heap with type c. for a field f c primedeclared in a class c the semantics of fare those pairs from the edge set whose label is f. definition leth angbracketleftv e l r p ... p k s1 ... s n angbracketright be a memory heap.
let c1 ... cjbe the classes for objects inv.
we define the semantics of class ci i j b y ci v v vhas type ci .
let class ci i j include a field f c prime.
the semantics of field finh denoted by h is the function f ci c prime null defined by h angbracketlefto1 o2 angbracketright e l angbracketlefto1 o2 angbracketright f .
a memory heap models a single test input.
we will extend the notion of class field semantics to sets of memory heaps.intuitively we will join the semantics of all fields over all the heaps into a single set of labeled pairs.
definition given a test suite set of memory heaps h h1 ... h n and a field f the semantics of finh denoted by h is defined as uniontext h h h. let f f1 ...fk be the set of fields in the class hierarchy for h. we define the class field semantics of suiteh denoted by sh b ysh uniontext f f h. definition below introduces field coverage criteria.
intuitively any criterion based on populating the semantics of class fields i.e.
pairs angbracketlefto1 o2 angbracketrightlabeled with the name of the field they are expected to belong to in some specific way is a field coverage criterion.
a suite will satisfy the criterion if the semantics of class fields as collected from the heap objects of the suite cover field values as specified by the criterion.
definition let c1 ... cnbe a class hierarchy for a methodmunder test i.e.
method mmay be executed in memory heaps whose objects are typed with types c1 ... cn.
letsbe a labeled set of object pairs i.e.
sis expected to be of the form s angbracketleftl1 r1 angbracketright ... angbracketleftlk rk angbracketright satisfying for each j k angbracketleftlj rj angbracketrightis labeled with a field fdeclared in c1 ... cn angbracketleftlj rj angbracketrightis correctly typed as per the labeling i.e.
if pair angbracketleftlj rj angbracketrightis labeled f field fis declared in class cand has type c prime thenljhas type candrjhas type c prime.
field coverage criterion fcc sis satisfied by a test suite tiff the class field semantics see def.
stof heaps in t satisfiesst s. figure presents an example.
method removemin in class binsearchtree removes the binsearchtreenode holding the least value in the binary search tree.
field coverage criteria are black box and therefore as the example shows we only need to know the interface of the method under test.
the example presents a set s0that needs to be fully covered i.e.
the field coverage criterion is fcc s0.
the three memory heaps induced by the given trees let us call these memory heaps h cover set s0and therefore are a valid suite according to this testing criterion fcc s0.
another example this time given by comprehension is non null field coverage this criterion establishes that each field semantics must contain at least one pair whose second component is non null.
any suite containing the second or third tree in fig.
would satisfy this criterion whereas a suite containing only the first one would not.
it is important to notice that memory heaps as we have described them require objects to be somehow identified since objects are the vertices of heap graphs.
selecting appropriate identifiers is important and may be related to the programming language in which the software under test is provided.
languages in which the specific memory address where an object is stored is relevant in particular languages supporting pointer arithmetic may lead one to choose object memory addresses as object identifiers.
notice that choosing such a concrete notion of object identifier can have a significant impact in how field coverage criteria are described as well as in how difficult it may be to guarantee that certain field coverage criteria are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
class binsearchtree root binsearchtreenode invariant binsearchtreerepok requires root !
null binsearchtreenode removemin .
.
.
boolean binsearchtreerepok .
.
.
class binsearchtreenode left binsearchtreenode value int right binsearchtreenode s0 left n0 n1 n1 n3 n2 n3 rights n0 n2 n1 null n2 n4 roots t0 n0 t0 n0 null nullroot left rightt0 n0 n1 n2 null null nulln3 null nullroot left rightt0 n0 n1 n2 null nulln3 null nulln4 null nullroot left right fig.
.
suite based on field coverage an example.
t0 n0root left right n1 null nulln2 null nullt0 n1root left right n3 null nulln2 null nullt0 n2root left right n3 null nulln4 null null fig.
.
achieving field base coverage by permuting object identifiers.
met.
in this paper we use a more abstract notion of object identifier based simply on the position of each object in the breadth first traversal of the heap.
this is in fact the object identifier notion employed in fig.
where the order is given on a per type basis .
this choice not only simplifies in our case the definition of field coverage criteria it also allows us to deal with symmetry breaking .
symmetry breaking requires canonical representations of memory heaps thus guaranteeing that if two heaps differ in their canonical representations then they are non isomorphic or equivalently if two memory heaps are isomorphic their canonical representations will be the same.
symmetry breaking is important in test generation since it removes redundant cases and mechanisms to guarantee it are available as part of various test generation tools.
for example symbolic pathfinder uses lazy initialization to explore memory heaps and in the process it removes symmetries.
similarly korat generates non isomorphic objects.
taco uses automatically generated symmetry breaking axioms to remove symmetries.
notice that if isomorphic structures are allowed then one may achieve better coverage without actually considering different cases e.g.
the suite depicted in fig.
satisfies fcc s0 see fig.
by permuting objects identifiers in only one structure.
another example of a field coverage criterion is fieldexhaustive testing .
in this criterion the underlying set sof pairs to be covered is defined as containing all pairs that occur in valid where validity is relative to some input s specification memory heaps bounded in size by a userprovided bound k. field exhaustive testing as presented in requires a specification of valid inputs e.g.
through adeclarative precondition or a declarative object invariant.
how strong such specification is has an impact on the feasible pairs of field semantics since these are restricted to those feasible in valid inputs .
finally let us remark that suites complying with fieldcoverage criteria do not need to be of minimal size.
for instance notice that the first tree in fig.
may be safely removed and the remaining two trees still conform a valid suite according to criterion fcc s0.
also a field coverage criterion may not lead to any valid suite since it may require certain infeasible field semantics pairs to be covered.
for instance if we add to s0 rootthe pair angbracketleftt0 null angbracketright no memory heap satisfying the method precondition root !
null may cover that pair.
iii.
a utomated generation of field coverage suites field coverage as defined in the previous section is a very broad class of black box coverage criteria.
in this section we will concentrate on how to automatically generate test inputs satisfying some specific field coverage criteria.
more precisely we consider the use of field coverage to reduce the number of symbolic paths during test generation via symbolic execution of arepok method i.e.
a routine that checks the representation invariant.
in this case the reduction is associated with imposing an estimated metric regarding field semantics and prematurely stopping the systematic visit of symbolic paths when such metric is achieved.
this approach then implies a lossy technique in the terminology of in the sense that it will compute an underapproximation of the set of required test inputs.
notice in particular that this work generalizes fieldexhaustive testing as presented in since here we do not require formal declarative specifications to compute suites.
moreover we will also present a series of test generation policies that do not demand fullfield exhaustive suites but approximate these suites.
in section iv we will analyze the bug finding capabilities of suites produced following these approaches in comparison with the corresponding original techniques.
a. approaching field exhaustive t esting with transcoping the notion of scope as a bounding parameter is present in many automated analysis techniques.
as a term it originates in the context of alloy where it indicates the maximum number of atoms to be considered as part of signatures thus making decidable the satisfiability of formulas in an undecidable logic the scope makes domains finite and thus formulas in relational logic can be flattened into propositional ones .
this concept is directly mapped to other analysis tools notably the test generation tools korat and testera that borrowed the concept as well as the term.
other analysis techniques also require a scope of some kind models in model checking need to be finite and therefore bounds that limit the number of states are common.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
test generation through symbolic execution often following a depth first traversal imposes a maximum depth size for bounded path exploration.
software model checkers impose a maximum number of iterations and recursive calls.
random testing bounds the length of test sequences during test generation .
transcoping is a technique for estimating characteristics of yet to be run analyses for large scopes based on data obtained from analyses performed in small scopes.
essentially the idea is to perform complete analyses in small scopes where they are still affordable and extrapolate harvested data to larger scopes that can be exploited to improve analysis e.g.
by identifying irrelevant parts of analysis that may be avoided or selecting values for parameters of the analysis that showed better performance in small scopes.
in this article we will extrapolate information regarding field semantics that will allow us to stop test input generation before it would naturally terminate with the goal of producing test suites that approximate field exhaustive ones and retain good coverage and bug finding capabilities.
field exhaustive suites cover all the pairs of object identifiers that may occur in valid as per method preconditions representation invariants and scopes memory heaps .
a problem that arises when computing test inputs through symbolic execution over an operational input specification i.e.
repok is that allbounded symbolic paths must be traversed along the search of inputs that might contribute new pairs to the field semantics.
thus even when our goal may be to achieve field coverage in practice one ends up generating a suite that guarantees bounded paths coverage.
of course some of the inputs generated along path traversal will not be added to the suite because they will be deemed superfluous in case they do not contribute any new pairs to the field semantics but still the computational cost ends up being the same as that of path coverage.
as we will discuss in section iv when symbolically executing a repok invariant it is possible to generate most of the required inputs early in the symbolic execution.
therefore deciding a suitable condition to stop the execution in a way that good coverage bug finding is achieved will allow us to reduce generation time considerably.
we use transcoping for this task.
transcoping field size field exhaustive suites cover those pairs of identifiers angbracketleftid0 id1 angbracketrightfsuch that id0.f id1 in some valid memory heap.
a field exhaustive suite then allows us to compute field f s semantics which is a set of pairs whose size we will note as size f. therefore given a class hierarchy for the method under test from a fieldexhaustive suite we will be able to extract field s semantics ... whose sizes will be noted size f1 ... size fk respectively.
notice that given a scope sfor the number of objects these sizes will vary depending on s. therefore for scopes s1 ... s nwe can obtain families of sizes sizes1 f1 ... sizes1 fk ... sizesn f1 ... sizesn fk.
in this setting we will use the following transcoping procedure.
we will com pute the actual sizes for scopes through i.e.
compute the values size1 f1 ... size1 fk ... size5 f1 ... size5 fk.
these values are computed by generating field exhaustive suites for scopes through our rationale is that for these small scope values field exhaustive generation can be performed efficiently.
algorithm shows a first approach to suite computation based on transcoping size information.
it receives as input an integer smallscope which characterizes those scopes in the range to smallscope for which field exhaustive suites will be computed.
also it receives scope the scope for which a field coverage suite will be generated.
the algorithm assumes that class analysis wraps a data generation tool such as java symbolic pathfinder.
we also assume this tool provides an iterator interface that allows us to ask for the next datum.
class fieldsizes stores for each field f the number of pairs that the inputs visited so far contribute to .
therefore lines store in array fsizes the sizes of fields according to field exhaustive suites for each of the scopes considered small namely scopes to smallscope .
line using an appropriate regression function we will discuss such functions below predicts the expected size of the fields semantics in scope scope .
these predicted values will be used in line to characterize the termination criterion of the while loop the loop will iterate as long as any of the actual sizes so far stored infieldssemantics has not reached the predicted value stored intranscopedsizes such is the meaning of operator triangleleft .
inside the while loop lines a new input is retrieved from the analysis and the current semantics is updated with the aid of function updatesemantics.
in case the new input actually contributes with new pairs to the field semantics the input is stored as part of the generated suite.
if there is no next input lines the algorithm terminates.
the underlying testing criterion fcc sthat algorithm satisfies is that the size of for each field is greater than or equal to the minimum between the actual field semantics size and the transcoped field semantics size for the given scope .
we present a running example of algorithm further below.
we will consider three extrapolation methods in order to infer field sizes for larger scopes namely using a linear regression line.
using a quadratic regression parabola.
the function computed as the average of the functions above.
we will now discuss the reasons for choosing these regression functions.
linear functions usually underestimate the number of tuples in the field s semantics cf.
.
yet on occasions as may be the case for singly linked lists they produce a good estimation.
also even when they underestimate the number of tuples this is useful when we aim at obtaining small test suites.
notice that the largest size a field s semantics may have for scope n i sn n each one of the at most n objects in the field s domain may point to at most nobjects or the nullvalue .
therefore a quadratic function may provide a good estimation of the actual field size.
on occasions though the inferred value may be too close or even exceed the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm field coverage test suite generation by field size transcoping.
function compute suite int smallscope scope analysis analysis new analysis fieldsizes fsizes new fieldsizes for inti i smallscope i do fsizes computefieldsizesforscope i analysis end for fieldsizes transcopedsizes transcopefieldsizes fsizes scope set angbracketleftinput angbracketrightsuite new set angbracketleftinput angbracketright map angbracketleftfieldname pair angbracketleftobject object angbracketright angbracketrightfieldssemantics new map angbracketleftfieldname pair angbracketleftobject object angbracketright angbracketright while transcopedsizes triangleleftsizesof fieldssemantics do input input getnextinput analysis if input null then break end if boolean addedpair updatesemantics fieldsemantics input if addedpair then suite .add input end if end while return suite end function actual field size leading to almost no profit in test generation time compared to exploring the whole space of alternatives.
therefore taking the average between the quadratic and the linear functions leads to a still quadratic function that grows slower than the original quadratic function.
in section iv we will evaluate input generation algorithms that use transcoping to determine when generation must terminate according to each of these stopping criteria.
transcoping generation time an alternative to transcoping the size of the semantics of fields as a termination criterion is to consider the time required to reach the fullsize field semantics.
the algorithm for test suite generation is similar to alg.
and is presented as alg.
.
algorithm field coverage test suite generation by time transcoping.
function compute suite int smallscope scope analysis analysis new analysis time times new times for inti i smallscope i do times computetimesforscope i analysis end for time transcopedtime transcopetime times scope set angbracketleftinput angbracketrightsuite new set angbracketleftinput angbracketright map angbracketleftfieldname pair angbracketleftobject object angbracketright angbracketrightfieldssemantics new map angbracketleftfieldname pair angbracketleftobject object angbracketright angbracketright while elapsedtime transcopedtime do input input getnextinput analysis if input null then break end if boolean addedpair updatesemantics fieldsemantics input if adde dpair then suite .add input end if end while return suite end function notice that unlike alg.
which kept track of the size of the semantics of each field in this case we only need to keeptrack of the elapsed time of the current analysis in order to determine when generation must be stopped.
the elapsed time is maintained by the variable elapsedtime .
the underlying testing criterion fcc swhich algorithm satisfies is that each field contains the fraction of the actual field semantic discovered using the transcoped time as a timeout.
we present a running example of algorithm further below.
the extrapolation methods considered when transcoping generation time will be the following a quadratic regression parabola.
an exponential regression.
the function computed as the average of functions above.
we will now discuss the reasons for selecting these regression functions.
the number of feasible paths in a program usually grows exponentially as the program size increases.
therefore an exponential function may provide a good estimation of the actual time.
however the exponential function has the same limitations that the quadratic function had when transcoping field size i.e.
inferring a value too close or even exceeding the actual generation time.
on the other hand a quadratic function may provide a suitable lower bound to flatten the average regression while providing a good underestimation of the actual time.
in order to compute the regression functions we use the ordinary least squares method ols to estimate the parameters of a multiple linear regression model.
in particular to compute an exponential regression we transform the observations to a linear model.
table i presents a running example for binsearchtree in order to illustrate how algorithms and work.
each row corresponds to a different execution of the symbolic pathfinder tool for the given scope.
columns suite size and generation time present the size of the generated suite and the time in seconds required for the generation using different test generation policies namely fet for fieldexhaustive testing size a for algorithm using the average between the linear and quadratic regressions and time a for algorithm using the average between the quadratic and exponential regressions .
notice that the suite size and generation time for scopes to are the same for the three test generation policies since algorithms and compute fieldexhaustive suites for these scopes.
column presents the actual field semantic sizes for fields leftand right whereas t presents the transcoped field semantic sizes using the average regression.
notice that in order to compute we need to compute a field exhaustive suite while algorithm only needs to reach the bound t e.g.
for scope the generation will stop when the field semantics size of field leftreaches pairs andthe field semantics size of field right reaches pairs.
this condition is met in seconds and the generated suite contains non isomorphic structures whose bug detection ability is comparable to that of field exhaustive suites as we will see in section iv.
finally columns time andtt i m e present the time required to reach the full size field semantics and the transcoped time respectively using the average regression.
notice the difference between columns authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i running example with binsearch tree suite size generation time t time t time scope fet size a time a fet size a time a left right generation time and time .
the former is the time required to generate a field exhaustive suite i.e.
exploring the whole space of alternatives and the latter is the time required to reach the full size field semantics i.e.
when it was the last time an input contributed new pairs to field semantics.
column time can only be computed once the generation has finished.
however we can use these values from scopes to to make predictions for larger scopes which is the key concept of algorithm e.g.
for scope the generation will stop when the transcoped timeout of seconds is reached generating a suite containing non isomorphic structures whose bug detection ability is comparable to that of field exhaustive suites as we will see in section iv.
t ermination and correctness of the field coverage suite generation algorithms termination of alg.
follows immediately because the while loop execution time is bounded by transcopedtime .
termination of alg.
on the other hand is not obvious.
notice that variable transcopedsizes may contain an inferred size for a field fthat is larger than the actual maximum size for the semantics of said field.
therefore the loop will only terminate in case getnextinput analysis returns null.
to this end we will only consider analyses that for a user given scope become finite state.
of course the number of states may be large and in this case the generation will terminate when the underlying analysis terminates.
this is why the presented techniques which allow us to predict when to stop the analyses are essential.
regarding correctness let us define set s angbracketlefti o angbracketrightf angbracketlefti o angbracketright fieldsemantics f i.e.
for each field f setscontains those pairs computed in variable fieldsemantics with label f. it is then clear that variable suite stores a test suite that satisfies testing criterion fcc s. iv .
e xperimental ev aluation in this section we present the experimental details and we will evaluate several field coverage criteria.
since our goal is to approximate field exhaustive testing we will compare for each criterion the achieved branch coverage and the mutation score as well as the suite size and the generation time against the values obtained for field exhaustive testing.
we will consider an implementation of the following subjects for our experimental evaluation i sorted singly linked lists ii binary search trees iii treesets iv a vl trees and v binomial heaps.
a. experimental details algorithm shows the driver needed to generate inputs with symbolic pathfinder spf transcoping field size.
line instructs spf to treat xas a symbolic variable thus enabling lazy initialization over xitself as well as its fields.
line prunes the search whenever an invalid instance is found we generate suites of valid inputs .
symbolically executing this line would potentially spawn infinite paths since the size of a binary search tree is not restricted by the repok .
however this potentially infinite exploration is bounded by the scope of the analysis.
the methods used in lines are captured by the listener except for verify.writeobjecttofile which is part of the verify api of spf.
method addedpair checks whether the new input xcontributes with new pairs to the field semantics.
if so line solves the remaining path condition.
notice that the symbolic execution of repok only concretizes as a consequence of lazy initialization fields left andright .
however for basic types as the ones used in field value we must resort to the smt solver and request concrete values to generate the actual input.
line serializes x and saves it to the unique path returned by getnextpath .
finally line checks whether the termination condition holds and stops the analysis if so.
notice that transcoping field size imposes a synchronic termination criterion i.e finding an input could lead to prematurely stop the analysis whereas transcoping generation time imposes an asynchronous termination criterion in a timeout wise manner.
in algorithms and we separated the computation of the suites in stages for the sake of simplicity.
the first stage where we call transcopefieldsizes and transcopetime for algorithms and respectively to estimate the target of each termination criterion and the second stage where we use these criteria to stop the analysis.
this requires to run spf twice the first time up to scope smallscope and the second one up to the desired scope however there is no such separation in algorithm .
as we discuss below we imposed a search heuristic that explores inputs in ascending order considering the size of the structures.
that said it is straightforward to take advantage of this property to compute the suite up to scope smallscope transcope the termination criteria and continue the analysis running spf only once.
to measure the coverage and mutation score we considered different methods of each case study namely insert remove andcontains .
all of this methods take as input an integer which we selected in a bounded exhaustive manner this is to run the tests with a suite generated up to scope n we considered the integers in the range .
the tests were written with the help of junit and the coverage and mutation analyses were performed with jacoco and mujava respectively.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm driver to run spf for binary search trees transcoping field size.
1public static void main string args 2bstree x new bstree 3x bstree debug.makesymbolicref x x 4verify.ignoreif x null !x.repok 5if addedpair solvepathcondition verify.writeobjecttofile x getnextpath stopifterminationconditionholds table ii suite size and time to generate suites with symbolic pathfinder tran scoping field size s scope .
size time size time size time size time size time s s s s s 5sortedlistfet fbt l fbt a fbt q s s s s s 0bstreefet to fbt l fbt a fbt q to s s s s s 0treesetfet to fbt l fbt a fbt q s s s s s 5a vltreefet to fbt l fbt a fbt q to s s s s s 3binheapfet to fbt l fbt a fbt q to the experiments were conducted on a .6ghz intel core i7 processor with gb ram running archlinux .
.
the replication package as well as the complete results can be found in b. results in tables ii vi we denote field exhaustive testing by fet.
we denote by fbt l fbt q and fbt e the criteria that transcopes a given attribute using a linear quadratic or exponential regression function respectively.
the criterion that uses the average of the two corresponding regression functions is denoted by fbt a. in tables ii and iii times are expressed in seconds and a hours timeout marked as to when reached is set.
in tables v and vi branch coverage and mutation score are expressed as percentages.
table iv shows the number of mutants generated for each case study.
a total of mutation operators were used for this task.
the list of these operators can be found in the replication package.table iii suite size and time to generate suites with symbolic pathfinder tran scoping generation time s scope .
size time size time size time size time size time s s s s s 5sortedlistfet fbt q fbt a fbt e s s s s s 0bstreefet to fbt q fbt a fbt e s s s s s 0treesetfet to fbt q fbt a fbt e s s s s s 5a vltreefet to fbt q fbt a fbt e s s s s s 3binheapfet to fbt q fbt a fbt e table iv number of mutants generated for each case study .
case study of mutants sortedlist bstree treeset avltree binheap table v comparing coverage and mutation score of field exhaustive and field coverage suites tran scoping field size s scope .
cov mut cov mut cov mut cov mut cov mut s s s s s 5sortedlistfet fbt l fbt a fbt q s s s s s 0bstreefet i fbt l fbt a fbt q s s s s s 0treesetfet fbt l fbt a fbt q s s s s s 5a vltreefet fbt l fbt a fbt q s s s s s 3binheapfet fbt l fbt a fbt q authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi comparing coverage and mutation score of field exhaustive and field coverage suites tran scoping generation time s scope .
cov mut cov mut cov mut cov mut cov mut s s s s s 5sortedlistfet fbt q fbt a fbt e s s s s s 0bstreefet fbt q fbt a fbt e s s s s s 0treesetfet fbt q fbt a fbt e s s s s s 5a vltreefet fbt q fbt a fbt e s s s s s 3binheapfet fbt q fbt a fbt e c. discussion tables ii and iii show that the generation of test suites resorting to the presented techniques is most times significantly faster than generating field exhaustive ones.
of course such speedup would be worthless if the produced suites were of poorer quality.
interestingly table v shows that suites produced by transcoping field size achieve in almost all cases all but the same branch coverage and mutation score than field exhaustive suites.
as it may be expected see table vi transcoping suite generation time is less precise although still achieves good branch coverage and mutation score.
we believe the results obtained when transcoping field size are quite compelling.
there is an interesting observation to be made regarding sorted singly linked lists.
table ii shows no gains for the proposed techniques.
the reason for this phenomenon is twofold.
on the one hand as we said before a linear function provides a good estimation when transcoping field semantic sizes indeed the computed linear regression is exact and the quadratic coefficient of the quadratic regression is almost .31e which produces an almost linear function for the values of x in the range .
this causes the linear the quadratic and the average regressions to produce the same and exact estimations.
on the other hand since there is only one valid instance per scope and this case study in particular is not fully benefited by the imposed search heuristic since it always requires a new node to produce a new instance and has no previous field discussed below we can not take advantage of the observation that most of the required inputs can be generated early in the symbolic execution.
table iii also shows an interesting result regarding singlylinked lists.
transcoping generation time is by definition sensitive to execution times and in this particular case the generation up to scope smallscope usually takes less than seconds.
the lack of data as well as its variability sometimes makes the ols method to produce negative quadratic regression functions stopping the analysis too early compared with the exponential regression.
even though the last two observations are negative tables v and vi show these shortcomings have no effect over the quality of the generated suites which in turn have the same coverage and mutation score as those generated using field exhaustive testing.
the reason for this to happen is that the methods that handle singly linked lists are simple enough they usually consider at most different scenarios to be covered and get their mutations killed even with a few inputs.
the counterpart of singly linked lists in terms of code complexity are binomial heaps and the results obtained transcoping time show that fewer inputs table iii negatively impact the mutation score in particular for higher scopes table vi .
however the results obtained transcoping size for the same case study are quite compelling.
as we said before when symbolically executing a repok invariant it is possible to generate most of the required inputs early in the symbolic execution.
this observation has a significant impact on the presented techniques since it allows us to prematurely stop the search and generate a significative fraction of the inputs that would be generated using field exhaustive testing.
this is achieved in symbolic pathfinder by imposing a search heuristic that prioritizes the use of previously created objects and the null reference over new objects as candidates when lazy initializing objects fields.
as a side effect this heuristic explores inputs in order considering the size of the structures.
the fact a repok procedural invariant is required may be perceived as a limitation of these techniques.
however as reported in having such methods is a good programming practice.
alternatively one may generate test inputs via symbolic execution of constructor methods.
this alternative has problems it is not obvious how to combine these methods to explore all the possibilities for instance for red black trees it is required to combine methods insert andremove in order to explore all valid tree nodes colorings .
the symbolic execution of combinations of complex constructor methods like insert andremove is by far more expensive than the symbolic execution of a repok method.
d. threats to v alidity experimental evaluations may be influenced by the selected subjects.
to reduce bias we selected the same subjects used in the evaluation of field exhaustive testing in .
the selected subjects are good examples of programs with heavily constrained memory heaps with a broad range of complexity going from those with simple invariants to some with complex constraints.
moreover they are often used as benchmarks authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in the evaluation of other analysis tools .
a more thorough evaluation is pending including more data structures but also general purpose software using said data structures.
v. r elated work automated test generation is a research area that has received substantial attention in recent years and important advances leading to the development of many tools and techniques have taken place.
some of the most effective and successful automated testing approaches are either based on random generation evolutionary computation model checking constraint solving including smt and sat solving or some forms of exhaustive search .
test suites generated by tools based on random generation and evolutionary computation such as randoop autotest quickcheck and evosuite generate large test suites.
a consequence of such large suites is increased testing time a problem we try to avoid with field exhaustive testing.
our approach corresponds to systematic test generation in the terminology of which makes it closer to tools like pex fajita symbolic pathfinder and korat .
these tools are also specification based i.e.
they produce tests from input specifications as opposed to tools like evosuite or randoop that use routines methods of the tested subject to produce test sequences.
in a set of experiments compare random testing and systematic testing for container classes arriving at the conclusion that random testing produces suites that are comparable in quality coverage mutation score to systematically produced suites using shape abstraction while consuming less computational resources less generation time .
in our approach computing field exhaustive suites produces field extensions.
these extensions are equivalent to upper tight field bounds and thus our work is related to approaches to compute such bounds e.g.
.
however none of these related approaches focuses on test generation does not collect instances produced along the tight bound computation process and even if it did so it would be significantly more inefficient than our approach since it requires a cluster of computers for tight bound computation follow a bounded exhaustive enumeration of instances to compute tight bounds as opposed to our field exhaustive mechanism.
vi.
c onclusions we proposed a generalization of the field exhaustive testing criterion that we called field coverage testing criterion whose satisfaction is associated with the coverage of feasible values for object fields.
besides formally defining the criterion we developed an algorithm that automatically produces underapproximations of field exhaustive suites using transcoping which estimates characteristics of yet to be run analyses for large scopes based on data obtained from analyses performed in small scopes.
the experimental results showed that transcoping field size provides a suitable condition to prematurely stop the search while producing suites whose bugdetection ability is comparable to those satisfying the fieldexhaustive testing criterion measured in terms of mutation score and branch coverage in a fraction of the time.
another conclusion we draw is that field exhaustive testing is a very powerful criterion which even if weakened still produces good coverage and mutation score.