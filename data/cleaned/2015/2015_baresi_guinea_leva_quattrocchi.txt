see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation a discrete time feedback controller for containerized cloud applications conf erence paper no vember .
.
citations 93reads author s luciano bar esi politecnic o di milano publica tions citations see profile sam guine a politecnic o di milano publica tions citations see profile albert o leva politecnic o di milano publica tions citations see profile giovanni quattr occhi politecnic o di milano publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y giovanni quattr occhi on sept ember .
the user has r equest ed enhanc ement of the do wnlo aded file.a discrete time feedback controller for containerized cloud applications luciano baresi sam guinea alberto leva and giovanni quattrocchi deib politecnico di milano piazza leonardo da vinci milan italy luciano.baresi sam.guinea alberto.leva giovanni.quattrocchi polimi.it abstract modern web applications exploit cloud infrastructures to scale their resources and cope with sudden changes in the workload.
while the state of practice is to focus on dynamically adding and removing virtual machines we advocate that there are strong bene ts in containerizing the applications and in scaling the containers.
in this paper we present an autoscaling technique that allows containerized applications to scale their resources both at the virtual machine vm level and at the container level.
furthermore applications can combine this infrastructural adaptation with platform level adaptation.
the autoscaling is made possible by our planner which consists of a grey box discrete time feedback controller.
the work has been validated using two application benchmarks deployed to amazon ec2.
our experiments show that our planner outperforms amazon s autoscaling by on average without containers and that the introduction of containers allows us to improve by yet another on average.
ccs concepts networks!cloud computing social and professional topics !software selection and adaptation quality assurance computing methodologies !computational control theory keywords adaptive systems cloud computing control theory containers software adaptation .
introduction nowadays many web applications are deployed and executed in the cloud to scale more easily according to the current workload.
industry has developed various techniques for automating and improving the management of these kinds of applications.
a concrete example is amazon s autoscaling which allows system administrators to determinewhen and how an application s resources should dynamically increase or decrease.
academia has also provided a large body of work on cloud management with a strong focus on self adaptation and on dynamic resource allocation at the infrastructure layer .
the state of the art comprises many mape based approaches where planning is based on heuristics arti cial intelligence queueing theory and control theory .
moreover literature distinguishes among three kinds of possible adaptations.
infrastructure adaptation changes the number of computing resources allocated to the application.
platform adaptation re con gures the platform stack on which the application runs its code e.g.
by changing the number of workers dedicated to an application server .
feature adaptation temporarily removes optional features from the application when the system is saturated e.g.
by turning o a recommendation system of an e commerce site .
in this context the paper introduces a novel mape based self adaptation framework for cloud based web applications centered around three important novelties.
the rst is that we allow system administrators to take advantage of both vm and container based resource management e ectively allowing them to mix two very di erent levels of granularity when conceiving infrastructure elasticity.
containerization is an emerging virtualization technique in which many processes called containers are run on the same physical machine without interference.
containers share a host operating system and are more lightweight and faster to boot than traditional vms .
renting a cluster of vms and using it to execute di erent containerized applications or micro services is an emerging architectural pattern but it still lacks a comprehensive approach for resource management.
the second is that we focus on coordinated infrastructure and platform adaptation although the use of containers would also allow us to support feature adaptation.
to the best of our knowledge there are no approaches that support all the di erent kinds of adaptations.
while infrastructure adaptation at the vm level is an industrial best practice platform adaptation has yet to be thoroughly investigated.
one of the main reasons for this is that the more aspects you want to adapt the more your planner becomes complex to the point in which it can lose generality and become over t for a speci c application.
feature adaptation on the other hand has been implemented in the past.
containers play an essential role in this coordinated approach.
while the planning phase of our mape control loopapplication logicapplication serverdevelopment runtimeoperating system virtualized infrastructurenodeapp 1tier 1app 2tier 1app 3tier 2bins libsbins libsbins libsguestosguestosguestoshypervisorhost oshost serverapp 1tier 1app 2tier 1app 3tier 2bins libsbins libsbins libscontainer enginehost oshost server a b figure a multi level nature of a node b virtual machines left vs. containers right .
treats containers as black boxes and focuses on their horizontal and vertical scaling the containers themselves present well de ned technology and application speci c callbacks through which their internals can be adapted.
these callbacks are triggered every time something changes at the container infrastructure level.
the third is a novel mape planner that consists of a discrete time feedback controller.
peculiar to our proposal is the structure of the said controller it is tailored to the structure of the command to metrics dynamics to be governed using a grey box approach.
the resulting controller is composed of a linear time invariant block plus a static linearization one.
the planner is also endowed with an internal saturation management anti windup mechanism.
the evaluation of our approach has been conducted using two cloud applications rubis an application for on line auctions and pwitter a simple twitter like social network.
our experiments show that our approach outperforms amazon s autoscaling i.e.
the industrial state of practice by on average when used solely with vms and without containers.
they also show that if we introduce containers and therefore adopt a ner granularity we can improve the performance of our adaptation by another on average.
the rest of the paper is organized as follows.
section introduces our use of container based technology.
section presents a high level overview of our approach.
section discusses our control theory planner while section explains how we combine infrastructure and platform adaptation.
section presents the experiments done to validate our work.
section discusses related approaches while section concludes the paper.
.
anatomy of a cloud application our work focuses on web applications that are deployed and executed on public private or hybrid clouds.
the virtualized infrastructure is considered to be a black box this means that we do not have access to the hypervisor or to the underlying physical machines.
modern web applications are typically developed using a multi tier architecture.
a tierlogically and physically separates software components that deal with di erent functional aspects of an application.
for example a traditional tier application comprises a presentation tier which manages the user interface a logic tier which executes the application s business logic and a data tier which handles user data.
modern applications however can be made of more than three tiers.
for example one might decide to use two data tiers one for a traditional relational database and one for a nosql database.each tier contains multiple nodes.
a node is a computing instance that runs platform software e.g.
a jvm and an application server as well as some actual application code see figure a .
given the de nition of tier that we use within this paper all the nodes in a tier execute the same technological stack.
this is not an uncommon assumption in fact it directly mimics the way amazon aws de nes application architectures in opsworks1.
managing multi tiered cloud applications while satisfying a set of functional and non functional requirements is a complex task.
it requires understanding the dependencies that exist between nodes belonging to di erent tiers.
for example in a tier application a load balancer must know all the ip addresses of the application tier s nodes.
moreover the nodes in a tier could change over time due to scale in and scale out policies.
.
virtual machines vs. containers a node is traditionally materialized as a vm.
however nowadays a node might also be a container.
containers provide a virtualization technique that operates at the operating system os level.
they exploit several features of the linux kernel such as namespaces andcgroups to create isolated views of the operating environment for di erent applications.
a container has its own process space virtualized network interface and le system and the operating system can allocate di erent amounts of resources e.g.
cpu memory and i o to each of them.
figure b shows a layered comparison between vms and containers.
if we start our comparison by looking at the lower layers we can see that multiple vms are managed by a single hypervisor that resides on a single host operating system.
each vm then contains its own guest operating system its own platform stack composed of di erent libraries middleware and application servers and its own application code.
on the other hand containers are executed directly on top of the host operating system optionally with the help of a container manager like docker .
each container has its own platform stack and its own application code.
containers have various advantages when compared to vms they are more lightweight and they are faster to boot and to terminate because they do not have to deal with a guest operating system .
industry is widely adopting containers as a means to favor portability and they are considered to be one of the main technological enablers of the devops movement .
different development teams may use di erent operating systems and di erent platform stacks making feature integration hard.
however thanks to containerization technology features can be developed in isolation with the guarantee that they will work the exact same way on any machine that supports containers.
many containerization technologies exist.
we chose docker because it has the highest industry penetration and its performance can be considered best in class .
docker is written in go and it uses the libcontainer library to manage the linux kernel.
it distinguishes between images and container instances the former are container snapshots that can then be used to generate new instances of these snapshots.
1opsworks is a con guration management tool provided by amazon aws for con guring and operating complex applications using devops technology.amazon ec2 cloudapp tier 1bins libsecoware sensoradaptation hooksecoware containerapp tier 3bins libsecoware sensoradaptation hooksecoware container createvm terminatevmguest osdocker engineecoware agentvm 1guest osdocker engineecoware agentvm napp tier 2bins libsecoware sensoradaptation hooksecoware container cloudwatchcreatecontainerterminatecontainerupdatecontainercreatecontainerterminatecontainerupdatecontainerec2providesmonitorshosted onhosted onhosted on interfacemonitoring dataincoming eventfigure a deployment of ecoware.
to conclude there is no need to choose between vms and containers.
containers can run inside vms to increase security and it is becoming quite common to nd containers being used in conjunction with virtualized cloud infrastructures.
for example with amazon ec2 container service one can run docker images directly across a cluster of ec2 vm instances.
.
ecoware the solution presented in this paper extends ecoware our framework for the development of self adaptive systems2.
the main extensions in this paper are i the support for containerized applications ii an entirely new planner designed from the ground up and based on control theory and iii an execution processing model that allows for coordinated infrastructure and platform adaptation.
figure shows how containerized applications are deployed using ecoware.
ecoware can also be used without containers by deploying applications directly onto vms but in that case it looses the capability of performing a more ne grained adaptation.
ecoware is not limited to supporting a single application it can manage multiple multi tiered applications running on the same shared virtualized infrastructure.
each node in a tier is a container that is hosted on a vm.
each node is equipped with i an ecoware sensor a component that has access to the node s internals and is responsible for generating application speci c monitoring data and ii a set of adaptation hooks for platform adaptation.
each employed vm also deploys an ecoware agent .
this component is responsible for providing adaptation actions for creating and manipulating containers on that vm and for collecting container speci c monitoring data such as the containers use of cpu and memory.
in the deployment shown in figure ecoware uses amazon ec2.
it exploits ec2 s vm management apis to provide infrastructure adaptation actions for creating and manipulating vms and amazon cloudwatch to obtain monitoring information about the vms themselves.
monitoring and analysis.
we have not extended ecoware with any major novel monitoring and analysis capabilities.
we did however produce new kinds of sensors to 2the implementation of all the components of ecoware can be found at both containers and vms and created a new java template to facilitate the creation of application speci c sensors.
in ecoware sensors generate data under the form of service data objects sdos and deliver them to a distributed messaging infrastructure implemented using rabbitmq .
in turn the distributed messaging infrastructure can deliver collected data to three di erent kinds of data manipulation tools aggregators kpi processors and analyzers .
ecoware s data manipulation tools were built using esper an instrument for implementing complex event processing activities.
aggregators collect data from multiple sensors to create new composite sdos.
kpi processors correlate and aggregate data coming from various sensors to generate business level key performance indicators such as response times and throughputs.
analyzers predicate over the actual contents of an sdo e.g.
to see whether a response time goes beyond a given threshold.
planning and execution.
planning can be considered technology and application agnostic since it only focuses on choosing how to adapt the infrastructural resources that are dedicated to each node in a tier.
now that ecoware supports containers we can distinguish between two kinds of infrastructural adaptation we can change either the vms or the containers being used by an application or both.
throughout the rest of this paper we will refer to the former in terms of vm adaptation and to the latter in terms ofcnt adaptation the term infrastructure adaptation will be used to refer to both jointly.
cnt adaptation operates directly on an application s containers.
it allows for instant reaction to workload changes both through horizontal and vertical container scaling.
horizontal scaling implies the addition or removal of containers vertical scaling implies increasing or decreasing the amount of resources dedicated to an already existing container.
the delays imposed by vm management are removed.
while booting a vm on a public cloud may take more than one minute adding a container or changing its resources is practically instantaneous given the adopted time scale.
since multiple applications that share the same virtualized resources may see di erent workloads the ner granularity and the higher adaptation speed of working with containers allow us to use the vms that we have more e ciently without necessarily having to add new ones.
since containers are deployed inside vms our planner can choose between ve kinds of infrastructure adaptations.
create vm allocates a new vm through the cloud provider.
terminate vm vm id removes a vm identi ed by an id vm id .create container vm id node type resources launches a container of type node type onvm id allocating to it resources cpu cores and memory .
terminate container container id removes the container with container id .scale container container id new resources changes the resources allocated to container id tonew resources .
ecoware also supports platform adaptation through the use of adaptation callbacks.
whenever ecoware operates a cnt adaptation a noti cation is sent to the interested ecoware agents .
their containers can then react by internally adapting their platform assets in technology and application speci c ways.
this is a novel contribution of this work and will be discussed in detail in section .
.
applications ecoware needs a thorough description of the self adapting systems that it is supposed to manage.
this includes both a description of the cloud infrastructure to use and a description of the applications being deployed.
the latter includes the de nition of their tiers and of the dependencies that exist between them.
this information is collected in an ecoware applications description json le inspired by tosca .
listing provides a partial description of the two applications and of the infrastructure that we used for the evaluation in section .
we use it here to illustrate the information that is stored in the le.
listing example of an applications description le.
f i n f r a s t r u c t u r e f c l o u d d r i v e r f name aws ec2 c r e d e n t i a l s usr me u t i l s aws .
p r o p e r t i e s vm flavor t2 .
small vm image id ami e f f f g max vms g apps f name rubis t i e r s f loadbalancer f name front load balancer max node docker image haproxy depends on app server on dependency scale usr me u t i l s r e l o a d s e r v e r p o o l max rt .
g app server f name application logic tier docker image polimi rubis j b o s s depends on db on node scale usr me u t i l s jboss hook on dependency scale usr me u t i l s r e l o a d c o n n e c t i o n s max rt .
g db f name data t i e r max node docker image mysql on node scale usr me u t i l s mysql hook max rt .
g g g f name pwitter t i e r s f. .
.g g g the infrastructure json object describes the infrastructure layer on which the applications will be deployed.
its attribute cloud driver contains the name of the provider thecredentials needed to access the provider and the type vm flavor and image id vm image id of the vm instances to use.
attribute max vm sets a limit to the al locable vms to avoid in nitely scaling the application and producing too high a bill.
in our example we used amazon ec2 t2.small vm instances each has cpu core and 2gb of memory.
for our image id we used an amazon machine image identi er3.
this vm image is con gured to launch both docker and an ecoware agent as soon as the vm has nished booting.
each application is identi ed by a name and its tiers are described using attribute tiers .
each tier has a name a maximum number of nodes permitted within that tier max node a reference to the docker image to be used for its nodes docker image a list of the dependencies that the tier has with other tiers depends on and any additional meta data about its adaptation strategy.
in our example we do not want the load balancer to be replicated so we set its attribute max node to the image for the load balancer tier is called haproxy and the load balancer manages a pool of application servers so it declares a dependency with that tier.
currently we support one to one one to many and many to one dependency relationships between tiers where byoneandmany we mean the number of nodes in each tier.
we do not support cyclic dependencies for the correctness of our processing model see section .
.
each tier also speci es what adaptation callbacks it supports and when they should be executed.
this is stated through two types of adaptation hooks .
a container s on node scale hook is invoked every time the container is scaled at run time a container s on dependency scale hook is invoked after something changes on a tier that the container s tier depends on.
for example a system administrator may write an on node scale hook to scale the number of workers of an application server when the resources allocated to the relevant container change.
similarly s he may write anon dependency scale hook to have the load balancer change its routing policy depending on the resources allocated to the servers.
finally the tier also declares the service level agreement sla that we want to guarantee using attribute max rt which refers to the maximum acceptable average response time passed to the planner.
.
planning this section describes the control theoretical design path followed to obtain the planner algorithm.
our goal was to develop a decentralized solution in which each application tier is endowed with a local controller devoted to maintaining a desired performance metric i.e.
response time in the presence of exogenous disturbances by computing the resources i.e.
cpu cores and memory that must be made available to that tier.
once this has been done we translate the computed resource allocations into concrete adaptation actions taking into account the current state of the system.
.
controlled system we shall now start by formalizing the hypotheses used to derive the model for the controlled system in controltheoretical terms.
hypothesis .
in any steady state situation the metric is a function of the assigned resources and of some disturbance input that re ects the system s load .
this function does userguide amis.htmlnot need to be linear but it must be regular enough to be inverted at least in well de ned regions of the resource load space.
we assume that the steady state response time depends on the ratio between the number of assigned cores and the request rate.
of course the steady state response time may also depend on the available memory but we focused on cores as they inherently provide a better granularity.
memory is either su cient for the application hence assigning more is useless or insu cient.
in the latter case the performance degradation will depend on many ne grained facts e.g.
caches swap system disk etc.
making the actuatorto metric relation more di cult to model.
hypothesis .
the static function that we are describing acts on the metric through an asymptotically stable linear time invariant dynamic system with unity gain and relative degree.
this means that once the resource and the load stay constant the metric will eventually reach its corresponding steady state value but not immediately.
for example this can be the case when acquiring a new core yields its response time improvement as some queue gets emptied.
the hypothesis of unity relative degree means that the e ects of an action on the assigned resource start showing up in the metrics from the control instant immediately after the action is applied.
under these hypotheses the evolution over the discrete time index kof the response time r k as an e ect of the assigned cores c k and the request rate r k is ruled by the following nonlinear time invariant dynamic system eu k f c k r k r k p r k p eu k that corresponds to the block diagram of figure .
c k r k f p z p r k figure block diagram of the dynamic model for the controlled system.
in this particular case function f is intuitively monotonically decreasing towards a possible lower horizontal asymptote as it can be assumed that once the parallelism degree of an application is ful lled by the available cores adding new ones causes no further decrease in the response time.
more speci cally we found a practically acceptable function to be f c k r k c1 c2 c3c k r k where parameters c1 c2 andc3were obtained through proling.
.
control synthesis the model structure of figure suggests a controller that is the compound of a lineariser plus a linear time invariant feedback regulator.
such a scheme is shown in figure .
the key point is to select function fc in such a way thateu k u k thereby leading the controller to see just a linear block.
this means setting r k r z fc u k r k c k f p z p tildewideu k r k figure block diagram of the dynamic model for the closed loop control system.
f c k fc u k r k u k which assuming f invertible at least in the signal range of interest gives fc u k r k r k f u k with the selected fc the directed relationship from u k to r k reduces to the z domain transfer function p z p thus the relationship between r k and r k also reduces to the z domain transfer function tr z t r z r z p z p r z p z p as frequently done in other control domains we conduct the selection of r z by prescribing the transfer function from set point to controlled variable.
the target for this transfer function must have a relative degree of at least one for realizability reasons.
we additionally select a unity gain to ensure asymptotic set point tracking and disturbance rejection and use the single pole chosen in the range to require a faster !
or slower !
error convergence.
in summary we set r k tr z t r z z and solving for r z we obtain r z p 1z p z the eigenvalues of the obtained control system are the prescribed one and the cancelled one p in the range in force of the stability properties we assumed for the controlled system.
hence the closed loop system is guaranteed to be asymptotically stable.
to turn into the control algorithm required by the planner we rst rewrite to highlight its direct feedthrough and strictly proper dynamics terms that is r z p p z and we obtain the controller in state space form as xr k xr k p r k r k c k r k f p xr k r k r k this provides the control algorithm but it does not manage saturations.
when confronted with a large change in the set point or a sudden and relevant disturbance the controller may compute an action c k that is not feasible typically either because it is negative or because it exceeds the maximum number of available cores.
if cminandcmax denote the minimum and maximum number of cores respectively c k must be clamped within the range and the state of must be recomputed to maintain consistency with the input and output that is xr k p 1f c k r k r k r k algorithmically if we omit initializations and highlight the errore k r k r k we have e r r xr xrp p ep c r finv p xr e c max cmin min cmax c xrp p f c r e ep e where the p subscript denotes previous values i.e.
those corresponding to the previous step while f and finv correspond to function f and its inverse respectively.
finally we present a simulation example that shows the controller in action.
figure reports the behavior of a system subject to the load in the top left plot while the required response time is the dashed line in the top right plot.
the dark line in the same plot is the actual response time obtained with the model matching the real process while the light line represents the same results with a quite relevant parametric mismatch up to in the coe cients of f and up to in p .
the bottom plot presents the allocated cores in the nominal dark line and parametrically perturbed light line cases.
although a formal robustness proof is out of the scope of this paper and is therefore deferred to future ones the controller behaves satisfactorily even in the presence of unavoidable model mismatches.
figure simulation example to test the synthesised controller.
.
from resource allocation to actions as previously stated each tier of each application has its own controller.
the controllers are all deployed onto acentralized ecoware planner node and are synchronized.
after each control step the outputs of these controllers are aggregated to create a new resource allocation le.
this le contains the number of cpu cores and memory units needed to sustain the current workload for each tier of each application.
one memory unit is equal to 512mb.
the planner translator takes as input the new desired resource allocations for each tier of each application and translates them into a mix of infrastructure adaptation actions that must be activated.
it uses the actions discussed in section create vm terminate vm create container scale container terminate container .
for example let us imagine that the controllers dedicated to managing a rubis application request that the load balancer tier receive cores and memory units and that the business logic tier receive cores and memory units.
in this case the list of adaptation actions could be to i create a new vm ii create a new container on that vm for the business logic tier with cores and memory units and iii update the container dedicated to load balancing by increasing its resources by cores and memory unit.
these actions will depend on the resources already associated with the various tiers at that point in time.
the planner translator uses the information it receives to search for a new vm and container allocation through the formulation of an integer programming problem i.e.
a variation of the two dimensional bin packing problem .
the bins are vms and we must pack the containers inside them.
the two dimensions that we take into account are cpu cores and memory.
to guarantee that the problem will terminate we calculate the upper bound for the number of vms as the total number of vms needed to satisfy the desired resource allocation for each tier in each application.
this number will depend on thevm flavor being used which is described in the ecowareapplications description le.
for example if tier t1 needs cores tier t2needs core and we are using a vm avor that provides cores then we would need vms for t1and vm for t2.
the upper bound for vms would be the sum that is vms.
we additionally consider the following constraint a vm can only host one container per tier of an application.
the reason is that we are able to vertically scale containers thus there is no need to create more than one instance per vm.
each of the ve infrastructure adaptation actions is associated with a weight w such that w terminatecontainer w scalecontainer w createcontainer w termi natevm w createvm .
these weights always favor container manipulation over vm manipulation moreover termination has a lower weight than creation to prefer the removal of unused resources.
we also add the additional constraint that the sum of the resources that are dedicated to the containers that are deployed on a single vm must be less than or equal to the total resources provided by the vm itself as in the bin packing problem .
by minimizing the weighted usage of containers and vms we can nd an optimal setup of containers and vms.
we solve the problem using or tools an ilp solver by google4.
its output is the input for our executor i.e.
the list of infrastructure adaptation actions that need to be performed.
execution the executor takes the list of actions created by the planner translator and enacts them using various actuators.
these actuators are used to ask the cloud provider to create terminate vms to ask docker to create update terminate containers and to access the container adaptation hooks defned in the applications description le.
when we started our project docker could not dynamically update the resources allocated to a running container.
this led us to extend docker5 however docker .
.
released in february of now supports an update command that ful lls our needs.
this command dynamically changes the cpu cores and the memory that are allocated to a container using options cpuset cpus and memory .
the former speci es what cpu cores the container can use e.g.
cpuset cpus states that the container can only use core andcore .
the latter limits the amount of memory that the container can use e.g.
memory 512m means that the container can only use 512mb of ram.
the same options are also available to the runcommand which is used to create a new container.
ecoware s executor node uses three sub modules topology graph topology manager and cloud driver .topology graph uses the ecoware applications description le to generate and maintain tier dependency graphs for the applications being managed.
topology manager keeps track of how the applications are deployed on containers and vms.
it also maintains the metadata required to interact with the containers vm and container ids ip addresses ports etc.
finally cloud driver is used to interact with the vm management apis of the selected cloud provider.
our current implementation of ecoware supports aws ec2 as our main public cloud provider and uses vagrant for simple private cloud setups.
however the system is modular and can support new cloud providers.
ecoware agent s are pre installed into the vm image that is speci ed in the ecoware applications description le.
the agents ensure that the sets of cpu cores allocated to different containers do not intersect i.e.
each cpu core must be used by only one container and invoke the adaptation hooks when speci c events occur in the system.
.
adaptation hooks and processing model adaptation hooks are the mechanism through which ecoware keeps the planner technology and application agnostic yet still o ering the capability of performing platform adaptations.
adaptation hooks are bash or python scripts that are mounted directly onto their related docker containers.
these scripts are launched by the ecoware agent s using the docker exec command.
the on node scale hook takes as input the previous and the new resource allocations old cpu cores new cpu cores old mem units new mem units .
the on dependency scale hook takes as input the old tier state and the new tier state .
these parameters contain metadata about the old and the new states of the nodes in a tier that the container depends on e.g.
the ip address of the nodes the resources allocated to the nodes etc.
.
the executor identi es the order of action execution.
it starts by executing all the vm adaptation actions i.e.
vm 5the forked version of docker is available at https github.com deib polimi ecoware dockercreates and terminates through cloud driver .
these actions are not application speci c and they take a relatively long time to execute.
as soon as the new vms if any are up and running the cnt adaptation actions are executed through the ecoware agent s installed on each machine.
the order in which the cnt adaptation actions and adaptation hooks are executed depends on the dependencies stored in the executor s topology graph sub module.
if a tiert1depends on tier t2 we start by executing the cnt actions of t2 whilet1waits.
the ecoware agent s in t2proceed to execute their assigned cnt adaptation actions.
if the cnt adaptations they must execute are a create container or a scale container once that action has been completed the agent proceeds to execute that container s on node scale hook.
when all the cnt adaptation actions and on node scale hooks for all the containers in tier t2have completed adaptation ont1can be re activated but only if t1does not depend on any other tiers that are still performing adaptation.
adaptation for t1proceeds in the following order.
its ecoware agents start by executing the cnt adaptation actions for the containers of t1 proceed to execute the on node scale hooks for those containers and conclude by executing their on dependency scale hooks.
this is possible since all the tier s dependencies have been previously resolved.
once completed if there are any tiers that in turn depend ont1they will activate their adaptation and so on.
.
evaluation we evaluated our work by using two web applications rubis and pwitter.
rubis is a well known internet application benchmark that simulates an on line auction site.
our deployment of rubis uses three tiers a load balancer a scalable pool of jboss application servers that run java servlets and a mysql database.
pwitter was developed in house6.
it is a simple twitter like social network that stores pweets i.e.
texts that are not limited in length and that are indexed by the polarity of their sentiment.
pwitter is written in python and is also a tier application.
it has a load balancer a scalable pool of gunicorn application servers and a redis database.
for our experiments we implemented two application speci c sensors they capture the data required to calculate the average response time of a jboss application server and of a gunicorn application server respectively.
we also implemented two application speci c adaptation hooks to enable platform adaptation these hooks dynamically recongure jboss and gunicorn to better exploit the available resources following industry best practices .
for example best practices for gunicorn suggest that the number of workers be numcores .
the goal of our experiments was to maintain the average response time of the application servers below a certain threshold.
after a pro ling phase we set the threshold sla to seconds for both applications.
indeed both are able to sustain this value under various kinds of workloads using from to cores i.e.
the maximum amount of resources that we can a ord for the experiments .
the experiments answer the following questions 6pwitter is available at pwitter.
question if we only use vms and no containers will ecoware perform better or worse than the current state of practice i.e.
aws autoscaling ?
question if we take into account containers will ecoware perform better or worse than with vms only?
performance is evaluated using the core second metric i.e.
we calculate how many cores are used during the experiments.
the lower the value the better the adaptation works.
to simulate a varying workload we used jmeter .
question .
we compared ecoware against amazon ec2 s autoscaling capabilities.
we made this choice because it is in practice the most widely adopted scaling solution.
for both applications we focused on adapting the business logic tiers.
we created two autoscaling groups on ec2 one per application.
these groups were con gured to scale from to t2.small vm instances each instance had cpu core and 2gb of memory .
both autoscaling groups were con gured to use amazon elastic load balancers.
the databases were deployed on m4.xlarge instances each instance had cpu cores and 16gb of memory .
with this setup and workload the databases were over allocated and never became the bottleneck during the experiments.
the ec2 autoscaling groups were each given two policies.
the rst added vm instance when the average cpu utilization for the entire group was over for interval of minute.
the second removed vm instance when the average cpu utilization for the entire group was below for interval of minute.
these thresholds were chosen following real world best practices .
we also decided to set the duration of the adaptation control to be as fast as possible to allow the aws autoscaling system to be as quick as possible to react to workload changes.
note however that amazon s autoscaling does not activate a scaling action if another action is executing.
this means that for example when a new vm is added we must wait for it to turn on boot up and be linked to the load balancer.
we measured that on average it took a vm seconds to complete its boot up process this was evaluated from the launch of the vm creation command to when it had successfully booted and linked to the elastic load balancer.
this has two intertwined consequences.
first to preserve the unity relative degree hypothesis see hypothesis in section .
the control period had to be longer than this delay.
second unless an unacceptably long period was selected a conservative control action i.e.
an alpha close to one was required to prevent oscillations or even instability.
this is not equally relevant if containers are used allowing for a lower alpha and for a more aggressive control action and stronger disturbance rejection.
for these reasons we parameterized our planner as follows we set the control interval to seconds we measured the response time and averaged it over seconds we set the planner to always read the most up to date measured response time before computing the next plan and we set the single pole alpha to .
we also de ned the set point of the planner to be seconds i.e.
about less than the sla.
this way the planner would have been able to range near the set point without violating the sla.
finally we stimulated the two applications with three di erent workloads low medium and high.
all the experiments lasted minutes the number of users for each interval in minutes is shown in table .
we repeated the experiments ve times.
a aws pwitter low b aws pwitter medium c aws pwitter high d ecoware pwitter low e ecoware pwitter medium f ecoware pwitter high g aws rubis low h aws rubis medium i aws rubis high j ecoware rubis low k ecoware rubis medium l ecoware rubis high m ecoware pwitter float n ecoware cnt rubis high o ecoware cnt pwitter floatfigure obtained results.
cnt stands for containers .table workloads number of users per time interval in minutes .
experiment pwitter low pwitter med pwitter high rubis low rubis med rubis high pwitter float figures a c show aws autoscaling s behavior with pwitter stimulated with the three di erent workloads.
this behavior is compared against the experiments illustrated in figures d f which used ecoware s adaptation capabilities.
the horizontal dotted line is the sla and the thin line is the response time they both refer to the left y axis.
the thicker line represents the allocated cores and refers to the right y axis.
the metric core second corresponds to the area under the thicker line cores .
aws autoscaling clearly over allocates resources.
this occurs even for lowworkloads and even if we used a very high threshold for cpu utilization.
this is because both jboss and gunicorn tend to use nearly of their allocated cpus even with moderate workloads.
furthermore this value is also a ected by how quickly a spike in the number of users is reached.
ecoware on the other hand allocates less cores on average.
the sla is only violated with the high workload for around minutes which is understandable since the approach is reactive.
after a large spike ecoware overallocates resources for one control interval and then slowly deallocates them and converges to a stable value as can be seen in figure d at minute and in figures e f at around minutes and .
though the two example applications are completely di erent we see similar results in figures g l which focus on rubis.
ecoware never incurs in violations while aws over allocates resources in all the scenarios.
aws can only add a static number of vms when reacting to an increment in cpu utilization so it is easy to nd a workload with high spikes in which aws is too slow to allocate resources causing various sla violations to occur.
we did not nd it useful to show such a case given the lack of space.
table shows the results of these experiments using the core second metric.
ecoware outperforms aws by on average when managing pwitter and by on average when managing rubis.
question .
with the second set of experiments we wanted to assess the bene t of using containers instead of focusing solely on vms.
to do this we compared ecoware as used previously against a new deployment that used an amazon m4.2xlarge vm instance cores and 32gb of memory .
on this machine we installed the dockerized versions of the business logic tiers of the two applications7.
our hypothesis was that the two applications had di erent workloads which means that there had to be moments in which one application would request a lot of resources while the other would not.
this allows us to exploit the advantages of using containers.
we only used one vm in our experiments to make the bene ts of using containers emerge clearer.
7dockerized rubis can be found at com u polimi .table aws vs ecoware without containers.
values are in cpu core second .
experiment aws ecoware gain pwitter low pwitter med pwitter high rubis low rubis med rubis high table ecoware with vms vs ecoware with containers.
values are in cpu core second .
experiment vms containers gain pwitter float rubis high the use of containers allows ecoware to have a faster control rate since creating updating and terminating containers can be done in milliseconds around to ms .
we parametrized the planner with a control interval of seconds we measured the response time and averaged it over seconds we set the planner to always read the most upto date measured response time before computing the next plan and we set the single pole alpha to .
we then simulated two di erent workloads for pwitter and rubis simultaneously.
for rubis we used the high workload for pwitter we used the oat workload see table .
figure m shows that ecoware without containers violated the sla on pwitter and allocated a peak of cores at minute .
figure o shows that ecoware with containers never violated the sla and remained quite close to the optimal resource allocation core for users cores for users .
similar results emerged with rubis.
working with vms only figure l required allocating up to cores while the use of containers figure n allowed us to allocate no more than cores the optimal allocation for users is cores .
this is due to the di erent parametrization of the planner and to the di erent control intervals.
table shows the results in detail.
if we aggregate the two experiments ecoware with containers outperforms ecoware with vms by .
to conclude the experiments show that a high level of cpu utilization does not always equate to a saturated system.
indeed both jboss and gunicorn continue to operate acceptably i.e.
they continue to satisfy their slas even when their cpu utilization is close to see figure a g .
furthermore the experiments also show that in these two applications memory saturation is not an issue.
.
threats to validity even though we performed our experiments on two applications built on di erent technologies the two are similar both in terms of domain and architectural design.
they are both cpu bound and in both cases the bottleneck lies in the application tier.
nevertheless these two applications cover a wide range of real world cases.
further research will evaluate ecoware with applications that follow a completely di erent architectural design e.g.
map reduce media streaming applications etc.
.
in our experiments we used the cloud infrastructure in two di erent ways depending on whether we were dealing with vms or with containers.
when dealing with vms we used up to ec2 instances per application each vm had core.when dealing with containers we used a single ec2 instance with cores and shared it between both applications.
the reason for using a vm with cores is that they were enough to satisfy the slas given the used workloads.
the reason for this di erence is that to truly take advantage of container technology one must have multiple applications running with di erent workloads on multiple shared vms.
this kind of setup is becoming increasingly common in companies due to the success of micro service architectures.
furthermore to e ciently manage containers one should have a number of cores on each shared vm that is su ciently large so that cnt adaptations can be e ective.
ideally one should have at least a number of cores that is equal to the number of application tiers being managed on that vm plus one.
memory was not an issue in these experiments.
our controller only computes the amount of cores required per tier and requests an appropriate amount of memory units based on this computation i.e.
the controller requests 1gb of ram per core.
this is always possible because ec2 s vms always provide more gbs of ram than cores.
the importance of memory must be re evaluated with applications that are memory bound this will be part of our future work.
.
related work the solution presented in this paper nds competitors in both industry and academia.
amazon recently presented the ec2 container service which allows one to deploy docker containers inside an ec2 vm cluster.
ecoware is more open and exible it provides full access to docker to build the adaptation hooks mechanism and allows for the vertical scaling of containers.
apache mesos is a cluster manager that supports docker containers.
it is a middleware platform that hides the complexity of a distributed infrastructure and provides application level apis for many services e.g.
hadoop spark and elastic search for resource management and for scheduling.
google kubernetes is another container cluster manager.
it o ers a comprehensive solution for deploying applications using containers and o ers an api for scaling and replicating groups of containers.
both mesos and kubernetes focus on the infrastructure layer and lack comprehensive multi level adaptation.
ecoware may embed these solutions as alternative ways to manage containers and it could complement them with advanced adaptation capabilities.
cloudify can only drive infrastructure auto scaling based on a simple rule based solution very similar to amazon autoscaling.
as for research initiatives brun et al.
witness the importance of control loops and autonomic computing when dealing with complex software systems while dustdar et al.
provide an interesting de nition of elastic computing.
they advocate that elastic processes should be modeled on three metrics resources cost and quality.
our work focuses on resources and cost and partially on quality.
our main goal is to keep response time under a certain threshold while quality provides more facets and ecoware could be further extended to support them in the future.
di erent approaches deal with the problem of resource allocation.
for example hu et al.
propose a resource allocation system for cloud computing.
they render application environments aes by means of queuing models and use a global arbiter to allocate resources among aes.
ardagna etal.
present a game theoretical approach for the runtime management of the infrastructure of a iaas provider that rents resources vms to multiple saas providers to ful ll their slas.
they formulate the generalized nash equilibrium problem and prove the existence of at least one equilibrium.
padala et al.
present a hierarchical controller that allocates resources to multiple multi tier applications in data centers.
they show how the controller can improve the utilization of servers and help applications meet their slas.
our approach is di erent with respect to these works and to other similar ones e.g.
because of the use of containers and the ability to exploit platform adaptation.
furthermore our solution adopts the perspective of cloud users with no access to the policies that regulate the behavior of the hypervisor.
finally as for feature adaptation klein et al.
introduce the brownout adaptation paradigm based on optional code that can be dynamically de activated through a controller.
as already said we are con dent that our adaptation hooks mechanism can also support feature adaptation and provide a similar solution but this is not the focus of this paper.
.
conclusions and future work the paper proposed a novel self adaptation framework based on i vm and container based resource management ii coordinated infrastructure and platform adaptation and iii a planner that implements a discrete time feedback controller.
experiments demonstrate that negrained adaptation capabilities can greatly improve performance when autoscaling cloud based web applications.
our future work comprises the integration of feature adaptation by extending the adaptation hook mechanism an extension of the planner to make it work hierarchically with respect to the controlled resources an even ner grained solution to control the cpu cores allocated to a container and further evaluation on more case studies of di erent kinds.