singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems deepstellar model based quantitativ e analysis of stateful deep deepstellar model based quantitativ e analysis of stateful deep learning systems learning systems xiaoning du xiaof ei xie singapor e management univ ersity xfxie smu.edu.sg yi li lei m a yang liu see next page for additional authors follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the os and networks commons and the softwar e engineering commons citation citation du xiaoning xie xiaof ei li yi m a lei liu y ang and zh ao jianjun.
deepstellar model based quantitativ e analysis of stateful deep learning systems.
.
proceedings of the 27th a cm joint meeting on e uropean softwar e engineering conf erence and symposium on the f oundations of softwar e engineering t allinn est onia a ugust .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
author author xiaoning du xiaof ei xie yi li lei m a yang liu and jianjun zh ao this conf erence pr oceeding ar ticle is a vailable at institutional k nowledge at singapor e management univ ersity https ink.libr ary.smu.edu.sg sis r esear ch deepstellar model based quantitative analysis of stateful deep learning systems xiaoning du nanyang technological university singaporexiaofei xie nanyang technological university singaporeyi li nanyang technological university singapore lei ma kyushu university japanyang liu nanyang technological university singapore zhejiang sci tech university chinajianjun zhao kyushu university japan abstract deep learning dl has achieved tremendous success in many cutting edgeapplications.ho wever the state of the artdlsystems stillsufferfromqualityissues.whilesomerecentprogresshasbeen made onthe analysisof feed forwarddl systems little studyhas been done on the recurrent neural network rnn based stateful dlsystems whicharewidelyusedinaudio naturallanguagesand video processing etc.
in this paper we initiate the very first step towards the quantitative analysis of rnn based dl systems.
we modelrnnasanabstractstatetransitionsystemtocharacterize its internal behaviors.
based on the abstract model we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of rnns.
we further propose two algorithms poweredbythequantitativemeasuresforadversarialsampledetection and coverage guided test generation.
we evaluate deepstellar onfourrnn basedsystemscoveringimageclassificationandautomatedspeechre cognition.theresultsdemonstratethattheabstract modelisusefulincapturingtheinternalbehaviorsofrnns and confirm that the similarity metrics could effectively capture the differences between samples even with very small perturbations achieving accuracy for detecting adversarial samples and thecoveragecriteriaareusefulinrevealingerroneousbehaviors generating three times more adversarial samples than random testingand hundreds times more than the unrolling approach .
ccsconcepts computingmethodologies neuralnetworks software and its engineering software testing and debugging .
xiaofeixie xfxie ntu.edu.sg and leima malei ait.kyushu u.ac.jp are the corresponding authors.
permissionto make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn ... .
deep learning recurrent neural network model based analysis adversarialsample testing acm reference format xiaoningdu xiaofeixie yili leima yangliu andjianjunzhao.
.
deepstellar model based quantitative analysis of stateful deep learning systems.
in proceedings of the 27th acm joint european software engineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse august tallinn estonia.
acm new york ny usa 11pages.
introduction deeplearning dl hasexperiencedsignificantprogressoverthe pastdecadeinmanyreal worldapplicationssuchasimageprocessing speechr ecognition natural language processing and autonomous driving .
however the state of the art dl systemsstillsufferfromquality reliabilityandsecurityproblems which could lead to accidents and catastrophic events especially whendeployedonsafety andsecurity criticalsystems.wehave witnessed manyquality and securityissues such asone pixel attack alexa sirimanipulationwithhiddenvoicecommand andthegoogle uberself drivingcaraccidents .anearlystage assessment of dl systems is of great importance in discovering defectsand improving the overall product quality.
althoughanalysisprocessesandtechniquesarewell established fortraditionalsoftware existingtechniquesandtoolchainscould notbedirectlyappliedtodlsystems duetothefundamentaldifferences in the programming paradigms development methodologies aswellasthedecisionlogicrepresentationsofthesoftwareartifacts e.g.
architectures .
to bridge the gap research on testing verification and adversarial sample detection of feed forward neural networks fnn e.g.
convolutionneuralnetworks cnn andfullyconnectedneural networks started to emerge recently.
yet the existing techniques are not speciallydesigned to be applicable to rnn.
particularly in contrast to fnn rnn captures thetemporalbehaviorsbyloopsandmemorizationwithinternal states to take into account the influence of previous or future observations.the architecture ofa simple rnn is shown infig.
.
a simple rnn is a network of neuron like nodes organized into successive iterations .
it takes as inputs both the data stream and the internal state vector maintained.
instead of taking the input data as a whole rnn processes a small chunk of data as it arrives 477esec fse august tallinn estonia xiaoning du xiaofei xie yi li lei ma yang liu and jianjun zhao si si si rnn input outputstate vector x0... xi xixi ...y0... yi yiyi ... input output s0 unroll figure architecture of a simple rnn.
andsequentiallyproducesoutputsineachiterationwhileupdating theinternalstates.inotherwords informationinanrnnnotonlyflowsfromfrontneurallayerstotherearones butalsofromthecur rentiterationtothesubsequentones.thestatefulnatureofanrnn contributes to its huge success in handling sequential data such as audios and natural languages.
at present long short term memory lstm andgatedrecurrentunit gru areamong the state of the art and most widely used rnns designed with enhancementtoovercomethe vanishinggradientproblem that existsinthetrainingprocessofmostdlsystems andisaggravated by the iterative design of vanilla rnns.
although recent work mentions the possibility to analyze an rnn through direct unrolling treating it as an fnn such a strategy is still far from sufficient to handle inputs of varied lengths.differentfromanfnn whereeachlayerhasafixedrole in feature extraction a layer in an unrolled rnn often does not preservethesamefeaturelatentspacefunction orsemantics for different input sequences.
therefore the same unrolling which works well for one input may not fit for another.
in addition there could be scalability issues when the input sequences are extremely long.
tobettercharacterizetheinternalbehaviorsofrnns weproposedeepstellar a general purpose quantitative analysis frameworkforrnn baseddlsystems.consideringitsstatefulnature we first model an rnn as discrete time markov chain dtmc to capture its statistical behaviors.
based on the dtmc model wedesigntwotracesimilaritymetricstoquantifytheprediction proximity of different inputs and five coverage criteria to measure the adequacy of test data from different perspectives.
to further demonstratethe usefulnessof deepstellar we developalgorithms for two applications based the quantitative analysis namely rnn testingandadversarialsampledetection towardsaddressinghighly concerned issues at present in both academia and industry.
we implemented deepstellar and empirically evaluated the usefulness of the abstract model similarity metrics and coverage criteria on four rnn based systems from image classification to automatedspeechre cognition asr .specifically wefirstperformed controlled experiments to evaluate the capability of the abstract model.theresultsdemonstratethat thetracesimilaritymetrics serve as good indicators of the discriminatory power of rnns and the abstract model is sensitive enough to distinguish inputs generated with very small perturbations and the coverage criteria derived from the abstract model are able to measure test adequacy andareeffectiveinmanifestingerroneousbehaviors.further we appliedthemetricsandcriteriaontwoapplications i.e.
adversarial sampledetectionandcoverage guidedtestingofrnns.theresults show that deepstellar detects and of the adversarial samples respectively forasrandimageclassificationsystems and generates tests with high coverage and yields at most hundredsoftimesmoreadversarialsamplesthanrandomtestingandexisting neuron coverage guided testing with unrolling .
themain contributionsofthispaperaresummarizedas follows weproposetoformalizeanrnn basedstatefuldlsystemasa dtmc model to characterize the internal states and dynamic behaviors of the systems.
basedonthedtmcabstraction wedesigntwosimilaritymetrics and five coverage criteria for stateful dl systems which areamongthefirsttoquantifysampledifferencesandtestdata adequacy for rnns.
withdeepstellar we design two algorithms for detecting adversarial samples and conducting guided testing for rnns based on the metricsand criteria.
weconductin depthevaluationtodemonstratetheusefulness ofdeepstellar withcontrolledexperimentsaswellastwotypical real world applications.
overview fig.2summarizes the workflow of our approach including the abstract model construction of rnn different quantitative measures defined over the abstract model and two applications to detect and generateadversarialsamplesof rnns.
the abstract model construction module takes a trained rnn as input and analyses its internal behaviors through profiling.
the inputsforprofilingarefromthetrainingdata whichcanbestreflect the characteristics of a trained rnn model.
specifically each input sequence isprofiled toderive a trace i.e.
asequence ofrnn state vectors.
after the profiling we can get a set of traces which record the states visited and transitions taken during the training stage.
inpractice theinternalstatespaceofanrnnandthenumber oftracesenabledbythetrainingdataareoftenbeyondouranalysis capability.therefore weperformabstractionoverthestatesand tracestoobtainanabstractmodelthatcapturestheglobalcharacteristics of the trained network.
at the state level we apply principle componentanalysis pca toreducethedimensionsofthe state vectors and keeps the first kmost dominant components.
for each of the kdimensions we further partition it into mequal intervals.
at the transition level we consolidate concrete transitions into abstract ones according to the abstract states.
we also take intoaccountthefrequenciesofdifferenttransitionsateachstate and effectively derive a discrete time markov chain dtmc model for the trained rnn.
based on the abstract model we design two metrics for evaluating the trace similarity induced by different inputs and five coveragecriteriatofacilitatethesystematictestingofrnns.the metrics and coverage criteria are designed from both the state and transition level.
specifically the trace similarity metrics in cludestate based trace similarity sbtsim and transition based tracesimilarity tbtsim .thecoveragecriteriaincludethe basic statecoverage bscov n stepstateboundarycoverage n sbcov weightedstatecoverage wscov basictransitioncoverage btcov andweighted transition coverage wtcov .
we then apply the metrics and criteria on two applications i.e.
theadversarialsampledetection andcoverage guidedtesting bothof whichaimtomitigatethethreatsfromadversarialsamples.with thesimilaritymetrics weproposeanapproachtodetectadversarial 478deepstellar model based quantitative analysis of stateful deep learning systems esec fse august tallinn estonia state trace statisticsstateful dnn abstraction model abstract model construction states and transitions profiling state abstraction transition abstraction dimension reduction interval abstractiontrace similarity quality measure.adversarial samples detection coverage guided testing quantitative indicators typical applications ... similarity metrics coverage criteriastsim ttsim bscov k sbcovbtcov wtcov state level trans.
levelwscov i figure overview of deepstellar and its typicalapplications.
x0 y0s0 s1s2 s prime 2s3x1 y1x2 y2 x prime y prime 2x prime1 y prime figure an example fst representing two traces.
samplesatruntime.withthecoveragecriteria wedevelopatesting framework to guide test generation with the aim to improve coverage and uncover defects for quality assurance.
the two techniques are complementary to each other.
the testing technique aims to generate unseen adversarial samples that help developers analyze andimprovetherobustnessofthemodel.theadversarialsample detectiontechniqueisabletoidentifymaliciousinputsandprevent potential damages at runtime.
state transition modeling of rnn .
rnninternal statesandstate transitions following werepresentaneuralnetworkabstractlyasadifferentiableparameterizedfunction f .theinputtoanrnnisa sequence x xn wherexistheinputdomainand nisthelength of the sequence.
let xi xbe thei th element of the sequence x.then whenpassing xintoanrnn itmaintainsastatevector s snwiths0 0and si yi f si xi wheresis the domainofthehiddenstate si sisthehiddenstateofrnnatthe i thiteration and yi oisthecorrespondingoutputatthatstep.
we usesd ito denote the d thdimension of the state vector si.
naturally each input sequence xinduces a finite sequence of state transitions t which we define as a trace.
thei th element in a trace t denoted by ti is the transition from sitosi 1after accepting an input xiand producing an output yi.
a finite state transducer fst canbeusedtorepresentacollectionoftraces more compactly as defined below.
definition1.
anfstisatuple s x o i f suchthat sisa non emptyfinitesetofstates xistheinputalphabet oistheoutput alphabet i sis the set of initial states f sis the set of final states and s x o s is the transition relation.
for example fig.
3shows a simple fst representing two traces namely s0s1s2s3ands0s1s prime 2s3withs0beingtheinitialstateand s3 beingthefinalstate.thefirsttracetakesaninputsequence x0x1x2 and emits an output sequence y0y1y2 the second trace takes an inputsequence x0x prime 1x2and emitsan output sequence y0y prime 1y2.
.
abstractstate transition model thenumberofstatesandtracesenabledwhiletraininganrnncan behuge.toeffectivelycapturethebehaviorstriggeredbyalarge numberofinputsequencesandbettercapturetheglobalcharacteristicsofthetrainednetwork weintroducean abstractstatetransition modelin this paper.
the abstract model over approximates the observed traces induced of an rnn and has a much smaller set of states and transitions compared with the original one.
the abstractionisalsoconfigurable onecantrade offbetweenthesizeand precision of the model so that the abstract model is still able to maintainusefulinformationoftheinputsequencesforparticular analysistasks.
to obtainan abstractmodel foratrained rnn we abstractover both the states and the transitions.
stateabstraction.
eachconcretestate siisrepresentedasavector s1 i ... sm i usuallyinhighdimension i.e.
mcouldbealargenumber .
intuitively an abstract state represents a set of concrete states whicharecloseinspace.toobtainsuchastateabstraction wefirst apply the principle component analysis pca to perform an orthogonaltransformationontheconcretestates findingthefirst kprinciplecomponents i.e.
axes whichbestdistinguishthegiven state vectors and ignore their differences on the other components.
this is effectively to project all concrete states onto the chosen k dimensionalcomponent basis denoted as pca k .
then we split the new k dimensional space into mkregular grids suchthatthereare mequal lengthintervalsoneachaxis ed i lbd i ubd lbd m lbd i ubd lbd m whereed irepresentsthe i thintervalonthe d thdimension lbdand ubdare the lower and upper bounds of all state vectors on the d th dimension respectively.
in this way all concrete states siwhich fall within the same grid are mapped to the same abstract state s si s1 i e1 sk i ek .we denote the set of all abstract states as s. noticeably the precision of the state abstraction can easily be configured by tuning the parameters kandm.
letj id s betheindexof sonthed thdimensionsuchthat for alls s sdfalls ined j j m .
for any two abstract states s and s prime we define their distanceas dist s s prime k d id s id s prime .
this definition can also be generalized to include space beyond the lower and upper bounds.
479esec fse august tallinn estonia xiaoning du xiaofei xie yi li lei ma yang liu and jianjun zhao s0 s1 s3 s2t1 t2t3 a example concrete traces.
s0 s1 s2 s31 .
.
.
.
b dtmc abstraction.
figure a set of concrete traces and their corresponding abstractstate transitionmodel.
transitionabstraction.
oncethestateabstractioniscomputed aconcretetransitionbetweentwoconcretestatescanbemapped as a part of an abstract transition .
an abstract transition represents asetofconcretetransitionswhichsharethesamesourceanddestinationabstractstates.inotherwords thereisanabstracttransition between two abstract states sand s primeif and only if there exists a concretetransitionbetween sands primesuchthat s s s prime s prime.the set of all abstract transitions is denoted as s s. for instance fig.
4adepicts three concrete traces i.e.
t1 t2and t3 wherestatesareshownasdotsandtransitionsaredirectededges.
thegridsdrawnindashedlinesrepresenttheabstractstates i.e.
s0 s1 s2 and s3 eachofwhichismappedtoasetofconcretestates inside the corresponding grid.
the set of abstract transitions is therefore s0 s1 s1 s0 s1 s1 s1 s2 s1 s3 s3 s3 .
.
trace similarity metrics to precisely compare two input sequences we define the trace similaritymetrics toquantifytheproximityoftheirinducedstate transitions on the abstractmodel.
given an abstract model mand an input x we denote the set of abstract states and transitions coveredby xas sxand x.then the state andtransition basedtrace similaritymetrics forthetwoinputs xandyaredefinedbasedonthe jaccardindicesoftheirstatesandtransitionscovered respectively stsimm x y sx sy sx sy ttsimm x y x y x y .
the trace similarity metrics range over where indicates disjoint sets i.e.
traces induced by xandyare totally different while1 indicatingequal sets i.e.
the traces are similar .
fig.5shows the concrete traces on an rnn based asr model inducedbytwos peechinput samples fig.
5a thisbookisabout science and fig.
5b this book is about literature .
the darker dots appear earlier in the sequence and vice versa.
we can see a cleardifferenceofthetwoatlaterpartsofthesequence.thetwo concrete traces are then projected onto the dimensional space underthepca 3abstractionwith5intervalsoneachdimension tocalculatethetracesimilarities.thestate andtransition based tracesimilaritiesof the two inputs are .
and .
respectively.
eachinputsequenceinthetrainingsetyieldsaconcretetrace of the rnn model.
the abstract state transition model captures all theconcretetracesenabledfromtrainingdata oritsrepresentative parts and other potential traces which have not been enabled.
the a thisbookisaboutscience.
b thisbookisaboutliterature.
figure visualization of concrete traces of two audio over an rnn based asr model with the pca abstraction.
defined state and transition abstraction make the resulting abstract modelrepresentanover approximationandgeneralizationofthe observed behaviors of the trained rnn model.
.
representing trained rnn as a discrete time markov chain toalsotakeintoaccountthelikelihoodoftransitionsatdifferent states weaugmenttheabstractmodelwithtransitionprobabilities effectively making it a discrete time markov chain dtmc .
definition .
a dtmc is a tuple s i t where sis a set of abstract states iis a set of initial states and t s s mapsto isthetransitionprobabilityfunction whichgivestheprobabilityof differentabstract transitions.
we write pr s s prime to denote the conditional probability of visiting s primegiven the current state s such that s prime spr s s prime .
we define the transition probability as the number of concrete transitions from sto s primeover the number of all outgoing concrete transitionsfrom s i.e.
pr s s prime s s prime s s s prime s prime s s s .forexample fig.4bshowstheabstractstatetransitionmodelfortheconcrete traces in fig.
4aas a dtmc.
the abstract transitions are labeled with their transition probabilities.
for instance since all outgoing transitions at s0end in s1 the transition probability from s0 to s1is .
there are four possible outgoing transitions at s1 i.e.
s1 s1 s1 s2 s1 s3 s1 s0 .
hence the transition probability from s1to s2iscomputedas pr s1 s2 .computationforother abstracttransitionsare similar.
asisshownintheexample adtmcmodelisconstructedbyfirst applyingthestateandtransitionabstractionsonasetofconcrete traces and then computing transition probability distributions for each abstract state.
the time complexity of the abstraction step dependsonthenumberofconcretetraces whilethecomplexityfor computingthetransitionprobabilitiesonlydependsonthenumber of abstract transitions.
coverage criteria for rnn inspired by traditional software testing we propose a set of testing coverage criteria for rnns based on the abstract state transitionmodel.
the goal of the rnn coverage criteria is to measure the sufficiencyoftestdatainexercisingthetrainedaswellastheunseen behaviors.
the state and transition abstractions are designed to reflecttheinternalnetworkconfigurationsatacertainpointaswell asthetemporalbehaviorsofthenetworkovertime respectively.
therefore tomaximizethechanceofdiscoveringdefectsinstateful 480deepstellar model based quantitative analysis of stateful deep learning systems esec fse august tallinn estonia neuralnetworks oneshouldcombinecoveragecriteriabased on boththestateandtransitionabstractionstosystematicallygenerate comprehensive and diverse test suites.
letm s i t beanabstractmodelofthetrainedrnnrepresented as a dtmc.
let t x0 ... xn be a set of test input sequences.wedefineboththe state level andtransition level coverageofttomeasurehowextensively texercisesthestatesand transitionsof m respectively.
.
state level coverage criteria the state level coverage criteria focus on the internal states of the rnn.
the set of abstract states srepresents a space generalization ofthevisitedstatesobtainedfromtrainingdata oritsrepresentative parts which is referred to as the major function region .
the space outside the major function region is never visited by the trainingdata andthusrepresentsthe corner caseregion .the test data should cover the major function region extensively to validate the trained behaviors and cover the corner case region sufficientlyin order to discover defects in unseen behaviors.
basicstatecoverage.
given an rnn abstract model mand a set oftestinputs t thebasicstatecoverage measureshowthoroughly t coversthemajorfunctionregionvisitedwhiletraining.toquantify this wecomparethesetofabstractstatesvisitedby thetraining inputs and the test inputs denoted by smand st respectively.
then thebasicstatecoverageisgivenbythenumberofabstract states visited by both the training and the test inputs over the number of states visited by the training inputs bscov t m st sm sm .
weighted state coverage.
thebasicstatecoveragetreatsevery state with equal weights.
during training not all states are visited equallyoftenandonemaywanttoemphasizemoreonsomestates thantheothers.totakeintoaccountthefrequenciesofdifferent statesandbeabletoassignweightstostates wedefinethe weighted state coverage and allow users to specify a weight function .
the defaultweightofanabstractstate sisdefinedastherelativefrequency of it among all the abstract states i.e.
w s s s s s wheres is the set of all distinct concrete states.
then the weighted state coverage is defined as wscov t m s st smw s s smw s .
inpractice theweightfunctioncanbedefineddifferentlyaccording to specific needs.
for example when a constant function is chosen the weighted state coverage is equivalent to the basic one.
in .
we evaluate two weight functions including the default and the reversed one which assigns larger weight to less visited states.
n stepstateboundarycoverage.
thetestdatamayalsotrigger newstatesthatarenevervisitedduringtraining.the n stepstate boundary coverage measures how well the corner case regions are covered by the test inputs t. the corner case regions smcare the setofabstractstatesoutsideof sm whichhavenon zerodistances fromanystatesin sm.then smccanbefurtherdividedintodifferent boundary regions defined by their distances from smc.
for example the n step boundary region smc n contains all abstractstateswhichhaveaminimaldistance nfrom sm ormoreformally smc n s smc min s prime smdist s s prime n .
then step state boundary coverage is defined as the ratio of states visited by the test inputs in the boundary regions of at most nstepsaway from sm n sbcov t m st uniontext.1n i 1 smc i uniontext.1n i 1 smc i .
.
transition level coverage criteria thestate levelcoverageindicateshowthoroughtheinternalstates ofanrnnareexercisedbutitdoesnotreflectthedifferentways transitionshavehappenedamongstatesinsuccessivetimesteps.
the transition level coverage criteria targets at the abstract transitions activated by various input sequences and a higher transition coverage shows that the inputs are more adequate in triggering diverse temporal dynamicbehaviors.
basic transition coverage.
to quantify transition coverage we comparetheabstracttransitionsexercisedduringboththetraining and testing stages written as mand t respectively.
the basic transition coverage is defined as btcov t m t m m .
basictransitioncoveragesubsumesbasicstatecoverage.inother words for any abstract model m a test input tsatisfies basic transitioncoverage regarding to m also satisfies basic state coverage.
weightedtransition coverage.
similar as the state level coverage wecancalculatethe weightedtransitioncoverage byconsidering the relative frequency of each transition.
more formally wtcov t m s s prime t mw s s prime s s prime mw s s prime .
where the weight function can be configured similarly as in the weightedstatecoverage.bydefault theweightofatransition s s prime is computed as transition probability defined in .
.
applications todemonstratetheusefulnessoftheabstractmodelandtheproposed quality measures we apply them in finding adversarial samples for rnns in two scenarios adversarial sample detection to identifyadversarial inputs at runtime and coverage guided testingto generate unseen adversarial samples offline.
.
adversarial sample detection for rnns adversarial sample detection aims to check whether a given input is an adversarial sample at runtime.
we propose to use the trace similaritymetricstomeasurethebehavioraldifferencesbetween twoinputs.basedonthisidea wedevelopanewapproachtodetect adversarialsamplesfor rnns.
given a target sample i we define a reference sample rsuch that the rnn gives the same predictions for both iandr.
the traces derivedfromtheoriginalandthereferencesamplesaretypically similar when iis benign.
however when iis adversarial the trace difference between the two samples can be much larger.
with such 481esec fse august tallinn estonia xiaoning du xiaofei xie yi li lei ma yang liu and jianjun zhao algorithm1 training an adversarial detection classifier input d rnn based dl system m abstract model of d output c a classifier for detecting adversarial samples 1prepare benign set b adversarial set aand reference set r 2disb 3forb bdo 4r prime select r b 5vec 6forr r primedo r1 state vec1 predict r r r2 state vec2 predict r b j tracesimilarity state vec1 state vec2 m vec vec uniontext.
j 11d average vec 12disb disb uniontext.
d 13computedisasimilar with disb 14c linearregressionclassifer disa disb insight we propose to detect adversarial sample based on the trace difference from its reference sample.
weusealearning basedapproach algorithm totrainaclassifier.theinputsincludeanrnn basedsystem dandanabstract modelm.
we first collect a set of benign samples band a set of adversarialsamples a. the set of reference samples rare also prepared for comparison more details in the next paragraph and the tracesimilarities betweenbenign adversarial samples andthe reference samples are calculated.
for each benign sample b theselect function obtains a group of corresponding reference samples r prime from the reference samples r. based on this we compute the trace similaritybetween bandeachreferencesample r r prime lines6 and take the average similarity line to represents the distance betweenbandthesetofreferencesamples r prime.thedistanceisadded intothebenigndistancelist disb line12 .similarly wecompute the adversarial distance list disa line13 .
withdisaanddisb a linear regression classifier cis learned.
given a new input i w e computethesimilarity dibetweeniandthereferencesamples and rely onc di to indicate whether iis a benign or adversarial.
weapplyandevaluatethedetectionalgorithmsontwodomains namely asr and image classification.
the approaches used to generate reference samples are as follows for asr given an audio inputathatistranscribedtotexts tbythernn wegeneratethe reference audios using off the shelf text to speec h engines e.g.
google cloud te xt to speec h that generate the audio twith correct and clear pronouncing.
for image classification given anew image iwith prediction result c i.e.
the image ibelongs to classc the reference images are selected from the training data suchthatthey share the same label as c. .
coverage guided testing of rnns in this section we propose a coverage guided testing cgt techniquewhichaimstogenerateadversarialsamplesincorrectlyrecognized by an rnn.
cgt uses the proposed coverage criteria t o guidethetestgenerationandevaluatesthequalityofthetestsfrom different perspectives.
during the testing process cgt maintains atestqueue.ineachrun itselectsaseed i.e.
testcase fromthealgorithm2 coverage guided testing of rnn input i initial seeds d rnn based dl system m abstract model ofd output f failed tests q test queue 1f 2q i 3whilea select q do 4randomly pick transformation twith a random parameter p 5a mutate t p a 6fora prime ado result state vec predict r a prime cov covanalysis state vec m iffailed a result then f f uniontext.
a prime else ifcoverageincrease cov q then q q uniontext.1a prime updatecoverage q queue and generates multiple mutants.
a mutant is an adversarial sampleifitispredictedincorrectlybythenetwork.otherwise ifthe mutant improves the coverage it is then retained as an interesting seed and added back to the queue.
algorithm 2presentstheprocesstogeneratetestsforrnns.the inputs include the initial seeds i the rnn based dl system dand the abstract model m. the outputs are benign tests and failed tests for which dgives correct and incorrect inference respectively.
the initialtestqueuecontains a setof initial seeds.
in eachrun cgt selectsoneinput afromthetestqueue line andrandomlypicks a transformationfunction twithparameter p line4 .then aset of new samples aare generated under transformation t line5 .
for each new sample a prime cgt first obtains the concrete trace by lettingddotheinference line andthencalculatesthecoverage information over the abstract model line .
if the inference is incorrect a primeis added into the failed test set f line10 .
ifa primeis correctly predicted and covers new states or transitions cgt puts it in the test queue and updates the coverage criteria of all tests currently in the queue line .
a challenge in dl testing is the lack of oracle that tells the ground truth label of any new mutant.
mutation operators areoften specific to the application domains.
for cgt we mainly focus on image classification and apply the metamorphic mutation strategy for generating new mutants that would keep the prediction meaning from the human s perspective during testing.
evaluation to demonstrate the usefulness of the proposed techniques we implemented deepstellar in pythonbased on the keras .
.
with tensorflow .
.
and .
.
we first study whether the abstractmodelisabletocharacterizethestatefulbehaviorsofrnns i.e.
rq1 rq2 .basedonthis wefurtherevaluatetheusefulness of the proposed quantitative measures on the two applications i.e.
rq3 rq4 .specifically weleverage deepstellar toinvestigate the followingresearch questions rq1 are the proposed trace similarity metrics suitable indicators forthediscriminatory power of rnns i.e.
sensitive to even small perturbations on inputs ?
482deepstellar model based quantitative analysis of stateful deep learning systems esec fse august tallinn estonia table subject model information.
subject modelkernel rnn trainable acc.
type state vec.
shape parameters train.
test.
deepspeech .
.
bi lstm none deepspeech .
.
lstm none .
mnist lstm lstm none .
.66mnist gru gru none .
.
rq2 how sensitive are different coverage criteria for capturing erroneous behaviors of rnns?
rq3 how useful is the trace similarity based detection algorithm for detectingadversarialsamplesof rnns?rq4 how effective is the coverage guided testing in achieving highcoverage and generating adversarial samples of rnns?
.
experimentsettings modelsanddataset.
we selected four rnn based dl models includingtwoasrmodelsandtwoimageclassificationmodels which cover popular rnn variants see table .
the trainable parameter size often reflects the complexity of models and the models we selectrangefromsmall scaleoneswithabout60kparameters to practical sized ones with over million parameters.
the highest dimension of the rnn state vectors column state vec.
shape hits indicatingthe high complexity of the models.
for the asr tasks we selected two versions of mozilla pretrained deepspeech i.e.
.
.1and .
.
that are among thestate of the artopensourceasrmodelswithdifferenttypesof rnncore.
deepspeech .
.
adoptsabi directionallstm and deepspeech .
.
usesaone directionallstm.thestatevectors ofbothmodelsarewithahighdimensionof64 bitfloatingpoint type.
for the image classification task we followed the instructions and trained two rnn based classifiers on mnist dataset i.e.
mnist lstm minist gru thatachievecompetitiveaccuracy.these two models are relatively lightweight whose internal states are conveyed via dimensional vectors of bit floating points.
abstractmodelconstruction.
forasrmodels weuse common voice trainingdatasettoperformtheprofiling whichisusedfor thetrainingofthedeeps peechmodels.ov erall there are audios inthe dataset.each sampleis processedby bothmodels to collectthestatetraces.asfortheimageclassificationmodels we use the official mnist training dataset that contains images.
forthepcatransformation duetothehugesizeofstatevectors forasrmodels werandomlyselected20 ofstatevectorstofitthe pca model and use it for all further analysis.
for image models we use all of the state vectors.
the model abstraction parameters k andmcan be configured to generate dtmc models with different granularity.
table 2summarizes different configurations we evaluated aswell as thenumber ofabstract states andtransitions ineachobtaineddtmcmodel.notethatweuse k m torepresent a configuration with kdimensionsand mpartitions.
data preparation.
for a comprehensive evaluation of the proposed metrics and coverage criteria we prepared three types of samples originalbenignsamplesfromthetestdata perturbed samples which are generated by a slight perturbation on the original benign samples and adversarial samples from the original benign samples.
for asr models we use word error rate wer table abstract model details under different configurationsfor each studied rnn.
config.
deepspeech .
.
deeps peech .
.
mnist lstm mnist gru k m st. trans.
st. trans.
st. trans.
st. trans.
tomeasuretheinferenceprecision.inparticular benignsamples are with wer of zero.
the perturbed samples would have a relativelysmallerwerwhilethetargeted adversarialsampleshavea largerwer.forimageclassificationtasks theperturbedsamples we generate are with slight perturbations but still remain benign.
initially werandomlyselected100benignaudiosand100benign imagesseparatelyfromtheirtestdatasets fromwhichwegenerate the perturbed and adversarial samples.
for each asr model we generate perturbed audios from original benign oneswithexistingaudiodataaugmentationtechniques i.e.
speed and volume adjustment low high frequency filtering noise blending .
finally we only successfully generate adversarial samples for deepspeech .
.
because there exists a compatibility issue between deepspeech .
.
andtheadversarialattacktools w e used.
it is worth noting that the generation of targeted adversarial audios is rather computationally intensive and time consuming.
tobespecific weselectthe11commands asthetargetsand generate seeds targets adversarial audios which tookabout12daysintotalon4gpus i.e.
48daysv100gputime .
for each mnist model we also generate benign perturbed sampleswithexistingimagetransformationtechniques i.e.
imagecontrast brightness translation scaling shearing rotation and add white noise .
besides we generate adversarial samples with each state of the art attack tool including fgsm bim anddeepfool .
for both audio and image case we set conservative parameters fortransformationsothat theperturbationon originalsamplesis slight and imperceptible.
note that all the adversarial samples are alsowith minimalperturbations and not perceptible by human.
coverage criteria instances.
for then sbcov criteria we empirically study two instances with n andn denoted as sbcovand6 sbcov respectively.forthewscovandwtcov besidesthedefaultweightfunction whichassignslargerweightsto states or transitions with high visiting frequency during profiling we introduce another weight function to assign smaller weights tomorefrequentlyvisitedstatesandtransitionsbyinvertingthe originalweights.weusewscovandwtcovtodenotethecriteria with default weight function and refer wscov and wtcov to theinvertedones.thecriteriaallowobservinghowthetestdata cover states and transitions that have high low visiting frequency.
all the experiments were run on a server with the ubuntu .
systemwith28 core2.0ghzxeoncpu 196gbramand4nvidia tesla v100 16g gpus.
483esec fse august tallinn estonia xiaoning du xiaofei xie yi li lei ma yang liu and jianjun zhao table3 correlationoftracesimilaritiesandpredictiondiff.
config.
deepspeech .
.
deeps peech .
.
mnist lstm mnist gru k m st. tr.
st. tr.
u st. u tr.
u st. u tr.
.
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
.
.
.
.
.e .
.e .
.e .
.e .
rq1 trace similarity setup.weperformastatisticalanalysisonthecorrelationbetween thetracesimilarity andthepredictiondifference overtheslightly perturbedsamplesandtheiroriginalbenignsamples.thedifference is difficult to capture because the sample and its slightly perturbed counterpartareperceivedalmostthesamefromhumanperceptions.
wecomputethepredictiondifferenceofasrwiththeword level levenshtein distance of their transcripts and the prediction difference in image classification by checking whether they belong to different classes.
foraudios weusethe10 000perturbedsamples whicharewith variouslevenshteindistancescomparedwiththeiroriginalseeds.
for the image case we take the perturbed samples and also randomly select another samples from all the generated adversarial samples.this is to include both correctly and wrongly inferring perturbed images.
for the statistical analysis we use spearmanrank ordercorrelation denotedas toanalyzethe monotonic association between two variables for the asr models and we use mann whitney u test denoted as u to check the binary association for mnist models.
results.
table3showstheresultsofthecorrelationbetweentrace similarityandpredictiondifferencemeasuredovertheperturbed data wherecolumn st. andcolumn u st. representtheresults ofstsim andcolumn tr.
andcolumn u tr.
representtheresultsofttsim.thebesttworesultsofeachcolumnarehighlighted in bold font.
all reported correlations are statistically significant withp .
.
negative association of spearman correlation indicates that the larger the similarity metrics the less different thepredictedtranscriptswouldbe.formnistmodels themannwhitney u test results indicate that when measuring the trace similaritycomparedwiththeoriginalbenignsamples perturbed samplesobtainsignificantlylargervalues thanadversarialones.
answertorq1 bothstate andtransition leveltracesimilarity metrics are capable of capturing the prediction difference even for slightly perturbed samples.
thus trace similarity could be useful for detecting adversarial samples see rq4 .
.
rq2.
coverage criteria setup.in this experiment we evaluate the sensitivity of the proposedcoveragecriteriatoadversarialsamples.theabstractionconfigurationsusedinrq2areselectedbasedontherq1results.intable coverage criteria sensitivity to the slightly perturbed samples and adversarial samples.
sub.
conf.datastate transition mod.
k m bscov wscov wscov sbcov sbcov btcov wtcov wtcov o67.
.
.
.
.
.
.
.
o p74.
.
.
.
.
.
.
.
.
.
o a74.
.
.
.
.
.
.
.
.
.
ds1 o13.
.
.
.
.
.
.
.
o p26.
.
.
.
.
.
.
.
.
.
o a35.
.
.
.
.
.
.
.
.
.
o1.
.
.
.
.
.
.
.
o p7.
.
.
.
.
.
.
.
.
.
o a13.
.
.
.
.
.
.
.
.
.
o85.
.
.
.
.
.
.
.
o a85.
.
.
.
.
.
.
.
.
.
ml o71.
.
.
.
.
.
.
.
o a89.
.
.
.
.
.
.
.
.
.
o54.
.
.
.
.
.
.
.
o a81.
.
.
.
.
.
.
.
.
.
the coverage increase ratio value w.r.t.
its seed group are marked with grey background.
ordertobetterdifferentiatevariouscoveragecriteria threeconfigurations were selected from rq1 which are most sensitive to minor perturbationsaccordingtoeitherstsimorttsim.wecomparethe coverageresultsofthe100originalbenignsamples denoteas o andthecoverageachievedbyincludingperturbedsamples denoted aso p oradversarialsamples denotedas o a seecolumn data oftable4 .notethattheperturbedsamplesarenotincludedforthe mnistmodelsbecausetheyareallbenign.for deepspeech .
.
thenumberofadversarialsamplesis1 andwealsoselectthe same number of perturbed samples to make a fair comparison.
for othermodels we use all perturbed adversarial samples.
results.
table4reports the coverage results using different coverage criteria on different dataset see results of deepspeech .
.
andmnist gru on website .
the coverage increase ratio indicates the sensitivity of the coverage criteria to adversarial samples.
we observed that finer grained abstract models tend to have larger coverage increase ratio.
this is because finer grained state and transitioninformationismorelikelytodistinguishadversarialsamples fromthe benign samples.we also foundthat the sensitivities ofvariouscriteriatoadversarialsamplesareratherdifferent.for example theincreaseratioof wscov wtcovisrelativelysmall becausetheymainlyconcernthefrequentlycoveredstatesandtransitions duringprofiling whichareoftenalreadyfullycoveredby benignsamples.incontrast theincreaseratioof wscov wtcov is larger with a competitive performance as bscov and btcov indicatingrarelyvisitedstatesandtransitionstendtobecoveredby adversarial perturbedsamples.forthe n sbcovcriteria wepresent theincrease value instead the increase ratio as the initial criteria are zero.
the increase is not quite significant as these states are really hard to cover even by adversarial samples.
furthermore for deepspeech .
.
wefindthatthecoveragecriteriaof o aare generally higher than those of o p. this is possibly due to the larger average wer of the adversarial audios.
answertorq2 thetestcoveragecriteriaaremoresensitive with finer grained abstraction.
all proposed coverage criteria are sensitive to erroneous behaviors in adversarial samples amongwhich bscov andbtcovarethemostsensitiveones.
484deepstellar model based quantitative analysis of stateful deep learning systems esec fse august tallinn estonia table auroc results of trace similarity based adversarialdetectionby configurations.
config.
deepspeech .
.
mnist lstm k m stsim ttsimfsgm bim deepfool stsim ttsim stsim ttsim stsim ttsim .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rq3.
adversarial sample detection setup.thissectionevaluates deepstellar foradversarialsampledetectiononthreemodels namely deepspeech .
.
mnist ltsm andmnist gru .wefirstpreparedthebenign adversarial andreferencesamples referto b aandrinalgorithm totrainthelinear regressionclassifier.fortheasrmodel werandomlyselected1 benign samples for training and for testing from the test dataset tomakeanequalnumberasthe1 100generatedadversarial samples.
specifically the target commands are divided into two setsc1andc2 whichcontain8and3commands respectively.the adversarial samples whose prediction results belong to c1are used astrainingdata i.e.
whiletheotherastestdata i.e.
.
the reference samples are constructed by re transcribing all the prediction results of both adversarial and benign samples to audios withthegooglecloudtext to speech .formnistmodels we takethe9 000benignsamplesfromthetestdataofmnistandgenerate adversarial samples with each of the three approaches i.e.
fgsm bimanddeepfool of which are used for training and30 areusedfortesting.forreferencesamples werandomly selected50samplesfromthetrainingdataofministforeachof the categories i.e.
.
with the constructed dataset we trained a classifier for each model to detect adversarial samples.
results.
table5showstheauroc resultsofadversarialsample detection using different trace similarity metrics i.e.
stsim and ttsim and dtmc models with different configurations columnconfig.
.thebesttworesultsofeachmodelarehighlighted e.g.
.
for deepspeech .
.
.
formnist lstm and .
for mnist gru see website .
the best results indicate that state based trace similarity is a bit more effective than transition based one in many cases.
the results of mnist models on detecting attacks generated by different tools show that our algorithm is robustness to a wide range of attacks respectively with accuracy of and .
furthermore the results under dtmc models with different configurations vary largely.
with finer grained model the result is not necessarily better.
overall the results confirm that the trace similarity based method is ef fective for adversarial sample detection under carefully selected abstractionconfigurations withmorethan89 predictionaccuracy.table coverage and unique adversarial samples detected.
criteria mnist lstm mnist gru crash seed s guid.
t guid.
ran.
deeptest seed s guid.
t guid.
ran.
deeptest bscov .
.
.
.
.
.
.
.
.
.
wscov .
.
.
.
.
.
.
.
.
.
wscov .
.
.
.
.
.
.
.
.
.
sbcov .
.
.
.
.
.
.
.
.
sbcov .
.
.
.
.
.
.
.
.
btcov .
.
.
.
.
.
.
.
.
.
wtcov .
.
.
.
.
.
.
.
.
.
wtcov .
.
.
.
.
.
.
.
.
.
unique cra.
the last row presents the number of unique crashes discovered in each experiment.
answertorq3 similaritymetricbasedmethodisusefulfor adversarialsampledetection.thedetectionaccuracyvaries underdifferent metricsand model configurations.
.
rq4.
coverage guided testing setup.we use the prepared original benign samples as the initialseeds whicharecorrectlypredictedbybothmnist ltsm and mnist gru.
based on the results of rq2 we use the finedgrained configuration for constructing the dtmc models andselectbscovandbtcovasthetestingguidance.finally we implementtwotestingstrategies i.e.
s guid.andt guid..tofurther demonstratetheusefulnessofthecoverageguidance weinclude random testing without coverage guidance and deeptest a neuron coverage guided testing tool for unrolled rnns as baseline approaches for comparison.
each testing configuration was run for hours upon which the studied coverage criteria tend to saturate.
to counter the randomness of testing tool each configuration is repeated times and averaged results are reported.
results.
table6summarizes the obtained coverage results for differentcoveragecriteriaandtheuniqueadversarialsamplesdetected withthetestingtools.thefirstcolumnliststhestudiedcoverage criteria.column seedrepresentsthecoverageoftheinitialseeds.
columns s guid.
t guid.
randomanddeeptestarethecoverage bydifferenttestingstrategies.wecanobservethatallofthestudied strategies improve the coverage to some extent.
transition coverage guidedstrategyoutperformstheothertwostrategiesin achieving higher coverage under all criteria.
furthermore state coverage guided strategy is often more effective in generatingadversarial samples although it does not obtain the highest coverage.
the overall results indicate that covering more new states could be potentially helpful in generating adversarial samples.
answer to rq4 the coverage guided testing is generally useful in terms of achieving higher coverage and guiding adversarialsampleexploration.amongthethreestrategies transition coverage guided method achieves higher coverage while state coverage guided method uncovers more unique adversarialsamples.
.
threatsto validity we summarize factors that could affect the validity of our study.
a major threat is related to the abstract model configuration settings.
485esec fse august tallinn estonia xiaoning du xiaofei xie yi li lei ma yang liu and jianjun zhao there could be many possible configurations in the abstract model.
duetothecomputationresourceconstraint wetriedourbestto experiment with as many settings as possible.
even though our results may still not generalize beyond the considered settings.
thesubjectmodelselection could beanotherthreattogeneralizability.
we mitigate this by choosing models with diverse complexitiesanddifferentapplicationdomains coveringbothsimple cases and industrial grade asr applications.
in addition for the asr model the training sets for the adversarial detection classifierisrelativelysmall whichmightaffectthetheperformanceof the detection.
adopting a larger training set may help obtain even better detection performance which we leave as future work.
furthermore randomnessisathreatinbothsampling testingacross ourstudiedresearchquestions.tocounteractthis werepeatthe same setting for all experiments five times and average the results.
related work inthissection wecompareourworkwithotherabstractiontechniques testingand adversarialsample detectionfor dl systems.
abstractionofrnn.
severalapproacheshavebeenproposedto modelrnn butmostlyintheformoffinitestateautomaton fsa .
fsahelpstomanifesttheinternalstatetransitionsexplicitlyand thus can be used to interpret the underlying decision rules embedded in an rnn.
dtmc is superior with the ability to capture state transition distributions making it more suitable for quantitative analysis.constructinganfsafromanrnnusuallyrequirestwo steps hiddenstatespacepartitionandabstraction and transition abstraction and automaton construction.
various partitioning strategiesandautomatonconstructionalgorithmshavebeenproposed.
omlin and giles proposed to split each dimension of the state vector into equal intervals so as to divide the state space intoregulargrids.unsupervisedclassificationalgorithmswerealso applied for state space partitions.
for example k means and its variants were studied in .
weiss et al.
devised an algorithmtodynamicallycreatepartitions whereansvmclassifier withanrbfkernelisfittedtoseparateseveralstatevectorsfrom itsoriginalpartitions.recentstudies havefocusedmore on the interpretability and visulizability of rnn behaviors and try to simplify the abstract model by reducing the size of the models.whenappliedtoreal world tasks includingnlpand speech recognition the state space of the trained rnn models could betremendously large.
this makes scalability an issue for partition techniquessuchas k meansandkernelalgorithms.however we adoptedacheaperintervalabstractionandcouldbenefitfromits flexibility in precision adjustment.
testing of dnn.
the lack of robustness places a major threat to the commercialization and wide adoption of dl systems.
re searchers have devoted a great amount of efforts to investigate effective and systematic approaches to test dl systems led with a pioneering work of pei et al.
.
the authors designed the first testingcriterion neuroncoverage tomeasurehowmuchinternal logic of dnns has been examined by a given set of test data.
severalnewcriteriahavebeenproposedsincethen includingaset ofmulti granularitytestingcriteriaproposedindeepgauge asetofadaptedmc dctestcriteria andcombinatorialtestingcriteria .sofar theproposedcoveragecriteriaareusedtoguidethemetamorphicmutation basedtesting concolictesting and coverage guided testing of dnn .
in addition mutation testing technique is also proposed to evaluate the test data quality through injecting faults into dl models .
in a black box differential testing framework is proposed for detecting the disagreements between multiple models.
mc dccriteriaarelimitedinscalability andothercriteriaare specifictothefnnarchitecture eventhoughapplicabletornnvia unrolling.theresultsreportedin demonstratedthattheneuron coverageworkseffectivelyonfnnbutfarfromidealonrnnwhen used to guide test generation.
this indicates that rnn is beyond a simple folding of cnn and existing criteria may not be well suited for it.
due to the page limit we refer the interested readers to a comprehensive survey on machine learning testing .
adversarial sample detection.
sometechniques areproposedtodetectadversarialsamplesthatarepredicted incorrectly.theauthorsfoundthatadversarialsamplesaremuch more sensitive in model mutants and proposed a sensitivity based approachtodetectadversarialsamplesforfeed forwardmodels .
thetechniquein augmentsthednnsbyaddingasmallsubnetwork which obtains inputs from the intermediate feature representations of the dnn and is trained to detect adversarial samples.
the authors proposed two features i.e.
density estimates and bayesian uncertainty estimates to show the differences between benignandadversarialsamples.xuetal.
adopttwotypesof featuresqueezing i.e.
reducingthecolorbitdepthofeachpixeland spatial smoothing to detect adversarial samples.
these approaches mainlyconsider cnnsand the image classificationdomain.
in the authors detect the adversarial samples based on that theadversarialsampleshavearelativelysmallersoftmaxprobability.
followingthisline thetechniqueproposedin observesthatthe softmaxprobabilitiesbetweenin andout of distributionsamples can be further enlarged by temperature scaling in the softmaxfunction.
such methods can be used on rnns but are limited to the classification problem.
compared with them our approach can handle sequential outputs of rnns e.g.
the outputs in automated speechre cognition basedontheabstractmodel.
conclusion vulnerabilities of dl systems are threatening the trust and mass adoption of these technologies.
this work initiates the first step towardsthequantitativeanalysisofstatefuldlsystems.wemodel an rnn as an abstract model based on which a set of similarity metricsandcoveragecriteriaareproposed.wedemonstratedthe usefulnessoftheproposedmodelsandquantitativemeasureson rnns testing and adversarial sample detection.
our long term goal is to provide quality assurance for the dl system life cycle .