automated reporting of anti patterns and decay in continuous integration carmine vassallo department of informatics university of zurich zurich switzerland vassallo ifi.uzh.chsebastian proksch department of informatics university of zurich zurich switzerland proksch ifi.uzh.chharald c. gall department of informatics university of zurich zurich switzerland gall ifi.uzh.chmassimiliano di penta department of engineering university of sannio benevento italy dipenta unisannio.it abstract continuous integration ci is a widely used software engineering practice.
the software is continuously built so that changes can be easily integrated and issues such as unmet quality goals or style inconsistencies get detected early.unfortunately it is not only hard to introduce ci into anexisting project but it is also challenging to live up to theci principles when facing tough deadlines or business decisions.previous work has identified common anti patterns that reducethe promised benefits of ci.
typically these anti patterns slowlycreep into a project over time before they are identified.
we arguethat automated detection can help with early identification andprevent such a process decay.
in this work we further analyzethis assumption and survey developers about ci anti patterns.from the results we build ci o dor a reporting tool for ci processes that detects the existence of four relevant anti patternsby analyzing regular build logs and repository information.
in astudy on the build logs of popular j ava projects we reveal the presence of high severity warnings spread acrossprojects.
we validate our reports in a survey among originaldevelopers of these projects and through general feedback from developers that confirm the relevance of our reports.
index t erms continuous integration anti pattern detection ci smell ci decay i. i ntroduction continuous integration ci is a common development practice and its great benefits on quality and productivity are widely accepted .
ci advocates full automation of all build steps i.e.
compilation testing and code quality assessment to create a new version of the software .
the ci process is most effective when developers follow best practices such as commit often that reduce conflicts in the team and ensure that the build is continuously executable .
in practice it is often challenging to live up to these standards and anti patterns can be observed common but ineffective solutions to a recurring problem that should be avoided.
for example failing tests are removed instead of fixing the root cause.
over the years researchers have defined catalogs of ci anti patterns which eventually become a threatening maintainability problem for a software project if not properly addressed .
a creeping decay of quality has been described before in other contexts.
fowler popularized the term code smell to describe a symptom that indicates the existence of a deeper problem in the source code.
while the perception of smell is subjective previous work could show that code smell intensitycorrelates with the likelihood of the existence of a deeperissue .
the smell metaphor was later adopted in other areas such as system design configuration files or spreadsheets .
hence the idea that a ci anti pattern manifests itself as a ci smell as well.
in contrast to anti patterns in development artifacts ci anti patterns affect the software development process and provoke ci decay.
in this paper we further study this phenomenon.
our results of a broad survey among professional developers confirm that ci decay is indeed a relevant problem.
most participants confirm that deviations from ci best practices happen inpractice both intentionally and unintentionally and that the benefits of ci diminish when many deviations exist in a project.
the awareness about the presence of anti patterns in the ci pipeline is key for an educated decision about whether adeviation needs to be fixed.
inspired by duvall s catalog of continuous integration and delivery cd anti patterns we built ci o dor an automated detection and reporting tool that provides awareness about ci decay caused by four different anti patterns.
through the analysis of build log and repository information our tool identifies slow builds and especially increasing trends of build time broken release branch andthe corresponding time to fix skipped failing tests and late merging of development branches.
by analyzing a total of recent builds logs of popular javaprojects we identified high severity anti pattern instances and with medium severity spread across all projects.
to evaluate our tool we have surveyed originaldevelopers about the relevance of reports containing recentinstances of detected smells generated for their project and developers about the general usefulness of ci o dor.
the reports are perceived as useful relevant and most participants would integrate ci o dor in their ci pipeline to increase their awareness about the ci process.
we also find untapped potential for future detectors and that more work on ci anti patterns is necessary to improve the handling of project specifics.
in summary this paper presents the following contributions verification of the relevance of ci decay in practice detectors of four relevant ci anti patterns ci o dor an automated ci anti patterns reporting tool anempirical study on the presence of ci decay and on the developers awareness about ci anti patterns.
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ii.
m ethodology overview in this paper we introduce ci o dor an automated reporting tool that can be integrated into ci pipelines to increase the awareness about anti patterns in ci.
figure illustrates our methodology to create and evaluate the tool.
this work is based on the existing anti patterns catalog of duvall which describes patterns and anti patterns that influence the effectiveness of a ci cd pipeline.
in an internal selection we identified a subset of ci anti patterns from this catalog that can be automatically detected by analyzing build log and versioning information .
for each of these antipatterns we added an explanation and an illustration of the detection strategy and validated their selection in a survey among professional software developers .
we asked the participants about the relevance of the anti patterns in practice and the suitability of our detection strategies.
based on the results of the survey we eliminated several candidates refined our detection strategies and ended up implementing a set of four detectors .
we integrated these detectors in a reporting tool ci o dor that aggregates the different analysis results and that presents statistics such as a trend analyses to increase the awareness about the different anti patterns.
we evaluated the usefulness and relevance of the reports created by ci o dor in a second survey .
for the survey we conducted a case study in which we analyzed the build logs of projects .
the resulting reports are publicly available and we asked the original developers of these projects to rate them .
in addition we selected reports that illustrate the full capabilities of our reporting e.g.
there is at least one detected instance of each anti pattern .
we ask both the original developers and other developers with experience in ci to rate these example reports.
iii.
w hich anti patterns to detect and how?
the existing anti pattern catalog of duvall is extensive and contains several examples that go beyond the scope of automated tools e.g.
decisions regarding the deployment strategies.
we started with selecting a subset of anti patterns for which we could develop appropriate detectors.
in this section we first introduce our pre selected list of candidates and the survey that we used to validate and finalize our selection.
a. pre selection the rationale of our pre selection was two fold.
we wanted to cover different aspects of the ci pipeline such as version control or build failure management and exclude others that are more related to cd.
at the same time we selected anti patterns that can be detected using data that is typically produced by every ci pipeline independently from custom settings i.e.
build logs and repository.
the following list introduces all antipattern candidates proposes a detection strategy and justifies their relevance for the ci process quality.
for traceability we include the name of duvall s positive example .
late merging merge daily agile teams often develop in features branches.
integration effort and conflict potential increase if completed features are not integrated timely.
wepropose to warn about cases in which the last commit of a branch is older than a predefined threshold.
aged branches short lived branches infrequently synced feature branches substantially diverge over time and end up being very hard to integrate.
we propose to warn when an open branch has not been merged into master for a release.
broken release branch stop the line a broken build that is not fixed timely prevents the ci pipeline from properly assessing the effect of new changes.
we propose to warn when a build stays broken for longer than usual.
bloated repository repository artifacts that can be created in a build or fetched through provisioning mechanisms should not be committed to the version control system.
we propose to warn when binaries can be found in the repository.
scheduled builds continuous integration a scheduled build either unnecessarily builds a change a second time or is a sign that a change is not automatically built which breaks the idea of always ensuring a working system.
we propose to warn about build configurations that schedule builds.
absent feedback continuous feedback developers are missing out on required feedback when they are not automatically notified about relevant build events especially build failures.
we propose to warn about configuration files that do not enable any notification channels.
email only notifications visible dashboards according to duvall email is inappropriate as a single notification channel because developers might not have access or notifications get lost among other messages.
we propose to warn about configurations files that only notify by email.
skip failed tests automate tests skipping a failed test can fix a broken build but addresses a symptom rather than fixing the cause.
it threatens the safety provided by the test suite.
we propose to warn about cases in which a previously failed test does no longer occur in the next fixed build.
slow build fast builds a slow build caused by a coding issue or by a high workload of build server produces waiting times for developers and adds overhead to the ci process.
we propose to warn about significant build slow downs.
please note that these descriptions are shortened introductions from our survey.
a complete version of the first survey is available on our artifact page .
b. survey on the practical relevance we conducted a survey to validate the relevance of the selected anti patterns and the proposed detection rules.
survey design.
the survey contains three sections.
the first section is about the perceived severity of the problem of deviations from ci best practices.
the second section has a focus on the anti patterns.
we introduced each one with an elaborated description that includes explanatory images and asked participants to evaluate the anti pattern relevance and our proposed detection strategy.
finally we asked for a general validation of the idea of anti patterns detection.
all survey questions were optional and had likert scales with either five strong disagree tostrong agree or four levels none tohigh .
the survey also contained open questions for authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
anti pattern detectorslate merging slow build broken master skipped failed testci odor existing catalogue of anti patterns q1 ... q2 ... validation2pre selection build logsproject reportsoriginal developers 7anti pattern candidatesrefinement example reports other developers9 survey study survey study 136source code repository fig.
overview over the different parts of this paper feedback in all sections.
figure includes an excerpt of our survey a complete export is available on our artifact page .
advertisement.
we advertised the survey on social media i.e.
twitter reddit sub forums dedicated to devopsand continuous integra tion in ci related newsletters and by sending it to personal contacts.
we promised to raffle off two vouchers to reward participation.
demographics.
in the end developers opened our questionnaire out of which finished all questions leaving us with a completion rate of .
.
we included three control questions in the survey that asked about the proficiency in programming ci theory and ci practice.
we excluded responses from participants that reported less than moderate experience in any of these questions.
after the filtering we ended up with qualified participants.
most of our participants .
report that they got in contact with ci in an industry position.
of our participants hold an academic degree related to computer science .
bachelor .
master ph.d. .
the large majority reports a high level of experience in programming .
ci theory .
and ci practice .
.
data analysis methodology.
to analyze the likert scale answers we create asymmetric stacked bar charts with proportions for various agreement levels that are shown in figure .
we performed card sorting to analyze the open answers .
we started by splitting the answers into individual statements grouped common arguments and finally organized these arguments hierarchically.
problem statement ci best practices are no strict rules they can be adapted for a project.
one can deviate from ci best practices unintentionally.
the benefit of using ci diminishes the more best practice deviations exist.
relevance and sufficiency of smell detection for all anti patterns anti pattern x is relevant in a typical ci pipeline.
the detection strategy is sufficient to identify occurrences of an anti pattern.
general validation of idea i would integrate such a tool in my ci pipeline.
fig.
questions of first survey shortened problem statement.
the survey results show that deviations from ci best practices happen in practice.
most participants state that a project can intentionally deviate from ci bestpractices .
and even more participants agree that a deviation might be unintentional .
.
the majority of participants .
agree that the ci benefits diminish when many best practice deviations exist.
these answers confirm our conjecture that ci decay is a relevant problem in practice.
relevance detection.
our survey contains questions about the practical relevance of each anti pattern and we received a very high level of agreement.
six detectors have an agreement of and other two have an agreement of .
the only notable exception is email only notifications for which .
participants disagree with its relevance.
these results confirm that we had pre selected relevant anti patterns.
we asked our participants to rate the sufficiency of our detection strategy for all anti patterns.
also here the agreement is very high with the notable exception being late merging .
for which many participants point out specific ways to use version control that would have not been detected.
the high agreement makes us confident that we have successfully identified the common case for our detection.
however several people made use of the open question for each anti pattern to provide feedback on the detection strategies such as pointing out alternate development processes that we did not cover so far.
revised detection strategies.
on average participants answered the open question about each anti pattern and we carefully analyzed these answers to revise or exclude some detectors.
next we discuss the results of our open card sorting and how we revised our detection strategies based on the suggestions from the survey.
late merging aged branches many participants point out similarities between both anti patterns.
also the suggestions for improving the detection strategies overlap in our answers.
as a result we decided to merge both anti patterns.
the feedback contains valuable suggestions to improve our simplistic detection strategies a git based detector must support both merge andrebase commands a feature authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
likert scale answers to first survey branch should not be considered aged when no changes in other branches occur the age of a branch is irrelevant as long as it is frequently synced with the master branches marked with release names e.g.
rel .
.
should not be reported projects can release multiple times per day so open branches frequently span several releases.
overall we decided to revise our strategy to incorporate these suggestions and keep the late merging anti pattern.
broken release branch the survey participants broadly agree to the detection of the anti pattern .
.
several participants point out that a broken release branch should neverhappen so everybody would be aware of it even withoutnotification.
in addition we got some minor comments onour detection strategy e.g.
providing an overview for incidents over time avoiding to consider the average time between commits as the commit frequency varies a lot.
we decided to incorporate these suggestions into our detection strategy and to keep broken release branch.
bloated repository despite the high agreement among the participants .
we received several comments against the detection of this anti pattern offending files arelanguage and project specific but can be easily fixed with a well defined .gitignore file this anti pattern is not ci specific several participants mention good reasonsto include binaries e.g.
the availability reliability and convenience of provisioning sources.
we agreed with these concerns and eliminated the anti pattern candidate.
scheduled builds despite a high agreement with the proposed detection strategy .
many survey participants point outgood reasons for scheduled builds.
these include for example running extensive performance tests ui tests or frequently asserting the compatibility with a changing environment.
we could not distinguish between good and bad cases of scheduled builds so we decided to drop this candidate.
absent feedback email only notifications the agreement rate to the detection strategy of both anti patterns is high .
and .
respectively .
however many participants state doubts regarding the detection feasibility feedback might be delivered in ways that cannot be automaticallychecked e.g.
physical build lights it is impossibleto validate successful notification delivery the best notification channel is a personal preference.
we agreed with these concerns and decided to drop both candidates.
skip failed tests the detection strategy for this anti pattern has a very high agreement rate .
but several participants mention good reasons to remove a test e.g.
removal of functionality.
we believe that in these scenarios tests would either be removed together with production code or the build would fail due to a compilation error.
both scenarios would not trigger our detector.
other participants point out thatremoving commenting and skipping tests have the same effect so we should cover all of these cases.
apart from this we did not receive further suggestions for improvement.
we decided to keep this anti pattern.
slow build most participants agree with the detection strategy for this anti pattern .
.
the main concern mentioned by several participants is the threshold that is used to identifyslow builds.
at the same time previous work mention that a slow creep is the worst case scenario for build times .
we kept this anti pattern.
overall we received valuable feedback on all presented antipattern candidates.
following the suggestions of our participants we dropped schedule builds absent feedback emailonly notifications and bloated repository for the reasons mentioned above.
we revised the detection strategies of the remaining anti patterns late merging which is now merged with aged branches slow build broken release branch and skip failed test and kept them for the remainder of the paper.
general feedback.
the last part of the survey contains a general open question to provide feedback on the whole ciodor idea which was filled by all participants.
most of them mention ci o dor as useful for ci training and for learning to adopt ci best practices rigorously also whendevelopers are not familiar with ci yet .
furthermore of our participants believe that ci o dor can reduce maintenance effort and improve reliability on the ci pipeline.
as stated by some anti patterns might go unnoticedwithout such a tool which can be useful to monitor the ci health and take countermeasures when needed.
of theparticipants suggest to have highly configurable detectors tosupport team organization specifics in ci pipelines.
finally of the participants are quite skeptical about our detectors.
as it happens with many quality check tools their main concern is the likelihood of generating several false positives.
iv .
r eporting ci p ractices to implement a proof of concept of ci o dor our ci antipattern detector we first chose supported technologies.
we analyzed whether the perceived relevance of each smell varies across people working on different programming languages.
based on responses to our previous survey a kruskal wallis test did not indicate for any of the anti patterns a statistically significant difference among the four main programming languages the study participants reported as their main working authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
build logs repository ci related metrics ci report detectors1 summary summary summary n... fig.
ci reporting process in ci o dor language p value .
i.e.
java javascript python and ruby.
thus based on their popularity we decided to focus onjavaand maven as our target programming language and respective build tool.
to lower the likelihood of build log deletion we mined build data from travis ci and consequently repository data from github .
the overview of our reporting process is shown in fig.
.
given the travis cibuild logs of a particular project we first extract ci related metrics for every build .
we then run detectors on top of this raw data to derive additional metrics that either indicate the presence of a phenomenon e.g.
a test has been removed or a change in a metric e.g.
a change in build time .
from all metrics we provide a reporting utility that visualizes several dimensions of the ci process figure shows screenshots for the four different summaries that we include in our reports.
next we discuss the details of the detection strategies for the four anti patterns.
a. slow build figure 5a shows an example summary of slow build that contains the following items.
a bar chart highlighting the average build duration per week over the considered time window months in our example and in the study of section v .
a linear regression trend line along with a textual message highlighting whether the build time is increasing stable or decreasing over the observed period.
we also experimented with the use of kernel smoothing but the resulting trend did not drastically change so we favored simplicity here.
a list of possible warnings for the last builds of each branch.
specifically we report i a medium severity warning when a build was slower than of more builds on the master branch i.e.
it is in the fourth quartile ii a highseverity warning when the build duration is an outlier with respect to the distribution of master builds in the observed time window.
we used the box and whisker plot outlier definition i.e.
a build is an outlier when its duration is greater than 3q .
iqr where3qis the third quartile and iqr the inter quartile difference.
in some cases and this is especially true for the considered ci infrastructure i.e.
travis ci the build time might depend on many external factors including the priority given to the project in travis ciprojects with a free account get a low priority .
however we do not consider this a threat in ourmeasurements because even in these cases ci o dor would highlight the need for using a better infrastructure.
b. skip failed tests we first extract the executions of all junittests and their outcomes from each build log i.e.
the containing maven module the test suite name the number of executed tests the number of failed tests incl.
test errors and the number of skipped test cases.
we then derive a set of test related ci metrics by matching tests run in jobs with the same id belonging to consecutive builds on the same branch.
specifically we compute runs i.e.
a change in the number of executed tests breaks i.e.
a change in the number of failed tests and skipped i.e.
a change in the number of skipped tests.
to mark a test as skipped in the next build we evaluate whether the following expression is true breaks runs skipped fig.
5b shows an example summary that contains a bar chart depicting the number of builds per month affected by skip failed tests note that we adopted a granularity of one month for this smell due to its lower frequency than slow build for each build where such an incident occurred the list of test suites affected by the skip failed tests issue.
c. broken release branch to detect a broken release branch we compute the final status of each master branch build i.e.
its build status i n the build history of a project.
in particular a build is errored when the install phase which retrieves and installs the needed dependencies returns a non zero exit code.
instead it is failed when any subsequent phase returns a non zero exit code.
we determine the final status of each build and consider all the errored and failed builds as broken.
fig.
5c shows an example summary that reports the average time a release branch remains broken over the observed time period considering consecutive broken builds a bar chart showing for each week of the observed period the number of broken builds a linear regression line and a textual message highlighting the presence of an increasing or decreasing trend if any.
d. late merging we consider four different metrics about version control that help us to identify the late merging anti pattern missed activity branch deviation branch activity and branch age .
in the following we introduce the different metrics using the example history of fig.
which contains a master branch andf1with several merge commits.
missed activity tma .quantifies the amount of activity on other branches of the same repository since the current branch was last synced with the master tma tlo tsync where tlois the date of the last commit on other branches and tsyncis the date of the last merge commit.
if tmagrows the potential integration effort increases.
to allow for more specific warnings in the summary we break this metric further down into its two components branch deviation andunsynced activity .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a slow build b skip failed tests c broken release branch d late merging fig.
example summaries of the four anti pattern detectors branch deviation tbd .quantifies the amount of activity in other branches since the last change in the current branch tbd tlo tlc where tlois the last commit date on other branches and tlcis the last commit date on the current branch.
iftbdgrows other branches deviate from the current branch which again increases the potential integration effort.
negative values mean that the current branch is ahead of other branches.
unsynced activity tua .quantifies the amount of activity in the current branch since the last sync with the master tua tlc tsync.
a growing tuaindicates a deviation from themaster and that the potential integration effort increases.
total activity t ta .quantifies the total amount of activity on a branch since its creation tta tlc tfork where tforkis the date of the branch creation.
feature branches should be merged back into the master timely a growing ttaindicates a long running deviation from the master.
when ci o dor raises a warning.
for each of the four metrics mentioned above we compare their values with distributions in recent history and consider a medium severity warning if avalue is above the third quartile a high severity warning if it is an outlier using a similar approach to the one in section iv a .
history rewrite.
githistory can be rewritten which makes it harder to analyze .
we include a second branch f2in our example to illustrate our handling.
we detect rebasing in build logs by matching the meta data of a commit that is built id time committer message to meta data of previous builds on the same branch.
when all meta data but the id can be matched to a previous commit we mark this as a rebasing.in the example the rebasing of triggers a new build of at the date tsync tforkis the date at which the first build of this branch was triggered.
now all derived metrics can be calculated as for the previous merge case.
improved detection strategies.
we consider two suggested improvements for the detection strategy.
first in addition to analyzing the build logs we also analyze the current repositorysnapshot for every build to identify deleted branches that do not need to be reported anymore.
second we filter out branches that mark releases e.g.
rel .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
masterf2timeline for f2 timeline for f1sync lo fork missed activity branch deviation unsynced activity total activityf1 lc sync lo fork11lc fig.
example of different late merging scenarios v. e mpirical assessment of the ci o dor summaries we conducted a study on open source software projects to assess the accuracy and usefulness of our reporting.
we first performed a selection of candidate projects.
after detecting antipatterns on such projects we sent the generated summaries to the mailing lists forums of said projects and we asked original developers to fill out a survey.
in addition we also asked participants to the study of section iii to answer the part of the survey concerning the usefulness of the summaries and of ci o dor in general.
a. projects selection we used data from ght orrent version apr to identify suitable projects for our study.
we filtered projects according to the following criteria they are written in java have not been deleted have at least one commit in have at least two project members are no forks and have been forked at least once.
this initial filtering left us with a set of project candidates.
to find projects in this set that perform ci and that are compatible with ci o dor we required the existence of configuration files for maven pom.xml and travis ci .travis.yml which further reduced our candidates to projects.
we then excluded projects with less than five project members to ensure a certain community size.
we then extracted build logs from travis cifor the remaining project candidates.
for some projects we could not access the logs or only found a very limited number.
as ci o dor is based on historical analyses and to ensure a minimum level of activity we excluded the first quartile from the distribution of available build logs for these projects which left us with candidates that have had at least builds in .
as the final step we manually identified the main communication channels for all remaining projects because we need to contact the corresponding developers.
keeping only these projects for which we found a public mailing list or could join a closed group channel such as google groups orslack we ended up with a final selection of projects for the validation.
these projects cover various domains like businessoriented software image processing development tools and have a diverse sizes from thousand to million loc ages from to thousand commits activity levels fromtable i detected ci anti patterns over the analyzed projects slowbuild proj.
with incr.
trend proj.
with decr.
trend proj.
with stable trend overall of medium sev.
warnings overall of high sev.
warnings min 1q median 3q max of medium sev.
warn.
.
.
.
.
of high sev.
warn.
.
.
.
.
broken release branch affected projects total of incidents proj.
with incr.
trend proj.
with decr.
trend min 1q median 3q max of incidents .
.
.
.
.
fixing time .
m .
h .
h .
d .
w skipfailed tests affected projects overall of detected incidents min 1q median 3q max of affected builds .
.
.
.
.
latemerging affected projects of medium severity warnings of high severity warnings min 1q median 3q max of affected branches .
of affected branches .
.
.
.
.
to thousand builds team sizes from to members with a median number of .
and popularity from to thousand githubstars .
the full list of these projects on the artifact page of this paper .
b. quantification of the phenomenon this section provides a short overview of the anti pattern instances and ci decay for the projects for which we asked for feedback with the goal of highlighting the magnitude of the investigated phenomenon.
the analysis concerns a total of builds from january to august .
this results in detected incidents if we consider only high severity warnings for slow build andlate merging .
concerning slow build projects exhibit an increasing trend in build time whereas only had a decrease and were stable.
the percentage of cases in which a medium severity warning could be generated is fairly high with a median of of the builds and a maximum authorjapps zerocode where nearly all builds are slower than the third quartile of the previous time window.
this indicates a slow increase in the build time which can be normal project evolution.
a single incident might not be worrisome per se so we visualize the overall trend see fig.
5a .
high severity warnings i.e.
outliers are not particularly frequent of the projects have less than of their builds exhibiting this warning indicating that while the slow build phenomenon is quite pervasive in most cases it manifests quite slowly over time.
even though working on master is discouraged and a pull request paradigm has been advocated we find broken release branch in all projects a median of .
of the master builds are broken.
at the same time our data indicates that breaks are typically fixed within one day i.e.
the median authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
concrete questions about a report for original developers the report is useful for my project and contains relevant warnings.
i learned something about my project that i have not been aware of before.
the results made me curious and i plan to investigate the different warnings.
general questions about usefulness of examples slow build failed test skipping late merging this summary helps me to identify anti pattern x. i know how to address the different warnings about anti pattern x. high severity warnings about anti pattern x should fail the build.
broken release branch this summary improves awareness about... ... the frequency of release branch failures.
... the time it takes to fix release branch failures.
i know how to improve the trend of this summary in the future.
general validation of tool and idea the ci report provides information that is not available in any other tool.
the reports provide a good overview of the ci practices used in a project.
frequent reports would have a positive influence on ci practices.
i would like to integrate such a reporting in my own ci pipeline.
fig.
questions of second survey shortened is about 17h even though the median fix time is above days for the upper quartile of projects.
we found one project rackerlabs blueflood for which the master branch remained broken on average for over weeks.
skip failed test is the least prominent problem in the analyzed project histories.
we found instances of this smell for out of the projects in a total of builds.
the percentage of builds affected by this anti pattern is below .
.
while instances of the anti pattern can be found developers seem to take failed tests seriously and do not skip them.
concerning late merging the anti pattern affected nearly all projects out of and we raised a total of high severity warnings and medium severity warnings.
the median number of branches affected by a warning is and only in one project evolveum midpoint the problem affected branches although the median percentage of affected branches is quite substantial .
.
c. survey on generated reports we have conducted a second survey study to validate the usefulness of our generated reports.
survey design.
to perform the study we designed a questionnaire composed of a demographics section plus three sections each one comprising likert scale questions and a field for open comments.
in the first section we asked original developers about the report that we have generated for their project.
this part was automatically skipped for developers that have not seen a report.
in the second section we introduced the four different detector categories through an exemplary screenshot.
we then ask questions about the understandability of the summary its actionability and whether detected deviationsshould fail the build if applicable .
a final section of thesurvey contained general questions about the usefulness of the presented summaries.
an excerpt shortened questions no demographics of the survey questionnaire the complete one is on our artifact page is depicted in fig.
.fig.
likert scale answers to second survey advertisement.
we had two separate advertisement strategies to find study participants.
to find original developers w e have created up to date summaries for our selected target projects and posted them on their corresponding communication channels asking project members for feedback.
at the same time to receive enough general feedback about our summaries we advertised the survey on twitter reddit targeting the same sub forums of the previous survey and we also sent a follow up email to every participant of our first survey that allowed us to contact her again.
demographics.
in the end developers opened our survey out of which answered all questions original and general developers resulting in a completion rate of .
.
we cannot calculate the return rate for the general population but we know that we sent the reports out to projects and we heard back from projects return rate of .
we included the control questions from our first survey again and excluded from the analysis participants that indicated low experience.
to increase the number of answers we also kept partial answers.
in total we considered answers as valid out of which were given by original developers.
data analysis methodology.
as for the likert scale answers in the first survey see section iii we present the resultsin asymmetric stacked bar charts .
we have received considerably less open answers so we did not open code all answers but we will rather discuss the main points raised.
report rating.
when we asked original developers about the usefulness of the reports for their projects almost two thirds .
of them agree that the report was useful and contained relevant information.
we analyzed the open answers of the .
that disagree and found that most either complain aboutproject specifics that are not considered in the reports or about a bug that we had in the beginning.
we got mixed answersabout the novelty of information the same amount .
of agreements and disagreements.
this could be a sign that experienced developers are aware of these deviations in their project but we did not have a question in the survey to back up this conjecture.
we got a similar result when we asked about their reaction.
.
of original developers agree that the reportmade them curious and that they plan an investigation.
overall authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we see these results as a sign that the reports are insightful for developers and introduce a minimal overhead.
identification and awareness.
across all anti pattern detectors the large majority slow build .
skip failed tests .
late merging broken release branch .
and .
of participants agree that the summaries are useful for antipatterns identification and for increased awareness respectively.
this makes us confident that we opted for the right statistics and picked good visualization strategies.
actionability.
the majority of participants agree that they know how to address the warnings for skip failed tests .
and late merging .
which is unsurprising because the report points to concrete problems to fix.
however a considerable number of participants disagree for broken release branch .
and especially for slow build .
.
given the high agreement on identification and awareness we think that the disagreement on actionability is a sign that the report unveils the problem but that deriving a fix is harder because it affects the process and team practices.
on using ci o dor to fail builds.
the majority of participants disagree with the idea that a build should fail when it is slow .
or when signs of a late merging exist .
and only a small group agrees with this idea and .
.
although build failures provide feedback about issues in newly committed code such as bugs or poor quality our participants typically do not want detected anti patterns in the ci process to break the build.
the only exception is represented byskip failed tests where .
are in favor of failing the build.
from the open answers in the first survey we know that some developers see this anti pattern as a serious problem.
general validation.
the last part of the survey contained several statements about the validity of the reports and the general idea.
.
of the participants agree that the summaries are useful and that they contain relevant information for the project.
.
expect a positive effect from integrating our tool to their ci discipline and .
are willing to integrate ci o dor in their pipeline.
vi.
d iscussion this paper has introduced the idea that monitoring the ci process might be useful to discover the decay of best practices over time.
building a proof of concept implementation ciodor and surveying developers about the idea and our tool we gained several valuable insights into the perceived or expected benefits of such an approach and actionable findings that have an impact on future work.
positive effect awareness.
the first survey has shown us that anti patterns are a relevant problem for ci.
best practices are not always being followed and can even be accidentally broken.
almost two thirds of the participants to the second survey expect that using such a reporting frequently would have a positive influence on their ci discipline.
transparency.
the study participants suggested that the tool should make the detection strategy fully transparent to increase the trust and acceptance among its users.
we onlybriefly described the detection rules in the summary pages to allow study participants performing their task efficiently but a production ready tool could involve a fully fledged description of the detectors.
learnability.
participants of our first survey confirmed the usefulness of the proposed ci monitoring especially in the early stages of ci adoption or to train project newcomers.
nearly half of the participants of our second survey were already aware of most of the highlight problems but it is important to remark that our analysis excluded inexperienced developers about ci.
the potential of a regular ci report can be seen by the .
of participants that got curious from the report and started to investigate the reported issues.
configurability.
our first survey indicated that since projects are very different developers may want the reports and the detection thresholds to be customized based on their needs.
while half of our participants are willing to integrate an anti pattern detector in their pipeline this percentage could increase by enhancing usability and reconfigurability e.g.
giving the freedom to enable disable specific detectors or configure thresholds.
ultimately the important question is how useful a concrete report is therefore we have asked original developers to rate the report that we have generated for their project.
the low disagreement rate in this question and the high agreement for the integration in their own pipeline make us confident that the described reporting is a promising tool for software development teams that follow ci principles.
a. threats to validity threats to construct validity are related to the relationship between theory and observations.
they mainly concern implementation bugs.
our implementation might contain bugs that cause the tool to report false positives or false negatives.
while the absence of a labeled dataset made it hard to evaluate the detection strategies we mitigated this threat through a manual review of a sample of the generated results and through a pilot study conducted before the second survey.
build cleanup.
travis ciallows projects to delete their old build logs e.g.
as part of regular maintenance activities and this could affect the historical analyses our tool performs.
however previous work has shown that such deletions are unlikely so we do not expect that our results are significantly affected.
threats to internal validity are related to confounding factors internal to our study.
in particular the validation of our summaries could be affected by our selection of projects.
we have mitigated this by selecting a diverse set of projects from githuband ensured a certain level of maturity by considering the popularity of a project.
unfortunately our data source ght orrent only approximates the number of committers in a project which might result in a less diverse project sample.
threats to external validity concern the work s generalizability and are related to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
sample size and diversity.
our evaluation would surely benefit from the analysis of a larger sample of projects as well as of the participation of more developers.
in this work the analysis of projects was limited to the ones for which we ask for feedback and the technological limitations java maven do not affect the relevance of the detected anti patterns as explained in section iv.
irrelevant selection of anti patterns.
it is possible that our work missed anti patterns that are highly relevant for developers.
we have reduced this threat through a first internal pre selection of supported anti patterns which we validated in our first survey.
also it is important to remark that it is not our goal to provide a comprehensive detector for all anti patterns proposed by duvall but rather to propose and validate the idea and its implementation in ci o dor of reporting ci decay to developers.
b. future work add additional detectors.
while as stated above we validated the general ci o dor perspective and four relevant anti patterns our future work primarily goes into providing additional detectors for new anti patterns.
consider more contextual information.
right now we only leverage information from the travis cilogs and basic information from the code repositories.
however future work would integrate additional process related metrics derived from other sources like bug trackers task management systems or communication platforms.
support more project specific policies.
a ci supporting tool with smart capabilities could learn the problems warnings in which developers are interested in and personalize the recommendation consequently.
derive project specific thresholds.
while we considered thresholds based on consolidated statistics future work could also consider adaptive project specific threshold learning and calibration.
vii.
r elated work researchers have investigated the ci adoption finding in particular numerous barriers for ci adoption e.g.
related to assurance security and flexibility in performing tasks such as source code debugging.
in such a context approaches like ci o dor can be used to help developers understanding when they are not using ci properly.
previous work has investigated best practices while using ci .
in their landmark work duvall et al.
identified principles and key practices of ci but also pointed out the risks deriving from the misuse of ci.
furthermore humble and farley performed a broader study analyzing the key ingredients of a continuous delivery cd pipeline as well as anti patterns to be avoided.
such anti patterns were better explained in the follow up work by duvall where all the practices contained in books about ci and cd were condensed in a catalog of bad good practices regarding the adoption of the whole cd pipeline with specific focus on the core part of cd i.e.
ci.
such a catalog is a comprehensiveset of patterns and anti patterns regarding several phases or relevant topics in the ci cd process.
as explained in section iii duvall s catalog constitutes the inception of our work.
one of the best practices associated with ci is the use of infrastructure as code iac in order to implement the desired pipeline.
sharma et al.
leveraged best practices associated with code quality management to assess configuration code quality and proposed a catalog of implementation and design configuration smells .
recent work by gallaba et al.
also investigated configuration smells and based on rules provided by linters e.g.
travis lint they measure smells and derive automated fixes for them.
our smells have a different focus because we look at process related smells rather than at configuration issues.
rahman and williams proposed a text mining approach to identify defective iac scripts focusing on security and privacy issues e.g.
related to file permissions or user accounts.
their work is complementary to ours as it deals with a very specific category of problems.
studying and proposing automated fixed for build failures has also been a topic of investigation.
previous work has investigated the phenomenon of build failures from different perspectives such as testing and code analysis .
also researchers have proposed fixes for some kinds of build failures e.g.
broken dependencies related or proposed approaches to augment the comprehensibility of build logs while inspecting the cause of such failures .
our ci smells detector increases the awareness of developers about problems degrading their current ci practice.
given that our smells are just symptoms of bad practices we do not provide any automated fix for such issues but we let developers decide whether taking action or not.
viii.
s ummary this paper investigates the phenomenon that ci development practices decay over time.
we survey developers from industry to understand the problem.
beyond agreeing on the problem relevance our respondents also confirm that ci anti patterns are a major cause for the degradation of ci processes.
to support developers in preventing ci pipeline from deteriorating we propose ci o dor an automated reporting tool of ci anti patterns.
we validate our approach ci o dor by surveying original developers about summaries for their projects and by asking another developers about the general usefulness.
the results show that ci o dor increases the awareness about ci anti patterns is perceived as useful and that developers would integrate it into their pipeline.
ix.
a cknowledgments we would like to thank all the study participants.
c. vassallo and h. c. gall acknowledge the support of the swiss national science foundation for their project surf mobileappsdata snf project no.
.
c. vassallo also acknowledges the student sponsoring support by choose the swiss group for software engineering.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.