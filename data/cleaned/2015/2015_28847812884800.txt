augmenting api documentation with insights from stack overflow christoph treude school of computer science university of adelaide adelaide sa australia christoph.treude adelaide.edu.aumartin p .
robillard school of computer science mcgill university montr al qc canada martin cs.mcgill.ca abstract software developers need access to di erent kinds of information which is often dispersed among di erent documentation sources such as api documentation or stack over ow.
we present an approach to automatically augment api documentation with insight sentences from stack over ow sentences that are related to a particular api type and that provide insight not contained in the api documentation of that type.
based on a development set of sentences we compare the performance of two state of the art summarization techniques as well as a pattern based approach for insight sentence extraction.
we then present sise a novel machine learning based approach that uses as features the sentences themselves their formatting their question their answer and their authors as well as part of speech tags and the similarity of a sentence to the corresponding api documentation.
with sise we were able to achieve a precision of .
and a coverage of .
on the development set.
in a comparative study with eight software developers we found that sise resulted in the highest number of sentences that were considered to add useful information not found in the api documentation.
these results indicate that taking into account the meta data available on stack over ow as well as part of speech tags can signi cantly improve unsupervised extraction approaches when applied to stack over ow data.
ccs concepts information systems !information extraction software and its engineering !documentation computing methodologies !supervised learning keywords api documentation stack over ow insight sentences .
introduction while much of the information needed by software developers is captured in some form of documentation it is often permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c acm.
isbn .
.
.
.
obvious where a particular piece of information is stored.
di erent documentation formats such as wikis or blogs contain di erent kinds of information written by di erent individuals and intended for di erent purposes .
for instance api documentation captures information about functionality and structure but lacks other types of information such as concepts or purpose .
some of the most severe obstacles faced by developers learning a new api are related to its documentation in particular because of scarce information about the api s design rationale usage scenarios and code examples .
on the other hand how to questions also referred to as how to do it questions are the most frequent question type on the popular question and answer q a site stack over ow and the answers to these questions have the potential to complement api documentation in terms of concepts purpose usage scenarios and code examples.
while a lot of research has focused on nding code examples for apis e.g.
less work has been conducted on improving or augmenting the natural language descriptions contained in api documentation.
to ll this gap we compare techniques for automatically extracting sentences from stack over ow that are related to a particular api type and that provide insight not contained in the api documentation.
we call these sentences insight sentences .
the idea is related to update summarization which attempts to summarize new documents assuming that the reader is already familiar with certain old documents.
update summarization is often applied to summarizing overlapping news stories.
applied to api documentation and content from stack over ow the idea is to create a summary of the discussions on stack over ow as they relate to a given api type assuming that the reader is already familiar with the type s api documentation.
our research is guided by two main questions rq1 to what extent are unsupervised and supervised approaches able to identify meaningful insight sentences?
rq2 do practitioners nd these sentences useful?
to answer the rst research question we applied two state of the art extractive summarization techniques lexrank and maximal marginal relevance mmr to a set of api types their documentation and related stack over ow threads.
these techniques assign a numeric value to each sentence and return the top ranked sentences as a summary.
we found these summarization techniques to perform poorly on our data mainly because sentences on stack over ow are often not ieee acm 38th ieee international conference on software engineering table motivating examples api type stack over ow sentence stack over ow id java.sql.drivermanager normally you use drivermanager when you just want a connection for one time while with datasource you get other features such as connection pooling and distributed transactions.
java.net.url urlencoder is meant for passing data as parameters not for encoding the url itself.
java.lang.thread join waits for something meaningful while sleep just sits there nothing.
table regular expressions for ltering stack over ow threads question part pattern body.
typename .
non quali ed api type surrounded by lower case words or punctuation marks title or body ?i .
bpackagename .typename b. fully quali ed api type case insensitive body.
code .
btypename b. code .
non quali ed api type in code body.
a. href.
packagename typename .html.
.
a .
link to the o cial api documentation title ?i .
b a an typename b. non quali ed api type pre xed with a or an case insensitive figure stack over ow example meaningful on their own without their surrounding code snippets or the question that prompted a given answer.
we also applied a pattern based approach that has been successfully used to detect and recommend fragments of api documentation potentially important to a programmer who has already decided to use a certain api element with similar results.
we then developed a novel machine learning approach called sise supervised insight sentence extractor which uses as features the sentences themselves their formatting their question their answer and their authors as well as part of speech tags and the similarity of a sentence to the corresponding api documentation.
with sise we were able to achieve a precision of .
and a coverage of .
on our development set.1in addition to the similarity of a sentence to its corresponding api documentation characteristics of the user asking the question the score and age of the answer question characteristics and the part of speech tags at the beginning of a sentence were among the features with the highest information gain.
to answer the second research question on whether practitioners nd these sentences useful we conducted a comparative study in which we asked eight software developers to rate sentences extracted with all four approaches lexrank mmr patterns and sise .
these sentences were related to java api types.
the study showed that sentences 1precision measures the fraction of sentences extracted by an approach that are meaningful while coverage measures the ratio of api types for which the approach extracts at least one sentence.extracted by sise were considered signi cantly more meaningful and resulted in the most sentences that added useful information not contained in the api documentation.
these results indicate that taking into account stack overow meta data as well as part of speech tags can signi cantly improve existing unsupervised approaches when applied to stack over ow data.
the main contributions of this work are the conceptualization of insight sentences as sentences from one documentation source that provide insight to other documentation sources and a comparative study of four di erent approaches for the extraction of such insight sentences as well as a list of factors that can be used to distinguish insight sentences from sentences that are not meaningful or do not add useful information to another documentation source.
.
motiv ating examples table shows for three java api types a sentence taken from stack over ow that contains useful information about this type that is not stated in the type s api documentation.
the goal of our work is to automatically extract such sentences and use them to augment api documentation.
the rst example is taken from our development set while the other two examples were automatically identi ed by sise.
figure shows the source of the sentence in the rst example.
in an answer to a question about datasource and drivermanager on j2se the stack over ow user rst cites a statement from the api documentation but then elaborates on it further than the api documentation does.
this is an example of the api documentation lacking information on purpose since it only states which alternative is preferred without explaining why.
in contrast the sentence added by the stack over ow user clearly distinguishes the roles of the api types discussed and explains which one should be used in which situation.
the second example shows a case where a stack over ow user clari es the relationship between types with similar names in an answer 393to http url address encoding in java .
in the third example a user again compares two alternatives and explains which one to use in a particular situation this time in an answer to thread sleep and thread join .
in the next sections we describe our investigation of the means to automatically identify sentences on stack overow that are meaningful and add useful information not contained in the api documentation.
.
linking documentation first we describe how we identify stack over ow threads related to a given api type as well as our development set used for comparing di erent extraction approaches.
.
linking our linking approach identi es stack over ow threads that are related to an api type in a two step process we perform a full text query for the non quali ed type name using the stack over ow api relying on its relevance ordering and we reject all results that do not match at least one of a number of regular expressions that are applied to the question that started the thread see table .
the advantage of using the stack over ow api over the stack over ow data dump used in previous research such as that of bacchelli et al.
is that sentences extracted by our linking approach always re ect the latest content available on stack over ow.
however the querying mechanism o ered by the stack over ow api is limited thus warranting the additional ltering in step .
we manually created a benchmark to measure the relevance of the threads that the lter identi es in step .
for randomly selected types of the java se api types with one word identi ers such as list and types with multi word identi ers such as arraylist we selected the rst ve threads that the stack over ow api returned for each non quali ed type and we manually annotated whether the thread actually mentioned the api type.
we evaluated our linking approach separately for one word types and multi word types.
for one word types precision was .
and recall was .
f1 measure .
and for multi word types precision was .
and recall was .
f1 measure .
.
.
annotated data we created a development set by manually annotating all sentences that belong to the top ten answers as indicated by their scores from the rst ten threads that our linking approach associated with ten java api types.
we did not remove false positives threads not mentioning a target type to ensure that the development set is a realistic representation of the sentences used as input to the extraction approaches.
to sample api types we identi ed the three most commonly used types from each of the ten most commonly used java packages as indicated by the apatite tool see table .
we chose this strati ed sampling strategy to ensure a wide coverage of commonly used types while not focusing on rarely used ones.
we then divided the data into the development set and the test set for the comparative study see section as follows the second most commonly used type from each package was used to construct the development set and the most commonly used type as well as the third most commonly used type was used for the comparative study.table three most commonly used types in the ten most commonly used java packages.
types in bold were used for sampling sentences in the development set and the remaining types were used to sample sentences for the comparative study.
package type java.applet applet audioclip appletcontext java.awt event image component java.beans propertychangelistener propertychangeevent propertydescriptor java.io file serializable inputstream java.lang object string thread java.net url urlclassloader socket java.security accesscontroller secureclassloader principal java.sql connection drivermanager resultset java.util list arraylist map javax.swing jcomponent jpanel jframe table development set type meaningful not meaningf.
arraylist audioclip drivermanager image jpanel propertychangeevent secureclassloader serializable string urlclassloader sum the rst author annotated each of the sentences belonging to the second most commonly used type in each of the ten packages with a yes no rating to indicate whether it was meaningful on its own.
sentences were rated as meaningful.
table shows the number of sentences considered meaningful for each of the ten api types.
as an example the rst author annotated the following three sentences related to java s arraylist as being meaningful the list returned from aslist has xed size there is one common use case in which linkedlist outperforms arraylist that of a queue and it s worth noting that vector also implements the list interface and is almost identical to arraylist .
examples for sentences that are not meaningful and related to the same api type are see the next step if you need a mutable list they serve two di erent purposes and use the javadocs .
all of these sentences make sense in the context of an entire question and answer thread but they do not convey meaningful information on their own.
.
unsupervised approaches in this section after outlining our preprocessing steps we present the results of using state of the art text summarization and pattern based approaches for the extraction of insight sentences.
2the complete coding guide is available in our online appendix at 394table precision and coverage for di erent lexrank con gurations precision coverage only rst sentence .
.
rst two sentences .
.
rst three sentences .
.
rst four sentences .
.
rst ve sentences .
.
score at least .
.
.
score at least .
.
.
score at least .
.
.
score at least .
.
.
we had developed a set of techniques for preprocessing software documentation in previous work .
we summarize them here for completeness.
we remove markup from the html les and preprocess the resulting text les to account for the unique characteristics of software documentation not found in other texts such as the systematic use of incomplete sentences and the presence of code terms.
in particular we pre x sentences that start with a verb in present tense third person singular such as returns or computes with the word this to ensure correct parsing of partial sentences such as returns the next page number .
in addition we pre x sentences that start with a verb in present participle or gerund such as adding or removing immediately followed by a noun with the word for to ensure correct parsing of partial sentences such as displaying data from another source .3we further con gure the stanford nlp parser to automatically tag all code elements as nouns.
in addition to code elements tagged with ttorcode tags in the original source all words that match one of about hand crafted regular expressions are treated as code elements.4the resulting sentences are then parsed using the stanford nlp toolkit.
these preprocessing steps are identical for all the approaches described below.
.
lexrank lexrank is a text summarization technique that conceptualizes a text as a graph where each node represents a sentence and the weight of each edge corresponds to the cosine similarity of the sentences it connects.5the importance of a sentence is then given by the eigenvector centrality of the corresponding node .
we chose lexrank because it is the best known graph based method for summarization .
we re implemented lexrank in java and calculated the eigenvector centrality of each sentence in our development set separately for each api type.6for each api type we considered all related sentences in our development set as the text to be summarized.
the result is a numeric score for each sentence.
there is no clear rule as to how many sentences should be considered for a summary or what a 3using other prepositions such as by does not signi cantly change the results.
4the regular expressions are available in our online appendix at 5similarity a b ja bj p jaj j bj where aandb are the tokens of the respective sentence.
6we implemented our own version of lexrank to be able to modify it if needed.
however all results in this paper are based on an unmodi ed implementation following erkan and radev .table precision and coverage for di erent mmr con gurations precision coverage only rst sentence .
.
rst two sentences .
.
rst three sentences .
.
rst four sentences .
.
rst ve sentences .
.
score at least .
.
.
score at least .
.
.
score at least .
.
.
score at least .
.
.
good threshold for eigenvector centrality is.
thus we experimented with di erent settings and evaluated the performance of di erent con gurations on our development set.
table shows the results in terms of precision and coverage.
we de ne coverage as the ratio of api types for which there is at least one sentence.
we focus on coverage instead of recall because our goal is the extraction of useful insight sentences and not the identi cation of all possible insight sentences.
if we only consider the sentence with the highest eigenvector centrality to be the insight sentence for each api type the average precision across the ten api types in our development set is .
.
the precision drops if we consider more sentences as insight sentences.
we also explored the e ects of di erent eigenvector centrality thresholds.
as table shows the precision remains low and coverage drops as well.
.
update summarization the goal of update summarization is to produce a summary of a new document under the assumption that the reader is already familiar with the content of a given set of old documents.
applied to insight sentence extraction the idea is to create a summary of stack over ow threads related to an api type under the assumption that the reader is already familiar with the type s api documentation.
as previously done by boudin et al.
we adopted the concept of maximal marginal relevance mmr in our implementation of update summarization using the lexrank score as a baseline for calculating the mmr scores see previous section .
we subtracted from each sentence s lexrank score the maximum cosine similarity between that sentence and any sentence in the api type s documentation.
in other words if the similarity between a sentence and each sentence in the api documentation is the mmr score is identical to the one assigned by lexrank.
however sentences that are similar to at least one sentence in the api documentation receive a score lower than the one assigned by lexrank.
table shows precision and coverage for di erent congurations of our mmr implementation when applied to the development set.
the results are worse than those for lexrank.
.
knowledge patterns previous research has successfully used knowledge patterns to detect and recommend fragments of api documentation potentially important to a programmer using an api element.
in their work chhetri and robillard categorize text fragments in api documentation based on whether they contain information that is indispensable valuable or neither.
from the fragments that contain potentially im395table features used for sise source feature type sentence sentence string part of speech tags string sentence the number of tokens in the sentence numeric whether the sentence is a codeblock i.e.
surrounded by pretags boolean the position of the sentence in the answer numeric the position of the api element in the sentence or numeric whether the sentence starts lower case boolean the number of characters that are code as indicated by preorcode tags numeric whether sentence contains html tag code pre a strong em i b h1 h2 h3 sup strike boolean the percentage of tokens tagged code pre a strong em i b h1 h2 h3 sup strike numeric question whether the question title or body contain the api element boolean question attributes score favorites views answer count size age numeric question attributes whether it was edited whether it contains a code block boolean question user attributes reputation acceptrate numeric whether the question user is registered boolean answer answer attributes score time di erence to question size age numeric answer attributes whether it was accepted or edited whether it contains a code block boolean answer user attributes reputation acceptrate numeric whether the answer user is registered boolean relative rank of the answer among all answers to that question by score and age numeric similarity cosine similarity between sentence and most similar sentence in api documentation numeric average cosine similarity between sentence and all sentences in api documentation numeric portant knowledge they extract word patterns to automatically nd new fragments that contain similar knowledge in unseen documentation.
in a study involving independent human participants indispensable knowledge items recommended for api types were judged useful of the time and potentially useful an additional of the time.
each knowledge pattern consists of a small number of words such as specify should argument or not exceed number should .
to match a knowledge pattern a sentence must contain all of the words in the pattern but not necessarily in the order speci ed by the pattern.
instead of a word a pattern can contain a special wildcard for code elements which indicates that only sentences containing a code element can match the pattern.
an example is given by the pattern must not code element null .
we applied the patterns for indispensable and valuable knowledge items extracted from the reference documentation of the java se sdk to our development set and calculated precision and coverage.
to ensure that the matching of sentences to patterns is not sensitive to di erent grammatical forms we applied stemming to each word in the patterns and the sentences using porter s stemmer before calculating the matches.
similar to the attempts of using text summarization techniques for the extraction of insight sentences the application of knowledge patterns to our development set did not produce encouraging results we obtained a precision of .
and a coverage of .
.
this can be explained by the patterns reliance on the systematic writing style of reference documentation which is not used in informal documentation formats such as stack over ow.
.
sise a supervised approach considering the poor results achieved with traditional summarization approaches we developed sise supervised insight sentence extractor a novel approach speci callyfor extracting insight sentences from stack over ow.
a supervised approach e ciently supports considering a large amount of potential factors available for each sentence on the q a website such as the reputation of the user authoring a sentence or the score of the corresponding answer.
after the preprocessing steps described in section we used the features shown in table for each sentence as input for machine learning algorithms.
we designed the feature set to cover all meta data available on stack over ow as well as basic syntactic and grammatical features.
in addition we included two features for the similarity of a sentence to the corresponding api documentation following the idea of update summarization .
most of the features are either boolean ornumeric.
for the two string features we used weka s stringtowordvector lter to turn the corresponding text into separate features for single words bigrams and trigrams.
for example the simple sentence list is slower than arrays would result in one feature for each lemmatized word code element be etc.
four features for the bigrams code element be be slow etc.
and three features for the trigrams code element be slow be slow than etc.
.
for the part of speech tags feature set the same number of features would be produced based on the part of speech text that corresponds to the sentence which in the example is nn vbz jjr in nn a noun followed by a verb in third person singular a comparative adjective a preposition and another noun .
we tested di erent machine learning algorithms that are commonly used for text classi cation in software engineering e.g.
on our development set k nearest neighbour ibk decision trees j48 naive bayes random forest and support vector machines smo the sequential minimal optimization implementation in weka .
apart from j48 and random forest all classi ers belong to di erent classes ensuring a wide coverage of di erent pos396table features with highest information gain feature cosine similarity between sentence and most similar sentence in api documentation average cosine similarity between sentence and all sentences in api documentation question user acceptance rate answer score answer age answer time di erence to question question score question favorites question user reputation question views number of nouns followed by verb in present tense third person singular in sentence question age sentence starts with noun followed by verb in present tense third person singular number of tokens in sentence position of api element in sentence or number of occurrences of api element in sentence answer score answer size number of nouns in sentence sentence starts with noun number of characters that are code number of occurrences of the verb be in sentence sible algorithms.
we used weka s default setting for each classi er except for the k nearest neighbour classi er which we instantiated with values of one ve and ten for k.7because of the large number of features generated by our treatment of string features we calculated the information gain of each feature and used attribute selection to reduce the data set in order to improve classi er performance.
to calculate the precision and coverage of each classi er we applied what we call ten type cross validation i.e.
we trained the algorithms on sentences belonging to nine api types in our development set and we tested them on the tenth type.
each type was used once as the test type.
we used ve di erent settings for ltering out features based on low information gain no ltering as well as ltering at a .
.
.
and .
threshold respectively.
only four cases achieved a precision of above .
the random forest classi er without attribute selection achieved a precision of .
with a coverage of .
i.e.
the classi er produced at least one sentence for seven out of the ten types in our development set .
the support vector machine classier showed a similar performance in terms of precision at a value of .
when combined with an information gain lter for features at thresholds of .
and .
with the same coverage.
finally the random forest classi er with an attribute selection threshold of .
achieved perfect precision but only covered a single type in the development set.
balancing precision and coverage using the harmonic mean we conclude that the most promising performance was shown by the support vector machine classi er at information gain thresholds of .
and .
.
we use the .
setting for the remainder of the paper.
7tuning machine learning parameters is part of our future work.table sentences in the comparative study lex mmr kp sise unique applet appletcont.
event component propertych.
file propertyd.
object inputstream url thread accesscontr.
socket connection principal list resultset jcomponent map jframe sum table shows the features in this setting.
the features are diverse ranging from a sentence s similarity to the api documentation and part of speech tags to attributes of the answer question and the user asking the question.
in answering our rst research question regarding the ability of di erent approaches to identify meaningful insight sentences we conclude that only the supervised approach was able to identify insight sentences with reasonable precision and coverage.
.
comparative study to investigate our second research question i.e.
whether practitioners nd these insight sentences useful we conducted a comparative study.
we selected the most commonly used type and the third most commonly used type from each of the ten most commonly used java packages as indicated by apatite cf.
table .
the motivation for this strati ed sampling was to cover a wide range of types while avoiding ones that are rarely used.
we then used all four approaches lexrank mmr patterns and sise to extract insight sentences for these api types.
for lexrank and mmr we used the con guration that achieved the highest precision on the development set i.e.
we considered only the sentence with the highest score .
table shows the number of sentences that each approach extracted.
lexrank and mmr extracted exactly one sentence per api type while patterns extracted between and sentences per api type and sise extracted between and sentences per api type.8as the last column shows the overlap between sentences extracted by di erent approaches was very low of the sentences selected by all approaches for all api types were unique.
seven of the nine overlaps occurred between lexrank and mmr one occurred between lexrank and patterns and one involved patterns and sise.
to keep the number of sentences man8this variation is explained by the length of the stack overow threads linked to each api type.
devising algorithms for ranking sentences is part of our future work.
397table participants in the study job title exp.
area technical consultant years automation mobile software engineer years web research assistant years embedded system postdoc researcher years web systems software engineer years data engineering student years web android developer years mobile software developer years web systems table comparative study results meaningf.
meaningf.
more no added inf.
no added inf.
context sense lex .
.
.
.
mmr .
.
.
.
kp .
.
.
.
sise .
.
.
.
ageable for a study we randomly selected up to four sentences per approach and per api type.
this resulted in at most ten sentences per api type one for lexrank one for mmr at most four for patterns and at most four for sise .
duplicate sentences selected by more than one approach were only shown to participants once.
we recruited eight participants from github randomly selecting from the github users who had made at least one contribution in the previous twelve months used java in at least one of their projects and had published their email address.
we randomly selected email addresses in batches of ten.
it took emails to recruit these eight participants response rate .
the study was conducted using google forms and there were no time constraints.
to minimize bias we did not explain the research goal to participants.
each participant was shown sentences belonging to ve api types leading to a maximum of sentences per participant.
all sentences were rated by exactly two participants.
we asked each participant whether developing software was part of their job about their job title for how many years they had been developing software and what their area of software development was.
table shows the answers.
all participants indicated that developing software was part of their job.
for each pair of an api type and a sentence we asked the participants to choose one of the following options the sentence is meaningful and adds useful information not found in the api documentation.
the sentence is meaningful but does not add useful information to the api documentation.
the sentence requires more context to understand.
the sentence does not make sense to me.
these options were motivated by binkley et al.
s observation that summaries should be judged based on their usefulness rather than their quality alone.
table shows the results of the comparative study.
sise resulted in most ratings indicating a meaningful sentence table inter rater agreement meaningf.
added inf.
meaningf.
no added inf.
req.
more context no sense both in absolute numbers and relatively.
in total of the sentences identi ed by sise were considered meaningful the rst two answer options compared to for patterns for mmr and for lexrank.
when comparing sise to each of the other approaches the di erence between meaningful sentences and not meaningful sentences the last two answer options is statistically signi cant pearson s chi square p .
in addition sise resulted in the highest number of sentences which were considered to add useful information not found in the api documentation.
table shows the inter rater agreement.
out of a total of pairs of ratings were in perfect agreement.
the highest number of disagreements was related to sentences that either require more context to understand or make no sense.
in answering our second research question on the usefulness of insight sentences as perceived by practitioners we conclude that our participants found more sentences which contained useful information in the output of sise compared to the output of other approaches.
.
discussion this section discusses the implications of our work in particular related to user interface design for insight sentence presentation and to the role that meta data on stack over ow can play for the extraction of insight sentences.
in addition we discuss the inter rater agreement from our comparative study in more detail and review the threats to validity.
.
sentence meta data this work shows that the large amount of meta data on stack over ow can be used for the extraction of insight sentences.
compared to state of the art summarization techniques or pattern based techniques which do not take any meta data into account sise achieved higher precision and usefulness.
out of the features with the highest information gain used in the classi er half of them and eight out of the rst ten represent stack over ow meta data such as the number of views on a question or the score of an answer cf.
table .
interestingly the features with the highest information gain also suggest that the meta data of the person asking the question is possibly more important than the meta data of the person authoring the answer.
for example the feature with the third highest information gain is the acceptance rate of the person asking the question.
we hypothesize that the acceptance rate of a user re ects the kinds of questions that such a user would ask and that insight sentences are more likely to come from answers to questions that ask about basic information instead of speci c use cases.
future work will have to be conducted to investigate this hypothesis.
another interesting nding is that the two features that represent the similarity of a potential insight sentence to the corresponding api documentation were the features with 398the highest information gain.
this nding suggests that there is an advantage to interpreting sentences on stack over ow in the context of other documentation sources.
our current hypotheses regarding the positive correlation between sentence similarity and meaningful insights is that a somewhat similar sentence combines content from the api documentation with new information while less similar sentences contain information completely unrelated to an api type.
.
user interface the results of our evaluation suggest that the context of sentences will play an important role when complementing api documentation with sentences from stack over ow.
in fact only of the ratings for sentences extracted by sise indicated that the sentence did not make sense.
another of the ratings indicated that more context was required for the sentence to be understandable.
since this context e.g.
surrounding code snippets the complete answer or the corresponding question is available on stack over ow it would be possible to display it along with an insight sentence.
for example each insight sentence could be accompanied by an expandable widget which shows the entire thread on stack over ow from which the insight sentence originated.
in addition user input similar to the one we gathered as part of our comparative study could be used to continuously improve the extraction of insight sentences.
figure shows the current version of our interface for sise.
in the top left corner of the api documentation a widget is added that shows up to ve insight sentences for the api type.
upon selection of one sentence the sentence is expanded to show the surrounding paragraph from the original source along with a link to the corresponding stack over ow thread.
.
inter rater agreement since the inter rater agreement in our comparative study was relatively low we analyzed the disagreements in more detail.
for the twelve cases where both raters agreed that the sentence was meaningful but disagreed as to whether it added useful information not contained in the api documentation we manually veri ed whether that information could indeed be found in the api documentation.
in nine out of the twelve cases the information in the sentence was available in the api documentation.
however the insight sentence often summarized information in a more succinct way than the api documentation did e.g.
list is an ordered sequence of elements whereas set is a distinct list of elements which is unordered which was extracted by sise for java s list.
in some contexts such sentences could still be useful since they provide a more succinct version of content that is available in the api documentation.
there were cases where one rater indicated that more context was required to understand the sentence while the other rater indicated that the sentence was meaningful.
in many of these cases the background of the users seems to determine whether they understand a sentence or not.
we found a similar situation in our previous work when we asked developers to rate the meaningfulness of task descriptions that we had automatically extracted from their software documentation.
in those cases we argue that displaying such sentences does little harm if some users donot understand them while other users nd them useful.
an example for such a sentence from our data set is yes you should close a resultset which was extracted by the pattern based approach for java s resultset .
arguably this sentence should be accompanied by a question to be more meaningful yet the message from the sentence is understandable without the speci c question.
in cases one participant indicated that a sentence did not make sense while the other participant found it meaningful.
a manual inspection of those cases suggests that in most cases the problem was missing context.
an example is i don t know what s your problem but if you have some problems to run this code you can try to close connection and open other to make the second query a sentence that was extracted by lexrank and mmr and is related to java s resultset .
while the sentence does require more context about the questioner s problem to be understandable it might be helpful without such context if a user is troubleshooting a connection issue related to a resultset.
as mentioned before we are addressing the context issue with a user interface that shows more context when requested.
.
threats to validity a threat to the validity of our results is the manual construction of the development set since we did not attempt to validate the annotated data.
however it would have been practically impossible for us to annotate sentences in a way that would favour speci c approaches.
when the development set was constructed we were not aware that sise would be based on machine learning thus our development set was not biased towards certain features.
the size of the development set is another limitation since the sentences were related to only ten java api types.
however for each api type we considered ten di erent questions on stack over ow and for each question we considered up to ten answers.
in total the sentences originated from di erent answers.
in addition our nding that sise outperformed state of the art summarization techniques and a pattern based approach was con rmed in a comparative study with sentences related to another twenty java api types.
the agreement about insight sentences between our study participants was relatively low.
it is natural for software developers with di erent backgrounds and di erent experience to disagree on what information is useful.
despite the disagreements the comparative study clearly showed that sise produced the most meaningful and useful insight sentences.
the evaluation of the usefulness of the insight sentences was based on subjective assessment from the study participants.
although all sentences were judged by two participants to eliminate the threat of individual bias it is nevertheless possible that the responses may be a ected by collective bias.
there was no mechanism to ensure participants read the api documentation before rating sentences and we acknowledge this threat to validity.
we cannot claim that sise generalizes beyond java.
however none of the features used in sise are speci c to java and we are optimistic that we can achieve similar results for other programming languages in future work.
we also cannot make claims regarding generalizability beyond stack over ow.
however with more than million answers stack over ow is a big enough data source to war399figure screenshot of java s mapdocumentation with insight sentences produced by sise rant specialized tools to utilize its data.
sise will only work if a topic is discussed on stack over ow.
since all insight sentences used in this paper were obtained from sets of ten stack over ow threads associated with an api type we would expect comparable results for any api type with at least ten threads on stack over ow.
as we found in our previous work of the types of the java api are discussed on stack over ow android thus we do not expect library popularity to be a major limitation.
.
related work work related to our approach for insight sentence extraction can be divided into work on harnessing stack over ow data and work on improving api documentation.
.
harnessing stack overflow data seahawk by bacchelli et al.
is an eclipse plug in that integrates stack over ow content into an integrated development environment ide .
seahawk automatically formulates queries from the current context in the ide and presents a ranked and interactive list of results.
the tool lets users identify individual discussion pieces and import code samples through drag drop.
in addition users can link stack over ow discussions and source code.
an evaluation of seahawk showed that the tool can produce surprising insights that aid a developer in program comprehension and software development .
a related tool called prompter was later proposed by ponzanelli et al.
.
given the ide context prompter automatically retrieves pertinent discussions from stack over ow evaluates their relevance and noti es developers about the available help if a given threshold is surpassed.
other approaches have focused on harnessing stack overow data for explaining stack traces in the ide.
cordeiro et al.
developed a tool that integrates the recommendation of q a web resources related to stack traces into eclipse.
their preliminary evaluation showed that their ap proach outperformed a simple keyword based approach.
in a similar line of work rahman et al.
developed a contextaware ide based meta search engine for recommendations about programming errors and exceptions.
the stack overow api is one of the search apis used in their work and their approach captures the context in a similar fashion to the work by cordeiro et al.
in an evaluation the authors found that the inclusion of di erent types of contextual information associated with an exception can enhance the accuracy of recommendations.
arguably the work that is most similar to ours is autocomment the automatic comment generation approach introduced by wong et al.
since it also harnesses the natural language text available on stack over ow.
autocomment extracts code descriptions mappings which are code segments together with their descriptions from stack over ow and leverages this information to automatically generate descriptive comments for similar code segments in open source projects.
the authors applied autocomment to java and android projects and they were able to automatically generate comments for projects.
in a user study the majority of participants found the generated comments to be accurate adequate concise and useful in helping them understand the code.
our work di ers from that by wong et al.
in that we focus on single sentences from stack over ow that are relevant to an api type instead of a code snippet.
related to our solution for linking stack over ow threads to api types is the work by rigby and robillard .
their traceability recovery approach discovers essential code elements in informal documentation such as stack over ow.
our linking approach for linking stack over ow threads to api types works the other way around.
we start from an api type and then use the stack over ow api as well as a number of regular expressions to nd threads that are related to that api type.
400in terms of using machine learning to discover content on stack over ow there are some common themes between sise and the work of de souza et al.
.
they developed an improved search engine for content on stack over ow which recommends question and answer pairs as opposed to entire q a threads based on a query.
the ranking criteria used by their approach consists of the textual similarity of the question and answer pairs to the query and the quality of these pairs.
in addition their search focuses on how to threads.
in an evaluation of their work the authors found that their approach was able to recommend at least one useful question and answer pair for most queries many of which included a reproducible code snippet.
in comparison the goal of sise is the extraction of insight sentences from stack over ow that add useful information to api documentation.
our catalogue of machine learning features is also more extensive and includes features that bridge the gap between di erent documentation formats.
.
improving api documentation several researchers have contributed e orts for the improvement of api documentation.
stylos et al.
introduced jadeite which displays commonly used classes more prominently and automatically identi es the most common ways to construct an instance of any given class.
emoose by dekel and herbsleb improves api documentation by decorating method invocations whose targets have associated usage directives such as rules or caveats of which authors of invoking code must be aware.
in a similar e ort pandita et al.
proposed to infer formal speci cations from natural language text.
more closely related to sise is the proposal for integrating crowdsourced faqs into api documentation by chen and zhang .
they propose to connect api documentation and informal documentation through the capture of developers web browsing behaviour.
in contrast we connect di erent forms of documentation through heuristics that match stack over ow threads to api types and instead of faqs sise produces insight sentences.
other work has focused on detecting and preventing api documentation errors.
zhong and su introduced docref an approach that combines natural language processing techniques and code analysis to detect and report inconsistencies in documentation.
the authors successfully used docref to detect more than one thousand documentation errors.
dagenais and robillard introduced addoc a technique that automatically discovers documentation patterns i.e.
coherent sets of code elements that are documented together and that reports violations of these patterns as the code and the documentation evolve.
previous work has successfully identi ed natural language text that is potentially important for a programmer using a given api type.
chhetri and robillard categorized text fragments in api documentation based on whether they contain information that is indispensable valuable or neither using word patterns.
when we applied their patterns to content on stack over ow we were not able to repeat their positive results in terms of precision and usefulness see section .
.
petrosyan et al.
proposed an approach to discover tutorial sections that explain a given api type.
they classi ed fragmented tutorial sections using supervised text classi cation based on linguistic and structural features and they were able to achieve high precision and recall on dif ferent tutorials.
their work di ers from ours in that we use stack over ow s meta data for sise.
in addition unlike petrosyan et al.
we focus on the extraction of single sentences instead of entire documentation fragments.
several researchers have focused on augmenting api documentation with code examples.
for example kim et al.
proposed a recommendation system that returns api documents embedded with code example summaries mined from the web.
their evaluation results showed that the approach provides code examples with high precision and boosts programmer productivity.
in a similar e ort subramanian et al.
introduced baker an iterative deductive method for linking source code examples to api documentation.
in contrast to these tools sise links natural language sentences from stack over ow to api documentation.
.
conclusion and future work while the rise of social media and q a sites such as stack over ow has resulted in a plethora of information for software developers available in many di erent formats on the web it can be di cult to determine where a particular piece of information is stored.
in an e ort to bring documentation from di erent sources together we presented an evaluation of di erent techniques for extracting insight sentences from stack over ow.
we de ne insight sentences as those sentences on stack over ow that are related to a particular api type and that provide insight not contained in the api documentation of the type.
in a comparative study with eight software developers to evaluate the meaningfulness and usefulness of the insight sentences we found that our supervised approach sise resulted in the highest number of sentences which were considered to add useful information not found in the api documentation.
we conclude that considering the meta data available on stack over ow along with natural language characteristics can improve existing approaches when applied to stack over ow data.
we believe that we are the rst to investigate augmenting natural language software documentation from one source with that from another source.
our work is a rst step towards a vision of presenting users with combined documentation from various sources rather than users having to look through di erent sources to nd a piece of information.
we plan to extend this work beyond the java api and we plan to experiment with more features that capture the grammatical structure of sentences on stack over ow.
determining whether a sentence is meaningful on its own is non trivial and while our evaluation showed that a supervised approach can detect such sentences based on part of speech tags with a higher precision than summarization or pattern based approaches we expect that the precision can further be improved with a deeper understanding of each sentence and its dependencies on other sentences or code snippets.
in addition we intend on applying the idea of insight sentence extraction to other textual artifacts produced by software developers such as bug reports commit messages or code comments.