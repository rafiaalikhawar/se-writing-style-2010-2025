automatically generating test templates from test names benwen zhang university of delaware newark de usa benwen udel.eduemily hill drew university madison nj usa emhill drew.edujames clause university of delaware newark de usa clause udel.edu abstract existing specification based testing techniques require specifications that either do not exist or are too difficult to create.
as a result they often fall short of their goal ofhelping developers test expected behaviors.
in this paper wepresent a novel natural language based approach that exploitsthe descriptive nature of test names to generate test templates.similar to how modern ides simplify development by providingtemplates for common constructs such as loops test templatescan save time and lower the cognitive barrier for writing tests.the results of our evaluation show that the approach is feasible despite the difficulty of the task when test names contain asufficient amount of information the approach s accuracy is over80 when parsing the relevant information from the test nameand generating the template.
i. i ntroduction software testing is an expensive and laborious activity that can account for the majority of the total cost of developing software.
in the future such high costs are likely to persist oreven increase as the growing size and complexity of modernsoftware exacerbates existing challenges.
techniques that im prove the efficiency and effectiveness of the testing processcan thus significantly reduce the overall cost of softwaredevelopment as well as improve software quality.
fortunately the software engineering research community has provided many techniques that can help developers reducethe costs of testing.
one group of techniques attempts tocompletely remove the burden of testing by automating the testgeneration process e.g.
.
while the tests generatedby such automated techniques can be successful at revealingsome types of unexpected behavior i.e.
sad paths theyare much less successful at helping developers test expectedbehaviors i.e.
happy paths .
in addition the tests often lookvery different from manually written tests and are difficult tomaintain and understand .
in contrast to techniques that attempt to completely automate the test generation process specification based testgeneration techniques attempt to help developers test expectedbehaviors by creating tests from specifications e.g.
.
ideally such specification based approaches candecrease the costs of testing by eliminating the tedious anderror prone work of manually translating specifications intoexecutable tests.
however they often fall short of this goalbecause the required specifications either do not exist or mustbe written in a format that takes as much if not more effortthan manually writing the tests.in this paper we present a new specification based approach that can help reduce the costs of testing by eliminatingsome of the tedious and error prone work of writing unit tests.at a high level the approach leverages test names whichcapture a developer s testing intent to automatically generatetest templates.
similar to how modern ides provide templatesfor common constructs such loops try catch blocks methodcalls etc.
our approach infers what and how the developerwants to test and generates the corresponding code.
notethat like the templates generated by ides the test templatesgenerated by our approach will likely have holes locationswhere developers will need to provide additional informationthat is not included in the test name.
developers can fill in suchholes by either providing the necessary information directly e.g.
adding concrete inputs for method calls or by improvingthe test s name.
encouraging developers to write better testnames in this manner is an additional benefit of the approach potentially simplifying future maintenance tasks.
to assess the feasibility of the approach we conducted a preliminary evaluation by manually comparing the templatesgenerated by our approach to the original developer writtentest bodies.
despite the difficulty of the task when test namescontain a sufficient amount of information the approach saccuracy at parsing the relevant information from the test nameand generating the corresponding template is over .
ii.
m otiv a tion our choice to infer developer intent from test names is based on a combination of two observations.
the first observa tion is that every unit test must have a corresponding name.
while this may seem unremarkable it does mean that develop ers are accustomed to providing such information.
therefore using it as the basis for our approach means that unlike forother specification based techniques there is no additionaloverhead for developers in either creating specifications ormodifying them to fit a specific format.
the second observationis that test names should describe and summarize important parts of the test s body.
more specifically they shouldindicate the scenario being tested and the expected outcome.as such they can serve as a partial specification for the test.
as examples of the descriptiveness of test names consider the tests shown in figure .
despite coming fromdifferent projects and being written by different develop ers the name of each test summarizes its body in testmaximumsize negative the test sets the maximum size 30th ieee acm international conference on automated software engineering .
ieee public void testmaximumsize negative cachebuilder builder new cachebuilder object object nobreakspace try nobreakspace builder.maximumsize nobreakspace nobreakspace fail nobreakspace catch illegalargumentexception expected a guava test for the cachebuilder class.
public void testentrysetclearchangesmap map map makefullmap set entryset map.entryset asserttrue map.size asserttrue entryset.size entryset.clear asserttrue map.size asserttrue entryset.size b apache commons collections test for the map class.
public void testsettingheightthatistoosmallleavesheightunchanged barcode barcode new barcodemock int height barcode.getheight barcode.setbarheight assertequals height barcode.getheight c barbecue test for the barcode class.
fig.
.
unit tests illustrating the descriptive nature of test names.
testsetting height changes default action predicate action object predicate object fig.
.
test name annotated with its parts of test.
of the cache builder to a negative number in testentrysetclearchangesmap the test checks whether clearing the map s entry set changes the contents of themap and in testsettingheightthatistoosmallleavesheightunchanged the test checks whether set ting the barcode s bar height to zero changes its height.
iii.
p arts of test beyond simply summarizing test bodies as shown in section ii we have found that test names frequently follow a setof well defined grammatical rules.
while in theory developershave the full range of flexibility allowed by the programminglanguage when naming tests in practice test names often follow a relatively well defined grammatical structure.
thisregular set of grammatical conventions makes it possible toextract relevant information from test names that relates tothe test s body.
more specifically it allows for automaticallytagging the name with what we refer to as parts of test pots the testing equivalent to parts of speech.
at a high level test names typically consist of two verb phrases an action phrase and a predicate phrase.
the action phrase describes the scenario being tested including actionswhich should be performed and the state of the environment whereas the predicate phrase describes the expected outcomeof the test.
for example consider the test name shown infigure .
here the action under test is setting the height andthe result of that action is that default should be changed .
in some cases the action phrase and predicate phrase can be further broken down into individual components of a verb an action or a predicate and an object noun phrase actionpublic void testgetdatareturnsdata string data ... barcode barcode new barcode data string actual barcode.getdata assertequals data actual a public void testgetdatareturnsdata string data barcode barcode new barcodemock data assertequals data barcode.getdata b fig.
.
comparison between the the test template generated by our approach a and the developer written test b .
object or predicate object.
in figure the action is setting its object is height what is being set the predicate is changes and the predicate object is default .
some very descriptive test names include additional information beyond the action or predicate phrase.
forexample in testsettingheightthatistoosmallleavesheightunchanged the phrase that is toosmall describes how the action object height shouldbe set.
similarly in testcopy bytearraytowriterwithencoding the phrases to writer and with en coding further describe how the byte array should be copied.these phrases are not part of the action phrase or predicatephrase but rather modify or further describe them.
in ourexperience we have observed that these modifiers describe theaction phrase and thus we refer to them as action modifiers.
in summary test names typically contain one or more of the following pots action action object action modifiers predicate and predicate object.
iv .
a utoma tically genera ting test templa tes given a test name and a class under test the approach creates a corresponding test template using two main phases.the first phase parsing involves parsing the test name to identify the relevant information it contains i.e.
the actionphrase and the predicate phrase .
note that because of po tential ambiguities in how a test name can be interpreted multiple parses may be produced by this step.
the secondphase generation takes as input the set of parses produced by the first phase and the class under test to produce oneor more test templates for each parse.
to produce the testtemplates the generation phase statically analyzes the codeof the application under test to map the identified pots tomethods fields parameters assertions etc.
again multipletemplates may be generated for a parse because of potentialambiguities in how the pots can be mapped to executablecode.
for example there may be multiple methods with thesame name or multiple ways to access a field.
in practice developers can choose among the generated test templatesin the same manner they currently choose among possibleautocompletion options with the ide allowing them to quicklycycle though the options.
a. concrete example as a concrete example of the output of each phase assume that a developer wants to create a test named testgetdata507returnsdata to test the barbecue application s barcode class.
the first phase analyzes the name and tags get as the action data as the action object returns as the predicate and data as the predicate object.
given this parse the second phase generates the template shown in figure a .
as a pointof comparison the developer written test body for this test isshown in figure b .
in the test template an instance of barcode the class under test is instantiated.
note that the constructor requires a singleargument of type string.
because the test name does not include any information about how the environment should besetup i.e.
any action modifiers a hole is left by the approach but the necessary type is clearly indicated.
note that in thiscase the approach could easily provide a randomly selectedstring as the specific value is irrelevant.
however because thisfact is difficult to infer in general the approach leaves the holefor the developer to complete instead.
to generate the scenario under test part of the template the approach examines the code of barbecue in order to mapthe action phrase get data to executable code.
here theapproach finds a method of the barcode class called getdata that matches the action phrase.
to generate the expectedoutcome part of the test the approach matches the predicate returns to a predefined assertion method.
in this case themost appropriate assertion is assertequals because the intention is to compare the result of performing the action.finally the predicate object data is added to the predicateand compared against the result of getting the barcode s data.while it may be surprising that the approach can identify thatthe string argument provided to the constructor should alsoserve as the expected value in the assertion it makes thisconnection using static analysis.
analyzing barbecue revealsthat the constructor s parameter is named data .
with thisadditional information it becomes possible to infer that thedata returned by getdata should be the same data that was used to create the barcode.
the fact that the test template generated by our approach closely matches the developer written test body demonstratesthe usefulness of our approach.
instead of having to type thename and then repetitively type the same information in thebody the developer can rely on the approach to generate anappropriate template.
beyond the test name the developer onlyneeded to provide a concrete value for the data used to createthe barcode.
by eliminating the need to type test bodies theapproach allows developers to focus on choosing relevant testdata or simply move on to other tasks more quickly.
b. phase parsing test names the goal of the parsing phase is to identify the action phrase and predicate phrase contained in a test name when these phrases exist.
our approach for identifying such phrasesis the result of combining information about english grammar knowledge of and intuition about the conventions developersfollow when writing tests and a manual examination ofexisting test cases.
extracting grammatical relations the first step in identifying action phrases and predicate phrases is to use anoff the shelf parsing system for english to identify the gram matical relations among the words in the test name excluding fig.
.
semantic graph for testcopy bytearraytowriterwithencoding .
the leading test .
the result of this step is a semantic graphthat shows the grammatical relationships among the words andtheir part of speech pos .
all of the semantic graphs extracteddirectly from the sentence fragment are passed onto the nextstep to identify the pots.
figure shows one of the semantic graphs that results from parsing testcopy bytearraytowriterwithencoding .
each node in the graph represents anoccurrence of a word in the sentence fragment and its part of speech based on the claws7 tagset verb noun preposition etc.
.
common word stems are indicated by .
for instance encoding is an ing verb participle vvg and it has thestem ing .
the edges in the graph represent grammaticalrelations such as subjects xsubj ncsubj clausal compli ments when overt subjects are missing xcomp ccomp clausal modifiers xmod ncmod cmod as well as directand indirect objects dobj iobj obj2 .
identifying parts of test the second step in identifying action phrases and predicate phrases is to use the semanticgraph to tag the test name with its pots.
in identifying pots we are looking to identify the action and predicate phrasesas well as action modifiers.
recall that simple sentences inenglish consist of a subject verb and then an object.
notethe use of the word subject in this subsection is distinct fromthe test s subject or class under test used elsewhere in thepaper.
if the semantic graph includes a word with the role ofsubject then the approach identifies that subject as the actionobject its verb as the predicate and the verb s correspondingdirect or indirect object as the predicate object.
prepositionsother than to denote action modifiers.
if the semantic graphdoes not contain a subject and the test name begins with averb we assume there is no predicate in the name and attemptto extract only an action phrase and action modifiers.
one of the major challenges in identifying pots is robustness.
a test name may yield many possible semanticgraphs some of which may be perfectly correct some that areworkable and some that are too incomplete to analyze.
thus we have designed our approach to be as robust as possible inthe face of errors in the grammatical relation parsing and partof speech tagging.
we take a best effort approach that attemptsto match as many rules as possible and prioritizes the outputof our approach by the most frequently identified pots.
508c.
phase generating test templates the goal of the generation phase is to transform the pots provided by the parsing phase into test templates.
at a highlevel the process for transforming a parse into a test templateis done in two steps.
the first step is to convert the actionphrase into the scenario under test part of the template and thesecond step is to convert the predicate phrase into the expectedoutcome part of the template.
generating the scenario under test intuitively action phrases are converted into the scenario under test part of atemplate by searching the code of the application under test inorder to find sequences of method calls and field accesses thatcorrespond to the elements of the action phrase.
briefly thesearch process starts with the class under test and performs abreadth first traversal along the field types and method returntypes of the encountered classes looking for entities that matchthe specified action.
the path between the identified action andthe subject is then the sequence of method invocations and fieldaccesses that will be performed by the test.
in order to facilitatedifferent ways of matching we use the strategy pattern to allowthe search process to transparently use different matchers.after the search is complete the matched fields and methodsare ranked according to the matcher that identified them.
generating the expected outcome checks the process of converting the predicate phrase into the expected outcomepart of the template is simpler than converting the action phraseinto the scenario under test.
because there are only a smallnumber of possible assertions we do not need to use a fullsearch process.
rather we can use a set of manually createdrules that look for specific words in the predicate phrase aswell as the attitude of the test name to choose an assertion.
ifnegative words such as not or cannot occur anywhere inthe test name we classify the attitude as negative otherwisethe attitude is positive.
for example if the predicate contains the words true or false we assume that the corresponding assertion shouldbeasserttrue orassertfalse respectively.
if the predicate is a linking verb the predicate object is a constant and the attitude is positive we choose assertequals .
if thepredicate contains the words throw fail or exception fail will be chosen.
currently this manually generated rule system is adequate but we plan to improve it in future workby using machine learning techniques to generalize theserules from a large set of actual test names and source code.such information will allow for a more accurate mapping ofpredicate phrases to assertions.
the final step in the template generation process is to combine the scenario under test part of the template and theexpected outcome part of the template.
unlike other test gen eration approaches the templates generated by our approachdo not need to be executable which can simplify the process.instead of needing to construct potentially complex data wecan simply leave a hole that the developer can customize tosuit their particular testing goals.
we identified five common patterns for the test templates the do and affect pattern the no effect pattern the before and after pattern the throw exception pattern and the default pattern.
of the five patterns the default pattern is the mostcommon.
it follows a simple sequential input and oraclestructure that consists of initializing the subject class a methodsequence invocation and asserting on the returned value.
thedo and affect pattern is similar but slightly different in thatit invokes a method sequence in the input part of the test andasserts the returned value from another invocation of a methodsequence or field access.
this pattern is especially suitable fortesting paired getters and setters.
the no effect pattern and thebefore and after pattern are similar in that the input and oracleparts of these two pattern are interlaced.
their code usuallyfollows an order of initializing the subject class a methodsequence invocation asserting a returned value or field anothermethod sequence invocation and another assert on a returnedvalue or field.
the difference between these two patterns istheir different testing objectives.
the no effect pattern checkswhether an action unexpectedly affects the status of the objectunder test.
on the other hand the before and after pattern testswhether the input changes the status of the object under testin expected way.
the last pattern the throw exception pattern tests whether the input causes an exception to be thrown.
v. e v alua tion the goal of our evaluation is to determine whether our approach can automatically create test templates that closelymatch tests written by developers.
to perform a meaningfulevaluation with respect to this goal we must therefore considertest names that are descriptive if we considered test namesthat are void of meaning it would be difficult to assessthe benefit that our approach can provide.
unfortunately notevery test name is descriptive.
in practice because naming isdifficult and there is no immediate downside to choosing poornames developers often create generic test names e.g.
test1 test2 etc.
or names that contain very little information e.g.
testadd testsubtract etc.
.
given a descriptive test name we determine if the generated test template bears any resemblance to the test bodywritten by the original developer.
since there may be manysemantically equivalent code sequences that test the same func tionality our goal is not to generate the exact same test caseas the original developer.
as such we determine feasibility ofour approach by investigating whether the basic components object under test action method call assert structure matchthe developer written test.
a. test name descriptiveness to avoid the potential bias that could result from choosing descriptive test names ourselves we instead had masters and phd students from the university of delaware s softwaretesting and maintenance class label test names with theirpots.
in the remainder of this section we refer to thesegraduate students as labelers.
first we used a weighted random selection strategy to choose test names from the test suites of barbecue commons beanutils .
.
commons cli .
commons collections .
.
commons io .
datastructures and gson .
.
.
because labeling test names with their pot is timeconsuming labelers took from 45min to 60min to label the pots in 15names and we want to choose as many descriptive test names as possible we biased the selectionby the number of words in each test name presumably test 509public void testioexceptionstringthrowable throwable cause new illegalargumentexception cause ioexceptionwithcause exception new ioexceptionwithcause message cause assertequals message exception.getmessage assertequals cause exception.getcause assertsame cause exception.getcause fig.
.
name containing an action phrase that does not describe the test.
names that contain more words are more likely to contain more information.
to count the number of words contained in a testname we stripped the leading test and used a purpose buildidentifier splitter.
the final set contained 10test names with 1or2words 30test names with 3words and 60test names with 4or more words.
after choosing the set of test names we randomly assigned 15names to each labeler.
because we had 20labelers assigning 15names to each allowed us to keep the expected length of the task below 1h and to have each test name labeled three times.
for each test name the labeler was shown the testname as well as the name of the class under test and a briefdescription of the project from which the test name was taken.the labeler was then asked to identify which part of the testname should be tagged with each of the five pots action action object action modifiers predicate and predicate object.if a labeler felt that the test name did not contain a specificpot they were instructed to enter a special n a value.
dueto labeler attrition 10of the test names were not labeled a sufficient number of times and were removed from the setleaving us with 90labeled test names.
we then calculated how often a majority of labelers agree on whether a test name contains each pot for action labellersagreed of the time for action object the labellers agreed of the time for action modifiers the labellers agreed of the time for predicate the labellers agreed o f the time and for predicate object the labellers agreed of the time.
as this data shows tagging test names with theirpot is a relatively difficult task.
using the pots assigned by the labelers and the agreement scores we identified which test names contain a sufficientamount of information as follows test names where a majorityof labelers agree that it contains either an explicit action i.e.
not get or set or an implicit action i.e.
get or set and an action object are presumed to contain an actionphrase and test names where a majority of labelers agree thatit contains both a predicate and a predicate object presumablycontain a predicate phrase.
under this classification of the test names out of have enough information for the template to include the scenario under test i.e.
theycontain an action phrase and 39out of have enough information for the template to include the expected outcome i.e.
they contain a predicate phrase .
b. comparing templates with developer written tests after identifying which test names contain a suitable amount of information we generated the corresponding test templates for the 54test names that have an action phrase and the test names that have a predicate phrase.
we manually compared the templates against the developer writtentable i. r esults of comparing the templa tes genera ted by our approach to the developer written test bodies .
outcome action phrase predicate phrase match incorrect parse incorrect template test bodies to determine whether they match.
we label a testtemplate as a match if it invokes the exact same method as themain test action which requires instantiating a similar type ofobject and when it uses the same assertion pattern includingsimilarly typed actual and expected values .
for templates thatdo not match the developer written test body we classified thecause of the mismatch as either an irrelevant name an incorrectparse or an incorrect template.
the first mismatch cause irrelevant name was assigned when the test name did not describe either the scenariounder test or the expected outcome.
although our labelersagreed that these names contained an action phrase or apredicate phrase the information from the name does notsummarize the test body.
figure shows an example ofan irrelevant name.
in this case the action phrase getioexception does not describe the scenario being tested creating a throwable illegalargumentexception.
the second mismatch cause incorrect parse was assigned to when the parser incorrectly identified the information contained inthe test name.
the third mismatch cause incorrect template was assigned when the parser correctly identified the informa tion contained in the name but the template generator chosean incorrect method field sequence assertion etc.
because cases where the name of the test is irrelevant are equivalent to cases where the test name does not containsufficient information we removed them from further consid eration.
if the specification is wrong it is nearly impossiblefor the template to be correct.
after removing the tests withirrelevant names we were left with 44relevant test names that contain an action phrase and 37relevant test names that contain a predicate phrase.
table i shows the results of comparing the test templates for the relevant test names to the developer written test bod ies.
the first column outcome shows whether the template matched was a parser mistake or template generation mistake.the second and third columns action phrase and predicate phrase show for the test names that contain an action phraseor predicate phrase respectively the number of times eachoutcome occurred.
when considered individually the accuracy of the components of the approach are above .
more specifically for test names that contain an action phrase the parser was ableto extract the correct information of the time and the generator was able to generate a template that matchedthe developer written body of the time .
for test names that contain a predicate phrase the parser was able toextract the correct information of the time and the generator was able to generate a template that matched thedeveloper written body of the time .
based on this data we conclude that it is feasible to automatically generatetest templates based on the developer intent expressed in testnames.
510vi.
r ela ted work specification based test case generation at a highlevel specification based approaches share our overall goal of reducing the cost of writing tests by automatically generatingtests from some form of documentation.
specific types ofdocumentation that have been considered include systemdescription languages sdls use cases uml and scenarios .
in addition to generating test cases specification based techniques have also been used to generateapplication code .
in contrast to the english test methodnames used by our approach the forms of documentationrequired by other specification based approaches are less likelyto exist and are more difficult to create.
code completion code completion techniques attempt to reduce the costs of coding rather than testing by reducinga developer s searching and typing.
v arious code completiontechniques have been developed based on hidden markovmodels hmms ranking statistical properties of source code machine learning data mining as well as other matchingapproaches to recommend apis according to users incompleteinputs e.g.
.
code snippet search code snippet search techniques attempt to locate and retrieve relevant source code in responseto a query.
related work in this field include experiments toevaluate diverse approaches to code search techniquesto improve the searchability of source code e.g.
techniques for extracting information from diverse sources e.g.
and interactive techniques e.g.
.
natural language program analysis finally in the area of natural language program analysis nlpa researchershave investigated identifier splitting techniques e.g.
the technique for tagging words with their part of speech andidentifying large semantic structures the technique foridentifying programming specific synonyms and antonyms and the comment generation technique .
these techniquesare not alternatives to our approach but can be used to improveits accuracy and effectiveness.
vii.
c onclusions we presented a novel specification based approach for automatically generating test templates based on the developerintent expressed in test names.
the approach can help reducetesting costs by automating some of the tedious and error pronework searching for code and manually writing tests.
in addi tion it encourages developers to write more descriptive testnames which can provide long term benefits by simplifyingtest comprehension and maintenance tasks.
our preliminaryevaluation provides evidence that the approach is feasibleand promising despite the difficulty of the task when testnames contain a sufficient amount of information its accuracyat parsing the relevant information from the test name andgenerating the corresponding template is over .
viii.
a cknowledgments this work is supported in part by national science foundation grant no.
.