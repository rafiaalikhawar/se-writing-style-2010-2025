concolic testing for deep neural networks youcheng sun university of oxford uk youcheng.sun cs.ox.ac.ukmin wu university of oxford uk min.wu cs.ox.ac.ukwenjie ruan university of oxford uk wenjie.ruan cs.ox.ac.uk xiaowei huang university of liverpool uk xiaowei.huang liverpool.ac.ukmarta kwiatkowska university of oxford uk marta.kwiatkowska cs.ox.ac.ukdaniel kroening university of oxford uk kroening cs.ox.ac.uk abstract concolictestingcombinesprogramexecutionandsymbolicanalysis to explore the execution paths of a software program.
this paper presents the first concolic testing approach for deep neural networks dnns .morespecifically weformalisecoveragecriteria fordnnsthathavebeenstudiedintheliterature andthendevelop a coherent method for performing concolic testing to increase test coverage.ourexperimentalresultsshowtheeffectivenessoftheconcolic testing approach in both achieving high coverage and finding adversarial examples.
ccs concepts software and its engineering software defect analysis keywords neural networks symbolic execution concolic testing acm reference format youchengsun minwu wenjieruan xiaoweihuang martakwiatkowska and daniel kroening.
.
concolic testing for deep neural networks.
in proceedingsofthe201833rdacm ieeeinternationalconferenceonautomated software engineering ase september montpellier france.
acm newyork ny usa 11pages.
introduction deep neural networks dnns have been instrumental in solving a rangeofhardproblemsinai e.g.
theancientgameofgo image classification andnaturallanguageprocessing.asaresult many potentialapplicationsareenvisaged.however majorconcernshave beenraisedaboutthesuitabilityofthistechniqueforsafety and security critical systems where faulty behaviour carries the risk of endangering human lives or financial damage.
to address these concerns a safety or security critical system comprising dnnbased components needs to be validated thoroughly.
kwiatkowskaandruanaresupportedbyepsrcmobileautonomyprogrammegrant ep m019918 .
wu is supported by the csc pag oxford scholarship.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september montpellier france copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
software industry relies on testing as a primary means to provide stakeholders with information about the quality of the software product or service under test .
so far there have been onlyfewattemptstotestdnnssystematically .
theseareeitherbasedonconcreteexecution e.g.
montecarlotree search orgradient basedsearch orsymbolicexecutionincombinationwithsolversforlineararithmetic .together withthesetest inputgenerationalgorithms severaltestcoverage criteria have been presented including neuron coverage a criterion that is inspired by mc dc and criteria to capture particular neuron activation values to identify corner cases .
none of these approaches implement concolic testing which combines concrete execution and symbolic analysis to explore the execution paths of a program that are hard to cover by techniques such as random testing.
we hypothesise that concolic testing is particularly well suited fordnns.theinputspaceofadnnisusuallyhighdimensional whichmakesrandomtestingdifficult.forinstance adnnforimage classification takes tens of thousands of pixels as input.
moreover owing to the widespread use of the relu activation function for hiddenneurons thenumberof executionpaths inadnnissimply toolargetobecompletelycoveredbysymbolicexecution.concolictestingcanmitigatethiscomplexitybydirectingthesymbolicanalysistoparticularexecutionpaths throughconcretelyevaluating given properties of the dnn.
in this paper we present the first concolic testing method for dnns.
the method is parameterised using a set of coverage requirements which we express using quantified linear arithmetic over rationals qlar .
for a given set rof coveragerequirements we incrementally generate a set of test inputs to improve coverage byalternating betweenconcrete executionand symbolicanalysis.
given an unsatisfied test requirement r we identify a test input twithin our current test suite such that tis close to satisfying r accordingtoanevaluationbasedon concreteexecution .afterthat symbolic analysis is applied to obtain a new test input t primethat satisfiesr.
the test input t primeis then added to the test suite.
this process is iterated until we reach a satisfactory level of coverage.
finally the generated test suite is passed to a robustness oracle which determines whether the test suite includes adversarial examples i.e.
pairs of test cases that disagree on their classificationlabelswhenclosetoeachotherwithrespecttoagiven distance metric.
the lack of robustness has been viewed as a major weakness of dnns and the discovery of adversarial examples and the robustness problem are studied actively in several domains including machine learning automated verification cyber security and software testing.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. sun m. wu w. ruan x. huang m. kwiatkowska d. kroening overall the main contributions of this paper are threefold we develop the first concolic testing method for dnns.
we evaluate the method with a broad range of test coverage requirements includinglipschitzcontinuity andseveralstructuralcoveragemetrics .weshow experimentallythatour newalgorithmsupports thisbroad range of properties in a coherent way.
weimplementtheconcolictestingmethodinthesoftware tooldeepconcolic1.
experimental results show that deepconcolicachieveshighcoverageandthatitisabletodiscover a significant number of adversarial examples.
related work we briefly review existing efforts for assessing the robustness of dnns and the state of the art in concolic testing.
.
robustness of dnns current work on the robustness of dnns can be categorised as offensive or defensive.
offensive approaches focus on heuristic search algorithms mainly guided by theforward gradient orcost gradient of the dnn to find adversarial examples that are as close as possible to a correctly classified input.
on the other hand the goalofdefensiveworkistoincreasetherobustnessofdnns.there is an arms race between offensive and defensive techniques.
in this paper we focus on defensive methods.
a promising approach is automated verification which aims to provide robustness guarantees for dnns.
the main relevant techniques include a layer by layer exhaustive search methods that use constraint solvers globaloptimisationapproaches andabstractinterpretation toover approximateadnn sbehavior.exhaustive searchsuffersfromthestate spaceexplosionproblem whichcan be alleviated by monte carlo tree search .
constraint based approachesarelimitedtosmalldnnswithhundredsofneurons.
global optimisation improves over constraint based approaches through its ability to work with large dnns but its capacity is sen sitive to the number of input dimensions that need to be perturbed.
theresultsofover approximatinganalysescanbepessimisticbecause of false alarms.
the application of traditional testing techniques to dnns is difficult andworkthatattemptstodosoismorerecent e.g.
.
methods inspired by software testing methodologies typicallyemploycoveragecriteriatoguidethegenerationoftest cases the resulting test suite is then searched for adversarial examples by querying an oracle.
the coverage criteria considered includeneuroncoverage whichresemblestraditionalstatement coverage.asetofcriteriainspiredbymd dccoverage isused in maetal.
presentcriteriathataredesignedtocapture particular values of neuron activations.
tian et al.
study the utility of neuron coverage for detecting adversarial examples in dnns for the udacity didi self driving car challenge.
we now discuss algorithms for test input generation.
wicker et al.
aimtocovertheinputspacebyexhaustivemutationtesting thathastheoreticalguarantees whilein gradient based search algorithms are applied to solve optimisation problems and sunetal.
applylinearprogramming.noneoftheseconsider testing and a general means for modeling test coverage requirements as we do in this paper.
.
concolic testing by concretely executing the program with particular inputs which includes random testing a large number of inputs can be tested at low cost.
however without guidance the generated test cases may be restricted to a subset of the execution paths of the program and the probability of exploring execution paths that contain bugs can beextremelylow.insymbolicexecution anexecution path is encoded symbolically.
modern constraint solvers can determine feasibility of the encoding effectively although performance stilldegradesasthesizeofthesymbolicrepresentationincreases.
concolic testing isan effective approach toautomated test inputgeneration.itisahybridsoftwaretestingtechniquethatalternates between concrete execution i.e.
testing on particular inputs andsymbolicexecution aclassicaltechniquethattreatsprogram variables as symbolic values .
concolic testing has been appliedroutinely in software testing and a wide range of tools is available e.g.
.
it starts by executing the program with a concrete input.
at the end of the concreterun anotherexecutionpathmustbeselectedheuristically.
this new execution path is then encoded symbolically and the resulting formula is solved by a constraint solver to yield a new concreteinput.
theconcrete execution andthe symbolicanalysis alternate until a desired level of structural coverage is reached.
thekeyfactorthataffectstheperformanceofconcolictestingis theheuristicsusedtoselectthenextexecutionpath.whilethereare simpleapproachessuchasrandomsearchanddepth firstsearch morecarefullydesignedheuristicscanachievebettercoverage .
automatedgenerationofsearchheuristicsforconcolictestingis an active area of research .
.
comparison with related work we briefly summarise the similarities and differences between our concolic testing method named deepconcolic and other existing coverage driven dnn testing methods deepxplore deeptest deepcover anddeepgauge .thedetails are presented in table where nc ssc and nbc are short for neuroncoverage sscoverage andneuronboundarycoverage respectively.
in addition to the concolic nature of deepconcolic we observe the following differences.
deepconcolic is generic and is able to take coverage requirementsasinput the othermethodsare adhoc andare tailored to specific requirements.
deepxplorerequiresasetofdnnstoexploremultiplegradient directions.
the other methods including deepconcolic need a single dnn only.
in contrast to the other methods deepconcolic can achieve good coverage by starting from a single input the other methods need a non trivial set of inputs.
untilnow thereisnoconclusiononthebestdistancemetric.
deepconcolic can be parameterized with a desired norm distance metric .
moreover deepconcolicfeaturesacleanseparationbetweenthe generationoftestinputsandthetestoracle.thisisagoodfitfor authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
concolic testing for deep neural networks ase september montpellier france table comparison with different coverage driven dnn testing methods deepconcolic deepxplore deeptest deepcover deepgauge coverage criteria nc ssc nbc etc.
nc nc mc dc nbc etc.
test generation concolic dual optimisation greedy search symbolic execution gradient descent methods dnn inputs single multiple single single single image inputs single multiple multiple multiple multiple multiple distance metric l andl0 norm l1 norm jaccard distance l norm l norm traditionaltestcasegeneration.theothermethodsusetheoracle as part of their objectives to guide the generation of test inputs.
deep neural networks a feedforward and deep neural network or dnn is a tuple n l t such that l lk k ... k is a set of layers t l lis a set of connections between layers and k k ... k isasetof activationfunctions.eachlayer lkconsistsof skneurons andthe l thneuronoflayer kisdenotedby nk l.w euse vk lto denote the value of nk l. values of neurons in hidden layers with k k need to pass through a rectified linear unit relu .forconvenience weexplicitlydenotetheactivation value before the relu as uk lsuch that vk l relu uk l braceleftbigg uk lifuk l otherwise relu isthe mostpopular activationfunction for neuralnetworks.
exceptforinputs everyneuronisconnectedtoneuronsinthe precedinglayerbypre definedweightssuchthat k k l sk uk l summationdisplay.
h sk wk h l vk h bk l wherewk h lis the pre trained weight for the connection betweennk h i.e.
theh th neuron of layer k andnk l i.e.
the l th neuron of layer k andbk lis thebias.
finally for any input the neural network assigns a label that is theindexoftheneuronoftheoutputlayerthathasthelargest value i.e.
label argmax1 l sk vk l .
due to the existence of relu the neural network is a highly non linearfunction.
inthis paper we usevariable xtorange over all possible inputs in the input domain dl1and uset t1 t2 ...to denote concrete inputs.
given a particular input t we say that the dnnnis instantiated and we use n to denote this instance of the network.
given a network instance n the activation values of eachneuron nk lofthenetworkbeforeandafterreluare denoted as u k landv k l respectively and the final classificationlabelis label .wewrite u kandv kfor k sktodenotethevectorsofactivationsforneurons in layerk.
when the input is given the activation or deactivation of each relu operator in the dnn is determined.
we remark that while for simplicity the definition focuses on dnns with fully connected and convolutional layers as shownin the experiments section our method also applies to other popular layers e.g.
maxpooling used in state of the art dnns.
test coverage for dnns .
activation patterns asoftwareprogramhasasetofconcreteexecutionpaths.similarly a dnn has a set of linear behaviours called activation patterns .
definition .
activation pattern .
given a network nand an inputt the activation pattern of n is a function ap that maps the set of hidden neurons to true false .
we write ap for ap ifnis clear from the context.
for an activation pattern ap w euseap k itodenotewhetherthereluoperatorofthe neuronnk iis activated or not.
formally ap k l false u k l v k l ap k l true u k l v k l intuitively ap k l trueif the relu of the neuron nk lis activated and ap k l falseotherwise.
givenadnninstance n eachreluoperator sbehaviour i.e.
eachap k l isfixedandthisresultsintheparticularactivation patternap whichcanbeencodedbyusingalinearprogramming lp model .
computing a test suite that covers all activation patterns of a dnn is intractable owing to the large number of neurons in pratically relevant dnns.
therefore we identify a subset of the activation patterns according to certain coverage criteria and then generate test inputs that cover these activation patterns.
.
formalizing test coverage criteria we use a specific fragment of quantified linear arithmetic over rationals qlar to express the coverage requirements on the test suite for a given dnn.
this enables us to give a single test input generation algorithm section fora varietyof coverage criteria.
we denote the set of formulas in our fragment by dr. definition .
.
given a network n we write iv x x1 x2 ... for a set of variables that range over the all inputs dl1of the network.
we define v u k l v k l k k l sk x iv tobeasetofvariablesthatrangeovertherationals.we fix the following syntax for dr formulas r qx.e qx1 x2.e e a triangleleft0 e e e e1 ... em triangleleftq a w c w p a a a a whereq w v c p r q n triangleleft andx x1 x2 iv.
we callra coverage requirement ea boolean authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. sun m. wu w. ruan x. huang m. kwiatkowska d. kroening formula and aan arithmetic formula.
we call the logic dr if the negation operator is not allowed.
we use rto denote a set of coverage requirement formulas.
theformula x.eexpressesthatthereexistsaninput xsuchthat eis true while x.eexpresses that eis true for all inputs x. the formulas x1 x2.eand x1 x2.ehave similar meaning except that theyquantifyovertwoinputs x1andx2.thebooleanexpression e1 ... em triangleleftqis true if the number of true boolean expressions intheset e1 ... em isinrelation triangleleftwithq.theotheroperators in boolean and arithmetic formulas have their standard meaning.
although vdoesnotincludevariablestospecifyanactivation patternap we may write ap k l ap k landap k l nequalap k l to require that x1andx2have respectively the same and different activation behaviours on neuron nk l. these conditions can be expressedinthesyntaxaboveusingtheexpressionsinequation .
moreover somenorm baseddistancesbetweentwoinputscanbe expressed using our syntax.
for example we can use the set of constraints x1 i x2 i q x2 i x1 i q i ... s1 to express x1 x2 q i.e.
we can constrain the chebyshev distancel between two inputs x1andx2 wherex i is thei th dimension of the input vector x. semantics.
we define the satisfiability of a coverage requirement r by a test suite t. definition .
.
given a set tof test inputs and a coverage requirement r the satisfiability relation t ris defined as follows.
t x.eif there exists some test t tsuch that t e wheree denotestheexpression einwhich the occurrences of xare replaced by t. t x1 x2.eif there exist two tests t1 t2 tsuch that t e the cases for formulas are similar.
for the evaluation of boolean expression eover an input t we have t a triangleleft0i fa triangleleft0 t e1 e2ift e1andt e2 t eif nott e t e1 ... em triangleleftqif ei t ei i ... m triangleleftq for the evaluation of arithmetic expression aover an input t u k landv k lderive their values from the activation patters of the dnn for test t andc u k landc v k l have the standard meaning where cis a coefficient p a1 a2 anda1 a2have the standard semantics.
notethat tisfinite.itistrivialtoextendthedefinitionofthe satisfaction relation to an infinite subspace of inputs.
complexity.
givenanetwork n adrrequirementformula r anda testsuite t checking t rcanbedoneintimethatispolynomial in the size of t. determining whether there exists a test suite t witht ris np complete.
.
test coverage metrics now we can define test coverage criteria by providing a set of requirements on the test suite.
the coverage metric is defined inthestandardwayasthepercentageofthetestrequirementsthat are satisfied by the test cases in the test suite t. definition .
coverage metric .
given a network n a setrof test coverage requirements expressed as dr formulas and a test suitet the test coverage metric m r t is as follows m r t r r t r r the coverage is used as a proxy metric for the confidence in the safety of the dnn under test.
specific coverage requirements inthissection wegivedr formulasforseveralimportantcoverage criteriafordnns includinglipschitzcontinuity andtestcoveragecriteriafromtheliterature .thecriteria weconsiderhavesyntacticalsimilaritywithstructuraltestcoverage criteria in conventional software testing.
lipschitz continuity is semantic specifictodnns andhasbeenshowntobecloselyrelated tothetheoreticalunderstandingofconvolutionaldnns and therobustnessofbothdnns andgenerativeadversarial networks .
these criteria have been studied in the literature using a variety of formalisms and approaches.
eachtestcoveragecriteriongivesrisetoasetoftestcoverage requirements.inthefollowing wediscussthethreecoveragecriteria from respectively.
we use t1 t2 qto denote thedistancebetweentwoinputs t1andt2withrespecttoagiven distance metric q. the metric qcan be e.g.
a norm based metricsuch asthe l0 norm the hammingdistance the l2 norm theeuclideandistance or the l norm thechebyshevdistance or a structural similarity distance such as ssim .
in the following we fix a distance metric and simply write t1 t2 .
section10 elaborates on the particular metrics we use for our experiments.
wemayconsiderrequirementsforasetofinputsubspaces.given arealnumber b wecangenerateafiniteset s dl1 b ofsubspaces ofdl1such that for all inputs x1 x2 dl1 i f x1 x2 b then there exists a subspace x s dl1 b such that x1 x2 x. the subspaces can be overlapping.
usually every subspace x s dl1 b canberepresentedwithaboxconstraint e.g.
x s1 and therefore t xcan be expressed with a boolean expression as follows.s1 logicalanddisplay.
i 1x i u x i l .
lipschitz continuity in lipschitz continuity has been shown to hold for a large class of dnns including dnns for image classification.
definition .
lipschitz continuity .
a network nis said to be lipschitzcontinuous ifthereexistsarealconstant c 0suchthat for allx1 x2 dl1 v v c x1 x2 recall that v 1denotes the vector of activation values of the neuronsin theinput layer.the value ciscalled the lipschitzconstant authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
concolic testing for deep neural networks ase september montpellier france andthesmallestsuch ciscalledthe bestlipschitzconstant denoted ascbest.
since the computation of cbestis an np hard problem and a smallerccan significantly improve the performance of verification algorithms it is interesting to determine whether a givencis a lipschitz constant either for the entire input space dl1orforsomesubspace.testingforlipschitzcontinuitycanbe guided using the following requirements.
definition5.
lipschitzcoverage .
givenareal c 0andanintegerb the set rlip b c of requirements for lipschitz coverage is x1 x2.
v v c x1 x2 x1 x2 x x s dl1 b where the s dl1 b are given input subspaces.
intuitively foreach x s dl1 b thisrequirementexpresses theexistenceoftwoinputs x1andx2thatrefutethat cisalipschitz constant for n. it is typically impossible to obtain full lipschitz coverage becausetheremayexistinconsistent r rlip b c .thus the goal for a test case generation algorithm is to produce a test suitetthat satisfies the criterion as much as possible.
.
neuron coverage neuron coverage nc is an adaptation of statement coverage in conventional software testing to dnns.
it is defined as follows.
definition .
.
neuron coverage for a dnn nrequires a test suitetsuchthat foranyhiddenneuron nk i thereexiststestcase t tsuch that ap k i true.
thisisformalisedwiththefollowingrequirements rnc each ofwhichexpressesthatthereisatestwithaninput xthatactivates the neuron nk i i.e.
ap k i true.
definition5.
ncrequirements .
theset rncofcoveragerequirements for neuron coverage is x.ap k i true k k i sk .
modified condition decision mc dc coverage in afamilyoffourtestcriteriaisproposed inspiredbymc dc coverage in conventional software testing.
we will restrict the discussion here to sign sign coverage ssc .
according to each neuron nk jcan be seen as a decisionwhere the neurons in the previous layer i.e.
the k th layer are conditions that define its activation value as in equation .
adapting mc dc to dnns we mustshowthatallconditionneuronscandeterminetheoutcome of the decision neuron independently.
in the case of ssc coverage we say that the value of a decision or condition neuron changes if the sign of its activation function changes.
consequently the requirements for ssc coverage are defined by the following set.
definition5.
sscrequirements .
forscccoverage wefirstdefinearequirement rssc forapairofneurons nk i nk j x1 x2.ap k i nequalap k i ap k j nequalap k j logicalandtext.
l sk l nequaliap k l ap k l and we get rssc uniondisplay.
k k i sk j sk 1rssc nk i nk j that is for each pair nk i nk j of neurons in two adjacent layerskandk we need two inputs x1andx2such that the signchangeof nk iindependentlyaffectsthesignchangeof nk j. other neurons at layer kare required to maintain their signs betweenx1andx2toensurethatthechangeisindependent.theidea of ss coverage and all other criteriain is to ensure that not onlytheexistenceofafeatureneedstobetestedbutalsotheeffects of less complex features on a more complex feature must be tested.
.
neuron boundary coverage neuronboundarycoverage nbc aimsatcoveringneuron activationvaluesthatexceedagivenbound.itcanbeformulated as follows.
definition .
neuron boundary coverage requirements .
given two sets of bounds h hk i k k i sk andl lk i k k i sk the requirements rnbc h l are x.u k i hk i x.u k i lk i k k i sk wherehk iandlk iaretheupperandlowerboundsontheactivation value of a neuron nk i. overview of our approach this section gives an overview of our method for generating a test suite for a given dnn.
our method alternates between concreteevaluation of the activation patterns of the dnn and symbolic generation of new inputs.
the pseudocode for our method is given as algorithm .
it is visualised in figure .
t0 seed input tr coverage requirements a heuristic r concrete executiont rnew inputt prime oracle adversarial examplesalgorithm top rankedsymbolic analysis figure overview of our concolic testing method algorithm 1takes as inputs a dnn n an input t0for the dnn aheuristic andaset rofcoveragerequirements andproducesa testsuite tasoutput.thetestsuite tinitiallyonlycontainsthe given test input t0.
the algorithm removes a requirement r r fromronce it is satisfied by t i.e.
t r. thefunction requirement evaluation line7 whosedetailsare given in section looks for a pair t r 2of input and requirement that accordingtoourconcreteevaluation isthemostpromising 2forsomerequirements wemightreturntwoinputs t1andt2.here forsimplicity we describethecaseforasingleinput.thegeneralisationtotwoinputsisstraightforward.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. sun m. wu w. ruan x. huang m. kwiatkowska d. kroening algorithm concolic testing for dnns input n r t0 output t t t0 andf t t0 while r s nequal do foreachr rdo ift rthenr r r while truedo t r requirement evaluation t r t prime symbolic analysis t r ifvalidity check t prime truethen t t t prime break else ifcost exceeded then f f r break returnt candidateforanewtestcase t primethatsatisfiestherequirement r.the heuristic is a transformation function that maps a formula rwith operator to an optimisation problem.
this step relies on concrete execution.
afterobtaining t r symbolic analysis line8 whosedetails are in section is applied to obtain a new concrete input t prime.
then afunction validity check line9 whosedetailsaregiveninsection9 is applied to check whether the new input is valid or not.
if so the test is added to the test suite.
otherwise ranking and symbolicinputgenerationarerepeateduntilagivencomputational cost is exceeded after which test generation for the requirement is deemed to have failed.
this is recorded in the set f. thealgorithmterminateswheneitheralltestrequirementshave been satisfied i.e.
r or no further requirement in rcan be satisfied i.e.
f r. it then returns the current test suite t. finally as illustrated in figure the test suite tgenerated by algorithm is passed to an oracle in order to evaluate the robustness of the dnn.
the details of the oracle are in section .
ranking coverage requirements thissectionpresentsourapproachforline7ofalgorithm .given asetofrequirements rthathavenotyetbeensatisfied aheuristic and the current set tof test inputs the goal is to select a concrete inputt ttogether with a requirement r r both of which will be used later in a symbolic approach to compute the next concrete inputt prime to be given insection .the selectionof tandris done by means of a series of concrete executions.
the general idea is as follows.
for all requirements r r we transform rinto r by utilising operators argoptforopt max min that will be evaluated by concretely executing tests int.a srmay contain more than one requirement we return the pair t r such that r argmaxr val t r r r .
note that when evaluating argoptformulas e.g.
argmin xa e if an input t tis returned we may need the value minxa e as well.
we use val t r to denote such a value for the returned inputtand the requirement formula r. theformula r isanoptimisationobjectivetogetherwithaset ofconstraints.wewillgiveseveralexampleslaterinsection .
.in the following we extend the semantics in definition .3to work with formulas with argoptoperators for opt max min includingargoptxa eandargoptx1 x2a e. intuitively argmax xa e argmin xa e resp.
determinestheinput xamongthosesatisfying the boolean formula ethat maximises minimises the value of the arithmetic formula a. formally theevaluationof argmin xa eontreturnsaninput t t such that t e and for all t prime tsuch that t e we havea a .
the evaluation of t argmin x1 x2a eontreturns two inputst1 t1 tsuch that t e and for allt prime t prime tsuch that t e x1 mapsto t prime x2 mapsto t prime we havea a x1 mapsto t prime x2 mapsto t prime .
the cases for argmaxformulas are similar to those for argmin b y replacing with .
similarly to definition .
the semantics is forasettoftestcasesandwecanadaptittoacontinuousinput subspace x dl1.
.
heuristics we present the heuristics we use the coverage requirements discussed in section .
we remark that since is a heuristic there exist alternatives.
the following definitions work well in our experiments.
.
.
lipschitz continuity.
whenalipschitzrequirement rasin equation is not satisfied by t we transform it into r as follows argmaxx1 x2.
v v c x1 x2 x1 x2 x i.e.
the aim is to find the best t1andt2intto make v v c t1 t2 as large as possible.
as described we also need to compute val t1 t2 r v v c t1 t2 .
.
.
neuron cover.
whenarequirement rasinequation is not satisfied by t we transform it into the following requirement r argmaxxck uk i true weobtaintheinput t tthathasthemaximalvaluefor ck uk i .
the coefficient ckis a per layer constant.
it motivated by the followingobservation.withthepropagationofsignalsinthednn activation values at each layer can be of different magnitudes.
for example if the minimum activation value of neurons at layer k andk 1ar e 10and respectively thenevenwhenaneuron u k i u k j we may still regard nk jas being closer to be activated than uk iis.
consequently we define a layerfactor ckforeachlayerthatnormalisestheaverageactivation valuationsofneuronsatdifferentlayersintothesamemagnitude level.
it is estimated by sampling a sufficiently large input dataset.
.
.
ss coverage.
insscoverage givenadecisionneuron nk j the concrete evaluation aims to select one of its condition neurons nk iatlayerksuchthatthetestinputthatisgeneratednegatesthe signsofnk iandnk jwhiletheremainderof nk j scondition authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
concolic testing for deep neural networks ase september montpellier france neurons preserve their respective signs.
this is achieved by the following r argmaxx ck u k i true intuitively given the decision neuron nk j equation selects the condition that is closest to the change of activation sign i.e.
yields the smallest u k i .
.
.
neuron boundary coverage.
wetransformtherequirement r inequation intothefollowing r whenitisnotsatisfiedby t it selects the neuron that is closest to either the higher or lower boundary.
argmax xck u k i hk i true argmax xck lk i u k i true symbolic generation of new concrete inputs this section presents our approach for line of algorithm .
that is given a concrete input tand a requirement r w en e e dt ofi n d the next concrete input t primeby symbolic analysis.
this new t primewill be added into the test suite line of algorithm .
the symbolic analysis techniques to be considered include the linear programmingin globaloptimisationforthe l0normin andanew optimisationalgorithmthatwillbeintroducedbelow.weregard optimisationalgorithmsassymbolicanalysismethodsbecause similarly to constraint solving methods they work with a set of test cases in a single run.
to simplify the presentation the following description may for each algorithm focus on some specific coverage requirements but we remark that all algorithms can work with all the requirements given in section .
.
symbolic analysis using linear programming as explained in section given an input x the dnn instance n maps to an activation pattern ap that can be modeled using linear programming lp .
in particular the following linear constraints yield a set of inputs that exhibit the same relu behaviour as x uk i summationdisplay.
j sk wk j i vk j bk i k i uk i uk i vk i ap k i true k u k i vk i ap k i false k continuous variables in the lp model are emphasized in bold.
theactivationvalueofeachneuronisencodedbythelinear constraintin whichisasymbolicversionofequation that calculates a neuron s activation value.
givenaparticularinput x theactivationpattern definition .
ap is known ap k iis either trueorfalse which indicateswhetherthereluisactivatedornotfortheneuronnk i. following and the definition of relu in for everyneuron nk i thelinearconstraintsin encoderelu activation when ap k i true or deactivation when ap k i false .thelinearmodel denotedas c givenby and represents an input set that results in the same activation pattern as encoded.
consequently thesymbolicanalysisforfindinganewinput t primefrom a pair t r of input and requirement is equivalent to finding a new activationpattern.
notethat tomakesurethattheobtainedtestcase ismeaningful anobjective isaddedto thelp modelthatminimizes the distance between tandt prime.thus the use of lprequires that the distance metric is linear.
for instance this applies to the l norm in but not to the l2 norm.
.
.
neuron coverage.
thesymbolicanalysisofneuroncoverage takestheinputtestcase tandrequirement rontheactivationof neuronnk i andreturnsanewtest t primesuchthatthetestrequirement is satisfied by the network instance n .
we have the activation patternap of the given n and can build up a new activation patternap primesuch that ap prime k i ap k i k1 k logicalanddisplay.
i1 sk1ap prime k1 i1 ap k1 i1 this activation pattern specifies the following conditions.
nk i s activation sign is negated this encodes the goal to activatenk i. inthenewactivationpattern ap prime theneuronsbeforelayer k preservetheiractivationsignsasin ap .thoughtheremay existmultipleactivationpatternsthatmake nk iactivated for the use of lp modeling one particular combination of activation signs must be pre determined.
other neurons are irrelevant as the sign of nk iis only affected by the activation values of those neurons in previous layers.
finally thenewactivationpattern ap primedefinedin isencoded by the lp model cusing and and if there exists a feasible solution then the new test input t prime which satisfies the requirementr can be extracted from that solution.
.
.
ss coverage.
tosatisfyansscoveragerequirement r w e needtofindanewtestcasesuchthat withrespecttotheinput t the activation signs of nk jandnk iare negated while other signs of other neurons at layer kare equal to those for input t. to achieve this the following activation pattern ap primeis constructed.
ap prime k i ap k i ap prime k j ap k j k1 k logicalandtext.
i1 sk1ap prime k1 i1 ap k1 i1 .
.
neuron boundary coverage.
in case of the neuron boundary coverage thesymbolicanalysisaimstofindaninput t primesuchthat the activation value of neuron nk iexceeds either its higher bound hk ior its lower bound lk i. to achieve this while preserving the dnn activation pattern ap we add one of the following constraints to the lp program.
ifu k i hk i lk i u k i uk i hk i otherwise uk i lk i. authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. sun m. wu w. ruan x. huang m. kwiatkowska d. kroening .
symbolic analysis using global optimisation the symbolic analysis for finding a new input can also be implemented by solving the global optimisation problem in .
that is by specifying the test requirement as an optimisation objective we applyglobaloptimisationtocomputeatestcasethatsatisfiesthe test coverage requirement.
forneuron coverage theobjectiveis tofinda t primesuchthat the specified neuron nk ihasap k i true.
incase ofsscoverage giventheneuronpair nk i nk j and the original input t the optimisation objective becomes ap k i nequalap k i ap k j nequal ap k j logicalandtext.
i prime nequaliap k i prime ap k i regarding the neuron boundary coverage depending on whether the higher bound or lower bound for the activation ofnk iis considered the objective of finding a new input t prime is eitheru k i hk ioru k i lk i. readers are referred to for the details of the algorithm.
.
lipschitz test case generation given a coverage requirement as in equation for a subspace x we lett0 rnbe the representative point of the subspace xto whicht1andt2belong.
the optimisation problem is to generate two inputs t1andt2such that v v d1 c t1 t2 d1 s.t.
t1 t0 d2 t2 t0 d2 where d1and d2denotenormmetricssuchasthe l0 norm l2 normor l norm and istheradiusofanormball forthe l1 andl2 norm orthesizeofahypercube forthe l norm centered ont0.
the constant is a hyper parameter of the algorithm.
theaboveproblemcanbeefficientlysolvedbyanovel alternating compasssearch scheme.specifically wealternatebetweensolving thefollowingtwooptimisationproblemsthroughrelaxation i.e.
maximizing the lower bound of the original lipschitz constant insteadofdirectlymaximizingthelipschitzconstantitself.todoso wereformulatetheoriginalnon linearproportionaloptimisation asalinearproblemwhenbothnormmetrics d1and d2 are thel norm.
.
.
stage one.
we solve mint1f t1 t0 v v d1 s.t.
t1 t0 d2 the objective above enables the algorithm to search for an optimal t1in the space of a norm ball or hypercube centered on t0with radius maximisingthenormdistanceof v 1andv .the constraint implies that sup t1 t0 d2 t1 t0 d2 .
thus a smallerf t1 t0 yields a larger lipschitz constant considering that lip t1 t0 f t1 t0 t1 t0 d2 f t1 t0 i.e.
f t1 t0 isthelowerboundof lip t1 t0 .therefore thesearchforatrace that minimises f t1 t0 increases the lipschitz constant.
tosolvetheproblemaboveweusethe compasssearchmethod whichis efficient derivative free andguaranteed toprovide firstorderglobalconvergence.becauseweaimtofindaninputpairthatrefutes the given lipschitz constant cinstead of finding the largest possiblelipschitzconstant alongeachiteration whenweget t1 wecheckwhether lip t1 t0 c.ifitholds wefindaninputpair t1andt0thatsatisfiesthetestrequirement otherwise wecontinue the compass search until convergence or a satisfiable input pair is generated.
if equation is convergent and we can find an optimalt1as t argmint1f t1 t0 s.t.
t1 t0 d2 butwestillcannotfindasatisfiableinputpair weperformthestage two optimisation.
.
.
stage two.
we solve mint2f t t2 v v t d1 s.t.
t2 t0 d2 similarly weusederivative freecompass search tosolvetheabove problemandcheckwhether lip t t2 choldsateachiterative optimisation trace t2.
if it holds we return the image pair t 1and t2that satisfies the test requirement otherwise we continue the optimisation until convergence or a satisfiable input pair is generated.ifequation isconvergentat t andwestillcannotfind suchainputpair wemodifytheobjectivefunctionagainbyletting t t 2inequation andcontinuethesearchandsatisfiability checking procedure.
.
.
stage three.
ifthefunction lip t t failstomakeprogress instagetwo wetreatthewholesearchprocedureasconvergent and have failed to find an input pair that can refute the givenlipschitz constant c. in this case we return the best input pair we found so far i.e.
t 1andt and the largest lipschitz constant lip t t2 observed.
note that the returned constant is smaller thanc.
insummary theproposedmethodisanalternatingoptimisation schemebasedoncompasssearch.basically westartfromthegiven t0to search for an image t1in a norm ball or hypercube where the optimisation trajectory on the norm ball space is denoted as s t0 t0 such that lip t0 t1 c this step is symbolic execution if we cannot find it we modify the optimisation objective functionbyreplacing t0witht thebestconcreteinputfoundin thisoptimisationrun toinitiateanotheroptimisationtrajectoryon the space i.e.
s t t0 .
this process is repeated until we have gradually covered the entire space s t0 of the norm ball.
test oracle weprovidedetailsaboutthevaliditycheckingperformedforthe generatedtestinputs line9ofalgorithm andhowthetestsuite is finally used to quantify the safety of the dnn.
definition9.
validtestinput .
wearegivenaset oofinputsfor which we assume to have a correct classification e.g.
the training dataset .
given a real number b a test input t prime tis said to be validif t o t t prime b. intuitively atestcase tisvalidifitisclosetosomeoftheinputs for which we have a classification.
given a test input t prime t w e authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
concolic testing for deep neural networks ase september montpellier france figure2 adversarialimages with l normformnist top row and l0 norm for cifar bottom row generated by deepconcolic and deepxplore the latter with image constraints light occlusion and blackout .
writeo t prime fortheinput t othathasthesmallestdistanceto t prime among all inputs in o. to quantify the quality of the dnn using a test suite t w eu s e the following robustness criterion.
definition .
robustness oracle .
given a set oof classified inputs a test case t primepasses the robustness oracle if argmax jv k j argmax jv k j wheneverweidentifyatestinput t primethatfailstopassthisoracle then it serves as evidence that the dnn lacks robustness.
experimental results wehaveimplementedtheconcolictestingapproachpresentedin this paper in a tool we have named deepconcolic3.
we compare it with other tools for testing dnns.
the experiments are run on a machinewith24coreintel r xeon r cpue5 2620v3and2.4ghz and 125gb memory.
we use a timeout of 12h.
all coverage results are averaged over runs or more.
.
comparison with deepxplore we now compare deepconcolic and deepxplore on dnns obtained from the mnist and cifar datasets.
we remark thatdeepxplore has been applied to further datasets.
for each tool we start neuron cover testing from a randomly sampledimageinput.notethat sincedeepxplorerequiresmore than one dnn we designate our trained dnn as the target model andutilisetheothertwodefaultmodelsprovidedbydeepxplore.
table2givestheneuroncoverageobtainedbythetwotools.we observe that deepconcolic yields much higher neuron coveragethan deepxplore in any of its three modes of operation light occlusion and blackout .ontheotherhand deepxploreismuch faster and terminates in seconds.
table neuron coverage of deepconcolic and deepxplore deepconcolic deepxplore l norml0 norm light occlusion blackout mnist .
.
.
.
.
cifar .
.
.
.
.
3the implementation and all data in this section are available online at .
.
.
.
.
mnist cifar 10coveragenc ssc nbc a l norm .
.
.
.
.
mnist cifar 10coveragenc nbc b l0 norm figure coverage results for different criteria figure2presentsseveraladversarialexamplesfoundbydeepconcolic with l norm and l0 norm and deepxplore.
although deepconcolic does not impose particular domain specific constraints on the original image as deepxplore does concolic testing generatesimagesthatresemble humanperception .forexample based on the l norm it produces adversarial examples figure top row that gradually reverse the black and white colours.
for thel0 norm deepconcolic generates adversarial examples similar to those of deepxplore under the blackout constraint which is essentially pixel manipulation.
.
results for nc scc and nbc wegivetheresultsobtainedwithdeepconcolicusingthecoverage criterianc ssc andnbc.deepconcolicstartsnctestingwithone singleseedinput.forsscandnbc toimprovetheperformance aninitialsetof1000imagesaresampled.furthermore weonlytest a subset of the neurons for ssc and nbc.
a distance upper bound of0.
l norm and100pixels l0 norm issetupforcollecting adversarial examples.
the full coverage report including the average coverage and standard derivation is given in figure .
table3contains the adversarial example results.
we have observed that the overhead for the symbolicanalysis withglobal optimisation section .
i st oo high.
thus the ssc result with l0 norm is excluded.
overall deepconcolic achieves high coverage and using the robustnesscheck definition .
detectsasignificantnumberof adversarial examples.
however coverage of corner case activation values i.e.
nbc is limited.
concolic testing is able to find adversarial examples with the minimumpossible distance thatis .0039forthe l norm and pixel for the l0norm.
figure 4gives the average distance of adversarialexamples fromonedeepconcolicrun .remarkably for the same network the number of adversarial examples found with nc can vary substantially when the distance metric is changed.
thisobservationsuggeststhat whendesigningcoveragecriteria for dnns they need to be examined using a variety of distance metrics.
.
results for lipschitz constant testing this section reports experimental results for the lipschitz constant testing ondnns.wetest lipschitzconstants rangingover .
.
on50mnistimagesand50cifar 10imagesrespectively.
every image represents a subspace in dl1and thus a requirement in equation .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. sun m. wu w. ruan x. huang m. kwiatkowska d. kroening a b figure a distance of nc ssc and nbc on minist and cifar10 datasets based on l norm b distanceofncandnbconthe two datasets based on l0norm.
a b c figure5 a lipschitzconstantcoveragegeneratedby1 000randomlygeneratedtestpairs and our concolic testing method for input image on mnist dnn b lipschitzconstant coverages generated by random testing and our method for input imageson mnist dnn c lipschitz constant coverage generated by random testing and our method for input images on cifar dnn.
table adversarial examples by test criteria distance metrics and dnn models l norm l0 norm mnist cifar mnist cifar adversary minimum dist.
adversary minimum dist.
adversary minimum dist.
adversary minimum dist.
nc .
.
.
.
.
.
ssc .
.
.
.
nbc .
.
.
.
.
.
.
.
baseline method.
since this paper is the first to test lipschitz constants of dnns we compare our method with random test case generation.
for this specific test requirement given a predefined lipschitz constant c an input t0and the radius of norm ball e.g.
for l1andl2norms orhypercubespace for l norm we randomly generate two test pairs t1andt2that satisfy the space constraint i.e.
t1 t0 d2 and t2 t0 d2 and then check whether lip t1 t2 cholds.
we repeat the random generation until we find a satisfying test pair or the number of repetitionsislargerthanapredefinedthreshold.wesetsuchthreshold asnrd .
namely if we randomly generate test pairs and none of them can satisfy the lipschitz constant requirement c we treat this test as a failure and return the largest lipschitzconstantfoundandthecorrespondingtestpair otherwise we treat it as successful and return the satisfying test pair.
.
.
experimental results.
figure5 a depictsthelipschitzconstantcoveragegeneratedby1 000randomtestpairsandour concolic test generation method for image on mnist dnns.
as we can see even though we produce test pairs by random test generation the maximum lipschitz converage reaches only3.23andmostofthetestpairsareintherange .our concolic method on the other hand can cover a lipschitz rangeof where most cases lie in which is poorly covered by random test generation.
figure5 b and c comparethelipschitzconstantcoverageof test pairs from the random method and the concolic method on bothmnistandcifar 10models.ourmethodsignificantlyoutperformsrandomtestcasegeneration.wenotethatcoveringalarge lipschitzconstantrangefordnnsisachallengingproblemsince most image pairs within a certain high dimensional space canproducesmalllipschitzconstants suchas1to2 .thisexplainsthe reasonwhyrandomlygeneratedtestpairsconcentrateinarange of less than .
however for safety critical applications such as self driving cars a dnn with a large lipschitz constant essentially indicates it is more vulnerable to adversarial perturbations .
as a result a test method that can cover larger lipschitz constants provides a useful robustness indicator for a trained dnn.
we arguethat forsafetytestingofdnns theconcolictestmethodfor lipschitzconstantcoveragecancomplementexistingmethodsto achieve significantly better coverage.
conclusions inthispaper weproposethefirstconcolictestingmethodfordnns.
weimplementitinasoftwaretoolandapplythetooltoevaluatethe robustness of well known dnns.
the generation of the test inputs can be guided by a variety of coverage metrics including lipschitz continuity.
our experimental results confirm that the combination ofconcreteexecutionandsymbolicanalysisdeliversbothcoverage and automates the discovery of adversarial examples.