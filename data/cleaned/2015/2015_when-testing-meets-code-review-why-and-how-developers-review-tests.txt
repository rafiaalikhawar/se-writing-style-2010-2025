when testing meets code review why and how developers review tests davide spadini delft university of technology software improvement group delft the netherlands d.spadini sig.eumaur cio aniche delft university of technology delft the netherlands m.f.aniche tudelft.nlmargaret anne storey university of victoria victoria bc canada mstorey uvic.ca magiel bruntink software improvement group amsterdam the netherlands m.bruntink sig.eualberto bacchelli university of zurich zurich switzerland bacchelli ifi.uzh.ch abstract automated testing is considered an essential process for ensuring softwarequality.however writingandmaintaininghigh quality test code is challenging and frequently considered of secondary importance.
for production code many open source and industrial software projects employ code review a well established software qualitypractice butthequestionremainswhetherandhowcode reviewisalsousedforensuringthequalityoftestcode.theaim ofthisresearchistoanswerthisquestionandtoincreaseourunderstanding of what developers think and do when it comes to reviewing test code.
we conducted both quantitative and qualitative methods to analyze more than code reviews and interviewed developers about how they review test files.
this workresultedinanoverviewofcurrentcodereviewingpractices a set of identified obstacles limiting the review of test code and a setofissuesthatdeveloperswouldliketoseeimprovedincodereview tools.
the study reveals that reviewing test files is very different fromreviewingproductionfiles andthatthenavigationwithinthe review itself is one of the main issues developers currently face.based on our findings we propose a series of recommendations and suggestions for the design of tools and future research.
ccs concepts software and its engineering software testing and debugging keywords software testing automated testing code review gerrit permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn ... .
reference format davide spadini maur cio aniche margaret anne storey magiel bruntink andalbertobacchelli.
.whentestingmeetscodereview whyand how developers review tests.
in proceedings of icse 40th international conference on software engineering gothenburg sweden may june icse pages.
introduction automatedtestinghasbecomeanessentialprocessforimproving thequalityofsoftwaresystems .automatedtests hereafter referredtoasjust tests canhelpensurethatproductioncodeis robust undermany usageconditions andthat codemeets performance and security needs .
nevertheless writing effective testsisaschallengingaswritinggoodproductioncode.atesterhas toensurethattestresultsareaccurate thatallimportantexecutionpathsareconsidered andthattheteststhemselvesdonotintroducebottlenecksinthedevelopmentpipeline .likeproductioncode test code must also be maintained and evolved .
astestinghasbecomemorecommonplace somehaveconsidered that improving the quality of test code should help improve the qualityoftheassociatedproductioncode .unfortunately thereisevidencethattestcodeisnotalwaysofhighquality .
vazhabzadeh et al.showed that about half of the projects they studied had bugs in the test code .
most of these bugs create false alarms that can waste developer time while other bugs cause harmfuldefectsinproductioncodethatcanremainundetected.wealsoseethattestcodetendstogrowovertime leadingtobloatand technical debt .
ascodereviewhasbeenshowntoimprovethequalityofsource code in general one practice that is now common in many development projects is to use modern code review mcr to improve the quality of test code.
but how is test code reviewed?
is it reviewed as rigorously as production code or is it reviewed atall?
are there specific issues that reviewers look for in test files?
doestestcodeposedifferentreviewingchallengescomparedtothereview of production code?
do some developers use techniques for reviewing test code that could be helpful to other developers?
to address these questions and find insights about test code review we conducted a two phase study to understand how test acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden d. spadini et al.
codeisreviewed toidentifycurrentpracticesandrevealthechallenges faced during reviews and to uncover needs for tools and featuresthatcansupportthereviewoftestcode.tomotivatethe importance of our work we first investigated whether being a test changesthechancesofafiletobeaffectedbydefects.havingfound no relationship between type of file and defects then in the first phase weanalyzedmorethan300 000codereviewsrelatedtothree opensourceprojects eclipse openstackandqt thatemployextensive code review and automated testing.
in the second phase we interviewed developers from these projects and from a variety of other projects from both open source and industry to understandhowtheyreviewtestfilesandthechallengestheyface.these investigations led to the following research contributions to motivate our study we first explored whether the type of code productionortest isassociatedwithfuturedefects.our resultsshowthatthereisnoassociation whichsuggeststhattest files are no less likely to have defects in the future and should benefit from code review.
weinvestigatedhowtestcodeisreviewedandfoundempirical evidencethattestfilesarenotdiscussedasmuchasproduction files during code reviews especially when test code and production code are bundled together in the same review.
when test files are discussed the main concerns include test coverage mocking practices code maintainability and readability.
wediscoveredthatdevelopersfaceavarietyofchallengeswhen reviewing test files including dealing with a lack of testing context poor navigation support within the review unrealistic time constraints imposed by management and poor knowledge of goodreviewingandtestingpracticesbynovicedevelopers.we discuss recommendations for practitioners and educators and implications for tool designers and researchers.
we created gerritminer an open source tool that extracts code reviews from projects that use gerrit.
when performing a review gerritminer provides information regarding files comments andreviewers.wedesignedthistooltohelpuscollectadatasetof654 570codereviewsfromthreepopularopen source industry supported software systems.
gerritminer and the dataset we studied are publicly available .
background and motivation pastresearchhasshownthatbothtestcodeandproductioncode suffer from quality issues .
we were inspired by the study by vahabzadeh et al.
who showed that around half of all the projects they studied had bugs in the test code and even though the vast majority of these test bugs were false alarms they negatively affected the reliability of the entire test suite.
they also foundthatothertypesoftestbugs silenthorrors maycausetests tomissimportantbugsinproductioncode creatingafalsesense of security.
this study also highlighted how current bug detection tools are not tailored to detect test bugs thus making the role of effective test code review even more critical.
someresearchershaveexaminedmcrpracticesandoutcomes andshowedthatcodereviewcanimprovethequalityofsourcecode.
for example bacchelli et al.
interviewed microsoft developers and found that code reviews are important not only for finding defectsorimprovingcode butalsofortransferringknowledge andfor creating a bigger and more transparent picture of the entire system.
mcintosh et al.
found that both code review coverage andparticipationshareasignificantlinkwithsoftwarequality producing components with up to two and five additional post release defects respectively.
thongtanunam et al.
evaluated the impactthatcharacteristicsofmcrpracticeshaveonsoftwarequality studying mcr practices in defective and clean source code files.
dibiaseetal.
analyzedthechromiumsystemtounderstand the impact of mcr on finding security issues showing that the majority of missed security flaws relate to language specific issues.
however these studies as well as most of the current literature on contemporary code review either focus on production files only or do not explicitly differentiate production from test code.
nevertheless pastliterature has shown that test code is substantially different from production code.
for instance van deursen et al.
showedthatwhenrefactoringtestcode thereisaunique set ofcode smells distinct from thatof production code because improving test code involves additional test specific refactoring.moreover test files have their own libraries that lead to specific codingpractices forexample spadini etal.
studiedatestpractice called mocking revealing that the usage of mocks is highlydependent on the responsibility and the architectural concern of the production class.
furthermore other literature shows that tests are constantly evolvingtogetherwithproductioncode.zaidman etal.
investigated how test and production code co evolve in both open source and industrial projects and how test code needs to be adapted cleaned and refactored together with production code.
due to the substantial differences between test code and production code we hypothesize that how they should be reviewed mayalsodiffer.however eventhoughcodereviewisnowwidely adopted in both open source and industrial projects how it is conductedontestfilesisunclear.weaimtounderstandhowdevelopers reviewtestfiles whatdevelopersdiscussduringreviewsessions whattoolsorfeaturesdevelopersneedwhenreviewingtestfiles and what challenges they face.
should test files be reviewed?
eventhoughpreviousliteraturehasraisedawarenessonthepreva lenceofbugsintestfiles suchastheworkbyvahabzadeh etal.
it may well be that these type of bugs constitute such a negligible number compared to defects found in production code that investingresourcesinreviewingthemwouldnotbeadvisable.iftestcodetendstobelessaffectedbydefectsthanproductioncode pragmaticdevelopersshouldfocustheirlimitedtimeonreviewingproduction code and research efforts should support this.
tomotivateourresearchandtounderstandwhethertestcode shouldbereviewedatall weconductedapreliminaryinvestigationtoseewhethertestandproductioncodefilesareequallyassociated with defects.
to studythis we useda research methodproposed bymcintosh etal.
where theyanalyzedwhethercodereview coverage and participation had an influence on software quality.
they built a statistical model using the post release defect count as a dependentvariable and metrics highly correlatedwith defectpronenessasexplanatoryvariables.theythenevaluatedtheeffects of each variable by analyzing their importance in the model .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
when testing meets code review why and how developers review tests icse may june gothenburg sweden weappliedthesameideainourcasestudy however asouraim was to understand whether test files are less likely to have bugs thanproductionfiles weaddedthetypeoffile i.e.
eithertestor production as an additional explanatory variable.
if a difference existed wewouldexpectthevariabletobesignificantforthemodel.
ifthevariablewasnotrelevant atestfileshouldneitherincrease ordecreaseincreasethelikelihoodofdefects indicatingthattest files should be reviewed with the same care as production files assuming one also cares about defects with tests .
similar to mcintosh et al.
we used the three most common familiesofmetricsthatareknowntohavearelationshipwithdefect pronenessascontrolvariables namely productmetrics size process metrics priordefects churn cumulativechurn and humanfactors total number of authors minor authors major authors author ownership .
to determine whether a change fixed a defect our dependentvariable wesearchedversion controlcommitmessages for co occurrences of defect identifiers with keywords like bug fix defect or patch .thisapproachhasbeenusedextensivelyto determine defect fixing and defect inducing changes .
weconsideredthesamesystemsthatweusedforthemainparts ofourstudy qt eclipse andopenstack astheyperformconsiderablesoftwaretestingandtheirrepositoriesareavailableonline.
due to the size of their code repositories we analyzed a sample of sub projects from each one of them.
for qt and openstack we chosethefivesub projectsthatcontainedmorecodereviews qt qt3d qtbase qtdeclarative qtwebengine qt and cinder heat neutron nova and tempest openstack .
eclipse on the other hand does not use identifiers in commit messages.
therefore we used the dataset provided by lam et al.
for theplatformsub project.
thisdatasetincludesallthebugsreportedintheeclipsebugtracker tool together with the corresponding commit hash files changed and other useful information.
we measured dependent and independent variables during the six month period prior to a release date in a release branch.
we chose the release that gave us at least months of information to analyze.incontrasttomcintosh etal.
s work wecalculated metrics at the file level not package level to measure whether the file being test code vs. production code had any effect.
we observed that the number of buggy commits was much smallercomparedtothenumberofnon buggycommits i.e.
classes were imbalanced which would bias the statistical model.
therefore we applied smote synthetic minority over sampling technique tomakebothclassesbalanced.allrscriptsareavailable in our online appendix .
toranktheattributes weusedweka asuiteofmachinelearningsoftwarewritteninjava.wekaprovidesdifferentalgorithmsfor identifyingthemostpredictiveattributesinadataset wechose information gain attribute evaluation infogainattributeeval which has been extensively used in previous literature .
infogainattributeeval is a method that evaluates the worth of an attribute by measuring the information gain with respect to the class.itproducesavaluefrom0to1 whereahighervalueindicates a stronger influence.
the precision and recall of the resulting model were above indicatingthatitisabletocorrectlyclassifywhethermostofthe files contain defects strengthening the reliability of the results.table1 rankingoftheattributes bydecreasingimportance attribute average merit average rank churn .
.
author ownership .
.
.
.
cumulative churn .
.
.
.
total authors .
.
major authors .
.
size .
.
prior defects .
.
minor authors .
.
is test .
.
we ran the algorithm using fold cross validation.
table showstheresultsoftheimportanceofeachvariableinthemodel as evaluated by infogainattributeeval.
the variable is testwas consistentlyrankedastheleastimportantattributeinthemodel whilechurn author ownership and cumulative churn were the mostimportantattributes inthatorder forpredictingwhethera filewilllikelycontainabug.thisisinlinewithpreviousliterature.
from this preliminary analysis we found that the decision to review a file should not be based on whether the file contains productionortestcode asthishasnoassociationwithdefects.motivated bythis result we conductedour investigation ofpractices discussions and challenges when reviewing tests.
research methodology themain goalofourstudyistoincreaseourunderstandingofhow test code is reviewed.
to that end we conducted mixed methods research to address the following research questions rq1 howrigorouslyistestcodereviewed?
previousliterature hasshownthatcodechangesreviewedbymoredevelopersare less prone to future defects and that longer discussions between reviewers help find more defects and lead to better solutions .basedonourpreliminarystudy section3 that showedhowthetypeoffile testvs.production doesnotchange its chances of being prone to future defects and to investigate the amount of effort developers expend reviewing test code we measuredhowoftendeveloperscommentontestfiles thelength ofthesediscussions andhowmanyreviewerscheckatestfile before merging it.
rq2 whatdoreviewersdiscussintestcodereviews?
in line withbacchelli bird weaimed tounderstandtheconcerns thatreviewersraisewheninspectingtestfiles.bacchelli bird notedthatdevelopersdiscusspossibledefectsorcodeimprovements andsharecommentsthathelponeunderstandthecode orsimplyacknowledgewhatotherreviewershaveshared.weinvestigated if similar or new categories of outcomes emerge when reviewing test code compared with production code to gatherevidenceonthekeyaspectsoftestfilereviewsandonthe reviewers needs when working with this type of artifact.
rq3 whichpracticesdoreviewersfollowfortestfiles?
little isknownaboutdeveloperpracticeswhenreviewingtestfiles.to identifythem weintervieweddevelopersfromthe3opensource projectsanalyzedinthefirsttworqs aswellasdevelopersfrom authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden d. spadini et al.
otherprojects includingclosedprojectsinindustry .weasked themhowtheyreviewtestfilesandiftheirpracticesaredifferent tothosetheyusewhenreviewingproductionfiles.thishelped discover review patterns that may guide other reviewers and triangulate reviewers needs.
rq4 whatproblemsandchallengesdodevelopersfacewhen reviewing tests?
we elicited insights from our interviews to highlightimportantissuesthatbothresearchersandpractitioners can focus on to improve how test code is reviewed.
inthefollowingsubsections wediscussthethreedatacollection methods used in this research.
section .
describes the three open source projects we studied and section .
explains how we extracted quantitative data related to the prevalence of code reviews intestfiles.section4.3discussesthemanualcontentanalysiswe conductedon astatisticallysignificant datasampleof comments pertainingtoreviewsoftestcode.section4.4describestheinterviewprocedureusedtocollectdataaboutpractices challenges and needs of practitioners when reviewing test files.
.
project selection to investigate what the current practices in reviewing test files are weaimedatchoosingprojectsthat testtheircode intensively reviewtheircode and usegerrit amoderncodereviewtoolthat facilitatesatraceablecodereviewprocessforgit basedprojects .
thethreeprojectswestudiedinourpreliminaryanalysis discussedinsection3 matchthesecriteria eclipse openstack and qtand we continue to study these projects to answer our research questions.
moreover these projects are commonly studied in code review research .
table lists their descriptive statistics.
table subject systems details after data pre processing o f prod.
files o f test files o fc od e reviews o f reviewers o f comments eclipse 973openstack 762qt total .
data extraction and analysis to investigate how developers review test files we extracted code review data from the gerrit review databases of the systems under study.gerritexplicitlylinkscommits inaversioncontrolsystem vcs to their respective code review.
we used this link to connect commits to their relevant code review obtaining information regardingwhichfileshavebeenmodified themodifiedlines andthe number of reviewers and comments in the review.
eachreviewingerritisuniquelyidentifiedbyahashcodecalled change id.
after a patch is accepted by all the reviewers it isautomatically integrated into the vcs.
for traceability purposes thecommitmessageoftheintegratedpatchcontainsthechangeid we extracted this change id from commit messages to link patches in the vcs with the associated code review in gerrit.
to obtain code review data we created gerritminer a tool that retrieves allthe code reviews from gerrit for each project using the gerrit rest api .
the tool saves all review relateddata e.g.
change id author files comments and reviewers ina mysql database.
through gerritminer we retrieved a total of654 codereviewspertainingto thethree systems.since we wereinterestedinjustproductionandtestfiles weonlystoredcode reviewsthatchanged sourcecode files e.g.
wedidnotconsider .txt file readme json files configuration files .
after this process we were left with reviews.
table presents the statistics.
toanswerrq weselectedonlyreviewsthatcontainedlessthan 50filesandhadatleastonereviewerinvolvedwhowasdifferent thantheauthor .infact asexplainedbyrigby etal.
a codereviewshouldideallybeperformedonchangesthataresmall independent and complete a small change lets reviewers focus on theentirechangeandmaintainanoverallpictureofhowitfitsintothesystem.aswewereinterestedincodereviewswherereviewers actually examined the code closely we did not consider reviews where theauthor wasthe onlyreviewer.we alsodid not consider bots as reviewers e.g.
jenkins and sanity bots .
at the end the distribution of the number of reviewers per review excluding the author isthefollowing have1reviewer have2 have have and have more than .
tounderstandhowoftenandextensivelydiscussionsareheld duringreviewsabouttestfiles weconsideredthefollowingmetrics asproxies whichhavebeenvalidatedinpreviousliterature number of comments in the file number of files with comments numberofdifferentreviewers andthelengthofthecomments.we only considered code reviews that contained at least one comment becausewewereinterestedinunderstandingwhethertherewas anydifferencebetweenreviewdiscussionsoftestandproduction files and reviews that do not contain any discussion are not useful for this investigation.
we used the production file metrics as a baseline and separately analyzedthethreedifferentreviewscenariosbasedonwhatneededtobereviewed bothproductionandtestfiles onlyproduction files or only test files.
.
manual content analysis to answer rq we focused on the previously extracted comments section .
by practitioners reviewing test files.
to analyze the contentofthesecomments weperformedamanualanalysissimilar to bacchelli bird .
due to the size of the total number of comments weanalyzedastatisticallysignificantrandomsample.oursampleof600commentswascreatedwithaconfidence level of and error e of .
themanualanalysiswasconductedbythefirsttwoauthorsof this paper using the following process each researcher wasresponsible for coding comments.
all the sampled comments were listed in a shared spreadsheet.
the researchers workedtogethertocategorizeandexplaincomments usinga negotiatedagreement technique toachieveagreement.asagreement is negotiated on the fly there is no inter rater agreement value.
agreement was found after comments at which point the work continuedinparallel.
asastartingpoint researchersusedthe same categories described by bacchelli bird including code improvement understanding socialcommunication defect knowledgetransfer miscellaneous testing externalimpact and reviewtool.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
when testing meets code review why and how developers review tests icse may june gothenburg sweden furthermore researchersdidasecondpasstoretrievemorefine grainedinformationforeachcategory obtainingmoredetailson whatdevelopersdiscuss.
incaseofdoubt i.e.
thecategoryof a specific comment was not clear to one of the researchers the category was then analyzed by both researchers together.
.
interviews to answer rqs and guided by the results of the previous rqs we designed an interview in which the goal was to understand which practices developers apply when reviewing test files.
the interviews were conducted by the first author of this paper and were semi structured aformofinterviewoftenusedinexploratoryinvestigations to understand phenomena and seek new insights .
each interview started with general questions about code reviews withtheaimofunderstandingwhytheintervieweeperforms code reviews whether they consider it an important practice and how they perform them.
our interview protocol also contained many questions derived from the results of previous research questions.ourfullinterviewprotocolisavailableintheappendix .
we asked interviewees the following main questions what is the importance of reviewing these files?
how do you conduct reviews?
do you have specific practices?
what are the differences between reviewing test files and production files?
whatchallengesdoyoufacewhenreviewingtestfiles?what are your needs related to this activity?
during each interview the researcher summarized the answers and before finalizing the meeting these summaries were presented totheintervieweetovalidateourinterpretationoftheiropinions.
we conducted all interviews via skype.
with the participants consent the interviews were recorded and transcribed for analysis.
we analyzed the interviews by initially assigning codes t o all relevant pieces of information and then grouped these codes into higher level categories.
these categories formed the topics we discuss in our results section .
we conducted interviews each lasting between and minutes withdevelopersthatperformcodereviewsaspartoftheir dailyactivities.threeofthesedevelopersworkedontheprojects westudiedinthepreviousrqs.inaddition wehadoneparticipant from another open source project and participants from industry.
table summarizes the interviewees demographics.
.
threats to validity and limitations we describe the threats to validity and limitations to the results of our work as posed by the research methodology that we applied.
construct validity.
when building ourmodel we assume that each post releasedefecthasthesameimportance wheninrealitythis could not be the case.
we mitigate this issue analyzing only therelease branch of the systems which are more controlled than a developmentbranch toensurethatonlytheappropriatechanges will appear in the upcoming release .
thecontentanalysiswasperformedmanually thusgivingriseto potentiallysubjectivejudgement.tomitigatethisthreat weemploy the negotiated agreement technique between the first and second authors until agreement was reached after comments .table3 interviewees experience inyears andworkingcontext oss project or company idyears as developeryears as reviewerworking context p1 oss p2 eclipsep3 company ap4 qtp5 company b p6 openstack p7 company c p8 company d p9 company e p10 company fp11 .
company dp12 .
company d internalvalidity credibility.
threatsto internalvalidity concern factorswedid notconsiderthatcouldaffectthe variablesandthe relations being investigated.
in our study we interview developers from the studied software to understand how they review testfiles.everydeveloperhasaspecificwayofreviewing whichmay differ from the practices ofother practitioners.
we try to mitigate this issue by interviewing a range of developers from different open source and industry projects.
in addition their interviewees opinionsmayalsobeinfluencedbyotherfactors suchascurrent literature on mcr which could may have led them to social desirability bias or bypractices in otherprojects that theyparticipatein.tomitigatethisissue weconstantlyremindedinterviewees that we were discussing the code review practices specifically of their project.
at the end of the interview we asked them to freely talk about their ideas on code reviews in general.
generalizability transferability.
our sample contains three opensource systems which is small compared to the overall population of software systems that make use of code reviews.
we reduce this issue by considering diverse systems and by collecting opinions from other open source projects as well as from industry.
results inthissection wepresenttheresultstoourresearchquestionsthat aimed to understand how rigorously developers review tests what developers discuss with each other in their reviews of test code andwhatpractices andchallengesdevelopersuseandexperience while performing these code reviews.
rq1.
how rigorously is test code reviewed?
intable we showthe distributionof commentsin codereviews forbothproductionandtestfileswhentheyareinthesamereview and for code reviews that only contain either type.
discussionincodereviewsoftestfiles.
thenumberoftestfile reviews that contain at least one comment ranges from in reviews that combinetest and production files to48 in reviews that only look at test files .
the number of production files that authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden d. spadini et al.
table the prevalence of reviews in test files vs production files baseline code review o f files of files w comments of files wo commentsodds ratio of commentsavg number of commentsavg of reviewersavg length of comments production .
.
.
.
test .
.
only production .
.
.
.
only test .
.
.
contain at least a single comment ranges from when together withtestfiles to44 inreviewsthatonlylookatproductionfiles .
in a code review that contains both types of files the odds of aproductionfilereceivingacommentis1 .
higher thanwithtestfiles.ontheotherhand whenareviewonlycontains onetypeoffile theoddsofatestfilereceivingacommentishigher than that of a production file .
.
wealsoobservedalargenumberoffilesthatdidnotreceiveany discussion.
thenumber ofcode filesthat did notreceive atleast a single comment ranges from in reviews that only look at test files to in reviews that combine test and production files .
discussion intensity in test files.
inthecodereviewsthatcontainbothtypesoffiles productionfilesreceivedmoreindividual comments than test files .
comments per file for production .27commentsfortests .thedifferenceisstatisticallysignificant butsmall wilcoxonp value .2e cliff sdelta .
this is due to the large number of files with no comments median 0inbothtestandproduction rdquantile .thedifferenceis largerwhenweanalyzeonlyfileswithatleastasinglecomment wilcoxon p value .2e cliff s delta .
.
again numberschangewhenbothfilesarenotbundledinthe same review.
code reviews on only production files contain fewer individualcommentsonaveragethanreviewsononlytestfiles .
comments for production .
comments for tests .
the difference is statistically significant but small wilcoxon p value .2e cliff s delta .
.
production files receive longer comments than test files on average both when they are in the same review an average of .
characters per comment in a production file against .
in a test and when they are not .
against .
.
the difference is againstatisticallysignificantbutsmall wilcoxonp value .2e cliff s delta .
.
reviewers of test files.
the number of reviewers involved in reviews containing both files and only tests is slightly higher compared to reviews containing production files.
however from the wilcoxon rank sum test and the effect size we observe that the overall difference is statistically significant but small wilcoxon p value .2e cliff s delta .
.
finding .
test files are almost times less likely to be discussed during code review when reviewed together with production files.
yet the difference is small in terms of the number and length of the comments and the number of reviewers involved.
code improvement understanding social communication defect knowledge transfer misc figure the outcomes of comments in code review of test files aftermanualanalysisin600comments cl ci .
rq2.
what do reviewers discuss when reviewing test code?
infigure1 wereporttheresultsofourmanualclassificationof600 comments.whencomparedtothestudybybacchelli bird that classified production and test code together we exclude the testing category as all our comments were related to test code.
in addition we did not observe any comments related to externalimpact and reviewtool .interestingly themagnitudeofthe remaining outcomes is the same as found by bacchelli bird .
codeimprovements .thisisthemostfrequentlydiscussed topic by reviewers when inspecting test files.
this category includessuggestionstousebettercodingpractices fixtypos write better java docs and improve code readability.
interestingly thecode improvements that we found are also similar to the ones foundbybacchelli bird .yetthereviewersmostlydiscussimprovements focused on testing as opposed to generic code quality practices suchasmaintainability whicharethefocusofreviews on production code .
more specifically of the code improvement comments concern improvements to testing practices such as better mocking usageand testcohesion.
inaddition we foundthat in12 ofthe cases developerswerediscussingbetternamingfortestclasses test methods and variables.
interestingly our interviewees mentioned that naming is important when reviewing test files as p9 put it mostofthetimeicomplainofcodestyling.sometimesit sdifficult to understand the name of the variables so i often complain to change the names.
ofthecodeimprovementcomments areaboutuntestedand tested paths.
according to our interviewees this is an important concernwhenreviewingtestfiles.someexamplesofsuchreview authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
when testing meets code review why and how developers review tests icse may june gothenburg sweden discussion comments are where is the assert for the non synchronized case?
add a test for context path of .
and i wouldn t do this test.
that s an implementation detail.
another6 ofthecommentsconcernwrongassertions.aswe discuss in the following rqs developers often complain aboutreadability of the assertions namely the assertion has to be asspecific as possible to let the developer better understand whythe test failed.
as an example a developer asked to change an asserttrue equals to anassertequal in one comment.
finally we observed that of the comments concern unused or unnecessary code and of them mention code styling.
these kinds of comments are in line with those found on reviews of production code .
understanding .this category represents all the comments where reviewers ask questions to better understand the code including posing questions asking for explanations suggestions and motivating examples.
in this category we included comments such as why do you need this for?
what does this variable name mean?
and why is this class static?
.
interestingly as opposed to review comments related to code improvements the comments in this category did not reveal any differences from what we found in the test and production files analyzed in our previous work i.e.
therewerenocommentsthatwerespecificallyrelatedtotesting practices suchas assertion and mocking.this provides additional evidence on the importance of understanding when performing code reviews regardless of the types of files under review.
defectfinding .within this category we see discussion concerningtestdefectssuchaswrongasserthandling missingtests misuseoftestconventions andincorrectuseofmocks.weobserve three different categories of defects severe not severe and wrong assertions.morespecifically ofthecommentsareaboutsevere issues i.e.
tests that completely fail because of a wrong variable initialization a wrong file path orincorrect use of mocks.
on the other hand of the comments are focused on less severe issues suchasmissingtestconfigurations.finally ofthecomments concern wrong assertion handling such as assertions of wrong scenarios.interestingly as opposedtotheresults reportedbybacchelli bird who found that review comments about defects ... mostly address micro level and superficial concerns a large portion of the defects discussed in test file code reviews concern ratherhigh levelandsevereissues.ahypothesisforthisdifference may be that a good part of the severe high level defects that affect testfilesarelocalized i.e.
visiblejustlookingatthechangedlines while production files are affected by more delocalized defects that may be harder to detect by simply inspecting the changed lines.
knowledge transfer .thiscategory whichalsoemergedin previous work represents all the comments where the reviewersdirectthecommitterofthecodechangetoanexternalresource e.g.
internaldocumentationorwebsites .weobservetwodifferent typesofcommentsinthiscategory commentsthatlinktoexternal resourcesandthatcontainexamples.morespecifically ofthese commentscontainlinkstoexternaldocumentation e.g.
mockito website python documentation to other classes of the project e.g.
othertests andtootherpatches.therestofthecomments areexampleswherethereviewershowedhowtotackletheissuewith an example within the review comment itself of how s he would do it.
socialcommunication .finally thiscategory inlinewith theworkbybacchelli bird includesallthecommentsthat aresocialinnatureandnotaboutthecode examplessuchas great suggestion!
and thank you for your help .
finding .
reviewers discuss better testing practices tested and untested paths and assertions.
regarding defects half of the comments regard severe high level testing issues as opposedtoresultsreportedinpreviouswork wheremostof thecommentsonproductioncoderegardedlowlevelconcerns.
rq3.
which practices do reviewers follow for test files?
we analyze the answers obtained during our interviews with developers.
we refer to individual interviewees using p .
testdriven reviews.
regardlessofhavingtestfilesinthepatch allparticipantsagreed thatbeforedivingintothe source code they firstgetanideaofwhatthechangeisabout byreadingthecommit message or any documentation attached to the review request .p2added ilookatwhatitsaysandithinktomyself how would i implement that?
just to give a sense of what are the files that i would be expecting to be changed what are the areas of the system that i m expecting that they touch.
if the files that are attachedtothepatchlookmuchdifferentfromwhattheyexpected they immediately reject the code change .
once they understood what the change is about developers start to look at the code.
in this case we identified two different approaches somedeveloperspreferto readtestfilesfirst followed byproductionfiles whileotherdeveloperspreferto read production files first and then review tests .
whenstartingfromtests developerssaytheycanunderstand what the feature should do even before looking atits implementation .p5says itissimilartoreadinterfacesbeforeimplementations.
i start from the test files because i want to understand theapifirst.
inthisapproach developersfirstseewhatthecodeistested for then check whether the production code does only what is tested for or if it does more than what is necessary if i starttofindsomethinginproductionthatismuchdifferentofwhat i am inferring from the tests those are my first questions .
moredevelopersinsteadstartreviewingthechangeinitsproduction files first .
as p8explained i start first from the production code because i can have a sense of what should be tested.
theadvantageofsuchanapproachisthatdevelopersdo not waste time validating whether the test covers every path fora change that is wrong in first place p .
p7also said that he would prefer to start reviewing the tests but the poor quality of thetestsmakesthisnotrealistic iwouldprefertostartingwith the tests the problem is that tests are usually very bad.
and the reason is because they usually tend to test what they implemented and not what they should have implemented.
in tdd i would also start to review tests first but as you know this is not the case.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden d. spadini et al.
finding .
similarly to when writing new code when reviewingsomedevelopersprefertostartfromtests othersfrom production.
reviewers who start inspecting tests use them to determinewhattheproductioncodeshoulddoandwhether itdoes only that.reviewers whostart inspectingproduction code prefer to understand the logic of production code before validating whether its tests cover every path.
reviewerslookfordifferentproblemswhenreviewingtests.
accordingto the interviewees reviewing testfiles require different practicestothoseusedforreviewingproductionfiles thisisinline withthedifferenceswefoundinthecontentofthesubcategoriesof commentsleftfortestfilesvs.productionfilesforrq2.interviewees explain that it is especially different in terms of whatto look for.
p10saidthatwhenreviewingproductioncodetheydiscussmore aboutthedesignandcleanlinessofthecode whereasforreviewsof teststheydiscussmoreontested untestedpaths testingpractices like mocking and the complexity of the testing code.
a main concern for all the developers is understanding if all the possible paths of the production code are tested especially corner cases .
ifthemethodbeingtestedreceivestwovariables and withthesetwovariablesitcanhaveonaveragefivetotendifferent returns i ll try to see whether they cover a sufficient number of cases so that i can prove that that method won t fail.
intervieweesexplainedthattheyoftencomplainaboutmaintainability and readability ofthe test i.e.
complex or duplicatedcode if the test can be simpler or divided into two smaller tests but especially the name of the tests and the assertions .
i often complainontheassertions someassertionsarereallydifficultto understand we should try to be as specific as possible.
finding .
a main concern of reviewers is understanding whether the test covers all the paths of the production code and to ensure tests maintainability and readability.
havingthecontextualinformationaboutthetest.
aswewill discuss in the next rq a main concern for developers is the small amountofinformationprovidedinthecodereview .
indeed within a code review reviewers can only see files that are changed and only the lines that have been modified while interviewees complain that they do not have the possibility to for example automatically switch between production code and its test code or to check other related test cases that are not modified in the patch .
for this reason two developers explained that they check out thecodeunderreviewandopenitwithanothertool forexamplealocalide p .inthisway theycanhavethefullpictureofwhat ischanged andget fullsupport ofother tools iparticularly like openingthepullrequesttoknowwhathascomeinagain which classeshavebeenedited.i github iaccesstheprandopenit myide.soicanlookatthecodeasawhole notjustthefiles changed as it is in github.
another advantage of checking outthecommitwiththepatchisthatdeveloperscanseethetests running most of the time i pull the pull request to see the feature and tests running so i can have a sense of what have changedandhow.
nevertheless thispracticecanonlybeaccomplished whenthecodebaseislimitedinsize andthecodingenvironment can be easily pulled to the local machine of the reviewers.
finding .
due to the lack on test specific information withinthecodereviewtool weobservedthatdeveloperscheck out the code under review and open it in a local ide this allows them to navigate through the dependencies have a fullpictureof thecode andrunthetest code.howeverthis workaround is limited to small scale code bases.
rq4.
what problems and challenges do developers face when reviewing tests?
testcodeissubstantiallydifferentthanproductioncode.
according to our interviewees even if sometimes writing tests is simplerthanwritingproductioncode thischangeswhen reviewing tests imagine you have to test a method that doesanarithmeticsum.theproductioncodeonlyaddstwonumbers whilethetestwillhavetodoapositivetest anegativeand haveaboutfifteendifferentalternatives forthesamefunction.
accordingtoourinterviewees reviewingtestfilesbecomescomplicatedduetolackofcontext .whenreviewingatest code review tools do not allow developers to have production and test files side by side having the complete context of the test is difficult we often use variables that are not initialized in the reviewed method so often i have to go around and understand wherethevariableisinitialized.
anotherdevelopersaidthat it shardtounderstandwhichtestisactuallytestingthismethod or this path in the function.
and that when the test involves a lot of other classes it is highly coupled s he never knows whether and how the other classes are tested .
furthermore one of the main challenges experienced by our intervieweesisthatoftenreviewingatestmeansreviewingcode additions which is more complicated than reviewing just code changes .
code changes make me think why is this line changingfromthegreaterthansidetoalessthanside?whilecode additions you have to think what s going on in there and tests are almost all the time new additions.
according to developers testcodeistheoreticallywrittenonceandifitiswrittencorrectlyit will not change .
the reason is that while the implementation ofthefeaturemaychange e.g.
howitisimplemented boththe result and the tests will stay the same .
finding .
reviewingtestfilesrequiresdeveloperstohave context about not only the test but also the production file under test.in addition test filesare oftenlong andare oftennew additions which makes the review harder to do.
theaveragedeveloperbelievestestcodeislessimportant.
i willgetacallfrommymanagerifthereisabuginproductionwhile i m not going to get a call if there s a bug in the test right?
accordingtoourinterviewees developerschoosesavingtimetothe detrimentofquality.thisisduetothefactthatthereisno immediate value on having software well tested as p 7explained it is only authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
when testing meets code review why and how developers review tests icse may june gothenburg sweden good for the long term.
for a product that is customer driven it is moreimportanttoreleasethefeatureontimewithoutbugs because that is the code that will run on the client s machine .
p7 said ifiwanttogetagoodbonusbytheendoftheyeariwillmake sure that my features make it into production level code.
if instead we would start to get punished for bugs or bad code practices you will see a very very different approach you would see way morediscussionsaboutteststhanproductioncode.
interviewees affirmedthatthemainproblemisthedevelopersmindset it is the same reason as why people write bad tests testing is considered as secondary class task it is considered not but it is not.
developers see test files as less important because a bug in a test is a developer s problem while a bug in production code is aclient sproblem.asexplainedbyp developersarenotrewarded for writing good code but for delivering features the clients want.
furthermore according to our interviewees during a review sometimes they donot even lookat the testfiles their presence is enough .
as p6said sometimes you don t look at the test becauseyouseethereisthefile youknowthatthecodeis what it has to do and you trust the developer who wrote it maybe we trust too much sometimes .
finding .
developers have a limited amount of time to spendonreviewingandaredrivenbymanagementpoliciesto reviewproductioncodeinsteadoftestcodewhichisconsidered less important.
better education on software testing and reviewing.
most intervieweesagreedontheneedtoconvincedevelopersandmanagers that reviewing and testing are highly important for the software systemoverallquality .educatingdevelopersongood and bad practices and the dangers of bad testing .
i would love tosee inuniversity peopleteaching goodpractice ontesting.
furthermore peoplecomingfromuniversitytheyhavenofreak ing clue on how a code review is done.
educating on testing and reviewing how to write a good test and review it.
furthermore withthehelpofresearchers developerscouldsolve partoftheeducationproblem oneintervieweesaidthatresearch should focus more on developers needs so that tool designerscan take advantage of these needs and improve their tools p .
i think it is important to give this feedback to the people who write tools so they can provide the features that the community wants.
having someone like you in the middle collectingthisfeedbackandsendingthemtotooldevelopers this could be very helpful.
finding .
novicedevelopersandmanagersarenotawareof what is the impact of poor testing and reviewing on software quality educationsystemsshouldfixthis.moreover research shouldfocusmoreondevelopers needsandexposethemto tool makers to have an impact.
tool improvements.
partoftheinterviewwasfocusedonwhat can be improved in the current code review tools.according to our interviewees the navigation between the productionandthetestfileswithinthereviewisdifficult .
we don t have a tool to easily switch between your tests and your productioncode wehavetogobackandforth thenyouhavetolook forthesamenameandtryingtomatchthem.
asmentioned before thecontextofthereviewislimitedtothefilesattachedto the review itself and this makes it difficult to have a big picture of the change.
for example test files are usually highly coupled to severalproductionclasses however developerscannotnavigate to the dependencies of the test or other test in general without openinganewwindow .
ifwecouldclickonthedefinition ofaclassandgotoitsimplementationwouldbeamazing.that s why ipullthe prevery time andi losea lotof time it.
p12 said it sveryunproductivetoreviewingithub becauseyou firstvisualizeallthecodes andthenattheendareallthetests and it ends up being more difficult having to keep going in the browser several times.
inaddition addingfine grainedinformationaboutcodecoverage during the review is considered helpful .
more specifically whichtestscoveraspecificline whatpathsare alreadycoveredbythetestsuite andwhethertestsexercise exceptional cases .
regarding the latter p 12says i think it s harder to automate but it is to ensure that not only the happy pathsarecovered.itistoensurethatthecoverisinthehappycase in case of errors and possible variations.
a lot of people end up covering very little or too much.
tool features that are not related to test also emerged during our interviewees.
for example enabling developers to identify the importance of each file within the code review and splitting the code review among different reviewers .
finding .
review tools should provide better navigation between test and production files as well as in depth code coverage information.
discussion wediscusshowourresultsleadtorecommendationsforpractitioners and educators as well as implications for future research.
.
for practitioners and educators underlinetheimportanceofreviewingtestcode.
the results of both our quantitative and qualitative analysis indicate that most reviewersdeemreviewingtestsaslessimportantthanreviewingproductioncode.especiallywheninspectingproductionandtest files that are bundled together reviewers tend to focus more on productioncodewiththeriskofmissingbugsinthetests.however previousresearchhasshownthatbugsintestfilescanlowerthequality of the corresponding production code because a bug in thetestcanleadtoseriousissuessuchas silenthorrors or false alarms .moreover ouranalysisprovidedempiricalevidence that being a test does not change the chances of a file to have futuredefects section3 .forthisreason practitionersshouldbe instructed and keep in mind to put the same care when reviewing test or production code.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden d. spadini et al.
set aside sufficient time for reviewingtest files.
ourintervieweesagreedthatreviewingtestfilesisanon trivialtask because changesinvolvingtests aremoreoftencode additions ratherthan codemodifications and several test options must be analyzed.
this indicates that correctly reviewing test files would hardly take less time than reviewing production code.
a good team culture must be developed in which the time spent on reviewing test code is consideredasimportantasthetimespentonreviewingproduction codeandscheduledaccordingly.infact aspreviousworkalready pointed out good reviewing effectiveness is found mostly withinteamsthatvaluethetimespentoncodereview testsshould not be treated differently.educate developers on how to review test code.
many books andarticleshavebeenwrittenbypractitionersonbestpracticesfor codereview andresearcherscontinuetoconductstudies to increase our empirical understanding of code review .
nevertheless bestpractices for reviewingtest code havenot been discussednorproposed yet.ourwork as afirststep collects current bestpractices for reviewingof tests.
thesepractices show us that developers should learn how to look for possible false alarms to check that the tests will be easily understandable and maintainable and to check whether all the possible paths of the production code are tested.
novice reviewers may also consider the practice of reviewing test code before production to make sure to give it enoughtimeand tobetterunderstandthegoalsoftheproduction code under test since novices may not know them beforehand.
.
for tool designers and researchers providingcontexttoaidinreviewingoftests.
thelackofcontext when reviewing test code is a concern for many developers.
specifically developersarguethatitisimportanttounderstandand inspecttheclassesthatareundertestaswellaswhichdependencies aresimulatedbytests i.e.
mockobjects .however knowingwhich classesareexecutedbyatestnormallyrequiresdynamicallyexecut ing the code during review which is not always feasible especially when large code bases are involved .
this is an opportunity to adapt and extend existing research that determines the coverage of tests using static analysis .
moreover developers would like to beabletoeasilynavigatethroughthetestandtestedclasses future researchstudiescouldinvestigatehowtoimprovefilenavigation forcodereviewingeneral anexistingopenresearchproblem but also to better support review of tests in particular.
providing detailed code coverage information for tests.
as ourresultsshow oneofthemostimportanttasksduringthereview ofatestcodeistomakesurethetestcoversallthepossiblepaths of the productioncode.
although external tools provide code coveragesupportfordevelopers e.g.
codecov thisinformation is usually not per test method i.e.
coverage reports focus on the finalcoverageaftertheexecutionoftheentiretestsuite andnot forasingletestmethod.therefore newmethodsshouldbedevised to not only provide general information on code coverage but also provide information thatis specific toeach test method.
aneffort inthisdirection hasbeenpresentedby oosterwaal etal.
our analysis points to the need for further research in this area.
understanding how to review test code and benefits of test reviews.
our research highlights some of the current practicesusedbydeveloperswhenreviewingtestfiles suchastestdriven review review tests before production code .
nevertheless therealeffectofthesepracticesoncoderevieweffectivenessandonthe eventual test code quality is not known some practices may be beneficial other may simply waste reviewers time.
this calls forin depth empiricalexperimentstodeterminewhichpractices should be suggested for adoption by practitioners.
conclusions automatedtestingisnowadaysconsideredtobeanessentialprocess for improving the quality of software systems.
unfortunately past literature showed that test code similarly to production code canoftenbeoflowqualityandmaybepronetocontaindefects .
tomaintaina highcodequalitystandard manysoftwareprojects employcodereview butistestcodetypicallyreviewedandifso how rigorously?
in this paper we investigated whether and how developers employ code review for test files.
to that end we studiedthreeossprojects analyzingmorethan300 000reviewsand interviewing three of their developers.
in addition we interviewed another developers both from oss projects and industry obtainingmoreinsightsonhowcodereviewisconductedontests.our results provide new insights on what developers look for when reviewingtests whatpracticestheyfollow andthespecificchallenges they face.
inparticular afterhavingverifiedthatacodefilethatisatest does not make it less likely to have defects thus little justification forlowerqualityreviews weshowthatdeveloperstendtodiscusstest files significantly less than production files.
the main reportedcause is that reviewers see testing as a secondary task and they arenotawareoftheriskofpoortestingorbadreviewing.wediscoveredthatwheninspectingtestfiles reviewersoftendiscussbettertesting practices tested and untested paths and assertions.
regarding defects often reviewers discuss severe high level testing issues as opposedtoresultsreportedinpreviouswork wheremostof the comments on production code regarded low level concerns.
among the various review practices on tests we found two approacheswhenareviewinvolvestestandproductioncodetogether some developers prefer to start from tests others from production.
in the first case developers use tests to determine what the production code should do and whether it does only that on the other hand when starting from production they want to understand the logic before validating whether its tests cover every path.
as for challenges developers main problems are understanding whether the test covers all the paths of the production code ensuring maintainability and readability of the test code gaining context for the test under review and difficulty reviewing large code additions involving test code.
weproviderecommendationsforpractitionersandeducators as well as viable directions for impactful tools and future research.
wehopethattheinsightswehavediscoveredwillleadtoimproved toolsandvalidatedpracticeswhichinturnmayleadtohighercode quality overall.