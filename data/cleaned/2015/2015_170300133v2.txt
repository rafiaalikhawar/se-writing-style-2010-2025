easy over hard a case study on deep learning wei fu tim menzies com.sci.
nc state usa wfu ncsu.edu tim.menzies gmail.com abstract while deep learning is an exciting new technique the bene f its of this method need to be assessed with respect to its computational cost.
t his is particularly important for deep learning since these learners need hours to weeks to train the model.
such long training time limits the ability of a a researcher to test the stability of their conclusion via repeated runs with different random seeds and b other researchers to repeat improve or even refute that original work.
for example recently deep learning was used to f ind which questions in the stack over f low programmer discussion forum can be linked together.
t hat deep learning system took hours to execute.
we show here that applying a very simple optimizer called de to f ine tune svm it can achieve similar and sometimes be t ter results.
t he de approach terminated in minutes i.e.
times faster hours than deep learning method.
we offer these results as a cautionary tale to the so f tware analytics community and suggest that not every new innovation should be applied without critical analysis.
if researchers deploy some new and expensive process that work should be baselined against some simpler and faster alternatives.
keywords search based so f tware engineering so f tware analytics parameter tuning data analytics for so f tware engineering deep learning svm differential evolution acm reference format wei fu tim menzies.
.
easy over hard a case study on deep learning.
inproceedings of 11th joint meeting of the european so f tware engineering conference and the acm sigsoft symposium on the foundations of so f tware engineering paderborn germany september esec fse pages.
.
.
introduction t his paper extends a prior result from ase by xu et al.
herea f ter xu .
xu described a method to explore large programmer discussion forums then uncover related but separate entries.
t his is an important problem.
modern se is evolving so fast that these forums contain more relevant and recent comments on current technologies than any textbook or research article.
in their work xu predicted whether two questions posted on stack over f low are semantically linkable.
speci f ically xu de f ine a question along with its entire set of answers posted on stack over f low as a knowledge unit ku .
if two knowledge units are esec fse paderborn germany .
.
.
.
.
.
.3106256semantically related they are considered as linkable knowledge units.
in their paper they used a convolution neural network cnn a kind of deep learning method to predict whether two kus are linkable.
such cnns are highly computationally expensive o f ten requiring network composed of to layers hundreds of millions of weights and billions of connections between units .
even with advanced hardware and algorithm parallelization training deep learning models still requires hours to weeks.
for example xu report that their analysis required hours of cpu.
le used a cluster with machines cores for three days to train a deep learner.
t his paper debates what methods should be recommended to those wishing to repeat the analysis of xu.
we focus on whether using simple and faster methods can achieve the results that are currently achievable by the state of art deep learning method.
speci f ically we repeat xu s study using de differential evolution which serves as a hyper parameter optimizer to tune xu s baseline method which is a conventional machine learning algorithm support vector machine svm .
our study asks rq1 can we reproduce xu s baseline results word embedding svm ?
using such a baseline we can compare our methods to those of xu.
rq2 can de tune a standard learner such that it outperforms xu s deep learning method?
we apply differential evolution to tune svm.
in terms of precision recall and f1 score we observe that the tuned svm method outperforms cnn in most evaluation scores.
rq3 is tuning svm with de faster than xu s deep learning method?
our de method is times faster than cnn.
we offer these results as a cautionary tale to the so f tware analytics community.
while deep learning is an exciting new technique the bene f its of this method need to be carefully assessed with respect to its computational cost.
more generally if researchers deploy some new and expensive process like deep learning that work should be baselined against some simpler and faster alternatives t he rest of this paper is organized as follows.
section describes the background and related work on deep learning and parameter tuning in se.
section explains the case study problem and the proposed tuning method investigated in this study then section describes the experimental se t tings of our study including research questions data sets evaluation measures and experimental design.
section presents the results.
section discusses implications from the results and the threats to the validity of our study.
section concludes the paper and discusses the future work.
before beginning we digress to make two points.
firstly just because de svm beats deep learning in this application this does not mean de is always the superior method for all other so f tware analytics applications.
no learner works best over all problems the trick is to try several approaches and select thearxiv .00133v2 jun 2017esec fse september paderborn germany wei fu tim menzies one that works best on the local data.
given the low computational cost of de minutes vs hours des are an obvious and low cost candidate for exploring such alternatives.
secondly to enable other researchers to repeat improve or refute our results all our scripts and data are freely available online github1.
background and related work .
why explore faster so f tware analytics?
t his section argues that avoiding slow methods for so f tware analytics is an open and urgent issue.
researchers and industrial practitioners now routinely make extensive use of so f tware analytics to discover e.g.
how long it will take to integrate the new code where bugs are most likely to occur who should f ix the bug or how long it will take to develop their code .
large organizations like microso f t routinely practice data driven policy development where organizational policies are learned from an extensive analysis of large data sets collected from developers .
but the more complex the method the harder it is to apply the analysis.
fisher et al.
characterizes so f tware analytics as a work f low that distills large quantities of low value data down to smaller sets of higher value data.
due to the complexities and computational cost of se analytics the luxuries of interactivity direct manipulation and fast system response are gone .
t hey characterize modern cloud based analytics as a throwback to the 1960s batch processing mainframes where jobs are submi t ted and then analysts wait wait and wait for results with li t tle insight into what is really going on behind the scenes how long it will take or how much it is going to cost .
fisher et al.
document the issues seen by industrial data scientists one of whom remarks fast iteration is key but incompatible with the jobs are submi t ted and processed in the cloud.
it is frustrating to wait for hours only to realize you need a slight tweak to your feature set .
methods for improving the quality of modern so f tware analytics have made this issue even more serious.
t here has been continuous development of new feature selection and feature discovering techniques for so f tware analytics with the most recent ones focused on deep learning methods.
t hese are all exciting innovations with the potential to dramatically improve the quality of our so f tware analytics tools.
yet these are all cpu gpu intensive methods.
for instance learning control se t tings for learners can take days to weeks to years of cpu time .
lam et al.
needed weeks of cpu time to combine deep learning and text mining to localize buggy f iles from bug reports .
gu et al.
spent hours of gpu time to train a deep learning based method to generate api usage sequences for given natural language query .
note that the above problem is not solvable by waiting for faster cpus gpus.
we can no longer rely on moore s law to double our computational power every months.
power consumption and heat dissipation issues effect block further exponential increases to 1h t tps github.com weifoo easyoverhardcpu clock frequencies .
cloud computing environments are extensively monetized so the total f inancial cost of training models can be prohibitive particularly for long running tasks.
for example it would take years of cpu time to learn the tuning parameters of so f tware clone detectors proposed in .
much of that cpu time can be saved if there is a faster way.
.
what is deep learning?
deep learning is a branch of machine learning built on multiple layers of neural networks that a t tempt to model high level abstractions in data.
according to lecun et al.
deep learning methods are representation learning methods with multiple levels of representation obtained by composing simple but non linear modules that each transforms the representation at one level starting with the raw input into a representation at a higher slightly more abstract level.
compared to the conventional machine learning algorithms deep learning methods are very good at exploring high dimensional data.
by utilizing extensive computational power deep learning has been proven to be a very powerful method by researchers in many f ields like computer vision and natural language processing .
in convolution neural networks method won the imagenet competition which achieves half of the error rates of the best competing approaches.
a f ter that cnn became the dominant approach for almost all recognition and detection tasks in computer vision community.
cnns are designed to process the data in the form of multiple arrays e.g.
image data.
according to lecun et al.
recent cnn methods are usually a huge network composed of to layers hundreds of millions of weights and billions of connections between units.
with advanced hardware and algorithm parallelization training such model still need a few hours .
for the tasks that deal with sequential data like text and speech recurrent neural networks rnns have been shown to work well.
rnns are found to be good at predicting the next character or word given the context.
for example graves et al.
proposed to use long short term memory lstm rnns to perform speech recognition which achieves a test set error of on the benchmark testing data.
sutskever et al.
used two multiplelayered lstm rnns to translate sentences in english to french.
.
deep learning in se we study deep learning since recently it has a t tracted much attentions from researchers and practitioners in so f tware community .
t hese researchers applied deep learning techniques to solve various problems including defect prediction bug localization clone code detection malware detection api recommendation effort estimation and linkable knowledge prediction.
we f ind that this work can be divided into two categories treat deep learning as a feature extractor and then apply other machine learning algorithms to do further work .
solve problems directly with deep learning .
.
.
deep learning as pre processor.
lam et al.
proposed an approach to apply deep neural network in combination with rvsm to automatically locate the potential buggy f iles for a giveneasy over hard a case study on deep learning esec fse september paderborn germany bug report.
by comparing it to baseline methods naive bayes learn to rank buglocator lam et al.
reported .
.
.
and .
.
higher top accuracy than baseline methods respectively .
t he training time for deep neural network was reported from to minutes for projects on a computer with cores .00ghz cpu gb memory.
however the runtime information of the baseline methods was not reported.
wang et al.
applied deep belief network to automatically learn semantic features from token vectors extracted from the studied so f tware program.
a f ter applying deep belief network to generate features from so f tware code naive bayes adtree and logistic regression methods are used to evaluate the effectiveness of feature generation which is compared to the same learners using traditional static code features e.g.
mccabe metrics halstead s effort metrics and ck object oriented code mertics .
in terms of runtime wang et al.
only report time for generating semantics features with deep belief network which ranged from seconds to seconds .
however the time for training and tuning deep belief network is missing.
furthermore to compare the effectiveness of deep belief network for generating features with methods that extract traditional static code features in terms of time cost it would be favorable to include all the time spent on feature extraction including paring source code token generation and token mapping for both deep belief network and traditional methods i.e.
an end to end comparison .
choetkiertikul et al.
proposed to apply deep learning techniques to solve effort estimation problems on user story level.
speci f ically choetkiertikul et al.
proposed to leverage long short term memory lstm to learn feature vectors from the title description and comments associated with an issue report and after that regular machine learning techniques like cart random forests linear regression and case based reasoning are applied to build the effort estimation models.
experimental results show that lstm has a signi f icant improvement over the baseline method bag of words.
however no further information regarding runtime as well as experimental hardware is reported for both methods and there is no cost of this deep learning method at all.
.
.
deep learning as a problem solver.
white et al.
applied recurrent neural networks a type of deep learning techniques to address code clone detection and code suggestion.
t hey reported the average training time for projects were ranging from seconds to seconds for each epoch on a computer with two .
ghz cpus and each project required at least epochs .
speci f ically for the jdk project in their experiment it would take hours on the same computer to train the models before ge t ting prediction.
for the time cost for code suggestions authors did not mention any related information .
gu et al.
proposed a recurrent neural network rnn based method d eepapi to generate api usage sequences for a given natural language query.
compared with the baseline method swim and lucene up miner d eepapi improved the performance signi f icantly.
however that improvement came at a cost that model was trained with a nivdia k20 gpu for hours .
xu utilized neural language model and convolution neural network cnn to learn word level and document level features to predict semantically linkable knowledge units on stack over f low.in terms of performance metrics like precision recall and f1 score cnn method was evaluated much be t ter than the baseline method support vector machine svm .
however once again that performance improvement came at a cost their deep learner required hours to train cnn model on a .5ghz pc with gb ram .
yuan et al.
proposed a deep belief network based method for malware detection on android apps.
by training and testing the deep learning model with features extracted from static analysis and dynamic analysis from sampled android app they got accuracy for deep learning method and for one baseline method svm .
however they did not report any runtime comparison between the deep learning method and other classic machine learning methods.
mou et al.
proposed a tree based convolutional neural network for programming language processing in which a convolution kernel is designed over programs abstract syntax trees to capture structural information.
results show that their method achieved accuracy which is be t ter than the baseline method rbf svm on program classi f ication problem .
however mou et al.
did not discuss any runtime comparison between the proposed method and baseline methods.
.
.
issues with deep learning.
in summary deep learning is used extensively in so f tware engineering community.
a common pa t tern in that research is to report deep learning s bene f its but not its cpu gpu cost or simply show the cost without further analysis .
since deep learning techniques cost large amount of time and computational resources to train its model one might question whether the improvements from deep learning is worth the costs.
are there any simple techniques that achieve similar improvements with less resource costs?
to investigate how simple methods could improve baseline methods we select xu study as a case study.
t he reasons are as follows most deep learning paper s baseline methods in se are either not publicly available or too complex to implement .
xu de f ine their baseline methods precisely enough so others can con f idently reproduce it locally.
xu s baseline method is svm learner which is available in many machine learning toolboxes.
further it is not yet common practice for deep learning researchers in se community to share their implementations and data where a tiny difference may lead to a huge difference in the results.
even though xu do not share their cnn tool their training and testing data are available online which can be used for our proposed method.
since the same training and testing data are used for xu s cnn and our proposed method we can compare results of our method to their cnn results.
some studies do not report their runtime and experimental environment which makes it harder for us to systematically compare our results with theirs in terms of computational costs .
xu clearly report their experimental hardware and runtime which will be easier for us compare our computational costs to theirs.esec fse september paderborn germany wei fu tim menzies .
parameter tuning in se in this paper we use de as an optimizer to do parameter tuning for svm which achieves results that are competitive with deep learning.
t his section discusses related work on parameter tuning in se community.
machine learning algorithms are designed to explore the instances to learn the bias.
however most of these algorithms are controlled by parameters such as t he maximum allowed depth of decision tree built by cart t he number of trees to be built within a random forest.
adjusting these parameters is called hyperparameter optimziation.
it is a well well explored approach in other communities .
however in se such parameter optimization is not a common task as shown in the following examples .
in the f ield of defect prediction fu et al.
surveyed hundreds of highly cited so f tware engineering paper about defect prediction.
t heir observation is that most so f tware engineering researchers did not acknowledge the impact of tunings exceptions and use the off the shelf data miners.
for example elish et al.
compared support vector machines to other data miners for the purposes of defect prediction.
however the elish et al.
paper makes no mention of any svm tuning study .
more details about their survey refer to .
in the f ield of topic modeling agrawal et al.
investigated the impact of parameter tuning on latent dirichlet allocation lda .
lda is a widely used technique in so f tware engineering f ield to f ind related topics within unstructured text like topic analytics on stack over f low and source code analysis .
agrawal et al.
found that lda suffers from conclusion instability different input orderings can lead to very different results that is a result of poor choice of the lda control parameters .
yet in their survey of lda use in se they found that very few researchers out of papers explored the bene f its of parameter tuning for lda.
one troubling trend is that in the few se papers that perform tuning they do so using methods heavily deprecated in the machine learning community.
for example two se papers that use tuning apply a simple grid search to explore the potential parameter space for optimal tunings such grid searchers run one for loop for each parameter being optimized .
however bergstra et al.
and fu et al.
argue that random search methods e.g.
the differential evolution algorithm used here are be t ter than grid search in terms of efficiency and performance.
method .
research problem t his section is an overview of the the task and methods used by xu.
t heir task was to predict relationships between two knowledge units questions with answers on stack over f low.
speci f ically xu divided linkable knowledge unit pairs into difference categories namely duplicate direct link indirect link andisolated based on its relatedness.
t he de f inition of these four categories are shown in table in that paper xu provided the following two methods as baselines table classes of knowledge unit pairs.
class description duplicate t hese two knowledge units are addressing the same question.
direct linkone knowledge unit can help to answer the question in the other knowledge unit.
indirect linkone knowledge provides similar information to solve the question in the other knowledge unit but not a direct answer.
isolated t hese two knowledge units discuss unrelated questions.
tf idf svm a multi class svm classi f ier with textual features generated based on the tf and idf values of the words in a pair of knowledge units.
word embedding svm a multi class svm classi f ier with word embedding generated by the word2vec model .
both of these two baseline methods are compared against their proposed method word embedding cnn.
in this study we select word embedding svm as the baseline because it uses word embedding as the input which is the same as the word embedding cnn method by xu.
.
learners and t heir parameters svm has been proven to be a very successful method to solve text classi f ication problem.
a svm seeks to minimize misclassi f ication errors by selecting a boundary or hyperplane that leaves the maximum margin between positive and negative classes where the margin is de f ined as the sum of the distances of the hyperplane from the closest point of the two classes .
like most machine learning algorithms there are some parameters associated with svm to control how it learns.
in xu s experiment they used a radial bias function rbf for their svm kernel and set to k where kis for tf idf svm method and for word embedding svm method.
for other parameters xu mentioned that grid search was applied to optimize the svm parameters but no further information was disclosed.
for our work we used the svm module from scikit learn a python package for machine learning where the parameters shown in table.
are selected for tuning.
parameter cis to set the amount of regularization which controls the tradeoff between the errors on training data and the model complexity.
a small value for c will generate a simple model with more training errors while a large value will lead to a complicated model with fewer errors.
kernel is to introduce different nonlinearities into the svm model by applying kernel functions on the input data.
gamma de f ines how far the in f luence of a single training example reaches with low values meaning far and high values meaning close .
coef0 is an independent parameter used in sigmod and polynomial kernel function.
as to why we used the tuning range shown in table and not some other ranges we note that a those ranges include the defaults and also xu s values b the results presented below show that by exploring those ranges we achieved large gains in the performance of our baseline method.
t his is not to say that larger tuning rangeseasy over hard a case study on deep learning esec fse september paderborn germany table list of parameters tuned by t his paper.
parameters default xue et al.
tuning range description c .
unknown penalty parameter c of the error term.
kernel rbf rbf specify the kernel type to be used in the algorithms.
gamma nfeatures kernel coefficient for rbf poly and sigmoid .
coef0 unknown independent term in kernel function.
it is only used in poly and sigmoid .
might not result in greater improvements.
however for the goals of this paper to show that tuning baseline method does ma t ter exploring just these ranges shown in table will suffice.
.
learning word embedding learning word embeddings refers to f ind vector representations of words such that the similarities between words can be captured by cosine similarity of corresponding vector representations.
it is been shown that the words with similar semantic and syntactic are found closed to each other in the embedding space .
several methods have been proposed to generate word embeddings like skip gram glove and pca on the word cooccurrence matrix .
to replicate xu work we used the continuous skip gram model word2vec which is a unsupervised word representation learning method based on neural networks and also used by xu .
t he skip gram model learns vector representations of words by predicting the surrounding words in a context window.
given a sentence of words w w1 w2 ... wn the objective of skip gram model is to maximize the the average log probability of the surrounding words nn i 1 c j c j 0lo afii10069.italp1wi jjwio where cis the context window size and wi jandwirepresent surrounding words and center word respectively.
t he probability ofp1wi jjwiois computed according to the so f tmax function p1wojwio exp1 v.alttwo v.altwio jwj w 1exp1 v.alttw v.altwio where v.altwiand v.altwoare the vector representations of the input and output vectors of w respectively.
jwj w 1exp1 v.alttw v.altwionormalizes the inner product results across all the words.
to improve the computation efficiency mikolove et al.
proposed hierachical so f tmax and negative sampling techniques.
more details can be found in mikolove et al.
s study .
skip gram s parameters control how that algorithm learns word embeddings.
t hose parameters include window size anddimensionality of embedding space etc.
zucoon et al.
found that embedding dimensionality and context window size have no consistent impact on retrieval model performance.
however yang et al.
showed that large context window and dimension sizes are preferable to improve the performance when using cnn to solve classi f ication tasks for twi t ter.
since this work is to compare performance of tuning svm with cnn where skip gram model is used to generate word vector representations for both of these methods tuning parameter of skip gram model is beyond the scope of this paper but we will explore it in future work .
.
given a model e.g.
svm with ndecisions e.g.
n tuner calls sample n ntimes.
each call generates one member of the population popi2n.
.
tuner scores each popiaccording to various objective scores o. in the case of our tuning svm the objective ois to maximize f1 score .
tuner tries to each replace popiwith a mutant mbuilt using storn s differential evolution method .
de extrapolates between three other members of population a b c. at probability p1 for each decision ak2a then mk ak 1p1 rand1o bk ckoo.
.
each mutant mis assessed by calling evaluate1model prior mo i.e.
by seeing what can be achieved within a goal a f ter f irst assuming that prior m. .
to test if the mutant mis preferred to popi tuner simply compare score m with score popi .
in case of our tuning svm the one with higher score will be kept.
.
tuner repeatedly loops over the population trying to replace items with mutants until new be t ter mutants stop being found.
.
return the best one in the population as the optimal tunings.
figure procedure tuner strives to f ind good tunings which maximizes the objective score of the model on training and tuning data.
tuner is based on storn s differential evolution optimizer .
to train our word2vec model knowledge units tagged with java from stack over f low posts table include titles questions and answers are randomly selected as a word corpus2.
a f ter applying proper data processing techniques proposed by xu like remove the unnecessary html tags and keep short code snippets incode tag then f it the corpus into gensim word2vec module which is a python wrapper over original word2vec package.
when converting knowledge units into vector representations for each word wiin the post processed knowledge unit including title question and answers we query the trained word2vec model to get the corresponding word vector representation v.alti.
t hen the whole knowledge unit with swords is converted to vector representation by element wise addition u v.alt v.alti v.alt2 v.alts.
t his vector representation is used as the input data to svm.
.
tuning algorithm a tuning algorithm is an optimizer that drives the learner to explore the optimal parameter in a given searching space.
according to our literature review there are several searching algorithms used in se community simulated annealing various genetic algorithms augmented by techniques such as differential evolution tabu search andsca t ter search particle swarm optimization numerous decomposition approaches 2without further explanation all the experiment se t tings including learner algorithms training testing data split etc strictly follow xu s work.esec fse september paderborn germany wei fu tim menzies that use heuristics to decompose the total space into small problems then apply a response surface methods nsga ii and nsga iii .
of all the mentioned algorithms the simplest are simulated annealing sa and differential evolution de each of which can be coded in less than a page of some high level scripting language.
our reading of the current literature is that there are more advocates for differential evolution than sa.
for example vesterstrom and t homsen found de to be competitive with particle swarm optimization and other gas.
des have already been applied before for parameter tuning in se community to do parameter tuning e.g.
see .
t herefore in this work we adopt de as our tuning algorithm and the main steps in de is described in figure .
experimental setup .
research q uestions to systematically investigate whether tuning can improve the performance of baseline methods compared with deep learning method we set the following three research questions rq1 can we reproduce xu s baseline results word embedding svm ?
rq2 can de tune a standard learner such that it outperforms xu s deep learning method?
rq3 is tuning svm with de faster than xu s deep learning method?
rq1 is to investigate whether our implementation of word embedding svm method has the similar performance with xu s baseline which makes sure that our following analysis can be generalized to xu s conclusion.
rq2 and rq3 lead us to investigate whether tuning svm comparable with xu s deep learning from both performance and cost aspects.
.
dataset and experimental design our experimental data comes from stack over f low data dump of september where the posts table includes all the questions and answers posted on stack over f low up to date and the postlinks table describes the relationships between posts e.g.
duplicate and linked .
as mentioned in section .
we have four different types of relationships in knowledge unit pairs.
t herefore linked type is further divided into indirectly linked anddirectly linked .
overall four different types of data are generated according the following rules randomly select a pair of posts from the postlinks table if the value in postlinktypeid f ield for this pair of posts is then this pair of posts is duplicate posts.
otherwise they re directly linked posts.
randomly select a pair of posts from the posts table if this pair of posts is linkable from each other according to postlinks table and the distance between them are greater than which means they are not duplicate or directly linked posts then this pair of posts is indirectly linked.
if they re not linkable then this pair of posts is isolated.
3h t tps archive.org details stackexchange word2vec wordembeddingslookup testing ku vectors predict results training ku pairs new training ku vectorstuningku vectorslookup svm ku textstrainevaluatetrain svm parametersde best tuningstraintrain word2vec train learnertest learnerparameter tuning testing ku pairsfigure t he overall work f low of building knowledge units predictor with tuned svm in this work we use the same training and testing knowledge unit pairs as xu where pairs of knowledge units for training and pairs for testing.
and each type of linked knowledge units accounts for in both training and testing data.
t he reasons that we used the same training and testing data as xu are it is to ensure that performance of our baseline method is as closed to xu s as possible.
since deep learning method is way complicated compared to svm and a li t tle difference in implementations might lead to different results.
to fairly compare with xu s result we can use the performance scores of cnn method from xu s study without any implementation bias introduced.
for training word2vec model we randomly select knowledge units title question body and all the answers from posts table that are related to java .
a f ter that all the training tuning testing knowledge units used in this paper are converted into word embedding representations by looking up each word in wrod2vec model as described in section .
.
as seen in figure instead of using all the knowledge units as training data we split the original training data into new training data and tuning data which are used during parameter tuning procedure for training svm and evaluating candidate parameters offered by de.
a f terwards the new training data is again f i t ted into the svm with the optimal parameters found by de and f inally the performance of the tuned svm will be evaluated on the testing data.
to reduce the potential variance caused by how the original training data is divided fold cross validation is performed.
speci f ically each time one fold with knowledge units pairs is used as the tuning data and the remaining folds with knowledge units are used as the new training data then the output svm model will be evaluated on the testing data.
t herefore all the performance scores reported below are averaged values over runs.
in this study we use wilcoxon single ranked test to statistically compare the differences between tuned svm and untuned svm.
speci f ically the benjamini hochberg bh adjusted p value is used to test whether a difference is statistically signi f icant at the level of .
to measure the effect size of performance scores between 4h t tps github.com xbwer asedataseteasy over hard a case study on deep learning esec fse september paderborn germany tuned svm and untuned svm we compute cliff s that is a nonparametric effect size measure .
as romano et al.
suggested we evaluate the magnitude of the effect size as follows negligible j j small j j medium j j and large .
j j .
.
evaluation metrics when evaluating the performance of tuning svm on the multiclass linkable knowledge units prediction problem consistent with xu we use accuracy precision recall and f1 score as the evaluation metrics.
table confusion matrix.
classi f ied as c1 c2 c3 c4actualc1c11 c12 c13 c14 c2c21 c22 c23 c24 c3c31 c32 c33 c34 c4c41 c42 c43 c44 given a multi classi f ication problem with true labels c1 c2 c3 andc4 we can generate a confusion matrix like table where the value of ciirepresents the number of instances that are correctly classi f ied by the learner for class ci.
accuracy of the learner is de f ined as the number of correctly classi f ied knowledge units over the total number of knowledge units i.e.
accurac y.alt icii i jcij where i jcijis the total number of knowledge units.
for a given type of knowledge units cj the precision is de f ined as probability of knowledge units pairs correctly classi f ied as cjover the number of knowledge unit pairs classi f ied as cjand recall is de f ined as the percentage of all cjknowledge unit pairs correctly classi f ied.
f1score is the harmonic mean of recall and precision.
mathematically precision recall and f1 score of the learner for class cjcan be denoted as follows prec j precision j cjj icij pdj recall j cjj icji f1j pdj prec j 1pdj prec jo where icijis the predicted number of knowledge units in class cjand icjiis the actual number of knowledge units in class cj.
recall from algorithm that we call differential evolution once for each optimization goal.
generally this goal depends on which metric is most important for the business case.
in this work we usef1 to score the candidate parameters because it controls the trade off between precision and recall which is also consistent with xu and is also widely used in so f tware engineering community to evaluate classi f ication results .
results in this section we present our experimental results.
to answer research questions raised in section .
we conducted two experiments figure score delta between our svm with xu s svm in in terms of precision recall and f1 score.
positive values mean our svm is better than xu s svm in terms of different measures otherwise xu s svm is better.
compare performance of word embedding svm method in xu and our implementation compare performance of our tuning svm with de method with xu s cnn deep learning method.
since we used the same training and testing data sets provided by xu and conducted our experiment in the same procedure and evaluated methods using the performance measures we simply used the results reported in the work by xu for the performance comparison.
rq1 can we reproduce xu s baseline results word embedding svm ?
t his f irst question is important to our work since without the original tool released by xu we need to insure that our reimplementation of their baseline method wordembedding svm has a similar performance to their work.
accordingly we carefully follow xu s procedure .
we use the svm learner from scikit learn with the se t ting 200andkernel rbf which are used by xu.
a f ter that the same training and testing knowledge unit pairs are applied to svm.
table comparison of our baseline method with xu s. t he best scores are marked in bold.
metrics methods duplicatedirect linkindirect linkisolated overall precisionour svm .
.
.
.
.
xu s svm .
.
.
.
.
recallour svm .
.
.
.
.
xu s svm .
.
.
.
.
f1 scoreour svm .
.
.
.
.
xu s svm .
.
.
.
.
accuracyour svm .
.
.
.
.
xu s svm .
table and figure show the performance scores and corresponding score delta between our implementation of wordembedding svm with xu s in terms of accuracy5 precision recall and 5xu just report overall accuracy not for each class hence it is missing in this table.esec fse september paderborn germany wei fu tim menzies figure score delta between tuned svm and cnn method in terms of precision recall and f1 score.
positive values mean tuned svm is better than cnn in terms of different measures otherwise cnn is better.
f1 score.
as we can see when predicting these four different types of relatedness between knowledge unit pairs our word embedding svm method has very similar performance scores to the baseline method reported by xu in with the maximum difference less than .
except for duplicate class where our baseline has a higher precision i.e.
v.s.
but a lower recall i.e.
v.s.
.
figure presents the same results in a graphical format.
any bar above zero means that our implementation has a be t ter performance score than xu s on predicting that speci f ic knowledge unit relatedness class.
as we can see most of the differences are within .
and the score delta of overall performance shows that our implementation is a li t tle worse than xu s implementation.
for this chart we conclude that overall our reimplementation of wordembedding svm has very similar performance in all the evaluated metrics compared to the baseline method reported in xu s study .
t he signi f icance of this conclusion is that moving forward we are con f ident that we can use our reimplementation of wordembedding svm as a valid surrogate for the baseline method of xu.
rq2 can de tune a standard learner such that it outperforms xu s deep learning method?
to answer this question we run the work f low of figure where de is applied to f ind the optimal parameters of svm based on the training and tuning data.
t he optimal tunings are then applied on the svm model and the built learner is evaluated on testing data.
note that in this study since we mainly focus on precision recall and f1 score measures where f1 score is the harmonic mean of precision and recall we use f1 score as the tuning goal for de.
in other words when tuning parameters de expects to f ind a pair of candidate parameters that maximize f1 score.
table presents the performance scores of xu s baseline xu s cnn method and tuned svm for all metrics.
t he highest score for each relatedness class is marked in bold.
note that without tuning xu s cnn method outperforms the baseline svm in10 figure score delta between tuned svm and xu s baseline svm in terms of precision recall and f1 score.
positive values mean tuned svm is better than xu s svm in terms of different measures otherwise xu s svm is better.
table comparison of tuned svm with xu s cnn method.
t he best scores are marked in bold.
metrics methods duplicatedirect linkindirect linkisolated overall precisionxu s svm .
.
.
.
.
xu s cnn .
.
.
.
.
tuned svm .
.
.
.
.
recallxu s svm .
.
.
.
.
xu s cnn .
.
.
.
.
tuned svm .
.
.
.
.
f1 scorexu s svm .
.
.
.
.
xu s cnn .
.
.
.
.
tuned svm .
.
.
.
.
evaluation metrics across all four classes.
t he largest performance improvement is for recall on direct link class.
note that this result is consistent with xu s conclusion that their cnn method is superior to standard svm.
a f ter tuning svm the deep learning method has no such advantage.
speci f ically cnn has advantage over tuned svm in4 12evaluation metrics across all four classes.
even when cnn performs be t ter that our tuning svm method the largest difference is for recall on direct link class which is less than .
figure presents the same results in a graphical format.
any bar above zero indicates that tuned svm has a be t ter performance score than cnn.
in this f igure cnn has a slightly be t ter performance on duplicate class for precision recall and f1 score and a higher recall on direct link class.
across all of figure in8 12evaluation scores tuned svm has be t ter performance scores than cnn with the largest delta of .
figure compares the performance delta of tuned svm with xu s untuned svm.
we note that de based parameter tuning never degrades svm s performance since there are no negative values in that chart .
tuning dramatically improves scores on predicting some classes of ku relatedness.
for example the recall of predicting direct link is increased from to which is improvement over xu s untuned svm to be fair for xu it is stilleasy over hard a case study on deep learning esec fse september paderborn germany figure score delta between tuned svm and our untuned svm in terms of precision recall and f1 score.
positive values mean tuned svm is better than our untuned svm in terms of different measures otherwise our svm is better.
improvement over our untuned svm .
at the same time the corresponding precision and f1 scores of predicting direct link are increased from to and to which are and improvement over xu s original report respectively.
a similar pa t tern can also be observed in isolated class.
on average tuning helps improve the performance of xu s svm by and in terms of precision recall and f1 score for all four ku relatedness classes.
figure compares the tuned svm with our untuned svm.
we note that we get the similar pa t terns that observed in figure .
all the bars are above zero etc.
based on the performance scores in table and score delta in figure figure and figure we can see that parameter tuning can dramatically improve the performance of word embedding svm the baseline method for the multiclass ku relatedness prediction task with the optimal tunings the traditional machine learning method svm if not be t ter is at least comparable with deep learning methods cnn .
when discussing this result with colleagues we are sometimes asked for a statistical analysis that con f irms the above f inding.
however due the lack of evaluation score distributions of the cnn method in we cannot compare their single value with our results from repeated runs.
however according to wilcoxon singed rank test over runs results tuned svm performs statistically be t ter than our untuned svm in terms of all evaluation measures on all four classes p .
according to cliff values the magnitude of difference between tuned svm and our untuned svm is not trivial j j for all evaluation measures.
overall the experimental results and our analysis indicate that in the evaluation conducted here the deep learning method cnn does not have any performance advantage over our tuning approach.
rq3 is tuning svm with de faster than xu s deep learning method?when comparing the runtime of two learning methods it obviously should be conducted under the same hardware se t tings.
since we adopt the cnn evaluation scores from we can not run on our tuning svm experiment under the exactly same system settings.
to allow readers to have a objective comparison we provide the experimental environment as shown in table .
to obtain the runtime of tuning svm we recorded the start time and end time of the program execution including parameter tuning training model and testing model.
table comparison of experimental environment methods os cpu ram tuning svm macos .
intel core i5 .
ghz gb cnn windows intel core i7 .
ghz gb according to xu it took hours to train their cnn model into a low loss convergence e .
our work on the other hand only takes minutes to run svm with parameter tuning by de on a similar environment.
t hat is the simple parameter tuning method on svm is xfaster than xu s deep learning method.
compared to cnn method tuning svm is about xfaster in terms of model building.
t he signi f icance of this f inding is that in this case study cnn was neither be t ter in performance scores see rq2 nor runtimes.
cnn s extra runtimes are a particular concern since a they are very long and b these would be incurred anytime researchers wants to update the cnn model with new data or wanted to validate the xu result.
discussion .
why de svm works?
parameter tuning matters .
as mentioned in section .
the default parameter values set by the algorithm designers could generate a good performance on average but may not guarantee the best performance for the local data .
given that it is most strange to report that most se researchers ignore the impacts of parameter tuning when they utilize various machine learning methods to conduct so f tware analytic evidence see our reviews in .
t he conclusion of this work must be to stress the importance of this kind of tuning using local data for any future so f tware analytics study.
better explore the searching space .
it turns out that one exception to our statement that most researchers do not tune is the xu study.
in that work they unsuccessfully perform parameter tuning but with with grid search.
in such a grid search for n parameters to be tuned nfor loops are created to run over a range of se t tings for each parameter.
while a widely used method it is o f ten deprecated.
for example bergstra et al.
note that grid search jumps through different parameter se t tings between some minandmax values of pre de f ined tuning range.
t hey warn that such jumps may actually skip over the critical tuning values.
on the other hand de tuning values are adjusted based on be t ter candidates from previous generations.
hence de is more likely than grid search to f ill in the gaps between the initialized values.esec fse september paderborn germany wei fu tim menzies t hat said although de svm works in this study it does not mean de is the best parameter tuner for all se tasks.
we encourage more researchers to explore faster and more effective parameter tuners in this direction.
.
implication beyond the speci f ics of this case study what general principles can we take from the above work?
understand the task.
one reason to try different tools for the same task is to be t ter understand the task.
t he more we understand a task the be t ter we can match tools to that task.
tools that are poorly matched to task are usually complex and or slow to execute.
in the case study of this paper we would say that deep learning is a poor match to the task of predicting whether two questions posted on stack over f low are semantically linkable since it is so slow differential evolution tuning svm is a much be t ter match since it is so fast and obtain competitive performance.
t hat said it is important to stress that the point of this study is not to deprecate deep learning.
t here are many scenarios were we believe deep learning would be a natural choice e.g.
when analyzing complex speech or visual data .
in se it is still an open research question that in which scenario deep learning is the best choice.
results from this paper show that at least for classi f ication tasks like knowledge unit relatedness classi f ication on stack over f low deep learning does not have much advantage over well tuned conventional machine learning methods.
however as we be t ter understand se tasks deep learning could be used to address more se problems which require more advanced arti f icial intelligence.
treat resource constraints as design challenges.
as a general engineering principle we think it insightful to consider the resource cost of a tool before applying it.
it turns out that this is a design pa t tern used in contemporary industry.
according to calero and pa t tini many current commercial redesigns are motivated at least in part by arguments based on sustainability i.e.
using fewer resources to achieve results .
in fact they say that managers used sustainability based redesigns to motivate extensive cost cu t ting opportunities.
.
t hreads to validity t hreats to internal validity concern the consistency of the results obtained from the result.
in our study to investigate how tuning can improve the performance of baseline methods and how well it perform compared with deep learning method.
we select xu s word embedding svm baseline method as a case study.
since the original implementation of word embedding svm baseline method in is not publicly available we have to reimplement our version of word embedding svm as the baseline method in this study.
as shown in rq1 our implementation has quite similar results to xu s on the same data sets.
hence we believe that our implementation re f lect the original baseline method in xu s study .
t hreats to external validity represent if the results are of relevance for other cases or the ability to generalize the observations in a study.
in this study we compare our tuning baseline method with deep learning method cnn in terms of precision recall f1 scoreand accuracy.
t he experimental results are quite consistent for this knowledge units relatedness prediction task.
nonetheless we do not claim that our f indings can be generalized to all so f tware analytics tasks.
however those other so f tware analytics tasks o f ten apply deep learning methods on classi f ication tasks and so it is quite possible that the methods of this paper i.e.
de based parameter tuning would be widely applicable elsewhere.
conclusion in this paper we perform a comparative study to investigate how tuning can improve the baseline method compared with state ofthe art deep learning method for predicting knowledge units relatedness on stack over f low.
our experimental results show that tuning improves the performance of baseline methods.
at least for word embedding svm baseline in method if not be t ter it performs as well as the proposed cnn method in .
t he baseline method with parameter tuning runs much faster than complicated deep learning.
in this study tuning svm runs 84xfaster than cnn method.
addendum as this paper was going to going to press we learned of a new deep learning methods that according to its creators runs times faster than standard deep learning .
note that in that paper the authors say their faster method does not produce be t ter results in fact their method generated solutions that were a small fraction worse than classic deep learning.
hence that paper does not invalidate our result since a our de based method sometimes produced be t ter results than classic deep learning and b our de runs times faster i.e.
much faster runtimes than those reported in .
t hat said this new fast deep learner deserves our close a t tention since using it we conjecture that our de tools could solve an open problem in the deep learning community i.e.
how to f ind the best con f igurations inside a deep learner faster.
based on the results of this study we recommend that before applying deep learning method on se tasks implement simpler techniques.
t hese simpler methods could be used at the very least for comparisons against a baseline.
in this particular case of deep learning vs de the extra computational effort is so very minor minutes on top of hours that such a try with simpler should be standard practice.
as to the future work we will explore more simple techniques to solve se tasks and also investigate how deep learning techniques could be applied effectively in so f tware engineering f ield.