when how and why developers do not test in their ides beller m. gousios g. panichella a. zaidman a. article in monograph or in proceedings nitto e. di ed.
esec fse proceedings of the 10th joint meeting on foundations of software engineering pp.
to publisher version of the following full text publisher s version downloaded from download date note to cite this publication please use the final published version if applicable .when how and why developers do not test in their ides moritz beller delft university of technology the netherlands m.m.beller tudelft.nlgeorgios gousios radboud university nijmegen the netherlands g.gousios cs.ru.nlannibale panichella andy zaidman delft university of technology a.panichella tudelft.nl a.e.zaidman tudelft.nl abstract the research community in software engineering and software testing in particular builds many of its contributions on a set of mutually shared expectations.
despite the fact that they form the basis of many publications as well as open source and commercial testing applications these common expectations and beliefs are rarely ever questioned.
for example frederic brooks statement that testing takes half of the development time seems to have manifested itself within the community since he first made it in the mythical man month in .
with this paper we report on the surprising results of a large scale field study with software engineers whose development activity we closely monitored over the course of five months resulting in over years of recorded work time in their integrated development environments ides .
our findings question several commonly shared assumptions and beliefs about testing and might be contributing factors to the observed bug proneness of software in practice the majority of developers in our study does not test developers rarely run their tests in the ide test driven development tdd is not widely practiced and last but not least software developers only spend a quarter of their work time engineering tests whereas they think they test half of their time.
categories and subject descriptors d. .
testing and debugging testing tools general terms experimentation human factors measurement theory verification keywords developer testing unit tests testing effort field study testdriven development tdd permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse bergamo italy copyright acm x xxxxx xx x xx xx ... .
.
.
introduction how much should we test?
and when should we stop testing?
since the dawn of software testing these questions have tormented developers and their managers alike.
in twelve software companies declared them pressing issues during a survey on unit testing by runeson .
fast forward to eight years later and the questions are still unsolved appearing as one of the grand research challenges in empirical software engineering .
but before we are able to answer how much we should test we must first know how much we aretesting.
post mortem analyses of software repositories by pinto et al.
and zaidman et al.
have provided us insights into how tests are created and evolved at the commit level.
however there is a surprising lack of knowledge of how developers actually test as evidenced by bertolino s call to gain a better understanding of testing practices .
this lack of empirical knowledge of when how and why developers test in their integrated development environments ides stands in contrast to a large body of folklore in software engineering including brooks statement from the mythical man month that testing consumes half of the development time.
in this paper we start to fill our knowledge gap with resilient empirical observations on developer testing in the real world.
in our investigation we focus on developer tests i.e.
codified unit integration or system tests that are engineered inside the ide by the developer.
developer testing is often complemented by work outside the ide such as manual testing automated test generation and dedicated testers which we explicitly leave out of our investigation.
by comparing the state of the practice to the state of the art of testing in the ide we aim to understand the testing patterns and needs of software engineers expressed in our five research questions rq1 when and why do developers test?
rq2 how and why do developers run tests?
rq3 how do developers react to test runs?
rq4 do developers follow test driven development tdd ?
rq5 how much do developers test?
if we study these research questions in a maximally large and varied population of software engineers the answers to them can provide important implications for practitioners designers of nextgeneration ides and researchers.
to this end we have set up an open ended longitudinal field study that has run for five months and involved software engineers from industry as well as open source projects around the world.
the field study is enabled by the eclipse plugin watchdog which instruments the ide and objectively observes how developers work on and with tests.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august september bergamo italy c acm.
... .
179our results indicate that over half of the studied users do not practice testing even if the projects contain tests developers rarely execute them in the ide test driven development is not a widely followed practice and completing the overall low results on testing developers overestimate the time they devote to testing twofold.
these results counter common wisdom about developer testing and could help explain the observed bug proneness of realworld software systems.
.
research design in this section we describe the setup of our study the acquisition and demographics of its participants and the statistical tests that we applied.
.
study design to be able to make statements about the general state of testing it is necessary that we study our research questions in a largescale field study with hundreds of participants.
by instrumenting the ide with a purpose built plugin watchdog we can reach the desired number of objective observations of developer testing.
using a mixed methods approach we compare the results from the automated monitoring of developers to their subjective survey answers.
.
study participants in this section we first explain how we attracted study participants and then report on their demographics.
.
.
acquisition of participants we reached out to potential developers to install watchdog in their ide by providing a high profile project website.
raffling off prizes.
delivering real value to the users of watchdog in that it gives feedback on their development behavior.
writing articles in magazines and blogs relevant to java and eclipse developers eclipse magazin jaxenter eclipseplanet heise news .
giving talks and presentations at developer conferences dutch testing day eclipsecon .
participating in a youtube java developer series.
penetrating social media reddit hackernews twitter facebook .
approaching software development companies.
contacting developers among them java developers on github.
putting our plugin in a well established eclipse marketplace.
launching a second marketplace which increases the visibility of scientific plugins within the eclipse ecosystem together with the eclipse code recommenders project.
we put emphasis on the testing focus of watchdog to attract developers interested in testing.
.
.
demographics of participants in total users had installed and registered watchdog by march 1st .
in this paper we report on the data we received testroots watchdog test analytics testroots watchdog0306090 sessions per developernumber of developers programming experience y ears number of developers figure distributions of the sessions per developer and their programming experience.
from november 1st to march 1st excluding student data that we had analyzed separately .
as we updated watchdog to fix bugs and integrate new features see section .
.
we also filtered out data from the deprecated versions .
and .
.
in total after filtering we consider different users and their user actions so called intervals see section .
from different countries between november 1st and march 1st .
the most frequent country of theses users is the united states of users followed by china germany netherlands and india .
the other half of users comes from remaining countries with less than total share each.
our developers predominately use windows of users use macos and linux.
their programming experience shown in figure is normally distributed a shapiro wilks test fails to reject the null hypothesis that it is not normally distributed at p .
.
generally we have slightly more in experienced years of users than experienced users.
on the other hand very experienced developers greaterorequalslant7 years represent more than of our population.
the participants registered unique projects a project cannot be shared among users .
the registered projects stem from industry as well as famous open source initiatives like the apache foundation but also include private projects.
in total we observed hours of working time in which eclipse was open for these registered projects.
using the average work time for oecd countries of hours per year 5this amounts to .
observed developer years in the ide.
in total these recorded .
years encompass distinct eclipse sessions.
this large scale approach broadens our technical study on developer testing in the ide to a very large set of developers compared to .
years of student data in our icse nier paper .
furthermore our technical development behavior data is complemented by survey responses from the registration of watchdog users and projects.
.
watchdog infrastructure starting with an initial prototype in we evolved watchdog into an open source and production ready software solution6with a client server architecture which is designed to scale up to thousands of simultaneous users.
.
.
watchdog client we implemented watchdog as an eclipse plugin because the eclipse java development tools edition jdt is one of the most widely used ides for java programming .
thanks to its integrated junit support the eclipse jdt facilitates developer testing.
watchdog instruments the eclipse jdt environment and registers listeners for ui events related to programming behavior and 180test executions.
we group coherent events as intervals which comprise a specific type a start and an end time.
this abstraction allows us to closely follow the workflow of a developer without being overwhelmed by hundreds of fine grained ui events per minute.
every time a developer reads modifies or executes a test or production code class watchdog creates a new interval and enriches it with type specific data.
.
.
watchdog server watchdog intervals are locally cached allowing offline work and automatically sent to our server as a json stream.
the watchdog server accepts the json data via a rest api.
after sanity checking the intervals are stored in a nosql database.
this infrastructure provides high extensibility up to thousands of clients and easy maintenance in case of changes in the data format in the client.
moreover we can remotely trigger an update of all watchdog clients which allows us to fix bugs and extend its functionality any time.
automated ping services monitor the health of our web api so we can immediately react if a problem occurs.
thereby our watchdog service achieved an uptime of .
.
.
developer survey having installed watchdog a developer first signs up as a user with watchdog or reuses an already registered id and after that registers the current eclipse workspace as a watchdog project.
both registrations includes a short survey that the developer can fill out in the ide.
key questions in the survey regard developers programming expertise whether and how they test their software and if so which testing frameworks they employ and how much time they think they spend on testing.
.
statistical evaluation when applying statistical tests in the remainder of this paper we regard results as significant at a confidence interval .
i.e.
iff p lessorequalslant .
all results of tests tiin the remainder of this paper are statistically significant at this level i.e.
i p ti lessorequalslant .
for each test ti we first perform a shapiro wilk normality test si .
since all our distributions significantly deviate from a normal distribution according to shapiro wilk i p si .
lessorequalslant we use non parametric tests for testing whether there is a significant statistical difference between two distributions we use the non parametric wilcoxon rank sum test.
for performing correlation analyses we use the non parametric spearman rank order correlation coefficient .
hopkins s guidelines facilitate the interpretation of they assume no correlation for lessorequalslant .
a weak correlation for .
lessorequalslant .
a moderate correlation for .
lessorequalslant .
and a strong correlation for .
lessorequalslant lessorequalslant1.
.
research methods in this section we describe how the watchdog plugin instruments the eclipse ide and collects data and how we prepared the data for our research questions on testing pattern correlations and the tdd process.
.
ide instrumentation watchdog focuses around the concept of intervals.
table gives a technical description of the different interval types.
they appear in the same order as in figure which exemplifies a typical development workflow to demonstrate how watchdog monitors ide activity with intervals a developer mike starts eclipse and watchdog creates three intervals eclipseopen perspective anduseractive .
thereafter mike executes the unit tests of the production timeinterval type ......... ... ... figure exemplary workflow visualization with intervals.
the interval types are described in the same order in table .
class he needs to change triggering the creation of a junitexecution interval enriched with the test result passed .
having browsed the source code of the file to understand which parts need to change a reading interval is triggered mike then performs the necessary changes.
a re execution of the unit test shows mike it fails after his edit .
mike steps through the test with the debugger and fixes the error.
the final re execution of the test succeeds.
intervals concerning the user s activity reading typing and other general activity are backed by an inactivity timeout so that we only record them when the user is actively working in the ide.
however if we detect that the ide lost the focus end of eclipseactive interval or the user switched from writing file x typing to reading file y reading we immediately end the currently opened interval.
intervals may overlap.
for example typing orreading intervals are wrapped inside a user activity which is again wrapped within an eclipseactive perspective andeclipseopen interval .
however reading andtyping intervals are by nature mutually exclusive.
we refer to an eclipse session as the timespan in which eclipse was open and not closed or interrupted for example because the developer suspended the computer.
all intervals that belong to one eclipse session are hence wrapped within one eclipseopen interval as in figure .
depending on the type of the interval we enrich it with different numerical and categorical information in a reading ortyping interval we store whether the underlying file is a java class a hash of its filename its length in source lines of code without whitespaces sloc and whether it is production or test code.
additionally for typing intervals we calculate the levensthein edit distance between the content of the file before and after the modification in the interval.
this gives us an indication of the size of the changes made in the typing interval.
if it is a java class we rate the file that the developer accesses in a reading ortyping interval as either production or test code.
we classify any other file type for example an xml configuration file as unknown.
we have four different recognition categories for test classes see table to designate the file as a test that can be executed 181table overview of watchdog intervals.
interval type description junitexecution interval creation invoked through the eclipse jdt integrated junit runner also working for maven projects .
each test execution is enriched with the sha hash of its test name making a link to a reading ortyping interval possible test result test duration and child tests executed.
reading interval in which the user was reading in the ide.
backed by inactivity timeout.
enriched with an abstract representation of the read file containing the sha hash of its filename its sloc and an assessment whether it is production code or test code.
a test can further be categorized into a test which uses junit and is therefore executable in the ide which employs a testing framework which contains test in its filename or contains test in the project file path case insensitive .
backed by inactivity timeout.
typing interval in which the user was typing in the ide.
backed by inactivity timeout.
useractive interval in which the user was actively working in the ide evidenced for example by keyboard or mouse events .
backed by inactivity timeout.
eclipseactive interval in which eclipse had the focus on the computer.
not shown in figure .
perspective interval describing which perspective the ide was in debugging regular java development ... eclipseopen interval in which eclipse was open.
if the computer is suspended the eclipseopen is closed and the current sessions ends.
upon resuming a new eclipseopen interval is started discarding the time in which the computer was sleeping.
each session has a random unique identifier.
in eclipse we require the presence of at least one junit import together with at least one method that has the test annotation or that follows the testmethod name convention.
this way we support both junit3 and junit4.
furthermore we recognize imports of common java test frameworks and their annotations mockito powermock .
as a last resort we recognize when a file contains test in its file name or the project file path.
it seems a common convention to pre or postfix the names of test files with test or to place all test code in one sub folder.
for example the standard maven directory layout mandates that tests be placed undersrc test java .7thereby we can identify and differentiate between all tests that employ standard java testing frameworks as test runners for their unit integration or system tests test related utility classes and even tests that are not executable.
we consider any java class that is not a test according to this broad test recognition strategy to be production code.
.
sequentialization of intervals for rq3 and rq4 we need a linearized stream of intervals following each other.
we generate such a sequence by ordering the intervals according to their start time.
for example in figure this sequenced stream after the first test failure in is failing test switch perspective start junit test read production code ... .
correlation analysis we address our research questions rq1 and rq2 with the help of correlation analyses.
for example one of the steps to answer rq1 is to correlate the amount of changed code i.e.
the code churn introduced in all typing intervals on test code with the number of test executions.
even though strongly coupled .
the code change is a more precise measure of how much a developer changed a file than purely counting the number of typing intervals on the file.
for our analysis the size of the modifications to a file is more important than in how many typing intervals the modification happened.
intuitively if developers change a lot of test code they should run their tests more often.
like all correlation analyses we first compute the churn and the number of test executions for each eclipse session and then calculate the correlation over these summedup values of each session.
eclipse sessions form a natural divider between work tasks and work days as we expect that developers typically do not close their ide or laptop at random but exactly when they do not need it anymore see table .
therefore we refrained from artificially dividing our data into smaller time units.
such a division would also pose a problem to a necessary sequentialization of our intervals because it is not clear how to linearize overlapping intervals.
.
recognition of test driven development test driven development tdd is a software development process originally proposed by beck .
while a plethora of studies have been performed to quantify the supposed benefits from tdd it is unclear how many developers use it in practice.
in rq4 we investigate how many developers follow tdd to which extent.
in the following we apply beck s definition of tdd to the watchdog interval concept providing the first verifiable definition of tdd in practice.
tdd is a cyclic process comprising a functionality evolution phase depicted in figure optionally followed by a functionalitypreserving refactoring phase depicted in figure .
we can best illustrate the first phase with the strict non finite automaton nfa at the top of figure and our developer mike who is now following tdd before mike introduces a new feature or performs a bug fix he assures himself that the test for the production class he needs to change passes join figure stands for a junitexecution that contains a successful execution of the test under investigation .
thereafter he first changes the test class hence the name test first software development to assert the precise expected behavior of the new feature or to document the bug he is about to fix.
we record such changes in a typing interval on test code.
naturally as mike has not yet touched the production code the test must fail je .
once work on the test is finished mike switches to production code type prod.
in which he makes figure strict top and lenient nfa of tdd.
182figure nfa for the refactoring phase of tdd.
figure compile errors while creating a tdd test.
precisely the minimal required set of changes for his failing test to pass again jo .
the tdd cycle can begin anew.
when we tried to apply this strict tdd process we found that it is very difficult to follow in reality specifically the clear separation between changes to the test and later changes to the production code.
especially when developing a new feature like watchdogupdate in figure developers face compilation errors during the test creation phase of tdd because the class or method they want to assert on watchdogupdate does not exist yet.
to be able to have an executing but failing test they have to mix in the modification or creation of production code.
moreover developers often know the result of a test without executing it for example if it contains compile errors and that a test case succeeds before they start to work on it for example because they fixed the test on their previous day at work .
to adjust for these deviations between a strict interpretation of tdd and its application we have created the lenient non finite automaton nfa at the bottom of figure which is more suitable for the recognition of tdd in practice.
due to the edge a tdd cycle can directly start with modifications of test code.
tdd does not only comprise a functionality changing phase but also the code refactor phase depicted in figure .
in this phase developers have the chance to perform functionality preserving refactorings.
once they are finished with refactoring the tests must still pass .
it is impossible to separate changes between production and test classes in the refactoring phase in practice as the latter rely on the api of the first.
to assess how strictly developers follow tdd we convert all three nfas to their equivalent regular expressions and match them against the linearized sequence of intervals see section .
.
for a more efficient analysis we can remove all intervals from the sequentialized stream except for junitexecution andtyping intervals which we need to recognize tdd.
to be able to draw a fine grained picture of developers tdd habits we performed the analysis for each session individually.
we count refactoring activity towards the total usage of tdd.
the portion of matches in the whole string sequence gives us a precise indication of a developer s adherence to tdd.
.
results in the following we detail the results to each of our research questions individually per subsection.
.
rq1 when and why do developers test?
to be able to answer how and why developers test we must first assess rq1.
how common is testing?
when we apply the broadest recognition of test classes as described in section .
and table of the analyzed projects contain tests that a user either read or modified.
hence for of projects we could not detect work on tests in the ide neither to execute nor to read or modify them.
if we narrow down the recognition of tests to junit tests which can be run through eclipse we find that projects have such tests.
when we compare these projects to the projects who claimed to have junit tests in the survey an intriguing discovery emerges only for of projects which claimed to have junit tests in the survey could we technically detect them in our interval data as eitherreading typing orjunitexecution .
our second sub research question is rq1.
how frequently are tests executed?
from the projects we observed test executions in the ide in projects .
the developers of these projects contributed sessions and ran test executions.
we can divide the sessions into two groups we find sessions in which no test was run but could have been run as we know the projects contains executable tests and only sessions in which at least one test was run.
consequently the average number of executed tests per session is relatively small .
for these projects.
when we consider only sessions in which at least one test was run the average number of test runs per session is .
.
when developers work on tests we expect that the more they change their tests the more they run their tests to inform themselves about the current execution status of the test they are working on.
rq1.
and following can therefore give an indication as to why and when developers test rq1.
do developers test their test code changes?
the correlation between test code churn and the number of test runs yields a moderately strong .
in our dataset.
while there is an obvious relationship between the two variables the correlation does not imply a causation or a direction.
therefore we cannot say that developers executed more tests because they changed more test code although this is one of the likely possibilities.
a logical next step is to assess whether the same holds for modifications to production code do developers assert that their production code still passes the tests?
rq1.
do developers test their production code changes?
this correlation is significant but weaker with .
.
finally in how many cases do developers modify their tests when they touch their production code or vice versa expressed in rq1.
do developers co evolve test and production code?
a weak .
suggests that tests and production code have some tendency to change together but it is certainly not the case that developers modify their tests for every production code change and vice versa.
.
rq2 how and why do developers run tests?
when developers run tests in the ide they want to see their execution result as fast as possible.
to be able to explain how and why developers execute tests we must therefore first know how long developers have to wait before they see a test run finish rq2.
how long does a test run take?
of all test executions finish within half a second and over within five seconds see table .
test durations longer than 183table descriptive statistics for important variables.
histograms are in log scale.
variable unit min median mean max histogram junitexecution duration sec .
.
.
.
tests perjunitexecution items .
percentage of executed tests .
time to fix failing test min .
.
.
.
one minute represent .
of the junitexecution s. only .
of runs take more than two minutes.
having observed that most test runs are short our next step is to examine whether short tests facilitate testing rq2.
do quick tests lead to more test executions?
to answer this research question we collect the test execution length and the number of times developers executed tests in each session as in section .
.
then we compute the correlation between the two distributions.
if our hypothesis was true we would receive a negative correlation between the test duration and the number of test executions.
this would mean that short tests are related to more frequent executions.
however the spearman rank correlation test shows that this is not the case as there is no correlation .
.
combined with the fact that only a small number of tests are executed it may suggest that developers explicitly select test cases .
while test selection is a complex problem on build servers it is interesting to investigate how developers perform it locally in their ide rq2.
do developers practice test selection?
in junit a test execution which we capture in a junitexecution interval may comprise multiple child test cases.
of test executions contain only one test case while only .
of test executions comprise more than tests and only .
more than tests .
test selection likely happened if the number of executed tests in onejunitexecution is smaller than the total number of tests for the given project modulo test renames test deletion and test moves .
the ratio between these two measures allows us to estimate the percentage of selected test cases.
if it is significantly smaller than developers practiced test selection.
our data in table shows that developers selected only of all their available tests for execution in of the cases while they ran all tests without test selection in only .
of cases.
to explain how and why test selection happens we investigate two possible scenarios in the first we assume that the developer picks out only one of the tests run in the previous test execution for example to examine why the selected test failed.
in the second scenario we assume that the developer excludes a few disturbing tests from the previous test execution.
in the cases in which developers performed test selection we can attribute of selections to scenario and to scenario .
hence our two scenarios are able to explain of test selections in the ide.
.
rq3 how do developers react to test runs?
having established how often programmers execute tests in their ide in the previous research questions it remains to assess rq3.
how frequently do tests pass and fail?
there are three scenarios under which a junit execution can return an unsuccessful result the java compiler might detect compilation errors an unhandled runtime exception is thrown during the test case execution or a test assertion is not met.
in either case the test acceptance criterion is never reached and we therefore consider them as a test failure following junit s definition.in the aggregated results of all observed test executions of junit executions fail and only pass successfully.
as test failures are apparently a situation developers are often facing we ask rq3.
how do developers react to a failing test?
for each failing test case we generate a linearized stream of subsequently following intervals as explained in section .
.
by counting and summing up developers actions after each failing test for up to five minutes seconds we can draw a precise picture of how developers manage a failing test in figure .
the most immediate reaction in over of the cases within the first seconds is to read production code.
the second most common reaction is to read test code with in the first second.
however already after five seconds switching to another window is more common than reading test code.
while switching focus away from eclipse becomes a popular reaction five seconds after a test failure reading production code mirrors this curve in the opposite direction decreasing by points within the first five seconds.
the other curves show a more steady distribution from the beginning.
interestingly switching to the debug perspective or altogether quitting eclipse almost never happens and is therefore not shown.
after two minutes the different reactions trend asymptotically towards their overall distribution with little variability.
the logical follow up to rq3.
is to ask whether developers reactions to a failing test are in the end successful and rq3.
how long does it take to fix a failing test?
to answer this question we determine the set of unique test cases per project and their execution result.
the failing test executions were caused by unique tests cases according to their file name hash .
for we observed at least one successful execution.
hence we never saw a successful execution for of all tests.
for the failing tests that we know have been fixed later we examine know how long developers take to fix a failing test.
of test repairs happen within minutes and within minutes .
.
rq4 do developers follow tdd?
in rq4 we aim to give an answer to the adoption of tdd in practice.
our results reveal that the sessions of only ten developers match against a strict tdd definition the top nfa in figure of all developers or of developers who executed tests see section .
.
in total only of sessions with test executions contain strict tdd patterns.
only one developer uses strict tdd in more than of the development process on average.
the majority of the developer s intervals are devoted to the refactoring phase of tdd depicted in figure .
the remaining nine developers use strict tdd in less than of their intervals.
refactoring is the dominant phase in tdd consuming on average of the tdd process.
all developers who practiced strict tdd have a lot of programming experience four declared an experience between and years the remaining six greater than years.
sessions from developers match against the lenient tdd nfa 300time s frequency of reactionreaction read prod.
code switched focus were inactive typed prod.
code read test code typed test code switched persp.
figure the immediate reactions to a failing test.
in figure of all developers or of developers who executed tests see section .
.
just two developers use lenient tdd in more than of their intervals including the developer who has over strict tdd matches.
six developers use lenient tdd in more than but less than of their intervals.
of the developers who use lenient tdd also refactor their code according to the tdd refactoring process in figure .
for them of intervals that match against the lenient tdd are due to refactoring.
of the developers seven have little programming experience years three have some experience years and the majority with are very experienced years .
even the top tdd users do not follow tdd in most sessions.
for example the user with the highest tdd usage has one session with compliance to tdd.
on the other hand in the majority of the remaining sessions the developer did not use tdd at all .
we verified this to be common also for the other developers who partially used tdd.
these low results on tdd are complemented by projects where users claimed to use tdd but in reality only of the did.
.
rq5 how much do developers test?
we motivated our investigation by asking how much time developers spend on engineering tests.
to answer this question we considerreading andtyping intervals and further split the two intervals according to the type of the document the developer works on either a production or test class.
the duration of test executions does not contribute to it as developers can typically work delta production reality vs. estimation points number of projects figure the delta between estimation and reality.while tests execute.
the short duration is negligible compared to the time spent on reading and typing because test executions normally finish within seconds see section .
.
when registering new projects developers estimated the time they spend on testing in the project.
hence we have the possibility to verify how accurate their estimation was by comparing it to their actual testing behavior.
there are two ways to aggregate this data at different levels of granularity.
the first is to explore the phenomenon on a per projectbasis we separately sum up the time developers are engineering i.e.
reading and writing production classes and test classes and divide it by the sum of the two.
then we compare this value to the developers estimation for the project.
this way we measure how accurate each individual prediction was.
the second way is to explore the phenomenon in our whole dataset by averaging across project and notnormalizing for the contributed development time only multiplying each estimation with it .
figure shows a histogram of the difference between the measured production percentage and its estimation per project .
a value of means the estimation was accurate.
a value of denotes that the programmer expected to only work on tests but in reality only worked on production code precisely the opposite .
the median of the distribution is shifted to the right of .
thus developers tend to overestimate the time they devote to testing on average by percentage points per project.
for our whole dataset we find that all developers spend in total of their time writing or reading production classes and of their time on testing.
however they estimated a distribution of on production code and on tests so they overestimated the time spent on testing twice.
the average time spent on production code versus test code is very similar to this overall ratio with and respectively.
moreover reading and writing are not uniformly spread across test and production code while developers read production classes for of the total time they spend in them they read tests much longer namely of the total time they spend in them.
to verify whether this preliminary finding is statistically relevant we use a one tailed wilcoxon rank sum test comparing the pairwise percentage of time spent in reading test and in reading production code for each project.
it confirms that relatively developers spend more time reading test than production code significant at p .
.
.
discussion in this section we first interpret our results and then present possible threats to validity.
.
interpretation of results in rq1 we established that in over half of the projects we did not see a single opened test even when considering a very lenient definition that likely overestimates the number of tests.
while this does not mean that the projects contain no tests a repository analysis might find that there exist a handful of test it does indicate that testing is not an important activity for the registered watchdog developers.
moreover only of the projects which claimed to have junit tests in the survey actually had intervals showing tests.
for the other their developer did not execute read or modify a single test within five months.
since we likely overestimate tests these two discoveries raise questions which value do such tests have in practice?
and further are developers answers true?
the majority of projects and users do not practice testing actively.
only of all projects comprise tests that developers can run in the ide.
for of projects that have such tests developers never use the possibility to execute them.
this gives a first hint that testing might not be as popular as we thought .
reasons might include that there are often no preexisting tests for the developers to modify that they are not aware of existing tests or that testing is too time consuming or difficult to do.
the apparent lack of tests might be one factor for the bug proneness of many current software systems.
even for projects which have tests developers did not execute them in most of the sessions.
in contrast the mean number of test runs for sessions with at least one test execution was high .
developers largely do not run tests in the ide.
however when they do they do it heftily.
one reason why some developers do not execute tests in the ide is that the tests would render their machine unusable for example during the execution of ui tests in the eclipse platform ui project.
the eclipse developers push their untested changes to the gerrit review tool and rely on it to trigger the execution of the tests on the continuous integration server.
in such a scenario the changes only become part of the holy repository if the tests execute successfully.
otherwise the developer is notified via email.
despite the tool overhead and a possibly slower reaction time our low results on test executions in the ide suggest that developers increasingly prefer such more complex setups to manually executing their tests in the ide.
ide creators could improve the continuous integration server support in future releases to facilitate this new workflow of developers.
every developer is familiar with the phrase oops i broke the build .
the weak correlations between test churn and test executions rq1.
and production churn and test executions rq1.
suggest an explanation developers simply do not assert for every change that their tests still run because this change cannot possibly break the tests.
even when the modifications to production or test code get larger developers do not necessarily execute tests in the ide more often .
these observations could stem from a development culture that embraces build failures and sees them as part of the normal development life cycle especially when the changes are not yet integrated into the main development line.the weak correlation between production and test code churn in rq1.
is on the one hand expected tests often serve as documentation and specification of how production code should work and are therefore less prone to change.
this conclusion is in line with previous findings from repository analyses .
if on the other hand a practice like tdd was widely adopted rq4 we would expect more co evolution of tests and production code expressed in a higher correlation.
tests and production code do not co evolve gracefully.
another factor that could influence how often developer run tests is how long they take to run.
in rq2 we found that testing in the ide happens fast paced.
most tests finish within five seconds or less.
tests run in the ide take a very short amount of time.
we could not observe a relation between the test duration and their execution frequency.
the reason for this could be that there is little difference between a test which takes .
seconds and one which takes seconds in practice.
both give almost immediate feedback to the programmer.
hence it seems unlikely that software engineers choose not to run tests because of their duration.
one reason for the short test duration is that developers typically do not execute all their tests in one test run.
instead they practice test selection and run only a small subset of their tests mostly less than of all available tests.
this observed manual behavior differs strongly from an automated test execution as part of the build which typically executes all tests.
developers frequently select a specific set of tests to run in the ide.
in most cases developers execute one test.
we can explain of these test selections with two scenarios developers either want to investigate a possibly failing test case in isolation of test selections or exclude such an irritating test case from a larger set of tests .
this finding complements and strengthens a study by gligoric et al.
who compared manual test selection in the ide to automated test selection in a population of developers .
one other possible explanation for the short time it takes tests to run in the ide is that of them fail rq3 once a test fails the developer might abort the execution of the remaining tests and focus on the failing test as discovered for rq2.
.
most test executions in the ide fail.
for of the failing tests we never saw a successful execution.
we built the set of tests in a project on a unique hash of their file names which means we cannot make a connection between a failed and a successful test execution when it was renamed in between.
however this very specific scenario is very rare as observed at the commit level by pinto et al.
.
consequently a substantial part of tests of up to are broken and not repaired immediately.
as a result developers exclude such broken tests from tests executions in the ide as observed for rq2.
.
since test failures in the ide are such a frequently recurring event software engineers must have good strategies to manage and react to them.
186the typical immediate reaction to a failing test is to dive into the offending production code.
closing the ide perhaps out of frustration that the test fails or opening the debug perspective to examine the test are very rare reactions.
five seconds after a test failure of programmers have already switched focus to another application on their computer.
an explanation could be that they search for a solution elsewhere for example in a documentation pdf or on the internet.
this is useful if the test failure originates from miss using a language construct the standard library or other well known apis and frameworks.
researchers try to integrate answers from internet fora like stack overflow into the ide to make this possibly interrupting context switch unnecessary.
tdd is one of the most widely studied software development processes .
even so it is unknown how widespread its use is in practice.
we have developed a formal technique that can precisely measure how strictly developers follow tdd.
in all our projects we found only three users that employed tdd for more than of their changes and only one session where the majority of changes happened according to tdd.
similar to rq1 we notice a stark contrast between survey answers and the observed behavior of developers.
only in of the projects claiming to do tdd the developers actually followed it to a small degree .
tdd is not widely practiced.
programmers who claim to do it neither follow it strictly nor for all their modifications.
possible reasons for the small adoption of tdd in practice are manifold developers might not find tdd practical or useful for most of their changes for example when implementing a ui they might take shortcuts and skip the mandatory test executions in figure because they know the test cannot or must succeed or the underlying code does simply not allow development in a test first manner for example because of framework restrictions.
furthermore tdd might simply be misunderstood by developers.
the question of how much time software engineers put into testing their application was first asked and anecdotally answered by brooks in .
nowadays everybody seems to know that testing takes of your time.
while their estimation was remarkably on par with brooks general estimation developers tested considerably less than they thought they would only of their time overestimating the real testing time two fold.
developers spend a quarter of their time engineering tests in the ide.
they overestimated this number twofold.
in comparison students tested of their time and overestimated their testing effort threefold.
hence real world developers test more and have a better understanding of how much they test than students.
surprisingly their perception is still far from reality.
while the reasons for this test effect might be psychological testing is usually not attributed with fun and a more destructive activity by nature its consequences can have severe implications on the quality of the resulting product.
software developers should be aware of how little they test and how much their perception deviates from the actual effort they invest in testing in the ide.
together with rq1 and rq3 this observation also casts doubt on whether we can trust untriaged answers from developers in surveys especially if the respondents are unknown to the survey authors.observed behavior often contradicted survey answers.
.
threats to validity in this section we discuss the limitations and threats that can affect the validity of our study and show how we mitigated them.
limitation.
the main limitation of our study is that we can only capture what happens inside eclipse.
conversely if developers perform work outside the ide we cannot record it.
examples for such behavior include pulling in changes through an external revision control tool like git orsvn or modifying a file loaded in the ide with an external editor.
we cannot detect work on a white board or thought processes of developers which are generally very difficult to quantify.
however in our research questions we are not interested in the absolute time of work processes but in their ratio.
as such it seems reasonable to assume that work outside the ide happens in the same ratio as in the ide.
for example we have no indication to assume that test design requires more planning or white board time than production code.
construct validity concerns errors caused by the way we collect data.
for capturing developers activities we use watchdog described in section .
which we thoroughly tested with endto end integration and developer tests.
moreover students already had used it before the start of our data collection phase .
to verify the integrity of our infrastructure and the correctness of the analysis results we performed end to end tests on linux windows and macos with short staged eclipse sessions starting from the original data collection behavior in eclipse and ending at the analyzed results.
internal validity regards threats inherent to our study.
our population see section .
.
shows no peculiarity like an unusually high number of users from one ip address or from a country where the software industry is weak.
combined with the fact that we use a mild form of security http access authentication we have no reason to believe that our data has been tampered with for example in order to increase the chances of winning a prize .
a relatively small set of power users contribute the majority of development sessions.
however the distribution in figure does not follow a power law distribution the goodness of fit test after clauset et al.
fails to reject that it is at p .
.
this does not mandate an a priori need to further filter or sample sessions or users.
moreover as we are exploring a phenomenon we would run the risk of distorting it through sampling.
since watchdog is freely available we cannot control who installs it.
due to the way we advertise it see section .
.
our sample might be biased towards developers who are actively interested in testing.
the hawthorne effect poses a similar threat participants of our study would be more prone to use run and edit tests than they would do in general because they know that they are being measured and they can preview a limited part of their behavior.
as discussed in section .
.
it was necessary to give users an incentive to install watchdog.
without the preview functionality we would likely not have had any users.
all internal threats point in the direction that our low results on testing are an overestimation of the real testing practices.
external validity threats concern the generalizability of our results.
while we observed over years of development worktime collected in intervals originating from developers over a period of five months the testing practices of particular individuals organizations or companies are naturally going to de187viate from our population phenomenon observation.
our contribution can be understood as an observation of the general state of developer testing among a large corpus of developers and projects.
however we also examined if certain sub groups deviated significantly form our general observations.
as an example of this we identified that only very experienced programmers follow tdd to some extent in section .
.
since other programming language communities have different testing cultures and use other ides that might not facilitate testing in the same way that the eclipse ide does their results might deviate from the relatively mature and test aware java community.
last but not least the time we measure for an activity like testing in the ide does not equal the effort an organization has to invest in it.
arguments against this are that developer testing per hour is as expensive as development since both are done by the same persons and that time is typically the critical resource in software development.
an in depth investigation with management data such as real project costs is necessary to validate this in practice.
our conclusions are drawn from the precisely defined and scoped setting of developer testing.
to draw a complete picture of the state of testing more multi faceted research in different environments and settings is needed.
.
related work a number of tools have been developed to assess development activity at the sub commit level.
these tools include syde spyware codingtracker the change oriented programming environment 8the eclipse usage data collector 9quantifieddev 10codealike 11and the work by minelli et al.
.
however none of these focuses on time related developer testing.
when investigating the presence or absence of tests kochar et al.
mined open source projects and found that contain unit tests .
latoza et al.
surveyed software engineers testers and architects at microsoft with of the respondents indicating to use unit tests.
our findings indicate that only of projects are concerned with testing.
one factor why our figure might be smaller is that we do not simply observe the presence of some tests but that we take into account whether they are actually being worked with.
pham et al.
interviewed computer science students and observed that novice developer perceive testing as a secondary task.
the authors conjectured that students are not motivated to test as they have not experienced its long term benefits.
similarly meyer et al.
found that out of surveyed software engineering professionals perceive tasks such as testing as unproductive .
zaidman et al.
and marsavina et al.
studied when tests are introduced and changed.
they found that test and production code typically do not gracefully co evolve.
our findings confirm this observation on a more fine grained level.
moreover they found that writing test code is phased after a longer period of production code development developers switch to test code.
marinescu et al.
observed that test coverage usually remains constant because already existing tests execute part of the newly added code.
feldt on the other hand notes that test cases grow old if test cases are not updated they are less likely to identify failures.
in contrast pinto et al.
found that test cases evolve over time.
highlight that tests are repaired when the production code evolves but they also found that non repair test modifications occurred nearly four times as frequently as test repairs.
deletions of tests are quite rare and if they happen this is mainly due to refactoring the production code.
a considerable portion of test modifications are for the purpose of augmenting a test suite.
the work presented in this paper differs from the aforementioned works in that the data that we use is not obtained from a software repository or purely by means of a survey or interview .
instead our data is automatically gathered inside the ide which makes it more fine grained than commitlevel activities and more objective than surveys.
.
conclusion our work studies how developers test in their ide.
our goal was to uncover the underlying habits of how developers drive software development with tests.
to this end we performed a large scale field study using low interference observation instruments installed within the developer s working environment to extract developer activity.
we complemented and contrasted these objective observations with surveys of said developers.
we found that testing at least in the ide is not a popular activity that developers do not test as much as they believe they do and that tdd is not a popular development paradigm.
this work makes the following key contributions a low interference method and its implementation to record fine grained activity data from within the developers ides.
a formalized approach to detect the use of tdd.
a thorough statistical analysis of the activity data resulting in both qualitative and quantitative answers in developers testing activity habits test run frequency and time spent on testing.
in general we find a distorting gap between expectations and beliefs about how testing is done in the ide and the real practice.
this gap manifests itself in the following implications software engineers should be aware that they tend to overestimate their testing effort and do not follow test driven development by the book.
this might lead to a lower thanexpected quality in their software.
ide creators could design next generation ides that support developers with testing by integrating solutions from internet fora reminders for developers to execute tests during large code changes automatic test selection and remote testing on the build server.
researchers can acknowledge the difference between common beliefs about software testing and our observations from studying developer testing in the real world.
specifically there is a discrepancy between the general attention to testing and tdd in research and their observed popularity in practice.
more abstractly developers survey answers did not match their behavior in practice and student data deviated significantly from real world observations.
this may have implications on the credibility of certain research methods in software engineering and showcases the importance of triangulation with mixed method approaches.
.