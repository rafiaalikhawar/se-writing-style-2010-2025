modeling readability to improve unit tests ermira daka jos campos and gordon fraser university of sheffield sheffield ukjonathan dorn and westley weimer university of virginia virginia usa abstract writing good unit tests can be tedious and error prone but even once they are written the job is not done developers need to reason about unit tests throughout software development and evolution in order to diagnose test failures maintain the tests and to understand code written by other developers.
unreadable tests are more difficult to maintain and lose some of their value to developers.
to overcome this problem we propose a domain specific model of unit test readability based on human judgements and use this model to augment automated unit test generation.
the resulting approach can automatically generate test suites with both high coverage and also improved readability.
in human studies users prefer our improved tests and are able to answer maintenance questions about them more quickly at the same level of accuracy.
categories and subject descriptors.
d. .
testing and debugging testing tools keywords.
readability unit testing automated test generation .
introduction unit testing is a popular technique in object oriented programming where efficient automation frameworks such as junit allow unit tests to be defined and executed conveniently.
however producing good tests is a tedious and error prone task and over their lifetime these tests often need to be read and understood by different people.
developers use their own tests to guide their implementation activities receive tests from automated unit test generation tools to improve their test suites and rely on the tests written by developers of other code.
any test failures require fixing either the software or the failing test and any passing test may be consulted by developers as documentation and usage example for the code under test.
test comprehension is a manual activity that requires one to understand the behavior represented by a test a task that may not be easy if the test was written a week ago difficult if it was written by a different person and challenging if the test was generated automatically.
how difficult it is to understand a unit test depends on many factors.
unit tests for object oriented languages typically consist of sequences of calls to instantiate various objects bring them to appropriate states and create interactions between them.
the particular permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august september bergamo italy copyright acm isbn ... .
.elementname elementname0 new elementname class object class0 object.class virtualhandler virtualhandler0 new virtualhandler elementname0 class class0 object object0 new object roothandler roothandler0 new roothandler objecthandler virtualhandler0 object0 objecthandleradapter objecthandleradapter0 new objecthandleradapter objecthandlerinterface roothandler0 assertequals objecthandleradapter objecthandleradapter0.getname objecthandleradapter objecthandleradapter0 new objecthandleradapter objecthandlerinterface null assertequals objecthandleradapter objecthandleradapter0.getname figure two versions of a test that exercise the same functionality but have a different appearance and readability.
choice of sequence of calls and values can have a large impact on the resulting test.
for example consider the pair of unit tests shown in figure .
both tests exercise the same functionality with respect to the constructor of the class objecthandleradaptor in the xineo open source project which treats null androothandler0 arguments identically .
despite this identical coverage of the subject class in practice they are quite different in presentation.
in terms of concrete features that may affect comprehension the first test is longer uses more different classes defines more variables has more parentheses and has longer lines.
the visual appearance of code in general is referred to as its readability if code is not readable intuitively it will be more difficult to perform any tasks that require understanding it.
despite significant interest from managers and developers a general understanding of software readability remains elusive.
for source code buse and weimer applied machine learning on a dataset of code snippets with human annotated ratings of readability allowing them to predict whether code snippets are considered readable or not.
although unit tests are also just code in principle they use a much more restricted set of language features for example unit tests usually do not contain conditional or looping statements.
therefore a general code readability metric may not be well suited for unit tests.
in this paper we address this problem by designing a domainspecific model of readability based on human judgements that applies to object oriented unit test cases.
to support developers in deriving readable unit tests we use this model in an automated approach to improve the readability of unit tests and integrate this into an automated unit test generation tool.
we present an analysis of the syntactic features of unit tests and their importance based on human judgement section .
.
a regression model based on an optimized set of features to predict the readability of unit tests section .
.
this is the author s version of the work.
it is posted here for your personal use.
not for redistribution.
the definitive version was published in the following publication esec fse august september bergamo italy c acm.
... a technique to automatically generate more readable tests section .
.
an empirical comparison between code and test readability models section .
.
an empirical evaluation of the test improvement technique section .
.
an empirical evaluation of whether humans prefer the tests optimized by our technique to the non optimized versions section .
.
an empirical study into the effects of readability on test understanding section .
.
analysis of syntactic features of unit tests shows that readability is not simply a matter of the overall test length several features related to individual lines identifiers or entropy have a stronger influence.
our optimized model using of these features results in a model with a correlation of 79with the user data which improves over the ability of general code readability models.
our technique to improve tests succeeds in increasing readability in more than half of all automatically generated unit tests and validation with humans confirms that the optimized tests are preferred.
although we observe a reduction in the time humans spend on understanding these tests our experiments point out that understandability goes beyond readability for example understanding exceptional behavior appears to be more difficult than understanding regular behavior even if a test with exceptions may be more readable.
this suggests potential for future work to improve test understandability.
.
unit test readability metric the source code readability metric by buse and weimer is built on a dataset of human annotator ratings where each code snippet received readability ratings in the range of to .
our aim is to create a predictive model that tells us how readable a given unit test is.
whereas buse and weimer trained a classifier to distinguish between readable and less readable code our aim goes beyond this we would like to use the model to guide test generation in producing more readable tests.
therefore we desire a regression model that predicts relative readability scores for unit tests.
our overall approach begins with producing a dataset of tests annotated with numeric human ratings of readability.
we then identify a range of syntactic features that may be predictive of readability e.g.
identifier length token entropy etc.
.
we then use supervised machine learning to construct a predictive model of test case readability from those features.
to predict the readability of a new test case we calculate its feature values and apply the learned model.
in this paper we use a simple linear regression learner although in principle other regression learners are also applicable e.g.
multilayer perceptron .
however in linear regression the resulting model which consists of weightings for individual features can easily be interpreted and the learning is quick which facilitates the selection of suitable subsets of features.
in this section we describe how we collected the data to learn this model the features of unit tests we considered and the machine learning we applied to create the final model.
.
human readability annotation data both the test cases considered and the human annotators chosen influence the quality of our readability model.
while it is relatively easy to assemble a diverse group of unit tests particular attention must be paid to participant selection and quality in this sort of human study.
for scalability we used crowdsourcing to obtain participants but found that the use of a qualification test is critical for such crowdsourced participants.
supervised learning requires a training set.
in this work we used a number of diverse and indicative open source java projects apachecommons poi trove jfreechart joda jdom itext and guava.
each of these projects comes with an extensive test suite of developerwritten unit tests.
in addition we applied the evosuite unit test generation tool in its default configuration to produce a branch coverage test suite for each of the projects.
training tests were then selected manually with the aim to achieve a high degree of diversity e.g.
short and long tests tests with exceptions if conditions etc.
to collect the human annotator data we used amazon mechanical turk 1and asked crowd workers to rate unit tests on a scale of to using the presentation setup of buse and weimer .
as in previous work annotators were not given a formal definition of what to consider readable and were instead instructed to rate code purely based on their subjective interpretation of readability.
however while the original buse and weimer survey involved undergraduates from the same institution we find that crowdsourcing leads to a much broader diversity of annotator expertise which must be accounted for to learn a useful model.
we began by assessing the utility of general crowd worker responses for this task.
we selected test cases from our eight benchmark projects including developer written as well as automatically generated tests and collected human ratings for these test cases i.e.
each annotator rated test cases .
to evaluate the quality of these responses we measured the inter annotator agreement by calculating the average pearson s correlation between each annotator and the average test scores.
the inter annotator agreement in this data set generated from participants with no expertise requirements or filtering is only weak with a value of .
.
this low correlation could arise for many reasons including an unsuitable choice of test cases or insufficient qualification of the human annotators.
to investigate this issue we manually selected test cases that are examples of either very high quality e.g.
concise well documented well formatted or very low quality e.g.
long complex badly formatted .
the initial selection was made by the first author and this set was refined by iterations with another author retaining only tests with unison agreement.
for these test cases we again gathered human annotator scores and measured the inter annotator agreement.
the results confirmed the need to require annotator expertise.
the agreement was even lower than on the first set of tests the inter annotator agreement in this experiment was only .
which leads us to the conclusion that the more likely explanation is insufficient qualification of the human annotators.
.
final annotation data set based on these pilot studies we designed our final experiment to use a qualification test.
this qualification test consisted of four questions of understanding based on example java code.
only human annotators who correctly answered three out of the four questions were allowed to participate.
our observations that crowdsourced participants can be fruitfully used for such human studies provided that care is taken to avoid participants who are simply trying to game the system is consistent with previous work.
our final experiment gathered data on human and machinewritten test cases ultimately obtaining human readability scores.
we conducted the experiment in stages initially focusing on all tests equally but subsequently gathering additional annotations on particular tests to ensure that each test feature considered see section .
had enough annotations for machine learning purposes.
restricting attention to qualified participants increased the interannotator agreement substantially to .
.
in addition we generated pairs of tests using the evosuite tool such that each pair had the same coverage quality but a different textual representation.
for these pairs of tests we gathered a separate set of forced choice judg1 accessed .
108scoresdensity .
.
.
.
.
.
.
.
.
.
.8figure score distribution for the human test annotation dataset.
ments i.e.
annotators were asked to select which of the two tests were the most readable for use in feature selection see section .
.
figure shows the underlying frequency distribution of readability scores for the tests.
the histogram shows that there are few tests with very high or very low scores and the majority of scores is in the range from .
.
.
we note that our distribution of test readability is quite different from the bimodal distribution of source code readability observed by buse and weimer motivating the need for a test specific model of readability.
.
features of unit tests the readability of a unit test may depend on many factors.
we aggregated features by combining structural logical complexity and density code factors.
all features used by buse and weimer are included in this set as well as the entropy and halstead features used by posnett et al.
.
additionally we included the following new features assertions this feature counts the number of standard junit assertions.
additionally we included a binary feature has assertions which has value if a test case contains assertions and otherwise.
exceptions we propose a feature to measure exceptions since exceptional behavior is handled differently from regular behavior in tests.
as we did not encounter any tests with more than one expected exception we use a binary feature has exceptions .
unused identifiers unit tests are typically short and contain few variables.
in our anecdotal experience with evosuite we found that developers do not prefer tests that define but never use variables.
comments we count the number of lines for single line and multi line ... comments.
however as evosuite and other tools generate comments mainly within the catch block of an expected exception where it shows the error message of the exception we refined the comments feature to two versions one that counts regular comments and one that counts the comments within catch blocks.
comments are removed before calculating code specific features e.g.
features based on numbers classes but included for general presentational features e.g.
line length .
token features we identified several additional common syntactic features not captured in past readability models.
in particular we observe that unit test generation tools often tend to include defensive sometimes redundant casts.
furthermore we propose a feature to count the overall tokens identified by the parser.
finally we refined the single character occurrence feature used by buse and weimer which counts the number of occurrences of different special characters parenthesis quotation marks etc.
with a feature that counts all the special characters single characters .
datatype features different primitive data types have a possible impact on readability.
hence we propose features based on the occurrence of the value null boolean values true and false array accesses type constants e.g.
object.class floating point numbers digits strings characters and the length of strings.
we also added a feature measuring the english ness of string literals using the language model of afshan et al.
.statement features we propose features to count different types of statements in particular constructor calls field accesses and method invocations.
class and method diversity in addition to diversity as captured by entropy features at the level of tokens and bytes we also propose features to capture diversity in terms of the classes methods and identifiers used.
for each of these we include a feature counting the unique number as well as the ratio of unique to total number.
table lists all the candidate features showing which of the features we used in terms of the total value for the unit test the average value per line and its max value in any line in the test case.
in total we considered candidate features in section .
we use feature selection to build our model from the most relevant .
to analyze the influence and predictive power of the individual features table shows the pearson s correlation between each feature and the average test scores ranging from a weak positive correlation of .
for average blank lines to a strong negative correlation of .
for the maximum line length and total identifier length and the result of a one feature at a time analysis.
for the latter we train a linear regression model using only one feature and determine the correlation with fold cross validation.
we also considered a leave one feature out analysis where one measures the effect of a feature by in terms of the difference between a model learned using all features and with all but the feature under consideration however leave one out analyses are not applicable in the presence of feature overlap and due to our very large set of related features the results are not representative.
finally we also applied the relief f method which agrees with the one feature at a time analysis and is thus omitted for brevity.
.
feature discussion considering that a unit test is often simply a sequence of calls one would expect the length of that sequence to be one of the main factors deciding on the readability.
however as shown in table the number of statements test length on its own surprisingly only has weak predictive power.
however other features related to the length have a larger influence on the readability.
in particular the line length plays an important role both in terms of maximum line length as well as the total line length.
the maximum line length presumably is important because a test case can have bad readability even if most lines are short and only a single line is very long.
the total line length essentially amounts to the total number of characters in the test and thus is a better representation of length than the number of statements.
we furthermore observe that the identifiers in a test have a large influence on its readability.
this refers to features related to the number of identifiers their length and their diversity and is a challenge for test generation tools which typically use simple heuristics to derive names for variables.
the diversity in general results in important features for example captured by byte and token entropy .
only a few features are positively correlated with test readability comments have a weak positive correlation as does the ratio of blank lines avg.
blank lines .
surprisingly exceptions also have very weak positive correlation.
we expected assertions to show a strong influence on readability but there is no correlation between assertions and the test score and the predictive power is weak.
the small influence of loops and conditional statements to some extent may be attributed to our choice of test cases for annotation many of the tests are generated by evosuite which generates only tests that are sequences of calls.
the manually written tests with loops and conditional statements included in the dataset tend to be short and well formatted contributing to the small but positive influence of these features.
109table predictive power of features based on correlation and one feature at a time analysis and optimized regression model.
correlation one feature a time correlation one feature a time feature name total max avg total max avg feature name total max avg total max avg identifier length .
.
.
.
.
.
commas .
.
.
.
.
.
line length .
.
.
.
.
.
halstead difficulty .
constructor calls .
.
.
.
has exceptions .
.
byte entropy .
.
identifier ratio .
.
unique identifiers .
.
.
.
.
.
method invocations .
.
.
.
.
.
identifiers .
.
.
.
.
.
string length .
.
.
.
.
.
assignments .
.
.
.
arrays .
.
.
.
.
.
casts .
.
.
.
.
.
indentation .
.
.
.
.
.
parentheses .
.
.
.
field accesses .
.
.
.
.
.
keywords .
.
.
.
.
.
halstead effort .
.
halstead volume .
.
assertions .
.
.
.
distinct methods .
.
.
.
.
.
additional assertions .
.
single characters .
.
.
.
.
.
nulls .
.
.
.
.
.
periods .
.
.
.
.
.
class ratio .
.
comparison operations .
.
.
.
blank lines .
.
.
.
tokens .
.
.
.
.
.
unused identifiers .
.
digits .
.
.
.
.
.
string score .
.
token entropy .
.
strings .
.
.
.
.
.
floats .
.
.
.
.
.
excep.
comments .
.
.
.
comments .
.
.
.
arithmetic operations .
.
.
.
test length .
.
branches .
.
.
.
numbers .
.
.
.
.
.
types .
.
.
.
.
.
spaces .
.
.
.
has assertions .
.
loops .
.
.
.
method ratio .
.
booleans .
.
.
.
.
.
characters .
.
.
.
.
.
test score total line length max line length total identifiers total identifier length max identifier length avg identifier length avg arithmetic operations avg branches avg loops total assertions total has assertions avg characters total class instances total distinct methods avg string length total has exceptions total unique identifiers max nulls total numbers avg nulls total identifier ratio total method ratio total floats total byte entropy .
feature selection feature selection is a widely used technique that reduces the dimension of a dataset with respect to a given value .
selecting a subset of potential inputs from the total feature set can help on compacting relevant information and removing the noise in prediction .
to improve the learning process and the generality of the resulting model it is thus desirable to reduce the number of features using feature selection techniques.
feature selection techniques are classified in two main categories called filter and wrapper models.
a filter model typically consists of removing features that are shown to have low predictive power.
for example as a baseline we considered the use of correlation and the relief f filter model method which select and features leading to a correlation of .
and .
respectively.
we desire a higher quality feature set however and thus focus on wrapper model feature selection.
a wrapper model selects subsets of variables considering the learning method as a black box and scoring inputs based on their predictive power.
we considered forward and backward feature selection in forward selection one starts from an empty set of features and iteratively adds features based on the resulting predictive power.
in backward selection the starting point is the full set of features and one iteratively removes individual features.
we used a steepest ascent hill climbing algorithm to perform this feature selection.
that is for forward selection we start with a randomly chosen feature create a regression model using only that feature and calculate the correlation using fold cross validation.
then for each other feature we determine the correlation of a model trained using this and the first feature.
the pair with the highest correlation is the new starting point and we explore all possible variants to add another feature.
this is done iteratively until there exists no feature that can be added while increasing thecorrelation value.
in our experiments forward feature selection achieved substantially better results than backward feature selection and is used for our model.
to avoid overfitting the model to the training set in addition to standard cross validation we used the set of pairs of test cases with human annotation data described in section .
.
for each of the tests in a pair we predicted the readability score and then ranked the paired tests based on their score represented with or .
then we measured the agreement between the user preference i.e.
or depending on whether more human annotators preferred the first or second test in the pair using pearson s correlation.
we applied the forward feature selection times see figure 3a and the best configuration consisting of features achieved a correlation of .
however when measuring user agreement the correlation of the same configuration is only which suggests a certain degree of overfitting to the training data.
to counter this we re ran feature selection but rather than using the correlation to guide the hill climbing we used the sum of correlation and user agreement.
as figure 3b shows this reduces the achieved correlation slightly but increases the agreement substantially.
the most frequently selected feature is total identifier length which occurred in of all runs the second most frequently selected feature is max line length of runs showing the importance of this feature.
interestingly total exceptions was selected in of the runs suggesting that although the predictive power of this feature on its own is not so strong it appears independent from other features.
byte entropy is selected more frequently than token entropy which suggests that token entropy is correlated with other features.
in the end the overall best configuration consists of features shown in bold in table and the resulting regression model is shown below the table.
this combination of features achieves a correlation agreement0.
.
.
.
.
a selection using correlation correlation agreement0.
.
.
.
.
b selection using correlation and agreement figure results of the feature selection measured in terms of pearson s correlation with fold cross validation and agreement with user preferences for test pairs.
algorithm test case optimization require test case t coverage objective c ensure optimized test case t0 procedure optimize t c t genalternatives t c length t t0 select highest ranked t0int return t0 procedure genalternatives t c start t ftg forp start down to 1do s statement at position pint for all s02get all replacements for sdo t0 replace swiths0int ift0satisfies cthen t t genalternatives t0 c p return t correlation of 79with a root relative squared error rate of .
and has a high user agreement .
.
generating readable tests given a predictive model of test readability we would now like to apply this model to improve automated unit test generation.
as search based testing is a common technique to generate unit tests in principle it would be possible to simply include the readability prediction as a second objective in a multi objective optimization and thus optimize tests towards both coverage and readability at the same time.
however there is a dichotomy between the need for search techniques to include redundancy in the tests to explore the state space i.e.
statements that do not contribute to the code coverage and the detrimental effects of this redundancy on the readability.
therefore we use a post processing technique to optimize unit tests which has the additional benefit that it is independent of the underlying test generation technique allowing our approach to apply to any such black box unit test generator.
algorithm describes this post processing algorithm we assume a test case tis a sequence of statements t hs1 s2 s liof length l where each statement is either a method call a constructor call some other form of assignment e.g.
primitive values public fields arrays etc.
or an assertion.
the algorithm is given a test casetgenerated for coverage obligation c. it is assumed that tis minimized with respect to c that is removing any of the statements intmeans that cis no longer satisfied.
given these inputs we generate the set of alternative versions of tthat still satisfy cas follows we iterate over the statements in the test from the last statement to the first statement line .
for each statement we determine the possible set of replacement statements line consisting of all possible method or constructor calls thatgenerate the same return type.
this restriction ensures that the variable defined at the statement if any still exists after the replacement.
we only consider replacements for statements calling constructors or methods but in the future other types of transformations could also be integrated.
any additional parameters of the replacement call are assigned randomly chosen existing variables of the desired types the value null or if no variable of the required type exists then an instance can also be generated by recursively inserting a random generator for that type and satisfying its dependencies e.g.
based on the statement inseration in e vosuite .
for each candidate replacement t0we determine if it still satisfies coverage objective cby executing t0and observing its coverage.
if it does then we recursively apply the replacement algorithm to t0starting at the position preceding the modified statement and keep all valid replacements.
in the end tcontains the set of valid replacements for tthat still satisfy c. the tests in tare then sorted by readability and the most readable test in tis selected.
for example consider the first test case in figure which was generated to cover the constructor of objecthandleradapter alternative generation would start with the last statement which is an assertion and thus is not modified.
the next statement considered is the constructor call.
the class objecthandleradapter has four different constructors but as the one called in the original test is the coverage objective of the test replacements with the three other constructors no longer satisfy this objective and are discarded.
because the algorithm is randomized it also attempts to replace the constructor call with a new parameter assignment.
assume it satisfies the parameter with a null reference the coverage obligation is still satisfied and minimization can now remove the first five statements of this alternative as they are no longer used in the constructor call and thus not needed in order to satisfy the coverage goal.
the new test has no more statements to modify so no further alternatives can be generated from this test and the algorithm continues generating alternatives with the next statement of the original statement which is the constructor call of roothandler .
in the end the alternative that calls the constructor with a null value has the highest readability value .
vs. .
for the original test and is chosen as replacement.
.
empirical ev aluation this section contains an empirical evaluation of default test cases i.e test cases generated with default parameters and test cases optimized for readability.
in particular we empirically aim to answer the following research questions rq1 how does the test readability metric compare to code readability metrics?
rq2 can our test readability metric guide improvement of generated unit tests?
rq3 do humans prefer readability optimized tests?
rq4 does readability optimization improve human understanding of tests?
.
experimental setup .
.
unit test generation tool we have implemented the algorithm described in section .
in the evosuite tool for automatic unit test generation.
evosuite uses search techniques to derive test cases with the aim to maximize coverage of a chosen target criterion e.g.
line coverage or branch coverage .
after the generation evosuite applies several post processing steps to improve readability for each individual coverage objective e.g.
branch a minimized test case is generated that is removing any statement from the test will lead to the coverage objective no longer being satisfied.
in these minimized tests primitive values are inlined to reduce the number of variables and 111then the primitive values are minimized i.e.
strings are shortened and numbers are decremented as close as possible to without violating the coverage objective .
finally assertions are added to the tests and minimized using an approach based on mutation analysis .
the readability optimization algorithm from section .
was integrated as a further step of this chain of optimizations.
.
.
experiment procedure for rq1 we used the public dataset by buse and weimer our dataset of annotated test cases section .
and the set of test pairs used to support feature selection section .
and measured the correlation and agreement of different readability models.
for rq2 we manually selected classes from open source projects with the criteria that they are testable by evosuite with at least code coverage do not exhibit features currently not handled by evosuite s default configuration such as gui components and have less than non comment source statements ncss and few dependencies such that they are non trivial yet understandable in a reasonable amount of time.
for each of the chosen classes we generated tests for each coverage objective i.e.
branch to account for the randomness of the test generation approach with and without the readability optimization introduced in this paper.
furthermore we generated an additional test cases per branch per class with both configurations but modified evosuite to generate failing assertions i.e.
during the assertion generation using mutation analysis the assertions were chosen to pass on the mutants rather than the original class .
to answer rq2 we compare the default and optimized tests in terms of their readability score as predicted by our test readability model.
we used the wilcoxonmann whitney statistical symmetry test and the vargha delaney aabstatistics to evaluate the significance of the optimization .
for rq3 we selected three random pairs of tests for each class from the rq2 dataset each pair consisting of one test generated with and one without the readability optimization both cover the same branch and they differ in readability score.
this resulted in a total of pairs of tests and we used a forced choice questionnaire on amazon mechanical turk see section .
to determine for each pair which test is preferred by users.
for rq4 we selected out of the classes with large differences in readability of its tests and for each class chose either a pair of passing or failing tests.
note that our procedure to generate failing tests did not guarantee failing tests hence the pass or fail status within pairs is not always identical.
to recruit students for rq4 we invited all computer science students undergraduate and postgraduate at the university of sheffield and asked them to perform a pre qualification quiz.
this quiz consisted of the four questions from the mechanical turk qualification plus one junit specific question and we selected students who answered at least questions correctly.
the experiment was conducted in the computer lab of the university s department of computer science.
all selected participants received a short introduction to the experiment and then answered questions in a web browser based quiz.
each question showed a test case and provided the source code of all classes required by the test case and asked the students to select if the test would pass or fail.
after minutes the students were asked to submit the answers they had produced up to that point filled in a short survey and were paid a fee of gbp10.
.
.
threats to validity construct for rq4 we use time and correctness of pass fail decision to measure understanding.
it is possible that using a different task that requires understanding would give a different result.
for example ceccato et al.
reported a positive effect when using random tests during debugging.
.
.
.
.
.
.
buse weimer simpler testcode dataset test datasetfigure fold cross validation of code and test readability models using different learners and data sets.
internal for all experiments involving humans the tests were assigned randomly.
to avoid learning effects for rq4 we ensured that no two tests shown to one participant originate from the same project.
participants without sufficient knowledge of java and junit may affect the results to avoid this problem we only accepted subjects who passed a qualification test.
experiment objectives may have been unclear to participants at least for rq4 to counter this threat we tested and revised all our material on a pilot study and interacted with the students during experiment to ensure they understood the objectives.
external all our experiments are based on either amazon mechanical turk users or students and thus may not generalize to all developers .
the set of target classes used in the experiment is the result of a manual but systematic selection process aiming to find classes that are understandable in the short duration of the expriment rq4 .
the chosen classes are not small but it may be that the readability optimization is more important for classes with more dependencies.
thus to which extent our findings can be generalised to arbitrary programming and testing tasks remains an open question.
we used evosuite for experiments and to support the generation of our data set and tests produced by evosuite and other tools may lead to different results.
however the output of evosuite is similar to that of other tools aiming at code coverage.
conclusion the human study to answer rq4 involved human subjects which resulted in significance in only two out of the test pairs.
however obtaining more responses per test pair by reducing the number of pairs was not possible as this would have implied that students would have to answer several questions related to the same class which would have led to undesired learning effects.
.
rq1 test vs. code readability as a baseline for the success of our domain specific readability model we used the code readability model by buse and weimer as well as the extended version by posnett et al.
.
both models are originally classification models and we replicated them as regression models.
we created two versions for each model one trained with our dataset of test cases with human annotations and buse and weimer s original dataset of code snippets with human judgments.
this allows us to distinguish between the effects of the choice of features and the training data.
figure shows the performance using seven different learners linear regression multilayer perceptron smoreg m5rules m5p additive regression bagging on the buse weimer code readability model posnett et al.
s simpler model which includes halsteadand entropy based features and our test readability model in terms of the correlation using fold cross validation.
the buse weimer and simpler models both perform better on code snippets than on test snippets.
in contrast our test model shows a poor performance on the code dataset while achieving the overall highest correlation on the test dataset with a median value .
which 112table model prediction agreement with user choices each model is trained with two available datasets and tested with pairs of test cases.
agreement shows the percentage of choices predicted by model agreeing with the overall user choice.
cohen s kappa shows inter rater agreement between the model prediction and user choices.
code dataset test dataset cohen s cohen s model kappa p value agreement kappa p value agreement buse weimer .
.
.
.
.
.
simpler .
.
.
.
.
.
test readability .
.
.
.
.
suggests that our choice of features is well adapted to the specific details influencing test readability.
these results demonstrate the importance of our domain specific model of software readability.
to compare these models with respect to their agreement with human judgement we used the dataset of pairs section .
and measured the agreement between each model and the majority preference of the human annotators i.e.
percentage of matching choices .
table shows the tests used to calculate the model user agreement.
our test readability model trained with test case snippets achieves a high inter rater agreement kappa .
p value with a correctness ratio of .
over test pairs.
as the table shows our test readability model significantly outperform previous code readability models at this task.
rq1 our test readability model performs better on test snippet datasets achieving a higher agreement with human annotators than previous work kappa .
.
rq2 improved test generation to evaluate the success of the test optimization technique we applied test generation to classes with repetitions each with and without optimization and compared the tests per branch in terms of their readability score.
that is each pair of tests is generated for the same coverage objective but differs in readability score.
we find that of test cases on which optimization was applied had at least one alternative to choose from for these tests on average there were .
alternatives.
table summarizes the overall results on all but three classes there is a significant increase of the readability score by an average of .
.
when considering only tests that had alternatives .
this increase may seem small but recall that readability scores have low variance cf.
the narrow range of values between .
and .
in figure and the syntactic differences for improvement steps are small and incremental.
the largest improvement is observable for classes where the largest numbers of alternatives can be generated.
as our algorithm see algorithm is based on varying method and constructor calls generation of alternatives works best when the dependency classes have many different constructors and parameters.
furthermore we observe that more alternatives are generated for classes with more and simpler methods.
for example math3.complex.
complex has branches but these are spread over methods and this results in the overall largest readability improvement .
with alternatives per test on average.
similar examples are the class org.joda.time.months andbeanutils.
locale.converters.datelocaleconverter .
the first one has five constructors plus many methods that also return months instances and the second has twelve different constructors.
this leads to a large number of alternatives for months and fordatelocaleconverter and readability improvements of .
and .
respectively.
on the other hand class cli.option has three constructors and methods that mainly take primitive values as parameters and thus results in just .
alternative tests on average with a significant readability improvement of .
.
an extreme example isgiven by class codec.language.metaphone where the number of alternatives is close to .
see table .
codec.language.
metaphone is one of the largest classes with branches but of those branches are in the same method public string metaphone string txt ... which receives and returns a string object.
therefore there is no other alternative of creating a metaphone string without calling that method.
for this specific class a better way to optimize readability may be by directly optimizing the strings using a language model .
rq2 alternatives were generated for of the unit tests resulting in a readability improvement of .
on average.
.
rq3 do humans prefer readability optimized tests?
to evaluate whether humans prefer the readability optimized test cases to the default tests generated by evosuite we applied test generation to classes with and without optimization.
for each class we chose the three pairs of tests with the largest difference in readability score and used a forced choice survey to let human annotators select which test cases they think are more readable.
that is for each pair both tests cover the same branch of the same class but differ only in their readability score.
table shows the details of the pairs used for this experiment and shows the joint probability of agreement which is overall.
on average for all classes there is a preference for the optimized tests.
the average pair wise agreement fraction of pairs rated by both raters on which they agree is a one sided wilcoxon signed rank shows that this is significantly better p than a random choice assuming agreement of .
for random choices .
the highest agreement is observed for the first pair for class net.n3.nanoxml.xmlelement .
here the optimized test expects an exception and has only three statements.
in contrast the default test has four statements and six assertions expects no exception and uses random strings e.g.
7i d7w5y ta .
in class org.magee.math.rational there are two pairs for which the users prefer the default test case.
for pair the optimized test expects a nullpointerexception and we have generally observed that exceptional tests tend to get slightly higher readability scores.
however in this case the users disagree possibly influenced by the rather meaningless comment in the catch block stating that there is no message .
for pair the tests are very similar and it is possible that domain knowledge influences the choice the default test subtracts a number from itself whereas the optimized test performs a syntactically similar but mathematically slightly more complex calculation.
rq3 our experiment showed an agreement of between human annotators and the readability optimization.
.
rq4 does readability optimization improve human understanding of tests?
table summarizes the results of the controlled experiment to answer rq4.
for seven out of the ten classes the time participants required to make a decision about the pass fail status of a test was lower for the optimized tests.
the average time spent on the nonoptimized tests was .
minutes compared with minutes for the optimized tests.
overall this suggests that improved readability helps when making this software maintenance decision.
on the other hand there are five classes where the ratio of correct responses increases and five where the ratio decreases suggesting that there are other factors influencing the difficulty of understanding a test that are not captured by our readability model.
for example the tests for class cli.option have a substantial difference in readability .
default .
optimized but the default one con113table readability value for the classes selected based on runs per branch.
for each class we report its version the number of branches and the readability value using default configuration.
for the optimized configuration we also report its readability value the effect sizes a12and relative average improvement compared to the default configuration and average number of optimized alternatives generated.
effect sizes a12 that are statistically significant are reported in bold.
readability relative average number class version branches default optimized a12 improvement of alternatives java2.util2.bitset .
.
.
.
.
net.n3.nanoxml.stdxmlreader .
.
.
.
.
.
.
net.n3.nanoxml.xmlelement .
.
.
.
.
.
.
net.xineo.xml.handler.objecthandleradapter .
.
.
.
.
.
.
nu.xom.attribute .
.
.
.
.
.
.
org.apache.commons.beanutils.locale.converters.datelocaleconverter .
.
.
.
.
.
.
org.apache.commons.chain.impl.chainbase .
.
.
.
.
.
org.apache.commons.cli.commandline .
.
.
.
.
.
org.apache.commons.cli.option .
.
.
.
.
.
org.apache.commons.cli.posixparser .
.
.
.
.
.
org.apache.commons.codec.language.metaphone .
.
.
.
.
.
org.apache.commons.codec.language.soundex .
.
.
.
.
.
org.apache.commons.collections4.comparators.comparatorchain .
.
.
.
.
org.apache.commons.collections4.comparators.fixedordercomparator .
.
.
.
.
org.apache.commons.collections4.iterators.filteriterator .
.
.
.
.
org.apache.commons.collections4.iterators.filterlistiterator .
.
.
.
.
org.apache.commons.collections.primitives.arrayintlist .
.
.
.
.
org.apache.commons.configuration.tree.mergecombiner .
.
.
.
.
.
org.apache.commons.digester3.plugins.pluginrules .
.
.
.
.
.
org.apache.commons.digester3.rulesbase .
.
.
.
.
.
org.apache.commons.lang3.charrange .
.
.
.
.
.
.
org.apache.commons.math3.complex.complex .
.
.
.
.
.
.
org.apache.commons.math3.fraction.fraction .
.
.
.
.
.
.
org.apache.commons.math3.genetics.listpopulation .
.
.
.
.
.
.
org.apache.commons.math3.stat.clustering.dbscanclusterer .
.
.
.
.
.
.
org.jdom2.attribute .
.
.
.
.
.
.
org.jdom2.doctype .
.
.
.
.
.
.
org.joda.time.months .
.
.
.
.
.
org.joda.time.yearmonthday .
.
.
.
.
.
org.magee.math.rational .
.
.
.
.
average .
.
.
.
.
table readability value for the classes selected based on the top three pairs that maximise the difference between default and optimized configurations.
for each class and pair we report the readability value of each configuration and the percentage of users that agree optimized test cases are better in terms of readability.
pair pair pair readability readability readability average class default optimized agree default optimized agree default optimized agree agree java2.util2.bitset .
.
.
.
.
.
.
.
.
.
net.n3.nanoxml.stdxmlreader .
.
.
.
.
.
.
.
.
.
net.n3.nanoxml.xmlelement .
.
.
.
.
.
.
.
.
.
net.xineo.xml.handler.objecthandleradapter .
.
.
.
.
.
.
.
.
.
nu.xom.attribute .
.
.
.
.
.
.
.
.
.
org.apache.commons.beanutils.locale.converters.datelocaleconverter .
.
.
.
.
.
.
.
.
.
org.apache.commons.chain.impl.chainbase .
.
.
.
.
.
.
.
.
.
org.apache.commons.cli.commandline .
.
.
.
.
.
.
.
.
.
org.apache.commons.cli.option .
.
.
.
.
.
.
.
.
.
org.apache.commons.cli.posixparser .
.
.
.
.
.
.
.
.
.
org.apache.commons.codec.language.metaphone .
.
.
.
.
.
.
.
.
.
org.apache.commons.codec.language.soundex .
.
.
.
.
.
.
.
.
.
org.apache.commons.collections4.comparators.comparatorchain .
.
.
.
.
.
.
.
.
.
org.apache.commons.collections4.comparators.fixedordercomparator .
.
.
.
.
.
.
.
.
.
org.apache.commons.collections4.iterators.filteriterator .
.
.
.
.
.
.
.
.
.
org.apache.commons.collections4.iterators.filterlistiterator .
.
.
.
.
.
.
.
.
.
org.apache.commons.collections.primitives.arrayintlist .
.
.
.
.
.
.
.
.
.
org.apache.commons.configuration.tree.mergecombiner .
.
.
.
.
.
.
.
.
.
org.apache.commons.digester3.plugins.pluginrules .
.
.
.
.
.
.
.
.
.
org.apache.commons.digester3.rulesbase .
.
.
.
.
.
.
.
.
.
org.apache.commons.lang3.charrange .
.
.
.
.
.
.
.
.
.
org.apache.commons.math3.complex.complex .
.
.
.
.
.
.
.
.
.
org.apache.commons.math3.fraction.fraction .
.
.
.
.
.
.
.
.
.
org.apache.commons.math3.genetics.listpopulation .
.
.
.
.
.
.
.
.
.
org.apache.commons.math3.stat.clustering.dbscanclusterer .
.
.
.
.
.
.
.
.
.
org.jdom2.attribute .
.
.
.
.
.
.
.
.
.
org.jdom2.doctype .
.
.
.
.
.
.
.
.
.
org.joda.time.months .
.
.
.
.
.
.
.
.
.
org.joda.time.yearmonthday .
.
.
.
.
.
.
.
.
.
org.magee.math.rational .
.
.
.
.
.
.
.
.
.
average .
114table human understanding results of tests for the classes randomly selected.
for each class selected we report the branch covered the test result of each individual pass fail number of asserts or fail keywords average time to answer and percentage of correct responses.
effect sizes of statistically significant differences p .
are shown in bold.
oracle readability assert fail time min correct answers class branch def.
optim.
def.
optim.
def.
optim.
def.
optim.
a12 def.
optim.
net.n3.nanoxml.stdxmlreader pass fail .
.
.
.
.
.
.
nu.xom.attribute pass pass .
.
.
.
.
.
.
org.apache.commons.chain.impl.chainbase pass pass .
.
.
.
.
.
.
org.apache.commons.cli.option fail pass .
.
.
.
.
.
.
org.apache.commons.collections4.comparators.fixedordercomparator pass pass .
.
.
.
.
.
.
org.apache.commons.collections4.iterators.filterlistiterator fail fail .
.
.
.
.
.
.
org.apache.commons.digester3.plugins.pluginrules pass pass .
.
.
.
.
.
.
org.apache.commons.digester3.rulesbase pass pass .
.
.
.
.
.
.
org.apache.commons.lang3.charrange pass pass .
.
.
.
.
.
.
org.joda.time.yearmonthday pass pass .
.
.
.
.
.
.
average .
.
.
.
.
.
.
buddhistchronology buddhistchronology0 buddhistchronology.getinstance yearmonthday yearmonthday0 new yearmonthday chronology buddhistchronology0 yearmonthday.property yearmonthday property0 new yearmonthday.property yearmonthday0 yearmonthday yearmonthday1 yearmonthday property0.
addwrapfieldtocopy assertequals yearmonthday1.tostring assertequals yearmonthday0.tostring assertequals yearmonthday property0.
getminimumvalueoverall yearmonthday yearmonthday0 null try yearmonthday0 new yearmonthday fail expecting exception illegalfieldvalueexception catch illegalfieldvalueexception e value for monthofyear must not be smaller than figure default and optimized test case for class org.joda.
time.yearmonthday .
tains regular assertions whereas the optimized one expects an exception to be thrown.
while all responses for the non optimized test were correct only of the responses for the optimized test were correct and the average time for responses increased from .21min to .69min.
the likely explanation for this is that even though the readability model suggests that exceptions improve readability it may be more difficult to understand exceptional control flow.
this conjecture is supported by the tests for class org.joda.
time.yearmonthday see figure where again the optimized test leads to an expected exception.
here the time to response increases from .06min to .49min on average.
however in contrast tocli.option the percentage of correct responses increases by .
possibly this improvement is influenced by the error message of the expected exception which is included as a comment the optimized test calls the constructor of class yearmonthday day month year with value and the exception message was value for monthofyear must not be smaller than .
the third class with an increase in time nu.xom.attribute tests exceptional behavior in both versions.
here the percentage of correct responses is only compared to in the default version.
this reduction may again be related to the specific error message as the default test complains about the use of an illegal initial scheme character in a uri parameter which apparently is easier to understand than the missing scheme in absolute uri reference message of the optimized test.
both default and optimized test expect a null pointer exception for class net.n3.nanoxml.stdxmlreader .
although thecatalogfactorybase catalogfactorybase0 catalogfactorybase catalogfactory.getinstance dispatchlookupcommand dispatchlookupcommand0 new dispatchlookupcommand catalogfactory catalogfactorybase0 hashmap object copycommand hashmap0 new hashmap object copycommand contextbase contextbase0 new contextbase map hashmap0 set set0 contextbase0.keyset chainbase chainbase0 new chainbase collection set0 removecommand removecommand0 new removecommand command commandarray0 new command commandarray0 command removecommand0 commandarray0 command dispatchlookupcommand0 commandarray0 command dispatchlookupcommand0 commandarray0 command chainbase0 chainbase chainbase1 new chainbase commandarray0 assertfalse chainbase1.equals object chainbase0 chainbase chainbase0 new chainbase command commandarray0 chainbase0.getcommands chainbase chainbase1 new chainbase commandarray0 assertnotsame chainbase1 chainbase0 figure default and optimized test case for class chain.
impl.chainbase .
rate of correct responses is comparable here the time spent on the optimized test is lower on average .35min default vs .76min optimized .
this is likely related to the difference in size the default test has seven statements the optimized one only two.
forcomparators.fixedordercomparator the reduction of correct responses may be a result of uncertainty arising from how null values are handled in the maps underlying the class the default test case does not use null the optimized one does.
the same may also hold for digester3.plugins.pluginrules where the optimized test case uses several null values.
these results are reflected by the participants opinions.
in free response questions at the end of our survey almost every user stated that a readable test case must be minimal no more than lines if possible have short lines not dependent on too many classes as for example indicated by the comment that due to deep inheritance and lots of underlying methods to construction it was somewhat hard to understand some classes under test.
forchain.impl.chainbase the slight reduction in correct responses is surprising as the optimized test case has a trivially true assertion assertnotsame with two objects resulting from two different constructor invocations cf.
figure .
however this assertion form is less common and might have been interpreted by several participants as the more common assertnotequals which may contribute to the increase in wrong responses.
finally for filterlistiterator andcharrange there are large improvements in the response time and in the correctness 115charrange charrange0 charrange.isnot character character0 character.valueof charrange charrange1 charrange.isnotin char character0 char char0 charrange1.getstart assertequals char0 boolean boolean0 charrange0.contains asserttrue boolean0 charrange charrange0 charrange.is boolean boolean0 charrange0.contains asserttrue boolean0 figure default and optimized test case for lang3.
charrange .
of responses.
for filterlistiterator the default test case uses a confusing chain of calls to construct the instance of the class under test for the charrange default test case this also holds but here maybe a different factor also plays a role.
as we can see in figure the default test case starts by constructing a negated charrange over a single character and later checks whether a different character is contained in the charrange whereas the optimized test case uses the same character twice when setting up thecharrange and when querying contains .
rq4 in our experiments the optimized tests reduced the response time by but did not directly influence participant accuracy.
.
related work readability metrics.
buse and weimer introduced a metric for code readability based on human judgements.
they collected human annotation data for code snippets and trained a classifier based on those scores.
our work is based on the same readability metric concept but produces a domain specific model for unit tests using dedicated test features and data resulting in an overall better performance and prediction power.
posnett et al.
used the same dataset to learn a simpler model of code readability using fewer features based on size halstead metrics and entropy.
we replicated this model as a regression model and applied it to test data and saw that the performance of our test specific model is still better.
readability optimization of tests.
the problem of understanding tests is well known.
the most common scenario where understanding a test is necessary is if a test fails.
to support developers with debugging failing tests leitner et al.
and lei and andrews minimised failing randomly generated tests using delta debugging in order to simplify debugging the failure cause.
zhang et al.
presented an approach to synthesise natural language documentation to explain the failure.
often a failing test is not a problem in the program that needs fixing but a maintenance problem in the test requiring a fix in the test code.
robinson et al.
discuss a range of optimizations to apply during test generation in order to reduce the number of false failures produced by tests and also improve their readability.
daniel et al.
and mirzaaghaei et al.
provide automated techniques to fix failing tests and hao et al.
use machine learning to predict whether a test failure is due to a code or test problem.
to avoid creating maintainability problems when writing tests there are established standard patterns for test design and there are certain test smells that help to detect problematic tests.
however the most discussed scenario related to understandability of tests is when the tests are generated automatically.
automated test generation typically uses systematic approaches for example to derive tests satisfying coverage criteria.
in order to find faults automatically generated tests require a test oracle that is some mechanism that decides whether the test execution revealed a failure or matched the expected behavior.common approaches to address the test oracle problem include an optimization of the number of tests generated in the first place .
however there have also been attempts to improve the readability of tests systematically fraser and zeller learn models of common object usage from existing code and tests such that newly generated tests match the developer expectations.
afshan et al.
apply a natural language model to optimize textual input values which are often simply random characters to more english like text.
we included this model as one of our features but found low predictive power for our dataset possibly including more string related tests would change this.
zhang describes a transformation that replaces variables with the aim to reduce the number of statements which works best on un minimized tests.
in contrast our optimization algorithm works with already minimized tests and integrates test generation mechanisms to replace calls and satisfy new dependencies.
fraser and zeller reduce the number of assertions in a test using mutation analysis.
xuan and monperrus split tests into one per assertion while the aim of this approach is to improve fault localization this likely also has an influence on readability.
.
conclusions a unit test may be written once but it is read and interpreted by developers many times.
if a test is not readable then it may be more difficult to understand it.
this problem is exacerbated for unit tests generated automatically by tools which in principle are intended to support developers in generating high coverage test suites but in practice tend to generate tests that do not look as nice as manually written ones.
to address this problem we have built a predictive model of test readability based on data of how humans rate the readability of unit tests.
we have applied this model in an automated unit test generation tool and validated that users prefer our readability optimized tests to non optimized tests.
our technique to increase the readability of unit tests is still quite limited in the scope of its changes to test appearance.
for example variable names are chosen according to a fixed strategy i.e.
class name in camel case with lower caps first letter and numeric id attached at the end .
our feature analysis suggests that identifiers have a very strong influence on readability and indeed the participants of our experiment mentioned the choice of variable names repeatedly in the post experiment survey.
this confirms previous research showing the importance of identifier names in source code and suggests that future work will need to address this problem for unit test generation leveraging existing work on improving identifiers e.g.
.
our experiment about the effects of readability on test understanding has also demonstrated the boundaries between readability and understandability not all tests that look nice are also easy to understand.
the inclusion of semantic features such as code coverage may lead to an improvement of the readability model.
it is even conceivable to use our human maintenance question data to learn a model of understandability rather than readability.
in summary we proposed a domain specific model of test readability and an algorithm for producing more readable tests.
we found our model to outperform previous work in term of agreement with humans on test case readability .
vs. .
we found our approach to generate alternate optimized tests that were more readable on average we found humans to prefer our optimized tests of the time and we found that humans can answer questions about our tests faster with no change in accuracy.
.