floating point precision t uning using blame analysis cindy rubio gonz alez1 cuong nguyen2 benjamin mehne2 koushik sen2 james demmel2 william kahan2 c o s t i ni a n c u3 wim lavrijsen3 david h. bailey3 a n dd a v i dh o u g h4 1university of california davis crubio ucdavis.edu 2university of california berkeley nacuong bmehne ksen demmel wkahan cs.berkeley.edu 3lawrence berkeley national laboratory cciancu wlavrijsen dhbailey lbl.gov 4oracle corporation david.hough oracle.com abstract while tremendously useful automated techniques for tuning the precision of floating point programs face importantscalability challenges.
we present blame analysis a novel dynamic approach that speeds up precision tuning.
blame analysis performs floating point instructions using different levels of accuracy for their operands.
the analysis deter mines the precision of all operands such that a given preci sion is achieved in the final result of the program.
our evaluation on ten scientific programs shows that blame analysisis successful in lowering operand precision.
as it executes the program only once the analysis is particularly useful when targeting reductions in execution time.
in such case the analysis needs to be combined with search based tools such as precimonious.
our experiments show that combining blame analysis with precimonious leads to obtaining better results with significant reduction in anal ysis time the optimized programs execute faster in three cases we observe as high as .
program speedup and the combined analysis time is faster on average and up to faster than precimonious alone.
ccs concepts software and its engineering dynamic analysis software performance formalsoftwareverification software testing and debugging mathematics of computing numerical analysis keywords floating point mixed precision program optimization the first two authors contributed equally to this paper.
acm acknowledges that this contribution was authored or co authored by an employee or contractor of the national government.
as such the government retains a nonexclusive royalty free right to publish or reproduce this article or to allow others to do so for government purposes only.
permission to make digital or hard copiesfor personal or classroom use is granted.
copies must bear this notice and the full ci tation on the first page.
copyrights for components of this work owned by others thanacm must be honored.
to copy otherwise distribute republish or post requires priorspecific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c circlecopyrt2016 acm.
isbn .
.
.
.
introduction algorithmic or automated program transformationtechniques totunetheprecisionoffloating point variables in scientific programs have been shown to significantly improve execution time.
developers prescribe the accuracy required for the program result and the tools at tempt to maximize the volume of data stored in the lowestnative precision.
generally this results in improved memorylocality and faster arithmetic operations.
since tuning floating point precision is a black art that requires both application specific and numerical analysis ex pertise automated program transformation tools are clearlydesirable and they have been shown to hold great promise.
state of the art techniques employ dynamic analyses that searchthroughtheprograminstructionspace orthroughthe program variable data space .
due to the empiricalnature a quadratic or worse in instructions or variables number of independent searches program executions with different precision constraints are required to find a solution that improves the program execution time for a given set ofinputs.
while some search based tools attempt to onlyprovide solutions that lead to faster execution time others provide solutions with no performance guarantees.
inthispaperwepresentanovelmethodtoperformfloatingpoint precision tuning that combines concrete and shadow program execution and it is able to find a solution after onlyasingle execution .
the main insight of blame analysis is that given a target instruction and a precision requirement one can build a blameset that contains all other program instructions with their operands in minimum precision.
inother words given an instruction and a precision require ment a blameset contains the precision requirements for the instructions that define the values of its operands.
asthe execution proceeds each instruction is executed withmultiple floating point precisions for each operand and itsblameset is updated.
the solution associated with a program point is computed using a mergeoperation over all blamesets.
this can be used to infer the set of program variables that can be declared as floatinstead of double while satisfying the precision requirements for a providedtest input set.
note that similar to blame analysis can only reduce precision with no performance guarantees.
we have implemented blame analysis using the llvm compiler infrastructure and evaluated it on eight pro grams from the gsllibrary and two programs from the nasparallel benchmarks .
to provide more context we also evaluated it against the precimonious search2016 ieee acm 38th ieee international conference on software engineering based tool.
we have implemented both an offlineanalysis that executes on program execution traces as well as an onlineanalysis that executes together with the program.
blame analysis was always successful in lowering the precision of all test programs for the given test input sets itidentified on average that of the program variables canbe declared as float variables in median.
the transformed programs did not always exhibit improved executiontime.
the offlineanalysis is able to lower the precision of a larger number of variables than the onlineversion but this comes with decreased scalability.
for the onlineversion to bound the overhead we had to restrict the analysis in some casestoconsideronlythelaststagesofexecution.
evenwith this restriction the online analysis produced solutions whileimposing running time overhead as high as comparable to other commercial dynamic analysis tools.
ifreductioninexecutiontimeisdesired blame analysis can be combined with feedback directed search tools suchasprecimonious which systematically searches for a type assignment for floating point variables so that the resultingprogram executes faster.
when using blame analysis to determine an initial solution for precimonious w ea l w a y s find better type assignments.
the total analysis time is faster on average and up to faster in comparison to precimonious alone.
in all cases in which the resulting type assignment differs from precimonious alone the type assignment produced by the combined analyses translatesinto a program that runs faster.
ourresultsareveryencouragingandindicatethatfloatingpointtuningofentireapplicationswillbecomefeasibleinthe near future.
as we now understand the more subtle behavior of blame analysis we believe we can improve both analysis speed and the quality of the solution.
it remainsto be seen if this approach to develop fast but conservativeanalyses can supplant the existing slow but powerful searchbased methods.
nevertheless our work proves that using a fast imprecise analysis to bootstrap another slow butprecise analysis can provide a practical solution to tuningfloating point in large code bases.
this work makes the following contributions we present a single pass dynamic program analysis fortuning floating point precision with overheads compa rable to that of other commercial tools for dynamicprogram analysis.
we provide an empirical comparison between single passandsearch based dual optimizationpurposetoolsfor floating point precision tuning.
we demonstrate powerful and fast precision tuning bycombining the two approaches.
the rest of this paper is organized as follows.
section presents an overview of precision tuning and current chal lenges.
we describe blame analysis in section and present its experimental evaluation in section .
we thendiscuss limitations and future work in section .
relatedwork is discussed in section .
we conclude in section .
.
tuning floating point precision programminglanguagesprovidesupportformultiplefloating point data types float single precision bit ieee double double precision bit ieee and long double bit extended precision .
software packages suchas qd provide support for even higher precision datatypes double double and quad double .
because reasoning about floating point programs is often difficult given the large variety of numerical errors that can occur one common practice is to use conservatively the highest availableprecision.
while more robust this can significantly degrade program performance.
many efforts have shown that using mixed precision can sometimes computea result of the same accuracy faster than when using solelythe highest precision arithmetic.
unfortunately determin ing the appropriate precision combination requires domainspecific knowledge combined with advanced numerical anal ysis expertise.
floating point precision tuning tools can help suggesting ways in which programs can be transformed to effectivelyuse mixed precision.
these tools serve multiple purposes.
for a given test input set one goal is to determine an opti mal minimal or maximal set of program variables orinstructions whose precision can be changed such thatthe answer is within a given error threshold.
if the goal is to improve accuracy expressions can be rewritten to reduce rounding errors .
another goal is to reduce memory stor age by maximizing the number of variables whose precisioncan be lowered.
finally improving program performance isanother important objective.
.
design and scalability concerns our main interest is in tools that target scientific computing programming and use a dual objective by targeting bothaccuracy and performance.
the state of the art tools com pute a solution by searching over global program state variables or instructions .
thus the search maintains a global solution and it requires multiple executions.
due to the em pirical nature and the heuristics to bound the search spaceand state the solutions do not capture a global optimum.
fromourperspective particularlyattractivearetoolsthat operate on the program variable space as they may suggestpermanent changes to the application.
the state of the art is reflected by precimonious which systematically searches for a type assignment also referred to as type configuration for floating point program variables.
its analysis time is determined by the execution time of the programunder analysis and by the number of variables in the pro gram.
the algorithm requires program re compilation and re execution for different type assignments.
the search is basedonthedelta debuggingalgorithm whichexhibitsa worst case complexity of o n where nis the number of variables in the program.
to our knowledge precimonious and other automated floating point precision tuners use empirical search and exhibit scalability problems with program size or program runtime.
in practice it is very difficult for programmers to predict how the type of a variable affects the overall precision of the program result and the precimonious analysis has to consider allthe variables within a program both global and local.
this clearly poses a scalability challenge to the overall approach.
in our evaluation of precimonious section we have observed cases in which the analysis takes hours for programs that have fewer than variables and native runtime less than seconds.
furthermore as the analysisis empirical determining a good solution requires repeatingit over multiple precision thresholds.
a solution obtained for a given precision e.g.
will always satisfy lower thresholds e.g.
.
given a target precision it is also 1075often the case that the solution determined independently for a higher precision provides better performance than thesolution determined directly for the lower precision.
inthiswork wesettodevelopamethodthatalleviatesthe scalability challenges of existing search based floating pointprecision tuning approaches by reducing the numberof required program analyses transformations executions and performing only local fine grained transformations without considering their impact on the global solution.
our blame analysis is designed to quickly identify program variables whose precision does not affect the final result for anygiven target threshold.
the analysis takes as input one or more precision requirements and executes theprogram only oncewhile performing shadow execution.
as output it produces a listing specifying the precision require ments for different instructions in the program which thencan be used to infer which variables in the program can definitely be in single precision without affecting the required accuracy for the final result.
when evaluating the approachwe are interested in several factors quality of solution i.e.
how much data is affected scalability of the analysis and impact on the performance of the tuned program.
blame analysis can be used to lower program precision to a specified level.
note that in general lowering precision does not necessarily result in a faster program e.g.
cast instructions might be introduced which could make the program slower than the higher precision version .
the analysis focuses on the impact in accuracy but does not con sider the impact in the running time.
because of this thesolutions produced are not guaranteed to improve program performance and a triage by programmers is required.
even when triaged solutions do not necessarily improve execution time.
as performance is a main concern we also consider combining blame analysis with dual objective search based tools as a pre processing stage to reduce the search space.
in the case of precimonious this approach can potentially shorten the analysis time while obtaining a good solution.
figure shows how removing variablesfrom the search space affects the analysis time for the blas program from the gsllibrary for the target precision .t h e blasprogram performs matrix multiplication and it declares floating point variables.
as shown at therightmost point in figure knowing a priori that out of17 floating point variables can be safely allocated as float reduces precimonious analysis time from .
hours to only minutes.
this simple filtering accounts for a speedup in analysis time.
in the rest of this paper we present blame analysis and evaluate its efficacy in terms of analysis running time and quality of solution in two settings applied by itself and as a pre processing stage for precimonious .w er e f e rt o quality of solution as whether the resulting type assignmentslead to programs with faster execution time.
.
blame analysis blame analysis consists of two main components a shadow execution engine and an integrated onlineanalysis.
the analysis is performed side by side with the program execution through instrumentation.
for each instruction e.g.
fadd floating point addition blame analysis executes the instruction multiple times each time using dif ferent precisions for the operands.
examples of precisioninclude float double a n d doubletruncated to digits.
g26 g1 g27 g26 g26 g26 g1 g28 g26 g26 g26 g1 g29 g26 g26 g26 g1 g30 g26 g26 g26 g1 g31 g26 g26 g26 g1 g32 g26 g26 g26 g1 g33 g26 g26 g26 g1 g34 g26 g26 g26 g1 g35 g26 g26 g26 g1 g27 g26 g26 g26 g26 g1 g26 g1 g27 g1 g29 g1 g31 g1 g33 g1 removed variables time secs figure the effect of reducing precimonious search space on analysis time for the blasbenchmark error threshold .
the horizontal axis shows the number of variables removed from the search space.
the vertical axis shows analysis time in seconds.
in this graph the lower the curvethe faster the analysis.
1double mpow double a double factor int n 2double res factor 3int i 4for i i n i res res a 7return res 10int main d o u b l ea .
double res t1 t2 t3 t4 double r1 r2 r3 t1 a t2 mpow a t3 mpow a t4 mpow a res a a a a r1 t4 t3 r2 r1 t2 r3 r2 t1 r e s r printf res .10f n res return figure sample program the analysis examines the results to determine which combinations of precisions for the operands satisfy given pre cision requirements for the result.
the satisfying precisioncombinationsarerecorded.
the blame analysis algorithm maintains and updates a blame set for each program instruction.
the blame set associated with each instruction speci fies the precision requirements for all operands such that theresult of the instruction has the required precision.
blame sets are later used to find the set of variables that can be declared in single precision.
.
blame by example consider the sample program shown in figure which computes and saves the final result in variable reson line .
in this example we consider three precisions fl float db double and db8 accurate up to digits compared to the double precision value .
more specifically the value inprecision db 8represents a value that agrees with the value obtained when double precision is used throughout the en1076table statement r r t isexecutedusingdifferent precisions for r2and t1.t h e c o l u m n op prec shows the precisions used for the operands flcorresponds to float dbto double and db8to double accurate up to digits .
columns r2and t1show the values for the operands in the corresponding precisions.
column r3shows the result for the subtraction.
precision db db produces a result that satisfies the precision requirement db8.
op prec r2 t1 r3 fl fl .
.
.
fl db .
.
.
fl db .
.
.
db fl .
.
.
db8 db8 .
.
.
... ... ... ... db db .
.
.
tire program in significant digits.
formally such a value can be obtained from the following procedure.
let vbe the value obtained when double precision is used throughoutthe entire program and v 8is the value of vin precision db8.
according to the ieee standard the binary repre sentation of vhas explicitly stored bits in the significand.
we first find the number of bits that corresponds to signifi cant decimal digits.
the number of bits can be computed as lg .
bits.
we therefore keep bits and set the remaining bits in the significand to to obtain the value v8.
back to our example when the final result is computed in doubleprecision the result is r e s .
.
when the computation is performed solely in singleprecision all variables in the program are declared as floatinstead of double the result is res .
.
assuming that we require the result to have precision db8 the result would beres .
xy where xandycan be any decimal digits.
precision tuning is based on the precision require ment set for the final result s of the program.
for each instruction in the program blame analysis determines the precision that the corresponding operands arerequired to carry in order for its result to be accurate to a given precision.
for example let us consider the statement on line r r t and precision db 8for its result.
since the double value of r3is .
this means thatwerequire r3tobe .
i.e.
thevaluematches to significant digits .
in order to determine the precision requirement for the two operands r2 and t1 we perform the subtraction operation with operands in all considered precisions.
table shows some of the precision combina tions we use for the operands.
for example fl db m e a n s that r2hasflprecision and t1hasdb8precision.
for this particular statement all but one operand precision combi nations fail.
only until we try db db then we obtain a result that satisfies the precision requirement for the result see last row of table .
blame analysis will record that the precision requirement for the operands in the statementon line is db db when the result is required to have precision db .
similarly operand precision requirements will also be recorded when the result is required to have other precisions under consideration fl a n d dbin this example .
statements that occur inside loops are likely to be executed more than once such as line r e s r e s a .f o r 1similarly if we are interested in or significant decimal digits we can keep or significant bits in the significand respectively and set other bits to .prog l instr instr x y aop z x yb o pz ifxgotol x nativefun y x c aop bop negationslash nativefun sin cos fabs l labels x y z variables c constants figure kernel language the precision requirement db8for the result of this operation the first time this statement is executed the analysis records the double values for the operands and the result .
.
.
.
the algo rithmtriesdifferentprecisioncombinationsfortheoperands and determines that precision fl db suffices.
the second time the statement is executed the analysis records new doublevalues .
.
.
.
after trying all precision combinations for the operands it isdetermined that this time the precision required is db db which is different from the requirement set the first time the statement was examined.
at this point it is necessary tomergeboth of these precision requirements to obtain a unified requirement.
in blame analysis the merge operation over approximates the precision requirements.
in thisexample merging fl db and db db8 would result in the precision requirement db db8 .
finally after computing the precision requirements for every instruction in the program the analysis performs a back ward pass starting from the target statement on line and considering the precision requirement for the final result.
the pass finds the program dependencies and requiredprecisions and collects all variables that are determined tobe in single precision.
concretely if we require the finalresult computed on line to be accurate to digits db the backward pass finds that the statement on line de pends on statement on line which depends on statementson lines and and so on along with the correspond ing precision requirements.
the analysis then collects the variables that can be allocated in single precision.
in this example only variable factorin function mpowcan be allocated in single precision it always stores integer constantswhich do not require double precision .
in the rest of this section we formally describe our blame analysis algorithm and its implementation.
our implementation of blame analysis consists of two main components a shadow execution engine for performing singleand double precision computation side by side with the concrete execution section .
and an online blame analysisalgorithm integrated inside the shadow execution runtime section .
.
finally we present analysis heuristicsand optimizations section .
.
.
shadow execution figure introduces a kernel language used to formally describe our algorithm.
the language includes standard arith metic and boolean expressions.
it also includes an assign ment statement which assigns a constant value to a variable.other instructions include if goto and native function call instructions such as sin cos a n dfabs.
in our shadow execution engine each concrete floatingpoint value in the program has an associated shadow value .
each shadow value carries two values corresponding to the 1077procedure faddshadow inputs lscript x y z instruction outputs updates the shadow memory mand the label map lm method single y single d o u b l e y double m single z single d o u b l e z double m 3m single y single zsingle d o u b l e y double zdouble 4lm lscript figure shadow execution of faddinstruction concrete value when the program is computed entirely in singleordoubleprecision.
we will represent a shadow value of a value vas single vsingle double vdouble where vsingleandvdoublearethevaluescorrespondingto vwhenthe program is computed entirely in singleanddoubleprecision respectively.
inourimplementation theshadowexecutionisperformed side by side with the concrete execution.
we instrumentcallbacks for all floating point instructions in the program.
the shadow execution runtime interprets the callbacks fol lowing the same semantics of the corresponding instructions however it computes shadow rather than concrete values.
letabe the set of all memory addresses used by the program sbe the set of all shadow values associated with the concrete values computed by the program and lbe the set of labels of all instructions in the program.
shadowexecution maintains two data structures .
a shadow memory mthat maps a memory address to a shadow value i.e.
m a s.i fm a sfor some memory address a then it denotes that the value stored at address ahas the associated shadow value s. .
a label map lmthat maps a memory address to an instruction label i.e.
lm a l.i flm a lscriptfor some memory address a then it denotes that the value stored at address awas last updated by the instruction labeled lscript.
as an example figure shows how mandlmare updated when an faddinstruction lscript x y zis executed.
in this example x y zare variables and lscriptis an instruction label.
we also denote x y zas the addresses of the variables x y z respectively in that state.
in this example theprocedure faddshadow isthecallbackassociatedwiththe faddinstruction.
the procedure re interprets the semantics of the faddinstruction see line but it uses the shadow values for the corresponding operands retrieved on lines 1and and creates updates the shadow value associated withx.t h el a b e lm a p lmis updated on line to record thatxhas been last updated at the instruction labeled lscript.
.
building the blame sets in this section we formally describe our analysis.
let a be the set of all memory addresses used by the program l be the set of labels of all instructions in the program pbe the set of all precisions i.e.
p fl db4 db6 db8 db10 db .
precisions fland dbstand for single and double precisions respectively.
precisions db4 db6 db8 db10denotevaluesthatfunction blameset inputs lscript x f y1 ... yn instruction with label lscript p precision requirement outputs lscript1 p1 lscriptn pn precision requirements of the instructions that computed the operands method 1accurate res trunc shadow m p s1 ... sn m ... m 3find minimal precisions p ... pnsuch that the following holds v1 ... vn t r u n c shadow s p1 ... trunc shadow s n pn trunc f v ... vn p accurate res 6return lm p1 ... lm pn figure blameset procedure are accurate up to and digits in double precision respectively.
we alsodefine atotalorderonprecisionsas fol lows fl db db6 db8 db10 db.i n blame analysiswe also maintain a blame map bthat maps a pair of instruction label and precision to a set of pairs of instructionlabelsandprecisions i.e.
b l p p l p where p x denotes the power set of x.i fb lscript p lscript p1 lscript2 p2 then it means that during an execution if instruction labeled lscriptproduces a value that is accurate up to precision p then instructions labeled lscript1and lscript2must produce values that are accurate up to precision p1andp2 respectively.
the blame map bis updated on the execution of every instruction.
we initialize bto the empty map at the beginning of an execution.
we illustrate how we update busing a simple generic instruction of the form lscript x f y1 ... y n wherex y1 ... y nare variables and fis an operator which could be sin log etc.
in a program run consider a state where this instruction is executed.
let us assume that x y ... y ndenote the addresses of the variables x y1 ... y n respectively in that state.
when the instruction lscript x f y1 ... y n is executed during concrete execution we also perform a side by side shadow execution of the in struction to update b lscript p f o re a c hp pas follows.
we use two functions blameset and merge unionsq t ou p d a t e b lscript p .
the function blameset receives an instruction and a precision requirement as input and returns the precision requirements for the instructions that define the values of its operands.
figure shows the pseudocode of the function blameset.
the function first computes the accurate result by retrieving the shadow value corresponding to theinput instruction and truncating the shadow value to precision p line .
function trunc shadow s p returns the floating point value corresponding to the precision pgiven the shadow value s. specifically if pis single precision then t h es i n g l ev a l u eo fs is returned otherwise trunc shadowreturns the double value of struncated to p.t h e s h a d o w values corresponding to all operand variables are retrieved on line .
then the procedure finds the minimal precisionsp ... pnsuch that if we apply fons1 ... sntruncated to precisions p1 ... pn respectively then the result truncated to precision pis equal to the accurate result computed on line .
function trunc x p returns xtruncated to precision p. we then pair each piwithlm the last instruction that computed the value yi and return the resulting set of pairs of instruction labels and precisions.
the merge function unionsqis defined as unionsq p l p p l p p l p 1078if lscript p lscript p ... lscript pn are all the pairs involving the label lscriptpresent in lp1orlp2 t h e n lscript max p1 p2 ... p n is the only pair involving lscriptpresent in lp unionsqlp2 .
given the functions blameset and merge unionsq we compute b lscript p unionsqblameset lscript x f y1 ... y n p and use the resulting set to update b lscript p .
at the end of an execution we get a non empty map b. suppose we want to make sure that the result computed by a given instruction labeled lscriptoutis accurate up to precision p. then we want to know what should be the accuracy of the results computed by the other instructions so thatthe accuracy of the result of the instruction labeled lscript outis p. we compute this using the function accuracy lscriptout p b which returns a set of pairs instruction labels and precisions such that if lscript prime p prime is present in accuracy lscriptout p b then the result of executing the instruction labeled lscript primemust have a precisionofatleast p prime.accuracy lscript p b canthenbedefined recursively as follows.
accuracy lscript p b lscript p unionsq unionsqdisplay lscript prime p prime b lscript p accuracy lscript prime p prime b aftercomputing accuracy lscriptout p b weknowthatif lscript prime p prime is present in accuracy lscriptout p b then the instruction labeled lscript primemust be executed with precision at least p primefor the result of executing instruction lscriptoutto have a precision p. .
heuristics and optimizations to attain scalability for large or long running programs the implementation of blame analysis must address memory usage and running time.
we have experimented with bothonlineandofflineversions of our algorithm.
the offline blame analysis first collects the complete execution trace and then builds the blame set for each dynamicinstruction i.e.
if a static instruction is executed more than once a blame set will be computed for each timethe instruction was executed .
as each instruction is exam ined only once merging operand precisions is not required.
thus whencomparedtoonline blame analysis theoffline approach often produces better lower precision solutions.
however the size of the execution trace and the blame setinformation explode for long running programs.
for exam ple when running offline blame analysis on the epnas benchmark with input class 2s the analysis terminates with an out of memory error exhausting gb of ram.
the online blame analysis is more memory efficient because the size of the blame sets is bounded by the number of staticinstructions in the program.
as shown in section .
the maximum analysis working set size is mb for ep.o n the other hand the blame sets for each instruction have to be merged across all its dynamic invocations making theanalysis slower.
in our implementation we allow develop ers to specify what part of the program they are interestedto analyze.
for short running programs such as functionswithin the gsl library examining all instructions isfeasible.
most long running scientific programs fortunatelyuse iterative solvers rather than direct solvers.
in this case analyzing the last few algorithmic iterations is likely to lead to a good solution given that precision requirements are in creased towards the end of the execution.
this is the casein the nas benchmarks we have considered.
if no options are specified blame analysis by default will be performed 2class s is a small input designed for serial execution.
llvm bitcode instrumentation instrumented llvm bitcode online blame analysis proposed type configuration analysis input parameters i l o ine bl de test inputs posed t figure blame analysis architecture throughout the entire program execution.
in summary our results show that offline blame analysisis fast no merge operations and produces better solutions for small programs but it is expensive in terms of memory usage which makes it impractical.
in contrast online blame analysis is memory efficient produces good solutions and it is not too expensive in terms of running time thus it has the potential to perform better when ana lyzing larger programs.
for brevity the results reported inthe rest of this paper are obtained using the online analysis.
.
experimental ev aluation the blame analysis architecture is described in figure .
we build the analysis on top of the llvmcompiler infrastructure .
the analysis takes as input llvm bitcode of the program under analysis a set of test in puts and analysis parameters that include the targetinstruction and the desired error threshold s .
because the analysis is implemented using llvm it can be applied to programs written in languages that have a llvm compilerfrontend e.g.
c c and fortran .
we use the origi nalprecimonious benchmarks written in c which have been modified by experts to provide acceptability criteria forthe result precision.
for blame analysis we select the acceptability code developed for precimonious as the target instruction set.
thus the results provided by both analyses always satisfy the programmer specified precision criteria.
the analysis result consists of the set of variables that can be in single precision.
in this section we present theevaluation of blame analysis b yi t s e l f a sw e l la sw h e n used as a pre processing stage for precimonious .w er e f e r to the latter as blame precimonious .
we compare this combined approach with using precimonious alone and perform an evaluation in terms of the analysis running time and the impact of the analysis results in improving programperformance.
we validate all the results presented in this section by manually modifying the programs according to the type assignments suggested by the tools and runningthem to verify that the corresponding final results are asaccurate as required for all test inputs.
.
experiment setup we present results for eight programs from the gsllibrary and two programs from the nasparallel benchmarks .
we use clangwith no optimizations3and a 3optimizations sometimes remove floating point variables which causes the set of variables at the llvm bitcode level to differ from the variables at the source code level.
1079table overhead of blame analysis program execution sec analysis sec overhead cg .
.
.
ep .
.
.
python wrapper to build whole program or whole library llvm bitcode.
note that we do apply optimizationlevel o2when performing final performance measurements on the tuned programs.
we run our experiments on an intel r xeon r cpu e5 .40ghz core machine running linux with gb ram.
we use the procedure described in to select program inputs.
for the nasbenchmarks programs epandcg we use the provided input class a. for the rest we generate1000 random floating point inputs which we classify intogroups based on code coverage.
we then pick one inputfrom each group i.e.
we want to maximize code coverage while minimizing the number of inputs to consider.
we log and read the inputs in hexadecimal format to ensure thatthe inputs generated and the inputs used match at the bitlevel.
we are indeed using the same set of inputs used in the original evaluation of precimonious.
in our experiments we use error thresholds a n d1 which correspond to and digits of accuracy respectively.
additionally for nasprograms ep and cg we configure blame analysis to consider only the last of the executed instructions.
for the rest of the programs blame analysis considers all the instructions executed.
.
analysis performance this section compares the performance of blame analysisand its combination with precimonious.w ea l s oc o m pare the onlineandofflineversions of blame analysis in terms of memory usage.
by itself blame analysis introduces up to slowdown whichiscomparabletotheruntimeoverheadreportedby widely used instrumentation based tools such as val grind and jalangi .
table shows the overhead for programs cgand ep.
for the rest of our benchmarks the overhead is relatively negligible less than one second .
tomeasuretheanalysistimeofthecombinedanalyses we add the analysis time of blame analysis and the search time of precimonious for each error threshold.
figure shows the analysis time of blame precimonious b p andprecimonious p for each of our benchmarks.
we use all error thresholds for all benchmarks except for program ep.
the original version of this program uses error threshold thus we do not consider error threshold .
overall we find that blame precimonious is faster than precimonious in out of experiments error thresholds for programs and error thresholds for program .
in general we would expect that as variables are removed from the search space the overall analysis time will be reduced.
however this is not necessarily true especiallywhen very few variables are removed.
in some cases re moving variables from the search space can alter the search path of precimonious which results in a slower analysis time.
for example in the experiment with error threshold 4forgaussian blame analysis removes only two variables from the search space see table a small reduc tion that changes the search path and actually slows downtable average analysis time speedup of blame precimonious compared to precimonious alone program speedup program speedup bessel .
sum .
gaussian .
fft .
roots .
blas .
polyroots .
ep .
rootnewt .
cg .
the analysis.
for programs epand cg the search space reduction results in analysis time speedup for precimonious .
however the overhead of blame analysis causes the combined blame precimonious running time to be slower than precimonious for programs ep 4and and cg .
figure shows the analysis time breakdown.
table shows the average speedup per program for all error thresholds.
we observe analysis time speedups for outof programs.
the largest speedup observed is .
and corresponds to the analysis of program rootnewt.
whenever we observe a large speedup blame analysis removes a large number of variables from the search space of precimonious at least for error thresholds 4and see table .
this translates into significantly shorter analy sis time for precimonious.
the only experiment in which blame precimonious is slower than precimonious o n average is when analyzing the program cg however the slowdown observed is only .
b p p b p p b p p b p p b p seconds a ep b p p b p p b p p b p seconds b cg figure analysis time breakdown for blame precimonious b p and precimonious p for two nas benchmark programs in terms of memory usage the onlineversion of blame analysis uses up to mb of memory in our experiments.
the most expensive benchmark in terms of analysis memory usage is program ep.
for this program the offlineversion of the analysis runs out memory gb .
.
analysis results table shows the type configurations found by blame analysis b blame precimonious b p and precimonious p which consist of the numbers of variables in double precision d and single precision f .
it alsoshows the initial type configuration for the original pro gram.
our evaluation shows that blame analysis is effective in lowering precision.
in particular in all experi ments blame analysis successfully identifies at least one variable as float.i fw ec o n s i d e ra l l3 9e x p e r i m e n t s blame analysis removes from the search space of the variables on average with a median of .
the type configurations proposed by blame precimonious and precimonious agree in out of experiments and differ in experiments.
table shows the p b p time secs error threshold a bessel time secs error threshold g26 g1 g31 g26 g26 g26 g1 g27 g26 g26 g26 g26 g1 g27 g31 g26 g26 g26 g1 g28 g26 g26 g26 g26 g1 g28 g31 g26 g26 g26 g1 g29 g26 g26 g26 g26 g1 g29 g31 g26 g26 g26 g1 g30 g26 g26 g26 g26 g1 g27 g26 g23 g22 g30 g1 g27 g26 g23 g22 g32 g1 g27 g26 g23 g22 g34 g1 g27 g26 g23 g22 g27 g26 g1 g5 g1 g2 g36 g5 g1 b gaussian p b p time secs error threshold c roots p b p time secs error threshold d polyroots p b p error threshold time secs e rootnewt p b p time secs error threshold f sum p b p error threshold time secs g blas p b p time secs error threshold h fft p b p error threshold time secs i ep error threshold time secs p b p j cg figure analysis time comparison between precimonious p and blame precimonious b p .
the vertical axis shows the analysis time in seconds.
the horizontal axis shows the error thresholds used in each experiment.
in these graphs a lower curve means the analysis is more efficient.
speedup observed when we tune the programs according to these type configurations.
in all cases in which the twoconfigurations differ the configuration proposed by blame p r e c i m o n i o u s produces the best performance improvement.
in particular in three cases we observe .
additional speedup.
in out of experiments blame precimonious finds configurations that differ from the configurations suggested by blame analysis alone.
among those experiments produce a configuration that is different from theoriginal program.
this shows that our analysis is conserva tive and precimonious is still useful in further improving configurations found by blame analysis alone.note that for blame analysis we have reported results only for the onlineversion of the analysis.
our experiments indicate that the offlineversion has memory scalability problems and while its solutions sometimes are better in terms of the number of variables that can be lowered to single precision it is not necessarily better at reducing anal ysis running time or the running time of the tuned program.
.
discussion blame analysis has several limitations.
first similar to other state of the art tools for precision tuning our anal ysis cannot guarantee accurate outputs for all possible in1081table configurations found by blame analysis b blame precimonious b p and precimonious alone p .
the column initial gives the number of floating point variables double d and float f declared in the programs.
for each selected error threshold we show the type configuration found by each of the three analyses b b p and p number of variables per precision .
denotes the cases where the tools select the original program as fastest.
error threshold 4error threshold initial b b p p b b p p p r o g r a m d fd fd fd fd fd fd f bessel gaussian roots polyroots rootnewt sum fft blas 17ep cg error threshold 8error threshold initial b b p p b b p p p r o g r a m d fd fd fd fd fd fd f bessel gaussian roots polyroots rootnewt sum fft blas ep cg puts thus there is the need for representative test inputs.
although input generation has a significant impact on thetype configurations recommended by our analysis the prob lem of generating floating point inputs is orthogonal to theproblem addressed in this paper.
in practice we expect programmers will be able to provide meaningful inputs or use complementary input generation tools .
still we believeour tool is a powerful resource for the programmer who willultimately decide whether to apply the suggested configurations fully or partially.
another limitation is that blame analysis does not take into account program performance by itself the suggested configurations might not lead to program speedup.
notethat in general lowering precision does not necessarily result in a faster program.
for example consider the addition v v2.
assume v1has type floatandv2has type double.
the addition will be performed in doubleprecision requiri n gt oc a s tv 1todouble.
when a large number of such casts is required the tuned program might be slower thanthe original program.
the analysis focuses on the impact inaccuracy but does not consider the impact in running time.because of this the solutions produced are not guaranteedto improve performance.
last the program transformations suggested by blame analysis arelimitedtochangingvariabledeclarationswhose precision will remain the same throughout the execution ofthe program.
we do not currently handle shift of precision during program execution which could potentially contribute to improving program performance.
also the anal ysis does not consider algorithmic changes that could alsopotentially improve running time.
note that both kinds oftransformations would require additional efforts to express program changes.
while very useful automated tools for floating point precision tuning have to overcome scalability concerns.
as it adds a constant overhead per instruction the scalability ofour single pass blame analysis is determined solely by the programruntime.
thescalabilityof precimonious isdetermined by both program runtime and the number of variablesin the program.
we believe that our approach uncovers very exciting potential for the realization of tools able to handle large codes.
there are several directions to improve the ef ficacy of blame analysis as a standalone tool as well as a filter for precimonious.
a future direction is to use blame analysis as an intraprocedural analysis rather than an interprocedural analy sis as presented in this paper.
concretely we can apply iton each procedure and use the configurations inferred foreach procedure to infer the configuration for the entire program.
so will enable the opportunity for parallelism and might greatly improve the analysis time in modular pro grams.
another future direction is to experiment with otherintermediate precisions.
in this paper we used four intermediate precisions db db6 db8 a n d db10 to track precision requirements during the analysis.
this proved a good trade off between the quality of the solution and runtimeoverhead.
for some programs increasing the granularity ofintermediate precisions may lead to more variables kept in low precision further pruning the search space of precimonious.
.
related work precimonious is a dynamic analysis tool for tuning floating point precision already detailed.
lam et al.
also propose a framework for finding mixed precisionfloating point computation.
lam s approach uses a brute force algorithm to find double precision instructions that can be replaced by single instructions.
their goal is to use as many single instructions in place of double instructionsas possible but not explicitly consider speedup as a goal.blame analysis differs from precimonious and lam s framework in that it performs a white box analysis on the 1082table speedup observed after precision tuning using configurations produced by blame precimonious b p andprecimonious alone p threshold 4threshold program b p p b p p bessel .
.
.
.
gaussian .
.
.
.
roots .
.
.
.
polyroots .
.
.
.
rootnewt .
.
.
.
sum .
.
.
.
fft .
.
.
.
blas .
.
.
.
ep .
.
.
.
cg .
.
.
.
threshold 8threshold program b p p b p p bessel .
.
.
.
gaussian .
.
.
.
roots .
.
.
.
polyroots .
.
.
.
rootnewt .
.
.
.
sum .
.
.
.
fft .
.
.
.
blas .
.
.
.
ep .
.
cg .
.
.
.
set of instructions executed by the program under analysis rather than through searching.
thus blame analysis is not bounded by the exponential size of the variable or in structionsearchspace.
similartolam sframework thegoalof our analysis is to minimize the use of double precision in the program without considering performance.
darulova et.
al develop a method for compiling a realvalued implementation program into a finite precision implementation program such that the finite precision implementation program meets all desired precision with respect to the real numbers however the approach does not sup port mixed precision.
schkufza et.
al develop a methodfor optimization of floating point programs using stochasticsearch by randomly applying a variety of program transformations which sacrifice bit wise precision in favor of performance.
floatwatch is a dynamic execution profiling tool for floating point programs which is designed to identifyinstructions that can be computed in a lower precision by computing the overall range of values for each instruction of interest.
as with other tools described in this paper all theabove also face scalability challenges.
darulova and kuncak also implemented a dynamic range analysis feature for the scala language that could beused for precision tuning purposes by first computing a dy namic range for each instruction of interest and then tun ing the precision based on the computed range similar tofloatwatch.
however range analysis often incurs overestimates too large to be useful for precision tuning analysis.
gappa is another tool that uses range analysis to verifyand prove formal properties of floating point programs.
onecould use gappa to verify ranges for certain program vari ablesandexpressions andthenchoosetheirappropriateprecisions.
nevertheless gappa scales only to small programs with simple structures and several hundreds of operations and thus is used mostly for verifying elementary functions.
a large body of work exists on accuracy analysis .
benz et al.
present a dynamic approachthat consists on computing every floating point instructions side by side in higher precision storing the higher precision values in shadow variables.
fpinst is another tool thatcomputes floating point errors to detect accuracy problems.it computes a shadow value side by side but it stores anabsolute error in double precision instead.
herbie estimates and localizes rounding errors and then rewrites numerical expressions to improve accuracy.
the above toolsaim to find accuracy problems and improve accuracy notto find opportunities to reduce floating point precision.
other large areas of research that focus on improving performance are autotuning e.g.
and ap proximate computing e.g.
.
however no previous work has tried to tune floating point precisionas discussed in this paper.
finally our work on blame analysis is related to other dynamic analysis tools that employ shadow execution and instrumentation .
these tools however are designed as general dynamicanalysis frameworks rather than specializing in analyzingfloating point programs like ours.
.
conclusion we introduce a novel dynamic analysis designed to tune the precision of floating point programs.
our implementation uses a shadow execution engine and when applied to a set of ten programs it is able to compute a solution with atmost runtime overhead.
our workload contains a combination of small to medium size programs some that arelong running.
the code is open source and available online .
when used by itself blame analysis is able to lower the precision for all tests but the results do not necessar ily translate into execution time improvement.
the largestimpact is observed when using the analysis as a filter to prune the inputs to precimonious a floating point tuning tool that searches through the variable space.
the combined analysis time is faster on average and up to in comparison to precimonious alone.
the resulting type configurations improve program execution time by as much as .
.
we believe that our results are very encouraging and indicate that floating point tuning of entire applications will become feasible in the near future.
as we now understand the more subtle behavior of blame analysis w eb e l i e v e we can improve both analysis speed and the quality of the solution.
it remains to be seen if this approach to developfast but conservative analyses can supplant the existing slowbut powerful search based methods.
nevertheless our work proves that using a fast imprecise analysis to bootstrap another slow but precise analysis can provide a practical so lution to tuning floating point in large code bases.
.