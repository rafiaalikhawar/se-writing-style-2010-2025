scalable approaches for test suite reduction emilio cruciani breno miranda roberto v erdecchia and antonia bertolino gran sasso science institute l aquila italy federal university of pernambuco recife brazil vrije universiteit amsterdam amsterdam the netherlands isti consiglio nazionale delle ricerche pisa italy emilio.cruciani gssi.it bafm cin.ufpe.br roberto.verdecchia gssi.it antonia.bertolino isti.cnr.it abstract test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large size test suites.
most existing techniques aretoo expensive for handling modern massive systems and moreoverdepend on artifacts such as code coverage metrics or specificationmodels that are not commonly available at large scale.
wepresent a family of novel very efficient approaches for similarity based test suite reduction that apply algorithms borrowed fromthe big data domain together with smart heuristics for findingan evenly spread subset of test cases.
the approaches are verygeneral since they only use as input the test cases themselves testsource code or command line input .
we evaluate four approachesin a version that selects a fixed budget bof test cases and also in an adequate version that does the reduction guaranteeing somefixed coverage.
the results show that the approaches yield a faultdetection loss comparable to state of the art techniques whileproviding huge gains in terms of efficiency.
when applied to asuite of more than 500k real world test cases the most efficientof the four approaches could select btest cases for varying b values in less than seconds.
index t erms clustering random projection similaritybased testing software testing test suite reduction.
i. i ntroduction in recent years testing has consistently been the most actively investigated topic of main software engineering conferences .
one prominent problem in software testing researchcan be abstracted as given a software sand an associated test suitet how can we efficiently verify whether spasses on t or if not identify the failing test cases?
in this formulation the emphasis is on the term efficiently otherwise the easysolution would be to just execute sont.
the research targets the common practical case that along the development processsneeds to be repeatedly tested on t see e.g.
and the plain retest all strategy may be too costly considering the available resources e.g.
time .
to address the above question in the last three decades many techniques have been proposed which can be roughlydivided in two groups those that aim at reordering the testcases intso that those more likely to fail are executed first test case prioritization and those that select a subset t prime t that should ideally include the failing test cases if any thelatter group of techniques is referred to as test case selection ortest suite reduction 1depending on whether when choosing 1some authors use the term minimization in place of reduction when the not selected test cases are permanently removed from the test suite.
here in line with we will consider the two terms as interchangeable.t primethe changes made to sare considered modification aware regression testing or not .
the proposed techniques have been evaluated and compared against each other using metrics relative to their fault detection effectiveness e.g.
the average percentage of fault detectionof the reordered test suite or the loss in faults detected by thereduced test suite t prime for test reduction and selection also metrics relative to cost savings e.g.
the size or the executiontime oft primeare compared against those of the full suite t. another important factor that should be taken into account is the cost of the technique itself both in terms of the computational effort and of the resources it requires.
in other words when evaluating whether investing on an automated approachaimed at reducing the cost of testing is worth a complete cost benefit analysis should also include the overheads implied bythe approach .
however not many of the proposed techniques have considered such implied costs.
in orso and coauthors alreadynoticed that in regression testing efficiency and precision needto be traded off because precise techniques are generallytoo expensive to be used on large systems .
gligoricand coauthors were the first to observe that the timeconsumed by any regression test technique should include ananalysis phase an execution phase and a collection phase.they noticed that most authors only considered the savingsin execution a few measured also the analysis time but noone before them measured also the last phase in which theinformation needed to apply the technique is collected.
aspointed out by elbaum and coauthors at scale industriesneed approaches that are relatively inexpensive and do notrely on code coverage information .
in fact for white boxtechniques the cost of collecting and saving up to date codecoverage information should also be considered as part ofthe collection phase.
this is confirmed by herzig whoobserves that code coverage is not for free as assumed in manyworks and can cause up to of time overhead!
in a recent work we addressed the prioritization of very large test suites and showed that as the size of the test suitegrows most existing approaches become soon not applicable.that work proposed the fast family of similarity based test prioritization approaches that outperformed in efficiency andscalability all the compared approaches except for the white box greedy total approach.
if we count the often ignored ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
costs of measuring coverage then fast appears as the only scalable prioritization approach.
this paper introduces a family of scalable approaches for test suite reduction called the fast r family.
as in fast r approaches are similarity based and borrow techniques from the big data domain.
however with respect to we apply here several new techniques that allow us to achieve even more efficient results.
in fast we used minhashing and locality sensitive hashing algorithms .
fast r approaches adopt other efficient heuristics that are used to derive a set of bevenly spread points in a big data space.
precisely one approach called fast applies the k means algorithm while another one called fast cs uses a recent importance sampling algorithm to construct coresets a clustering technique that scales up to massive datasets .
moreover we further enhance the scalability of both approaches by applying the random projection technique that reduces the space dimensionality while preserving the pairwise distances of the points .
fast and fast cs are extremely practical techniques in the sense required by all of i thanks to the heuristics imported from the big data domain they are computationally very efficient ii to reduce a test suite tthey require no other information beyond titself.
based on the applied algorithms the most natural scenario for fast and fast cs is that of finding a fixed budget bof test cases.
this is referred in literature as inadequate test suite reduction.
in the paper we also show how they can be adapted to perform adequate reduction i.e.
preserving coverage we apply a filtering strategy and search for the most dissimilar test cases only among the ones that cover not yet covered elements.
however we acknowledge that at large scale such adequate scenario is not realistic because as already said coverage information cannot be assumed.
although originally proposed for prioritization we note that fast approaches could be easily adapted for test reduction instead of ordering the whole test suite the algorithm is stopped when the budget b or the desired coverage is reached.
accordingly we also include in fast r and evaluate the reduction version of fast pw and fast all the most precise and the most efficient of the fast family .
summarizing this paper proposes four test suite reduction approaches two original ones and two adapted from that can be applied in two testing scenarios under a fixed budget or for adequate test suite reduction.
we evaluated the four proposed approaches on commonly used c and java benchmark programs against state of theart reduction techniques obtaining comparable results for effectiveness but notable improvements in efficiency.
more interestingly to validate our claims on the scalability of the approaches we applied all four of them to the budget reduction of a test suite formed by more than 500k java test cases collected from github.
at such large scale not considering the preparation time fast pw and fast required several hours to reduce the suite e.g.
hours and hours respectively for a size but fast all required secondsand fast cs seconds.
actually fast cs looks as a real breakthrough as it took less than seconds for the reduction independently from the percentage and needed just minutes for preparation in contrast to more than hours taken by fast all .
the original contributions of this work include the fast r family of scalable approaches for inadequate test suite reduction.
a variant of all the approaches for adequate test suite reduction.
a large scale experimentation for evaluating the efficiency and effectiveness of the approaches in three scenarios including a very large scale test suite.
an open source automated framework along with all the data used for the experiments to support verifiability.
the paper is structured as follows.
in the next section we survey related work.
in section iii we present the approaches used.
in section iv and v respectively we present the evaluation methodology and the achieved results.
finally section vi draws conclusions and hints at future work.
ii.
r ela ted work this work is related to software regression testing and more specifically to test suite reduction techniques.
the literature on software regression testing is huge two surveys provide a broad overview of prioritization reduction or minimization used here in interchangeable way and selection techniques.
in particular y oo and harman reviewed the literature until .
concerning reduction techniques most of the surveyed works consists of heuristics over white box coverage criteria at various level of granularity including statement branch function or call stack .
some approaches augment the coverage information with additional inputs by the tester e.g.
weighting coefficients or priority assignments which may be costly or even biased .
among the few interesting exceptions black box reduction they report some combinatorial fault based and model based techniques.
more recently do surveys further advances over .
in particular for test suite reduction she reviews four more recent techniques two of which are again coverage based and two ones introduce specific reduction techniques one for gui testing and another for combinatorial interaction testing .
note that both surveys include no work on similarity based test suite reduction as we propose here.
a recent systematic survey by rehman and coauthors focuses specifically on test suite reduction.
the study surveyed the literature between and identifying a set of relevant primary studies.
based on the adopted algorithms they classify the approaches into greedy mostly coverage based clustering and search based plus hybrid combinations thereof.
our approach would fit in the clustering group in which out of the surveyed studies they only find three works one using machine learning algorithms and two using hierarchical clustering.
we take here a distance from most of the techniques surveyed in the above studies since fast r is expressly authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
motivated by considerations of scalability and practical applicability.
in this perspective our approach is more closely related to few recent works based on coarse grained heuristics clustering and similarity.
in recent years some collaborative efforts between academic and industrial researchers start to appear that develop coarsegrained approaches trading precision with efficiency scalability.
strictly speaking such works focus on test case selection and not test suite reduction in that the choice of tests to execute is modification aware.
for example knauss and coauthors use a statistical model that relates the changed code fragments or churns with test outcomes on ericsson systems considering a continuous integration development environment elbaum and coauthors propose a strategy apt for google testing process which combines test case selection during pre submit testing and test case prioritization in post submit testing.
both selection and prioritization apply heuristics based on failure history and execution windows.
by relying on very efficient algorithms our fast r approaches can scale up to large industrial systems as the above works while not sacrificing much of precision in deriving a representative subset of the test cases.
our similarity based approach is related to several techniques that exploit the diversity among test cases for guiding selection.
some techniques build on the notion of adaptive random testing art that in a few words first selects a random set of test cases and then filters them based on their distance from the already selected test cases.
several variants instantiations of art have been proposed including art d and art f that we use as competitors to fast r and that are further described in section iv.
some black box approaches use similarity to reduce modelbased test suites.
both test case reduction and test case selection techniques have been proposed.
these techniques have been conceived for industrial use for example hemmati and coauthors pursue as a main goal a selection of test cases adjusted to the available testing budget.
however all such model based approaches rely on the assumption that a formal model of program behavior e.g.
a l ts is available.
in contrast fast r does not need to assume anything else beyond the test cases themselves.
a few works have proposed to leverage clustering of test cases as we do here e.g.
.
however they calculate the similarity between two test cases based on code coverage information which as said already could be too expensive at the testing scale we aim.
iii.
t he approaches given a test suite tand some fixed budget b t the goal of similarity based test suite reduction is to select b evenly spread test cases out of the test suite.
if we model each test case as a point in some d dimensional space then the problem could be thought of as that of finding the central points of bclusters.
the problem of clustering is np hard but we are able to perform scalable similarity based test suitet1 grep e foo file .
vector space model term frequency grep e v f foo bar filet1 t2 t33.
random projection comp1 comp2comp3t1 t2 t31.
test suite t2 grep v e foo file t3 grep f bar file fig.
visual representation of fast r preparation phase.
reduction by borrowing a technique from the big data domain and using it in combination with some efficient heuristics.
we consider an euclidean space a metric space where the distance between any two points is expressed by the euclidean distance what one could think of as the straight line connecting them.
let x y rdbe two points the euclidean distance between them is defined as d x y radicalbig summationtextd i xi yi .
in the preparation phase of our approaches we transform test cases into points in the euclidean space via the vector space model the textual representation of each test case e.g.
test source code or command line input fig.
.
is mapped into an n dimensional point where each dimension corresponds to a different term of the source code and nis equal to the total number of terms used in the whole test suite.
the components are weighted according to term frequency scheme i.e.
the weights are equal to the frequency of the corresponding terms fig.
.
.
the computation of the euclidean distance between any twon dimensional points can be expensive when nis large.
to overcome this problem we exploit a dimensionality reduction technique called random projection .
roughly speaking random projection works because of johnson lindenstrauss lemma which states that a set of points in a highdimensional space can be projected into a much lowerdimensional space in a way that pairwise distances are nearly preserved.
in particular we use sparse random projection an efficient implementation of the technique that is suitable for database applications fig.
.
.
we model the clustering problem as a k means problem withk b.g i v e nnpoints in a metric space the goal of kmeans is to find a k partition p p1 ... p k of the points that minimizes the sum of the squared euclidean distances between each point to its closest center of one partition.
formally the goal is to find arg minp summationtextk i summationtext x pid x i where iis the center of the points belonging to partition pi.
there exist efficient techniques that are able to find an approximate solution to k means.
one is k means authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm fast input test suite t budget b output reduced test suite r p randomprojection t preparation phase s firstselection p r list s d distance squared distance to closest point in r d s while size r b do for allt pdo ifd parenleftbig p t p s parenrightbig2 d t then d t d parenleftbig p t p s parenrightbig2 squared euclidean distance s proportionalsample p d r append r s d s returnr which achieves an o logk approximation ratio2in expectation and finds the centers of the clusters in klinear time iterations.
the algorithm is the de facto standard technique for the initialization phase of k means algorithms.
after the initial centers are selected standard k means algorithms would iteratively compute the clusters.
in our case to be more efficient we stop at this stage and use the kselected centers as the test cases of the reduced test suite.
the reduction approach that exploits k means as greedy reduction strategy is called fast algorithm .
fast starts by preprocessing the test suite t mapping each test case into a vector according to the vector space model and then lowering its dimensionality via random projection line .
after the preparation phase the reduction algorithm works only on the projected data pon which the greedy selection of k means is applied.
first pick the first point uniformly at random3 line .
then until bpoints have not been selected i for each projected point t p compute the squared distance d t r 2betweentand its nearest center in rthat has been already picked lines this can be done incrementally by maintaining the minimum distance and computing only the distance with the last selected point lines ii pick next point swith probability proportional to its distance to r line .
another possible approach to simplify the clustering problem is that of using coresets .
given a set of points s a coreset is a small subset of sthat well approximates the geometric features of s. one usually constructs a coreset first and then finds the centers of the clusters on it reducing the complexity of the problem while still having theoretical guarantees on the solution.
in our case though the size of the reduction grows linearly with the size of the test suite making this standard approach less efficient the complexity of the problem would not lower much.
instead exploiting a recent extremely efficient algorithm developed for massive datasets we construct a coreset of size band use it as reduced test suite.
the algorithm is based on importance sampling all points have nonzero 2in a minimization problem an approximation algorithm finds a solution which is not worse than times the optimum.
3note that this is to stick with k means algorithm but any other criterion for the choice of the first test case is possible.algorithm fast cs input test suite t budget b output reduced test suite r p randomprojection t preparation phase mean p for allt pdo q t t d parenleftbig p t parenrightbig2 summationtext t prime pd parenleftbig p t prime parenrightbig2 importance sampling r proportionalsamplewithoutreplacement p q b returnr probability of being sampled but points that are far from the center of the dataset potentially good centers for a clustering are sampled with higher probability.
we call the reduction approach that use this technique fast cs algorithm .
fast cs starts with the preparation phase to compute the set of projected points p line .
then it only requires two full passes on p first it computes the mean of the data points line and then it uses it to compute the importance sampling distribution lines .
the probability of each point to be sampled is a linear combination of the uniform distribution first term in line and of the distribution which is proportional to the squared euclidean distance between the data point and the mean of the data second term in line .
then bpoints are sampled out of pwithout replacement with probability proportional to their importance sampling probability line and used as reduced test suite.
both fast and fast cs have also been adapted to be adequate i.e.
to perform a reduction that guarantees some fixed coverage.4getting coverage information of each test case as an extra input both the proposed approaches are able to reduce the test suite such that some fixed coverage is achieved.
this is possible thanks to a filtering phase .i n fast all test cases which would not add any extra coverage are filtered out after each selection and the next selection is carried out only among the remaining ones.
as for fast cs log t test cases are picked at each subsequent iteration and then importance sampling probabilities are recomputed setting to the ones relative to test cases which are filtered out.
picking log t tests per iteration instead of just one makes the algorithm scale better to big test suites.
moreover this choice does not increase the size of the reduced test suite since the selected test cases are still diverse among them and thus the chance of covering different parts of the software under test is still high.
finally instead of stopping when the reduction reaches sizeb both adequate approaches stop whenever the reduction achieves some fixed coverage.
as said this work was inspired by the fast family of test case prioritization approaches roughly speaking those approaches could be also used for the goal of test suite reduction by only picking the first b test cases of the prioritized test suite.
to assess also their efficiency and effectiveness when applied to test suite reduction we modified 4the pseudocodes of adequate versions are not reported for lack of space but they can be found online .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
all the original algorithms to stop after b test cases are prioritized.
moreover we adapted them to be adequate as well again using the same filtering phase introduced in fast and fast cs .
iv .
e v alua tion methodology and setup we conducted some experiments to evaluate the effectiveness and the efficiency of the proposed approaches in different application scenarios.
as a first scenario we considered the case in which test resources are limited and a tester can only run a small subset of test cases from an existing test suite we call this the budget scenario because we fix a priori a reduction percentage of test suite size.
in this scenario we can apply the natural version of the proposed approaches.
as a second case we considered adequate scenario in which the code coverage measures of the whole test suite are preserved.
to study this scenario we applied the adequate version of the approaches.
we also studied a third case called the largescale scenario in which we apply the inadequate reduction on a very large test suite.
a. research questions we address the following research questions rqs rq1 how effective are the proposed test suite reduction approaches in comparison with state of the art techniques?
the goal of test suite reduction is to reduce the size of a test suite while maintaining its fault detection effectiveness.
thus the effectiveness of reduction approaches is commonly measured in terms of the fault detection loss fdl and for adequate approaches also in terms of test suite reduction tsr .
consequently we articulate the above rq1 into the two following subquestions rq1.
what is the fault detection loss of the proposed approaches compared with that of state of the art techniques?
to answer rq1.
we measure fdl f f prime f wherefis the set of faults detected by tandf primeis the set of faults detected by t prime.
rq1.
what is the test suite reduction achieved by the proposed approaches compared with that of state ofthe art techniques?
to answer rq1.
we measure tsr t t prime t .
we answer rq1.
in both budget and adequate scenarios and rq1.
only in the adequate scenario.
to evaluate the efficiency we address the following rq rq2 how much time is taken by the proposed approaches to produce the reduced test suite?
we measure the time spent in preparation and in reduction.
we answer rq2 in all the three scenarios in the budget and adequate scenarios we compare the time taken by the proposed approaches against state of the art competitors in the largescale scenario we could only apply our proposed techniques as all competitors approaches require coverage information that at such scales are not available.b.
compared reduction approaches we recall that the fast r family of proposed approaches consists of the newly devised fast and fast cs plus the modified reduction versions of fast pw and fast all first introduced for prioritization .
the competitor approaches we consider are art d and art f which belong to the family of adaptive random testing techniques .
in brief they both work by first deriving a candidate set of test cases from those not yet selected that would increase coverage and then selecting from within the candidate set the most distant test case from those already selected.
the two techniques differ on the candidate set size dynamically changing in art d and fixed in art f and on the adopted distance metric jaccard and mahattan respectively .
we selected these approaches because they also aim at obtaining an evenly spread set of test cases as in our approaches and also because in the results reported in they were among the best competitors to fast .
differently from fast r art d and art f use coverage measures.
finally we also applied the ga greedy additional approach which for its simplicity and effectiveness is often considered as a baseline.
ga selects the test case that covers the highest number of yet uncovered elements.
for all three competitors we consider three variants applied to coverage of function statement and branch.
c. experiment material to evaluate the budget scenario and the adequate scenario we took c and java programs as experimental subjects.
the c programs consisting of flex v3 grep v3 gzip v1 sed v6 and make v1 were gathered from the software infrastructure repository sir .
for each of these programs subsequent versions are available each containing a varying number of seeded faults.
in our experiment we considered for each program the version containing the highest number of difficult to reveal faults i.e.
faults that are discovered by less than of the test cases.
this was done to avoid including in the experiment anomalous versions e.g.
versions in which most faults are revealed by the majority of the test cases or no faults are revealed at all.
in total the c subjects amounted to loc containing faults and were accompanied by a test suite comprising test methods.
the java programs taken into account namely closure compiler commons lang commons math jfreechart and joda time were taken from the defects4j database .
such database provides a set of programs available in different versions each containing a single real fault.
for our experiment we considered the first version of the programs.
in total the java subjects amounted to loc and were accompanied by a test suite comprising test classes.
to evaluate the large scale scenario we used a set of more than 500k real world test cases gathered through the github hosting service.
to efficiently collect a high number of heterogeneous test cases we selected classes committed to the master branches of the available java repositories precisely commits adding a single class which adheres to common authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
naming conventions for junit classes.
in total through this process we collected test cases amounting to roughly million loc for a total size of gb.
d. experiment procedure the experiment was performed on an amd opterontm with .3ghz cpu 16mb l2 cache 64gb ram running ubuntu .
.
l ts.
the procedure varied according to the scenario considered.
more specifically budget scenario we fixed a set of budgets b for each experimental subject both c and java .
the budgets considered ranged between and of the total test suite size of each subject with a step increase of .
while the fast r approaches only required the test suite for the reduction process all competitors could take in input different coverage types namely function statement and branch.
we therefore performed a single study for the fast r approaches and for each of the competitors.
we used each compared approach to reduce the test suite of the experimental subjects by considering all bbudgets.
the metrics considered were fault detection loss preparation time and reduction time .
the measurements were repeated times for each study given the stochastic nature of the approaches.
adequate scenario the fast r approaches require coverage information for the filtering phase as an extra input to have an adequate reduction.
the competitor approaches instead require exclusively the coverage information.
for this scenario we considered function statement and branch coverage.
we used the compared approaches to reduce the test suite of each experimental subject both c and java so to maintain the coverage prior of the reduction.
we measured fault detection loss test suite reduction preparation time and reduction time .
the measurements were repeated times for each study given the stochastic nature of the approaches.
large scale scenario as for the budget scenario w e considered a set of budgets branging from to of total test suite size of the subjects with a step increase of .
in this setting we exclusively evaluated fast r approaches as the other approaches require coverage information which in this scenario is not available.
to answer rq2 we applied the approaches to the github dataset for each possible reduction ofb and measured preparation time andreduction time .
v. r esul ts in this section we report and discuss the results.
note that with the aim of supporting independent verification and replication we make available the artifacts produced as part of this work .
the replication package includes approaches input data statistical analyses and additional results.
a. the budget scenario fault detection loss the box plots of figure display the fdl of the compared approaches and more details are provided in table i. the results are grouped by programming language because the c and java programs investigated contain different types of faults see section iv c .
the approaches javacfast fast cs fast pw fast all art d f art d s art d b art f f art f s art f b ga f ga s ga b 0255075100fault detection loss in fig.
fdl for the test suite reduction approaches in .
from the fast r family are applied in a black box fashion while the competitors target different coverage criteria.
for this reason we have three boxes for each competitor the letter insides the parenthesis is the first letter of the targeted criterion function statement or branch .
for this metric the lower the result the better.
the visual assessment of the data can show us that overall the median fault detection loss is similar for all the approaches with two exceptions while ga outperformed all the competitors for c statement and branch it performed very poorly and always missed the fault for the java subjects statement and branch fast pw on its turn was comparable to most of the competitors when considering the c subjects but similarly to ga it performed poorly for java.
after the visual inspection we proceeded with the statistical analysis of the data.
we adopted a non parametric statistical hypothesis test the kruskal wallis rank sum test as our data could not be assumed to be normally distributed.
we assessed at a significance level of the null hypothesis that the differences in the fdl values are not statistically significant.
for the particular case of c programs when targeting the function coverage criterion the null hypothesis could not be rejected p value .
i.e.
no significant difference in fault detection loss was observed.
for all the other cases the observed differences in fdl are statistically significant at least at the confidence level.
provided that significant differences were detected by the kruskal wallis test we performed pairwise comparisons to determine which approaches are different.5the results are 5a significant kruskal wallis test indicates that at least one reduction approach stochastically dominates one or multiple competitors but does not identify the dominance relationship among pairs of techniques.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able i fault detection loss in the budget scenario.
approachcj ava mdn mdn fast .
.
b .
.
a fast cs .
.
b .
.
b fast pw .
.
b .
.
c fast all .
.
b .
.
ab art d f .
.
b .
.
ab art f f .
.
b .
.
ab ga f .
.
b .
.
d art d s .
.
b .
.
ab art f s .
.
b .
.
ab ga s .
.
a .
.
e art d b .
.
b .
.
ab art f b .
.
b .
.
ab ga b .
.
a .
.
e mdn is the median fault detection loss is the standard deviation and is the group for the pairwise comparisons after the kruskal wallis test.
t able ii reduction times for the budget scenario including and excluding preparation time .
approachtotal time reduction time mdn mdn fast .
.
b .
.
b fast cs .
.
c .
.
a fast pw .
.
i .
.
d fast all .
.
i .
.
c art d f .
.
a .
.
e art f f .
.
c .
.
f ga f .
.
a .
.
e art d s .
.
f .
.
j art f s .
.
h .
.
k ga s .
.
g .
.
k art d b .
.
d .
.
g art f b .
.
e .
.
i ga b .
.
d .
.
h mdn is the median time total or reduction is the standard deviation and is the group for the pairwise comparisons after the kruskal wallis test.
displayed in table i inside the parenthesis.6the statistical analysis confirmed the conclusions drawn from the visual inspection with the exception of ga and fast pw that had varying performance depending on the programming language and coverage criterion when applicable all the approaches investigated had overall comparable fdl.
time the results obtained by the approaches in terms of efficiency are displayed in table ii.
it contains the total time which includes preparation time and the time for only the reduction itself.
for this metric we do not make any distinction between the programming languages c or java because the efficiency of the approaches could be impacted only by the size of the test case representation adopted.
6if two approaches have different letters they are significantly different with .
.
if on the other hand they share the same letter the difference between their ranks is not statistically significant.
for example looking at the results for the java subjects in table i we can tell that fast all ab is not different from fast a and it is also not different from fast cs b even though fast a is different from fast cs b .
the approach or group of approaches that yields the best performance is assigned to the group a .t able iii fault detection loss in the adequate scenario.
approachcj a v a tsr fdl tsr mdn mdn mdn functionfast .
.
b .
.
bc .
.
b fast cs .
.
c .
.
bc .
.
c fast pw .
.
c .
.
b .
.
bc fast all .
.
e .
.
a .
.
d art d .
.
d .
.
a .
.
e art f .
.
e .
.
a .
.
f ga .
.
a .
.
c .
.
a statementfast .
.
b .
.
d .
.
b fast cs .
.
c .
.
d .
.
c fast pw .
.
d .
.
d .
.
b fast all .
.
e .
.
b .
.
d art d .
.
f .
.
a .
.
e art f .
.
g .
.
a .
.
e ga .
.
a .
.
c .
.
a branchfast .
.
b .
.
d .
.
b fast cs .
.
c .
.
d .
.
c fast pw .
.
d .
.
c .
.
d fast all .
.
e .
.
b .
.
e art d .
.
f .
.
a .
.
f art f .
.
g .
.
a .
.
f ga .
.
a .
.
e .
.
a mdn is the median fault detection loss is the standard deviation and is the group for the pairwise comparisons after the kruskal wallis test.
results for fdl are not displayed for java because all the approaches were able to always reveal the existing fault with the reduced test suite.
if we consider only the reduction time the fast r approaches outperformed all the competitors.
fast and fast cs are much more efficient than fast pw and fast all during the preparation phase.
indeed even if we consider total time fast and fast cs are still very efficient they would beat all the competitors with the exception of the ones targeting function coverage which is a very coarse grained criteria that allows the approaches to finish the reduction task after just a few iterations.
b. the adequate scenario test suite reduction and fault detection loss for the adequate scenario the fast r approaches are still applied in a black box fashion but they can use coverage information which entities are covered by which test cases in the reduction phase to filter out test cases that cannot contribute to increase coverage anymore.
for this reason we report the results of our study grouped by programming language and by coverage criteria.
an ideal reduction approach should be capable of maximizing tsr while maintaing the same fault detection effectiveness of the original test suite.
thus it is important to analyze tsr and fdl together.
figure displays for each approach a sideby side box plot for each metric tsr in the left and fdl in the right .
for better readability in figure we display tsr which represents the size reported in of the reduced test suite such that we can visually interpret the two metrics in the same direction the lower the value the better.
additional results from the statistical analysis are reported in table iii.
for our analysis we again performed the kruskal425 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
c function c statement c branch java function java statement java branch fast fast cs fast pw fast all art d art f ga fast fast cs fast pw fast all art d art f ga fast fast cs fast pw fast all art d art f ga fast fast cs fast pw fast all art d art f ga fast fast cs fast pw fast all art d art f ga fast fast cs fast pw fast all art d art f ga0255075100size of the reduced suite and fdl in size of the reduced suite fault detection loss fig.
test suite reduction and fault detection loss in .
wallis rank sum test followed by the pairwise multiple comparisons to identify the differences among the comparedapproaches.
all the results reported in table i are statisticallysignificant at the significance level.
considering the results for c two main groups seem to emerge fast all and art achieve the best results in terms of fdl but at the price of having much bigger test suites e.g.
art f achieved a median reduction of .
w.r.t.
the originaltest suite considering branch on the other hand we have thatthe other members of the fast r family achieve reduction levels that are very similar to those of ga while remainingcompetitive with ga in terms of fdl.
in fact for some cases the three members from the fast r family outperformed ga in terms of fdl e.g.
column of c fdl branch .
for the particular case of java all the approaches were able to always reveal the existing fault with the reduced test suite.that explains why we do not have the fdl column for javain table iii.
ga always achieved the best results in terms oftsr followed by the fast r family then by the art based approaches.
this result was somehow expected as ga aimsat reaching the maximum achievable coverage with as fewtest cases as possible while the fast r approaches aim at maximizing the diversity of the reduced test suite withouthaving coverage as a target to be achieved.
time table iv summarizes the results of the statistical analysis of our data after the kruskal wallis test andthe pairwise comparisons.
overall the performance of theapproaches remained stable when compared with the efficiencystudy for the budget scenario.
with the exception of total time forfunction where ga performed better at least one of the fast r approaches achieved the best performance for all the other cases.
the art based approaches are in general not competitive art d performs better than fast pw and fast all only when we consider total time forfunction.
then their performance degrades very quickly as we move from coarse to fine grained coverage criteria.t able iv reduction times for the adequate scenario including and excluding preparation time .
approachtotal time reduction time mdn mdn functionfast .
.
b .
.
a fast cs .
.
b .
.
b fast pw .
.
d .
.
d fast all .
.
d .
.
bc art d .
.
c .
.
e art f .
.
e .
.
f ga .
.
a .
.
cd statementfast .
.
a .
.
c fast cs .
.
a .
.
c fast pw .
.
c .
.
b fast all .
.
c .
.
a art d .
.
d .
.
e art f .
.
e .
.
f ga .
.
b .
.
d branchfast .
.
a .
.
a fast cs .
.
b .
.
b fast pw .
.
d .
.
b fast all .
.
d .
.
a art d .
.
e .
.
d art f .
.
f .
.
e ga .
.
c .
.
c mdn is the median time total or reduction is the standard deviation and is the group for the pairwise comparisons after the kruskal wallis test.
the very efficient preparation phase of fast and fast cs make them good candidates even if we had to consider the costs incurred by the preparation phase.
c. the large scale scenario the goal of this scenario is to provide empirical evidence to support our claim of scalability for the fast r approaches.
the line plots in figure depict the time spent by the four fast r approaches to reduce the test suite formed by 500k test cases gathered from github down to a budget b with bvarying from to of the full size .
precisely we plot the total time in figure .a and the reduction time in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
0time in seconds log10 scale a total time reduction percentagetime in seconds log10 scale fast fast cs fast pw fast all b reduction time fig.
time required to reduce 500k test cases to different reduction targets.
figure .b.
note that in both plots we use on the vertical axis a logarithmic scale.
the slowest approach is fast pw and the fastest one is fast cs .
while the reduction time of fast cs is about seconds independently of the reduction percentage the reduction time taken by fast pw varies with the reduction percentage between 25k seconds for and 329k seconds hours for see figure .b i.e.
the time difference between the two approaches spans over orders of magnitude.
considering the reduction time only also fast all is quite efficient as its time varied over the reduction percentage between and seconds.
the comparison of values between the two plots also evidences how the preparation phase of fast and fast cs is faster than the one of fast pw and fast all for the former two the preparation time over the 500k github test cases was seconds whereas for the latter two it grew up to 13k seconds i.e.
it took times longer see table v .
unfortunately in this scenario we could not measure fdl but if the results of the budget scenario generalize i.e.
fdl is comparable to other state of the art techniques then we have here two approaches that in seconds can select a representative subset of dissimilar test cases from half million test cases.
considering also its lightweight requirements table v fast cs is definitively the approach we would push to industrial applications.
for sake of completeness we have also applied the fast r family to a synthetic set of coverage data for the large scale test suite and could also assess the scalability of the adequate versions of fast r .
we do not show here the results for lack of space but they are included in the replication package.t able v time and space needed to compute and store prepared data by fast r in the large scale scenario .
algorithms input data preparation time prepared data fast fast cs .
gb .
s .
mb fast pw fast all .
gb .
s .
mb d. answering the rqs trying to draw a concise summary of the results in the three scenarios we can conclude that rq1.
the fast r family shows fdl values that are statistically comparable to those of competitor approaches see tables i and iii .
rq1.
in terms of tsr fast r approaches excluding fast all resulted the first for c and second only to ga for java see figure .
rq2 time wise fast r beats all competitors in reduction time and remains competitive even including preparation time resulting second only to art d and ga on function coverage see tables ii and iv .
at large scale fast cs achieves outstanding results figure being able to prioritize a 500k test suite in minutes including the preparation time.
e. costs and benefits preparation phase the preparation phase of fast and fast cs is crucial for scalability the random projection technique has a cost of o ndd wherenis the number of points we are projecting dis the actual dimensionality of the data and dthe dimensionality of the projected data.
in this work the test cases are projected into a space with d dimensions a higher dimensionality would better preserve the original pairwise distances but this choice seems to provide a good trade off between effectiveness and efficiency .
as can be seen in table v fast and fast cs require much smaller preparation times and also less space to store the prepared data on disk.
this is because even if the dimensionality of the projected data fast fast cs i s the same as the length of the minhash signatures fast pw fast all the sparsity of the random projection makes many of the components of the projected data null.
this results in space savings through sparse representation of the data which is not possible for the minhash signatures.
note that also the new preparation phase is suitable to scenarios in which additions edits are made to the test suite e.g.
regression testing.
in fact it is enough to update the random projection matrix to handle the increased dimensionality of the space and to process only new modified test cases the old are not affected by the updated matrix .
budget version with a fixed budget b the reduction time complexity is o nbd for fast ando nd for fast cs the former performs biterations each of which computes ndistances in o nd the latter instead just needs two full iterations on the data with a cost of o nd t o compute the distances between the mean and each point.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adequate version with a worst case analysis the time complexity of fast would increase since an adversarial input could make it select the entire test suite in niterations other than canceling out the advantages of the filtering phase in practice though filtering should lower problem complexity iteration by iteration making the algorithm much faster on non adversarial input.
regarding fast cs instead the complexity increases to o nrd logn whereris the size of the reduction.
in fact another pass of the data is needed after each filtering phase to recompute the importance sampling probability.
the lognfactor is due to the selection of logn points per iteration.
picking logntest cases per iteration instead of just one helps to mitigate the higher complexity w.r.t.
the budget scenario and make the algorithm scale better on big test suites.
f .
threats to validity despite our best efforts the presented results could suffer from validity threats.
following the classification in we consider four aspects.
construct validity if the experiments we set are appropriate to answer the rqs.
to answer rq1 we measured fdl and tsr which are de facto standard metrics .
this should nullify potential threats in answering rq1.
concerning rq2 we measured preparation and reduction time a potential threat is that other factors than time could prevent fast r to scale up.
to mitigate this risk we used real world test suites in all scenarios.
internal validity if the observed results are actually due to the treatment and not to other factors.
a common internal threat is the accuracy of measures that can be affected by random factors to mitigate this threat we replicated all observations times.
external validity whether and to what extent the observations can be generalized.
the experiments we performed are in line with other studies in the literature.
we have observed the proposed techniques over c and java subjects of varying dimensions.
perhaps the programs and test sets from sir and defects4j may not well represent actual regression testing scenarios.
however we opted for such subjects because i we could not find other subjects providing faults information ii they are used in many other studies.
notwithstanding from current observations we cannot draw general conclusions and more experimentation is needed.
reliability whether and to what extent the observations can be reproduced by other researchers.
to ensure reproducibility as said we make available all data and settings related information.
vi.
c onclusions and future work this paper addressed the problem of reducing the size of test suites during regression testing.
our focus is on very large scale test suites existing coverage based or model based techniques cannot be used in such scenario.
we rather propose to apply similarity based selection which intuitively picks thetest cases so that they are the most distant from each other according to some distance metric.
to efficiently find a subset of btest cases we introduced two novel techniques fast and fast cs and adapted to the problem of reduction the fast pw and fast all techniques previously proposed in for test prioritization.
all four approaches import smart heuristics from the big data domain and provide different degrees of precision and efficiency.
we evaluated the effectiveness and efficiency of the fast r family on the commonly used sir and defects4j benchmark programs.
the effectiveness is measured through the metric of fault detection loss.
moreover even though we would not need to use coverage information we have implemented an adequate variant of the four techniques on which we measured test suite reduction.
the results from both budget and adequate scenarios are that fdl and tsr remain both comparable with state of the art reduction techniques namely ga art d and art f .
on the other hand the efficiency of the proposed approaches in terms of the reduction time is better than all the compared approaches but ga applied to function coverage already for the relatively small scale benchmarks.
we also applied the fast r family to a much larger test suite 500k test cases and here we got impressing results as presented in section v. for the future we have made several plans to extend this work.
we would like to challenge our fast r family on a real large scale testing scenario.
although in our studies we measured the time employed in deriving the reduced test suite on a real set of half million test cases we could not also assess the effectiveness at such scale because we did not have fault data.
moreover we would also like to apply the techniques in real development environment to consider other possible process factors that may impact scalability.
in particular we would like to embed the techniques into a continuous development practice where other criteria could be also considered when picking test cases.
in this work we did not consider which files are modified when selecting test cases as in modification aware regression test selection.
we could extend the proposed reduction approaches by filtering out the test cases that do not impact on modified files.
or other smarter more efficient heuristics could be conceived.
this is an extension very relevant also in light of the conclusions in that selection techniques achieve higher tsr than reduction ones and are safer.
as already said in in modern complex and distributed software systems test suites deserve to be managed in the same way as big data and we do hope that our novel approaches to test suite reduction contribute to further advance the fields towards scaling up test automation.
acknowledgment this research has been partly funded by the h2020 european project elastest in the h2020 programme under ga no .
breno miranda wishes to thank the postdoctoral fellowship jointly sponsored by capes and facepe apq0826 .
bct .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.