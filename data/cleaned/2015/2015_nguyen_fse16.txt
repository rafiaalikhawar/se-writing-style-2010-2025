api code recommendation using statistical learning from fine grained changes anh tuan nguyen1 michael hilton2 mihai codoban3 hoan anh nguyen1 lily mast4 eli rademacher2 tien n. nguyen1 danny dig2 1department of electrical and computer engineering iowa state university usa 2school of eecs oregon state university usa 3microsoft usa 4college of engineering and computer science university of evansville usa abstract learning and remembering how to use apis is difficult.
while codecompletion tools can recommend api methods browsing a long list of api method names and their documentation is tedious.
moreover users can easily be overwhelmed with too much information.
we present a novel api recommendation approach that taps into the predictive power of repetitive code changes to provide relevant api recommendations for developers.
our approach and tool apirec is based on statistical learning from fine grained code changes and from the context in which those changes were made.
our empirical evaluation shows that apireccorrectly recommends an api call in the first position of the time and it recommends the correct api call in the top positions of the time.
this is a significant improvement over the state of the art approaches by for top accuracy and for top accuracy respectively.
our result shows that apirecperforms well even with a one time minimal training dataset of publicly available projects.
ccs concepts software and its engineering !software evolution integrated and visual development environments keywords api recommendation fine grained changes statistical learning .
introduction today s programs use application programming interfaces apis extensively even the hello world program invokes an api method.
one great challenge for software developers is learning and remembering how to use apis .
the state of the practice support for working with apis comes in the form of code completion tools integrated with ides .
code completion tools allow a user to type a variable and request a possible api method call recommendation.
code completion tools are among the top most used features of ides .
still a work done while being an intern at oregon state universitydeveloper learning an api or trying to remember it can waste a lot of time combing through a long list of api method names available on a receiver object.
for example invoking the code completion on an object of type string in jdk populates a list of possible methods and additional methods inherited from superclasses .
the state of the art research in code completion takes advantage of api usage patterns which researchers mine via the deterministic algorithms such as frequent itemset mining pair associations frequent subsequence or subgraph mining.
when a recommendation is requested these approaches analyze the surrounding context.
if the context matches a previously identified pattern the recommender will suggest the rest of the api elements in the pattern.
other approaches use statistical learning via language models to recommend the next token including api calls.
they rely on the regularity of source code and create a model that statistically learns code patterns from a large corpus.
the model then can predict what token is likely to follow a sequence of given code elements.
a key limitation to the approach is that it is difficult to determine which tokens belong to a project specific code idiom.
these tokens produce noise for recommendation.
we present a novel approach to code completion that leverages the regularity and repetitiveness of software changes .
our intuition is that when developers make low level changes even noncontiguous changes are connected.
these connections exist because the developer made the changes with a higher level intent in mind e.g.
adding a loop collector .
grouping fine grained changes by higher level intent allows us to cut through the noise of unrelated tokens that may surround the recommendation point.
to find these groups of fine grained changes we use statistical learning on a large code change corpus.
the changes that belong to higher level intents will co occur more frequently than non related changes.
additionally we also consider the surrounding code context at the recommendation point.
for example when adding a loop collector while the code tokens for and hashset were not changed they are good indicators for a tool to recognize that high level intent.
thus being aware of the code context a tool would recommend correctly the next token e.g.
hashset.add .
we implemented our approach in a tool apirec that computes the most likely api call to be inserted at the requested location where an api call would be valid.
apirecworks in three steps i it builds a corpus of fine grained code changes from a training set ii it statistically learns which fine grained changes co occur and iii it computes and then recommends a new api call at a given location based on the current context and previous changes.
for the first step we trained our model on a corpus of fine grained code changes from the change commits in open source projects from github.
apireciterates over commits and detectsthe differences in abstract syntax trees ast nodes using the state of the art ast diff tool gumtree .
for the second step we developed an association based inference model that learns which changes frequently co occur in the same changed file.
additionally the model operates on the code context of fine grained changes e.g.
forloops preceding method calls .
in the third step using the change context of previous changes the code context of the recommendation point and the trained inference model apirecdetermines the likelihood of a user inserting an api method call at this location.
if it determines that an api method insertion is indeed likely then it returns a list of candidate api calls ranked by the computed likelihood of being selected by a developer.
to empirically evaluate the usefulness of our approach we answer three following research questions.
rq1 accuracy how accurate is apirecin suggesting api calls?
rq2 sensitivity analysis how do factors such as the size of training data the requested location the sizes of the change context and code context impact accuracy?
rq3 running time what is the running time of a pirec?
to answer rq1 we measure the accuracy of the recommender.
top kaccuracy measures how likely the correct api is in the first krecommended apis.
we measure the accuracy in three different evaluation editions.
in the community edition we first train apirec on open source projects and then measure apirec s accuracy on a corpus of projects that other researchers have previously used.
in the project edition we do a fold validation on each of the above projects.
for the user edition we also do a fold validation on the same projects as above but only on the commits coming from a single user.
to answer rq2 we studied the impact of several factors on accuracy e.g.
the size of training data previous changes surrounding context and the location of recommendation invocation.
to answer rq3 we look at the running time of apirec.
for each evaluation we compare apirecwith the previous stateof the art learning approaches n gram bruch et al.
and gralan .
this paper makes the following contributions .approach.
we present a novel approach that uses statistical learning on fine grained changes with surrounding code context to create a new code completion tool.
we set forth a new direction that takes advantage of the repetiveness of both source code and fine grained code changes .
.implementation.
we implemented our approach in a tool apirec that computes the most likely api method call to be inserted at the requested location in the code.
.empirical evaluation.
our empirical evaluation on realworld projects shows that apirechas high accuracy in api code completion .
top 1accuracy.
this is an improvement over the state of the art approaches for top accuracy.
our evaluation shows that apirecalso performs well even with a one time minimal training dataset of publicly available projects.
interestingly we found that considering code authorship one could train with less data yet achieve higher accuracy than when training with an entire project .
training the model with the community corpus still results in higher accuracy than training with the data from the project or an individual developer.
this finding suggests that developers should obtain a community trained model and then further refine it with their own change histories .
.
motivating example figure shows a real world example that we collected from prior work in mining fine grained code changes.
this is a common change pattern called adding a loop collector .
in this change a1for task t tasks t.execute a set taskresult results new hashset 2for task t tasks t.execute results.
add t.getresult b figure a change pattern adding a loop collector developer introduces a new variable that collects or aggregates the values processed in a loop.
in the example the fine grained code changes include the changes at line and line in figure 1b.
specifically the changes at line include the addition of the declaration of the variable results with the type set taskresult and the addition of its instantiation via new hashset .
line has the additions of two method calls results.add andt.getresult .
assume that the current editing location is at line of figure 1b after a developer has typed the changes at line and the name of the variable results .
s he then triggers the code completion tool.
here is how different tools respond to their request.
the de facto modern ide will present the list of methods and fields of hashset in a pre defined usually alphabetical order.
s he must browse through a list of methods to find the desired method.
advanced code completion engines will recommend a list of api calls based on the api usage patterns that are mined from the code corpus.
there are two common mining strategies.
the first strategy relies on deterministic mining algorithms such as frequent itemset common subsequence or common subgraph mining .
the second one uses statistical learning from code .
both strategies share the same principle that source code is repetitive .
code suggestion techniques e.g.
n gram model which rely on the code context identify the context as the sequence of tokens preceding the variable results in our example the sequence t.execute .
however this code sequence is specific to this project and is not part of any code pattern related to the method add.
thus tools based on code patterns might not recommend the correct api callhashset.add .
more advanced approaches which also consider program dependencies among the entities e.g at line and line still might not see hashset.add as a good candidate because other api calls from hashset are just as likely to occur.
key ideas instead of relying on source code repetitiveness apirecis based oncode change repetitiveness .
hindle et al.
reported that software exhibits its naturalness source code has a higher degree of repetitiveness than natural language texts.
we expect the same principle of naturalness of software to occur on fine grained code changes i.e.
naturalness of code changes since similar changes may be performed to introduce similar behavior.
in our example the addition of a new hashset generally a collection is followed by the addition of the call hashset.add on the same variable.
since apirecobserved the addition of a new hashset in the current change context it is able to correctly suggest hashset.add at line .
for a pirecto work we rely on the following key ideas first we develop an association based model toimplicitly capture change patterns i.e.
frequent co occurring fine grained code changes in our training data.
our insight for using associations among fine grained changes is that such changes in a pattern do not need to have strict order as required in a traditional n grammodel.
for example hashset.add could be edited before or after the addition of the declaration of the hashset variable.
second the recent fine grained code changes in the change context of the current code lead the trained model to recommend the next method call e.g.
the addition of a hashset object often leads to the call hashset.add .
moreover we use the code context surrounding the requested location which might contain the code tokens that are part of the change patterns.
for example the token for is part of both the code context and the change pattern of adding a loop collector because users often collect the elements into a collection via a forloop.
thus it helps suggest the method call addat line .
third not all the changes in the current context are useful in recommendation because they can be project specific and considered as noise in the change patterns.
recent work confirms that not all code changes are repetitive.
to address this we rely on thebasis of consensus given a large number of changes in many projects the project specific changes are less likely to appear frequently than the changes belonging to a higher level intent pattern.
.
definitions .
api call completion anapi method call is a call to an api of an external or internal library while a method call is a call to a method within a project.
for brevity we call both types api calls.
we distinguish them if needed.
in traditional code completion a programmer invokes code completion by placing a dot after a variable such as v.apirec supports any recommendation where the addition of a method call would result in syntactically correct code.
for example after the sign in an assignment as in v it recommends a method call.
.
fine grained atomic code changes we represent source code as asts to avoid formatting changes which are not helpful for suggestion.
for a changed file we compare the asts before and after the changes to derive the fine grained changes.
the state of the art differencing tool gumtree is used.
definition a tomic change .a fine grained atomic change is represented by a triplet of operation kind ast node type label .
table shows the atomic changes for the editing scenario in figure .
each atomic change corresponds to a change to an ast node in the program.
we consider each change to be one of the following operations change add delete and move .
the ast node types represent the java ast nodes.
the labels represent the textual information of the ast nodes.
we only use labels for some types of ast nodes.
for method invocation simple type and simple name of a method invocation or a simple type we use the labels to identify the name of a method or type.
we use the name to find change patterns and to recommend.
for example in table two simpletype s have their labels of setand taskresult .
we also keep the labels forboolean constants andthe null value .
these labels are special literals that help in detecting change patterns involving those values.
for the ast nodes for which we keep the labels in addition to node types and operation kinds we use the labels when comparing asts.
definition t ransaction .atomic changes from the same changed file in a commit are collected into a transaction .
we use a bag or multiset which allows for multiple instances of an element to represent a transaction rather than a list since the atomic changes might be different depending on each programmer even though they belong to the same change pattern.
thus if weestablish a strict order we might not be able to statistically learn the patterns for recommendation.
moreover because the atomic code changes are recovered from the committed changes in a code repository we do not have the order in which they were written.
.
change context and code context definition c hange context .thechange context is the bag of fine grained atomic changes that occurred before the requested change at the current location in the same editing session.
the change context at the underlined location in figure contains atomic changes at line partially shown in table and the addition of results not shown for space reason .
they are useful in recommending the method addat line .
the instantiation of a hashset object and the call to the method addare part of a change pattern.
identifying these changes as the beginning of a pattern helps apirecrecommend the addition of hashset.add .
we also give a larger weight to the changes made to the program elements that have data dependencies with the current code element i.e.
results because they are more likely to occur together in a change pattern than other elements with no data dependency.
we currently consider only the dependencies between variables definitions and their uses and between the method calls on the same variable.
definition c ode context .the code context is the set of code tokens that precede the current editing location within a certain distance in terms of code tokens.
we obtain the code tokens from the ast.
for example the tokens for task t tasks and execute will be used as the code context to recommend the api call hashset.add .
the rationale is that the tokens surrounding the recommendation point might often go together with the api call as part of a pattern even though they might not be recently changed.
thus the code context helps us recommend the correct api call.
we do not consider separators and punctuation.
for both change and code contexts we consider the distance and thescope of a change and token.
we attribute a higher impact to the preceding changes or code tokens that are nearer to the current location.
thus we give them higher weights in the decision process.
we measure the distance by the number of code tokens in the program.
since we focus on recommending api calls in a method we give higher weights to the changes and code tokens within the method under edit and lower weights to the changes tokens outside of it.
.
change inference model to rank and recommend the candidates of api calls apirecuses our association based change inference model.
the model learns from a fine grained change corpus to compute the likelihood scores for each candidate change cto occur at the requested location given both the change and code contexts .
to do that it computes the contributions of individual changes and code tokens in the contexts by counting in the corpus the co occurrence frequency of each atomic change in the change context with candthe frequency of each code token in the code context appearing in the change c. finally the contributed scores are integrated and adjusted via the weighting factors for the distances between changes and the scope of changes.
.
model overview the goal of our model is to compute the likelihood score that a change coccurs at the requested location given i the fine grained code changes in the change context cpreceding the current change c and ii the code tokens of interest in the code context tsurrounding the requested location.
cis in the form add methodinvocation methodname and a pirecneeds to predict methodname .table fine grained atomic code changes ind.
oper.
ast node type label content of ast s sub tree c1 add variabledeclarationstatement vds variabledeclarationstatement set taskresult results new hashset c2 add parameterizedtype pt parameterizedtype set taskresult c3 add simpletype st set set c4 add simplename sn set set c5 add simpletype st taskresult taskresult c6 add simplename sn taskresult taskresult c7 add variabledeclarationfragment vdf variabledeclarationfragment results new hashset ... ... ... ... ... the occurrence likelihood score c c t of a change cis a function of c c and t. generally learning that function from data is challenging.
to make it computable we assume that both change and code contexts have independent impacts on the occurrence of the next token.
we denote the impacts of change and code contexts on the occurrence of cbyscore c c andscore c t respectively.
we aim to learn the occurrence likelihood score c c t of change cin the form of a weighted linear combination of two impacts score c c t wc score c c wt score c t where wcandwtare the weights corresponding to the impacts of contexts.
other types of combinations can be explored in future.
to compute the impact of the change context via score c c with cconsisting of the atomic changes c1 c2 .
.
.
cn we adapted the concept of trigger pairs by rosenfeld in language models for natural language texts.
that is if a word sequence ais significantly correlated with another word sequence b then a!b is considered as a trigger pair.
when aoccurs before does not need to be within a n gram it triggers b causing its probability estimate to change .
while rosenfeld considers words we consider each of the changes in the context cas a trigger where the order among cis is not needed as explained in section .
however instead of using maximum entropy in apirec we approximate score c c by the product of each trigger pair score c ci i ...nbecause the number of changes in a change context is much smaller than the number of words in a document.
we then compute each trigger pair score c ci via the conditional probability pr c ci .
this probability can be estimated with their association score i.e.
the ratio between the number of transactions having both changes candciover the number of transactions having ci.
finally we also incorporate the weights for distance between the two changes and the scope of a change.
we similarly compute the impact score c t of the code context via the trigger pair score c ti with tiis a token in t. .
details on model computation .
.
computing score c c the value of score c c represents the impact of the atomic changes in the change context for predicting c. one could compute score c c score c c1 c2 .
.
.
cn n c c1 c2 ... cn n c1 c2 ... cn where n c1 c2 .
.
.
cn is the number of transactions containing the changes c1 c2 .
.
.
cn and n c c1 c2 .
.
.
cn is the number of transactions containing the changes c c1 c2 .
.
.
cnincluding change c. however we cannot always compute those numbers.
this is because in the training data we might not frequently encounter the cases where those changes occurred in the same transaction.
that is we might have a small number of such co appearance.
thus we adapted the trigger pair concept to compute equation as follows.
score c c score c c1 c2 .
.
.
cn score c c1 score c c2 ... score c cn in maximum entropy the impact of a term on the presence of another is modeled by a set of constraints.
their intersection is the set of probability functions that are consistent for all the terms.
the function with the highest entropy in that set is the me solution .
inapirec because the number of changes in cis usually small we do not aim to find the combined estimation.
instead we estimate score c ci i.e.
the probability that coccurs given that cioccurred as the following and then take the product of all scores score c ci pr c ci n c ci n ci where n c ci is the number of transactions in which the changes c andcico appear and n ci is the number of transactions having ci.
the ratio represents the association score between two changes.
to account for the distance between a change ciand the current change cand to avoid underflow we use the logarithmic form log score c ci 1 d c ci logn c ci n ci where d c ci is the distance between candci which is measured as the number of tokens in the code between the two tokens corresponding to the two changed nodes of candci.
we sort the changes ciaccording to their distances to c. the smaller the distance the higher the change ranks.
then we use the rank for a change cias its distance d c ci .
the logform is to avoid underflow in computation.
because c add methodinvocation methodname and we want to recommend an addition of a method invocation the method name denoted by mname is the only variable in c. thus we have log score c ci logscore cmname ci d c ci logn c ci n ci cmname is an addition of a method call with the name of mname .
to consider the scope of the changes e.g.
the changes outside of the current method are weighed lower than those occurring inside it we set different constants for the weights of the factors log score c ci logscore cmname ci wscope ci d c ci logn c ci n ci wscope ciis the weight accounting for the scope of the change ci.
it equals if cioccurs in the current method of c and equals .
ifcioccurs outside of the method.
similarly we set the values of the weights wdep cifor the changes to code elements having data dependencies with the current element.
finally from equation log score c c log score cmname c log score c c1 log score c c2 ... log score c cn i ..nwscopeci wdepci d c ci logn c ci n ci .
.
computing score c t we estimate the likelihood score score c t ofcgiven the code context tin the same manner as the computation for the scorescore c c .score c t is estimated according to log score c t log score cmname t log score c t1 log score c t2 ... log score c tm i ..mwscopeti wdepti d c ti logn c ti n ti in this formula .t1 t2 .
.
.
tmaremcode tokens of interest in the code context.
.score c ti is the likelihood score that the code token tiin the surrounding context e.g.
the token for indicates the occurrence the change c e.g.
the addition of hashset.add .
.wscope tiis the weight on the scope of the token ti.
it equals if tiis within the current method of c and equals .
if tiis outside.
.d c ti is the distance between the token tiand the requested location.
it is computed similarly as d c ci .
.n c ti is the number of transactions in which the token tiis in the nearby code context of the change cin the change history.
n ti the number of transactions in which tiappeared.
from formulas and we compute score c c andscore c t .
.
training and recommendation before apireccan recommend api calls we must first train it.
we will explain how we train and use the model for recommendation.
.
learning change and code co occurrences according to formulas and to be able to compute score c c andscore c t apirecneeds to learn three types of parameters the numbers of co occurrences of the fine grained atomic changes i.e.
n c ci andn ci in formula the numbers of co occurrences of atomic changes and code tokens of interest i.e.
n c ti andn ti in formula .
the weights wcandwt one of which we fix by using wc wt .
we use hill climbing adaptive learning to learn the value forwcfrom a training set.
the idea of the training algorithm to adjust that weight is via gradient descent as follows.
first it is initialized with a value.
we train on k folds and test on one fold.
the parameters of the trained model is used to estimate the scores score c c andscore c t .
the combined score is computed with the current value of the weight wc.
the candidates for care ranked.
we compute the goal function map mactual plist between the list of predicted method calls plist and the actual one mactual .
the weight is then adjusted.
the process is repeated.
finally the optimal weight corresponding to the highest value of map is used.
.
api call recommendation after all above parameters were trained we use the formulas and with all the occurrence counts obtained during training to estimate the likelihood of a change c i.e.
an addition of a method invocation with the method name mname .
we compute the occurrence likelihood for all candidate changes in the vocabulary that satisfy the following i it is an addition of a method invocation and ii it has appeared in at least one transaction with at least one change in c1 .
.
.
cn or in at least one transaction with at least one token in t1 .
.
.
tm.
the second condition avoids the trivial cases of zero occurrences.
finally we rank the candidates by their scores.
let us explain this computation using the example in figure .
let us assume that the programmer finishes the changes and stops right after typing the variable results at line of figure 1b.
they request apirecto complete the code with an api method invocation.
our goal is to recommend a method call for the variable results .
table shows the computation of the scores.
all the atomic changes that preceded the current location are collected into thetable example of score calculation for candidates bold are highest component scores see operation notations in table ind.
operation type label score a candidate given ci candidates add remove contains addall clear c1add vds vds .
.
.
.
.
c2add pt pt .
.
.
.
.
c3add st set .
.
.
.
.
c4add sn sn .
.
.
.
.
... ... ... ... ... ... ... c11add st hashset .
.
.
.
.
... ... ... ... ... ... ... c13add es es .
.
.
.
.
ind.
token type label score a candidate given ti t1token for for .
.
.
.
.
t2token type task .
.
.
.
.
t3token var t .
.
.
.
.
... ... ... ... ... ... ... t6token mi execute .
.
.
.
.
combined score .
.
.
.
.
rank change context c1 c2 .
.
.
c13 assume that in this example there are no other changes outside of the method .
the tokens prior to the current location are considered including t1 t2 .
.
.
t6 i.e.
the code context .
the token types and labels are presented in table .
the candidate method invocations that satisfy the above conditions i and ii are listed in the columns including the method calls add remove contains addall and clear.
the scores in table shows the likelihood scores score c ci representing how likely the change coccurs given cioccurred in the change context of interest and score c ti representing how likely the change coccurs given the token tiappear in the code context .
score c ci andscore c ti are computed by formulas and .
for example the scores for the candidates with respect to the previous change c3 add st set adding a simple type set and to the previous change c11 add st hashset adding a simple type hashset are higher than others since in the training data the changes involving adding a variable with the type setorhashset often cooccur with the changes involving the addmethod of the variable of that type.
that is the change pattern consists of an addition of a variable of the type setorhashset followed by an addition of a call to the addmethod of that variable.
both of the scores for set andhashset are high since in the training data programmers might declare the type of hashset in some cases and that of setin others.
the scores for the candidates with respect to the prior tokens are computed similarly.
for example in table the scores for the candidates with respect to the token for is higher than those of the other tokens because a for iteration and the api method call hashset.add is part of the change pattern adding a loop collector .
among the candidate method calls more precisely the candidate changes with the change kind of addand the change ast node of methodinvocation the method call addhas the highest scores.
this is reasonable because programmers often use the method to collect elements into a collection via a loop.
finally the combined score for each candidate api call is computed according to the formulas and .
the method call addis ranked highest when other factors such as distance scope and dependency are considered as well.
.
empirical methodology to evaluate apirec we answer the following research questions rq1 accuracy how accurate is apirecwhen recommending api calls?
how does its accuracy compare to the state of the art ap proaches bruch et al.
set based approach on code only available as an advanced feature in eclipse n gram sequence based statistical approach with program dependencies and gralan statistical learning without modeling changes ?
rq2 sensitivity analysis how do factors such as the size of training data the requested location the sizes of the change context and code context impact accuracy?
rq3 running time what is the running time of a pirec?
.
corpora we compiled two disjunct corpora to train and test a pirec.
large corpus .
this corpus consists of randomly selected java projects from github that have long development histories commits each .
table shows the number of commits contained in this corpus.
based on previous research in order to avoid large commit size we do not select repositories which were migrated to github from a centralized version control system.
we extract the atomic changes from all the commits in the corpus.
to do this we iterate over all the files in all the commits.
we then use gumtree to compute the atomic changes see definition in section .
between the before and after versions of each file.
community corpus .
this smaller corpus contains eight projects from github that have been used by previous researchers .
the third column in table lists the statistics about this corpus.
we extract atomic changes from this corpus in the same manner.
.
evaluation setup we aim to investigate the foundation of our hypothesis the repetitiveness of changes.
we posit that changes performed on different projects and by different programmers have different degrees of repetitiveness.
thus to evaluate the impact that the project s culture and individual developer s habits play we designed three scenarios community edition .
we trained apirecwith the large corpus and then we tested it against the community corpus.
project edition .
for each project in the community corpus we trained apirecon the first of commits and then we tested on the remaining of commits fold validation .
user edition .
this is similar to the project edition scenario but we only used the commits from one user from each project.
we selected the user who authored the most commits in each project.
.
procedure metrics and settings to measure apirec s accuracy we reenact real world code evolution scenarios.
we use the real api calls from the corpora as the oracle for determining the correct recommendation.
for each transaction we choose an atomic change to be used as the prediction point .
the prediction point represents the change that apirecwill try to predict.
this mimics real development where a developer has typed part of the changes in a commit and at some point they invoke apirec.
we used the following procedure to choose the prediction point.
first we order the atomic changes in the transaction according to their locations in the file.
let nbe the transaction s size i.e.
it contains nchanges.
we assume the change is at different positions l ...nto study the impact of the prediction point on accuracy.
if the change at position lis the addition of an api callm we will use mas a prediction point.
otherwise we will check the atomic change at l 1and so on until we encounter a method addition.
if no such change is found we skip that transaction.
we use the atomic changes in the current transaction preceding the prediction point mas the change context.
we collect all the code tokens prior to min the current file as the code context.
the changes and code tokens that are outside of the method containing mare assigned lower weights section .
we invoke apirecwith thistable collected data on fine grained code changes large corpus community corpus projects total source files total slocs number of commits total changed files total ast nodes of changed files total changed ast nodes total detected changes total detected changes with jdk apis context to recommend the candidate list lof api calls.
if the method m the api call from the oracle is in the top kpositions of the list l of api calls we count it as a hit for top kaccuracy.
otherwise it is a miss.
the top kaccuracy for a project is calculated as the ratio between the number of hits over the total number of hits and misses.
model parameters.
apirechas two parameters the weights wcandwtfor the change and code contexts.
however we use adaptive learning to learn the optimal weights wcandwt from a training data section .
.
the other parameters are wscope ci wscope ti wdep ci and wdep ti which are the weights for the scopes and dependencies of the elements in the two contexts in relation to the prediction point.
since we focus on recommending the api call in the current method we use two levels of weights for the contexts inside and outside of the containing method and two levels of weights for having or not having data dependencies section .
.
empirical results .
accuracy community edition in this experiment we evaluate apirec s recommendation accuracy when it is trained on the large corpus and tested on the community corpus.
we compare apirecwith the state of the art api completion approach by raychev et al.
.
we implemented their n gram api recommendation model according to the description in their paper.
we also compare apirecwith bruch et al.
which uses association among apis in a setfor recommendation and with gralan a graph generative model.
we trained all those n gram based set based and graph based models with the source code of the entire last snapshots of the projects in the large corpus.
we compared the approaches in two settings on all apis in all libraries in the corpora and on the apis of the jdk library.
.
.
accuracy comparison for general api calls apirecis a statistical data driven approach which could not predict an api call that it has not seen in the training data.
this is often referred to as out of vocabulary oov .
an api call is considered to be oov if it is neither declared nor used in the training data but is in the testing data.
this can occur when apirecis trained on the large corpus but is tested on the community corpus.
for a fair comparison we measured in vocabulary accuracy in ofapirecand three above approaches figure .
to compute invocabulary accuracy we followed the same procedure as explained earlier except when we search for the prediction point we only stop when we find an api call m0thatpreviously appeared in the training data in our vocabulary .
if we cannot find m0in our training data we skip the current transaction and continue.
thus apireconly tried to predict methods which it had previously seen.
in this experiment we used apirecto predict the middle point of the change transaction l bn 2c .
the total number of recommendations for a project appears in parentheses after the project name.figure api recommendation accuracy for community edition per project the parentheses are the number of recommendations apirecoutperforms others across the board.
at top it is better from and at top from .
apirecachieves high accuracy.
top is correct in .
.
of the cases in .
top accuracy is as high as .
in .
we investigated the differences in accuracy between the approaches.
we found that n gram model requires strict unnecessary ordering between api calls.
for example in hashmap clone !put!remove andclone !remove !putare not considered as the same pattern by n gram.
thus clone !putcannot be used to recommend remove .
in contrast a clone was changed added together with an addition of either a putor a remove .apirecuses the context with the modification addition of clone to recommend either putorremove .
that is the atomic changes for the same high level intent often co occur in the same transaction e.g.
the tasks clone a hash map and add to it or clone a hash map and remove from it .
bruch et al.
does not consider the order of api calls for recommendation.
this could lead to more noise due to its use of different subsets of api calls for recommendation.
in contrast gralan uses partial orders among api calls leading to better performance.
for example clone !put!remove andclone !remove !putbelong to a pattern that clone appears before others but there is no strict order between putand remove .apirechas higher top kaccuracy than gralan for small ks.
there are two reasons apirecrelies on the repeated changes that a user has with high level intents e.g.
adding a loop collector that connect the fine grained changes logically and apirec s consideration of the impacts of the distance between context changes and the recommendation location nearby changes or code tokens are better for recommendation .
their accuracies are comparable for larger ks as more candidates are considered.
.
.
accuracy comparison for jdk in this section we evaluate apirec s accuracy in recommending a specific api library in contrast to general api calls .
we chose jdk for this experiment since it is frequently used in java programs.
we followed a similar evaluation procedure as before.
however while selecting the prediction point we search for a method mthat belongs to the jdk library after the location l. that is the actual change must be the addition of a method call from jdk.
we skip the transaction if we do not find such a method.
table shows the accuracy comparison.
the accuracy for recommending api calls in jdk is high.
in .
.
of the cases apireccan correctly recommend the jdk method call as the top recommendation.
in .
.
of the recommendation cases the actual jdk method call is in the top five candidates.
the accuracy in all the projects is higher than those for recommending general api calls figure .
this is expected since jdk is popular.
with the projects for training all the jdk api calls are in the vocabulary in .
at top apirecimproves over n gram by over bruch et al.
s by and over gralan by .
.
accuracy project edition in this experiment we aimed to evaluate the impact of project s culture on apirec s accuracy.
we trained tested it on the finegrained changes of the same project.
for comparison we used the testing projects in the community edition.
however for each project we sorted all the commits in the chronological order.
we then used the oldest of the project commits for training apirecand the most recent commits for recommendation.
tables and show accuracy results for the project edition setting for recommending general and jdk api calls.
the changes in individual projects do not repeat as much as those across projects.
thus the accuracy is generally lower than those in the community edition comparing table and figure tables and .
since jdk is a popular java library the code changes involving jdk apis still occur and repeat more frequently than the projectspecific method calls.
thus the accuracy in recommending jdk apis is higher than the accuracy in recommending general api calls comparing tables and .
note that in this experiment with our training projects all jdk api calls are in the vocabulary.
thus table only contains the result for a pirec.in.table jdk recommendation accuracy for community edition no oov apis in jdk library in this experiment system model top1 top2 top3 top4 top5 top10 galaxy apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
log4j apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
spring apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
antlr4 apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
jgit apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
froyo e apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
grid s apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
itext apirec .
.
.
.
.
.
n gram .
.
.
.
.
.
bruch et al.
.
.
.
.
.
.
gralan .
.
.
.
.
.
table api recommendation accuracy project edition system model top1 top2 top3 top4 top5 top10 galaxy apirec.oov .
.
.
.
.
.
apirec.in .
.
.
.
.
.
log4j apirec.oov .
.
.
.
.
.
apirec.in .
.
.
.
.
.
spring apirec.oov .
.
.
.
.
.
apirec.in .
.
.
.
.
.
antlr4 apirec.oov .
.
.
.
.
.
apirec.in .
.
.
.
.
.
jgit apirec.oov .
.
.
.
.
.
apirec.in .
.
.
.
.
.
froyo apirec.oov .
.
.
.
.
.
email apirec.in .
.
.
.
.
.
grid apirec.oov .
.
.
.
.
.
sphere apirec.in .
.
.
.
.
.
itext apirec.oov .
.
.
.
.
.
apirec.in .
.
.
.
.
.
table shows that the values for apirec.in are slightly higher than those for apirec.oov.
this is reasonable since most used method calls existed.
thus if we use a change history in a project covering as many project specific methods as possible apireccan be used to recommend the calls to those methods.
the same trends apply for top kaccuracy numbers for other models not shown in table .
a pirecoutperforms the other models across the board.
.
accuracy user edition in this experiment we evaluate how apirecperforms when being trained only on the commits from a single user.
from each project in the test corpus we selected the user who has the mosttable jdk recommendation accuracy for the project edition no oov apis in jdk library in this experiment system model top1 top2 top3 top4 top5 top10 galaxy apirec31.
.
.
.
.
.
log4j apirec24.
.
.
.
.
.
spring apirec34.
.
.
.
.
.
antlr4 apirec24.
.
.
.
.
.
jgit apirec16.
.
.
.
.
.
froyo e apirec22.
.
.
.
.
.
grid s apirec60.
.
.
.
.
.
itext apirec33.
.
.
.
.
.
table api recommendation accuracy for user edition system user top1 top2 top3 top4 top5 top10 galaxy dandiep .
.
.
.
.
.
log4j ceki .
.
.
.
.
.
spring jhoeller .
.
.
.
.
.
antlr4 parrt .
.
.
.
.
.
jgit spearce .
.
.
.
.
.
froyo e mblank .
.
.
.
.
.
grid s novotny .
.
.
.
.
.
commits.
a user makes up a significant part of each project ranging from to of all commits in a single project.
to compare results we used the same projects as the project edition.
we used the same sorting technique so that of the commits were used for training and the most recent were used for recommendation.
table shows the result for the user with most commits from each project.
when compared with figure and table accuracy in the user edition is lower than that in community edition.
we expect this result because more training should yield better results.
interestingly the user edition s accuracy generally is higher than that of project edition .
for the data we randomly selected each user commits to only one project.
this leads us to infer that there is a subset of the training data that is more important.
considering code authorship we could train with less data yet have more precise results than when training with the entire project .
.
sensitivity analysis out of vocabulary data as with all statistical learning methods apirec s results are affected by the sufficiency of the training data.
thus we evaluated to what extent oov impacts apirec s accuracy.
we chose a random project froyo email from the community corpus.
we conducted two executions with two experimental procedures to compare accuracy when oov occurs and does not occur.
in the first execution we followed the same procedure in section .
to measure top k accuracy.
for this study we use the prediction point at the middle of a transaction i.e.
l bn 2c .
when an api call is oov we counted it as a miss.
in the second execution we followed the same procedure to measure in vocabulary accuracy as described in section .
.
.
to be able to compare against the two runs apirec made exactly recommendations in each execution.
table shows the top kaccuracy for the two cases.
even with the oov issue apirecis able to achieve high accuracy.
with a single recommendation it is able to correctly recommend the api call in almost of the cases.
in .
of the cases the actual call is in the list of only five candidates.
apirec s accuracy is even higher if trained with enough api calls the in case .
here it is able to correctly recommend the call with a single recommended candidate in almost of the cases.
in of the cases it correctly recommends the call with only five candidate apis.table impact of oov on recommendation accuracy top top top top top top oov .
.
.
.
.
.
in .
.
.
.
.
.
figure impact of change context s size on accuracy when apirecuses in vocabulary elements it makes better recommendations ranging from .
to .
improvements in accuracy.
we measured the oov rate defined as the percentage of predicted api call in the community corpus that are not contained in the large corpus.
the oov rate for froyo email project is .
a lower bound of the loss in accuracy.
this result shows that oov has impact on accuracy.
this is expected because as a learning model apirecneeds to observe the api calls to recommend them.
.
sensitivity analysis change context and code context apirecrelies on both change and code contexts in recommendation.
thus we also conducted an experiment to measure the impact of the size of the change context number of changes and that of the code context number of code tokens preceding the prediction point.
we randomly chose a project antlr in community corpus.
we varied the contexts sizes and measured in vocabulary accuracy.
figures and show accuracy with various sizes of either contexts.
as seen when increasing either context s size accuracy improves.
however the impact on accuracy for the size of the change context is higher when considering small sizes.
for example increasing the change context s size from to top accuracy improves while increasing the code context s size from to accuracy improves only .
thus adding more changes to the context helps connect better the changes in a high level intent leading to higher accuracy.
the increase is smaller when adding more code tokens to the code context.
when contexts sizes are greater than prior changes or tokens accuracy improves only slightly since the information needed to correctly recommend was sufficient.
thus we chose as the default sizes for both contexts.
this result also shows that apirecmaintains reasonably high accuracy with small sizes of contexts of prior changes and code tokens .
.
sensitivity analysis prediction locations because apirecis also based on the change context i.e.
prior changes selecting a prediction point among nchanges in a transaction might have impact on accuracy.
thus we conducted another experiment to measure that.
we first chose a random project jgit from the community corpus after training apirecon the large corpus.
we chose a prediction point at three locations among n changes in a transaction the first quartile point l1 bn 4c the middle point l2 bn 2c and the third quartile point l3 b3 n 4c .
we followed the same procedure to measure top k accuracy.
we used the in vocabulary setting.
table shows the accuracy with different prediction locations.
as seen accuracy slightly increases if we move the point to a later figure impact of code context s size on accuracy table impact of recommendation points on accuracy location top top top top top 1st quartile .
.
.
.
.
middle point .
.
.
.
.
3rd quartile .
.
.
.
.
part of a transaction from 1stto3rdquartile point.
this is expected because apireccollects more prior changes in the change context.
.
sensitivity analysis training data s size we want to analyze the impact of the size of the training dataset of fine grained changes on accuracy with the test project antlr in the community corpus.
we built training datasets by increasing their sizes with additional projects in github from to projects.
we ranapirecfor each dataset .
as seen top accuracy increases from .
to .
with more training data.
as expected larger training data sets perform better however the improvements are small.
this shows that a minimum training set of projects produces good results.
results get stable when we use projects.
.
running time all experiments were run on a computer with xeon e5 .1ghz configured with thread and 32gb ram .
the time complexity is reported in table .
the training time is significant but training can be done off line for our model.
the recommendation time is short second per recommendation thus apirecis suitable to be interactively used in an ide.
.
threats to validity our corpus in java only might not be representative.
for different projects with different oov rates the results vary.
for comparison we ran all approaches on the same dataset and measured in accuracy.
we also evaluated the impact of oov data section .
.
we do not have the tool in raychev et al.
it is not publicly available but we carefully followed the approach described in the paper.
the simulated procedure in our evaluation is not true editing.
the choice of the prediction point at the middle of a change transaction could affect the accuracy as seen in section .
.
however it is representative as it achieves neither the best nor the worst accuracy.
the study on usefulness needs to involve human subjects and will be part of our future work.
the results for jdk might be different for other libraries.
but apirecis general for any library and language.
.
discussion .
limitations our approach also has shortcomings.
first out of vocabulary is an issue.
however as seen even with only projects in the pres table impact of training data s size on accuracy number of projects top top top top top dataset .
.
.
.
.
dataset .
.
.
.
.
dataset .
.
.
.
.
dataset .
.
.
.
.
dataset .
.
.
.
.
dataset .
.
.
.
.
table performance cross project within project storage mbytes mbytes training time hrs projs min recommendation time avg .
sec change .
sec change max 2s change sec change ence of oov apirec s accuracy is high.
moreover if trained with sufficient data apirecperforms better than existing approaches in vocabulary accuracy .
second using the file level as the scope for transactions may also lead to a loss of accuracy.
however the majority of change patterns appear in the same file .
in some cases files may contain tangled changes changes from multiple tasks that are committed together .
this can introduce noise while learning as spurious changes would be associated with a pattern.
we also miss changes that span across files but are part of the same pattern.
finally we handle code context by finding the associations between code tokens in transactions.
a potential alternative is the combination between apirecand a language model for code e.g.
gralan cache model or rnn lm .
.
implications we group implications by three categories of audiences developers tool builders and researchers.
developers .apirecis data driven so choosing the right training corpus is important.
our results suggest that training on the community corpus leads to higher accuracy than when training on a project.
when training and testing on changes of one user the accuracy was between that of the community corpus and each project .
even though the project edition results are not directly comparable to the other two because the user commits are fewer than the projects the results suggest that with a user specific change history the user trained model achieves better accuracy.
the implication is that users should obtain a community trained model and then further refine it with their own changes .
tool builders .
section .
shows that a challenge when using statistical learning recommenders is the long training time for the model.
the problem can be mitigated in different ways.
first the community models can be trained by the tool vendors and offered with the tool.
second the user s continuous integration server can incrementally augment the community model each night with the changes that the user committed during the day.
researchers .
our paper together with recent related work shows that fine grained code changes are highly repetitive.
this opens up new research topics in mining fine grained changes.
first researchers can mine changes to actively help developers during development code completion dynamically learned refactorings record replay of bug fixes etc.
second they can mine changes to learn and develop theories on the nature of software change such as building catalogs of software change building blocks incorporating change patterns into atomic changes via language and ide design predicting the changes required for a task etc.
.
related work code and api completion based on statistical language models.
hindle et al.
use the n gram model on code tokens to show that source code has high repetitiveness and then use it to recommend the next token.
tu et al.
enhance n grams with caching of recently seen tokens.
raychev et al.
and gralan were explained earlier.
white et al.
applied rnn lm on code tokens to achieve higher accuracy than n gram.
mou et al.
s tree based convolutional neural network is applied to source code to recommend syntactic units.
allamanis et al.
use bimodal modeling for short texts and code snippets.
maddison and tarlow use probabilistic context free grammars and neural probabilistic language models for source code.
naturalize learns coding conventions to recommend identifier names and formatting conventions.
in comparison to those statistical approaches apirechas key differences.
first they are based on the principle of code repetitiveness while apirecrelies on the repetitiveness of fine grained code changes .
second apirecis tailored toward method invocation recommendation including api calls.
other models except gralan and raychev et al.
are either for general tokens or for special purpose recommendations e.g.
ast structures coding conventions or methods classes names .
code and api completion based on mined patterns.
bruch et al.
propose best matching neighbor algorithm for code completion that uses as features the setof api calls of the current variable vand the names of the methods using v. the set features in the current code is matched against those in the codebase.
grapacc mines patterns as graphs and matches them against the current code.
robbes and lanza improve code completion using recent modified inserted code during an editing session.
we train our model on change histories rather than the editing changes in a session.
there exist other deterministic approaches to improve code completion andcode search by using editing history cloned code api examples and documentation structural context parameter filling interactive code generation specifications documentation type information feature requests .
apiexplorer leverages structural relations between apis to facilitate their discoverability.
.
conclusion this work is the first that leverages the regularity of fine grained code changes in the context of api code completion .
whereas the previous approaches used the regularity of idioms of code tokens in this paper we address the problem of api method recommendation by a statistical learning model that we train on fine grained code changes.
when we mine these in a large corpus the changes belonging to higher level intents will appear more frequently than project specific changes.
our thorough empirical evaluation shows thatapirecimproves over the state of the art tools between for top recommendations.
it performs well even with a one time minimal training dataset of publicly available projects.
we found that training the model with the changes from individuals achieves higher accuracy than training with changes in their entire projects.
thus the recommender could be trained from a large corpus of community projects and an individual user could further refine the model with their own changes.
.