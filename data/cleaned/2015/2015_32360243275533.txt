facts automated black box testing of fintech systems qingshun wang1 lintao gu1 minhui xue2 lihua xu1 wenyu niu4 liang dou1 liang he1 tao xie5 1east china normal university china2optus macquarie university cyber security hub australia 3new york university shanghai china4cfets information technology co. ltd. china 5university of illinois at urbana champaign usa abstract fintech short for financial technology has advanced the process of transforming financial business from a traditional manualprocess driven to an automation driven model by providing various software platforms.
however the current fintech industry still heavily depends on manual testing which becomes the bottleneck of fintech industry development.
to automate the testing process we propose an approach of black box testing for a fintech system with effective tool support for both test generation and test oracles.
for test generation we first extract input categories from business logic specifications and then mutate real data collected from system logs with values randomly picked from each extracted input category.
for test oracles we propose a new technique of priority differential testing where we evaluate execution results of system test inputs on the system s head i.e.
latest version in the version repository against the last legacy version in the version repository only when the executed test inputs are on new not yet deployed services and against both the currently deployed version and the last legacy version only when the test inputs are on existing deployed services .
when we rank the behavior inconsistency results for developers to inspect for the latter case we give the currently deployed version as a higherpriority source of behavior to check.
we apply our approach to the cstp subsystem one of the largest data processing and forwarding modules of the china foreign exchange trade system cfets platform whose annual total transaction volume reaches trillion us dollars.
extensive experimental results show that our approach can substantially boost the branch coverage by approximately and is also efficient to identify common faults in the fintech system.
ccs concepts software and its engineering software testing and debugging keywords fintech black box testing automated test generation lihua xu is the corresponding author.
email lihua.xu nyu.edu.
liang dou is the corresponding author.
email ldou cs.ecnu.edu.cn.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
reference format qingshun wang lintao gu minhui xue lihua xu wenyu niu liang dou liang he and tao xie.
facts automated black box testing of fintech systems.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction fintech short for financial technology refers to a set of new information technologies to innovate financial services.
these latest innovations not only benefit consumers but also push businesses to transform from a manual processing driven model to an automation driven model by creating robust and high quality software systems.
according to citigroup an estimated billion of investment poured into fintech in .
china foreign exchange trade system cfets information technology co. ltd. as a subsidiary of china s central bank provides a trading platform for exchange rate swaps as well as currency swaps and forwards for more than active clients.
the annual total transaction volume reaches trillion us dollars for which any unexpected failures on trading services could result in huge losses.
therefore it is critical to conduct sufficient testing on such fintech software systems to ensure system robustness and correctness.
unfortunately according to our field observation despite its increasing needs the current state of the practice in the fintech industry still heavily relies on manual testing due to high complexity of such fintech systems.
we have made attempts to apply existing testing tools to the foreign exchange platform developed and owned by cfets .
we name such platform as the cfets platform which consists of multiple cfets subsystems and each subsystem provides a set of services to the users.
based on our such attempts we identify three main challenges that hinder direct adoption or adaptation of existing testing techniques and tools.
first a cfets subsystem takes high dimensional inputs each of which contains a group of fields with drastically different data types.
such high dimensional inputs are beyond the capabilities of most constraint solvers which are the underlying engine for dynamic symbolic execution tools such as klee exe and pex posing challenges to apply these white box testing tools.
second each input dimension is defined as various data types and formats typically of complex user defined data structures.
most automated test generation tools such as randoop and evosuite fail to obtain the input type and let alone generate valid input values.
third although specification based robustness testing tools such asballista can be applied to a cfets subsystem testing the system requires manually defined exceptional input values which are missed by such tools due to the high complexity of the subsystems.
esec fse november lake buena vista fl usa q. wang l. gu m. xue l. xu w. niu l. dou l. he and t. xie moreover to ensure its intended functionalities testing the system needs not only a set of exceptional input values but also normal input values in the huge value space of system inputs.
even worse due to a cfets subsystem s complex behavior and various formats of output it is practically impossible to completely portray the system behavior as assertions and hence executable test scripts with assertions are impractical for the system.
additionally we observe that it is a central business requirement for cfets as a service provider to serve more than active clients to render the existing deployed services unchanged by users perspective during the services constant system upgrades.
as client population grows drastically by day new services are constantly added to the deployed system to supplement the existing deployed services during these system upgrades.
even before these new services deployment they undergo substantial evolution during the development process.
before deployment these new services do not have their counterpart services in the deployed system as the reference version required in traditional differential testing.
as a result cfets still primarily relies on domain experts to manually examine the execution results for every test input being very time consuming and error prone.
in the end we reach a consensus with the domain experts and developers at cfets that what we need is not only an automated testing generation technique but also a means to evaluate the execution results for these generated inputs.
additionally both the normal and exceptional inputs are equally important to test the system.
in this paper we aim at studying the aforementioned challenges of automatically testing such fintech systems.
to achieve this goal we propose facts automated bla ck box testing for fintech systems.
to address the challenges of high dimensional inputs we leverage the input data collected from system logs as seeds.
messages collected from real execution runs serve as the basis of these highdimensional inputs.
to handle the user defined data type for each input dimension we leverage the business logic specification to retrieve their data types.
to tackle the challenges of exploring the huge input space we extract input categories from business logic specifications and randomly sample from each input category to produce both normal and exceptional inputs.
finally to tackle the challenges of the oracle lacking problem for new services before deployment we propose a new technique of priority differential testing where we evaluate execution results of system test inputs on the system s head i.e.
latest version in the version repository against the last legacy version in the version repository only when the executed test inputs are on new notyet deployed services and against both the currently deployed version and the last legacy version only when the test inputs are on existing deployed services .
for the head version of existing services the behavior of the currently deployed version already experienced by many users represents a higher priority reference behavior to check against than the last legacy version in the version repository.
however for the head version of new not yet deployed services given that there exists no currently deployed version we leverage the last legacy version in the version repository to check against.
note that any new service added to the system could potentially affect the behavior of the existing services with theirearlier version deployed already .
despite stable the last legacy version may still include unintended behaviors for the existing services due to the addition of new services.
hence we consider the currently deployed version to have higher priority to check against than the last legacy version when both versions are available for existing services under test.
applying facts to a major subsystem of the cfets platform helps achieve a substantial increase on branch coverage by more than in comparison to the original manual tests.
such benefit comes from our generation of both normal and exception inputs while the record of manual testing shows that testers mainly focus on normal inputs.
the improvements over manual testing effort are not only statistically significant but also practically substantial given that the number of active cfets clients is daily increasing.
it is not unusual to have exceptional inputs from such a large scale input space and any failure behavior caused by exceptional inputs may lead to great financial losses.
furthermore the priority differential testing built in facts is able to quickly prioritize problematic services greatly reducing human effort.
selectively using both the currently deployed version and the legacy version the execution of most generated test inputs can be checked automatically.
in our current implementation in addition to commonly used crash metrics we raise concerns to different types of inconsistent behaviors between different versions.
the top of the prioritized list is the inputs that trigger the currentlydeployed version and the legacy version to produce the same result while the head version shows a different result which we consider has the highest probability of containing failures due to faults.
the second top to the list is the inputs that trigger all three versions to produce different results which need to be checked manually.
the full ranked list is described in section .
.
all these inconsistent behaviors are recorded but records with low priority can be filtered by developers first so that they can focus on those inputs whose executions are likely to trigger failures due to faults.
in our empirical study we find that only a very small portion of test inputs need to be examined manually instead of the whole set of test inputs.
in summary this paper makes the following main contributions the first industrial case study of testing a fintech software system with over active clients including banks investment funds etc.
a new technique to automatically test a software system with high dimensional inputs including various data types.
a new technique of priority differential testing to automatically evaluate and rank test results.
background to illustrate the challenges faced in testing a fintech system we take as an illustrative example the cstp subsystem owned and developed internally by cfets as one of the subsystems of the cfets platform for data processing and forwarding.1we choose to target on cstp because it belongs to the fundamental part of the cfets platform which is responsible for processing all data generated in each transaction transforming them into correct format and forwarding to appropriate targets its main functionality 1we have access to all source code and previous system logs under a non disclosure agreement.
840facts automated black box testing of fintech systems esec fse november lake buena vista fl usa figure cfets platform table data field for input messages fieldname datatype length format accuracy range valuedate date yyyy mm dd lendingamount number clearingmethod enum a b c tradercode text traderid institueid quoteid text a.b.ccccccc .... .... ... ... ... includes parsing messages and manipulating on data making it possible to observe its behavior through outputs and logs.
during the operation of the cfets platform whose interface is shown as figure the cstp subsystem receives from upstream subsystems message data generated for every transaction applies proper transformation to the data translates to the imix format which is a standard electronic communication protocol for realtime transmission of financial information in the inter bank market trading activities.2the resulting imix message is then forwarded to appropriate institutions subscribed in the system and gives the corresponding permissions to access the data.
the message data i.e.
the input data of the cstp subsystem is shown in key value pairs.
each input message consists of two parts aheader represents the message type followed by a sequence of key value pairs.
each key value pair corresponds to a field which describes a part of information about the transaction related to the message.
the keycorresponds to the field name and the value can be one of the various data types with some specific requirements.
table shows the data field requirement for each message type.
some fields have additional domain specific regulations such as thequoteid field.
we do not include details here due to space limit.
a valid value of a field should meet its requirement.
our goal is that invalid values such as date values not matching the format or numbers out of range should be properly detected by the system thus avoiding crashing the whole system.
in the meantime valid values should be properly handled and their execution results should be checked against expected results.
challenges software testing is the process of generating and executing test inputs to cause failures for the system under test.
it has been shown to be an effective way to ensure the robustness and correctness of the system under test.
however manually writing test cases i.e.
test inputs and test oracles for a large and complex system under test such as the cstp subsystem can be highly expensive .
thus figure a subset of fields in an executionreport message there exist many automated testing techniques and tools such as randoop and evosuite which automatically generate test inputs and execute them to reduce human effort.
unfortunately such existing automated testing techniques and tools cannot be effectively applied to test a fintech system due to four main challenges as listed below.
high dimensional input.
the cstp subsystem processes transaction data as messages.
the messages are stored and transmitted as different types each of which contains hundreds of different fields.
figure shows a small subset of fields contained in a message of type executionreport .
test input generation for such subsystems requires constructing such a complex message structure which can be very challenging for state of the art testing tools as well as time consuming and error prone for current manual testing.
various data types and formats.
each input dimension i.e.
the fields of each message has again drastically different data structures many of which are user defined and domain specific see table .
it is difficult to extract such data types and generate meaningful data accordingly.
as a result existing tools such as randoop andevosuite generate plenty of trivial exceptional inputs which typically trigger an exception and then are blocked at the beginning of the system execution.
these generated inputs are inefficient to test the system and let alone cause faults located deep in the system to be exposed as failures.
huge value space.
to explore the input space of the cstp subsystem one needs to explore the cartesian product of input dimensions.
thus it is difficult for fuzzing to effectively expose failures in the cstp subsystem especially for those that can be triggered by only specific corner cases of related input dimensions.
specificationbased testing tools such as ballista can be applied to eliminate the pressure by constructing inputs using values drawn from a predefined dictionary.
it typically requires to manually define the input values that are likely to trigger failures such manual process is very time consuming given the complexity of cstp s input domain.
additionally it is highly challenging to produce a complete set of values.
if a failure is triggered by a specific value not included in the dictionary such tool then would fail to expose the corresponding failure.
lack of testing oracles.
faults that lead to certain generic failures such as system crashes and hangs can be detected without strong test oracles.
however the functional correctness of the system under test is also of great value and faults that lead to functional incorrectness on system outputs cannot be detected without strong test oracles.
for large scale fintech systems incorrect system outputs may lead to a huge loss.
manually verifying the output of every generated test input is too expensive and infeasible.
similarly specifying an oracle for black box testing requires huge amount of work and a comprehensive oracle for the cstp subsystem is not yet available due to its complexity.
841esec fse november lake buena vista fl usa q. wang l. gu m. xue l. xu w. niu l. dou l. he and t. xie system logpriority differential testing business logic mutation operators010100010100100input seeds category settest inputstest generatorexecution enginehead version currently deployed legacy version prioritized listname ...date ...phase i test generationphase ii execution figure overview of facts algorithm equivalent category partitioning input spec the message business logic document for the message type.
output eqvcatmap a map message structure that contains the equivalent category information for each field.
dataformatmap parsedocument spec eqvcatmap new emptymap fordataformat dataformatmap do fieldname dataformat.getfieldname formatinfo dataformat.getformatinfo eqvcatlist partitioneqvcat formatinfo eqvcatmap.put fieldname eqvcatlist end return eqvcatmap our approach as discussed in section at a high level each input data to the cstp subsystem is represented as a message that contains a set of fields each of which further represents different types of data.
each field has its own requirements and definitions for possible values and in turn each message is of different type based on its data fields.
the cstp subsystem processes many different types of transactions at the same time each of which has its own set of related data fields.
in theory verifying the robustness and correctness of the cstp subsystem needs to feed it with all possible valid and exceptional messages and check whether the system returns an expected output.
in practice so is infeasible.
a more practical approach is to craft a set of representative test inputs instead and cover the system behavior as much as possible.
as shown in figure facts generates input as objects recognized and processed by the subsystem and then evaluates the execution results with proposed priority differential testing.
facts collects system logs from real transactions and retrieves passing messages as input seeds to construct the high dimensional test messages in the meantime facts also identifies the input category of each data field for each corresponding test message and mutates over the seed messages to cover all possible field categories.
facts then compares execution results from three versions of thecstp subsystem to determine failure triggering inputs and outputs a prioritized input list.table mutation operators operator description rpl f c replacement .
replace the value of a field fwith a value randomly picked from a category c. del f deletion .
delete a field fappearing in the message.
ins f c insertion .
insert a field fwith a value randomly picked from a category c. .
test generation we first construct the input categories used for deriving test inputs.
we take as input the business logic specification that defines requirements for each data field and output a map that contains all the equivalent categories of each field see algorithm .
an equivalent category of a field is hereby defined as a set of all possible values where each element in the same category is equivalent to revealing the system s behavior.
for example for a field of data type date all the valid and invalid values can be classified into different categories respectively.
each category can be further subdivided accordingly.
different types of incorrectness may involve different parts of code in the system.
a date value in a wrong format may be blocked by the first check in a method of the system while an input in a correct format however representing a nonexistent date e.g.
feb 30th may pass through and crash the whole system.
valid inputs can also be divided into smaller pieces such as a field of data type enum which has a finite set of possible values.
different choices from a set can lead the system execution to execute different branches each of which belongs to a unique category.
in theory a category is a set of values with some properties in common.
however it is infeasible to enumerate every element in the set in real world applications.
in facts an equivalent category is characterized with a finite set of rules.
pick an element from the category is realized with generating a value that satisfies a rule for the equivalent category.
for example a category that contains all strings with a specific format can be described as a regularexpression pattern such as strings representing a date .
we can construct an automata for the corresponding pattern that walks through from the start state to the accept state.
the composition of symbols used in the transition is the value that we expect.
once the equivalent categories are obtained facts generates test messages based on real messages collected from actual transactions taking place on the platform.
test inputs are created by mutating the collected real messages and transformed to corresponding objects to the subsystem.
we define three types of mutation operators in table .
each operator takes at least one parameter that points to the target field.
randomly picking a value from a given category is equivalent to constructing a value that satisfies the rule characterizing the category as mentioned earlier.
842facts automated black box testing of fintech systems esec fse november lake buena vista fl usa algorithm input generation input eqvcatmap the output of the phase .
input initmessageset a set of real test messages extracted from the system log.
output testmessageset the test messages generated.
testmessageset new emptyset formessage m initdataset do fieldlist getfields m forfield f fieldlist do generate a new message mdby applying del f onm testmessageset.add md categorylist eqvcatmap.getcategories f forcategory c categorylist do generate new messages mrby applying rpl f c onm testmessageset.add mr end end missingfieldlist getmissingfields m forfield fm missingfieldlist do categorylist eqvcatmap.getcategories fm forcategory c categorylist do generate new messages miby applying ins f c onm.
testmessageset.add mi end end end return inputset table an example of a generated test message name valuedate lendingamount clearingmethod quoteid ... value .
a .
.
... algorithm describes how we generate test messages.
due to space limit we show here a partial example of a generated test message in table .
the algorithm takes as input initially computed eqvcatmap andinitmessageset .initmessageset is a set of messages collected from real transactions.
we iteratively apply three mutation operators to an original message to create a new message with entirely valid or partially invalid values.
the mutation is applied to a single field of a newly created message.
although applying mutation operators to multiple fields may be more comprehensive the combinatorial explosion could lead to a very large set of test cases that require unaffordable time and computational resources to execute.
thus we choose to apply mutation to a single field and according to a recent study so can help detect most of faults in the system under test.
.
test oracles we feed the generated test inputs into the system under test and record the inputs that cause potential failures.
to determine potential failures based on the observed system outputs we propose the technique of priority differential testing.
in particular we test the system under test with two previous versions in parallel the currently deployed version denoted as sc and the last legacy version in the version repository denoted as sl.
we assume that the currently deployed version represents the correct existing system behavior because it has been stably running for a long time in a platform with a large number of clients which send countless messages to the system.
such currently deployed version can be regarded as fully tested.
we define that the last legacy version refers to the latest stable version in the version repository note that the last legacy version includes new services which do not exist inthe currently deployed version.
the version under test is named the head version denoted as sh which typically contains all the services from the last legacy version with additional updates.
the currently deployed version is considered as the primary reference version for the existing system behavior because despite stable the legacy version may still include uncertain behavior due to the addition of new services.
in the meantime the legacy version is utilized as the reference version for new services which do not exist in the currently deployed version.
if a test input involves new features e.g.
a test message contains newly defined fields at least the two newer versions legacy and head versions should have the same behavior.
we consider any inconsistent output as a potential failure and record the corresponding input.
we prioritize recorded inputs along with their triggered potential failures for developers to inspect.
table lists details of different priorities of different system behaviors where rank represents the highest priority and is the lowest.
we assign inputs that cause system crashes or hangs with the top priority followed by inputs that trigger scand slto produce the same results while sh produces a different result.
shis the most likely to contain faults that break an existing service in this situation.
inputs that trigger all three versions to produce different results may involve new services added in sh but another possibility is this situation reveals that sh breaks some services added in sl.
both situations need additional manual checking so we assign such situations with the third top priority.
if scand shproduce the same results different from sl it may be related to a bug fix.
if two newer versions produce the same output but different from sc the input may involve some new services in sl.
although these two situations are very likely to be correct we still recommend them for developers to further inspect in the lowest priority.
evaluation in this section we apply our approach to test the cstp subsystem and intend to answer the following two research questions rq1 how effective is our approach to comprehensively test the system?
rq2 can our approach effectively expose faults as failures in the system?
to answer rq1 we use branch coverage measured by using jacoco to evaluate the effectiveness of our approach after executing the generated test cases.
full branch coverage requires that each branch in the system under test has been executed at least once during testing.
to construct a comparison baseline we first tried existing automated test generation tools such as evosuite andrandoop .
unfortunately as discussed earlier in section these tools were not able to generate meaningful inputs.
thus we resort to the manuallydesigned test cases stored at the development repository.
due to the space limit we show branch coverage for five important classes in the system under test achieved by the manually written test cases and our approach respectively.
as shown in table nearly half of the branches are not covered by manually written test cases indicating that many situations especially exceptional situations of test messages have never been tested unable to offer high confidence on the system s robustness.
843esec fse november lake buena vista fl usa q. wang l. gu m. xue l. xu w. niu l. dou l. he and t. xie table priority rank system behavior rank system crashes or hangs scandslget same output different from sh three different outputs scandshget same output different from sl slandshget same output different from sc table branch coverage comparison class name manually written test our approach fxcldeallogparser .
.
fxoptiondeallogparser .
.
fxdeallogparser .
.
firdvdeallogparser .
.
cstpmemberconfirminfoparser .
.
table fault cases for the cstp subsystem faulty version description faulty remove code involving field tradingmodecode .
faulty remove code handling a certain value of enum field exercisestatus .
faulty remove code for setting field value of output imix message when processing field quoteinstitutiontradercode .
faulty remove code for checking value formats when processing field dealtime .
faulty set value to wrong field in the output imix message when processing field period .
in contrast our approach can generate test cases that cover most of the branches in the classes under test offering much higher confidence on the system s robustness.
to answer rq2 we have approached the developers of the cstp subsystem for understanding the system s common faults introduced during past development.
it turns out that many faults were introduced because a careless developer neglected to deal with some rarely used data fields or conditions.
these faults were not exposed by manually written test cases and consequently were uncovered during real transactions upon deployed services.
to investigate whether our approach is able to expose such common faults we construct real faulty versions to reflect those previously exposed faults during system deployment and also create additional synthetic faulty versions following the observed common fault patterns by manually removing code blocks related to a field or some conditional branches table shows the five fault cases .
our experimental results show that our approach is able to identify all of these five fault cases demonstrating that applying mutation on a single field can effectively expose faults in the system under test as failures.
discussion and lessons learned in this section we discuss multiple lessons learned in our collaboration with the developers of the cstp subsystem.
generic tools are not effective for complex systems.
typically generic automated test generation tools are not effective in the context of testing a fintech system.
methods in a fintech system accept high dimensional data as parameters each of which may contain a group of fields with drastically different data types making it difficult for generic tools to generate effective test cases in our preliminary studies state of the art tools such as randoop and evosuite cannot even construct a valid argument.
exceptional inputs should receive more attention.
a fintech system usually serves as an application platform for millions of clients and a large amount of data is fed into the system each day.
such situation implies that most of the regular execution paths have been fully exercised.
if a valid message can trigger a failure this failure has a great chance of being discovered in a previous system version.
in contrast those invalid messages that contain some exceptional values are less likely to appear in real execution environments.
if there exists any fault remaining unrevealed in the system the fault is more likely to be triggered by exceptional inputs.
in our approach we mutate real data not only with valid values but also with different types of exceptional values to check whether there is any fault hidden in irregular execution paths.
universal oracles do not exist.
it is hard to find a universal test oracle for large scale complex and domain specific systems such as fintech systems.
cfets still relies on domain experts to examine the execution results for test inputs.
we propose priority differential testing to reduce human effort by checking the consistency among multiple versions but it still takes much time for experts to determine whether an inconsistency reveals a fault in the system.
conclusion in this paper we have presented our work on testing fintech systems in collaboration with china foreign exchange trade system information technology co. ltd a subsidiary of china s central bank .
to address challenges faced when testing a fintech system we have proposed facts an approach of black box testing for a fintech system with effective tool support for both test generation and test oracles.
we have applied our approach to the cstp subsystem and the evaluation results show that our approach can achieve much higher branch coverage than the system s existing manually written test cases and our approach is effective to expose common faults introduced during system development.