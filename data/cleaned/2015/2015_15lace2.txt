lace2 better privacy preserving data sharing for cross project defect prediction fayola peters tim menzies and lucas layman lero the irish software research centre university of limerick ireland.
email fayolapeters gmail.com computer science north carolina state university usa.
email tim.menzies gmail.com fraunhofer center for experimental se college park usa email llayman fc md.umd.edu abstract before a community can learn general principles it must share individual experiences.
data sharing is the fundamental step of cross project defect prediction i.e.
the process of using data from one project to predict for defects in another.
prior work on secure data sharing allowed data owners to share their data on a single party basis for defect prediction via data minimization and obfuscation.
however the studied method did not consider that bigger data required the data owner to share more of their data.
in this paper we extend previous work with lace2 which reduces the amount of data shared by using multi party data sharing.
here data owners incrementally add data to a cache passed among them and contribute interesting data that are not similar to the current content of the cache.
also before data owner ipasses the cache to data owner j privacy is preserved by applying obfuscation algorithms to hide project details.
the experiments of this paper show that a lace2 is comparatively less expensive than the single party approach and b the multiparty approach of lace2 yields higher privacy than the prior approach without damaging predictive efficacy indeed in some cases lace2 leads to better defect predictors .
i. i ntroduction when data are insufficient or non existent for building quality defect predictors software engineers can use data from other organizations or projects.
this is called cross project defect prediction cpdp .
acquiring data from other sources is a non trivial task when data owners are concerned about confidentiality.
in practice extracting project data from organizations is often difficult due to the business sensitivity associated with the data.
for example at a keynote address at esem elaine weyuker doubted that she will ever be able to release the at t data she used to build defect predictors .
due to similar privacy concerns we were only able to add seven records from two years of work to our nasa wide software cost metrics repository .
in a personal communication barry boehm stated that he was able to publish less than cost estimation records even after years of cocomo effort.
to enable sharing we must assure confidentiality.
in our view confidentiality is the next grand challenge for cpdp in software engineering.
in previous work we allowed data owners to generate minimized and obfuscated versions of their original data.
our morph algorithm reflects on theboundary between an instance and its nearest instance of another class and morph s restricted mutation policy never pushes an instance across that boundary.
morph canbe usefully combined with the cliff data minimization algorithm .
cliff is an instance selector that returns a subset of instances that best predict for the target class.
previously we reported that this combination of cliff morph resulted in7 10defect data sets studied retaining high privacy scores while remaining useful for cpdp .
this is a startling result since research by grechanik et al.
and brickell et al.
showed that standard privacy methods increase privacy while decreasing data mining efficacy.
while useful cliff morph only considered a singleparty scenario where each data owner privatized their data individually without considering privatized data from others.
this resulted in privatized data that were directly proportional in size number of instances to the original data.
therefore in a case where the size of the original data is small enough any minimization might be meaningless but if the size of the original data is large minimization may not be enough to matter in practice.
in this paper we mitigate this issue with leaf formultiparty data sharing.
leaf is based on the leader follower algorithm for clustering data explained in iii d2 .
it allows a multi party scenario where data owners can incrementally add interesting data to a private cache passed among them based on the content already in the private cache.
this means that the size of the privatized data is no longer dependent on the size of the original data.
instead it will depend on the dis similarity of the data among different data owners i.e.
the more similar the data the less each data owner will contribute to the private cache.
we implement multi party data sharing as an extension of cliff morph and introduce the framework called lace which is a l arge scale a ssurance of confidentiality e nvironment that allow both the single party and multi party methods to be used by data owners.
this paper proposes and evaluates lace2 a multi party privacy policy based on the following scenario.
consider the problem of lparties data owners p1...pl each with local data xi.
they want to work together securely to create a private cache containing pooled minimized and obfuscated data from all parties involved.
each data owner pidetermines what data to add to the private cache based on what others have added previously.
the final private cache can then be published in a public data repository such as promise .
note in the rest of this paper when referring to cliff morph which uses a single party privacy policy wedenote this as lace1.
also when the term lace is used we are referring to both lace1 and lace2.
the specific contributions of this paper and lace2 are private data remains with data owner inside firewalls.
all the algorithms are run behind firewalls by data owners.
hence in lace there is no need for a central server or some third party privatization service.
most data are never shared.
lace prunes away most of the data while retaining interesting data points.
the shared data are obfuscated such that queries to that data return different values than in the raw data.
obfuscation of the data does not change the classifications of the training data.
that is lace privatizes data without damaging data mining efficacy.
lace2 has several benefits over lace1 and multi party sharing first because of leaf lace2 releases less overall data as low as .
second lace2 overcomes known issues with multi party sharing high network traffic and high computational costs as it only requires the private cache to visit a data owner once.
we privatized seven proprietary data sets with over instances using lace2 and used the result to predict defects for10open source data sets.
with the wide range of parameter values involved with our privacy algorithms we run the experiment r times and report the median results.
the experiments and results address three research questions rq1 does lace2 offer more privacy than lace1?
our definition of more privacy is shown in iii e. rq2 does lace2 offer more useful defect predictors than lace1?
to measure usefulness we compare the performance of defect predictors built with local data vs. single party privatized data lace1 vs. multi party privatized data lace2 .
results are shown in v b. rq3 are the systems costs of lace2 runtime and memory worse than lace1?
lace2 does more work than lace1 specifically it uses an instance based nearest neighbor method to check the data should be added to the private cache .
it is therefore wise to check if lace2 runs too slowly and outputs to many instances to be practical v c .
ii.
b ackground lace1 and lace2 mitigate for sensitive attribute disclosure and has been tested on cross project defect prediction .
the intuition behind lace2 is based on software code reuse.
according to a study done by selby in a set of programs were comprised of reused code not including libraries .
we conjecture that data will be similar over multiple projects allowing lace2 to reduce the amount of data each data owner contributes by adding instances that are not similar to those in the private cache.
the lace2 innovation is that it supports secure multi party computation .
the goal of this novel method is to mitigate the disadvantage of lace1 where the number of instances each data owner contributes to the private cache is directly proportional to the number of instances in the original data set.
all italicized terms are defined in this section.a.
cross project defect prediction the usefulness of laced data is measured via its utility for cross project defect prediction.
cpdp is useful because local data is not always available to many software companies for defect prediction .
according to zimmermann et al.
is due to the companies may be too small and the product being in its first release and so there is no past data.
kitchenham et al.
who studied cross versus within company cost estimation saw problems with relying on local data the time required to collect enough data on past projects from a single company may be prohibitive collecting local data may take so long that technologies used by the company would have changed and so older projects may no longer represent current practices.
with the use of better selection tools for training data researchers have found it possible to predict defects for software projects with insufficient data by using data from other projects .
however although the field of cpdp is useful and active its main component is data sharing which brings up privacy concerns.
b. privacy preserving data sharing to understand sensitive attribute disclosure we first offer the following definitions.
data consists of a set of classes which we refer to as targets t t1 t2 ... t t .
each target t t is a tuple of attribute values representing the individual target class.
each attribute falls into one or more of the following categories direct identifier the attribute explicitly identifies an individual or project e.g.
social security number or filename .
quasi identifier qid can be used to infer a target s identity alone or in combination with other attributes.
sensitive attribute s an attribute we do not want attackers adversaries to associate with a target t. dependent attribute used when evaluating the utility of data via classification.
in this work utility is measured via cpdp.
privacy is threatened by unwanted disclosure of directidentifiers quasi identifiers and sensitive attributes.
privacy threats are classified as identity disclosure or reidentification membership disclosure and sensitive attribute disclosure .
when protecting a personal data from privacy threats the goal is to prevent re identification .
re identification occurs when an attacker with external information such as a voters list can re identify an individual from data that has been stripped of personally identifiable information such as a social security number.
prominent examples of this are the re identification of william weld from released health care data and thelma arnold from the aol search data .
membership disclosure is another privacy threat that focuses on protecting a person s micro data.
it can happen if an attacker is able to confirm that the target s data is contained in a particular data set.
for example if the data set contains information only on hiv patients then the attacker can infer that the target is hiv positive .sensitive attribute disclosure occurs when a target is associated with information about their sensitive attributes such as software code complexity.
for example in the case of defect data one attribute that might want to be kept hidden are the lines of code loc associated with the shared data.
it is well documented that loc is highly correlated to development effort and development effort is something most organizations wish to keep private since it effects how many billable hours they can charge their clients .
in this paper we evaluate lace1 and lace2 against the third privacy threat sensitive attribute disclosure.
evaluation of lace1 and lace2 against re identification and membership disclosure is left to future work.
hence neither re identification nor membership disclosure are explored further in this work.
when a data owner releases a privatized version of their data an attacker tries to associate a specific target to a sensitive attribute value.
for instance table i b shows an equal frequency binned version of table i a .
equal frequency binning divides the range of possible values into nbins or sub ranges each of which holds the same number of attribute values.
if duplicate values are placed in different bins boundaries of every pair of neighboring bins are adjusted so that duplicate values belong to one bin only .
the result is table i b .
table i c is a minimized reduced to three instances and obfuscated in instance wmc changed to wmc version of table i b .
column headers are the c k object oriented metrics used in the data sets studied in this paper.
for an explanation of those metrics see table ii.
we assume that the sensitive attribute is loc the dependent attribute is bug and all other attributes are quasi identifiers except for the first column which we consider to be a direct identifier.
given table i b if an attacker knows that the wmc value of their target is in the range then the attacker will know with certainty that the sensitive attribute value for loc is in the range .
with lace we seek to reduce the attackers certainty with data minimization and obfuscation of quasi identifiers in order to disassociate quasi identifier values from sensitive attribute values.
therefore if the attacker instead is given table i c which shows a minimized and obfuscated version of table i b then the attacker with the same knowledge of wmc will only be certain about the loc range of values associated with the target.
c. secure multi party computation lace2 is based on the success and failures of prior work on multi party computation.
as explained by vaidya et al.
the goal of perfectly secure multi party computation is that nothing is revealed.
they offer a simple example of such a computation.
suppose we want the average age of everyone attending the icse conference.
first we generate a large random number rand pass it to a random attendee.
the attendee adds their age and passes the sum to another attendee selected at random .
this repeats till all attendees have been sampled at which point the sum returns to the origin.
after subtracting r we have the sum of the ages from which we can find the mean.table i an example of preserving privacy of defect data via minimization and obfuscation .
a partial ant .
defect data wmc dit noc cbo rfc lcom ca ce loc bug b ant .
after equal frequency binning wmc dit noc cbo rfc lcom ca ce loc bug c ant .
after minimization and obfuscation wmc dit noc cbo rfc lcom ca ce loc bug the benefits of this protocol are that if ris kept private then no single participant can decode the passed value to find the sum of the ages.
also if the ordering of the sampled attendees is also kept private and randomized then no pair of attendees a ccan compare their numbers to determine the age of the attendee bwho was sampled between aandc.
vaidya et al.
discussed an experiment with a distributed data miner based on c4.
that used a variant of the above multiparty computation whenever it searched data from different organizations.
they declared that experiment a failure for two reasons.
first the network overhead of that approach was prohibitive.
second this approach conducted so many queries across different sites that it was possible for pairs of sites to collude to decode the passed values.
our analysis of the vaidya et al.
experiment suggests that multiple micro queries of a distributed data source lead to poor privacy and performance.
however a single pass random sampling approach mitigates against collusion and reduces the network traffic associated with the query.
lace2 is such a single pass randomized query whose outcome is a private cache containing exemplars from each site.
iii.
lace d esign and operation a. assumptions for lace2 when implementing lace2 we make the following assumptions.
i since data is pooled into a private cache for defect prediction each data owner must provide data with the same features or attributes.
ii data involved in lace2 are not extreme .
for example consider a case where microsoft windows and several small startups contribute to a private cache.
even with the random perturbation in morph described in iii d3 it will be obvious which defect data camefrom windows vs. all of the others because windows will have attributes orders of magnitude greater than anyone else.
b. top level loop of lace fig.
gives an overview of how lace is executed at each data owner s site.
each data owner takes part in the process once.
the shaded box where leaf is applied with cliffed data and the current private cache highlights the difference between lace1 and lace2.
lace1 excludes the use of leaf iii d2 and only adds cliffed morphed data to the private cache.
lace2 uses leaf so that data owners can use the current content in the private cache to determine what data to add to the private cache.
the high level steps involving multiple data owners are explained as follows with the process of fig.
reflected from steps the initiator data owner is chosen at random.
data owner applies cliff to identify the subset of data that best represents the target classes.
only the data selected by cliff are used in lace.
if the data owners decided to use lace1 then go to the next step otherwise with lace2 leaf is applied to further prune the cliffed data and facilitate collaboration among data owners.
the results are then obfuscated with morph iii d3 and privacy is measured iii e .
steps are repeated until a user defined privacy criterion is met.
once this is achieved the morphed data is added to the private cache.
the private cache is sent to the next randomly chosen site data owner where they execute step .
as in step if lace1 is used then move on to step otherwise with lace2 test each instance of their cliffed data using leaf with the private cache.
the test involves each instance finding its nearest exemplar in the private cache.
if the instance and exemplar are a certain distance away then the new instance is morphed and added to the cache.
the private cache moves on to the next random data owner and steps are repeated.
the protocol is complete when all data owners involved have had a chance to contribute to the private cache.
this final private cache can be added to a public data repository.
the rest of this section offers further details on lace guided by the main components in fig.
.
c. inputs for lace lace accepts three inputs provided by the data owner data privacy criteria based on the privacy threat of sensitive attribute disclosure and the private cache which can be empty or contain privatized data from other data owners.
d. privacy algorithms in lace lace uses the cliff algorithm to remove uninformative instances and the morph algorithm to obfuscate the remaining instances.
on top of that the lace2 innovation is to apply the leaf leader follower algorithm to add more intelligence to what instances are selected for the private cache.
fig.
.
overview of lace for a single data owner.
cliff for data reduction cliff assumes that tables of training data can be divided into classes .
for example for a table of defect data containing code metrics different rows might be labeled accordingly defective or not defective .
cliff executes as follows for each column of data find the power of each attribute sub range i.e.
how frequently that sub range appears in one class more than any other.
we then find the product of the powers for each row then remove the less powerful rows of each class keeping of the most powerful rows.
we use based on the results from previous work where selecting of the top ranked rows provided a relatively better balance between privacy and utility.
the result is a reduced data set with fewer rows.
finding the power of each attribute sub range is based on the bore best or rest algorithm.
to apply bore first we assume that the target class is divided into one class as first and the other classes as rest.
this makes it easy to find the attribute values that have a high probability of belonging to the current first class using bayes theorem.
the theorem uses evidence eand a prior probability p h for hypothesis h first rest to calculate a likelihood hereafter like of the evidence selecting for one class like h e p e h p h .
this calculation is then normalized to create probabilities p first e like first e like first e like rest e jalali et al.
found that equation was a poor ranking heuristic for low frequency evidence.
to alleviate this problem the support measure was introduced.
note that like first e is also a measure of support since it is maximal when a value occurs all the time in every example of one class.
hence adding the support term is just equation p first e support first e like first e like first e like rest e leaf for data selection leaf is based on the leaderfollower algorithm .
it is an online incremental technique for clustering data.
the cluster centers are the leaders and all other instances are the followers .
for this work we areonly interested in the leaders.
the basic algorithm works as follows first initialize cluster centers then for each instance in the data find its nearest center.
if the distance to the center is less than a user defined distance then update the cluster.
otherwise create a new cluster with the instance as the center.
to define distance we use the standard euclidean measure recommended for instance based reasoning by aha et al.
dist x y radicalbigg summationdisplay i xi yi wherexiandyiare normalized values between and .
leaf is applied to each instance selected by cliff from the data owner s data set to determine if it should be included in the private cache.
for our work we adapt leaf as follows.
first the cluster centers are never updated to create centroids this saves some time in the algorithm .
second instead of a user defined distance we randomly select instances from the initiator and find the distances from their nearest neighbor with a different class label.
we use the median of these distances d to determine if data from a data owner should be included in the private cache new data is added to the cache if it falls outside of d .
third prior to a new instance being added to the cache it is first morphed using the method described in the next section.
morph for data obfuscation morph s role in the lace process is to obfuscate the output from either cliff if lace1 is used or leaf if lace2 is used prior to the output s addition to the private cache.
morph is an instance mutator used as a privacy algorithm .
it changes the numeric attribute values of each row by replacing these original values with morphed values.
morphed instances are created by applying equation to each attribute value of the instance.
morph will not change an instance such that it moves across the boundary between the original instance and instances of another class.
this boundary is determined by rin equation .
a small rvalue means the boundary is closer to the original row while a large rvalue means the boundary is farther away from the original row.
yi xi xi zi r letx data be the original instance to be changed ythe resulting morphed instance and z data the nearest unlike neighbor of x i.e.
whose class label is different from x s class label.
distance is calculated using the euclidean distance.
previously in our work on cliff morph the random number rwas calculated with the property r where .15and .
.
we use this range of values based on results of previous work which produced privatized data candidates with high privacy and accurate defect prediction.
e. measuring privacy to measure privacy we use the increased privacy ratio ipr used in our previous work .
informally it can bedefined as follows.
suppose the same query is posed to a database before andafter some algorithm has tried to privatize that data.
the privacy ratio is the percent of data found before that was also found afterwards if that ratio is then this would be an example of a very poor privacy algorithm.
if on the other hand none of the data found before was found in after then this would be an example of a very good privacy algorithm.
we report the ipr as the percent of data not found therefore a poor privacy algorithm will have iprs closer to while a good privacy algorithm will have iprs closer to .
it should be noted that in cpdp if the goal of the attacker is to associate a target to the number of defects then no privacy algorithm can defend against this except to generalize the values of defects for each target as done in this work.
here any number of defects are replaced with the value one.
to formally define ipr we assume that attackers have access to privatized data in this case exemplars prior to joining the private cache denoted as t of an original data set t and some background knowledge of non sensitive quasi identifier values for a specific target in t. we refer to the background knowledge as a query.
to generate queries we use a query generator to generate queries based on what the attacker may know about a target in the original data set.
to maintain some realism to the attacks a selected sensitive attribute s and the class attribute are not used as part of query generation the attacker is trying to discover this information but does not know it beforehand.
here we are assuming that the only information an attacker could have is information about the non sensitive qids in the data set.
to illustrate a query generator we use an example defect data set shown in table i a and table i b .
next to create a query we proceed as follows.
our inputs are a set of attributes and a query size measured as the number of attribute subrange pairs.
for this study we use a query size of 1since previous work showed that even with an increase in query size iprs were comparable.
from those inputs we randomly choose an instance from the data.
for this example we use row 1in table i b then randomly select an attribute from a e.g.
wmc .
in the end the query we generate is wmc .
we continue this process until we have used all instances.
in previous work we also used unique queries as a stopping criterion because of query sizes 2and4.
we stopped at because it would not be practical to generate and test every possible query of size 2and4.
however with a query size of1this stopping criterion is unnecessary because with equal frequency binning set at bins each attribute in the data sets used in this work will have at most sub ranges and with the number of quasi identifiers at the most number of queries generated are .
each query must also satisfy the following sanity checks they must not be the same as another query.
they must return at least one instance from the original data set.
they must not include attribute value pairs from either the designated sensitive attribute or the class attribute.
when all the queries are generated the next steps are as follows for each query q q q1 ... q q g iis a group of rows from any data set which matches qi.giis the group from the original data set and g iis the group from the private data candidate which matches qi.
next for every sensitive attribute sub range in the set s s1 ... s s the most common sensitive attribute value is smax g i .
now we define a breach of privacy as follows breach s g i braceleftbigg if s max gi smax g i otherwise.
therefore the privacy level of the exemplars is p1 ipr t q q summationdisplay i 0breach s g i .
ipr t has some similarity to aacc of brickell and shamtikov where ipr t measures the adversary s ability to cause breaches after observing the exemplars t compared to a baseline of the original data set t. to be more precise ipr t measures the percent of total queries that did not cause a breach .
f .
upper and lower bounds on ipr when used in conjunction with instance selection algorithms like leaf and cliff equation is a lower bound on privacy.
recall that from with nprojects cliff and leaf discards xrows equation is applied to the remaining n xprojects.
since data from the xdiscarded projects is never shared it is fully private.
therefore an upper bound on privacy is p2 x n n x n p1 wherep1comes from equation .
for example cliff and leaf typically discard of the data and on the remaining data we achieve an ipr of .
the resulting increased privacy is hence .
.
.
.
note that p2is an upper bound on privacy since it is possible that the patterns in the discarded data might repeat in the cached data.
that said given a large enough community sharing their data there would always be some doubts about which members of the community had the exact values found in particular query.
in table v we take care to report the lower and upper bound p1 p2 on all our privacy results.
g. output once data have been moprhed and meet the data owner s criterion of high ipr the data are added to the private cache and either sent to another data owner or made public for cpdp.
when applying the data owner s ipr criterion we use the conservative bound of equation rather than the more optimistic equation .iv.
e xperimental setup a. experimental design these experiments are designed to address the three research questions from the introduction i .
first to determine if lace2 offers more privacy than lace1 rq1 we calculate the iprs for the privatized data produced by each method explained in iii e prior to being added to the private cache.
in practice the data owner may choose to lower or raise their privacy criterion.
this means that no matter how many data owners are involved in lace2 the ipr will always be adequate for the data owner.
we define adequate to be the equivalent of a data owners privacy criterion.
in our experiments we use an arbitrary privacy criterion of therefore adequate .
results are shown in table v. second to determine if lace2 offers better defect predictors than lace1 rq2 we baseline our work with a cross validation experiment on local data.
cross validation is a standard evaluation approach in machine learning where an experiment is repeated ntimes on mrandom subsamples of data.
in other words n times m all miis treated as the training set and m all training set is the test set.
we use a way crossvalidation where n is 1and m is 10and report on the median performance section iv d shows how this is measured .
last to determine if lace2 consumes more processing and storage resources than lace1 rq3 we measure the time seconds it takes for each to produce a private cache and also measure the size of the cache.
results are shown in table viii.
b. data the evaluation was conducted using of the jureczko static code defect data sets .
table ii describes the attributes of these projects and table iii lists the names of the data sets.
each instance in these data sets represents a source code class and consists of two parts independent static code attributes and the dependent attribute labeled defects indicating the number of defects in the class.
for our work we refer to each class as an instance.
additionally instances with no defects are labeled as and instances with one or more defects are labeled as .
table iii also indicates that the first data sets are from open source projects while the remaining seven are from proprietary projects.
c. data mining algorithms to assess the performance implications of applying our privacy algorithms we used a k nearest neighbor k nn algorithm.
cover and hart describes k nn as a simple nonparametric decision procedure which classifies an unknown instance in the category of its nearest neighbor.
k nn is one of the simplest defect predictors that can be used.
it can therefore be used as a baseline for more complicated methods.
a k nn algorithm generates an estimate for a test instance by finding the mean of the knearest neighbors in the training data.
to define distance in this context we use equation .table ii thec k metrics of the data sets used in this work table iii .
t he last row is the dependent variable .
jureczko et al .
provide more information on these metrics .
attributes description amc average method complexity average method size as measured by the number of java binary codes avg cc average mccabe average mccabe s cyclomatic complexity seen in class ca afferent coupling the number of classes the access the members of the specified class cam cohesion amongst classes summation of number of different types of method parameters in every method divided by a multiplication of number of different method parameter types in whole class and number of methods cbm coupling between methods total number of new redefined methods to which all the inherited methods are coupled cbo coupling between objects increased when the methods of one class access services of another ce efferent couplings the number of classes whose members are access by the specified class dam data access metric ratio of the number of private protected class attributes to the total number of class attributes dit depth of inheritance tree the level on which the class is positioned in the inheritance tree dit root ic inheritance coupling number of parent classes to which a given class is coupled includes counts of methods and variables inherited lcom lack of cohesion in methods number of pairs of methods that do not share a reference to an instance variable locm3 another lack of cohesion measure ifm a are the number of methods attributes in a class number and a is the number of methods accessing an attribute then lcom3 a summationtexta j aj m m loc lines of code number of lines of binary code max cc maximum mccabe maximum mccabe s cyclomatic complexity for class mfa measure of function abstraction number of methods inherited by a class plus number of methods accessible by member methods of the class moa measure of aggregation count of the number of data declarations class fields whose types are user defined classes noc number of children measures the number of immediate descendants of the class.
npm number of public methods counts all the methods in a class that are declared as public.
the metric is known also as class interface size cis rfc response for a class sum of the number of methods and the number of methods invoked within a class s method bodies wmc weighted methods per class the number of methods in the class assuming unity weights for all methods .
defects number of defects per class seen in post release bug tracking systems.
converted to the boolean false if no defects otherwise true.
table iii objective data sets for open source and proprietary project data.
defect data type instances defects defects ant .
open source .
camel .
open source .
ivy .
open source .
jedit .
open source .
lucene .
open source .
poi .
open source .
synapse .
open source .
velocity .
open source .
xalan .
open source .
xerces .
open source .
prop1 ver192 proprietary .
prop2 ver276 proprietary .
prop3 ver318 proprietary .
prop4 ver362 proprietary .
prop5 ver185 proprietary .
prop42 ver454 proprietary .
prop43 ver512 proprietary .
note that in our initial experiments we used naive bayes neural networks and support vector machines but found that these learners generated unacceptably high false alarm rates median values of or higher .
hence in this work in addition to using k nn as a classifier it is also used for relevancy filtering .
inrelevancy filtering only the training data nearest to the test data is used to learn predictive models.
the filter applies k nn to each member of lace2 s cache to build such a nearest neighbor training set.
however instead of using k we tune kusing the best k procedure used by kocaguneli et al.
to determine the best for each test set.
for this study we used k nn for our relevancy filtering.
the results of relevancy filtering are then passed to noise filtering to remove outliers.
for this study we used cliff to for noise filtering.
note that we also experimented with usingonly one of relevancy or noise filtering but those results had unacceptably high false alarm rates.
d. performance evaluation we assess our privacy algorithms using the ipr privacy measure described above and the g measure that summaries the performance measures of table iv.
tp tn fp and fn are true positive true negative false positive and false negative respectively.
probability of detection or pdis equal to how much of the target defective instances are found.
the higher the pd the fewer the false negative results.
the probability of false alarm or pfmeasures how many of the instances that triggered the detector actually did not contain the target defects concept.
like pd the highest pf is however its optimal result is .
the g measure is harmonic mean of pd and pf .
the pf represents value is known as specificity not predicting instances without defects as defective.
specificity is used together with pdto form the g mean 2measure seen in jiang et al.
.
measures such as accuracy precision and f measure are not shown in our experimental results since they are poor indicators of performance for data where the target class is rare in our case the defective instances .
this is based on a study done by menzies et al.
which shows that when data sets contain a low percentage of defects precision can be unstable.
if we look at the data sets in table iii we see that defects are rare in most cases.
v. e xperimental results we organize our results around the three research questions in the introduction i .
a. rq1 does lace2 offer more privacy than lace1?
table v displays the median lower and upper bound results of iprs of the data submitted to the private cache by each datatable iv measures used in software defect prediction .
actual yes no predictedyes tp fp no fn tn pdtp tp fn pffp fp tn g measure2 pd pf pd pf table v median ipr s for proprietary projects after 10runs .
calculated using equation 5and equation .
lace1 lace2 data lower upper lower upper prop42 ver454 .
.
.
.
prop43 ver512 .
.
.
.
prop3 ver318 .
.
.
.
prop2 ver276 .
.
.
.
prop4 ver362 .
.
.
.
prop5 ver185 .
.
.
.
prop1 ver192 .
.
.
.
owner.
the medians are calculated after experimental runs for lace1 and lace2.
looking at the minimum lower bound values we see that for lace1 none of these minimum iprs are greater than or equal to the minimum iprs of lace2.
we find that with lace1 results range from .
to .
while lace2 minimum iprs range from to .
.
looking at the maximum upper bound values we see lace2 topping in5 7of these runs.
hence we say overall lace2 provides more privacy than lace1.
recall that we set an ipr as adequate if it was so that if any data owner has an ipr less the adequate they can run lace again.
however if the adequate measure is not reached we hypothesize that for these cases the data lacks diversity and so any subset of the data are similar to all the data.
for these rare occurrences data owners can choose not to add their exemplars to the private cache.
in future work we will test the utility of not adding these exemplars.
b. rq2 does lace2 offer better defect predictors than lace1?
previous work with lace1 did not consider relevance filtering nor noise reduction of its privatized data.
in this work with lace2 we added these elements for improved defect predictors.
table vi shows the median pd pf and g measure values seen in three treatments local running a way cross validation iv a on each open source data set table iii lace1 training on the private cache resulting from lace1 lace2 training on the private cache resulting from lace2.table vi k nn r esults shown are the pds pfs and g measures .
data k nn local lace1 lace2 ant .
pd .
.
.
pf .
.
.
g measure .
.
.
camel .
pd .
.
.
pf .
.
.
g measure .
.
.
ivy .
pd .
.
.
pf .
.
.
g measure .
.
.
jedit .
pd .
.
.
pf .
.
.
g measure .
.
.
lucene .
pd .
.
.
pf .
.
.
g measure .
.
.
poi .
pd .
.
.
pf .
.
.
g measure .
.
.
synapse .
pd .
.
.
pf .
.
.
g measure .
.
.
velocity .
.
pd .
.
.
pf .
.
.
g measure .
.
.
xalan .
pd .
.
.
pf .
.
.
g measure .
.
.
xerces .
pd .
.
.
pf .
.
.
g measure .
.
.
table vii mann whitney results confidence for local lace1 and lace2 for the 10data sets for pd pf and g measure .
zeros mean no significant difference minuses mean that either lace1 orlace2 are significantly worse than local or lace1.
mann whitney lace1 local lace2 local lace2 lace1 pd pf g measure note that the data set names in table vi are different from table v. to mimic true cross project learning in these experiments the proprietary data sets of table v are used to build the cache of shared data and the resulting prediction model is evaluated against the open source data sets of table vi.
table vi comments on the benefits of sharing .
note that lace s intelligent selection of training data can lead to much higher pds.
overall in6 10data sets the median pd seen after learning from lace2 was relatively higher that learning from the local data and lace1 data.
more generally consider the fivelocal pd results that are less than for ant .
camel1.
ivy .
jedit .
xerces .
.
lace2 boosts allof these results by for xerces to ivy .
.
as to pfs increasing the probability of detection usually means some more false alarms.
hence lace2 s pfs are higher than those using the local data or lace1.
that said the pfs shown here for lace2 are not abnormally large compared to prior results median pf median here median pf in a ieee tse paper .
also some of those large pfs are associated with substantial pd improvements.
for example ivy .
s pd pf for local and lace2 are and respectively which is a marked improvement .while individual results differ there is no overall loss of predictive efficacy due to lace2.
table vii checks for significant differences between these prediction results.
in the column headers the arrows indicate the direction to interpret the results.
for example for the pf values for lace1 local lace1 has significantly worse pfs than local.
when we compare lace2 with local result we find that for pd and g measure there are no significant difference in the results.
however we find that lace2 pfs are significantly worse than local.
the same can be said when lace2 is compared with lace1 and lace1 is compared with local.
the interesting feature of these results is that the lace2 results are no worse than lace1.
this is surprising since in lace1 data owners contribute approximately three times more data than those data owners who apply lace2 as seen in the next section .
thus we say overall there is no loss of predictive efficacy due to the multi party computation of lace2.
c. rq3 are the systems costs of lace2 runtime and memory worse than lace1?
table viii shows the number of instances and the percentage of instances that are added to the private cache by each proprietary project for lace1 and lace2.
the last row shows the median runtimes in seconds that it takes to build the final private cache.
the data sets in the first column are sorted from the least number of instances to the most number of instances.
we recognize that if the actual values in column two are small enough any reduction might be essentially meaningless but if the numbers are large a reduction might not be enough to matter in practice.
from our results we find that this is the case with lace1 whose reduction is solely the responsibility of cliff iii d1 which selects the top of the most powerful instances in a data set.
while with lace2 in addition to cliff reduces the number of instances shared by using leaf iii d2 and leaf selection is based on instances being dissimilar to those in the private cache rather than a fixed percentage.
therefore in the case of lace1 as the data sets get larger the reduction will eventually not be enough to matter.
lace2 avoids this reality when data shared by different data owners are similar for example in table viii prop43 ver512 and prop5 ver185 contains and instances respectfully.
this is a difference of instances however the selected instances for lace1 is a difference of while for lace2 it s .
results in table viii also show that lace2 takes similar time to lace1 to create the final private cache.
further the memory requirements for that cache are reduced from .
of the data with lace1 to .
of the data with lace2 .
hence we conclude lace2 s multi party computation does not take more resources than lace1.
this is an important result since as discussed in ii c prior results reported a significant systems overhead associated with multi party computation.d.
threats to validity as with any empirical study biases can affect the final results.
therefore any conclusions made from this work must be considered with the following issues in mind .
sampling bias threatens any classification experiment i.e.
what matters there may not be true here.
for example the data sets used here comes from the promise repository and were supplied by one individual.
also even though we use ten open source data sets for cpdp table iii and seven to run lace table iii and the data covers a large scope of applications including text xml processing systems search engines source code integration build tools and management information systems they are all from java systems.
.
learner bias for building the defect predictors in this study we elected to use k nearest neighbor.
we chose the knearest neighbor because its results were comparable to the more complicated algorithms and can act as a baseline for other algorithms.
classification is a large and active field and any single study can only use a small subset of the known classification algorithms.
.
evaluation bias this paper uses one measure of privacy ipr.
other privacy measures used in software engineering include guessing anonymity and entropy discussed in iv d .
measuring privacy with other measures is left for future work.
.
order bias with lace2 the order that the data owners get access to the private cache affects the amount of data that they submit to the cache.
to mitigate this order bias we run the experiment times randomly changing the order of the data owners each time.
.
input bias for the morph algorithm we randomly select input values for a set range to determine the boundary between the an instance and its nearest unlike neighbor within which we create morphed instances.
since different input values can result in different outputs we mitigate this bias with runs of the experiment for lace1 and lace2.
e. relation to other work lace2 is designed based on the privacy needs of cpdp.
other researchers in se focus on privacy in software testing and debugging this becomes an issue when it involves collecting user information after a software system has been deployed or outsourcing the software testing to third parties e.g.
see budi et al.
taneja et al.
and li et al .
in this case companies do not wish to release actual cases for testing.
hence they anonymize the test cases before releasing them to testers.
work published by castro et al.
in sought to provide a solution to the problem of software vendors who need to include sensitive user information in error reports to reproduce a bug.
to protect sensitive user information the authors used symbolic execution along the path followed by a failed execution to compute path conditions.
their goal was to compute new input values unrelated to the original input.
the new input values satisfied path conditions required to make the software follow the same execution path until it failed.table viii number of instances added to the private cache by each proprie tary project .
the last row shows the median runtimes in seconds that it takes to build the final private cache .
proprietary data instances lace1 lace1 lace2 lace2 prop42 ver454 prop43 ver512 prop3 ver318 prop2 ver276 prop4 ver362 prop5 ver185 prop1 ver192 .
.
median runtime seconds seconds as a follow up to the castro et al.
paper clause et al.
presented an algorithm which anonymized input sent from users to developers for debugging.
like castro et al.
the aim of clause et al.
was to supply the developer with anonymized input which causes the same failure as the original input.
to accomplish this they first use a novel path condition relaxation technique to relax the constraints in path conditions thereby increasing the number of solutions for computed conditions.
in contrast to the work done castro and clause taneja et al.
proposed priest a privacy framework.
unlike our work which privatizes data randomly within nearest unlike neighbor border constraints the privacy algorithm in priest is based on data swapping where each value in a data set is replaced by another distinct value of the same attribute.
this is done according to some probability that the original value will remain unchanged.
work by taneja et al.
followed work done by budi et al.
.
similarly their work focused on providing privatized data for testing and debugging.
they were able to accomplish this with a novel privacy algorithm called kb anonymity.
this algorithm combined k anonymity with the concept of program behavior preservation which guide the generation of new test cases based on known ones and make sure the new test cases satisfy certain properties .
the difference with the followup work by taneja et al is that while budi et al.
replaces the original data with new data in taneja s work the data swapping algorithm maintains the original data and offers individual privacy by swapping values.
software test outsourcing work by li et at.
follows a similar approach to our work in privacy for cpdp lace1 and now lace2 with leaf don t use all the data minimize and obfuscate data that are used.
li et al.
accomplish this through the process of securing centroids using a novel combination of data mining approaches program analysis and privacy constraints.
f .
future work in the study of data privacy modeling the adversary s background knowledge is important to determine how private a data set is.
in this paper we only focused on background knowledge specific to the original data sets.
other types of background knowledge need to be considered.the above results need to be explored on a wider range of data sets.
for example it would be interesting to check if the above results hold for more than just defect prediction.
the runtimes reported above were generated from a single core machine simulating data being passed around a community of data owners.
it is possible that a much faster parallel computation could be achieved if a when sending a cache it gets dispatched to n 1other data owners and b when receiving n 1caches there is some work on combining data from different caches.
finally lace2 considers all attributes somewhat equal with respect to the semantic meaning of their data.
future work would consider that some attributes not identifiers have higher impact and should be treated differently.
vi.
c onclusions studies have shown that early detection and fixing of defects in software projects is less expensive than finding defects later on .
organizations with local data can take full advantage of this early detection benefit by local defect prediction.
when an organization does not have enough local data to build defect predictors they might try to access relevant data from other organizations in order to perform cross defect prediction.
that access will be denied unless the privacy concerns of the data owners can be addressed.
this paper has presented lace2 a novel private multiparty sharing protocol for cpdp.
lace2 is an extension of our prior system lace1 and offers and additional method for data sharing with significant improvement over our lace1.
lace2 is a multi party computation that works incrementally on sub samples of the data.
the experiments of this paper show that this approach generates higher privacy than lace1 without damaging predictive efficacy.
better yet measured in terms of runtimes and how much data must be based around the network lace2 is not more expensive than lace1.
we hope that this result encourages more data sharing more cross project experiments and more work on building software engineering models that are general to large scale systems.
acknowledgment this work was partially funded by a national science foundation cise medium grant science foundation ireland grant ce i1855 and by the european research council advanced grant asap .