a novel neural source code representation ased on abstract syntax tree jian zhang x uw a n g hongyu zhang hailong sun kaixuan wang and xudong liu sklsde lab school of computer science and engineering beihang university beijing china beijing advanced innovation center for big data and brain computing beijing china the university of newcastle australia zhangj wangxu act.buaa.edu.cn hongyu.zhang newcastle.edu.au sunhl wangkx liuxd act.buaa.edu.cn abstract exploiting machine learning techniques for analyzing programs has attracted much attention.
one key problem is how to represent code fragments well for follow up analysis.traditional information retrieval based methods often treatprograms as natural language texts which could miss importantsemantic information of source code.
recently state of the artstudies demonstrate that abstract syntax tree ast based neuralmodels can better represent source code.
however the sizes ofasts are usually large and the existing models are prone tothe long term dependency problem.
in this paper we proposea novel ast based neural network astnn for source coderepresentation.
unlike existing models that work on entire asts astnn splits each large ast into a sequence of small statementtrees and encodes the statement trees to vectors by capturingthe lexical and syntactical knowledge of statements.
based on thesequence of statement vectors a bidirectional rnn model is usedto leverage the naturalness of statements and finally produce thevector representation of a code fragment.
we have applied ourneural network based source code representation method to twocommon program comprehension tasks source code classificationand code clone detection.
experimental results on the two tasksindicate that our model is superior to state of the art approaches.
keywords abstract syntax tree source code representation neural network code classification code clone detection i. i ntroduction many software engineering methods such as source code classification code clone detection defect prediction and code summarization have been proposed to improve software development andmaintenance.
one main challenge that is common across allthese methods is how to represent source code in orderto effectively capture syntactical and semantic informationembedded in the source code.
traditional approaches such as information retrieval ir usually treat code fragments as natural language texts andmodel them based on tokens.
for example programs arerepresented by token sequences or bag of tokens for code clonedetection bug localization and code authorshipclassification .
in addition a number of researchers uselatent semantic indexing lsi and latent dirichletallocation lda to analyze source code .however according to the common problem of theseapproaches is that they assume the underlying corpus i.e.
corresponding author xu wang wangxu act.buaa.edu.cn.the source code is composed of natural language texts.
eventhough code fragments have something in common with plaintexts they should not be simply dealt with text based or token based methods due to their richer and more explicit structuralinformation .
recent work provides the strong evidence that syntactic knowledge contributes more in modeling source codeand can obtain better representation than traditional token based methods.
these approaches combine abstract syntaxtree ast and recursive neural network rvnn tree based cnn or tree lstm to capture both the lexical i.e.
the leaf nodes of asts such as identifiers and syntactical i.e.
the non leaf nodes of asts like the grammar constructwhilestatement information.
such ast based neural models are effective yet they have two major limitations.
first similarto long texts in nlp these tree based neural models arealso vulnerable to the gradient vanishing problem that thegradient becomes vanishingly small during training especiallywhen the tree is very large and deep .
forexample as our experiments show section v the maximalnode number depth of asts of common code fragments inc and java are and respectively.
asa result traversing and encoding entire asts in a bottom up way or using the sliding window technique may lose long term context information second these approaches either transform asts to or directly viewasts as full binary trees for simplification and efficiency which destroys the original syntactic structure of source codeand even make asts much deeper.
the transformed anddeeper asts further weaken the capability of neural modelsto capture more real and complex semantics .
in order to overcome the limitations of the above astbased neural networks one solution is to introduce explicit long term control flow and data dependencies graphs andemploy a graph embedding technique to represent sourcecode.
for instance one recent study considers the long range dependencies induced by the same variable or functionin distant locations .
another study directly constructscontrol flow graphs cfgs of code fragments .
however as depicted in the above work precise and inter proceduralprogram dependency graphs pdgs i.e.
control flow and dataflow dependencies usually rely on compiled intermediaterepresentations or bytecodes and are not applicableb ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
static public string readtext final string path throws ioexception final inputstream stream filelocator.getasstream path final stringbuilder sb new stringbuilder try bufferedreader reader new bufferedreader new inputstreamreader stream string line while line reader.readline !
null sb.append line .append n return sb.tostring a code fragment and statementsmethod declarationmodifier ... bodypublic static localvariable throws localvariable trystatement returnstatementtype ... type ... tryresource block localvariable whilestatement... ...inputstream stringbuilderreadtext b ast and statement treesmethoddeclaration localvariable localvariable trystatement ... localvariable ... ... ... ... returnstatement ... whilestatement ... ... c statement naturalness fig.
.
an example of ast statement nodes marked in red to uncompilable and incomplete code fragments.
such a limitation hinders the applications of the code representations in many areas that involve arbitrary code fragments.
in this paper we propose a novel approach for representing code fragments that do not have to be compilable called ast based neural network astnn which splits the large ast of one code fragment into a set of small trees at the statement level and performs tree based neural embeddings on all statement trees.
it produces statement vectors which can represent the lexical and statement level syntactical knowledge.
here statements refer to the statement ast nodes defined in program language specification .
we also treat methoddeclaration as a special statement node.
as an example figure shows a code fragment from an open source project1.
the code snippet between line and line contains a whole try statement and the code snippet between line and line includes only the localvariable statement initializing variable sb.
for each statement like the try statement that includes the header and other statements in the body we split the header of the statement and all included statements.
in this way the large ast is decomposed to a short sequence of small statement trees.
we use recurrent neural network rnn to encode statements and the sequential dependency between the statements into a vector.
such a vector captures the naturalness of source code and can serve as a neural source code representation.
more specifically first we build an ast from the code fragment and split the whole ast to small statement trees one tree consisting of ast nodes of one statement and rooted at the statement node .
for example in figure the statement trees are denoted by dashed lines and the corresponding statements or statement headers in the original code fragment are also marked by dashed lines.
second we design a recursive encoder on multi way statement trees to capture the statement level lexical and syntactical information and then represent them in statement vectors.
third based on the sequence of statement 9c552c5c4f92af00d9d008b8c7f9e9d326a2450a ctakes core src main java org apache ctakes core resource filereadwriteutil.java l32vectors we use bidirectional gated recurrent unit gru one type of recurrent neural network to leverage the sequential naturalness of statements and finally obtain the vector representation of an entire code fragment.
in summary our proposed neural source code representation aims to learn more syntactical and semantic information about source code than the state of the art ast based neural models.
it is general purpose and can be used in many program comprehension related tasks such as source code classification and code clone detection.
we have conducted experiments on the two tasks on public benchmarks and compared with state of the art approaches.
the experimental results show that our model is more effective.
for example for source code classification our approach improves the accuracy from to .
.
for clone detection our approach improves the results of f1 values from to .
and .
to .
on two benchmark datasets respectively.
our main contributions are as follows we propose a novel neural source code representation which can capture the lexical statement level syntactical knowledge and the naturalness of statements we have applied our representation to two common program comprehension tasks code classification and clone detection .
the experimental results show that our approach can improve the state of the art methods.
the remainder of this paper is structured as follows.
section ii introduces the background.
section iii presents our approach.
section iv describes the applications of our neural source code representation.
section v provides our experimental results.
related work and discussion about threats to validity are presented in section vi and section vii respectively.
finally section viii concludes our work.
ii.
b ackground a. abstract syntax tree abstract syntax tree ast is a kind of tree aimed at representing the abstract syntactic structure of the source code .
it has been widely used by programming language and software engineering tools.
as illustrated in figure b nodes authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of an ast are corresponding to constructs or symbols of the source code.
on the one hand compared with plain source code asts are abstract and do not include all details such as the punctuation and delimiters.
on the other hand asts can be used to describe the lexical information and the syntactic structure of source code such as the method name readtext and the control flow structure whilestatement in figure b .
some studies directly use asts in token based methods for source code search program repair and source code differencing .
due to the limitation of token based approaches these methods can catch little syntactical information of source code.
b. tree based neural networks recently tree based neural networks tnns have been proposed to accept asts as the input.
given a tree tnns learn its vector representation by recursively computing node embeddings in a bottom up way.
the most representative tree based models for asts are recursive neural network rvnn tree based cnn tbcnn and tree based long short term memory tree lstm .
recursive neural network rvnn was first proposed for the recursive structure in natural language and image parsing .
specifically given a tree structure suppose that one parent node y1has two children nodes c1 c2 wherec1and c2are word embeddings or intermediate vector representations of nodes.
the vector of node y1is computed by p f w b wherew is a matrix of parameters fis an element wise activation function and b is a bias term.
to assess the quality of this vector representation a decoding layer is used to reconstruct the children c prime c prime w p b then the training loss is measured by e c1 c prime c2 c prime .
in this way rvnn can recursively compute and optimize parameters across the tree and the final vector of the root node will represent the given tree.
based on rvnn a recursive autoencoder rae is incorporated for automatically encoding asts to detect code clones where asts are transformed to full binary trees due to the fixed size inputs for simplification.
tree based convolutional neural network tbcnn performs convolution computation over tree structures for supervised learning such as source code classification .
its core module is an ast based convolutional layer which applies a set of fixed depth feature detectors by sliding over entire asts.
this procedure can be formulated by y tanh n summationdisplay i 1wconv i xi bconv wherex1 xnare the vectors of nodes within each sliding window wconv i are the parameter matrices and bconv is the bias.
tbcnn adopts a bottom up encoding layer to integratesome global information for improving its localness.
although nodes in the original ast may have more than two children tbcnn treats asts as continuous full binary trees because of the fixed size of convolution.
tree based long short term memory tree lstm is a generalization of lstms to model tree structured topologies.
different from standard lstm child sum tree lstm recursively combines current input with its children states for state updating across the tree structure.
cdlh uses tree lstm to learn representations of code fragments for clone detection where code fragments are parsed to asts.
to deal with the variable number of children nodes asts are transformed to full binary trees.
after a bottom up way of computation the root node vectors of asts are used to represent the code fragments.
c. the limitations of the existing work these three tree based methods have two major limitations.
first during gradient based training of tree topologies the gradients are calculated via backpropagation over structures .
however the structures of asts are usually large and deep due to the complexity of programs especially the nested structures.
thus the bottom up computations from the leaf nodes to the root nodes may experience the gradient vanishing problem and are difficult to capture long range dependencies which will miss some of the semantics carried by distant nodes from the root nodes such as identifiers in the leaf nodes.
second the existing tree based methods view asts as binary trees by moving three or more children nodes of a parent node to new subtrees for simplification which changes the original semantics of source code and makes the long term dependency problem more serious.
for example cdlh can only have the f1 value of in one public benchmark for clone detection and the studies in nlp show that the tree size and depth do matter and have significant impact on the performance.
iii.
o urapproach we introduce our ast based neural network astnn in this section.
the overall architecture of astnn is shown in figure .
first we parse a source code fragment into an ast and design a preorder traversal algorithm to split each ast to a sequence of statement trees st trees which are trees consisting of statement nodes as roots and corresponding ast nodes of the statements as illustrated in figure .
all st trees are encoded by the statement encoder to vectors denoted as e1 et.
we then use bidirectional gated recurrent unit bi gru to model the naturalness of the statements.
the hidden states of bi gru are sampled into a single vector by pooling which is the representation of the code fragment.
a. splitting asts and constructing st tree sequences at first source code fragments can be transformed to large asts by existing syntax analysis tools.
for each ast we split it by the granularity of statement and extract the sequence of statement trees with a preorder traversal.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
statement encoder ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... st trees encoding layer recurrent layerrepresentation ... ... h h h h h h pooling layer fig.
.
the architecture of ast based neural network formally given an ast tand a set of statement ast nodess each statement node s sintcorresponds one statement of source code.
we treat methoddeclaration as a special statement node thus s s methoddeclaration .
for nested statements as shown in figure we define the set of separate nodes p block body whereblock is for splitting the header and body of nested statements such astry andwhile statements and body for the method declaration.
all of the descendants of statement node s s is denoted by d s .
for any d d s if there exists one path from stodthrough one node p p it means that the node dis included by one statement in the body of statement s. we call node done substatement node of s. then a statement tree st tree rooted by the statement nodes sis the tree consisting of node sand all of its descendants excluding its substatement nodes in t.f o r example the first st tree rooted by methoddeclaration is surrounded by dashed lines in figure b which includes the header part such as static public and readtext and excludes the nodes of the two localvariable onetry and onereturn statement in the body.
since nodes of one sttree may have three or more children nodes we also call it multi way st tree for distinguishing it from a binary tree.
in this way one large ast can be decomposed to a sequence of non overlapping and multi way st trees.
the splitting of asts and the construction of st tree sequences are straightforward by a traverser and a constructor.
the traverser visits each node through the asts in a depthfirst walk in preorder and the constructor recursively creates a st tree to sequentially add to the st tree sequences.
such a practice guarantees that one new st tree are appended by the order in the source code.
in this way we get the sequence of st trees as the raw input of astnn.
note that the selection of splitting granularity of asts is not trivial.
we choose the statement trees in this work since statements are essential units for carrying source code semantics.
we also experimented with other granularities suchmodifier readtext formal parameter throws method declaration pooling reference type body statement vector h1 h2 h3 hn hn hn h4 ... fig.
.
the statement encoder where blue orange and green circles represent the initial embeddings hidden states and statement vector respectively.
as the node level of asts the code blocks within brace pairs and the full asts.
we will discuss these experiments in section v. if the size of selected granularity is too large e.g.
the full ast similar to the related work we may also experience the gradient vanishing problem mentioned in section ii.
but if it is too small e.g.
the node level of ast the model will become a token based rnn that may capture less syntactical knowledge of statements than ours.
our experimental results show that proposed statement level granularity is better since it has a good trade off between the size of st tree and the richness of syntactical information.
b. encoding statements on multi way st trees statement v ectors given the st trees we design a rvnn based statement encoder which is used for learning vector representations of statements.
since there are a variety of special syntactic symbols in asts we obtain all the symbols by preorder traversal of asts as the corpus for training.
the word2vec is used to learn unsupervised vectors of the symbols and the trained embeddings of symbols are served as initial parameters in the statement encoder.
because all the leaf nodes of asts representing the lexical information such as identifiers are also incorporated in the leaf nodes of st trees our symbol embeddings can capture the lexical knowledge well.
taking the first st tree rooted by the node of methoddeclaration in figure as an example the encoder traverses the st tree and recursively takes the symbol of current node as new input to compute together with the hidden states of its children nodes.
this is illustrated in figure .
we only show the first two levels here.
in the st tree the two children nodes readtext i.e.
the method name and f ormalparameter i.e.
the grammar structure defining parameters of the method as well as other siblings enrich the meaning of methoddeclaration.
if we transform the st tree to one binary tree as described in for example moving the node of readtext to one child node or descendant of the f ormalparameter node the original semantics may be destroyed.
instead we take original multi way st trees as input.
specifically given a st tree t letndenote a non leaf node and cdenote the number of its children nodes.
at authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the beginning with the pre trained embedding parameters we r v dwherevis the vocabulary size and dis the embedding dimension of symbols the lexical vector of node ncan be obtained by vn we latticetopxn wherexnis the one hot representation of symbol nandvnis the embedding.
next the vector representation of node nis computed by the following equation h wn latticetopvn summationdisplay i hi bn wherewn rd kis the weight matrix with encoding dimension k bnis a bias term hiis the hidden state for each childreni his the updated hidden state and is the activation function such as tanh or the identity function.
we use the identity function in this paper.
similarly we can recursively compute and optimize the vectors of all nodes in the st tree t. in addition in order to determine the most important features of the node vectors all nodes are pushed into a stack and then sampled by the max pooling.
that is we get the final representation of the st tree and corresponding statement by equation where nis the number of nodes in the st tree.
et i n these statement vectors can capture both lexical and statement level syntactical information of statements.
batch processing for improving the training efficiency on large datasets it is necessary to design the batch processing algorithm to encode multiple samples i.e.
code fragments simultaneously.
however generally batch processing on multiway st trees makes it difficult since the number of children nodes varies for the parent nodes in the same position of one batch .
for example given two parent nodes ns1with children nodes and ns2with children nodes in figure directly calculating equation for the two parents in one batch is impossible due to different cvalues.
to tackle this problem we design an algorithm that dynamically processes batch samples in algorithm .
intuitively although parent nodes have different number of children nodes the algorithm can dynamically detect and put all possible children nodes with the same positions to groups and then speed up the calculations of equations of each group in a batch way by leveraging matrix operations.
in algorithm we batch lsamples of st trees and then breadthfirst traverse them starting from the root nodes line .
for the current nodes nsin the same position of the batch we firstly calculate equation in batch line then detect and group all their children nodes by the node positions line .
as shown in figure we separate the children nodes to three groups by their positions and record the groups in the array lists candci.
based on these groups we recursively perform batch processing on all children nodes line .
after getting the results of all children nodes we compute equation in batch line and push all node vectors ofalgorithm dynamic batching algorithm of st trees input the array of root nodes in batched st trees b output the vectors of batched st trees bv l len b bi st tree indexes in the batch s rn l k record all node vectors call dynamicbatch b bi perform pooling on sby eq.
to get bv rl k returnbv function dynamic batch ns ids the batched current nodes nsand their indexes ids l len ns bc rl d initialize the matrix calculate eq.
in batch for nsand fill into bc according to ids initialize two array list c ci to record children nodes and their batch indexes foreachnode nsdo foreach children node child ofnode do groupchild by its position and record child tocand its batch index to ci end for end for fori len c 1do h rl k dynamicbatch c ci h h end for calculate hby eq.
in batch bz rl k for calculating bv fillhintobz according to idsand addbz tos returnh end function fig.
.
an example of dynamically batching children nodes batched st trees to the stack s line .
finally we can obtain the vectors of st tree samples and corresponding statements by pooling described in equation line .
c. representing the sequence of statements based on the sequences of st tree vectors we exploit gru to track the naturalness of statements.
we also considered the choice of using lstm and the comparison between lstm and gru will be discussed in our experiment.
given one code fragment suppose there are tsttree extracted from its ast and let q rt k t denote the vectors of encoded authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
st trees in the sequence.
at time t the transition equations are as follows rt wret urht br zt wzet uzht bz ht tanh whet rt uhht bh ht zt ht zt ht wherertis the reset gate to control the influence of previous state ztis the update gate to combine past and new information htis the candidate state and used to make a linear interpolation together with previous state ht 1to determine the current state ht.wr wz wh ur uz uh rk mare weight matrices and br bz bhare bias terms.
after iteratively computing hidden states of all time steps the sequential naturalness of these statements can be obtained.
in order to further enhance the capability of the recurrent layer for capturing the dependency information we adopt a bidirectional gru where the hidden states of both directions are concatenated to form the new states as follows ht gru et t ht gru et t ht t similar to the statement encoder the most important features of these states are then sampled by the max pooling or average pooling.
considering the importance of different statements are intuitively not equal for example api calls in themethodinvocation statements may contain more functional information thus we use max pooling for capturing the most important semantics by default.
the model finally produces a vector r r2m which is treated as the vector representation of the code fragment.
iv .
a pplications of the proposed model the proposed astnn model is general purpose.
it can be trained for task specific vector representations of source code fragments to characterize different source code semantics for many program comprehension tasks.
in this work we take two common tasks including source code classification and code clone detection as examples to show the applications of the proposed model.
source code classification .
this task aims to classify code fragments by their functionalities which is useful for program understanding and maintenance .
given the code fragment vector rand the number of categories m we obtain the logits by x wor bo wherewo r2m mis the weight matrix and bois the bias term.
we define the loss function as the widely used cross entropy loss j x y summationdisplay parenleftbigg logexp xy summationtext jexp xj parenrightbigg where denotes parameters of all the weight matrices in our model and yis the true label.
code clone detection .
detecting code clones is widely studied in software engineering research which is to detect whether two code fragments implement the same functionality.
suppose there are code fragment vectors r1andr2 and their distance is measured by r r1 r2 for semantic relatedness .
then we can treat the output y sigmoid x as their similarity where x wor bo.
the loss function is defined as binary cross entropy j y y summationdisplay y log y y log y to train astnn models for the two tasks the goal is to minimize the loss.
we use adamax in this paper because it is computationally efficient.
after all the parameters are optimized the trained models are stored.
for new code fragments they should be preprocessed into sequences of st trees and then fed into the reloaded models for prediction.
the output are probabilities pfor different labels.
for code classification since there are multiple categories the inferred value can be obtained by prediction a r gm a x i pi i m while for clone detection pis a single value in the range thus we get the prediction by prediction braceleftbiggtrue p false p where is the threshold.
v. e xperiments in this section we evaluate the proposed source code representation on two tasks of code classification task and clone detection task and compare it with several state ofthe art approaches.
a. dataset description there are two public dataset benchmarks for code classification and clone detection.
one dataset consists of simple c programs collected from the online judge oj system and made public by mou et al.
.
the programs in oj benchmark are for different programming problems.
programs have the same functionality if they aim to solve the same problem.
the other dataset bigclonebench bcb was provided by svajlenko et al.
for evaluating code clone detection tools.
bcb consists of known true and false positive clones from a big data inter project java repository.
as benchmarks the two datasets have been used by many researchers concerning on code similarity and clone detection .
the basic statistics corresponding to our two tasks are summarized in table i. b. experiment settings we used the pycparser3and javalang tools4to obtain asts for c and java code respectively.
for both tasks we trained embeddings of symbols using word2vec with skip gram authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i the statistics of datasets used for our two tasks code classification clone detection dataset oj dataset oj bcb programs code fragments classes true clone pairs .
.
max tokens max tokens avg.
tokens avg.
tokens max ast depth max ast depth avg.
ast depth .
avg.
ast depth .
.
max ast nodes max ast nodes avg.
ast nodes avg.
ast nodes algorithm and set the embedding size to be .
the hidden dimension of st tree encoder and bidirectional gru is .
we set the mini batch size to and a maximum of and epochs for the two tasks respectively.
the threshold is set to .
for clone detection.
for each dataset we randomly divide it into three parts of which the proportions are for training validation and testing.
we use the optimizer adamax with learning rate .
for training.
all the experiments are conducted on a server with cores of .4ghz cpu and a titan xp gpu.
c. evaluation on two tasks source code classification we conduct extensive experiments on the oj dataset.
apart from the state of the art model tbcnn we also take into account of traditional and other neural network based approaches including svms with statistical features textcnn lstm lscnn and pdg based graph embedding approaches as follows svms .
we use the linear svms with traditional ir methods.
tf idf n gram and lda are used to extract textual features.
the corpus are tokens extracted from the source code files.
for n gram we set the number of grams to and the number of max features to thousand.
the number of topics for lda is .
textcnn and lstm .
these two models are widely used for sentence classification in nlp.
we adapt them for this task with token sequences by treating code fragments as plain texts.
for textcnn the kernel size is set to and the number of filters is .
for lstm the dimension of hidden states is set to .
lscnn .
originally proposed for bug location lscnn extracts program features with cnn for statement embedding and uses lstm for statement sequences.
pdg based graph embedding .
most recently some studies construct program graphs by considering control flow and data flow dependencies and adopt graph embedding techniques such as hope and gated graph neural network ggnn for code representation.
although original code fragments in the oj dataset is incomplete and uncompilable they can be manually complemented by adding standard c header files and third party libraries and we use an open sourcetool frama c5to get their pdgs.
based on the pdgs we represents nodes of pdgs by the numerical id of statements in hope and average the embeddings of all tokens in each pdg node as its initial embedding in ggnn6.
after graph embedding we add a max pooling layer on all nodes of pdgs to obtain the final code representation.
to evaluate the effectiveness of source code classification we use the test accuracy metric which computes the percentage of correct classifications for the test set.
code clone detection there are generally four different types of code clones .
type is identical code fragments in addition to variations in comments and layout apart from type type is identical code fragments except for different identifier names and literal values type is syntactically similar code snippets that differ at the statement level type4 is syntactically dissimilar code snippets that implement the same functionality.
for bcb the similarity of clone pairs is defined as the average result of line based and token based metrics .
the similarity of two fragments of type and type is .
type is further divided into strongly type and moderately type of which the similarities are in range .
and .
.
respectively.
the similarity of type is in range .
and its clone pairs take up more than over all clone types.
while in oj two programs for the same problem form a clone pair of unknown type.
as table i shows similar to the previous work we choose programs from each of the first programming problems in oj namely ojclone.
it will produce more than million clone pairs which is extremely time consuming for comparison thus we randomly select thousand samples instead.
likewise we parsed nearly million true clone pairs and thousand false clone pairs from bcb.
we compare our approach with existing state of the art neural models for clone detection including rae and cdlh .
for rae the unsupervised vectors of programs are obtained by the authors open source tool7and we use them for supervised training namely rae .
its configurations are set according to their paper.
cdlh is not made public by the authors so we directly cite their results from the paper since their experiments share the same datasets with ours.
since other traditional clone detection methods like deckard and common neural models such as doc2vec8have been compared in rae and cdlh we omit them in our experiment.
similar to the experiments on code classification we also compare with the two pdg based graph embedding approaches in ojclone.
however bcb mainly contains incomplete and uncompilable method level code fragments we fail to get their pdgs.
since the code clone detection can be formulated as a binary classification problem clone or not we choose the commonly used precision p recall r and f1 measure f1 as evaluation metrics.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii compared approaches for code classification groups methods test accuracy svms svm tf idf .
svm n gram .
svm lda .
neural models textcnn .
lstm .
tbcnn .
lscnn .
pdg hope .
pdg ggnn .
our approach astnn .
d. research questions and results based on the evaluation on the two tasks we aim to investigate the following research questions rq1 how does our approach perform in source code classification?
in the task of code classification the samples are strictly balanced among the classes and our evaluation metric is the test accuracy.
experimental results are provided in table ii.
the best results are shown in bold.
we can see that traditional methods such as svms perform poorly in our experiment.
these methods mainly rely on the semantics of tokens or shallow semantics features of programs to distinguish the code functionalities but the tokens used in oj dataset are mostly arbitrary.
for example the names of identifiers are usually a i j etc.
thus most tokens contribute little to recognize the functionalities.
for those neural models the results of textcnn and lstm are better than token based methods above because they can capture some local functional features.
for example the semantics of a short scanf statement can be captured by the sliding window of textcnn or the memory cell unit in lstm.
as a neural network based on entire asts tbcnn significantly improves the accuracy since it uses the convolution sliding window over asts to capture tree structural features.
lscnn has a relatively competitive performance among existing neural models.
this can be inferred that the sequential information of statements does contribute to recognize the functionality but the accuracy is still lower than tbcnn because it cannot capture the rich structural semantics.
graph based approaches including hope and ggnn on pdgs perform poorly among the above approaches.
in particular pdg with hope gets an accuracy of only .
because the nodes of pdgs are represented by their numerical id which miss lexical knowledge and only focuses on the explicit dependency information in a high abstraction level .
pdg with ggnn uses tokens for node embedding and has an improvement but still lacks the syntactical information.
among all the approaches our model achieves the best accuracy.
specifically our model improves tbcnn by .
since our astnn model performs rvnn on much smaller st trees than original asts.
unlike existing neural models our model does not use the sliding window and binary treetable iii code clone detection models on bcb typerae cdlh astnn p r f1 p r f1 p r f1 bcb t1 bcb t2 .
.
.
bcb st3 .
.
.
.
.
.
bcb mt3 .
.
.
.
.
.
bcb t4 .
.
.
.
.
.
bcb all .
.
.
.
.
.
table iv code clone detection models on ojc lone metric rae cdlh pdg hope pdg ggnn astnn p .
.
.
.
r .
.
.
.
f1 .
.
.
.
transformation.
instead it captures knowledge about ast statements and the sequential dependencies between statements.
rq2 how does our approach perform in code clone detection?
in this research question we want to explore whether our model is effective on the challenging problem of code clone detection.
we conduct experiments on the ojclone and bcb datasets.
from ojclone we sample thousand clone pairs for experiments.
while from bcb we firstly sample thousand false clone pairs as the negative samples.
for type to strongly type we fetch all the true clone pairs belonging to that type as positive samples since their numbers are less than thousand.
for other types we sample thousand true clone pairs.
then we turn them into five groups to detect each type.
the detailed results are shown in table iii bcb and iv ojclone .
in table iii as mentioned before we cite the results of cdlh from .
since there are no p and r values reported for detailed clone types we fill them by instead.
bcb st3 and bcb mt3 represent strongly type and moderately type in bcb respectively and so on.
the bcball is a weighted sum result according to the percentage of various clone types .
in table iv we compare with two more pdg based graph embedding methods pdg hope and pdg ggnn as described before.
we first discuss the performances of rae cdlh and our astnn model on bcb.
obviously all the three approaches are quite effective in recognizing the similarities of two code fragments in type and type since both code fragments are almost the same excepting different identifier names comments and so on.
while for other types of bcb rae performs much worse than the other two approaches since it has no mechanism on memorizing history information such as lstm or gru in cdlh and astnn.
comparing cdlh with our approach we can see that astnn outperforms cdlh in terms of f1 measure especially for type .
in bcb type false clone pairs share syntactical similarity as well which authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v comparison between the proposed model and its design alternatives descriptioncode classification clone detection accuracy f1 ojclone f1 bcb ast full .
.
.
ast block .
.
.
ast node .
.
.
removing pooling i .
.
.
removing pooling ii .
.
.
lstm instead of gru .
.
.
long code fragments .
.
.
astnn .
.
.
is validated to be coincidental and is challenging to be distinguished.
this indicates our astnn model can capture more subtle syntactical difference and complex semantics than cdlh by overcoming its limitations described in section iii and capturing sequential naturalness of statements.
in ojclone similar results can be observed by rae and our model.
however cdlh performs much worse than on bcb.
unlike bcb the variable names of programs in ojclone are usually meaningless thus cdlh may miss a lot of lexical information and can only capture some syntactical information.
by contrast our astnn model can further measure the functional similarities by learning more local syntactical knowledge and the global sequential naturalness among statements.
similar to code classification we also compare with pdg based graph embedding techniques hope and ggnn.
they achieve worse performance than our astnn model due to the facts mentioned in the last research question.
rq3 what are the effects of different design choices for the proposed model?
we conduct experiments to study how different design choices affect the performance of the astnn model on the two tasks.
as shown in table v we consider the following design choices splitting granularities of asts .
given a large ast there are many ways to split it into different sequences of nonoverlapping small trees.
the two extreme ways are treating the original full ast as one special subtree ast full or extracting all nodes of the ast as special trees astnode .
besides the statement level splitting another possible way ast block is to split the ast according to blocks compound statements that include multiple statements within the same brace pairs .
after splitting the follow up encoding and bidirectional gru processings are the same as those in astnn.
we can see that ast block and astnn outperform both extreme splitting approaches of ast full and ast node.
our astnn model achieves the best performance as analyzed in section iii this is because it has a good trade off between the size of st tree and the richness of syntactical information.
pooling .
in our astnn model we use the max pooling on st trees in the statement encoder pooling i and the max pooling layer on the statement sequences after the recurrent layer pooling ii as described in section iii.
we study whether the two pooling components affect the performance or not by removing them and directly using the last layer hiddenbatch size .
.
.
.
.
.
.
.
.
.
.
.
.
.
64time cost min epoch twb pbr dba .
.
.
.
.
.
.
.
.
.
fig.
.
the time cost of different batching methods states.
from the table we can see that the pooling on statement sequences provides a comparatively significant performance boost whereas pooling on st trees matters little.
this shows that different statements of the same code fragments actually have different weights.
lstm .
in the recurrent layer of our proposed model we use gru by default.
if replacing gru by lstm the results indicate that overall lstm has a slightly poor but comparative performance with gru.
we prefer gru in our astnn model since it can achieve more efficient training.
long code fragments .
considering only long code fragments which have more than statements the percentage of long code fragments is .
in oj and the percentage of clone pairs which include at least one long code fragment is .
in bcb.
from the table we can see that our astnn model can also deal with long sequences of statements well and the performance on long code fragments remains good.
rq4 t o what extent does the proposed dynamic batching algorithm contribute to the efficiency?
as we described in section iii our statement encoder can accept batch samples with arbitrary tree structures as inputs thus can accelerate the speed of training.
however it is still unknown how efficient this algorithm is.
in order to test and verify its capability we train our model in three different ways totally without batching twb batching only on the recurrent layer pbr batching on the recurrent layer and the encoding layer by using our dynamic batching algorithm in algorithm dba .
in detail twb means calculating one sample each time pbr accepts batch samples but encodes only one st tree at each time and performs batching on the recurrent layer by padding sequences of st tree vectors dba encodes all batch samples of st trees at once and then deals with st tree sequences as pbr does.
the experiment is conducted for task and the time cost is the average running time of training and testing for each epoch.
in the experiment the batching has no effect on the performance but changes the efficiency a lot.
we find that the average time required by twb is .
minutes per epoch.
from figure we can see that both pbr and dba speed up the training and testing process when compared with twb.
dba shows a significant improvement over the others.
furthermore dba authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
runs times faster than pbr and times faster than twb when the batch size is .
this confirms that the proposed batching algorithm makes our astnn model more efficient.
vi.
r elated work a. source code representation how to effectively represent source code is a fundamental problem for software engineering research.
traditional ir and machine learning methods have been widely used for textual token based code representation.
programs are transformed to regularized token sequences for code clone detection .
sourcerercc has an improvement by exploiting token ordering along with an optimized invertedindex technique.
besides the lexical information deckard enriches programs with some syntax structured information for clone detection as well.
based on the statistical and machine learning methods the n gram model and svm are used for classifying source code authorship and domains.
maletic et al.
adopts lsi to identify semantic similarities of code fragments and the cohesion of classes in software is evaluated by lda .
recently deep learning based approaches have attracted much attention to learn distributed representation of source code .
raychev et al.
adopts rnn and n gram model for code completion.
allamanis et al.
uses a neural context model to suggest method and class names.
for defect prediction semantic features are extracted from source code by a deep belief network .
deepbugs represents code via word2vec for detecting name based bugs.
in order to capture the syntactical information of asts white et al.
exploits a recursive auto encoder over the asts with pre trained token embeddings.
tbcnn uses custom convolutional neural network on asts to learn vector representations of code snippets.
cdlh incorporates tree lstm to represent the functionality semantics of code fragments.
furthermore allamanis et al.
performs gated graph neural networks on program graphs which track the dependencies of the same variables and functions to predict variable names and detect variable misuses.
deepsim encodes code control flow and data flow into a semantic matrix for measuring code functional similarity.
multiple different code representations such as identifiers cfgs and bytecodes can also be integrated by the ensemble learning technique .
compared with these neural networks our model focuses on improving existing ast based methods and can capture the lexical statementlevel syntactical knowledge and the sequential naturalness of statements.
b. deep learning in software engineering in recent years there are many emerging deep learning applications in software engineering.
deepapi uses a sequence to sequence neural network to learn representations of natural language queries and predict relevant api sequences.
lam et al.
combines deep neural network with ir technique to recommend potential buggy files.
xu et al.
adopts word embeddings and convolutional neuralnetwork to predict the related questions in stackoverflow.
the neural machine translation is used to automatically generate commit messages .
guo et al.
proposes a rnn based neural network to generate trace links.
a joint embedding model is used in code search to map source code and natural language descriptions into a unified vector space for evaluating semantics similarity .
the above related work mainly uses neural network models to understand software related natural language texts for various tasks while we focus on the neural representation of source code.
vii.
t hreats to validity there are three main threats to the validity.
first the oj dataset is not collected from the real production environment.
however bigclonebench includes code snippets of real world java repositories from sourceforge which reduces this threat.
the second threat is about the construction of ojclone.
as we described programs under the same problem belong to a clone pair.
this leads to the uncertainty about whether they are true clone pairs although similar practice has been done by previous work .
nevertheless bigclonebench is also used for validation where the code clones are inspected manually.
therefore we believe it is of little influence on experimental results.
the last threat is that we cannot reproduce the approach of cdlh due to some details missed in that paper.
alternatively we construct the same datasets described in their paper to reduce this threat.
we will make supplement for comparison when the cdlh tool is available.
viii.
c onclusion in this work we have presented an efficient ast based neural network astnn to learn vector representations of source code fragments which can capture the lexical statementlevel syntactical knowledge and naturalness of statements.
the model decomposes large asts of code fragments into sequences of small statement trees obtains statement vectors by recursively encoding multi way statement trees and finally learns the vector representations of code fragments by leveraging the naturalness of statements.
we have evaluated astnn on two common program comprehension tasks source code classification and code clone detection.
the experimental results show that our model significantly outperforms existing approaches.
our code and experimental data are publicly available at in the future we will further evaluate the proposed model on larger scale datatsets in different programming languages and for a variety of software engineering tasks.
we will also explore other neural models to capture more deep semantics of source code.
acknowledgment this work was supported partly by national key research and development program of china no.2016yfb1000804 partly by national natural science foundation of china no.
and .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.