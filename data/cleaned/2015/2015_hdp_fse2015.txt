see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation heterogeneous defect prediction conf erence paper august .
.
citations 210reads author s jaechang nam pohang univ ersity of scienc e and t echnolog y publica tions citations see profile sunghun kim pusan national univ ersity publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y jaechang nam on sept ember .
the user has r equest ed enhanc ement of the do wnlo aded file.heterogeneous defect prediction jaechang nam and sunghun kim department of computer science and engineering the hong kong university of science and technology hong kong china jcnam hunkim cse.ust.hk abstract software defect prediction is one of the most active research areas in software engineering.
we can build a prediction model with defect data collected from a software project and predict defects in the same project i.e.
within project defect prediction wpdp .
researchers also proposed crossproject defect prediction cpdp to predict defects for new projects lacking in defect data by using prediction models built by other projects.
in recent studies cpdp is proved to be feasible.
however cpdp requires projects that have the same metric set meaning the metric sets should be identical between projects.
as a result current techniques for cpdp are di cult to apply across projects with heterogeneous metric sets.
to address the limitation we propose heterogeneous defect prediction hdp to predict defects across projects with heterogeneous metric sets.
our hdp approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets.
our empirical study on subjects shows that about of predictions using our approach outperform or are comparable to wpdp with statistical signi cance.
categories and subject descriptors d. .
management software quality assurance general terms algorithm experimentation keywords defect prediction quality assurance heterogeneous metrics .
introduction software defect prediction is one of the most active research areas in software engineering .
if software quality assurance teams can predict defects before releasing a software product they can e ectively allocate limited resources for quality control .
for example ostrand et al.
applied defect prediction in two large software systems of at t for e ective and e cient testing activities .
most defect prediction models are based on machine learning therefore it is a must to collect defect datasets to train a prediction model .
the defect datasets consist of various software metrics and labels.
commonly used software metrics for defect prediction are complexity metrics such as lines of code halstead metrics mccabe s cyclometic complexity and ck metrics and process metrics .
labels indicate whether the source code is buggy or clean for binary classi cation .
most proposed defect prediction models have been evaluated on within project defect prediction wpdp settings .
in figure 1a each instance representing a source code le or function consists of software metric values and is labeled as buggy or clean.
in the wpdp setting a prediction model is trained using the labeled instances in project a and predict unlabeled ?
instances in the same project as buggy or clean.
however it is di cult to build a prediction model for new software projects or projects with little historical information since they do not have enough training instances.
various process metrics and label information can be extracted from the historical data of software repositories such as version control and issue tracking systems .
thus it is di cult to collect process metrics and instance labels in new projects or projects that have little historical data .
for example without instances being labeled using past defect data it is not possible to build a prediction model.
to address this issue researchers have proposed crossproject defect prediction cpdp .
cpdp approaches predict defects even for new projects lacking in historical data by reusing prediction models built by other project datasets.
as shown in figure 1b a prediction model is trained by labeled instances in project a source and predicts defects in project b target .
however most cpdp approaches have a serious limitation cpdp is only feasible for projects which have exactly the same metric set as shown in figure 1b.
finding other projects with exactly the same metric set can be challenging.
publicly available defect datasets that are widely used in defect prediction literature usually have heterogeneous metric sets .
for example many nasa datasets in thet est training ?
?
model project a metric value buggy labeled instance clean labeled instance ?
unlabeled instance a within project defect prediction wpdp ?
?
?
?
?
training t est model project a source project b target same metric set b cross project defect prediction cpdp ?
training t est model project a source project c target ?
?
?
?
?
?
?
heterogeneous!metric sets c heterogeneous defect prediction hdp figure various defect prediction scenarios promise repository have metrics but aeeem datasets used by d ambroas et al.
have metrics .
the only common metric between nasa and aeeem datasets islines of code loc .
cpdp between nasa and aeeem datasets with all metric sets is not feasible since they have completely di erent metrics .
some cpdp studies use only common metrics when source and target datasets have heterogeneous metric sets .
for example turhan et al.
use the only common metrics between the nasa and softlab datasets that have heterogeneous metric sets .
however nding other projects with multiple common metrics can be challenging.
as mentioned there is only one common metric between nasa and aeeem.
also only using common metrics may degrade the performance of cpdp models.
that is because some informative metrics necessary for building a good prediction model may not be in the common metrics across datasets.
for example in the study of turhan et al.
the performance of cpdp .
by their approach did not outperform that of wpdp .
in terms of the average f measure .
in this paper we propose the heterogeneous defect prediction hdp approach to predict defects across projects even with heterogeneous metric sets.
if the proposed approach is feasible as in figure 1c we could reuse any existing defect datasets to build a prediction model.
for example many promise defect datasets even if they have heterogeneous metric sets could be used as training datasets to predict defects in any project.the key idea of our hdp approach is matching metrics that have similar distributions between source and target datasets.
in addition we also used metric selection to remove less informative metrics of a source dataset for a prediction model before metric matching.
our empirical study shows that hdp models are feasible and their prediction performance is promising.
about of hdp predictions outperform or are comparable to wpdp predictions with statistical signi cance.
our contributions are as follows propose the heterogeneous defect prediction models.
conduct an extensive and large scale empirical study to evaluate the heterogeneous defect prediction models.
.
background and related work the cpdp approaches have been studied by many researchers of late .
since the performance of cpdp is usually very poor researchers have proposed various techniques to improve cpdp .
watanabe et al.
proposed the metric compensation approach for cpdp .
the metric compensation transforms a target dataset similar to a source dataset by using the average metric values .
to evaluate the performance of the metric compensation watanabe et al.
collected two defect datasets with the same metric set object oriented metrics from two software projects and then conducted cpdp .
rahman et al.
evaluated the cpdp performance in terms of cost e ectiveness and con rmed that the prediction performance of cpdp is comparable to wpdp .
for the empirical study rahman et al.
collected datasets with the same process metric set .
fukushima et al.
conducted an empirical study of just intime defect prediction in the cpdp setting .
they used datasets with the same metric set .
the datasets were provided by kamei et al.
but projects were newly collected with the same metric set of the datasets .
however collecting datasets with the same metric set might limit cpdp.
for example if existing defect datasets contain object oriented metrics such as ck metrics collecting the same object oriented metrics is impossible for projects that are written in non object oriented languages.
turhan et al.
proposed the nearest neighbour nn lter to improve the performance of cpdp .
the basic idea of the nn lter is that prediction models are built by source instances that are nearest neighbours of target instances .
to conduct cpdp turhan et al.
used nasa and softlab datasets in the promise repository .
ma et al.
proposed transfer naive bayes tnb .
the tnb builds a prediction model by weighting source instances similar to target instances .
using the same datasets used by turhan et al.
ma et al.
evaluated the tnb models for cpdp .
since the datasets used in the empirical studies of turhan et al.
and ma et al.
have heterogeneous metric sets they conducted cpdp using the common metrics .
there is another cpdp study with the top k common metric subset .
however as explained in section cpdp using common metrics is worse than wpdp .
nam et al.
adapted a state of the art transfer learning technique called transfer component analysis tca and proposed tca .
they used datasets in two groups relink and aeeem with and metrics respectively .however nam et al.
could not conduct cpdp between relink and aeeem because they have heterogeneous metric sets.
since the project pool with the same metric set is very limited conducting cpdp using a project group with the same metric set can be limited as well.
for example at most of defect datasets in the promise repository have the same metric set .
in other words we cannot directly conduct cpdp for the of the defect datasets by using the remaining datasets in the promise repository .
cpdp studies conducted by canfora et al.
and panichella et al.
use java projects only with the same metric set from the promise repository zhang et al.
proposed the universal model for cpdp .
the universal model is built using projects from sourceforge and google code and leads to comparable prediction results to wpdp in their experimental setting .
however the universal defect prediction model may be di cult to apply for the projects with heterogeneous metric sets since the universal model uses metrics including code metrics object oriented metrics and process metrics.
in other words the model can only be applicable for target datasets with the same metrics.
in the case where the target project has not been developed in object oriented languages a universal model built using object oriented metrics cannot be used for the target dataset.
he et al.
addressed the limitations due to heterogeneous metric sets in cpdp studies listed above .
their approach cpdp ifs used distribution characteristic vectors of an instance as metrics.
the prediction performance of their best approach is comparable to or helpful in improving regular cpdp models .
however the approach by he et al.
is not compared with wpdp .
although their best approach is helpful to improve regular cpdp models the evaluation might be weak since the prediction performance of a regular cpdp is usually very poor .
in addition he et al.
conducted experiments on only projects in dataset groups .
we propose hdp to address the above limitations caused by projects with heterogeneous metric sets.
contrary to the study by he et al.
we compare hdp to wpdp and hdp achieved better or comparable prediction performance to wpdp in about of predictions.
in addition we conducted extensive experiments on projects in dataset groups.
in section we explain our approach in detail.
.
approach figure shows the overview of hdp based on metric selection and metric matching.
in the gure we have two datasets source and target with heterogeneous metric sets.
each row and column of a dataset represents an instance and a metric respectively and the last column represents instance labels.
as shown in the gure the metric sets in the source and target datasets are not identical x1tox4 andy1toy7respectively .
when given source and target datasets with heterogeneous metric sets for metric selection we rst apply a feature selection technique to the source.
feature selection is a common approach used in machine learning for selecting a subset of features by removing redundant and irrelevant features .
we apply widely used feature selection techniques for metric selection of a source dataset as in section .
.
after that metrics based on their similarity such as distribution or correlation between the source and target metx1 x2 x3 x4 label buggy clean clean metric matching source project a target project b prediction model build training predict test metric selection y1 y2 y3 y4 y5 y6 y7 label ?
?
?
buggy clean clean buggy clean clean ?
?
?
figure heterogeneous defect prediction rics are matched up.
in figure three target metrics are matched with the same number of source metrics.
after these processes we nally arrive at a matched source and target metric set.
with the nal source dataset hdp builds a model and predicts labels of target instances.
in the following subsections we explain the metric selection and matching in detail.
.
metric selection in source datasets for metric selection we used various feature selection approaches widely used in defect prediction such as gain ratio chi square relief f and signi cance attribute evaluation .
according to benchmark studies about various feature selection approaches a single best feature selection approach for all prediction models does not exist .
for this reason we conduct experiments under di erent feature selection approaches.
when applying feature selection approaches we select top of metrics as suggested by gao et al.
.
in addition we compare the prediction results with or without metric selection in the experiments.
.
matching source and target metrics to match source and target metrics we measure the similarity of each source and target metric pair by using several existing methods such as percentiles kolmogorov smirnov test and spearman s correlation coe cient .
we dene the following three analyzers for metric matching percentile based matching panalyzer kolmogorov smirnov test based matching ksanalyzer spearman s correlation based matching scoanalyzer the key idea of these analyzers is computing matching scores for all pairs between the source and target metrics.
figure shows a sample matching.
there are two source metrics x1andx2 and two target metrics y1andy2 .
thus there are four possible matching pairs x1 y1 x1 y2 x2 y1 and x2 y2 .
the numbers in rectangles betweensource metrics target metrics x1 x2 y1 y2 .
.
.
.
figure an example of metric matching between source and target datasets.
matched source and target metrics in figure represent matching scores computed by an analyzer.
for example the matching score between the metrics x1andy1 is .
.
from all pairs between the source and target metrics we remove poorly matched metrics whose matching score is not greater than a speci c cuto threshold.
for example if the matching score cuto threshold is .
we include only the matched metrics whose matching score is greater than .
.
in figure the edge x1 y2 in matched metrics will be excluded when the cuto threshold is .
.
thus all the candidate matching pairs we can consider include the edges x1 y1 x2 y2 and x2 y1 in this example.
in section we design our empirical study under di erent matching score cuto thresholds to investigate their impact on prediction.
we may not have any matched metrics based on the cuto threshold.
in this case we cannot conduct defect prediction.
in figure if the cuto threshold is .
none of the matched metrics are considered for hdp so we cannot build a prediction model for the target dataset.
for this reason we investigate target prediction coverage i.e.
what percentage of target datasets could be predicted?
in our experiments.
after applying the cuto threshold we used the maximum weighted bipartite matching technique to select a group of matched metrics whose sum of matching scores is highest without duplicated metrics.
in figure after applying the cuto threshold of .
we can form two groups of matched metrics without duplicated metrics.
the rst group consists of the edges x1 y1 and x2 y2 and another group consists of the edge x2 y1 .
in each group there are no duplicated metrics.
the sum of matching scores in the rst group is .
.
.
and that of the second group is .
.
the rst group has a greater sum .
of matching scores than the second one .
.
thus we select the rst matching group as the set of matched metrics for the given source and target metrics with the cuto threshold of .
in this example.
each analyzer for the metric matching scores is described below.
.
.
panalyzer panalyzer simply compares percentiles 10th 20th .
.
.
90th of ordered values between source and target metrics.
first we compute the di erence of n th percentiles in source and target metric values by the following equation pij n spij n bpij n where pij n is the comparison function for n th percentiles ofi th source and j th target metrics and spij n and bpij n are smaller and bigger percentile values respectively at n thpercentiles of i th source and j th target metrics.
for example if the 10th percentile of the source metric values is and that of target metric values is the di erence is .
pij .
using this percentile comparison function a matching score between source and target metrics is calculated by the following equation mij 9p k 1pij k where mijis a matching score between i th source and j th target metrics.
the best matching score of this equation is .
when the values of the source and target metrics of all percentiles are the same.
.
.
ksanalyzer ksanalyzer uses a p value from the kolmogorov smirnov test ks test as a matching score between source and target metrics.
the ks test is a non parametric two sample test that can be applicable when we cannot be sure about the normality of two samples and or the same variance .
since metrics in some defect datasets used in our empirical study have exponential distributions and metrics in other datasets have unknown distributions and variances the ks test is a suitable statistical test to compute p values for these datasets.
in statistical testing a p value shows the probability of whether two samples are signi cantly di erent or not.
we used the kolmogorovsmirnovtest implemented in the apache commons math library.
the matching score is mij pij where pijis a p value from the ks test of i th source and j th target metrics.
a p value tends to be zero when two metrics are signi cantly di erent.
.
.
scoanalyzer in scoanalyzer we used the spearman s rank correlation coe cient as a matching score for source and target metrics .
spearman s rank correlation measures how two samples are correlated .
to compute the coe cient we used the spearmanscorrelation in the apache commons math library.
since the size of metric vectors should be the same to compute the coe cient we randomly select metric values from a metric vector that is of a greater size than another metric vector.
for example if the sizes of the source and target metric vectors are and respectively we randomly select metric values from the source metric to agree to the size between the source and target metrics.
all metric values are sorted before computing the coe cient.
the matching score is as follows mij cij where cijis a spearman s rank correlation coe cient between i th source and j th target metrics.
.
building prediction models after applying metric selection and matching we can nally build a prediction model using a source dataset with selected and matched metrics.
then as a regular defect prediction model we can predict defects on a target dataset with metrics matched to selected source metrics.table the defect datasets from ve groups.
group dataset of instances of metricsprediction granularity all buggy aeeem eq .
classjdt .
lc .
ml .
pde .
relink apache .
file safe .
zxing .
morph ant .
.
classarc .
camel .
.
poi .
.
redaktor .
skarbonka .
tomcat .
velocity .
.
xalan .
.
xerces .
.
nasa cm1 .
functionmw1 .
pc1 .
pc3 .
pc4 .
softlab ar1 .
functionar3 .
ar4 .
ar5 .
ar6 .
.
experimental setup .
research questions to systematically evaluate heterogeneous defect prediction hdp models we set three research questions.
rq1 is heterogeneous defect prediction comparable to wpdp baseline1 ?
rq2 is heterogeneous defect prediction comparable to cpdp using common metrics cpdp cm baseline2 ?
rq3 is heterogeneous defect prediction comparable to cpdp ifs baseline3 ?
rq1 rq2 and rq3 lead us to investigate whether our hdp is comparable to wpdp baseline1 cpdp cm baseline2 and cddp ifs baseline3 .
.
benchmark datasets we collected publicly available datasets from previous studies .
table lists all dataset groups used in our experiments.
each dataset group has a heterogeneous metric set as shown in the table.
prediction granularity in the last column of the table means the prediction granularity of instances.
since we focus on the distribution or correlation of metric values when matching metrics it is bene cial to be able to apply the hdp approach on datasets even in di erent granularity levels.
we used ve groups with defect datasets aeeem relink morph nasa and softlab.
aeeem was used to benchmark di erent defect prediction models and to evaluate cpdp techniques .
each aeeem dataset consists of metrics including objectoriented oo metrics previous defect metrics entropy metrics of change and code and churn of source code metrics .
datasets in relink were used by wu et al.
to improve the defect prediction performance by increasing the quality of the defect data and have code complexity metrics extracted by the understand tool .the morph group contains defect datasets of several open source projects used in the study about the dataset privacy issue for defect prediction .
the metrics used in morph are mccabe s cyclomatic metrics ck metrics and other oo metrics .
nasa and softlab contain proprietary datasets from nasa and a turkish software company respectively .
we used ve nasa datasets which share the same metric set in the promise repository .
we used cleaned nasa datasets ds0version .
for the softlab group we used all softlab datasets in the promise repository .
the metrics used in both nasa and softlab groups are halstead and mccabe s cyclomatic metrics but nasa has additional complexity metrics such as parameter count and percentage of comments .
predicting defects is conducted across di erent dataset groups.
for example we build a prediction model by apache in relink and tested the model on velocity .
in morph apache velocity .
.
we did not conduct defect prediction across projects in the same group where datasets have the same metric set since the focus of our study is on prediction across datasets with heterogeneous metric sets.
in total we have possible prediction combinations from these datasets.
.
matching score cutoff thresholds to build hdp models we apply various cuto thresholds for matching scores to observe how prediction performance varies according to di erent cuto values.
matched metrics by analyzers have their own matching scores as explained in section .
we apply di erent cuto values .
and .
.
.
.
.
.
for the hdp models.
if a matching score cuto is .
we remove matched metrics with the matching score .
and build a prediction model with matched metrics with the score .
.
the number of matched metrics varies by each prediction combination.
for example when using ksanalyzer with the cuto of .
the number of matched metrics is four in cm1 ar5 while that is one in ar6 pc3.
the average number of matched metrics also varies by analyzers and cuto values panalyzer ksanalyzer and scoanalyzer in the cuto of .
but panalyzer ksanalyzer and scoanalyzer in the cuto of .
on average.
.
baselines we compare hdp to three baselines wpdp baseline1 cpdp using common metrics cpdp cm between source and target datasets baseline2 and cpdp ifs baseline3 .
we rst compare hdp to wpdp.
comparing hdp to wpdp will provide empirical evidence of whether our hdp models are applicable in practice.
we conduct cpdp using only common metrics cpdpcm between source and target datasets as in previous cpdp studies .
for example aeeem and morph have oo metrics as common metrics so we use them to build prediction models for datasets between aeeem and morph.
since using common metrics has been adopted to address the limitation on heterogeneous metric sets in previous cpdp studies we set cpdp cm as a baseline to evaluate our hdp models.
the number of matched metrics varies across the dataset group.
between aeeem 1hereafter a rightward arrow denotes a prediction combination.and relink only one common metric exists loc .
nasa and softlab have common metrics.
on average the number of common metrics in our datasets are about ve.
we include cpdp ifs proposed by he et al.
as a baseline .
cpdp ifs enables defect prediction on projects with heterogeneous metric sets imbalanced feature sets by using the distribution characteristics of values of each instance such as mode median mean harmonic mean minimum maximum range variation ratio rst quartile third quartile interquartile range variance standard deviation coe cient of variance skewness and kurtosis .
the distribution characteristics are used as features to build a prediction model.
.
experimental design for the machine learning algorithm we use logistic regression which is widely used for both wpdp and cpdp studies .
we use logistic regression implemented in weka with default options .
for wpdp it is necessary to split datasets into training and test sets.
we use random splits which are widely used in the evaluation of defect prediction models .
for the random splits we use one half of the instances for training a model and the rest for test round .
in addition we use the two splits in a reverse way where we use the previous test set for training and the previous training set for test round .
we repeat these two rounds times i.e.
tests since there is a randomness in selecting instances for each split .
simply speaking we repeat the two fold cross validation times.
for cpdp cm cpdp ifs and hdp we build a prediction model by using a source dataset and test the model on the same test splits used in wpdp.
since there are different test splits for a within project prediction the cpdpcm cpdp ifs and hdp models are tested on di erent test splits as well.
.
measures to evaluate the prediction performance we use the area under the receiver operating characteristic curve auc .
the auc is known as a useful measure for comparing different models and is widely used because auc is una ected by class imbalance as well as being independent from the cuto probability prediction threshold that is used to decide whether an instance should be classi ed as positive or negative .
mende con rmed that it is di cult to compare the defect prediction performance reported in the defect prediction literature since prediction results come from the di erent cuto s of prediction thresholds .
however the receiver operating characteristic curve is drawn by both the true positive rate recall and the false positive rate on various prediction threshold values.
the higher auc represents better prediction performance and the auc of .
means the performance of a random predictor .
to compare hdp by our approach to baselines we also use the win tie loss evaluation which is used for performance comparison between di erent experimental settings in many studies .
as we repeat the experiments times for a target project dataset we conduct the wilcoxon signed rank test p .
for all auc values in baselines and hdp .
if an hdp model for the target dataset outperforms a corresponding baseline result after the statistical test we mark this hdp model as a win .
in a similar way table comparison results among wpdp cpdpcm cpdp ifs and hdp by ksanalyzer with the cuto of .
in a median auc.
targetwpdp baseline1 cpdp cm baseline2 cpdp ifs baseline3 hdp ksanalyzer cuto .
eq .
.
.
.
jdt .
.
.
.
lc .
.
.
.
ml .
.
.
.
pde .
.
.
.
apache .
.
.
.
safe .
.
.
.
zxing .
.
.
.
ant .
.
.
.
.
arc .
.
.
.
camel .
.
.
.
.
poi .
.
.
.
.
redaktor .
.
.
.
skarbonka .
.
.
.
tomcat .
.
.
.
velocity .
.
.
.
.
xalan .
.
.
.
.
xerces .
.
.
.
.
cm1 .
.
.
.
mw1 .
.
.
.
pc1 .
.
.
.
pc3 .
.
.
.
pc4 .
.
.
.
ar1 .
.
.
.
ar3 .
.
.
.
ar4 .
.
.
.
ar5 .
.
.
.
ar6 .
.
.
.
all .
.
.
.
we mark an hdp model as a loss when the results of a baseline outperforms that of our hdp approach with statistical signi cance.
if there is no di erence between a baseline and hdp with statistical signi cance we mark this case as a tie .
then we count the number of wins ties and losses for hdp models.
by using the win tie loss evaluation we can investigate how many hdp predictions it will take to improve baseline approaches.
.
results in this section we report the experimental results of the hdp approach by signi cance attribute selection for metric selection and ksanalyzer with the cuto threshold of .
.
among di erent metric selections signi cance attribute selection led to the best prediction performance overall.
in terms of analyzers ksanalyzer led to the best prediction performance.
since the ksanalyzer is based on the p value of a statistical test we chose a cuto of .
which is a commonly accepted signi cance level in the statistical test .
.
comparison result with baselines table shows the prediction performance a median auc of baselines and hdp by ksanalyzer with the cuto of .
for each target as well as all targets the last row in the table .
baseline1 represents the wpdp results of a target project and baseline2 shows the cpdp results using common metrics cpdp cm between source and target projects.
baseline3 shows the results of cpdp ifs proposed by he et al.
.
the last column shows the hdp results by ksanalyzer with the cuto of .
.
if there are better results between baseline1 and our approach with statisticaltable median aucs of baselines and hdp in ksanalyzer cuto .
by each source group.
sourcewpdp baseline1 cpdp cm baseline2 cpdp ifs baseline3 hdp ks .05target coverage of hdp aeeem .
.
.
.
relink .
.
.
.
morph .
.
.
.
nasa .
.
.
.
softlab .
.
.
.
signi cance wilcoxon signed rank test p .
the better auc values are in bold font as shown in table .
between baseline2 and our approach better auc values with statistical signi cance are underlined in the table.
between baseline3 and our approach better auc values with statistical signi cance are shown with an asterisk .
we observed the following results about rq1 in out of targets hdp by ksanalyzer with the cuto of .
leads to better or comparable results against wpdp with statistical signi cance.
the wpdp results in only ml pc3 and pc4 are in bold font.
hdp by ksanalyzer with the cuto of .
outperforms wpdp with statistical signi cance when considering results from all targets allin the last row in the table together in our experimental settings.
the following results are related to rq2 hdp by ksanalyzer with the cuto of .
leads to better or comparable results to cpdp cm with statistical signi cance.
no underlines in cpdp cm of table hdp by ksanalyzer with the cuto of .
outperforms cpdp cm with statistical signi cance when considering results from alltargets in our experimental settings.
in terms of rq3 we observed the following results hdp by ksanalyzer with the cuto of .
leads to better or comparable results to cpdp ifs with statistical signi cance.
no asterisks in cpdp ifs of table hdp by ksanalyzer with the cuto of .
outperforms cpdp ifs with statistical signi cance when considering results from alltargets in our experimental settings.
.
target prediction coverage target prediction coverage shows how many target projects can be predicted by the hdp models.
if there are no feasible prediction combinations for a target because of there being no matched metrics between source and target datasets it might be di cult to use an hdp model in practice.
for target prediction coverage we analysed our hdp results by ksanalyzer with the cuto of .
by each source group.
for example after applying metric selection and matching we can build a prediction model by using eq in aeeem and predict each of target projects in four other dataset groups.
however because of the cuto value some predictions may not be feasible.
for example eq apache was not feasible because there are no matched metrics whose matching scores are greater than .
.
instead another source dataset jdt in aeeem has matched metrics to apache.
in this case we consider the source group aeeem covered apache.
in other words if any dataset in a source group can be used to build an hdp model for a target we count the target prediction is as covered.table shows the median aucs and prediction target coverage.
the median aucs were computed by the auc values of the feasible hdp predictions and their corresponding predictions of wpdp cpdp cm and cpdp ifs.
we conducted the wilcoxon signed rank test on results between wpdp and baselines .
like table better results between baselines and our approach with statistical signi cance are in bold font underlined and or with asterisks.
first of all in each source group we could observe hdp outperforms or is comparable to wpdp with statistical signi cance.
for example target projects were predicted by some projects in relink and the median auc for hdp by ksanalyzer is .
while that of wpdp is .
.
in addition hdp by ksanalyzer also outperforms or had a comparable prediction performance against cpdp cm.
there are no better results in cpdp cm than those in hdp by ksanalyzer with statistical signi cance no underlined results in third column in table .
in addition hdp by ksabalyzer outperforms cpdp ifs in all source groups.
the target prediction coverage in the morph and softlab groups yielded as shown in table .
this implies our hdp models may conduct defect prediction with high target coverage even using datasets which only appear in one source group.
aeeem relink and nasa groups have and respectively since some prediction combinations do not have matched metrics because of low matching scores .
.
thus some prediction combinations using matched metrics with low matching scores can be automatically excluded.
in this sense our hdp approach follows a similar concept to the two phase prediction model checking prediction feasibility between source and target datasets and predicting defects.
.
win tie loss results to investigate our performance evaluation in detail we report the win tie loss results of hdp by ksanalyzer with the cuto of .
against wpdp baseline1 cpdp cm baseline2 and cpdp ifs baseline3 in table .
ksanalyzer with the cuto of .
conducted out of prediction combinations since combinations do not have any matched metrics because of the cuto threshold.
in table the target dataset eq was predicted in four prediction combinations and our approach hdp outperforms baseline1 and baseline3 in the four combinations i.e.
wins .
however hdp outperforms baseline2 in only two combinations of the target eq wins .
against baseline1 the six targets such as eq zxing skarbonka tomcat ar3 and ar4 have only win results.
in other words defects in those six targets could be predicted better by other source projects using hdp models by ksanalyzer compared to wpdp models.
however the eight targets such as jdt ml redaktor velocity .
xalan .
xerces .
pc3 and pc4 have no wins at all against baseline1.
in addition other targets still have losses even though they have win or tie results.
overall the numbers of win and tie results are and respectively out of all of the prediction combinations.
this means that about .
of prediction combinations by our hdp models achieve better or comparable prediction performance than those in wpdp.
the win tie loss results against baseline2 and baseline3 show a similar trend.
the hdp results in the .
out of prediction combinations show hdp out table win tie loss results of hdp by ksanalyzer cuto .
against wpdp baseline1 cpdp cm baseline2 and cpdp ifs baseline3 .
targetagainst wpdp baseline1 cpdp cm baseline2 cpdp ifs baseline3 win tie loss win tie loss win tie loss eq jdt lc ml pde apache safe zxing ant .
arc camel .
poi .
redaktor skarbonka tomcat velocity .
xalan .
xerces .
cm1 mw1 pc1 pc3 pc4 ar1 ar3 ar4 ar5 ar6 total147 .
.
.
.
.
.
.
.
.
performs and is comparable to cpdp cm.
against baseline3 .
predction combinations are win or tie results.
the win tie loss results show that with our hdp model by ksanalyzer there is a higher possibility of getting a better prediction performance.
however there are still about loss results against wpdp.
in section we discuss and analyze why loss results happen.
.
discussion .
why matched metric works?
in figure we use box plots to represent distributions of matched metrics.
the gray black and white box plots shows distributions of matched metrics in all buggy and clean instances respectively.
the three box plots on the lefthand side represent distributions of a source metric while the three box plots on the right hand side represent those of a target metric.
the bottom and top of the boxes represent the rst and third quartiles respectively.
the solid horizontal line in a box represents the median value in each distribution.
black points in the gure are outliers.
figure explains how the prediction combination of ant1.
ar5 can have a high auc .
.
suppose that a simple model predicts that an instance is buggy when the metric value of the instance is more than in the case of figure .
in both datasets approximately or more buggy and clean instances will be predicted correctly.
in figure source ant .
rfc target ar5 unique operands distributionmetric valuesinstancesallbuggycleanfigure distribution of metrics matching score .
from ant .
ar5 auc .
.
source safe countlinecode target velocity .
loc distributionmetric valuesinstancesallbuggyclean figure distribution of metrics matching score .
from safe velocity .
auc .
.
the matched metrics in ant .
ar5 are the response for class rfc number of methods invoked by a class and the number of unique operands unique operands respectively.
the rfc and unique operands are not the same metric so it might look like an arbitrary matching.
however they are matched based on their similar distributions as shown in figure .
typical defect prediction metrics have tendencies in which higher complexity causes more bugproneness .
in figure instances with higher values of rfc and unique operands have the tendency to be more bug prone.
for this reason the model using the matched metrics could achieve such a high auc .
.
we could observe this bug prone tendency in other win results.
since matching metrics is based on similarity of source and target metric distributions hdp also addresses several issues related to a dataset shift such as the covariate shift and domain shift discussed by turhan .
some prediction combinations have loss results in table .
we investigated matched metrics in these prediction combinations.
in velocity .
of table all results are loss.
thus as a representative loss result we discuss the prediction combination safe velocity .
whose auc is .
.
as observed loss results were usually caused by di erent tendencies of bug proneness between source and target metrics.
figure shows how the bug prone tendencies of source and target metrics are di erent.
interestingly the matched source and target metric by the ksanalyzer is the same as loc countlinecode and loc in both.
as we observe in the gure the median of buggy instance values of the source metric is higher than that of clean instances intable prediction performance a median auc and of win in di erent metric selections.
approachagainsthdpwpdp cpdp cm cpdp ifs auc win auc win auc win auc gain ratio .
.
.
.
.
.
.
chi square .
.
.
.
.
.
.
signi cance .
.
.
.
.
.
.
relief f .
.
.
.
.
.
.
none .
.
.
.
.
.
.
that the more loc implies the higher bug proneness in the case of safe.
however the median of buggy instance values in a target metric is lower than that of clean instance values in that the less loc implies the higher bug proneness in velocity .
.
this inconsistent tendency of bug proneness between the source and target metrics could degrade the prediction performance although they are the same metric.
we regard the matching that has an inconsistent bugprone tendency between source and target metrics as a noisy metric matching.
we could observe this kind of noisy metric matching in prediction combinations in other loss results.
however it is very challenging to lter out the noisy metric matching since we cannot know labels of target instances in advance.
if we could design a lter for the noisy metric matching the loss results would be minimized.
thus designing a new lter to mitigate these loss results is an interesting problem to address.
investigating this new lter for the noisy metric matching will remain as future work.
figure also explains why cpdp cm did not show reasonable prediction performance.
although the matched metrics are same as loc its bug prone tendency is inconsistent.
thus this matching using the common metric was noisy and was not helpful for building a prediction model.
we also investigated whether the performance of hdp can be a ected by the size of a target dataset.
if the size of a target dataset gets smaller it might be di cult to precisely compute distribution similarity between source and target metrics.
as in table the sizes of datasets vary from ar5 and ml and the results of hdp in table also vary.
we could not nd any relation between the size of a target and the prediction performance.
in ar5 of table out of predictions led to win results while in ml all six predictions led to loss results.
in velocity .
that has a relatively small number of instances compared to other datasets all three predictions had loss results.
however tomcat that is of a bigger size than velocity .
had only win results.
as discussed the prediction performance of hdp is dependent on the bug prone tendencies of the matched metrics.
if the matched metrics are consistent in terms of the bug prone tendency hdp can lead to promising results regardless of the size of the target dataset.
.
performance in different metric selections table shows prediction results on various metric selection approaches including with no metric selection none .
we compare the median aucs of the hdp results by ksanalyzer with the cuto of .
to those of wpdp cpdp cm or cpdp ifs and report of win results.
overall we could observe metric selection to be helpful in improving prediction models in terms of auc.
when applying metric selection the win results account for more than about in most cases against wpdp and cpdp table prediction performance in other analyzers with the matching score cuto s .
and .
.
anz analyzer tgtcov target coverage anzcut o againsthdp tgt covwpdp cpdp cm cpdp ifs auc win auc win auc win auc p0.
.
.
.
.
.
.
.
p0.
.
.
.
.
.
.
.
ks0.
.
.
.
.
.
.
.
ks0.
.
.
.
.
.
.
sco0.
.
.
.
.
.
.
.
sco0.
.
.
.
.
.
.
.
cm.
against cpdp ifs the win results of hdp account for more than after appying the metric selection approaches.
this implies that the metric selection approaches can remove irrelevant metrics to build a better prediction model.
in addition this result con rms the previous studies that we can build prediction models better than or comparable to wpdp models with even a few key metrics .
however the percentages of win results in none were lower than those in applying metric selection.
among metric selection approaches chi square and signi cance based approaches lead to the best performance in terms of the percentage of the win results .
.
against wpdp.
.
performance in various analyzers in table we compare the prediction performance in other analyzers with the matching score cuto thresholds .
and .
.
hdp s prediction results by panalyzer with a cuto of .
are comparable to wpdp.
this implies that comparing percentiles between source and target metrics can evaluate the similarity of them well with a threshold of .
.
however panalyzer with the cuto of .
did not achieve target coverage and is too simple an approach to lead to better prediction performance than ksanalyzer.
in ksanalyzer with a cuto of .
the auc .
outperforms it .
in wpdp with statistical signi cance.
hdp by ksanalyzer with a cuto of .
could lead to signi cant improvement in the auc value .
compared to that .
with the cuto of .
.
however the target coverage is just .
this is because some prediction combinations are automatically ltered out since poorly matched metrics whose matching score is not greater than the cuto are ignored.
in other words defect prediction for of targets was not conducted since the matching scores of matched metrics in prediction combinations for the targets are not greater than .
so that all matched metrics in the combinations were ignored.
an interesting observation in panalyzer and ksanalyzer is that auc values of hdp by those analyzers improved when a cuto threshold increased.
as the cuto threshold increased as .
.
.
.
.
.
and .
we observed prediction results by panalyzer and ksanalyzer gradually improved from .
to .
and .
to .
in auc respectively.
this means these two analyzers can lter out negative prediction combinations well.
as a result the percentage of win results are also signi cantly increased.
results by scoanalyzer were worse than wpdp results.
in addition prediction performance rarely changed regardless of cuto thresholds results by scoanalyzer in di erent cuto s from .
to .
did not vary as well.
a possibletable win tie percentages of hdp by ksanalyzer cuto .
against wpdp cpdp cm and cpdp ifs by di erent machine learners.
hdp learnersagainst wpdp cpdp cm cpdp ifs win tie win tie win tie logistic .
.
.
.
.
.
randomforest .
.
.
.
.
.
bayesnet .
.
.
.
.
.
svm .
.
.
.
.
.
j48 .
.
.
.
.
.
simplelogistic .
.
.
.
.
.
lmt .
.
.
.
.
.
reason is that scoanalyzer does not directly compare the distribution between source and target metrics.
this result implies that the similarity of distribution between source and target metrics is a very important factor for building a better prediction model.
.
performance in various learners to investigate if hdp works with other machine learners we built hdp models ksanalyzer and the cuto of .
with various learners used in defect prediction literature such as random forest bayesnet svm j48 decision tree simple logistic and logistic model trees lmt .
table shows win tie results.
logistic regression logistic led to the best results among various learners.
the logistic regression models works well when there is a linear relationship between a predictor variable a metric and the logit transformation of the outcome variable bug proneness .
in our study this linear relationship is related to the bug prone tendency of a metric that is a higher complexity causes more bug proneness .
as the consistent bug prone tendency of matched metrics is important in hdp the hdp models built by logistic regression can lead to the best prediction performance.
hdp models built by other learners such as simple logistic and lmt led to comparable results to logistic regression against cpdp cm and cpdp ifs.
against baseline2 win results .
and .
in simple logistic and lmt are comparable to win results .
in logistic.
simple logistic also uses the logit function and lmt adopts logistic regression at the leaves of decision tree .
in other words both learners consider the linear relationship like logistic regression .
in our experimental settings hdp tends to work well with the learners based on the linear relationship between a metric and a label bug proneness .
.
practical guidelines for hdp we proposed the hdp models to enable defect prediction on software projects by using training datasets from other projects even with heterogeneous metric sets.
when we have training datasets in the same project or in other projects with the same metric set we can simply conduct wpdp or cpdp using recently proposed cpdp techniques respectively .
however in practice it might be that no training datasets for both wpdp and cpdp exist.
in this case we can apply the hdp approach.
in section and table we con rm that the overall result from hdp by ksanalyzer with the cuto of .
outperforms the wpdp and shows target coverage.
since ksanalyzer can match similar source and target metrics we guide the use of ksanalyzer for hdp.
in terms of the matching score cuto threshold there is a trade o between prediction performance and target coverage.
since a cuto of .
that is the widely used level of statistical signi cance we can conduct hdp using ksanalyzer with the cuto of .
.
however we observe some loss results in our empirical study.
to minimize the percentage of loss results we can sacri ce the target coverage by increasing the cuto as table shows ksanalyzer with the cuto of .
led to win results in feasible predictions against wpdp.
.
threats to validity we cannot generalize our conclusions since datasets for the experiments may not be representative.
however we tried to select various datasets used in papers published in top software engineering venues .
in addition we used datasets from both open source aeeem relink and morph and proprietary projects nasa and softlab .
we evaluated our hdp models in auc.
auc is known as a good measure for comparing di erent prediction models .
however validating prediction models in terms of both precision and recall is also required in practice.
to fairly compare wpdp and hdp models in precision and recall we need to identify a proper threshold of prediction probability.
identifying the proper threshold is a challenging issue and remains as future work.
we computed matching scores using all source and target instances for each prediction combination.
however with the matching scores we tested prediction models on a test set from the random splits because of the wpdp models as explained in section .
.
to conduct wpdp with all instances of a project dataset as a test set we need a training dataset from the previous release of the same project.
however the training dataset is not available for our subjects.
instead of using the random splits we additionally conducted experiments about hdp against cpdp cm and cpdp ifs by testing a prediction model on a test set using all instances of a target dataset in each prediction combination.
in this setting the median auc of hdp was .
and the median aucs of the corresponding results in cpdp cm and cpdp ifs were .
and .
respectively.
these results are similar to those from the random splits in section .
in other words as long as the bug prone tendency of matched metrics is consistent hdp yields promising results.
.
conclusion cross project defect prediction cannot be conducted across projects with heterogeneous metric sets.
to address this limitation we proposed heterogeneous defect prediction hdp based on metric matching using statistical analysis .
our empirical evaluation showed the proposed hdp models are feasible and yield promising results.
hdp is very promising as it permits potentially all heterogeneous datasets of software projects to be used for defect prediction on new projects or projects lacking in defect data.
in addition it may not be limited to defect prediction.
this technique can potentially be applicable to all prediction and recommendation based approaches for software engineering problems.
as future work we will explore the feasibility of building various prediction and recommendation models using heterogeneous datasets.
.