a comparative study of programming languages in rosetta code sebastian nanz carlo a. furia chair of software engineering department of computer science eth zurich switzerland firstname.lastname inf.ethz.ch abstract sometimes debates on programming languages are more religious than scientific.
questions about which language is more succinct or efficient or makes developers more productive are discussed with fervor and their answers are too often based on anecdotes and unsubstantiated beliefs.
in this study we use the largely untapped research potential of rosetta code a code repository of solutions to common programming tasks in various languages which offers a large data set for analysis.
our study is based on solution programs corresponding to tasks in widely used languages representing the major programming paradigms procedural c and go object oriented c and java functional f and haskell scripting python and ruby .
our statistical analysis reveals most notably that functional and scripting languages are more concise than procedural and objectoriented languages c is hard to beat when it comes to raw speed on large inputs but performance differences over inputs of moderate size are less pronounced and allow even interpreted languages to be competitive compiled strongly typed languages where more defects can be caught at compile time are less prone to runtime failures than interpreted or weakly typed languages.
we discuss implications of these results for developers language designers and educators.
i. i ntroduction what is the best programming language for.
.
.
?
questions about programming languages and the properties of their programs are asked often but well founded answers are not easily available.
from an engineering viewpoint the design of a programming language is the result of multiple tradeoffs that achieve certain desirable properties such as speed at the expense of others such as simplicity .
technical aspects are however hardly ever the only relevant concerns when it comes to choosing a programming language.
factors as heterogeneous as a strong supporting community similarity to other widespread languages or availability of libraries are often instrumental in deciding a language s popularity and how it is used in the wild .
if we want to reliably answer questions about properties of programming languages we have to analyze empirically the artifacts programmers write in those languages.
answers grounded in empirical evidence can be valuable in helping language users and designers make informed choices.
to control for the many factors that may affect the properties of programs some empirical studies of programming languages have performed controlled experiments where human subjects typically students in highly controlled environments solve small programming tasks in different languages.
such controlled experiments provide the most reliable data about the impact of certain programming language features such as syntax and typing but they are also necessarily limited in scope and generalizability by the numberand types of tasks solved and by the use of novice programmers as subjects.
real world programming also develops over far more time than that allotted for short exam like programming assignments and produces programs that change features and improve quality over multiple development iterations.
at the opposite end of the spectrum empirical studies based on analyzing programs in public repositories such as github can count on large amounts of mature code improved by experienced developers over substantial time spans.
such set ups are suitable for studies of defect proneness and code evolution but they also greatly complicate analyses that require directly comparable data across different languages projects in code repositories target disparate categories of software and even those in the same category such as web browsers often differ broadly in features design and style and hence cannot be considered to be implementing minor variants of the same task.
the study presented in this paper explores a middle ground between highly controlled but small programming assignments and large but incomparable software projects programs in rosetta code.
the rosetta code repository collects solutions written in hundreds of different languages to an open collection of over programming tasks .
most tasks are quite detailed descriptions of problems that go beyond simple programming assignments from sorting algorithms to pattern matching and from numerical analysis to gui programming.
solutions to the same task in different languages are thus significant samples of what each programming language can achieve and are directly comparable.
the community of contributors to rosetta code nearly users at the time of writing includes programmers that scrutinize revise and improve each other s solutions.
our study analyzes solution programs to tasks in widely used languages representing the major programming paradigms procedural c and go object oriented c and java functional f and haskell scripting python and ruby .
the study s research questions target various program features including conciseness size of executables running time memory usage and failure proneness.
a quantitative statistical analysis cross checked for consistency against a careful inspection of plotted data reveals the following main findings about the programming languages we analyzed functional and scripting languages enable writing more concise code than procedural and object oriented languages.
languages that compile into bytecode produce smaller executables than those that compile into native machine code.arxiv .0252v4 jan c is hard to beat when it comes to raw speed on large inputs.
go is the runner up also in terms of economical memory usage.
in contrast performance differences between languages shrink over inputs of moderate size where languages with a lightweight runtime may be competitive even if they are interpreted.
compiled strongly typed languages where more defects can be caught at compile time are less prone to runtime failures than interpreted or weakly typed languages.
beyond the findings specific to the programs and programming languages that we analyzed our study paves the way for follow up research that can benefit from the wealth of data in rosetta code and generalize our findings to other domains.
to this end section iv discusses some practical implications of these findings for developers language designers and educators whose choices about programming languages can increasingly rely on a growing fact base built on complementary sources.
the bulk of the paper describes the design of our empirical study section ii and its research questions and overall results section iii .
we refer to a detailed technical report for the complete fine grain details of the measures statistics and plots.
to support repetition and replication studies we also make the complete data available online1 together with the scripts we wrote to produce and analyze it.
ii.
m ethodology a. the rosetta code repository rosetta code is a code repository with a wiki interface.
this study is based on a repository s snapshot taken on june henceforth rosetta code denotes this snapshot.
rosetta code is organized in tasks .
each task is a natural language description of a computational problem or theme such as the bubble sort algorithm or reading the json data format.
contributors can provide solutions to tasks in their favorite programming languages or revise already available solutions.
rosetta code features languages with at least one solution per language for a total of solutions and lines total lines of program files .
a solution consists of a piece of code which ideally should accurately follow a task s description and be self contained including test inputs that is the code should compile and execute in a proper environment without modifications.
tasks significantly differ in the detail prescriptiveness and generality of their descriptions.
the most detailed ones such as bubble sort consist of well defined algorithms described informally and in pseudo code and include tests input output pairs to demonstrate solutions.
other tasks are much vaguer and only give a general theme which may be inapplicable to some languages or admit widely different solutions.
for instance task memory allocation just asks to show how to explicitly allocate and deallocate blocks of memory .
2cloned into our git repository1using a modified version of the perl module rosettacode .
.
available from task selection whereas even vague task descriptions may prompt wellwritten solutions our study requires comparable solutions to clearly defined tasks.
to identify them we categorized tasks based on their description according to whether they are suitable for lines of code analysis loc compilation comp and execution exec tcdenotes the set of tasks in a category c. categories are increasingly restrictive lines ofcode analysis only includes tasks sufficiently well defined that their solutions can be considered minor variants of a unique problem compilation further requires that tasks demand complete solutions rather than sketches or snippets execution further requires that tasks include meaningful inputs and algorithmic components typically as opposed to datastructure and interface definitions .
as table shows many tasks are too vague to be used in the study but the differences between the tasks in the three categories are limited.
all loc comp exec perf scal tasks table classification and selection of rosetta code tasks.
most tasks do not describe sufficiently precise and varied inputs to be usable in an analysis of runtime performance.
for instance some tasks are computationally trivial and hence do not determine measurable resource usage when running others do not give specific inputs to be tested and hence solutions may run on incomparable inputs others still are well defined but their performance without interactive input is immaterial such as in the case of graphic animation tasks.
to identify tasks that can be meaningfully used in analyses of performance we introduced two additional categories perf and scal of tasks suitable for performance comparisons perf describes everyday workloads that are not necessarily very resource intensive but whose descriptions include welldefined inputs that can be consistently used in every solution in contrast scal describes computing intensive workloads with inputs that can easily be scaled up to substantial size and require well engineered solutions.
for example sorting algorithms are computing intensive tasks working on large input lists cholesky matrix decomposition is an everyday task working on two test input matrices that can be decomposed quickly.
the corresponding sets tperf andtscal are disjoint subsets of the execution tasks texec table gives their size.
c. language selection rosetta code includes solutions in languages.
analyzing all of them is not worth the huge effort given that many languages are not used in practice or cover only few tasks.
to find a representative and significant subset we rank languages according to a combination of their rankings in rosetta code and in the tiobe index .
a language s rosetta code ranking is based on the number of tasks for which at least one solution in that language exists the larger the number of tasks the higher the ranking table lists the top languages lang in the rosetta code ranking rosetta with the number of tasks they implement tasks .
the tiobe programming community index is a long standing monthly published language popularity ranking based on hits 2in various search engines table lists the top languages in the tiobe index with their tiobe score tiobe .
rosetta lang tasks tiobe tcl racket python perl ruby j c d go picolisp perl ada mathematica rexx haskell autohotkey java bbc basic icon ocaml table rosetta code ranking top .
tiobe lang tasks rosetta c java objective c c visual basic c php python javascript transact sql perl visual basic .net f ruby actionscript swift delphi object pascal lisp matlab assembly table tiobe index ranking top .
a language must satisfy two criteria to be included in our study c1.
ranks in the top positions in the tiobe index c2.
implements at least one third of the rosetta code tasks.
criterion c1 selects widely used popular languages.
criterion c2 selects languages that can be compared on a substantial number of tasks conducing to statistically significant results.
languages in table that fulfill criterion c1 are shaded the top in tiobe are in bold and so are languages in table that fulfill criterion c2.
a comparison of the two tables indicates that some popular languages are underrepresented in rosetta code such as objective c visual basic and transact sql conversely some languages popular in rosetta code have a low tiobe ranking such as tcl racket and perl .twenty four languages satisfy both criteria.
we assign scores to them based on the following rules r1.
a language receives a tiobe score t iff it is in the top in tiobe otherwise t .
r2.
a language receives a rosetta code score r corresponding to its ranking in rosetta code first column in table .
using these scores languages are ranked in increasing lexicographic order of t r .
this ranking method sticks to the same rationale as c1 prefer popular languages and c2 ensure a statistically significant base for analysis and helps mitigate the role played by languages that are hyped in either the tiobe or the rosetta code ranking.
to cover the most popular programming paradigms we partition languages in four categories procedural objectoriented functional scripting.
two languages r and matlab mainly are special purpose hence we drop them.
in each category we rank languages using our ranking method and pick the top two languages.
table shows the overall ranking the shaded rows contain the eight languages selected for the study.
procedural object oriented functional scripting t r t r t r t r c java f python go c haskell ruby ada c common lisp perl pl i d scala javascript fortran erlang php scheme tcl lua table combined ranking the top languages in each category are selected for the study.
d. experimental setup rosetta code collects solution files by task and language.
the following table details the total size of the data considered in our experiments lines are total lines of program files .
c c f go haskell java python ruby all tasks files lines our experiments measure properties of rosetta code solutions in various dimensions source code features such as lines of code compilation features such as size of executables and runtime features such as execution time .
correspondingly we have to perform the following actions for each solution file fof every task tin each language merge iffdepends on other files for example an application consisting of two classes in two different files make them available in the same location where fis fdenotes the resulting self contained collection of source files that correspond to one solution of tin .
3no rank means that the language is not in the top in the tiobe index.
4not represented in rosetta code.
5only represented in rosetta code in dialect versions.
patch iffhas errors that prevent correct compilation or execution for example a library is used but not imported correct fas needed.
loc measure source code features of f. compile compile finto native code c go and haskell or bytecode c f java python executable denotes the files produced by compilation.6measure compilation features.
run run the executable and measure runtime features.
actions merge and patch are solution specific and are required for the actions that follow.
in contrast loc compile and run are only language specific and produce the actual experimental data.
to automate executing the actions to the extent possible we built a system of scripts that we now describe in some detail.
merge.
we stored the information necessary for this step in the form of makefiles one for every task that requires merging that is such that there is no one to one correspondence between source code files and solutions.
a makefile has one target for every task solution f and a default all target that builds all solution targets for the current task.
each target s recipe calls a placeholder script comp passing to it the list of input files that constitute the solution together with other necessary solution specific compilation files for example library flags for the linker .
we wrote the makefiles after attempting a compilation with default options for all solution files each compiled in isolation we inspected all failed compilation attempts and provided makefiles whenever necessary.
patch.
we stored the information necessary for this step in the form of diffs one for every solution file that requires correction.
we wrote the diffs after attempting a compilation with the makefiles we inspected all failed compilation attempts and wrote diffs whenever necessary.
some corrections could not be expressed as diffs because they involved renaming or splitting files for example some c files include both declarations and definitions but the former should go in separate header files we implemented these corrections by adding shell commands directly in the makefiles.
an important decision was what to patch .
we want to have as many compiled solutions as possible but we also do not want to alter the rosetta code data before measuring it.
we did not fix errors that had to do with functional correctness or very solution specific features.
we did fix simple errors missing library inclusions omitted variable declarations and typos.
these guidelines try to replicate the moves of a user who would like to reuse rosetta code solutions but may not be fluent with the languages.
as a result of following this process we have a reasonably high confidence that patched solutions are correct implementations of the tasks.
diffs play an additional role for tasks for performance analysis tperf andtscal in section ii b .
solutions to these tasks must not only be correct but also run on the same inputs tasks tperf and on the same large inputs tasks tscal .
we checked all solutions to tasks tperf and patched them when necessary to ensure they work on comparable inputs but we 6for ruby which does not produce compiled code of any kind this step is replaced by a syntax check of f.did not change the inputs themselves from those suggested in the task descriptions.
in contrast we inspected all solutions to tasks tscal and patched them by supplying task specific inputs that are computationally demanding.
a significant example of computing intensive tasks were the sorting algorithms which we patched to build and sort large integer arrays generated on the fly using a linear congruential generator function with fixed seed .
the input size was chosen after a few trials so as to be feasible for most languages within a timeout of minutes for example the sorting algorithms deal with arrays of size from 104elements for quadratic time algorithms to 106elements for linear time algorithms.
loc.
for each language we wrote a script locthat inputs a list of files calls cloc7on them to count the lines of code and logs the results.
compile.
for each language we wrote a script compile that inputs a list of files and compilation flags calls the appropriate compiler on them and logs the results.
the following table shows the compiler versions used for each language as well as the optimization flags.
we tried to select a stable compiler version complete with matching standard libraries and the best optimization level among those that are not too aggressive or involve rigid or extreme trade offs.
lang compiler version flags c gcc gnu .
.
o2 c mcs mono .
.
.
.
.
optimize f fsharpc mono .
.
.
o go go .
haskell ghc .
.
o2 java javac oraclejdk .
.0 11 python python cpython .
.
.
.
ruby ruby .
.
c c compile tries to detect the c dialect gnu90 c99 .
.
.
until compilation succeeds.
java compile looks for names of public classes in each source file and renames the files to match the class names as required by the java compiler .
python compile tries to detect the version of python .x or .x until compilation succeeds.
ruby compile only performs a syntax check of the source flag c since ruby has no standard stand alone compilation.
run.
for each language we wrote a script run that inputs an executable name executes it and logs the results.
native executables are executed directly whereas bytecode is executed using the appropriate virtual machines.
to have reliable performance measurements the scripts repeat each execution times discard the timing of the first execution to fairly accommodate bytecode languages that load virtual machines from disk it is only in the first execution that the virtual machine is loaded from disk with corresponding possibly significant one time overhead in the successive executions the virtual machine is read from cache with only limited overhead check that the remaining execution times are within one standard deviation of the mean log the mean execution time.
if an execution does not terminate within a time out of minutes it is forcefully terminated.
overall process.
a python script orchestrates the whole experiment.
for every language for every task t for each action act2floc compile rung if patches exist for any solution of tin apply them if no makefile exists for task tin call script act directly on each solution file foft if a makefile exists invoke it and pass actas command comp to be used the makefile defines the self contained collection of source files fon which the script works.
since the command line interface of the loc compile and runscripts is uniform the same makefiles work as recipes for all actions act.
e. experiments the experiments ran on a ubuntu .
lts 64bit gnu linux box with intel quad core2 cpu at .
ghz and gb of ram.
at the end of the experiments we extracted all logged data for statistical analysis using r. f .
statistical analysis the statistical analysis targets pairwise comparisons between languages.
each comparison uses a different metric m including lines of code conciseness size of the executable native or bytecode cpu time maximum ram usage i.e.
maximum resident set size number of page faults and number of runtime failures.
metrics are normalized as we detail below.
let be a programming language ta task and ma metric.
m t denotes the vector of measures of m one for each solution to task tin language .
m t may be empty if there are no solutions to task tin .
the comparison of languages x andybased on mworks as follows.
consider a subset tof the tasks such that for every t2t both xandyhave at least one solution to t.tmay be further restricted based on a measuredependent criterion for example to check conciseness we may choose to only consider a task tif both xandyhave at least one solution that compiles without errors solutions that do not satisfy the criterion are discarded .
following this procedure each tdetermines two data vectorsxa mandya m for the two languages xandy by aggregating the measures per task using an aggregation function a as aggregation functions we normally consider both minimum and mean.
for each task t2t the t th component of the two vectors xa mandya mis xa m t a xm t nm t x y ya m t a ym t nm t x y where nm t x y is a normalization factor defined as nm t x y min xm t ym t if min xm t ym t otherwise where juxtaposing vectors denotes concatenating them.
thus the normalization factor is the smallest value of metric m measured across all solutions of tinxand in yif such a value is positive otherwise when the minimum is zero the normalization factor is one.
this definition ensures that xa m t and ya m t are well defined even when a minimum of zero occurs due to the limited precision of some measures such as running time.as statistical test we normally8use the wilcoxon signedrank test a paired non parametric difference test which assesses whether the mean ranks of xa mand of ya mdiffer.
we display the test results in a table under column labeled with language xat row labeled with language y and include various measures the p value which estimates the probability that chance can explain the differences between xa mand ya m. intuitively if pis small it means that there is a high chance thatxandyexhibit a genuinely different behavior with respect to metric m. the effect size computed as cohen s d defined as the standardized mean difference d xa m ya m s where vis the mean of a vector v and sis the pooled standard deviation of the data.
for statistically significant differences destimates how large the difference is.
the signed ratio r sgn fxa m fya m max fxa m fya m min fxa m fya m of the largest median to the smallest median where evis the median of a vector v which gives an unstandardized measure of the difference between the two medians.
sign and absolute value of rhave direct interpretations whenever the difference between xandyis significant ifmis such that smaller is better for instance running time then a positive sign sgn fxa m fya m indicates that the average solution in language yisbetter smaller with respect to mthan the average solution in language x the absolute value of rindicates how many times x is larger than yon average.
throughout the paper we will say that language x is significantly different from language y ifp and that it tends to be different from yif p .
we will say that the effect size is vanishing ifd small if d medium if d and large ifd .
g. visualizations of language comparisons each results table is accompanied by a language relationship graph which helps visualize the results of the the pairwise language relationships.
in such graphs nodes correspond to programming languages.
two nodes 1and 2are arranged so that their horizontal distance is roughly proportional to the absolute value of ratio rfor the two languages an exact proportional display is not possible in general as the pairwise ordering of languages may not be a total order.
vertical distances are chosen only to improve readability and carry no meaning.
asolid arrow is drawn from node xtoyif language y is significantly better than language xin the given metric and a dashed arrow if ytends to be better than x using the terminology from section ii f .
to improve the visual layout edges that express an ordered pair that is subsumed by others are omitted that is if x!w!ythe edge from xtoyis 8failure analysis rq5 uses the utest as described there.
9the definition of ruses median as average to lessen the influence of outliers.
5omitted.
the thickness of arrows is proportional to the effect size if the effect is vanishing no arrow is drawn.
iii.
r esults rq1.
which programming languages make for more concise code?
to answer this question we measure the non blank noncomment lines of code of solutions of tasks tlocmarked for lines of code count that compile without errors.
the requirement of successful compilation ensures that only syntactically correct programs are considered to measure conciseness.
to check the impact of this requirement we also compared these results with a measurement including all solutions whether they compile or not obtaining qualitatively similar results.
for all research questions but rq5 we considered both minimum and mean as aggregation functions section ii f .
for brevity the presentation describes results for only one of them typically the minimum .
for lines of code measurements aggregating by minimum means that we consider for each task the shortest solution available in the language.
table shows the results of the pairwise comparison where pis the p value dthe effect size and rthe ratio as described in section ii f. in the table edenotes the smallest positive floating point value representable in r. lang c c f go haskell java python c p0.
d0.
r .
f p e e d0.
.
r .
.
go p0.
.
d0.
.
.
r .
.
.
haskell p e e .
e d1.
.
.
.
r .
.
.
.
java p0.
.
d0.
.
.
.
.
r .
.
.
.
.
python p e e e .
e d0.
.
.
.
.
.
r .
.
.
.
.
.
ruby p e e .
e .
e .
d0.
.
.
.
.
.
.
r .
.
.
.
.
.
.
table comparison of lines of code by minimum .
cc f go haskell java python ruby figure comparison of lines of code by minimum .
figure shows the corresponding language relationship graph remember that arrows point to the more concise languages thickness denotes larger effects and horizontal distances are roughly proportional to average differences.
languages are clearly divided into two groups functional and scripting languages tend to provide the most concise code whereas procedural and object oriented languages are significantly more verbose.
the absolute difference between the two groups is major for instance java programs are onaverage .
.
times longer than programs in functional and scripting languages.
within the two groups differences are less pronounced.
among the scripting languages and among the functional languages no statistically significant differences exist.
python tends to be the most concise even against functional languages .
.
times shorter on average .
among procedural and object oriented languages java tends to be slightly more concise with small to medium effect sizes.
functional and scripting languages provide significantly more concise code than procedural and objectoriented languages.
rq2.
which programming languages compile into smaller executables?
to answer this question we measure the size of the executables of solutions of tasks tcomp marked for compilation that compile without errors.
we consider both native code executables c go and haskell and bytecode executables c f java python .
ruby s standard programming environment does not offer compilation to bytecode and ruby programs are therefore not included in the measurements for rq2.
table shows the results of the statistical analysis and figure the corresponding language relationship graph.
lang c c f go haskell java c p e d .
r .
f p e d .
.
r .
.
go p d .
.
.
r .
.
.
haskell p e d .
.
.
.
r .
.
.
.
java p e e e e d .
.
.
.
.
r .
.
.
.
.
python p e e e e d .
.
.
.
.
.
r .
.
.
.
.
.
table comparison of size of executables by minimum .
cc f go haskelljavapython figure comparison of size of executables by minimum .
it is apparent that measuring executable sizes determines a total order of languages with go producing the largest and python the smallest executables.
based on this order three consecutive groups naturally emerge go and haskell compile to native and have large executables f c java and python compile to bytecode and have small executables c compiles to native but with size close to bytecode executables.
size of bytecode does not differ much across languages 6f c and java executables are on average only .
.
times larger than python s. the differences between sizes of native executables are more spectacular with go s and haskell s being on average .
and .
times larger than c s. this is largely a result of go and haskell using static linking by default as opposed to gcc defaulting to dynamic linking whenever possible.
with dynamic linking c produces very compact binaries which are on average a mere .
times larger than python s bytecode.
c was compiled with level o2optimization which should be a reasonable middle ground binaries tend to be larger under more aggressive speed optimizations and smaller under executable size optimizations flag os .
languages that compile into bytecode have significantly smaller executables than those that compile into native machine code.
rq3.
which programming languages have better runningtime performance?
to answer this question we measure the running time of solutions of tasks tscal marked for running time measurements on computing intensive workloads that run without errors or timeout set to minutes .
as discussed in section ii b and section ii d we manually patched solutions to tasks intscal to ensure that they work on the same inputs of substantial size.
this ensures that as is crucial for running time measurements all solutions used in these experiments run on the very same inputs.
name input billion names of god the integer n anagrams unixdict.txt .
mb arbitrary precision integers combinations count in factors n cut a rectangle rectangle extensible prime generator 107th prime find largest left truncatable prime 107th prime hamming numbers 107th hamming number happy numbers 106th happy number hofstadter q sequence flips up to 105th term knapsack problem from task description ludic numbers from task description lzw compression unixdict.txt .
mb man or boy test n n queens problem n perfect numbers first perfect numbers pythagorean triples perimeter self referential sequence n semordnilap unixdict.txt sequence of non squares non squares sorting algorithms n sorting algorithms n text processing from task description .
mb topswops n towers of hanoi n vampire number from task description table computing intensive tasks.
table summarizes the tasks tscal and their inputs.
it is a diverse collection which spans from text processing tasks on large input files anagrams semordnilap to combinatorial puzzles n queens problem towers of hanoi to np complete problems knapsack problem and sorting algorithms of varying complexity.
we chose inputs sufficiently large to probe the performance of the programs and to make input output overhead negligible w.r.t.
total running time.table shows the results of the statistical analysis and figure the corresponding language relationship graph.
lang c c f go haskell java python c p0.
d0.
r .
f p0.
.
d0.
.
r .
.
go p .
.
d0.
.
.
r .
.
.
haskell p .
.
d0.
.
.
.
r .
.
.
.
java p .
.
.
.
d0.
.
.
.
.
r .
.
.
.
.
python p .
.
.
.
d0.
.
.
.
.
.
r .
.
.
.
.
.
ruby p .
.
.
.
.
d0.
.
.
.
.
.
.
r .
.
.
.
.
.
.
table comparison of running time by minimum for computing intensive tasks.
c c f go haskelljava pythonruby figure comparison of running time by minimum for computing intensive tasks.
c is unchallenged over the computing intensive tasks tscal.
go is the runner up but significantly slower with medium effect size the average go program is .
times slower than the average c program.
programs in other languages are much slower than go programs with medium to large effect size .
.
times slower than go on average .
c is king on computing intensive workloads.
go is the runner up.
other languages with object oriented or functional features incur further performance losses.
the results on the tasks tscal clearly identified the procedural languages c in particular as the fastest.
however the raw speed demonstrated on those tasks represents challenging conditions that are relatively infrequent in the many classes of applications that are not algorithmically intensive.
to find out performance differences under other conditions we measure running time on the tasks tperf which are still clearly defined and run on the same inputs but are not markedly computationally intensive and do not naturally scale to large instances.
examples of such tasks are checksum algorithms luhn s credit card validation string manipulation tasks reversing the space separated words in a string and standard system library accesses securing a temporary file .
the results which we only discuss in the text for brevity are definitely more mixed than those related to tasks tscal which is what one could expect given that we are now looking 7into modest running times in absolute value where every language has at least decent performance.
first of all c loses its undisputed supremacy as it is not significantly faster than go and haskell even though when differences are statistically significant c remains ahead of the other languages.
the procedural languages and haskell collectively emerge as the fastest in tasks tperf none of them sticks out as thefastest because the differences among them are insignificant and may sensitively depend on the tasks that each language implements in rosetta code.
among the other languages c f java python and ruby python emerges as the fastest.
overall we confirm that the distinction between tperf andtscal tasks which we dub everyday and computing intensive is quite important to understand performance differences among languages.
on tasks tperf languages with an agile runtime such as the scripting languages or with natively efficient operations on lists and string such as haskell may turn out to be efficient in practice.
the distinction between everyday and computingintensive workloads is important when assessing running time performance.
on everyday workloads languages may be able to compete successfully regardless of their programming paradigm.
rq4.
which programming languages use memory more efficiently?
to answer this question we measure the maximum ram usage i.e.
maximum resident set size of solutions of tasks tscal marked for comparison on computing intensive tasks that run without errors or timeout this measure includes the memory footprint of the runtime environments.
table shows the results of the statistical analysis and figure the corresponding language relationship graph.
lang c c f go haskell java python c p d2.
r .
f p0.
.
d0.
.
r .
.
go p .
d0.
.
.
r .
.
.
haskell p .
.
d0.
.
.
.
r .
.
.
.
java p .
.
d0.
.
.
.
.
r .
.
.
.
.
python p .
.
.
.
d0.
.
.
.
.
.
r .
.
.
.
.
.
ruby p .
.
.
.
.
d0.
.
.
.
.
.
.
r .
.
.
.
.
.
.
table comparison of maximum ram used by minimum .
c and go clearly emerge as the languages that make the most economical usage of ram.
go s frugal memory usage on average only .
times higher than c is remarkable given that its runtime includes garbage collection.
in contrast all other languages use considerably more memory .
.
times on average over either c or go which is justifiable in light of their bulkier runtimes supporting not only garbage collection but also features such as dynamic binding c andc go c f python rubyjava haskell figure comparison of maximum ram used by minimum .
java lazy evaluation pattern matching haskell and f dynamic typing and reflection python and ruby .
differences between languages in the same category procedural scripting and functional are generally small or insignificant.
the exception are object oriented languages where java uses significantly more ram than c on average .
times more .
among the other languages haskell emerges as the least memory efficient although some differences are insignificant.
while maximum ram usage is a major indication of the efficiency of memory usage modern architectures include many layered memory hierarchies whose influence on performance is multi faceted.
to complement the data about maximum ram and refine our understanding of memory usage we also measured average ram usage and number ofpage faults .
average ram tends to be practically zero in all tasks but very few correspondingly the statistics are inconclusive as they are based on tiny samples.
by contrast the data about page faults clearly partitions the languages in two classes the functional languages trigger significantly more page faults than all other languages in fact the only statistically significant differences are those involving f or haskell whereas programs in other languages hardly ever trigger a single page fault.
the difference in page faults between haskell programs and f programs is insignificant.
the page faults recorded in our experiments indicate that functional languages exhibit significant non locality of reference.
the overall impact of this phenomenon probably depends on a machine s architecture rq3 however showed that functional languages are generally competitive in terms of running time performance so that their non local behavior might just denote a particular instance of the space vs. time trade off.
procedural languages use significantly less memory than other languages.
functional languages make distinctly non local memory accesses.
rq5.
which programming languages are less failure prone?
to answer this question we measure runtime failures of solutions of tasks texec marked for execution that compile without errors or timeout.
we exclude programs that time out because whether a timeout is indicative of failure depends on the task for example interactive applications will time out in our setup waiting for user input but this should not be recorded as failure.
thus a terminating program fails if it returns an exit code other than for example when throwing an uncaught exception .
the measure of failures is ordinal and not normalized fdenotes a vector of binary values one for each solution in language where we measure runtime failures a value in fis iff the corresponding program fails and it is if it does not fail.
data about failures differs from that used to answer the other research questions in that we cannot aggregate it by task since failures in different solutions even for the same task are in general unrelated.
therefore we use the mannwhitney utest an unpaired non parametric ordinal test which can be applied to compare samples of different size.
for two languages xand y the utest assesses whether the two samples xfandyfof binary values representing failures are likely to come from the same population.
c c f go haskell java python ruby ran solutions no error table number of solutions that ran without timeout and their percentage that ran without errors.
table shows the results of the tests we do not report unstandardized measures of difference such as rin the previous tables since they would be uninformative on ordinal data.
figure is the corresponding language relationship graph.
horizontal distances are proportional to the fraction of solutions that run without errors last row of table .
lang c c f go haskell java python c p0.
d0.
f p0.
.
d0.
.
go p .
d0.
.
.
haskell p0.
.
.
.
d0.
.
.
.
java p0.
.
.
d0.
.
.
.
.
python p0.
.
.
.
d0.
.
.
.
.
.
ruby p0.
.
.
.
.
d0.
.
.
.
.
.
.
table comparisons of runtime failure proneness.
goc haskellf c rubyjavapython figure comparisons of runtime failure proneness.
c c f go haskell java python ruby comp.
solutions no error table number of solutions considered for compilation and their percentage that compiled without errors.
go clearly sticks out as the least failure prone language.
if we look in table at the fraction of solutions that failed to compile and hence didn t contribute data to failure analysis go is not significantly different from other compiled languages.
together these two elements indicate that the go compiler is particularly good at catching sources of failures at compile time since only a small fraction of compiled programs fail at runtime.
go s restricted type system no inheritance no overloading no genericity no pointer arithmetic mayhelp make compile time checks effective.
by contrast the scripting languages tend to be the most failure prone of the lot.
this is a consequence of python and ruby being interpreted languages10 any syntactically correct program is executed and hence most errors manifest themselves only at runtime.
there are few major differences among the remaining compiled languages where it is useful to distinguish between weak c and strong the other languages type systems sec.
.
.
.
f shows no statistically significant differences with any of c c and haskell.
c tends to be more failure prone than c and is significantly more failure prone than haskell similarly to the explanation behind the interpreted languages failure proneness c s weak type system may be responsible for fewer failures being caught at compile time than at runtime.
in fact the association between weak typing and failure proneness was also found in other studies .
java is unusual in that it has a strong type system and is compiled but is significantly more error prone than haskell and c which also are strongly typed and compiled.
future work will determine if java s behavior is spurious or indicative of concrete issues.
compiled strongly typed languages are significantly less prone to runtime failures than interpreted or weakly typed languages since more errors are caught at compile time.
thanks to its simple static type system go is the least failure prone language in our study.
iv.
d iscussion the results of our study can help different stakeholders developers language designers and educators to make better informed choices about language usage and design.
the conciseness of functional and scripting programming languages suggests that the characterizing features of these languages such as list comprehensions type polymorphism dynamic typing and extensive support for reflection and list and map data structures provide for great expressiveness.
in times where more and more languages combine elements belonging to different paradigms language designers can focus on these features to improve the expressiveness and raise the level of abstraction.
for programmers using a programming language that makes for concise code can help write software with fewer bugs.
in fact some classic research suggests that bug density is largely constant across programming languages all else being equal therefore shorter programs will tend to have fewer bugs.
the results about executable size are an instance of the ubiquitous space vs. time trade off.
languages that compile to native can perform more aggressive compile time optimizations since they produce code that is very close to the actual hardware it will be executed on.
in fact compilers to native tend to have several optimization options which exercise different trade offs.
gnu s gcc for instance has a os flag that optimizes for executable size instead of speed but we didn t use this highly specialized optimization in our experiments .
however with the ever increasing availability of cheap and compact memory differences between languages have 10even if python compiles to bytecode the translation process only performs syntactic checks and is not invoked separately normally anyway .
9significant implications only for applications that run on highly constrained hardware such as embedded devices where in fact bytecode languages are becoming increasingly common .
interpreted languages such as ruby exercise yet another tradeoff where there is no visible binary at all and all optimizations are done at runtime.
no one will be surprised by our results that c dominates other languages in terms of raw speed and efficient memory usage.
major progresses in compiler technology notwithstanding higher level programming languages do incur a noticeable performance loss to accommodate features such as automatic memory management or dynamic typing in their runtimes.
nevertheless our results on everyday workloads showed that pretty much any language can be competitive when it comes to the regular size inputs that make up the overwhelming majority of programs.
when teaching and developing software we should then remember that most applications do not actually need better performance than python offers .
another interesting lesson emerging from our performance measurements is how go achieves respectable running times as well as good results in memory usage thereby distinguishing itself from the pack just as c does in fact go s developers include prominent figures ken thompson most notably who were also primarily involved in the development of c .
go s design choices may be traced back to a careful selection of features that differentiates it from most other language designs which tend to be more feature prodigal while it offers automatic memory management and some dynamic typing it deliberately omits genericity and inheritance and offers only a limited support for exceptions.
in our study we have seen that go features not only good performance but also a compiler that is quite effective at finding errors at compile time rather than leaving them to leak into runtime failures.
besides being appealing for certain kinds of software development go s concurrency mechanisms which we didn t consider in this study may be another feature to consider go also shows to language designers that there still is uncharted territory in the programming language landscape and innovative solutions could be discovered that are germane to requirements in certain special domains.
evidence in our as well as others section vi analysis tends to confirm what advocates of static strong typing have long claimed that it makes it possible to catch more errors earlier at compile time.
but the question remains of which process leads to overall higher programmer productivity or in a different context to effective learning postponing testing and catching as many errors as possible at compile time or running a prototype as soon as possible while frequently going back to fixing and refactoring?
the traditional knowledge that bugs are more expensive to fix the later they are detected is not an argument against the test early approach since testing early may be the quickest way to find an error in the first place.
this is another area where new trade offs can be explored by selectively or flexibly combining featuresthat enhance compilation or execution.
v. t hreats to validity threats to construct validity are we measuring the right things?
are quite limited given that our research questions and the measures we take to answer them target widespread well defined features conciseness performance and so on with straightforward matching measures lines of code running time and so on .
a partial exception is rq5 which targets the multifaceted notion of failure proneness but the question and its answer are consistent with related empirical work that approached the same theme from other angles which reflects positively on the soundness of our constructs.
regarding conciseness lines of code remains a widely used metric but it will be interesting to correlate it with other proposed measures of conciseness.
we took great care in the study s design and execution to minimize threats to internal validity are we measuring things right?
we manually inspected all task descriptions to ensure that the study only includes well defined tasks and comparable solutions.
we also manually inspected and modified whenever necessary all solutions used to measure performance where it is of paramount importance that the same inputs be applied in every case.
to ensure reliable runtime measures running time memory usage and so on we ran every executable multiple times checked that each repeated run s deviation from the average is moderate less than one standard deviation and based our statistics on the average mean behavior.
data analysis often showed highly statistically significant results which also reflects favorably on the soundness of the study s data.
our experimental setup tried to use standard tools with default settings this may limit the scope of our findings but also helps reduce biasdue to different familiarity with different languages.
exploring different directions such as pursuing the best optimizations possible in each language for each task is an interesting goal of future work.
a possible threat to external validity do the findings generalize?
has to do with whether the properties of rosetta code programs are representative of real world software projects.
on one hand rosetta code tasks tend to favor algorithmic problems and solutions are quite small on average compared to any realistic application or library.
on the other hand every large project is likely to include a small set of core functionalities whose quality performance and reliability significantly influences the whole system s rosetta code programs can be indicative of such core functionalities.
in addition measures of performance are meaningful only on comparable implementations of algorithmic tasks and hence rosetta code s algorithmic bias helped provide a solid base for comparison of this aspect section ii b and rq3 .
the size and level of activity of the rosetta code community mitigates the threat that contributors to rosetta code are not representative of the skills and expertise of real world programmers.
however to ensure wider generalizability we plan to analyze other characteristics of the rosetta code programming community.
to sum up while some quantitative details of our results may vary on different codebases and methodologies the big mainly qualitative picture is likely robust.
another potential threat comes from the choice of programming languages.
section ii c describes how we selected languages representative of real world popularity among major paradigms.
classifying programming languages into paradigms has become harder in recent times when multi paradigm languages are the norm many programming languages offer 10procedures some form of object system and even functional features such as closures and list comprehensions .
11nonetheless we maintain that paradigms still significantly influence the way in which programs are written and it is natural to associate major programming languages to a specific paradigm based on their rosetta code programs.
for example even though python offers classes and other object oriented features practically no solutions in rosetta code use them.
extending the study to more languages and new paradigms belongs to future work.
vi.
r elated work controlled experiments are a popular approach to language comparisons study participants program the same tasks in different languages while researchers measure features such as code size and execution or development time.
prechelt compares programming languages on a single task in solutions written by studentsand other volunteers.
measures include program size execution time memory consumption and development time.
findings include the program written in perl python rexx or tcl is only half as long as written in c c or java performance results are more mixed but c and c are generally faster than java.
the study asks questions similar to ours but is limited by the small sample size.
languages and their compilers have evolved since when was published making the results difficult to compare however some tendencies conciseness of scripting languages performance dominance of c are visible in our study too.
harrison et al.
compare the code quality of c against the functional language sml s on tasks finding few significant differences.
our study targets a broader set of research questions only rq5 is related to quality .
hanenberg conducts a study with students over hours of development time comparing static vs. dynamic type systems finding no significant differences.
in contrast to controlled experiments our approach cannot take development time into account.
many recent comparative studies have targeted programming languages for concurrency and parallelism.
studying students on a single problem szafron and schaeffer identify a message passing library that is somewhat superior to higher level parallel programming even though the latter is more usable overall.
this highlights the difficulty of reconciling results of different metrics.
we do not attempt this in our study as the suitability of a language for certain projects may depend on external factorsthat assign different weights to different metrics.
other studies compare parallel programming approaches upc mpi openmp and x10 using mostly small student populations.
in the realm of concurrent programming a study with undergraduate students implementing one program with locks monitors or transactions suggests that transactions leads to the fewest errors.
in a usability study with students we find advantages of the scoop concurrency model over java s monitors.
pankratius et al.
compare scala and java using students and one software engineer working 11at the laser summer school on innovative languages for software engineering mehdi jazayeri mentioned the proliferation of multi paradigm languages as a disincentive to updating his book on programming language concepts .on three tasks.
they conclude that scala s functional style leads to more compact code and comparable performance.
to eschew the limitations of classroom studies based on the unrepresentative performance of novice programmers for instance in about a third of the student subjects fail the parallel programming task in that they cannot achieve any speedup previous work of ours compared chapel cilk go and tbb on solutions to tasks that were checked for style and performance by notable language experts.
also introduced language dependency diagrams similar to those used in the present paper.
a common problem with all the aforementioned studies is that they often target few tasks and solutions and therefore fail to achieve statistical significance or generalizability.
the large sample size in our study minimizes these problems.
surveys can help characterize the perception of programming languages.
meyerovich and rabkin study the reasons behind language adoption.
one key finding is that the intrinsic features of a language such as reliability are less important for adoption when compared to extrinsic ones such as existing code open source libraries and previous experience.
this puts our study into perspective and shows that some features we investigate are very important to developers e.g.
performanceas second most important attribute .
bissyand et al.
study similar questions the popularity interoperability and impact of languages.
their rankings according to lines of code or usage in projects may suggest alternatives to the tiobe ranking we usedfor selecting languages.
repository mining as we have done in this study has become a customary approach to answering a variety of questions about programming languages.
bhattacharya and neamtiu study projects in c and c to understand the impact on software quality finding an advantage in c .
with similar goals ray et al.
mine projects in languages from github.
they find that strong typing is modestly better than weak typing and functional languages have an advantage over procedural languages.
our study looks at a broader spectrum of research questions in a more controlled environment but our results on failures rq5 confirm the superiority of statically strongly typed languages.
other studies investigate specialized features of programming languages.
for example recent studies by us and others investigate the use of contracts and their interplay with other language features such as inheritance.
okur and dig analyze open source applications with parallel programming to identify adoption trends and usage problems addressing questions that are orthogonal to ours.
vii.
c onclusions programming languages are essential tools for the working computer scientist and it is no surprise that what is the right tool for the job can be the subject of intense debates.
to put such debates on strong foundations we must understand how features of different languages relate to each other.
our study revealed differences regarding some of the most frequently discussed language features conciseness performance failureproneness and is therefore of value to software developers and language designers.
the key to having highly significant statistical results in our study was the use of a large program 11chrestomathy rosetta code.
the repository can be a valuable resource also for future programming language research that corroborates or otherwise complements our findings.
besides using rosetta code researchers can also improve it by correcting any detected errors and can increase its research value by maintaining easily accessible up to date statistics .