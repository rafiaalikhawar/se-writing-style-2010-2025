deep learning code fragments for code clone detection martin white michele tufano christopher vendome and denys poshyvanyk department of computer science college of william and mary williamsburg virginia usa mgwhite mtufano cvendome denys cs.wm.edu abstract code clone detection is an important problem for software maintenance and evolution.
many approaches consider either structure or identifiers but none of the existing detection techniques model bothsources of information.
these techniques also depend on generic handcrafted features to represent code fragments.
we introduce learning based detection techniques where everything for representing terms and fragments in source code is mined from the repository.
our code analysis supports a framework which relies on deep learning for automatically linking patterns mined at the lexical level with patterns mined at the syntactic level.
we evaluated our novel learning based approach for code clone detection with respect to feasibility from the point of view of software maintainers.
we sampled and manually evaluated file and method level pairs across eight real world java systems of the file and method level samples were evaluated to be true positives.
among the true positives we found pairs mapping to all four clone types.
we compared our approach to a traditional structure oriented technique and found that our learning based approach detected clones that were either undetected or suboptimally reported by the prominent tool deckard.
our results affirm that our learning based approach is suitable for clone detection and a tenable technique for researchers.
ccs concepts software and its engineering reusability keywords code clone detection machine learning deep learning neural networks language models abstract syntax trees .
introduction abstraction is the most important word in software engineering se .accordingly softwarerepositoriesarereplete with abstractions which give software engineers the ability permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c circlecopyrt2016 acm.
isbn .
.
.
.
manage complexity by separating concerns and handling different details at different levels.
abstractions at all level s of granularity are complemented by implementations.
these implementations can be developed from scratch or they can beclonedfrom existing code fragments .
if existing code provides a reasonable starting point for the implementation then a software engineer may clone the code by copying and pasting the fragment.
another way that clones can be introduced in a software system is when an engineer unknowingly develops an implementation that is similar to an existing one.
copying and pasting code and subsequently modifying the copied fragment may yield textually similar code fragments where the similarities can be characterized by their syntax.
on the other hand when an engineer unknowingly develops an implementation that is similar in intent to some thing that already exists she may create clones that are functionally similar yet syntactically different.
detecting clones is an important problem for software maintenanceandevolution.althoughpriorworkhasdemonstrated several adverse impacts of code cloning cloning is not necessarily harmful .
nor should clones necessarily be refactored .
nonetheless the ability to automatically detect that two fragments are similar is critical in many applications e.g.
detecting library candidates aiding program comprehension detecting malicious software detecting plagiarism or copyright infringement detecting context based inconsistencies and searching for refactoring opportunities .
roy and cordy classified clone detection techniques by their internal source code representation synthesizing a taxonomy of text token tree graph and metrics based techniques.
in this paper our newfangled approach to mining internal source code representations gives way to a new learning based paradigm.
the clone detection process begins by transforming in situ code into representations suitable for assessing similarity .
for instance to represent fragments traditional tree based clone detection tools depend on handcrafted features that are tightly coupled to generic programming constructs.
in this respect the domain information that is rooted in identifiers is discarded breaking the link between information that can be learned at boththe lexical level and syntactic level.
moreover declaring features e.g.
the occurrence counts of programming constructs applies a great deal of prior knowledge to how we can automatically represent fragments.
however it is reasonable to expect that software systems from different application domains and at different stages of development yielduniquepatterns in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
source code that would be revealing for problems like code clone detection.
yet these patterns are not necessarily captured using approaches that establish a generic feature spa ce andtheonlywaytheseuseful latentfeaturescanbedescried is by using perspectives of code that are learned i.e.
learning the representations themselves.
automatically learning the representations or representation learning relaxes the prior knowledge used to transform raw data like source code into suitable representations automating what has been a manual step in the detection process.
mining effective source code features analyzing the language of identifiers in source code analyzing syntactic patterns and engineering approaches that can adapt to changing repositories are fundamental se research problems.
engineering a clone detection approach that considers all of these concerns is what motivates our work.
our key result is a new set of techniques that fuseanduse.
we fuse information on structure and identifiers in code and use the data in repositories to automate the step of specifying transformations.
our key insight to representing code fragments for code clone detection is two fold.
first our approach maps the terms in fragments to continuous valued vectors such that terms used in similar ways in the source code repository map to similar vectors sec.
.
.
this transformation from terms to vectors is fundamentally different than the token abstraction used by token based techniques sec.
.
.
second our representation learning based approach is designed to learn discriminating features for fragments at different levels of granularity sec.
.
rather than depend on intuitive yet limited features that are designed around the structural elements of a language like tree based techniques sec.
.
.
the essence of our approach goes back to abstraction and handling different details at different levels in se.
we propose exploiting this guiding principle in software construction so our techniques for modeling source code exploit empirically basedpatternsin structures oftermsincodejust as language modeling has exploited patterns in sequences of terms.
to this end we pair lexical analysis with recurrent neural networks sec.
.
and syntactic analysis with recursive neural networks sec.
.
.
the purpose of coupling the front end of the compiler with deep neural networks and deep learning sec.
.
is to provide a framework for linking patterns mined at the lexical level by modeling how terms are used with patterns mined at the syntactic level by modeling how fragments are composed .
clone detection is one important application of this framework.
.
background and related work acode fragment orfragment isacontiguoussegment ofsourcecode specifiedbythesourcefileandthelineswhere the segment begins and ends .
code clones orclones are two or more fragments that are similarwith respect to aclone type .
acandidate is a clone pair reported by a clone detector .
we introduce learning based detection techniques where everything for representing terms and fragments is mined from the source code repository.
indeed our approach aims to move clone detection from the art of feature engineering to the science of automated discovery.to substantiate our progress against this goal consider the following difference.
the related work sec.
.
uses handcrafted feature vectors to represent fragments.
in our work sec.
this handcrafting is supplanted by methods for automatically discovering empirically based features.this supplantation is evidenced by the fact that feature vectors in traditional approaches generally lend themselves to interpretation.
a feature may correspond to the occurrence count of a programming construct in a tree based technique or a measure of central tendency in a metrics based technique etc.
alternatively our feature vectors do notlend themselves to interpretation.
why?
we use a special type of machine learning sec.
.
that shifts our clone detection approachfromanimperativestyle here is how i want to represent fragments to a declarative style here is what i want to represent .
hence our work does not replace existing techniques but rather provides a completely new perspective.
.
code clone detection generally there are four clone types.
type i identical fragments except for variations in comments and layout .
type ii identical fragments except for variations in identifier names and literal values in addition to type i differences .
type iii syntactically similar fragments that differ at the statement level.
the fragments have statements added modified or removed with respect to each other in addition to type ii differences .
type iv syntactically dissimilar fragments that implement the same functionality .
type i ii and iii clones indicate textual similarity whereas type iv clones indicate functional similarity.
recall that detection techniques generally begin by representing code before measuring similarity and these techniques can be classified by their source code representation.
text based techniques apply slight transformationstocodeandmeasuresimilaritybycomparingsequences of text.
consequently text based techniques are limited in their ability to recognize two fragments as a clone pair even if the difference between them is as inconsequential as a systematic renaming of identifiers.
token based techniques mollify the scrupulous text based rule by operating at a higher level of abstraction.thesetechniqueslexicallyanalyzethecodetopro duce a stream of tokens and compare subsequences to detect clones.
matching subsequences of tokens generally improves the recognition power but the token abstraction has a tendency to admit more false positives .
our learning based approach differs from token based techniques in at least two ways.
first the token abstraction maps each term to a discrete class whicheffectively binstheterms whereasourapproach maps terms to continuous valued vectors in a feature space where similarities are encoded as distances.
second our approach incorporates context e.g.
syntax beyond the token abstraction as tree based techniques do.
tree based techniques measure the similarity of subtrees in syntactic representations.
our primary related workistheinfluentialworkbyjiangetal.
whopresented deckard which transforms parse trees into characteristic vectors andclusterssimilarvectors usinglocalitysensiti ve hashing to detect clones.
we use abstract syntax trees asts rather than parse trees and while deckard distinguishes between relevant and irrelevant nodes we regard every nonempty node in an ast as relevant.
designating a subset of nodes as relevant amounts to handcrafting an abstraction for fragments.
each component of a characteristic vector represents the occurrence count of relevant nodes in the corresponding subtree so the vector s dimension is the number of tree patterns deemed relevant to approximate a given tree .
this feature engineering represents a funda88mental point of divergence in our work where we learn discriminating features from the data as opposed to declare a priori a number of specific features.
moreover characteristic vectors approximate structural information while neglecting domain information rooted in identifiers .
in fact there is generally no special treatment for identifiers and literal types in ast based approaches .
our work operates on identifiers and literal types.
graph based techniques use static program analysis to transform code into a program dependence graph pdg an intermediate representation of data and control dependencies .
gabel et al.
augmented deckard withsemanticinformationderivedfrompdgs theymapped subgraphs to related structured syntax defining significant nodes to be those that descend from the parent statement class and then detected clones using deckard.
chen et al.
used a geometry characteristic of dependency graphs to measure methods similarities before combining methodlevelsimilaritiestodetectapplicationclonesinandroidma rkets.
they began by extracting methods from android application packages and transforming each method to a control flow graph cfg .
they compute a centroid for each method by mapping every cfg node to a three dimensional point where the dimensions represent structures.
our work uses more resolution by operating on ast nodes rather than basic blocks.
they only use the cfg part of the pdg whereas our approach is designed to learn models of how terms are composed at any level of granularity.
by mapping methods in a three dimensional space of engineered features chen et al.
place an extraordinarily strong prior on the source code representation .
they imply the definition of the centroid can be extended so the centroid can be impacted by the invokestatement but this augmentation constitutes more feature engineering designed to improvetheperformanceforthespecificapplicationofandroid app clone detection.
our unique approach obviates the need to engineer this kind of feature since handcrafting is timeconsuming and limited .
other detection approaches include the following.
davey et al.
ignored identifiers and operators and instead considered the frequency of keywords indentation pattern and length of each line to represent fragments.
these feature vectors were passed to a self organizing map to detect clones.
marcus and maletic examined identifiers and comments to identify implementations of similar high level concepts .
nguyen et al.
extracted structural features from generic graph based representations to build characteristic vectors.leeetal.
measuredstructuralsimilaritieslikedeckard and proposed a multidimensional indexing structure to support fast inference.
kim et al.
proposed a semantic detectiontechniquethatcomparedprograms abstractmemory states.
hermans et al.
proposed a text based algorithmfordetectingclonesinspreadsheets.finally jiangand su presented eqminer a novel approach to identifying functionally equivalent fragments.
eqminer runs fragments with random inputs and defines functional equivalence in terms of i o behavior.
our work aims to detect both textual and functional similarity at compile time.
.
language modeling our approach is based in part on language models.
a statisticallanguage model is a probability distribution over sentences in a language .
traditionally statistical la n guage models have been effective abstractions for natural language processing nlp tasks.
recently their effectiveness has suffused se tasks such as code suggestion deriving readable string test inputs to reduce human oracle cost predicting comments to improve search over code bases and code categorization improving error reporting generating feasible test cases to improve coverage improving stylistic consistency to aid readabili ty and maintainability code migration synthesi zing api completions code review fault localization and suggesting method and class names .
a statistical language model is a tractable representation of sentences e.g.
lines of code or traces of method invocations in a language.
tractability is realized by decomposing a joint distribution and analyzing probabilistic automata e.g.
n grams p s producttextm i 1p wi wi producttextm i 1p wi wi i n .
however n grams suffer apparent limitations .
for example since they are simply smoothed counts of term co occurrences they are limited in their ability to generalize beyond the explicit features observed in training .
they are also limited by the amount of context they consider yet the novel approach of casting application s in terms of naturalness in se contexts has spawned many applications of this technology to new problems.
we enlist a particular type of language model to map the terms in source code to continuous valued vectors called embeddings sec.
.
.
to the best of our knowledge we are the first to propose language models and embeddings for clone detection .
.
deep learning compositional learning algorithms typify deep learning a nascent field of machine learning.
these stateof the art learning algorithms have seeded new approaches in computer vision and nlp.
nlp in particular has realized substantial improvements applying deep learners e.g.
the recurrent neural network rtnn to corpora.
our prior work considered the limitations of n grams and aimed toimprovetherepresentationpoweroftheabstractions e.g.
software language models we use in se research by examining rtnns on software corpora for a code suggestion engine .
while rtnns are powerful architectures for modeling sequences of terms their generalization the recursive neural network rvnn is capable of modeling arbitrary structures to for instance predict the sentiment of natural language sentences .
in our work we cast clone detection as a recursive learning procedure designed toadequately represent fragments that serve as constituents of higher order components.
of course recursive learning is inherently compositional which gives way ipso facto to deep learning.
hence the purpose of deep learning in this new application is to synchronize the source code representation that we use in the clone detection process with the manner in which the code is conceptually organized.
to the best of our knowledge we are the first to propose a deep compositional learning based detection approach capabl e of inducing representations at different levels of granularit y. .
deep learning code fragments in this section we specify our learning based approach in two parts.
the first part sec.
.
describes how we use a particular type of language model an rtnn to map each term in a fragment to an embedding.
we rely on related work from the nlp community and se lit89lexical system syntactic combine methodrtnn rvnn ast2bin embeddings match clonesgreedyast based figure coupling deep learners to front end stages erature for some of the technical details behind training and evaluating these models.
the second part sec.
.
describes how we use the language s grammar and a recursive learning procedure implemented as an rvnn to encode arbitrarily long sequences of embeddings to characterize fragments.
we are not going to specify the features for modeling these fragments at different levels of granularity the purpose of using deep learning is to automate this manual step .
.
deep learning code lexical level an rtnn is a deep learner that is well suited for modeling sequences of terms in a source code corpus with vocabulary vwhere v mterms.
let n a user specified hyperparameter be the number of hidden units.
an rtnncomprisesaninputlayer x rm n ahiddenlayer z rn and an output layer y rm assuming away heuristics such as class based output layers .
adjusting nregulates the model s capacity .
the depth of an rtnn is attributed to the recurrence where the hidden state is copied back to the input layer so the input layer in an rtnn agglutinates the current term t i and the previous state z i x i thisinputvectorismultipliedbyamatrix rn m n and passed to a nonlinear vector function f i.e.
z i f t i z i this state vector is multiplied by another matrix rm n and normalized to compute a posterior over terms y i p t x i softmax z i eq.
specify an rtnn.
eq.
highlights its depth by making its composition a bit more explicit to show how its outputis a highly nonlinear function of its previous inputs y i softmax f t i f t i the model is trained using a cross entropy criterion but we omit the technical details here .
in software language modeling the model s output y i can be used to predict the next term in a line of code as argmaxkyk i .
however deep learners are not simply useful for their output their internal components are useful too.
in this work the most important component of rtnnbased software language models is the matrix of embeddings int foo figure rtnn.
white nodes are one hot term vectors black nodes are continuous valued states gray nodes are posterior distributions.
we extract the matrix of embeddings represented by the red arc.
rn min eq.
.
each column of corresponds to a term.
the column space of comprises semantic representations for every term in vsuch that the model imputes similar vectors to terms used in similar ways in the corpus .
given that each term is one hot encoded when presented to the model the matrix vector product t in eq.
amounts to mapping any term in vto a column in thereby mapping sequences of terms in fragments to sequences of embeddings.
thus to represent fragments we encode arbitrarily long sequences of embeddings.
.
deep learning code syntactic level our learning based archetype diverges from traditional techniques.
given a fragment information will flow up from the terminal nodes through the nonterminal nodes to the root of a hierarchical structure fig.
.
this bottom up flowofinformationisliketheproceduresforcomputingcharacteristicvectorsintraditionalstructure orientedtechniques or computing metrics in metrics based techniques.
however we mine vector representations for terminal nodes sec.
.
and the features for nonterminal nodes are not indicatorbased occurrence counts sec.
.
.
the feature space is inducedbylearningtodiscriminatefragments sec.
.
.
.furthermore after information is synthesized in a bottom up traversal to compute characteristic vectors or metrics traditional techniques terminate and pass the source code representations to a match detection algorithm to find similar fragments.
in a way we regard the bottom up flow of information as necessary but not sufficient to adequately represent fragments.
hence our termination condition is fundamentally different.
in our approach the procedure for mining representations terminates when the model has converged to a solution such that it can adequately represent programming constructs at different levels of granularity eq.
.
this criterion where information at the lexical level is transmitted from terminals to a structure s root and a supervised signal is broadcasted from the root back through the structure lies at the heart of our approach.
.
.
from asts to full binary trees the front end of a compiler decomposes a program into constituents and produces intermediate code according to the syntax of the language .
these constituents are calle d programming constructs and a context free grammar specifies the syntax of programming constructs .
the ast is one type of intermediate code that represents the hierarchical syntactic structure of a program .
ultimately our goal is to specify learning based techniques for encoding arbitrarily long sequences of lexical elements.
since the nonterminal nodes in asts subsume sequences of lexical elements suppose each ast node has a special attribute 90reprthat stores a vector representation a code1that characterizes the node and by extension the sequence of lexical elements the node subsumes.
we mine the codes in such a way that similar sequences have similar codes.
one learningbased technique is based on the ast a tree representation that can have an arbitrary number of levels comprising nodes with an arbitrary number of children but herein lies the problem.
our learner only accepts fixed size inputs sec.
.
.
so we transform the ast to a full binary tree to fix the size of the input and we apply the learner recursively to model the structure at different levels.
the degree of an ast node is either zero one two or greater than two.
by definition ast nodes with degree zero or two satisfy the property of nodes in a full binary tree but subtrees rooted at nodes with degree one case i or greater than two case ii must be transformed in order to refashion the local subtree into a full binary tree.
the first step of our transformation is to scan the ast and delete metadata e.g.
javadoc nodes in asts for java fragments as well as nodes for empty anonymous class declarations empty array initializers empty blocks empty classes empty compilation units and empty statements.
as we scan the ast for empty nodes we also look for sequences of identical literal types with the same parent.
the learner will encode pairwise combinations of ast nodes therefore we avoid encoding pairs of the same literal type by visiting nonterminal nodes inspecting their children and collapsing ad jacent identical literal types to one instance.
for example truetrue truetruetrue etc.
all become true.
collapsing these sequences also helps control the depth of the binary tree at the risk of losing some resolution.
next to obtain a binary tree subtrees rooted at case ii nodes i.e.
nodes with degree greater than two need to be reorganized so the children are suitably arranged.
we defined a grammar based approach for each nonterminal type to systematically reorganize the children of case ii nodes.
for example ifstatement instances can have either twoorthreechildren.forthisnonterminaltype wedefineda new grammar that only produces binary subtrees assuming away the syntax of expression and statement nodes since every production body has either one or two constructs.
to doso weaugmentedthelanguage sgrammarbyintroducing new artificial nonterminal types such as branches an bracketle tifstatement an bracketri ht an bracketle texpression an bracketri ht an bracketle tbranches an bracketri ht an bracketle tbranches an bracketri ht an bracketle tstatement an bracketri ht for nonterminal types with arbitrary maximum degree e.g.
block nodes we organized their children into binary lists.
since the children of block nodes are represented by a sequence of statements we replaced the original production an bracketle tblock an bracketri ht an bracketle tstatement an bracketri ht with a new production where block nodes can have the form of a recursive list of statements an bracketle tblock an bracketri ht an bracketle tstatementlist an bracketri ht an bracketle tstatementlist an bracketri ht an bracketle tstatement an bracketri ht after we transform each case ii instance using the new grammar weobtainabinarytreefromtheoriginalast but 1we use codeto refer to source code intermediate code and representations.
the context will always disambiguate the term.the binary tree may or may not be a fullbinary tree since nodes may have one and only one child.
in other words we need to handle case i nodes i.e.
nodes with degree one .
we traverse the binary tree in a top down manner and when we reach a case i node we merge the node and its child into one node.
then we recursively continue the transit from the new merged node.
the top down visit ensures that instances of parent nodes with one and only one child are eventually merged into one node.
our merging procedure is governed by a precedence list that assigns a value to each nonterminal type.
when merging two nodes the precedence value is used to decide whether to assign the current node type or the child type to the new node.
table precedence typedeclaration methoddeclaration othertype expressionstatement qualifiedname simpletype simplename parenthesizedexpression block artificialtypetab.
shows the precedence list we defined where types higher in the list have higher precedence.
when two nodes have the same precedence value which may be the case with two othertypenodes the merge keeps the parent node.
this design decision comes from the observation that the parent node is typically more expressive and representative of the programming construct than the child node.
we determined the list upon several empirical observations.
in particular with this order we ensure the following.
certain levels of granularity are protected and never overwritten by other nodes.
when merging two nodes more expressive types are preferred over more general types such as parenthesizedexpression and block.
artificial nonterminal nodes created in the previous step to handle case ii nodes will never replace nonterminal types in the original grammar.
the implications for protecting certain levels of granularity are apparent in se applications such as clone detection where for example our approach is capable of representing and thereby reporting clones at well defined abstraction boundaries to better support software maintainers.
.
.
from full binary trees to olive trees now we describe how we transform a full binary tree to what we informally call an olive tree which is the result of converting intermediate code to a full binary tree and then annotating this tree with mined representations.
consider the statement int foo .
the ast for this statement is already a full binary tree depicted in fig.
.
suppose again that each ast node has a special attribute repr e.g.
.repr stores the representation for the simplename in fig.
.
we initialize this attribute for each terminal by using its lexical element to select the corresponding column in the matrix of embeddings .
for example if the lexical element intmaps to the jth column of then repr for the primitivetype in fig.
is initialized such that .repr j. this attribute is initialized to null for nonterminal nodes such as the variabledeclarationfragment and the variabledeclarationstatement in fig.
.
at this juncture we have used patterns mined at the lexical level sec.
.
to initialize a sequence of embeddings.
next we use an autoencoder to combine embeddings.
the canonical 9112345 2 3 l l r r l r int foo figure ast based123445 l r r l int foo figure greedy form of an autoencoder is a neural network with one input layerx one hidden layer z and one output layer y z g x z y h z y where rn 2nis the ncoder r2n nis the ecoder z rnand y r2nare iases.
the tie that binds patterns mined at the lexical level with patterns mined at the syntactic level is n which is the same nthat governed the size of the hidden layer zin eq.
.
g is a nonlinear vector function and his typically the identity function.
in sec.
.
.
we claimed that our learner only accepts fixed size inputs which prompted the transformation of asts to full binary trees.
concretely the input to the autoencoder is a vector of two sibling nodes codes i.e.
x r2n.
for example to compute the representation for the variabledeclarationfragment in fig.
we would present x to the model.
constricting the size of the hidden layer i.e.
z n 2n coerces the model into learning a compressed representation of its input.
this compression zin eq.
serves as the mined representation that we store in the nonterminal node s repr attribute.
essentially the model embeds the input in a lower dimensional feature space just as the language model embedded one hot term vectors sec.
.
.
in other words the language model transforms lexical elements to embeddings and the autoencoder compresses any two embeddings to a vector with the same dimensions as a term embedding.
the output y r2nis referred to as the model s reconstruction of the input.
training the model involves measuring the distance between the original input vector and the reconstruction e xl xr z y xl xl xr xr if the model can effectively learn discriminating features of the input then it will be able to generalize and faithfully reconstruct anyinput vector sampled from the domain.
we just demonstrated how conventional autoencoders can compress modest sequences of two lexical elements but to supportclonedetection welearncodesfor much more .since the code for every node in the tree has the same size we can apply the autoencoder recursively an rvnn to model the full binary tree at different levels.
the autoencoder that we used to compress the simplename and numberliteral in fig.
can be applied recursively insofar as the code for the variabledeclarationfragment is coalesced with the code for the primitivetype and presented to the same model to compute the code for the variabledeclarationstatement .repr g z .
as before to train the model we decode the representation i.e.
y h y and compare the reconstruction to the input i.e.
x to adjust the weights.
but now the error is a weighted sumof all reconstruction errors where larger programming constructs will have more influence on shaping the representation for the fragment.
for example the variabledeclarationfragment has a greater influence on tuning .repr than the primitivetype .
after computing the code for each nonterminal node in a forward pass the backpropagation through structure algorithm computes partial derivatives of the global error function with respect to the model s components.
then the error signal is optimized using standard methods.
once the deep learner has converged after a number of epochs we inlay the full binary tree with the representations to produce an olive tree.
why is deep learning a good approach for clone detection?
techniques that analyze identifiers generally use latent semantic analysis lsa .
deep learning has three apparent advantages over lsa.
first autoencoders are nonlinear dimensionality reducers.
second recursively applying an autoencoder operates on input with severalnonlinear transformations as opposed to using one linear decomposition of the input.
third the recursion considers the orderof terms.
on the other hand techniques that analyze structure discard identifiers whichweuseaspriorknowledge.ratherthanuse genericstructuralelements ourlearningframeworkbasesits representation on the discriminative power of identifiers and literal types so even when the syntax is only weakly similar deep learning can still recognize similarities among terms.
socher et al.
applied recursive autoencoders to natural language sentences for sentiment analysis.
the novelty in socher s work was the semi supervised augmentation designed to train the model to classify the sentiment of sentences using sentence level labels.
we use recursive autoencoders to learn representations instantiated as syntacticlevel attributes of arbitrary sized code fragments.
one final remark on the nature of the attributes that we use in compiler parlance an attribute i.e.
a quantity associated with a programming construct is said to be synthesized or inherited but the attribute we mine in this work is technically neither.
a synthesized attribute for a node is computed from the attribute values for the node and the node s children whereas an inherited attribute is computed from the node its parent and its siblings .
however in our work attributes are synthesized in a bottom up traversal butthenthetrainingalgorithmwilladjusttheattributes in a top down manner as the errors for general programming constructs are divvied up among their constituents .
.
.
olive trees for clone detection once the model is trained inference is straightforward.
recognizing a clone pair amounts to comparing the representations for two fragments which can be at different levels of granularity.
specifically given a fragment we build the ast and then transform the ast to a full binary tree.
if there are kterminal nodes in the full binary tree then there will bek nonterminal nodes.
as a result encoding the sequence requires k matrix vector multiplications each followed by the application of a vector function to derive the representation for the fragment.
naturally the topology of the full binary tree governs the order in which the nodes representations are combined.
for the specific application of code clone detection all that is required is a threshold for comparing two representations to determine whether their propinquity classifies them as a clone pair the threshold completes the clone detection specification.
.
.
greedy combinations for clone detection here we draw from an approach proposed by socher et al.
for combining pairwise representations in a greedy manner.
first we summarize the training procedure.
for each fragment we build the ast but rather than transform the ast as before we encode each pair of adjacent terminal nodes.
then we select the pair with the lowest reconstruction error eq.
to encode first.
for example in fig.
the first iteration derives two codes the model does a better job at reconstructing rather than the variabledeclarationfragment .
the next iteration substitutes the chosen pair with their new parent and then computes the pairwise reconstruction errors again selecting the pair with the minimum error.
if there are kterminal nodes covering the fragment then this procedure repeats until a representation has been computed for k nonterminal nodes.
once the ad hoc tree is in place the model is trained as before with the backpropagation through structure algorithm and a standard optimization method.
once the model is trained inference again is straightforward.
given a fragment we build the ast and then greedily encode nodes until deriving a code for a node that subsumes the fragment.
this code is compared to other greedily encodedfragmentsusingathresholdtodetectcodeclones.one important note on the training and inference procedures for greedily encoding nodes is that we do not needto build the ast.
in fact since the language model stores an embedding for every term in the corpus we can operate directly on the concrete fragment.
the reason we build the ast is to filter lexical elements such as punctuation to control the depth of the tree that we use for training and inference.
there are some remarkable differences between the two combining methods.
first for the ast based method the clone granularity is generally fixed i.e.
it combines fragments within syntactic boundaries .
on the other hand for the greedy method the clone granularity is generally free i.e.
it combines fragments without syntactic boundaries .
second training requires more computational resources for the greedy method than the ast based method.
the ast based method has k matrix vector products to compute whereas the greedy method has k generally dense matrix matrix products to compute.
third since the greedy method is trained without explicitknowledge of the syntax it does not need to build the ast so the model may better handle syntactically invalid fragments.
despite the differences the methods together reify a new learningbased paradigm for code clone detection.
.
empirical v alidation thegoalofourempiricalstudywasto analyze oursource code representations for the purpose of evaluating them for code clone detection with respect tofeasibility from thepoint of view of software maintainers in the context of ph.d. students and real world java systems .
our intent for establishing feasibility as the quality focus was twofold.
first we are not only presenting an innovative approach to transforming source code but also introducing the idea of framing clone detection as a robustlearning problem.
hence we seek to provide some understanding of the practical relevance of this new perspective.
second given a new approach to clone detection the evaluation in and of itself is a formidable task beset by undecidable problems and variable human judgment .
roy et al.
high table subject systems statistics system files loc tokens v antlr apache ant .
.
argouml .
carol .
.
dnsjava .
.
hibernate jdk .
.
jhotdraw light a number of factors that make evaluating and comparing detection tools challenging including but not limite d to the diverse nature of detection techniques the lack of standard similarity definitions the absence of benchmarks the diversity of target languages and the sensitivity of tuning parameters.
further many clone detection tools are not available.
indeed the community s knowledge of code clone detection tools performances on real world systems is limited .
in this respect our experimental design analysis and reporting are consistent with current studies in the field.
we discuss limitations of our empirical study in sec.
.
and consolidate threats to the validity of our work in sec.
.
notwithstanding the challenges we aimed to determine whether the idea of learning representations for fragments can berelevant for clone detection and a tenable technique for researchers.
we examined the following questions.
rq1are our representations suitable for detecting fragments that are similar with respect to a clone type?
rq2is there evidence that our compositional learningbased approach is capable of recognizing clones that are undetected or suboptimally reported by a traditional structure oriented technique?
considering our goal and questions we intended to estimate the precision of our approach at different levels of granularity to answer rq1 and to synthesize qualitative data on code clones across two detection techniques for rq2.
judging code clones is inherently difficult even among experts because of imperfect definitions and the lack of oracles so we developed a research instrument to support consistent evaluations and control construct threats.
we describe the guidelines used to manually examine candidates in sec.
.
.
.
data collection procedure our subject systems included eight real world java systems tab.
used in previous studies .
we used antlr to tokenize the source code and the rnnlm toolkit to train several rtnns for each system varying hidden layer sizes and depths .
we selected the highest quality model for each system using perplexity as a proxy for quality and extracted the matrix of embeddings .
researchers have not established a correlation between intrinsic evaluation metrics such as perplexity and the quality of model components like the matrix of embeddings.
however anecdotally we have observed interesting patterns in good models induced from java corpora where embeddings for similar terms are collocated in feature space.
for each system except carol we used a hidden layer size of i.e.
z r500in eq.
.
for carol our simplest system in terms of tokens and vocabulary size we used .
next we used the eclipse java development tools to build the ast for each file in every system.
each ast node represents a programming construct and we relied on the visitor 93design pattern to traverse asts identify nodes types and implement ast2bin sec.
.
.
.
empirically we found differentprogrammingconstructsthathaveatleastonecaseii instance so we implemented productions using different artificial types to handle each construct and verified that our ast2bin procedure transformed the asts across our eight systems to full binary trees.
the roots in all but of these trees were compilationunit nodes.
the others were rooted at typedeclaration nodes.
to generate method level corpora we used a methodvisitor collecting methods with loc.
we only considered methods with no more than loc to focus the method level evaluation on small code fragments and complement the coarse file level evaluation.
given the embeddings we induced an ad hoc annotated full binary tree for each file using the greedy method.
then we used the embeddings and the ast based full binary trees to induce an olive tree for each file.
our experimental design planned to compare results from our approach to the state of the practice so we ran deckard on our systems.
to configure deckard we used the settings proposed by jiang et al.
setting mint to stride to and similarity to .
which correspond to standard choices in other tools.
.
analysis procedure rq1.after running the ast based and greedy methods the next step in the clone detection process and the first step in our analysis procedure was to select a similarity metric and threshold.
we selected the l2norm to measure the similarity of fragments codes.
for the ast based method we used the same file level threshold .0e for each system.
for the greedy method the distances were dispersed across several orders of magnitude so we selected file level thresholds such that the number of candidates was approximately equal to the number proposed by the ast based method for each project.
likewise we used general thresholds for methods.
our selections were not optimized in accordance with our goal of evaluating feasibility rather than improving effectiveness .
in other words we are studying the feasibility of a new learning based paradigm for code clone detection improvingtheeffectivenessoflearning basedtec hniques by tuning project dependent hyperparameters such as the size of the embeddings or the threshold for classification constitutes a different problem.
given the lack of oracles for our systems we set out to manuallyexaminerandomsamplesofcandidates.toprovide a reasonable scope for the manual evaluation we settled on assaying file and method level candidates using two autho r agreement.
two ph.d. students evaluated file and methodlevel samples for each combining method and every system.
if our approach performed well on several hundred oracled pairs at multiple levels of granularity then it is sensible to conclude that our source code representations are suitable for clone detection.
to support consistent evaluations we adapted the taxonomy of editing scenarios designed by roy et al.
to model clone creation and be general enough to apply to any level of granularity.
in our scenario based evaluation both participants were presented with samples and instructed to compare them systematically i.e.
top down from scenario i to scenario iv where clones created by the scenarios correspond to one of the four clone types to assess each sample as a true positive or false positive.
after independently evaluating the samples authors disagreements were discussed and resolved.in addition to providing a reasonable scope for the manual evaluation another reason why we examined file level samples is we expected the coarse granularity a mixture of compilation units and types to be harder for our recursive learning procedure which amounts to applying the chain rule for partial derivatives.
larger fragments yield deeper trees but training deep architectures is notoriously difficult .
consequently if the rvnn is capable of producing good results at coarse granularity then it is reasonable to expect its representations at lower levels of granularity are effective and we substantiate this claim with our method level evaluation.
moreover empirical studies have underscored several practical uses for file level clone detection to include inter alia detectin g similar projects and measuring third party library reuse.
sec.
reports estimates of the precision of our approach.
measuringrecallisacommonlimitationtomanyclonedetectionstudies.weconsideredusingasyntheticclonebench mark but our approach is based on learning from how terms are used in a corpus.
by using a mutation analysis procedure we would increase our control over estimating recall but we would reduce the degree of realism which risks setting real influential factors e.g.
patterns mined at the lexical level outside the scope of the study .
rq2.our second research question was intended to frame an exploratory study on our results as compared to state ofthe practice results where differences may admit important practical impacts and theoretical advances.
from a software maintainer s point of view a detection technique that is capable of reporting clones at fixed levels of granularity is useful .
for example given an oracled pair of file clones it would be ideal for a detection technique to report the files as clones rather than splinter the compilation units and report their constituents as clones.
structure oriented techniques like deckard try to account for similar code of any size with ad hoc user provided input e.g.
the width of a sliding window but automated support for this practical concern is not designed into the approach as it is in our work.
automatically reporting clones at a fixed level without requiring input from the user beyond specifying the level would be a notable strength of our compositional learning based paradigm where information is communicated between generalized constructs such as types and specialized constructs such as statements to train the model.
to provide a reasonable scope for the exploratory study we settled on file level pairs.
sec.
synthesizes qualitative data from the study.
.
empirical results ourrvnnimplementationforkedsocheretal.
which used l bfgs to optimize costs in batch mode.
we trained each model for at least epochs on one compute node serving two intel xeon e5 v2 processors at .
ghz.
tab.
reports the average training time in seconds per epoch.
once a model is trained inference at any level of granularity amounts to matrix multiplications so tab.
reports the average time in seconds to infer the representation of a file.
these results contained outliers so we also report the median time in parentheses.
sec.
summarizes lessons learned from training these models on source code.
rq1.sampling candidates for each combining method and system tab.
reports the ratio of true positives as well as the total number of samples used to build the estimate.
altogether we sampled and manually evaluated file level 94table performance results systemtraining sec inference sec ast based greedy ast based greedy antlr .
.
.
.
apache ant .
.
.
.
argouml .
.
.
.
carol .
.
.
.
dnsjava .
.
.
.
hibernate .
.
.
.
jdk .
.
.
.
jhotdraw .
.
.
.
pairs from a pool of candidates and method level pairs from a pool of candidates.
of the file level samples were evaluated to be true positives where of the false positives came from one configuration dnsjava ast based .
then we applied the model that was trained on the file corpus to the method corpus.
of the methodlevelsampleswereevaluatedtobetruepositives.oncemore neither file nor method level thresholds were optimized.
for systems that had less than or equal to candidates after applying the generic threshold we manually evaluated every candidate.
for instance hibernate ast based only had file level pairs with distances below the threshold and all candidates were true positives.
for systems that had more than candidates we sampled of them.
in one case carol ast based the threshold on file level pairs was too strict.
nonetheless tab.
provides empirical evidence that our learning based paradigm is feasible for real world systems.
among the file level true positives we found pairs mapping to allfour clone types i ii iii and iv .
as expected the distances were near zero for type i clones and there was more dispersion for the other types.
four of the five type iv clones were found by the ast based method.
we placed several examples of true positives and false positives in our online appendix .
rq2.for a traditional structure oriented technique we selected the prominent tool deckard .
for the exploratory study we queried the file level true positives and filtered them to remove pairs with at least one file that had less than tokens and to remove type i and type ii pairs.
we focusedtheexploratorystudyonhowdeckardreportedfragments in the remaining pairs and we found evidence that pairs were either undetected or suboptimally reported.
undetected.
in hibernate our approach detected nonuniqueobjectexception and wrongclassexception which were evaluated to be type iii clones.
both classes have the same private fields and implement the same methods using similar syntax.
discounting type i and type ii variations the fewnotabledifferencesarereordereddataindependentstatements in the constructors minor syntactic differences in a getter and one class overloads its constructor.
deckard did not report any similar fragments for this pair.
another hibernate pair nonstrictreadwritecache and readonlycache implement the same interface but the placement of their methods is noticeably different.
deckard detected the similarity from the package declarations through the field declarations but these classes share many more points of commonality.
in argouml our approach detected gonamespacetodiagramandgoprojecttostatemachine.both classes extend abstractperspectiverule which implements perspectiverule.
two of the three methods in the interface are type i clones.
for the third method one class defines an arraylist wrapped as a list and iterates through a list of diagrams conditionally adding diagrams to the list.
the other class defines an arraylist wrapped as a collectiontable precision results systemfile level method level ast based greedy ast based greedy antlr apache ant argouml carol dnsjava hibernate jdk jhotdraw and iterates through a list of models adding models to the collection where conditional checks appear to be abstracted away.
deckard only reported similarity between the package declarations and import statements.
similarly in apache ant deckard detected similarities in the front matter of difference and intersect but the classes have more similarity.
both classes extend baseresourcecollectioncon tainer with their main functionality in the method getcollection.
the first seven lines of getcollection are type ii clones but then the classes differ on how they populate the collection.
difference uses a for loop to iterate over a list of resourcecollectionswhereasintersectusesawhileloop.our approach detected the clone pair despite the classes using distinctly different control statements.
finally our approa ch detected minforecord and srvrecord in dnsjava.
despite some syntactic differences there are evident similarities yet deckard did not report any similar fragments between these classes.
in sum our approach detected pairs with strong and weak syntactic similarity that were undetected by deckard weplacedseveralexamplesinouronlineappendix .
suboptimally reported.
in jhotdraw our approach detected two instances of connectiontool which were evaluated to be type iii clones.
the two instances share most of their source code with identical syntax except for small numbers of additional lines in some cases one line in different locations throughout the files.
these were larger files which indicates that our approach is capable of handling gaps throughout a pair of large files and detecting their similarity.
deckard reported nearly clone pairs that covered mostofthefiles however fromasoftwaremaintainer spoint of view this fragmentation makes it difficult to detect these strong type iii file clones.
.
discussion internal validity.
we acknowledge the confounding configuration choice problem .
we did not adopt arbitrary configurations and tried to justifyeach configuration in our approach.
we also tried to justify our deckard configuration.
external validity.
fromthepointofviewofsoftwaremaintainers two ph.d. students conducted the evaluation on eight real world software systems.
thus we believe everything to be representative.
construct validity.
we recognize that analytical studies such as our empirical validation cannot adequately evaluate the behavior of the developers while using a tool based on our approach .
we do not infer developer behavior from our results and understand that humans must be observed while using the approach .
finally to mitigate monomethod bias two judges used a uniform set of guidelines to measure the similarity of code fragments.
lessons learned.
while our results affirm that deep learning is suitable for clone detection reducing training times is one area that needs more attention.
to this end we identified some corrective action.
first we removed files with 95antlr apache ant argouml carol dnsjava hibernate jdk jhotdraw .
.
.
.
.
.
.
.
.
.20ast based greedy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure relative frequency histograms of file level featu res more than lexical elements from the training set but we should have been more aggressive with this cutoff.
extremely large files significantly bogged down training times and they may not be effective examples for the recursive learning algorithm.
second we should have sorted files by their size before feeding the training set to the learning algorithm to improve worker utilization.
since we optimize the objective in batch mode the order used to process examples is inconsequential to learning yet the order can significantly impact times when training for several epochs.
third training rvnns is embarrassingly parallel so we are modifying the implementation to run on a cluster of compute nodes.
.
future work on scaling deep learning for clone detection.
here we draw from work in the machine learning community on semantic hashing and show how a seemingly innocuous machine learning detail in a deep learner can have important practical impacts in se.
in sec.
.
.
we casually described g eq.
as a nonlinear vector function.
gis called an activation function and there are a number of activation functions used in practice e.g.
g tanh.
models are initialized with small random weights which implies the pre activation e.g.
x zin eq.
would lie in the approximately linear part of tanh .
as the model trains weights increase drawing pre activations away from zero and introducing nonlinearities .
when using tanh activations weights may be directed positively or negatively away from zero.
for instance we initialized our rvnns by sampling from approximately unif .
.
and used tanh activations.
after the models were trained we encoded each file in every system.
fig.
shows the relative frequency histograms of features we used weight decay for regularization.
distributions across the configurations reveal an interesting bimodal structure.
suppose we select some rso features greater than map to and features less than or equal to map to .
transforms continuous valued feature vectors into binary codes allowing us to measure the similarity of fragments in a different metric space.
thus given a fragment clones can be detected by looking in small hamming balls around the fragment for other fragments in the repository a computation that can be optimized by fast algorithms on modern computer architectures .
not only would the binary codes enable fast search because measuring similarity amounts to finding fragments that only differ by a few bits but they would also require less memory a key concern for massive repositories.
while conducting our empirical study we noticed a significant amount of type i and ii cloning in jhotdraw so we transformed jhotdraw s greedy filelevel feature vectors using .
of samples were evaluated to be true positives all type iii clones which was noticeably worse than measuring similarity with l2.
to tune models for binary feature vectors we plan to experiment with different learning heuristics.
salakhutdinov and hinton reported that semantic hashing with their generative based approach was much faster than lsh in their experiments hashing natural language documents.
type prediction.
fig.
shows how the original model z y can be augmented with another decoder trained to predict the type of a programming construct or fragment given its code zin eq.
.
socher et al.
used a similar design to analyze sentiment in a semi supervised way with manually generated multinomial distributions.
12345 2 3 l l r r l r int foo figure decoderour augmentation would not require manually generated coarse grained labels like the sentiment task because the types here are automatically imputed by the compiler.
for example in fig.
if we present to the model then we expect variabledeclarationfragment.
the only change to the criterion eq.
is adding an expression to compute cross entropy misclassification costs.
.
conclusion weintroducedacompletelynewwaytodetectcodeclones.
ourlearning basedparadigmdivergesfromtraditionalstructure oriented techniques in at least two important ways.
first terms including identifiers influence how fragments at different levels of granularity are represented.
second our techniques are designed to automatically discover discriminating features of source code whereas traditional structureoriented and metrics based techniques use fixed transformations.
our results indicate that learning how to represent fragments for clone detection is feasible.
we found that our techniques detected file and method level pairs mapping to all four clone types and evidence that learning is robust enough to detect similar fragments with reordered data independent declarations and statements data dependent statements and control statements that have been replaced .
our online appendix is publicly available .
.