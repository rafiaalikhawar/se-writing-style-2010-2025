multifaceted automated analyses for variability intensive embedded systems sami lazreg visteon electronics universit ec ote d azur cnrs i3s francemaxime cordy snt university of luxembourg luxembourgphilippe collet universit ec ote d azur cnrs i3s francepatrick heymans university of namur belgiums ebastien mosser universit ec ote d azur cnrs i3s france abstract embedded systems like those found in the automotive domain must comply with stringent functional and nonfunctional requirements.
to fulfil these requirements engineers are confronted with a plethora of design alternatives both at the software and hardware level out of which they must select the optimal solution wrt.
possibly antagonistic quality attributes e.g.
cost of manufacturing vs. speed of execution .
we propose a model driven framework to assist engineers in this choice.
it captures high level specifications of the system in the form of variable dataflows and configurable hardware platforms.
a mapping algorithm then derives the design space i.e.
the set of compatible pairs of application and platform variants and a variability aware executable model which encodes the functional and non functional behaviour of all viable system variants.
novel verification algorithms then pinpoint the optimal system variants efficiently.
the benefits of our approach are evaluated through a real world case study from the automotive industry.
i. i ntroduction in many embedded systems requirements engineering and design activities are tightly intertwined and involve complex multi criteria decision making over various concerns.
as an example let us consider an infotainment feature in an automotive system.
specifying such a feature typically entails defining a set of functional and non functional aka.
quality requirements.
functional requirements define for example not only what content should be displayed through the human machine interface hmi but also constraints imposed by the hardware like not exceeding the available memory.
typical examples of non functional requirements constrain manufacturing costs or execution time.
a notoriously difficult problem is to establish whether such a set of requirements is feasible and what is the best design to implement it.
design options are not only constrained by the requirements but also by the existing software and hardware architectures.
in our case an hmi rendering automotive system consists of a data processing application i.e.
data flow oriented embedded software and a resource constrained hardware platform i.e.
heterogeneous hardware components like nonprogrammable processors and data storage units .
the application and the platform are however not completely fixed as they have variation points.
there are three main sources of extensive variability.
first at the application level multiple data flow variants can achieve the functional requirements this work was done while maxime cordy was part of the u. of namur.differing in e.g.
the size of the flowing data chunks the ordering of the operation tasks or the choice between alternative functionally equivalent tasks.
second there exists a diversity of configurable hardware platforms which can differ e.g.
in memory capacities and processing pipelines.
third there are various ways of mapping and deploying a given application on a specific platform e.g.
choose a processor to perform a given task or select a memory unit to store a given data.
this threefold variability is typical in automotive and many other kinds of embedded systems .
unfortunately it leads to a high number of variants in this particular case each of which represents a specific system design alternative ordesign for short that is a specific mapping of a specific application variant to a specific platform variant.
among these design alternatives not all are able to realize the functional requirements to the same extent and the same holds for the non functional requirements.
given the sheer number of variants a systematic consideration of all design alternatives is unfeasible for the software and system engineers whereas the high level of competition in industry puts a high pressure on them to deliver optimal solutions and do so timely .
efficient automations therefore appear as a necessity.
examples of questions the engineers need to quickly answer are can the specified hmi be properly rendered on platform x?
which feasible designs can be built with a budget of y?
which feasible designs can execute in less than z time?
which feasible designs with a rendering quality higher than p and a manufacturing cost lower than q exhibit the fastest execution time?
which feasible designs reach the best tradeoff between rendering quality manufacturing cost and execution time?
answering those questions not only requires a way to deal with the variability induced combinatorial explosion see previous paragraph but also a way to reason simultaneously about different types of concerns feasibility satisfiability and optimality functional and non functional requirements the structure and the behaviour of the system.
although significant progress was made in the recent years to automate reasoning on variability intensive systems existing solutions only address specific facets of the problem in isolation.
as revealed by our experiments partial solutions give suboptimal designs while complete but non variability aware solutions do not scale.
hence the need for multifaceted variability aware analyses capable of answering all the above questions.
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
without such solutions engineers mostly resort to intuition and experience.
theoretical analyses can be made but are timeconsuming and often turn out largely suboptimal if not completely wrong.
simulators are sometimes provided by platform suppliers but analyzing all system variants requires simulations for all of them which is unrealistic.
quick prototyping may occur when the platform is finally supplied but it is then too late to backtrack if the wrong platform was picked.
in the end current practices are deemed very unsatisfactory.
these observations were made by our partner visteon electronics an international leader in automotive systems and are corroborated by surveys such as .
they formed the motivation for the industry academia partnership which led to this research effort.
in this paper we propose an approach that combines and extends existing research results in order to provide the first tooled framework able to solve the multifaceted problem described above.
our contributions are modelling languages based on y chart to capture functional and non functional specifications of variability intensive embedded systems that can vary at both the application and platform levels mapping algorithms to derive from the application and platform models the resulting design space i.e.
all design alternatives while capturing all the structural behavioural functional and non functional variations the first variability aware model checking algorithm to optimize multiple behaviour dependent quality attributes across all variants an integrated tool chain to evaluate the functional feasibility and the non functional satisfiability and optimality of the whole design space at once qualitative and quantitative evaluations of the approach based on a real system developed by visteon.
the qualitative evaluation shows that functional and non functional requirements were properly captured by our framework and optimal system designs were correctly identified.
the quantitative evaluations assess the scalability of our approach and give us confidence that our framework is applicable to the majority of visteon s systems and similar systems developed elsewhere in industry.
although its motivation originates from an industrial partnership our contribution is not domain specific.
our modelling method covers a large class of variability intensive data floworiented systems with quality attributes.
our model checking algorithm is language independent and can be applied to a broader range of variability intensive systems.
ii.
i ndustrial case study this research originates from a collaboration with visteon electronics a leading company developing solutions for the automotive industry such as instrument clusters infotainment and connected vehicles.
in this section we introduce a simplified exerpt of the system we use in our evaluations see sec.
viii to further illustrate and justify our approach.
aninstrument cluster generally consists of a speedometer and other instruments which unlike traditional analog gauges fig.
our instrument cluster application case study d1 d2a b rend.
quality c sizes 1024kb rend.
quality 2p1 p2 p3size 512kb dp4 task datapath path split join legend fig.
a variable data flow specification appear on an electronic visual display see fig.
.
by applying various data flow image processing effects e.g.
3d gauges 3d view of the car it improves the driving experience.
to achieve economies of scale such systems are often built and sold to car manufacturers as product lines which have to meet and optimize the manufacturers variable requirements for an entire range of cars.
to be competitive they are highly constrained on quality and cost.
as such one must know as early as possible in the development whether the expected hmi can be rendered properly on a candidate platform.
at industrial scale the threefold variability introduced in sec.i i.e.
from application platform and mapping prevents any product based exhaustive feasibility checking let alone exhaustive reasoning optimization on quality attributes e.g.
cost rendering quality runtime .
yet some separation of concerns is intrinsically possible as data flows can be abstracted from the implementation by domain experts i.e.
rendering engineers .
platforms are already specified by hardware experts to organize competitive tendering on hardware providers.
fig.
shows an example of data flow specification with quality attributes.
the different processing flows that meet the hmi functional requirements are captured by a variable dataflow.
images are processed by graphical tasks.
image d1can be processed by tasks aorb.d2has three different possible resolutions and is processed by task d. the images produced bya orb and dare then processed by task c which delivers the final result.
task and image resolution impacts the hmi rendering quality.
in our case performing binstead ofasignificantly improves the rendering quality.
also as the resolution of d2is increased the overall quality raises as well.
at the platform level image processing functions a b c d are provided by a non programmable graphical processing unit gpu and an advanced display controller unit dcu .
within dcu there are three functions a candd authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
resource interconnectionsize 4096kb cost bytes per cycles latency cycles freq 200gpu needs rama 4bpcc 8bpcr0b 2bpca 8bpcr0 rom ramd 4bpcr1d 16bpc size 2048kb cost bytes per cycles latency cycles freq 200cost freq 100cost freq 200dcu gpu functionmemory storagebuffer optional legendprocessor fig.
platform specification input model and two buffers r0andr1 .
directed edges denote the flows followed by data that transit through the processing pipeline of processors while functions may be applied or not.
the platform also includes data storage on ram androm.gpu andram are optional as everything can be stored in rom and then rendered directly to the screen by dcu.
yet they improve the runtime efficiency for a higher manufacturing cost.
there is a presence condition between ram andgpu asram acts as a dedicated cache memory for gpu while ram can be used without gpu e.g.
to store more data or larger images.
ram comes in two alternative capacities at different costs .
similarly ram gpu androm have two possible frequencies.
frequency acts as a scale factor for data processing transfer bandwidth for processors and memories.
our overall goal is to assist engineers in determining which variants i.e.
alternative designs consisting of a data flow variant deployed on a platform configuration of the instrument cluster can satisfy the imposed functional fc and nonfunctional nf requirements.
to ensure the executabilty of the application on the platform a first fc requirement states any data required by a task must be stored in a memory storage accessible to the processor that processes the task .
its satisfaction depends only on the structure of the design e.g.
which tasks must be processed which tasks exchange data with each other which memory storage is accessible by the processors and how tasks resp.
data are mapped onto processors resp.
memory storage .
another fc requirement states that the execution of the application on the platform must eventually terminate.
this not only depends on the structure of the design but also on its runtime behaviour as bad scheduling of tasks and data transfers may cause deadlocks.
checking the satisfaction of this requirement by a given design is more complicated as it requires analysing many if not all its executions.
in addition to fc requirements the design must also meet nf requirements.
these commonly include a maximal manufacturing cost a minimal rendering quality and a maximal execution time i.e.
time to render graphics on the visual display .manufacturing cost and rendering quality are quality attributes which depend only on the structure of the design e.g.
size of input data choice between alternative tasks components of the platform .
execution time however depends on both structure e.g.
processor frequency and behaviour e.g.
scheduling of tasks and memory access operations .
those requirements are not enough though as market competition forces engineers to deliver the best system to each specific customer.
among the variants they must thus find those offering the best trade off between the quality attributes.
achieving our goal thus requires solving the problem of efficiently identifying the variants that satisfy the fc requirements .
we further decompose this sub problem in two challenges a checking the fc requirements that depend only on the structure of the design challenge fcs and b checking fc requirements that depend also on its behaviour fcb .
satisfy the nf requirements that is checking a those that depend only on the structure nfs and b those that also depend on the behaviour nfb .
optimize the trade off between the quality attributes nfob .
this challenge requires considering all quality attributes.
optimizing only those that depend on the structure nfos is easier but leads to suboptimal solutions as revealed by our evaluations .
we must solve all those challenges to give engineers the means of making appropriate design decisions at an early stage.
iii.
s tate of the art and related work many approaches were proposed to tackle parts of the above challenges.
first research in variability modeling is black box oriented and focused on structural aspects.
it attempts to assess or efficiently predict non functional properties of the whole product line thus tackling both fcs and nfs.
extensions of these methods with multi objective optimization allow to find optimal variants wrt.
structural quality attributes thereby solving nfos.
however those approaches lack reasoning support on the system behaviour and are thus unable to e.g.
search for optimal schedulings.
behavioural analyses of variable systems were addressed by variability aware model checking against functional requirements challenge fcb or one particular nonfunctional aspect e.g.
real time reliability income quality of service .
resource optimal execution and worst case execution time are still determined product by product.
also contrary to our framework these approaches are unable to automatically map variable workflow specifications on configurable platform descriptions in order to infer a system design space.
therefore applying them obliges to manually model and assess all possible systems designs.
they thus inefficiently tackle nfob.
even when both levels are captured in software hardware product lines with dependencies and constraints the expressiveness is also insufficient because either behaviour is not considered or only functional requirements are checked.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
some system design frameworks model assess and optimize quality attributes nfb and nfob but do not capture nor manage all variability dimensions.
specific techniques attempt to efficiently handle either platform variability application variability or deployment variability but they are limited to one dimension at a time and cannot reason on the whole problem.
other approaches tackle both functional and platform variabilities by focusing on an optimal platform configuration for a multi variant application.
contrarily to our solution they do not find a design i.e.
a mapping of a functional variant onto a platform configuration that ensures an optimal execution.
our recent work can capture and reason on variability at all three levels.
it is however limited to functional requirements challenges fcs and fcb .
iv .
o verview of the framework our approach follows the model driven y chart process which explicitly separates application and platform.
this allows for the modular specification of and the reasoning on the different parts of the system.
as shown in fig.
our framework involves several models and processes.
on the left the inputs are application and platform models such as those of fig.
and fig.
.
domain experts are expected to model the application as an extended concurrent data flow model with quality attributes.
the model contains the classic structure and behaviour of the data flow data task edge and its variability.
additionally platform experts provide the platform specification as a templated concurrent componentbased system which also captures platform variability.
first process detailed in sec.
v from the input models the framework generates a variability intensive design space that captures all valid deployment mappings of the variable application onto the configurable platform.
a mapping basically allocates tasks to processors and data to memory components.
second process detailed in sec.
vi from the design space we generate representations that allow for reasoning on the structure and the behaviour of all designs.
in addition to the input models experts define the nf requirements as constraints and a cost function representing trade offs between quality attributes.
to relate nf requirements with the design space we rely on feature models with quality attributes for the structural part and on featured transition systems with added weights for the behavioural part.
third process detailed in sec.
vii we reuse featuremodel reasoning and apply our new model checking algorithms to identify the designs that meet the requirements fcs fcb nfs and nfb and are optimal nfob .
additionally we can provide the execution traces that optimize the quality attributes while satisfying the requirements.
such a trace shows not only the designs able to execute it but also the behaviour it exhibits i.e.
how the application tasks are executed and scheduled onto the platform thereby helping the upcoming engineering of the designs.v.
a pplications platforms and mappings to model the application the platform and their quality attributes we extend our former modelling framework to support nf requirements as it is limited to checking designs that satisfy the fc requirements.
thus we define formal models for variable dataflows and configurable platforms.
definition a variable data flow graph is a tuple vdg n p e d wheren t dis a set of nodes t are tasks and dare data pis a set of communication paths by which data flows between producers tasks and data and consumers tasks e n p tis the set of flow processing between producers and consumers via available paths is a set of attributes d n uniontext true false is a function that associates a node nto the set of values that all or a subset of the attributes in can take where is the finite set of values that can take.
squaresolid intuitively these graphs encode two forms of variability.
the first consists of variations in the data flow as we allow data paths to have multiple connected producers and consumers this follows the same approach as in variable workflows .
the second lies in the alternative attribute values that a node can take.
for example consider again fig.
.
we see that datum d2can have three size values.
this corresponds to a design variation of the application i.e.
the size of the processed data .
on the contrary the quality ofd2 represents the impact of data size on the overall quality of the system which is also determined by the quality value of b. thus the overall quality depends on i the size of d2 and ii whether aorbconsumes d1.
in our case study the property value of the system is obtained by summing the property values of its constituents.
we make this assumption in the rest of the paper without loss of generality one can use instead other aggregation functions e.g.
average maximum .
our definition of dallows one to define cross cutting constraints over the attribute values of a given node n.f o r instance the quality value of d2is directly linked to its size property a size of leads to a quality of to and to .
we see that d n defines which valuations of the attributes are valid altogether .
this flexible definition akin to the notion of configuration of non boolean parameters can express that some attribute values are forbidden in n and that the value of given attributes in n restricts the values of the others.
definition a variable resource graph is a tuple vrg r c r wherer p sis a set of resources pare processors and sare memory storage c s p p s is a set of connections between processors and storage where p s c resp.s p c means that processor pwrites from resp.
reads to storage s 2r true false encodes which subsets of resources constitute a valid platform configuration is a set of attributes r r uniontext true false associates a resource rto the set of values that the attributes in can take.
squaresolid the function ris defined similarly to its counterpart in variable data flow graphs and offers the same benefits.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
priced feature model featured weighted automatavariabilityaware mapping section v variability intensive design spacecontributions on expressiveness non functional constraints and cost functiongeneration of executable models section vi variability aware cost optimal model checking section vii contributions on reasoning powerextensions and novel applications of existing back ends execution traces optimal variantsvariable platformvariable application fig.
framework models and processes another source of variability encoded by this model comes from the optional nature of resources that is two variants of the platform may differ by their constituent resources.
symbolically encodes which subsets of resources constitute a valid platform in the form of boolean constraints.
the third and last part of our formal model is the mapping rules that must be satisfied to deploy any given application variant on any given platform variant.
these rules are automatically generated from the variable data flow graph and the variable resource graph.
a mapping allocates each task to at least one processor function each datum to at least one memory storage and each path to at least one buffer or memory storage.
a valid mapping must ensure that the required data can be read and written by the hardware functions using them.
for each path p producer node iand consumer task oofp there must exist a buffer or storage b associated to p a function fiassociated to i and a function foassociated to o such that fican write to bandfocan read fromb.
altogether the two graphs and their mapping rules constitute the variability intensive design space as they define the set of the design alternatives that can result from a valid deployment of a variant of the application onto a configuration of the platform.
vi.
e nabling reasoning on the design space from the variability intensive design space we generate intermediate representations to reason on the structure and the behaviour of all variants and on their quality attributes.
in product line engineering structural variability is commonly addressed by modelling features and their interdependencies as a feature model fm .
since we consider nf requirements we use an extension of fms where features have quality attributes which we name priced feature model pfm .
reasoning on the pfm allows for determining which design variants satisfy the requirements that depend on the structure of the design fcs and nfs .
definition a pfm is a tuple pfm f q wherefis a set of boolean features qis a set of positive real valued quality attributes is a set of cross cutting constraints over fdefining which subsets of features form a valid variant f q r 0is a function defining howeachf fchanges the value of each q q is a set of constraints over qdefining what are the valid aggregated values for a given attribute q. the semantics of a pfm is a partial function llbracketpfm rrbracket 2f q r 0such that i llbracketpfm rrbracket is defined only for the valid variants f prime dom llbracketpfm rrbracket f prime ii the aggregated value of each attribute qsatisfies the constraints q q f prime q iii each attribute is associated with its aggregated value llbracketpfm rrbracket f prime q f prime q where f prime q is the aggregated value of qin the variant represented by f prime f. squaresolid an excerpt of pfm is shown in fig.
.
each variation point that may occur in the application e.g.
consumer of p1is aorb platform e.g.
ram is present or not or mapping e.g.
d1can be stored in rom orram gives rise to an optional feature in the pfm.
the attributes representing design variations e.g.
the size of d2 are transformed into alternative features while those representing quality attributes e.g.
cost of manufacturing become the pfm s quality attributes.
results directly from the mapping rules 3a and 3b the presence conditions between resources consistency rules requiring that alternative application node or optional resources used in the mapping become mandatory and constraints over attribute values encoded in dand r. correspond to the nf requirements defined by the engineers.
finally is obtained from dand r. to check requirements that depend on behaviour we need to systematically evaluate all executions of all design variants.
a common approach to achieve this for a single design is to model check an automaton modelling the system behaviour which results from the scheduling of an application automaton on a platform automaton.
these approaches however cannot handle variability and are thus limited to one system at a time.
accordingly we propose to generate from our variability intensive design space a featured weighted automaton fwa a recent formalism that combines featured transition systems with priced automata .
basically fwa is a variability aware weighted extension of finite state machines where each transition t is annotated with a formula defining i which variants can executet and ii conditional weight values that depend on the variant executing t. this formula named weighted feature authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
an excerpt of the pfm corresponding to fig.
and .
expression allows one to represent compactly the evolution of quality attributes as the execution of the variants progresses.
definition letfbe a set of features and 1... n an ordered set of npositive real valued weights.
then a weighted feature expression is a possibly partial function 2f rn 0that associates a variant f prime fwith a vector wsuch thatwiis the value of the i th weight for f prime.
squaresolid intuitively the weighted feature expression associated to a given transition tis such that a variant f primebelongs to dom if and only if f primecan execute t and t returns the value of t s weights for f prime.
in our case each weight corresponds to a distinct quality attribute and represents the value added to this attribute when f primeexecutes the transition t. definition lett 1... n be a set of positive realvalued quality attributes and pfm f q be a pfm such that q t.af w ao v e rt andpfm is a tuple fwa s s0 sf t wheresis a set of states s0is the initial state sfis the final state t s sis the transition relation and t 2f rn 0associates each transition with a weighted feature expression.
squaresolid a fwa defines for each variant f prime f the set of paths starting from s0tosfthatf primecan execute together with the weight vector associated to each path f prime f llbracketfwa rrbracket f prime p w s rn wherep s0 sk sfand such that i pexists in fwa j j k sj sj t ii all transitions of pare executable by f prime f prime intersectiontextk j 0dom sj sj iii wis the sum over all associated weight vectors w k j 0 sj sj .
note that we assume all quality attribute values to be positive.
negative values are supported modulo transformation.
in pfm one can replace negative values on a feature bypositive values on the alternative features.
in fwa one canreplace negative values on a transition by positive values on the alternative transitions.
from the design space we generate a network of fwa where each source datum task processor and memory storage corresponds to a distinct fwa whose states and transitionsencode the different steps of their process.
data automataimplement the process of sending the data onto a task au tomaton which itself transmits the data to the automaton ofthe processor that provides the function whereon the task fig.
an fwa modelling memory storage behaviour.
is mapped.
the automaton of a processor models standardprocessing pipeline behaviour .
during this processing the processor can store input and output data by transferring them to its dedicated storage automaton.
an excerpt of fwa modelling storage behaviour is shown in fig.
.
it models memory read and write via input output inter faces while checking that memory capacity is never exceeded.weighted feature expressions occur in the transitions of thehardware resources automata i.e.
processors and memories that correspond to processing instructions and memory ac cesses as these operations impact the execution time qualityattribute.
in the excerpt such an expression is shown in pinkand models that the increase of time depends on ram fre quency.
the automaton modelling all the designs results from the parallel composition of the individual automata which can synchronize their transitions on specific synchronizationactions e.g.
the transfer action in fig.
sends data only when another fwa executes the corresponding action .
our fwa differs from that of fahrenberg et al.
i ours is linked to a pfm to also check structural requirements ii we support parallel composition and consider time as a special weight in that we execute fastest actions first anddynamically update the remaining time delay of other parallelactions to simulate time elapsing iii fahrenberg et al.
represents as a partition of the set of variants because their verification algorithm which is not tooled relies on matrixrepresentations whereas ours relies on antichains to scalewrt.
the large state space incurred by our evaluation cases.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
vii.
a ll in oneverification and optimization our fwa formalism allows us to design a first algorithm to solve all challenges fcs fcb nfs and nfs for all variants at once.
the key idea is to perform an exploration of the state space of the automaton in search of paths that can reach the final state while satisfying the fc and nf requirements.
the first step filters out variants that do not satisfy pfm constraints and thereby ensuring that the structure of the variants does not violate the requirements.
then we explore all paths starting from the initial state.
as we visit a new state we retain the set of variants able to execute the sequence of transitions that led to the state.
we also accumulate the sum of the weights over all executed transitions and for all variants and assert that these values satisfy the nf requirements.
in the end we obtain a set of paths going from the initial to the final state together with for each path p a the valid variants that can execute pand b the values of the quality attributes corresponding to pand each of those variants.
this first algorithm finds all variants satisfying the requirements.
we have to find what are the optimal variants satisfying the requirements while providing the best values for the quality attributes.
since these attributes can be antagonistic e.g.
manufacturing costs can decrease to the detriment of rendering quality this is assimilated to multi objective optimization.
to drive our search for optima we define a cost function over the attributes let 1 ... n 1 1... n nbe our cost function such that i r is the coefficient associated to the attribute i. then our objective is to discover the variant that minimizes .
this is achieved by modifying our exploration algorithm in order to i record the optimal variants and their associated attribute values and ii stop exploring a path as soon as all quality attributes reach a worse i.e.
higher value.
this latter heuristics requires the cost function to be monotonic as more states are explored along a given path hence why we assume that all iand iare .
negative values can be supported by disabling the heuristics line in algorithm .
algorithm details this exploration procedure.
it takes as input a fwa a pfm and a cost function .
it iteratively computes r the reachability relation that associates each state sto the function encoding all variants that can reach sand their associated attribute values.
at first line rcontains s0together with 0 such that dom 0 dom llbracketpfm rrbracket and 0 f prime i for any variant f primeand attribute i. then we start the exploration from s0 l3 and iterate over the states encountered successively l5 .
at each iteration we retrieve the state sreached last together with its associated function l6 .
here encodes the variants that can reach s and associates to each variant the values of its attribute values when following the path that led to s. if all these variants yield a value for greater than the current optimum l7 we do not pursue the exploration further from this state.
otherwise we distinguish between the cases where sissf l8 and where it is not l11 .
in the first case we assign to the minimal cost over all variants that can reach the final state.
in the second case we compute the set of successors of s input fwa s s0 sf pfm f q 1 ... n r output f the set of optimal variants that reach sf their associated minimal cost 1r s0 0 2 3stack 4push s0 0 stack 5whilestack negationslash do s pop stack if f prime dom f prime then ifs sfthen min f prime dom f prime end else foreach s prime prime post s do if negationslash s prime prime prime r prime followsequal prime primethen push s prime prime stack r r unionsq s prime prime end end end end 20end 21f f f sf f r f f 22return f algorithm optima fwa pfm l12 given by post s s prime prime s s prime t prime s s prime wheredom 1 2 dom 1 dom 2 and f prime dom 1 2 1 2 f prime 1 f prime 2 f prime .
this means that primeis defined only for the variants that can reachsand execute the transition from stos prime and it sums the attribute values of each variant in with its attribute values on the transition s s prime .
then we add a successor iff itimproves the reachability relation l13 that is if for at least one variant there is no element in rthat gives a better value for all attributes.
this rule ensures that infinite cycles are avoided.
to do so we use the comparison operator followsequalover weighted feature expressions defined as 1 followsequal 2 f prime dom 1 1 f prime 2 f prime .i fris improved we continue the exploration l14 and add the successor to r l15 using a particular union operator unionsqthat keeps ras an antichain.
this is achieved by a split and combine algorithm along the lines of which we do not detail due to lack of space.
finally we return the set of variants f that can reach sf while minimizing together with the optimal cost l21 .
viii.
i mplementation and ev aluation a. implementation we implemented our framework as a toolchain combining a new java tool with extensions of existing model checkers.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
our java tool1consists of two modules each of which implements a process depicted in fig.
.
the first one allows for specifying a variable data flow graph and a variable resource graph via a fluent api.
then it calls our mapping algorithm to generate the design space.
second an automata generator takes as inputs the design space nf requirements and the cost function to generate an fwa with its associated pfm.
two concrete syntaxes are used as we can then invoke two independent model checkers to search for optima.
one is uppaal cora an established tool to carry out cost optimal reachability analyses that we reuse as is with optimal settings.
it takes as input a network of linearly priced timed automata lpta .
lpta can be regarded as fwa without variability and as such can only encode the behaviour of the variants separately.
our automata generator actually transforms our design space into a network of lpta in the uppaal cora format.
it also generates an additional automaton dedicated to configuring the other lptas before their execution starts by setting variables that correspond to the variation points of the design space.
we thus follow the model approach .
additionally we use splot s feature model reasoning library to restrict the configuration to valid products.
then uppaal cora can find an execution of a variant that reaches the accepting state while satisfying all the nf requirements and optimizing the cost function.
the other model checker is provelines which can check variability intensive systems.
we chose this tool because it was extended over the years by both its original developers and others to solve multiple modelchecking problems including real time verification .
we fully implemented alg.
in a new version of provelines.2to achieve this we first extended provelines input language promela to associate promela statements with weighted feature expressions.
actually each promela process encodes a single fwa.
like uppaal cora our provelines extension is able to provide the execution trace associated to an optimal variant.
the difference lies in that weighted feature expressions allow an all at once verification of all variants.
to encode the structural variability we generate a pfm in the format supported by provelines viz.
tvl .
b. qualitative evaluation our first evaluation assesses the usefulness of our approach for practitioners on the basis of visteon s instrument cluster system see sec.
ii .
more precisely our evaluation concerns an important module of the whole system.
yet it remains a real world case that was selected by our partner as representative in terms of size complexity and variability.
with the assistance of an expert engineer we reversemodelled the variable application and platform of the existing system.
some technical simplifications were made as we aim at facilitating early design decisions we do not consider data and parameters that only have minor impact on the system s we abstract away from data content and consider data sizes as the most influential factor for runtime performance we do not model mechanisms like internal cache replacement policies axi bus and internal backup communication buffers as these implementation details have no fundamental impacts and are handled by engineers later in the development process.
these simplifications were deemed harmless by our industrial expert and did not endanger the correctness of our results.
this results in an application model with two input data types four tasks and flow processing and in a platform model with one rom one ram one dcu and two gpus.
in total the variability yields candidate designs.
our generated mapping rules reduced this number to .
by adding the nf requirements rendering quality 2and manufacturing cost we further diminish this number to .
by checking the behaviour against the requirements i.e.
the end of execution is reached within processing cycles we obtain variants that satisfy all requirements.
incorporating the cost function representing trade offs defined with the expert i.e.
with time m.cost r.qual.
yielded optimal variants with time manufacturing cost and rendering quality .
we performed the behavioural analyses using both uppaal cora and provelines which provided the same results for all variants.
this increases our confidence that the transformation of the design space into lpta and promela is consistent.
expert s confirmation is also needed though as mistakes may originate from the input models themselves.
the expert validated that the optima returned by our tools conform to the very best designs that the company could produce over the past years.
the quality attributes values also corresponded to what is expected.
regarding execution time cycles there are slight differences due to hardware modelling simplifications however the numbers are close to reality difference and the relative orders between variants are preserved.
in the end the expert validated that our approach helps make optimal design decisions that will provide significant gains at all stages of development.
c. quantitative evaluations the second part of our evaluation focuses on the efficiency and the scalability of our approach in terms of execution time.
this is indeed essential as the total number of variants can be high in real world systems.
in addition to the instrument cluster case study numbered case we constituted a dataset of realistic topologies that were generated based on our partner s history.
to do so our model generator relies on multivariate gaussian distributions whose parameters were settled on the basis of visteon s past systems.
thereby it ensures that the characteristics of our generated data flow and platform models are similar to real world cases.
amongst all the models we generated we selected of them that appropriately summarize our findings.
these models exhibit different state densities i.e.
average number of system states per variant and variability intensities i.e.
numbers of valid variants to check .
we carried out three series of experiments authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i results for the three quantitative evaluations.
times are in seconds.
fcs fcb nfs nfb nfob nfos priorization product based family based p .v .l.
optim upp .
cora clafermoo p .v .l.
case density variants time explored time explored time time time oos inc. ins.
cl.
.
.
.
.
.
.
gen. .
.
.
.
gen. .
.
.
.
.
gen. .
.
.
oom.
gen. .
.
.
.
.
.
gen. .
.
.
oom.
gen. .
.
.
.
.
.
gen. .
.
.
oom.
gen. .
.
.
oom.
.
.
gen. .
.
.
oom.
.
.
gen. .
.
.
oom.
.
gen. .
.
.
oom.
.
.
presented hereunder.
table i provides the results such that the results of the different series of experiments are separated by double borders.
all benchmarks were run on a macbook pro with a ghz intel core i7 processor and gb of ddr3 ram.
we repeated each experiment ten times and computed the average although random deviations were low.
product based vs. family based.
our first experiments evaluate the efficiency of our method to verify that allvariants satisfy the requirements that is we consider only the four challenges fcs fcb nfs and nfb.
we compare the runtime of our family based verification algorithm with an alternative product based one that checks each variant separately.
both algorithms are implemented in provelines which allows us to compare both approaches on an equal technological ground.
the results are presented on the left part of table i. it depicts for each approach and model the time needed to check all variants as well as the total number of states that were explored by each algorithm.
in the family based case fewer states are explored since one state common to multiple variants is explored only once.
for case the family based method outperforms the product based one reducing the verification time from .
seconds to .
.
the generated models allow us to observe that the benefits of the family based method grow with the number of variants.
when this number is low our family based algorithm either brings insignificant improvements and or performs way worse than the product based approach .
on the contrary for models with higher variability we obtain reductions in verification time of minimum most often substantial ones.
the most impressive results are obtained for the case with the most variants which is also the case where variants share the most commonalities our algorithm is .
times faster achieving an absolute reduction of .
seconds.
we also see that a higher state density often reduces the gain offered by our algorithm e.g.
cases and .
to explain this we analyzed the models and observed that a small number of variants exhibit a large state space while all the others encompass far fewer states.
the variants thus have fewer states in common while family based algorithms generally perform better as variants share more commonalities.
this also explains the poor performance of our algorithm in case .optimization in provelines and uppaal cora.
our second experiment compares the efficiency of the two model checkers to solve challenge nfob that is we compare uppaal cora cost optimal reachability algorithm against our algorithm .
the two tools present clear differences notably in history uppaal cora s development started in the early s while provelines was released in in focus cost optimal reachability in continuous time models vs. family based model checking in discrete time models and in input syntax lpta vs. promela .
still we believe this comparison can highlight interesting research directions.
results are given at the centre of table i. like any productby product approach uppaal cora suffers from every increase in the number of variants except for cases and where it luckily alleviates complexity with branch andbound optimizations.
it systematically performs poorer than provelines.
even worse apart from cases and uppaal cora consumes too much memory 16gb and raises a fatal error.
we assumed two reasons behind this.
first uppaal cora does not implement partial order reduction and thus considers all possible interleavings between lptas including during their configuration.
this creates an exponential blow up as the number of lptas increases.
second we use a model that encodes the behaviour of all variants and thus accumulates all their state space.
yet applying an alternative product based approach where each variant is turned into a separate model did not solve the problem and even led to slower times.
process interleaving is therefore responsible.
one way to circumvent this is to assign priorities over the automata.
however in most cases this will cause to miss better scheduling opportunities and will yield suboptimal results.
in spite of the disappointing results for uppaal cora the fact that it outperforms productbased provelines on case tends to indicate that combining our family based algorithm with uppaal cora s efficient search heuristics can be a promising future work.
through these experiments we also observe that looking only for optima in provelines as opposed to verifying all variants can yield significant reductions in execution time up to in case .
more importantly computing the optima without a family based algorithm boils down to applying the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
product based algorithm used in the first series of experiments.
in this regard our algorithm exacerbates the benefits of a family based algorithm.
for the models with the most variants algorithm is to times faster than the productbased method.
interestingly it seems to be way less affected by the number of variants than the all variant verifications.
prioritization based on structural optima.
the last part of our evaluation studies whether the structural optima i.e.
the solutions of nfos are sufficiently close to the overall optima i.e.
the solutions of nfob .
if that is the case solving nfos would yield an imperfect but relevant cost effective solution as structural optimality can be resolved statically that is without requiring the exploration of a large state space.
we use the clafermoo tool to compute the structural optima based on our pfms.
to complete the toolchain we extended it to generate pfms in clafermoo s format as well as our verifier module to retrieve the variants that are structural optima and only allow these variants by adding a constraint in the pfm.
then we use provelines in family based mode to assess the time needed to discover which structural optima lead to the lowest cost function value as well as the difference between this value and that of the overall optimum.
results are given on the right side of table i. the first two columns give the number of structural optima returned by clafermoo as well as the time needed to run provelines only on those structural optima.
the last column gives the increase of cost function in percentage due to considering only structural optima instead of all variants.
in cases and the variants differ only by their behaviour and are thus all structural optima.
for the other cases we see that provelines runtime is dramatically decreased as it has to check fewer variants.
this however comes at the cost of a notable increase in cost function value up to except for case where it increases by less than .
we conclude that structural optima are still far from being overall optima and thus are not sufficient.
yet in some cases they can constitute viable solutions to get a quick answer or when the state space of the system is too large to be exhaustively explored.
d. threats to v alidity internal .
the main threat to internal validity is the selection of the used case study.
although it was built from specification and feedback meetings with system and platform experts from our partner company it is a single case.
still visteon chose it as being representative in terms of shape and complexity of the data flow the platform and their variability.
external .
several threats to external validity exist.
first our scalability experiments have been conducted over large but simulated data sets.
we nevertheless expect the creation procedures to respect the structure and behaviour of the potential applications and platforms of our industrial partner.
more generally we also expect the proposed approach to be applicable in other domains where data flows can be an appropriate modelling support like stateless enterprise processes with micro services.
on the platform side the componentbased representation seems general enough to cover differentforms of deployment architectures but we do not have any evidence yet of this generalization possibility.
second satisfiability and optimality checking processes depend highly on the level of detail and on the quality of application and platform modelling.
on the platform side a design not detailed enough could lead to loss of relevance in the performance metrics and could even produce false optimal results.
from the feedback we got from our industrial partner the level of detail is currently sufficient as it is at the level they use to specify the platform when selecting suppliers.
still in other contexts some details may be hidden due to intellectual property management.
as for applications they are described with data flows that are well mastered by domain experts in our case study but they have been built manually.
reverse engineering from existing application code could be envisaged to build data flow or at least templates of them but their quality would be directly related to the good organization of existing software.
more globally as the engineers were able to understand proposed designs that result from our framework machine learning technique fed by their feedback on good or bad solutions could help when details are missing.
introducing this in the framework remains an open issue.
ix.
c onclusion in many data flow oriented embedded systems three levels of variability significantly increase the complexity of the design space variable high level data flows are deployed in many different ways over highly configurable componentbased hardware platforms.
we provided a modelling and reasoning framework that unifies state of the art techniques on structural reasoning with a novel variability aware modelchecking algorithm which evaluates the functional feasibility the non functional satisfiability and optimality on the whole design space.
this design space comes as a mapping between two other models one represents the variable applications with a data flow model complemented with quality attributes the other models platform variability through connected components also with quality attributes.
we showed how the design space is transformed into featured priced timed automata to enable different reasoning and optimization operations.
the application of the proposed approach to a medium scale industrial case of an automotive instrument cluster demonstrates its end to end ability to check and optimize a complex design space.
system experts revealed that even on small data flow and platform models the optimal designs were non trivial to identify for them giving us confidence on the relevance of the approach.
experiments on large simulated design spaces also show that the prototyped toolchain exhibits good scalability and outperforms non variability aware solutions.
we believe that the models used in the framework dataflows and components make the contribution potentially applicable in different contexts.
our future works will start with an extension of the framework to facilitate its usage introducing domain specific languages for input models.
we also plan to conduct larger evaluations on different data flows coming from various product lines of our industry partners.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.