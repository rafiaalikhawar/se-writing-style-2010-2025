from ui design image to gui skeleton a neural machine translator to bootstrap mobile gui implementation chunyang chen1 ting su1 guozhu meng1 zhenchang xing2 and yang liu1 1school of computer science and engineering nanyang technological university singapore 2research school of computer sciecne australian national university australia 3sklois institute of information engineering chinese academy of sciences china chen0966 e.ntu.edu.sg suting gzmeng ntu.edu.sg zhenchang.xing anu.edu.au yangliu ntu.edu.sg abstract a gui skeletonis the starting point for implementing a ui design image.toobtainaguiskeletonfromauidesignimage developers have to visually understand ui elements and their spatial layout in the image and then translate this understanding into properguicomponentsandtheircompositions.automatingthisvisual understandingandtranslationwouldbebeneficialforbootstraping mobile gui implementation but it is a challenging task due to the diversityofuidesignsandthecomplexityofguiskeletonstogener ate.existingtoolsarerigidastheydependonheuristically designed visualunderstandingandguigenerationrules.inthispaper we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a ui designimageintoaguiskeleton.ourtranslatorlearnstoextract visual features in ui images encode these features spatial layouts and generate gui skeletons in a unified neural network framework withoutrequiringmanualruledevelopment.fortrainingour translator we develop an automated gui exploration method to automaticallycollectlarge scaleuidatafromreal worldapplications.wecarryoutextensiveexperimentstoevaluatetheaccuracy generality and usefulness of our approach.
ccs concepts software and its engineering human centered computing graphical user interfaces keywords user interface reverse engineering deep learning acm reference format chunyangchen tingsu guozhumeng zhenchangxing andyangliu.
.
from ui design image to gui skeleton a neural machine translator tobootstrapmobileguiimplementation.in proceedings of icse 40th international conference on software engineering gothenburg sweden may june icse pages.
corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
introduction mobile applications apps are event centric programs with rich graphical user interfaces guis .
an app s gui should not only provideaworkinginterfaceforuserinteractions butalsocreateanintuitiveandpleasantuserexperience.infact thelateriscrucialfor anapp ssuccessinthehighlycompetitivemarket .developingtheguiofanapproutinelyinvolvestwoseparatebutrelated activities design a ui and implement a ui.
designing a ui requires proper user interaction information architecture and visual effects of the ui while implementing a ui focuses on making the ui work with proper layouts and widgets of a gui framework.
a ui design canbecreatedfromscratchoradaptedfromuidesignkits or existing apps guis and it is usually con veyed to developers in the form of design images to implement.
a ui design image depicts the desired ui elements and their spatiallayoutinamatrixofpixels.toimplementauidesignimage using a gui framework developers must be able to translate the pixel based depiction of the ui or parts of the ui into a gui skele ton.asillustratedinfigure1 aguiskeletondefineswhatandhowthecomponentsofaguibuilder e.g.
androidlayoutsandwidgets should be composed in the gui implementation for reproducing the ui elements and their spatial layout in the ui design image.
this gui skeleton is like the initial bootstrap instructions which enablesthesubsequentguiimplementation e.g.
settingupfont color padding background image and etc.
however thereisaconceptualgapbetweenauidesignimage i.e.
a ui design in a pixel language and the gui skeleton i.e.
the ui design in a language of gui framework component names .
to bridge this gap developers need to have a good knowledge of a gui framework s components and what visual effects interactions and compositions these components support in order to create an appropriate gui skeleton for different kinds of ui elements andspatiallayouts.ifdevelopersdonothavethisknowledge the gui implementation will become stucked because modern gui implementation cannot be achieved by hardcode positioning some texts images and controls.
this is especially the case for mobile apps that have to run on a wide range of screen sizes.
to overcome the knowledge barrier between ui design image and gui skeleton developers may attempt to figure out what and howtheguicomponentsshouldbecomposedforauidesignimagethroughatrial and errorapproach.althoughmodernguibuildersprovidestronginteractivesupport e.g.
drag drop what you seeis what you get forcreating aguiimplementation thistypeof trail and error attempt would be very cumbersome and frustrating.
first a mobile app s gui often involves many gui componentsand complex spatial layout see figure b .
second a complex acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden c. chen t. su g. meng z. xing y. liu imageview linearlayoutrelativelayoutscrollviewimagebuttontex t vi ew relativelayout interactiveplayerview figure translating a design image into a android gui skeleton not all guiframeworksupportsdozensoflayoutsandwidgets somemay beinterchangeable andflexiblecompositionoftheselayoutsand widgets.developerscaneasilygetlostduringthetrial and error of an unfamiliar gui framework.
alternatively developers can learn from gui framework tutorials or existing gui implementations.
to that end they must be able to find some tutorials or gui implementations that implement theui designsthat aresimilar tothe desiredui.
findingsuchtutorials or gui implementations through the ui design image is achallenging image search task.
it is also difficult to formulate a concise accuratetextqueryoftheuidesignandtheneededgui components for using information retrieval ir methods.
devel opers can also seek solutions for implementing a ui design from the developer community e.g.
stack overflow but they may not always be able to obtain useful advices in time.
the uis of apps can be very sophisticated to support complex tasks and they may undergo many revisions during the apps lifespan.
considering millions ofapps beingdeveloped and maintained automating the translation from ui design to gui implementation wouldbebeneficialformobileappdevelopment.sometools can automatically generate the gui implementation given a ui design image.
this automatic generative approach overcomes thelimitations of the trial and error search based or ask developercommunity approaches for transforming ui design image into gui skeleton.
however existing tools are rigid because they depend onhand designedvisualunderstandingandguigenerationtemplates which incorporate only limited ui image to gui skeleton translation knowledge.
inthiswork wepresentadeeplearningarchitecturethatdistills thecrowd scaleknowledgeofuidesignsandguiimplementations fromexistingappsanddevelopagenerativetooltoautomatically generate the gui skeleton given an input ui design image.
our generative tool can be thought of as an expert who knows a vast variety of ui designs and gui skeletons to advise developers what and how the components of a gui framework should be composed for implementing a ui design image.
to build this expert we must tackle two fundamental challenges.
first to be a knowledgeable expert the generative toolmust be exposed to a knowledge source of a vast variety of ui designs and gui skeletons from a large number of apps.
second to advise developers how to translate a ui design into a gui skeleton the generative tool must capture not only the ui elements con tained in a ui design image but it also must express how theseuielementsrelatetoeachotherintermsofacompositionoftheguicomponents.inthispaper wepresentanautomatedguiexplorationtechniquefortacklingthefirstchallengeinknowledge source and develop a neural machine translator that combinesrecent advances in computer vision and machine translation for tacklingthesecondchallengeinvisualunderstandingandskeleton generation.
the neural machine translator is end to end trainable using a large dataset of diverse ui screenshots and runtime guiskeletons that are automatically collected during the automated gui exploration of mobile app binaries.
weimplementanandroiduidatacollector anduseitto automatically collect pairs of ui images and gui skeletons from android apps.
we adopt this dataset to train our neural machine translator and conduct unprecedented large scale evaluationoftheaccuracyofourtranslatorforui image to gui skeleton generation.ourevaluationshowsthatourtranslatorcanreliably distinguish different types of visual elements and spatial layoutsin very diverse ui images and accurately generate the right guicomponents and compositions for a wide range of gui skeleton complexity.wealsoapplyourtranslatortotheuisof20android appsthatarenotinourtrainingset andthisstudyfurtherconfirms the generality of our translator.
through a pilot user study we providetheinitialevidenceoftheusefulnessofourapproachfor bootstraping gui implementations.
our contributions in this work are as follows wedevelopadeep learningbasedgenerativetoolforovercomingtheknowledgebarrierfortranslatinguiimagesto gui skeletons.
our generative tool combines cnn and rnn models forlearning a crowd scale knowledge of ui images and gui skeletons from a large number of mobile apps.
we develop an automated gui exploration framework toautomatically build a large dataset of ui images and gui skeletons for training the deep learning models.
we show our tool s robust visual understanding and gui skeleton generation capability through large scale experiments and provide initial evidence of our tool s usefulness by a pilot user study.
problem formulation we formulate the ui image to gui skeleton generation as a machine translation task.
the input ito the machine translator is a ui designimage canberegardedasauidesigninapixellanguage e.g.
rgbcolororgrayscalepixels .asshowninfigure2 themachine translator should be able to translate the input ui design image into a gui skeleton i.e.
a composition of some container components i.e.
the non leaf nodes and atomic components i.e.
the leaf nodes of a gui framework.
a gui skeleton can be regarded as the ui design in a gui frameworklanguagewhosevocabularyconsistsofthecomponentnames of the gui framework such as android s relativelayout textview imagebutton andtwospecialtokens e.g.
brackets and expressingthecompositionofguicomponents.asshowninfigure2 a component hierarchy can be represented as an equivalent token sequence via depth first traversal dft and using and to enclose a container s contained components in the token sequence.
in this work we use the token sequence representation of the gui skeleton as the output of the machine translator.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
from ui design image to gui skeleton a neural machine translator to bootstrap mobile gui implementationicse may june gothenburg sweden relativelayout view imagebutton textview linearlayout textview textview edittext edittext linearlayout edittext imagebutton button buttonrelativelayout view linearlayout imagebutton textview textview textview edittext edittext linearlayout edittext imagebutton figure an example of ui image to gui skeleton generation neural machine translator unlike normal machine translation tasks where both source and targetlanguagesaretextdata ourmachinetranslationtaskrequires jointprocessingofimageandtextdata.furthermore unliketext whichis asequence ofwords our inputui designimage contains the spatial layout information of ui elements and our output gui skeleton is a hierarchical composition of gui components.
taking intoaccountthesecharacteristicsofourtask wedesignaneural machinetranslatorwhichintegratesavisionconvolutionalneural network cnn a recurrent neural network rnn encoder and a rnn decoder in a unified framework.
as showninfigure3 givenaninputuiimage thevisioncnnextracts a diverseset ofimage featuresthrough asequence ofconvolution and pooling operations.
the rnn encoder then encodes the spatial layout information of these image features to a summary vector c which is then used by the rnn decoder to generate the gui skeleton in token sequence representation.
.
feature extraction by vision cnn to learn about visual features and patterns of numerous ui elements from a vast amount of ui images we need a sophisticated model with capability of visual understanding.
convolutional neuralnetworks cnns constituteonesuchclassofmodels.cnnis inspired by the biological findings that the mammal s visual cortex hassmallregionsofcellsthataresensitivetospecificfeaturesof visual receptive field .
these cells act as local filters over the inputspace and they visuallyperceive the worldaround them usingalayeredarchitectureofneuronsinthebrain .cnnis designed to mimic this phenomenon to exploit the strong spatially local correlation present in images.
a cnn is a sequence of layers that transform the original imagespatiallyintoacompactfeaturerepresentation called feature map .inourcnnarchitecture weusetwomaintypesof layers convolutional layer conv andpooling layer pool following the pattern conv pool.
we stack a few conv poollayers to createadeepcnn becauseadeepcnncanextractmorepowerful features of the input images .
we do not use fully connected layers since we want to preserve the locality of cnn features in order to encode their spatial layout information later.
.
.
convolutional layer a convolutional layer accepts an input volume i rwihidi wherewi hianddiare the width height and depthof the input volumerespectively.ifthefirstconvolutionallayertakesasinput an image then wiandhiis the width and height of the image anddiis for gray scale image or for rgb color image i.e.
red green blue channels respectively .
each cell of the input volume isapixelvaluefrom0to255.thecellvaluesoftheinputvolume forthesubsequentconvolutionallayersdependontheconvolution and pooling operations of the previous layers.a convolutional layer performs convolution operations using filters or kernels .
a filter is a neuron that learns to look for some visualfeatures e.g.
variousorientededges intheinput.thefilters inalayerwillonlybeconnectedtothelocalregionsoftheinput volume.
the spatial extent of this local connectivity is called the receptive field of the neuron i.e.
filter size .
the extent of the connectivityalongthedepthisequaltothedepth dioftheinput volume.theconvolutionoperationperformsdotproductsofafilter andthelocalregionsoftheinputfollowedbyabias b roffset.we apply the non linear activation function relu x max x totheoutputofaconvolutionoperation.weperformzero padding aroundtheborderoftheinputvolumesothattheinformationat the border will also be preserved in convolution .
belowisanexampleofapplyinga3 3filtertoa3 3regionof a gray scale image i.e.
di followed by reluactivation conv p 1p2p3 p4p5p6 p7p8p9 w 1w2w3 w4w5w6 w7w8w9 max summationdisplay.
n 1pnwn b wherepnis the pixel value and wnis the weight of the filer.
the resultingconvolutionvaluerepresentstheactivationofaneuron over a regionof the input image.intuitively this value represents the likelihood of a neuron seeing a particular visual feature over theregion.duringthemodeltraining thecnnwilllearnfiltersthat activatewhenthey see variousvisualfeatures suchasanedgeof some orientation on the first convolutional layer and shape like patterns e.g.
rectangle circle andmoreabstractvisualpatterns e.g.
image region text on higher layers of the network .
aconvolutionallayercanhave kfilters.the kfiltersthatare applied to the same region of the input produce kconvolution values.these kvaluesforma feature vector representingtheobservations of all kneurons over this particular region of the image.
a filterisappliedtoeachpossiblelocalregionsoftheinput specified by the stride s the number of pixels by which we slide the filter horizontally and vertically .
this produces a kernel map containing thevaluesofperformingaconvolutionofthe d thfilteroverthe inputvolumewithastrideof s.intuitively akernelmaprepresents the observations of a neuron over the entire image.
all kkernel maps form a feature map of a convolutional layer.
.
.
pooling layer poolinglayerstakeasinputtheoutputfeaturemapofthepreceding convlayers and produce a spatially reduced feature map.
theyreducethespatialsizeofthefeaturemapbysummarizingthe values of neighboring groups of neurons in thesame input kernel map.
a pooling layer consists of a grid of pooling units spaced s pixels apart each summarizing a region of size z zof the input volume.
different from the filters in the convlayers pooling units havenoweights butimplementafixedfunction.inourarchitecture we adopt max pooling which takes the maximum value inthez zregion.asthepoolinglayeroperatesindependentlyon every input kernel map the depth dimension of the output feature map remains the same as that of the input feature map.
poolinglayersprogressivelyreducethespatialsizeofthefeature maptoreducetheamountofparametersandcomputationinthe network and hence to also control overfitting.
meanwhile pooling also keeps the most salient information as it preserves the maximum value of each region in the depth slice.
it also benefits to the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden c. chen t. su g. meng z. xing y. liu y convolution pooling convolution pooling rnn encoder rnn decoderc ...feature extraction by vision cnn spatial encoding linearlayout button imageview ......skeleton generation figure architecture of our neural machine translator for ui image to gui skeleton generation invariance to shifting rotation and scaling.
even if the image is shifted rotated scaledbyafewpixels theoutputbymaxoperation will still stay the same when pooling over a region.
.
spatial layout encoding by rnn encoder our neural machine translator takes as input only a raw ui image requiring no detailed annotations of the structure and positions ofvisualelements.therefore giventhefeaturemapoutputtedby the vision cnn it is important to localize the relative positions of visual features within the input image for the effective generation of proper gui components and their compositions.
to encodes spatial layout information of cnn features we run a recurrent neural network rnn encoder over each of the feature vectors of the feature map outputted by the vision cnn.
the input to a rnn is a sequence of vectors e.g.
words in a sentenceintheapplicationofrnnstonaturallanguageprocessingtasks .
to apply the rnn model in our task we convert the feature map fm rwhdoutputtedbythevisioncnnintoasequenceof d dimensionalfeaturevectors disthedepthofthefeaturevector .
thelengthofthisimage basedsequenceis w h i.e.
thewidth and height of the feature map .
the conversion can be done by scanning the feature map along the width axis first and then the heightaxis orviceversa.inordertocapturetherow orcolumn informationinthesequence weinsertaspecialvectorattheend ofeachrow orcolumn canbethoughtofasa .
intext which are referred to as positional embeddings.
an rnn recursively maps an input vector xtand a hidden state ht 1to a new hidden state ht ht f ht xt wherefis a nonliner activation function e.g.
a lstm unit discussed below .
after readingtheendoftheinput thehiddenstateofthernnencoderisavector csummarizingthespatiallayoutinformationofthewhole input feature map.
modeling long range dependencies between cnnfeaturesiscrucialforourtask.forexample weneedtocapture the dependency between the bottom right and top left features of avisualelementinanimage basesequence.therefore weadopt long short term memory lstm .
an lstm consists of a memorycellandthreegates namelytheinput outputandforget gates.
conceptually the memory cell stores the past contexts and theinputandoutputgatesallowthecelltostorecontextsforalong period of time.
meanwhile some contexts can be cleared by the forgetgate.thisspecialdesignallowsthelstmtocapturelongrange dependencies which often occur in image based sequences.
.
gui skeleton generation by rnn decoder based on the rnn encoder s output summary vector c the target tokens of the gui framework language i.e.
the names of gui componentsandthespecialtokens and arethengeneratedby a decoder.
the token sequence representation of a gui skeleton start linearlayout relativelayout framelayoutscrollview image imagebutton image imagebutton imageviewimage imagebutton imageview... ... ... figure an illustration of beam search beam width canbeconvertedtoacomponenthierarchyviadepth firsttraversal dft as seen in figure .
as the length of the generated token sequencevariesfordifferentuiimages weadoptarnndecoder which is capable of producing a variable length sequence.
the hiddenstateofthedecoderattime tiscomputedas f ht yt c fisalsolstm .theconditionaldistributionofthenexttoken ytis computedas p yt angbracketleftyt ... y1 angbracketright c softmax ht yt c where softmax function produces valid probabilities over the language vocabulary.notethatthehiddenstateandthenexttokenarenot only conditioned on pastcontexts but also the summary vector c of the cnn features of the input image.
.
model training althoughourneuralmachinetranslatoriscomposedofthreeneural networks a vision cnn a spatial layout rnn encoder anda gui skeletongenerationrnndecoder thesenetworkscanbejointly trainedend to endwithonelossfunction.thetrainingdataconsists of pairs of ui images iand corresponding gui skeletons s see section for how we construct a large scale training dataset .
the gui skeleton is represented as a sequence of tokens s s0 s1 ... where each token comes from a gui framework language i.e.
the namesofguicomponentsandthetwospecialtokens and .each token is represented as a one hot vector.
given a ui image ias input the model tries to maximize the conditional probability p s i of producing a target sequence of the gui skeleton s s0 s1 ... .
since the length of sis unbounded it iscommontoapplythechainruletomodelthejointlogprobability overs0 ... sn where n is the length of a particular sequence as logp s i n summationdisplay.
t 0logp st i angbracketlefts0 ... st angbracketright at training time we optimize the sum of log probabilities over the whole training set using stochastic gradient descent .
rnn encoder and decoder backpropagates error differentials to its input i.e.
thecnn allowingustojointlylearntheneuralnetwork parameters to minimize the error rate in a unified framework.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
from ui design image to gui skeleton a neural machine translator to bootstrap mobile gui implementationicse may june gothenburg sweden .
gui skeleton inference aftertrainingtheneuralmachinetranslator wecanuseittogeneratetheguiskeleton sforauidesignimage i.thegeneratedgui skeleton should have the maximum log probability p s i .
generatingaglobaloptimalguiskeletonhasanimmersesearchspace.
therefore we adopt the beam search to expand only a limited set of the most promising nodes in the search space.
as illustrated in figure at each step t the beam search maintains a set of the k beam width best sequences as candidates to generates sequences of sizet .
at the step t the neural machine translator producesaprobabilitydistributionoverthelanguagevocabularyfor expandingeachofthecurrentcandidatesequencestonewcandidatesequencesofsize t .thebeamsearchkeepsonlythebest k sequences among all new candidate sequences of size t .
this processcontinuesuntilthemodelgeneratestheend of sequence symbol for the kbest sequences.
then the top ranked sequence is returned as the generated gui skeleton for the input ui image.
collecting large scale model training dataset to train our neural machine translator we need a large set of pairs of ui images and gui skeletons from existing mobile apps.
this requiresustoexploretheapps guis takeuiscreenshots obtain runtimeguicomponenthierarchies andassociatescreenshotswith component hierarchies.
although some tools e.g.
apktool ui automator mayassistthesetasks noneofthemcanautomate the whole data collection.
inspired by automated gui testing techniques wedevelopanautomatedtechnique termedstoat to explore the guis.
during exploration the ui screenshots paired with their corresponding runtime gui component hierarchies will be automatically dumped.
the dumped ui images and corresponding gui component hierarchies are like the example in figure .
.
exploring application guis mobile apps are event centric with rich guis and users interact with them by various actions e.g.
click edit scroll .
stoat emits various ui events to simulate user actions and automatically exploredifferentfunctionalitiesofanapp.tothoroughlyexplorean app sguis ourdatacollectortriestoidentify executable gui components e.g.
clickable long clickable editable scrollable onthecurrent uiandinfer actionsfromthesecomponents type.forexample if theuicontainsa button stoatcansimulatea clickactiononit.
however mobile platforms like android also permit developers to implementactionsintheappcode.forexample a textview widget may be registered with a longclick action which is invisible in theui.withoutincorporatingtheseimplicitactions wemayfail to execute some app functionalities i.e.
miss some ui images .
to overcome this issue we integrate static analysis method e.g.
to scan app code and detect actions that are either registered with ui widgets e.g.
setonlongclicklistener or implemented by overriding class methods e.g.
oncreateoptionsmenu .
figure shows an example of using stoat to explore an android app budget andcollecttherequireddata.startingfromthe main page whichliststhebalanceofeachexpensegroup e.g.
baby bill car stoat can click which opens the distributed page.
on the distributed page stoat can edit the amount of money distributed to an expense group e.g.
baby .
it can also scroll the a main page d setting page b balance page c distribution page click food edit baby scroll c scroll a long click fruit backclick click figure automatically exploring an app s guis distributed page toshowmoreexpensegroupsor click theback button the hardwarebackbuttonon thephone togo backtothe main page.
on the main page stoat can also click an expense groupwhichopensthe balance page or click thesettingbutton whichopensthe setting page.onthebalance page itcan longclick a transaction e.g.
fruit to select it.
.
prioritizing ui exploration figure5showsthereareoftenseveralexecutablecomponents actions onaui.todeterminewhichcomponent actiontoexecute stoat implementsa prioritized ui exploration method.weconducta formativestudyoftheguisof50googleplayappsfrom10categories e.g.
communications personal tools and summarize three key observations that can affect the ui exploration performance a frequency of action.
each action should be given chance to execute.
when an action is more frequently executed than others its priority should be lowered.
b number of subsequent uis.
if an action exhibits moresubsequent uisafter its execution its shouldbeprioritizedinfuturesothatmorenewfunctionalitiescanbevisited.
c type of action.
different types of actions should be given different priorities.forexample a hardware back orascrollactionshouldbe executedatrighttime.or itmaydiscardthecurrentuipageand prevent the execution of other normal actions e.g.
edit .
basedontheseobservations weassigneachactiononanexecutable component on a ui with an execution weight and dynamicallyadjustthisvalueatruntime.theactionwiththehighestweight value will be queued as next action to execute.
the weight of an action is determined by the formula below execution wei ht a ta ca fawhereais the action tais the weight of different types of actions for normal ui actions e.g.
click edit .
for hardware back andscroll and for menu selection cathe number of unexplored components on the current ui fais the times that ahas been executed and and are the weight parameters which can be determined empirically.
.
excluding duplicate uis after simulating an action our data collector takes the screenshot of the current ui and dump its runtimegui component hierarchy.
as seen in figure the same ui may be visited many times for example to execute different features on the main page o rt ov i e w thesameexpensegrouponthe balance page again.furthermore afteranaction theappmaystayonthesameui forexample after editing baby amount on the distribution page .
to collect as diverse uiimagesandguicomponenthierarchiesaspossible weshould avoid collecting such duplicate uis.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden c. chen t. su g. meng z. xing y. liu 0100200300400500600app number category a app category b gui skeleton complexity figure android ui dataset statistics to that end we compare the newly collected pair of ui image and gui component hierarchy angbracketleftinew snew angbracketrightwith already collected pairs angbracketleftia sa angbracketright.ascomparingimagesistime consuming wecompare component hierarchies.
we convert gui component hierarchies into their token sequence representation by a depth first traversal andthencomputethehashvaluesfortheresultingtokensequences.
onlyifthehashvalueof snewdoesnotmatchthatofany sa the newly collected pair angbracketleftinew snew angbracketrightwill be kept.
otherwise it will bediscard.forexample whengoingbackfromthe balance page to themain page the main page ui data will not be collected again.
as another example after editing baby amount to a different value thebalance page ui image will be slightly different but the gui component hierarchy remains the same.
therefore the balance pageui data will not be collected after editing baby amount.
someuiactionsmaychangetheui sruntimecomponenthierarchies even the app stays on the same ui after the actions like deletinganexpensegrouporscrollingthe distribution page sothat different numbers of expense groups are visible.
if the resulting gui component hierarchy has not been collected before the newly collectedpair angbracketleftinew snew angbracketrightwillbekept.insuchcases theuiimages beforeandaftertheactionsmaybesimilar butwillnotbeidentical.
constructing android ui dataset weimplementedourautomatedandroiduidatacollector stoat asdescribedinsection4.stoatusesandroidemulators configured withthepopularkitkatversion sdk4.
.
1280screensize to runandroidapps.ituses android ui automator todump pairs of ui images and corresponding runtime gui component hierarchies.soot anddexpler areusedforstaticanalysis.
stoatrunsona64 bitubuntu16.04serverwith32intelxeoncpus and 189g memory and controls emulators in parallel to collect data each app is run for minutes .
.
dataset of android application uis we crawl android apps with the highest installation numbersfromgoogleplay.5043appsrunssuccessfullybystoatand they belong to categories.
figure a shows the number of apps ineachcategory.theother957appsrequireextrahardwaresupport or third party libraries which are not available in the emulator.
stoat collected totally pairs of ui images and gui skeletons on average about .
pairs per app .
this ui dataset is used for training and testing our neural machine translator see section6 .thecollectedguiskeletonsuse291uniqueandroidgui components includingandroid snativelayoutsandwidgetsand those from third party libraries.
the box plots in figure b shows the complexity of the collected gui skeletons which varies greatly.
1dataset can be downloaded in ac distribution00.
.
.
.
.
.
.
.8covergae categorymonkey ours b average ac over app category figure effectiveness of automated gui exploration on average a gui skeleton has .
gui components .
containers non leaf components and .
layers the longest path from the root to a leaf component .
.
effectiveness of automated ui exploration to train a knowledgeable neural machine translator we need a diverse set of uis.
note that we already exclude duplicate uisduring data collection see section .
.
therefore the diversityof the collected ui data depends on stoat s ability to thoroughly explore an app s guis.
to confirm the ui exploration effectiveness of stoat we compared it with monkey an automated gui testing tool developed by google and released with android sdks.
we use activity coverage ac rather than code coverage criteria toevaluatetheui explorationeffectiveness.asandroid appsarecomposedofactivities whichareresponsibleforrendering uipages accanmeasurethepercentageofhowmanydifferent activities ui pages have been explored.
we randomly selected apps from our crawled apps and apply both tools on them.
to achieveafaircomparison weallocatethesameexplorationtime minutes for the two tools.
figure a shows the ac values achieved by the two tools in box plots and figure b presents the average ac values over different app categories.
on average stoat achieves0.513ac .
higherthanmonkey .
.amongall categories of these apps stoat also outperforms monkey.
evaluation we evaluate our neural machine translator in three aspects i.e.
accuracy generality and usefulness as follows.
.
implementing neural machine translator we implement the proposed neural machine translator with six conv poollayers in thecnn model.
the first convlayer uses 64filters andeachsubsequentlayerdoublesthenumberoffilers.
thisisbecausehigher convlayershavetocapturemoreabstract anddiversevisualfeatures andthusneedmoreneurons.
followingthecnnlayersizingpatterns forvisiontasks we setthefiltersize3 thestride1andtheamountofzeropadding2for convolutionallayers.thissettingallowsustoleaveallspatialdown samplingtothe poollayers withthe convlayersonlytransforming the input volume depth wise determined by the number of filters of aconvlayer .
for thepooling layers we use the mostcommon form of pooling layer setting i.e.
pooling units of size appliedwithastride2.thissettingdownsampleseverykernelmap by2alongbothwidthandheight discarding75 oftheneurons.
forthernnencoderanddecoder thereare256hiddenltsmunits to store the hidden states.
we implement our model based on the torch framework written in lua.
we train the model using randomly selected authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
from ui design image to gui skeleton a neural machine translator to bootstrap mobile gui implementationicse may june gothenburg sweden of the android ui dataset i.e.
pairs of ui images and gui skeletons andfine tunemodelhyperparameters thenumberof cnnlayers thenumberof convlayerfilersandthenumberofrnn hiddenstates usinganotherrandomlyselected3 ofandroidui dataset.themodelistrainedinanvidiam40gpu 24gmemory with epochs for about .
days.
at inference time our translator cangenerateguiskeletonsfor20uiimagespersecond whichis about times faster than existing ui reverse engineering techniques basedontraditionalcomputervisiontechniques e.g.
edge detection optical character recognition ocr .
.
evaluation metric let angbracketleftit st angbracketrightbe a pair of ui image and gui skeleton in the testing dataset.wesay stistheground truthguiskeletonfortheuiimage it.
lets be the generated gui skeleton for the itusing our neural machine translator.
both stands are in their token sequence representation see section in our evaluation.
thefirstmetricweuseis exact match rate i.e.
thepercentage of testing pairs whose stexactly match s .
exact match is a binary metric i.e.
0ifanydifference otherwise1.itcannottelltheextentto which a generated gui skeleton differs from the ground truth gui skeleton.
for example no matter one or differences between the two gui skeletons exact match will regard them as .
therefore weadoptbleu bilingualevaluationunderstudy as another metric.
bleu is an automatic evaluation metric widely usedinmachinetranslationstudies.itcalculatesthesimilarityof machine generatedtranslationsandhuman createdreferencetranslations i.e.
groundtruth .bleuisdefinedastheproductofn gram precisioni and brevity penalty bleu bp exp parenleftbig summationtext.1n n 1wnlo pn parenrightbig whereeach pnistheprecisionofthen grams i.e.
theratiooflength ntoken subsequences generatedby the machine translatorthat are also present in the ground truth translation.
wnis the weight of different length of n gram summing to one.
it is a common practice tosetnas4andwn n.bpisthebrevitypenaltywhich prevents the system from creating overly short hypotheses that may have higher n gram precision .
bpis c r otherwise e r c whereris the length of ground truth translation and c isthelengthofmachine generatedtranslation.bleugivesareal valuewithrange andisusuallyexpressedasapercentage.the higher the bleu score the more similar the machine generated translation is to the ground truth translation.
if the translation results exactly match the ground truth the bleu score is .
.
accuracy evaluation we use randomly selected of android ui dataset pairs ofuiimagesandguiskeletons astestdataforaccuracyevaluation.
none of the test data appears in the model training data.
.
.
overall performance as seen in figure a among all testing ui images the generatedguiskeletonsfor6513 .
uiimagesexactlymatch thegroundtruthguiskeletons andtheaveragebleuscoreover all test ui images is .
when the beam width is i.e.
greedy search .
furthermore for only of all test ui images our model fails to generate closed brackets.
this result shows that our model successfully captures the composition information of container components.
when the beam width increases to the exact match020406080100 12345value percentage beam widthbleu exactmatch a beam width020406080100 3456789value percentage hierarchy depthbleu exactmatch b the depth of component hierarchy 5value percentage gui componentbleu exactmatch c gui components020406080100 4value percentage containerbleu exactmatch d container figure impact of beam width and generation target complexity rateandtheaveragebleuscoreincreaseto63.
and86.
respectively.
however the increase after beam width is marginal.
therefore we use beam width in the following experiments considering a balance of computation cost and accuracy.
.
.
performance by generation target complexity as we show in figure b the ground truth gui skeletons i.e.
the targets to generate in our dataset vary greatly in terms of the number of gui components gui components the number of containercomponents containers compositions andthedepth ofcomponenthierarchy.thesethreedimensionsdefinethecomplexity of gui skeleton generation tasks.
to better understand the capability of our neural machine translator we further analyze the accuracy of our translator for the ground truth gui skeletons with different gui components containers and depth.
as guicomponentshasawiderange webucketthegroundtruth gui skeletons by intervals of gui components i.e.
... .
similarly we bucket the ground truth gui skeletons by3 intervals of containers i.e.
... .
we average the exact match rate or the bleu score of a generated gui skeleton and the corresponding ground truth gui skeleton for each bucket.
figure b c and d present the results.
intuitively the moreguicomponentsandthe morecontainers to generate the deeper of acomponent hierarchy to generate the more challenginga generationtask is.however our resultsshow that our translator works very well for a wide range of generation target complexity.the bleuscore remainsvery stable above when there are or more gui components to generate to containerstogenerate and or4to8depthofcomponenthierarchies to generate.
the test data in these ranges accounts for .
.
and .
of all test data respectively.
the exact match rateremainsaroundorabove60 forarelativelynarrowerrange of gui components containers or the depth of component hierarchies .
although our translator is less likely togenerateanexactmatchwhentheguiskeletontogenerateis too complex gui components containers and or depth it can still generate a gui skeleton that match largely with the ground truth i.e.
high bleu scores .
asurprisingfindingisthatourtranslator saccuracydegrades whentheguiskeletonsistoosimple 10guicomponents authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden c. chen t. su g. meng z. xing y. liu a text like image b ui elements on background image c deep component hierarchy d complexspatial layout figure examples of visual understanding and generation capability containers and or 3depth .wewillelaboratethecommoncauses for this result in section .
.
.
.
.
analysis of visual understanding and generation capability werandomlysampled10 ofthegeneratedguiskeletonsfor manualobservation.wemanuallystudythedifferencesbetween these generated gui skeletons and their ground truth input ui images .thissectionanalyzesourtranslator svisualunderstanding andguiskeletongenerationcapability.section6.
.4summarizes common causes of generation errors.
we find that our translator can reliably distinguish different typesofvisualelementsandgeneratetherightguicomponents.
figure a and b show two challenging cases.
figure a shows thesettinguiofapuzzlegameinwhichthegameicon highlighted in red box contains a table of characters.
our translator correctly recognizes the region in the red box as an image and generates aimageview foritinsteadof textview.theuiinfigure9 b containsabackgroundimagewithsomeuielementsintheforeground highlightedinredbox .ourtranslatorcorrectlyteasesapartthe foregroundelementsandthebackgroundimage ratherthanconsidering the ui elements as part of the background image.
an interesting observation is that our translator can reliably determine what text elements in ui images look like even when the texts are written in different languages e.g.
figure c and figure b .
during automated ui exploration different language settings of an app may be triggered so that we can collect ui imagescontaining textsof differentlanguages.for theguiskeleton generation task the exact text content does not matter much.
our translator canabstract language independenttext like features in ui images which makes it language independent.
wefindthatourtranslatorisrobusttocomplexspatiallayout of ui elements in a ui design.
figure c shows a ui design that requires6depthofcomponenthierarchy andfigure9 d showa ui design with gui components that vary in shape size and alignment.forbothcases ourtranslatorgeneratestheexact match gui skeleton as the ground truth.
we observe that many differences between the generated and ground truth gui skeletons represent alternative implementations rather than generation errors.
for example the ground truth for thecontrolintheredboxinfigure11 a isa togglebutton while our translator generates a switchfor the control.
togglebutton and switchwould be interchangeable for implementing this control.
figure11 b c and11 d showthreeexamplesofusingdifferent androidlayouts composecomponentsinalayout generated or a togglebutton or switch b using layout or hard positioning c listvieworrecyclerview d linearlayout or relativelayout figure examples of alternative implementations a many similar ui elements b similar texts on one line c partially visible ui elements d image likeuielements figure common causes of generation errors a low contrast background b little context information c displayed contentasuitogenerate d displayed contentasuitogenerate figure common causes of generation errors simple uis hardcode position components ground truth use listview generated ornewerapi recyclerview ground truth oruse linearlayout generated or relativelayout ground truth .whichoptionismore appropriateforanappdependsontheapp sdevelopmenthistory and usage scenarios but both options would produce the same spatial layout effects of the ui designs.
.
.
common causes for generation errors our qualitative analysis identifies some common causes of generation errors.
first when a ui has many similar ui elements e.g.
figure a our translator sometimes may not generate the exact same number of gui components.
second when several neighborhoodtextsinonelineusesimilarfontsandstyles e.g.
figure11 b ourtranslatormayregardthemasonetextcomponent.third when auielementisonlypartiallyvisible e.g.
coveredbysuspension menuinfigure11 c ourtranslatormaynotrecognizetheblocked uielement.fourth ourmodelsometimescannotdiscriminatesmall ui elements on top of a complex image especially when the ui elementshassimilarvisualfeaturestosomepartsoftheimage e.g.
the red box in figure d .
in addition we identify two common causes for the degraded accuracy on generating simple uis.
first ui elements in simple authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
from ui design image to gui skeleton a neural machine translator to bootstrap mobile gui implementationicse may june gothenburg sweden table the accuracy results for completely unseen apps id app name category installation image exactmatch bleu 1advanced task killer productivity 50m 100m .
.
2oovoo video call text voice social 50m 100m .
.
3colornote notepad notes productivity 50m 100m .
.
44shared entertainment 100m 500m .
.
5badoo meet new people social 50m 100m .
.
6mono bluetooth router music audio 1m 5m .
.
7automatic call recorder tools 50m 100m .
.
8flashlight hd led tools 50m 100m .
.
9solitaire game 50m 100m .
.
10avg antivirus communication 100m 500m .
.
11askfm social 50m 100m .
.
12color x theme zero launcher personalization 1m 5m .
.
13smart connect tools 100m 500m .
.
14history eraser privacy clean tools 10m 50m .
.
15pixlr free photo editor photography 50m 100m .
.
16soundcloud music audio music audio 100m 500m .
.
17office documents viewer free personalization 1m 5m .
.
18super backup sms contacts tools 5m 10m .
.
19photo effects pro photography 50m 100m .
.
20mobile security antivirus tools 100m 500m .
.
average .
.
.
uis often contain little context information for determining the appropriate gui components for them.
for example the green rectangle in figure b is actually a button but our translatormistakes it as an image.
other design factors like low contrast background in figure a could be more problematic for a simple uiwithlittlecontextinformation.second somemobileapps like mapnavigationandwebbrowser haveverysimplemainuiwith just several high level encapsulated components inside but the contentbeingdisplayedinthecomponentcanberathercomplex.
for example the navigation map in figure c and a web pagein figure d can be displayed by one gui component such as mapview andwebview.however ourtranslatormaymistakethe content displayed as part of the ui to implement and generate unnecessary basic gui components.
.
generality evaluation to further confirm the generality of our translator we randomly select another apps that are not in our ui dataset.
to ensure the quantity of test data apps that we select have at least millon installations popular apps often have rich content guis .
among theseapps werandomlyselect20appsforwhichourdatacollector collects more than ui images.
these apps belong to categories.wecollectintotal 1208uiimages in average60.4per app .
we set beam width as for generating gui skeletons.
table summarizes the information of the selected apps and theaccuracyresultsofthegeneratedguiskeletonsontheuiimagesof these apps sortedby exact match rate in descending order .
theaverage exact match rate is .
slightly lower than the average exactmatchrate .
ofandroiduitestdata andtheaverage bleuscoreis88.
slightlyhigherthantheaveragebleuscore .
of android ui test data .
these results demonstrate the generality of our translator.
wemanuallyinspecttheappswithlowexactmatchrate or bleu score .
we observe similar causes for generation errors as those discussed in section .
.
.
for example personal ization and photography apps have uis for users to upload and manipulatedocumentsorimages.similartothemapandwebpage examples in figure c and figure d our translator may mistakesomecontentdisplayedintheuisaspartoftheuistogenerate whichresultsinlowexactmatchrateorbleuscorefortheseapps.
however although the exact match rate is low for some such apps e.g.
office document viewer photo effects pro the bleuscore is high which indicates that the generated gui skeletons still largely match the ground truth.
theapp mobile security antivirus isaninterestingcase which has the lowest exact match rate and the lowest bleu score.
we findthatitsdevelopersuse linearlayout orrelativelayout rather randomlywhenthetwolayoutsproducethesameuieffect similar to the example in figure d .
in contrast our translator tends to useoneguicomponentconsistentlyforatypeofuispatiallayout for example just linearlayout which results in many mismatches betweenthegeneratedguiskeletonsandtheactualimplementation ofmobile security antivirus.
.
usefulness evaluation we conduct a pilot user study to evaluate the usefulness of the generated gui skeleton for bootstraping gui implementation.
.
.
procedures werecruiteightphdstudentsandresearchstaffsfromourschool.
weask eachparticipanttoimplement thesamesetof 5uiimages in android.
we select two relatively simple ui design images two medium complex images and one complex image for the study.
participants need to implement only a skeleton gui that replicates uielementsandtheirspatiallayoutinauiimage withouttheneed tosetupcomponents properties e.g.
font color padding etc.
.the studyinvolvestwogroupsoffourparticipants theexperimental groupp1 p2 p3 p4who start with the generated gui skeletons by ourtool andthecontrolgroup p5 p6 p7 p8whostartfromscratch.
according to pre study background survey all participants have morethantwo yearsjavaandandroidprogrammingexperience andhavedevelopedatleastoneandroidapplicationfortheirwork.
each pair of participants angbracketleftpx px angbracketrighthave comparable development experience so that the experimental group has similar expertise to the control group in total.
participants are required to use android studiotoavoidtoolbiasandhaveupto20minutesforeachdesign.
we record the time used to implement the ui design images.
after each ui image s implementation participants are asked to rate how satisfied they are with their implementation in five point likert scale not satisfied at all and highly satisfied .
after the experiment we ask a research staff not involved in the study to judge the similarity of the implemented skeleton guis to the respectiveuiimages alsofive pointlikertscale notsimilaratall and identical layout .
this judge does not know which skeleton gui is implemented by which group.
.
.
results box plot in figure shows that the experiment group implementstheskeletonguisfasterthanthecontrolgroup withaverage .
minutes versus .
minutes .
in fact the average time of the control group is underestimated because three participants fail to complete at least one ui image within minutes which means that they may need more time in the real development.
in contrast all participants inthe experimentgroup finishall thetasks within 15minutes.theexperimentalgrouprates90 oftheirimplemented guis as highly satisfactory point as opposed to highly satisfactorybythecontrolgroup.thisisconsistentwiththesimilarity ratings of the implemented guis to the ui images given by the judge.
on average the satisfactory ratings for the experiment and control group is .
versus .
and the similarity ratings for the experiment and control group is .
versus .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden c. chen t. su g. meng z. xing y. liu measures control experimental time minutes .
.
satisfactoriness .
.
similarity .
.
figure the comparison of the experiment and control group.
denotes p .01and denotes p .
we believe the above results are because the generated gui skeletons by our tool give participants a reliable starting point forguiimplementation.guidedbythegeneratedguiskeletons participants are clear about what components to use and how to composetheminaproperorder.then theymainlyneedtofixsome generation errors and make some adjustment of component layouts.
without the help of the generated gui skeletons the control grouphastodetermineinatrial and errormannerwhatcomponentstouseandhowtocomposethem whichresultsinthelonger implementation time and less satisfactory gui implementations.
to understand the significance of the differences between the two groups we carry out the mann whitney u test specifically designed for small samples on the implementation time satisfactory and similarity ratings.
the test results in figure table suggests that our tool can significantly help the experimental group implements skeleton guis faster p value .
creates more satisfactory guis p value .
that are more similar to the ui design images p value .
.
according to our observations starting with the generated gui skeletons even theless experienced participants in the experimental group achieve thecomparableperformancetothemostexperiencedparticipant inthecontrolgroup.althoughbynomeansconclusive thisuser studyprovidesinitialevidenceoftheusefulnessofourapproach for bootstraping gui implementation.
related work ui design and implementation require different mindset and expertise.
the former is performed by user experience designers and architectsviadesigntools e.g.
sketch photoshop whilethe latterperformedbydevelopersviadevelopmenttools e.g.
android studio xcode .
our work lowers the transition barrier from uidesignimages theartifactsfromuidesign toguiskeletons the starting point of gui implementation .
existing tools well support these two phases respectively but none of them supports effective transition from ui design images to gui skeletons.
supporting this transition is challenging due to the diversity of ui designs and the complexity of gui skeletons see figure b .some tools use blockwise histogram based features e.g.
scale invariant feature transform and image processing methods e.g.
edgedetection ocr toidentifyuielements fromimages.other tools e.g.
exportkit usethe metadataof ui elements in complex image formats exported by design tools e.g.
the psd file by photoshop to assist the transition from ui designimagestoguiimplementations.
however these toolsare rigid because they are built on limited hand designed rules for visual feature extraction and image to gui transformation.
different from these rule based tools our work is inspired by recentsuccessesofdeeplearning dl modelsinimageclassification captioning andmachinetranslation .dlmodelsareentirelydata driven anddonotrequiremanualruledevelopment.themostrelatedworkareimagecaptioningtech niques buttheytakeasinputnaturalsceneordigitaldocumentim agesandgenerateasequenceofwordsinnaturallanguages or markup ones e.g.
latex expressions .
in software engineering community some dl based methods have been proposed to generatecodegiveninput output examples partiallycompletedcode orfeaturedescriptions orgeneratecode comments orcode changecommitmessages .butourwork isthefirstdeeplearningbasedtechnique trainedwithreal world app ui data to convert ui requirements in the form of ui images into a hierarchy of gui components.
beforedeployingdeeplearningmodels ahigh qualitydatasetis requiredforthemodelstolearnimportantdatafeaturesforagiven task.
computer vision and nlp communities usually adopt crowdsourcingapproachtodevelopsuchdatasetsformodeldevelopment and benchmark .
a major contribution of our work is to developanautomatedprogramanalysistechniquetoautomatically collectlarge scaleuidatafromreal wordappsforimplementing and evaluating the deep learning based ui image to gui skeleton generation.
this makes our work significantly different from existingwork whichhassimilargoalbutaredevelopedbasedon artificial ui data generated by rules.
our approach is generative based on the ui design and gui implementationknowledgelearnedfromexistingapps.analternative waytoreusesuchknowledgeinexistingappsissearch basedmethods.
to use ir based code search methods an accurate concisedescriptionofuielementsandspatiallayoutinanimageis requiredbuthardtoachieve.itwouldbedesirabletosearchgui implementation by ui image directly but a challenge in image gui search is how to match two heterogeneous data.
the only workhaving this flavor is reiss s work .
but this work internally uses templates to transform an input ui sketch into a structuredquery for matching gui code.
these sketch to query templates limit the generality of the approach.
furthermore the fundamental differencebetweenourgenerativeapproachandsearch basedapproaches is that our approach can generate gui skeletons that are not present in a code base while the searching method can only return the information available in the code base.
conclusion thispaperpresentsagenerativetoolforui image to gui skeleton generation.
our tool consists of two integral parts a deep learning architecture and an automated ui data collection method.
our tool possesses several distinctive advantages it integrates feature extraction spatialencodingandguiskeletongenerationintoan end to end trainable framework.
it learns to abstract informativeuifeaturesdirectlyfromimagedata requiringneitherhandcraft features nor image preprocessing.
it learns to correlate uifeaturesandguicomponentsandcompositionsdirectlyfrom ui images and gui skeletons requiring no detailed annotationsof such correlations.
it is trained with the first large scale ui image gui skeleton dataset of real world android applications.
theseadvantagesgivesourtoolunpr ecedenteds peed reliability accuracyandgeneralityinover12000ui image to gui skeleton generation tasks.
in the future we will further test our tool with differentuiresolutionsandorientations.wewillalsoextendour neural network components to web design and implementation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
from ui design image to gui skeleton a neural machine translator to bootstrap mobile gui implementationicse may june gothenburg sweden