improving software text retrieval using conceptual knowledge in source code zeqi lin yanzhen zou junfeng zhao and bing xie key laboratory of high confidence software technologies ministry of education beijing china school of electronics engineering and computer science peking university beijing china beida binhai information research tianjin china flinzq14 zouyz zhaojf xiebingg sei.pku.edu.cn abstract a large software project usually has lots of various textual learning resources about its api such as tutorials mailing lists user forums etc.
text retrieval technology allows developers to search these api learning resources for related documents using free text queries but it suffers from the lexical gap between search queries and documents.
in this paper we propose a novel approach for improving the retrieval of api learning resources through leveraging software specific conceptual knowledge in software source code.
the basic idea behind this approach is that the semantic relatedness between queries and documents could be measured according to software specific concepts involved in them and software source code contains a large amount of software specific conceptual knowledge.
in detail firstly we extract an api graph from software source code and use it as software specific conceptual knowledge.
then we discover api entities involved in queries and documents and infer semantic document relatedness through analyzing structural relationships between these api entities.
we evaluate our approach in three popular open source software projects.
comparing to the stateof the art text retrieval approaches our approach lead to at least .
improvement with respect to mean average precision map .
index terms software text retrieval conceptual knowledge api graph semantic relatedness.
i. i ntroduction reusing apis of existing software projects is a common practice during software development.
a large software project usually has lots of various textual learning resources about its api such as tutorials mailing lists user forums etc.
when developers have issues in reusing an api they usually use text retrieval systems to search its api learning resources for documents that can help resolve these issues.
for example stackoverflow provides a text retrieval system for its users to search related questions.
most existing software text retrieval systems are based on lexical similarity especially the cosine similarity between tfidf vectors representing the query and the document in a vector space model .
this approach usually suffers from the lexical gap between queries and documents a document related to a query may be long and contain many keywords this paper is supported by national key research and development program of china grant no.
2016yfb1000801 national natural science fund for distinguished young scholars grant no.
and national natural science foundation of china grant no.
.which do not occur in the query.
as a result the document will be scored low and ranked poorly in the search results.
to address this problem researchers pointed out that documents should be ranked according to not only lexical similarity but also conceptual knowledge.
here conceptual knowledge is defined as concepts and entities in the real world and relationships between them.
recent years with the rapid development of large scale conceptual knowledge bases on the internet such as wordnet dbpedia yago etc many researches have proven that conceptual knowledge can improve text retrieval systems semantically.
however this idea is not easy to be applied in software text retrieval systems.
it is because that documents in api learning resources usually involve many softwarespecific concepts while these software specific concepts are rarely contained in universal conceptual knowledge bases on the internet.
our idea is that we can extract software specific conceptual knowledge from software source code and leverage it to bridge the lexical gap between queries and documents.
this is mainly because most software specific concepts are defined as api entities such as classes interfaces and methods in software source code the semantic relatedness among these software specific concepts is reflected in structural dependencies such as inheritances declarations and method invocations between api entities and we can leverage it to infer the semantic relatedness between queries and documents.
in this paper we present a novel approach for improving software text retrieval using software specific conceptual knowledge in software source code.
in detail firstly we represent conceptual knowledge in source code as an api graph in which nodes represent api entities and heterogeneous directed edges represent structural dependencies between them.
then we discover api entities involved in each query and document using recodoc a tool for recovering traceability links between an api and its learning resources and several word matching based heuristic rules.
each query or document is represented as a weighted collection of api entities.
after that we leverage transr a recent multirelational data embedding algorithm to analyze the api graph so that the pairwise api entity similarity could be measured.
for each document we score its semantic relatedness to the .
c ieeease urbana champaign il usa technical research123 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
query based on the pairwise similarity of api entities involved in them.
finally we use the score to re rank search results in software text retrieval systems.
comparing to the state of the arts our work makes the following contributions we represent each document in api learning resources as a weighted collection of api entities so that it can be explained by software specific conceptual knowledge in source code.
we measure the semantic relatedness between queries and documents in terms of software specific conceptual knowledge by leveraging multi relational data embedding technology.
we use it to improve document ranking in software text retrieval systems.
we evaluate our approach in three popular open source software projects.
comparing to the state of the art approaches our approach lead to at least .
improvement with respect to mean average precision map .
the rest of this paper is organized as follows.
we provide background on software text retrieval and conceptual knowledge based text retrieval in section .
section presents an example to illustrate our motivation.
section describes the details of our approach.
section presents experiments that evaluate our approach.
we conclude and discuss future work in section .
ii.
b ackground and related work we discuss background and related work from two aspects the first is improving text retrieval in software engineering the second is conceptual knowledge based text retrieval approaches in the information retrieval community.
a. software text retrieval text retreival is one of the most popular technologies used in software engineering where it has been successfully applied to seek out textual information from various software artifacts such as source code files e.g.
software documentation e.g.
bug reports e.g.
questions in online question answering communities e.g.
etc.
the performance of software text retrieval systems is usually suboptimal due to the lexical gap between queries and documents.
to deal with this problem researchers have proposed many query reformulation approaches to improve software text retrieval.
haiduc et al.
proposed an automatic query reformulation approach called refoqus for software text retrieval through learning from a sample of queries and relevant results.
panichella et al.
proposed an approach based on genetic algorithms for improving software text retrieval using topic models e.g.
latent semantic indexing lsi and latent dirichlet allocation lda .
some other approaches have been proposed to build software specific word similarity database and then use it to expand queries in software text retrieval.
for example tian et al.
proposed an approach for building software specific word similarity database through mining stackoverflow questions and answers usingpositive pointwise mutual information ppmi ye et al.
calculate semantic similarities between words from software documentation using word embedding techniques howard et al.
and yang et al.
infer semantically related words from software source code context wang et al.
infer semantically related software terms and their taxonomy through mining tags in freecode etc.
besides these lexical features a lot of rank assistant approaches have been proposed to improve document ranking in software text retrieval.
for example ponzanelli et al.
proposed an approach for searching stackoverflow discussions.
in this approach question score accepted answer score user reputation and question tags are leveraged to rank search results.
ye et al.
proposed an approach for searching source code files relevant to bug reports.
in this approach features like bug fixing recency and bug fixing frequency are leveraged to rank search results.
zou et al.
proposed a question oriented approach for improving document ranking in software text retrieval.
in this approach how search results are ranked depends on interrogatives such as how why and which in the queries.
these approaches are effectiveness in their software text retrieval tasks.
however most of these approaches need large annotated datasets usually are query document pairs for training.
comparing to them our approach has the advantage that it does not need any annotated dataset.
b. conceptual knowledge based text retrieval recent years with the rapid development of large scale universal machine readable knowledge bases on the internet such as wordnet1 dbpedia2 freebase3 google knowledge graph4 yago5 etc researchers have proposed many approaches to leverage conceptual knowledge in these knowledge bases to improve text retrieval e.g.
.
here conceptual knowledge means concepts and entities in the real world and relationships between them such as michael jackson publish song billi jean barack obama born in honolulu etc.
the basic process of leveraging conceptual knowledge to improve text retrieval is that first detect entities mentioned in documents second use relationships between these entities to measure semantic document relatedness third use the semantic document relatedness to improve document ranking in text retrieval.
these researches prove that it is feasible to use conceptual knowledge to improve text retrieval.
inspired by these researches we aim to improve software text retrieval using conceptual knowledge.
however in software text retrieval systems documents are extracted from various software artifacts thus lots of software specific entities which are not contained in universal knowledge bases are mentioned in these documents.
therefore authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i t wosample documents on stack overflow doc a lucene query parser to use filters for wildcard queries the newly asked question body my problem is how to parse wildcard queries with lucene that the query term is passed through a tokenfilter.
i m using a custom analyzer with several filters e.g.
asciifoldingfilter .
my problem is that whether lucene s queryparser detects that one of the sub queries is a wildcardquery ... doc b how to get a token from a lucene tokenstream?
a ground truth related question to doc a body i m trying to use apache lucene for tokenizing and i am baffled at the process to obtain tokens from a tokenstream.
the worst part is that i m looking at the comments in the javadocs of tokenstream.incrementtoken that address my question.
somehow an attributesource is supposed to be used rather than tokens.
i m totally at a loss.
can anyone explain how to get token like information from a tokenstream?
... we need to look for software specific conceptual knowledge resources.
tang et al.
proposed an approach for improving document ranking in software text retrieval using conceptual knowledge.
in this approach the software specific conceptual knowledge resources are domain ontologies constructed by domain experts.
as most software projects do not have domain ontologies the available of this approach is limited.
in software engineering community structural information in software source code are successfully leveraged in many automatic software engineering tasks to help software developers learn and understand software projects during software development maintenance and reuse e.g.
.
for example mcmillan et al.
studied how to leverage structural information in software source code as conceptual knowledge to recovery traceability links among requirement artifacts.
in this paper we further study how to leverage structural information in software source code as conceptual knowledge to improve the retrieval of api learning resources.
iii.
m otivating example in this section we present two sample documents selected from stackoverflow to further motivate our research.
these two sample documents are both questions about the api of apache lucene.
the first question6 denoted as doc a is lucene query parser to use filters for wildcard queries while the second question7 denoted as doc b is how to get a token from a lucene tokenstream?
.
table i shows titles and detailed descriptions of them.
doc b is a ground truth related question to doc a which was pointed out in the answer of doc a. consider the scenario that doc a is a newly asked question and the user want to find some questions related to it.
in the keyword matching based text retrieval system provided bystackoverflow doc b the ground truth related question to fig.
a brief illustration of how we can use conceptual knowledge in source code to capture the semantic relatedness between two documents doc a is not ranked high since it contains many keywords which do not occur in doc a. to address this problem we aim to leverage conceptual knowledge in source code to capture the semantic relatedness between doc a anddoc b. this idea is mainly composed of two phases first we represent each document as a weighted collection of api entities so that it can be explained by software specific conceptual knowledge in source code then we measure the semantic relatedness between these two documents in terms of software specific conceptual knowledge.
for instance consider this sentence in doc a my problem is how to parse wildcard queries with lucene that the query term is passed through a tokenfilter... class tokenfilter is mentioned directly in this sentence.
moreover as the document contains keywords parse wildcard query and term it is reasonable to guess that class wildcardquery class queryparser and class term are involved in this sentence as well.
based on this idea we present doc aas a collection of api entities.
similarly we present doc bas a collection of api entities containing class token class tokenstream method tokenstream.incrementtoken class attributesource etc.
we assign weights to these api entities based on term frequency inverse document frequency tfidf .
fig.
illustrates how we can use conceptual knowledge in software source code to capture the semantic relatedness between doc a anddoc b. this figure shows a partial api graph of apache lucene in which api entities involved in doc a are colored blue and api entities involved in doc b are colored orange.
though doc b contains many keywords which do not occur in doc a we can capture the semantic relatedness between them for api entities involved in them are structurally related in the api graph.
to sum up there are two major technical issues to be addressed how a document should be represented as a weighted collections of api entities how the relatedness between two weighted collections of api entities should be scored.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
workflow of our approach for improving software text retrieval using conceptual knowledge in source code iv.
a pproach in this paper we present a novel re ranking approach for improving the retrieval of related api learning resources or software text retrieval for short using software specific conceptual knowledge in software source code.
fig.
shows the workflow of our approach.
it consists of three phases api graph extraction conceptual document representation and document relatedness calculation.
api graph extraction.
this phase aims to extract an api graph from source code of the software project.
an api graph consists of api entities such as classes interfaces and methods in the software source code and various structural dependencies such as inheritances declarations and method invocations between them.
in our approach the api graph is used as the softwarespecific conceptual knowledge to capture the semantic relatedness between queries and documents.
conceptual document representation.
this phase aims to represent each query or document as a weighted collection of api entities.
in detail firstly we discover api entities involved in the document or query using recodoc approach and several keyword matching based heuristic rules.
then we assign weights to these api entities based on tf idf.
document relatedness calculation.
this phase measures the semantic relatedness between queries and documents based on the conceptual document representations.
firstly we leverage transr a recent multi relational data embedding algorithm to analyze the api graph so that the pairwise api entity similarity is measured.
then we use the pairwise api entity similarity as naturalbuilding blocks to calculate the conceptual relatedness between two weighted collections of api entities.
we use the result to score how much a document is semantically related to the query.
in software text retrieval systems we re rank the search results based on the following rules if a document gains a higher score it will be re ranked towards the top and conversely it will be ranked lower down.
a. api graph extraction in this paper we use astparser inorg.eclipse.jdt.core8to extract an api graph from source code of a java project.
in an api graph nodes represent api entities in the software source code and heterogeneous directed edges represent structural dependencies between them.
we define four different types of api entities class interface method andfield and we define eleven different types of structural dependencies between them see table ii .
b. conceptual document representation this phase aims to represent each query or document as a weighted collection of api entities.
here document means not only text segments in api learning resources but also queries.
this phase is composed of two parts api entity discovery and api entity weighting.
api entity discovery the most important thing in the conceptual document representation phase is to discover api entities involved in documents.
firstly we discover api entities directly mentioned in documents using recodoc .
then as api entities directly mentioned in documents are far away from enough for conceptual document representation authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii e leven different edge types in api g raph edge type description extend inheritance of a child class interface to its parent implement a class to interfaces it implements havemethod a class interface to its member methods havefield a class to its member fields throw a method to types of exceptions it throws fieldtype a field to its type parametertype a method to types of its parameters variabletype a method to types of variable objects constructed in it returntype a method to types of its return objects methodinvocation a method to methods invoked in it fieldinvocation a method to fields invoked in it we discover more api entities involved in documents based on keyword matching.
recodoc is a tool for recovering traceability links between an api and its learning resources.
we use it to discover api entities directly mentioned in documents.
for example consider the sentence in the motivating example my problem is how to parse wildcard queries with lucene that the query term is passed through a tokenfilter... .
recodoc will extract class tokenfilter from this sentence.
however api entities discovered by recodoc are far away from enough for conceptual document representation.
in the above example sentence recodoc discovers only one api entity class tokenfilter.
it is unacceptable for us to represent the sentence using only class tokenfilter since the majority of information such as keywords parse wildcard query term etc in the sentence is neglected.
therefore we need to discover more api entities for the sentence through leveraging these keywords.
for example it is reasonable to guess that class wildcardquery should be added to the collection of api entities for the sentence since this sentence contains keywords wildcard and query.
similarly class queryparser and class term should be discovered as well.
though these api entities are not directly mentioned in the sentence they represent the conceptual information hidden in its keywords which is important for the conceptual document representation.
therefore we propose a keyword matching based method for discovering api entities in documents besides those discovered by recodoc.
in this method a document is firstly split into words using non word characters such as white characters and punctuations .
then we filter english stopwords such as a in us etc out and stem the rest of words using porter stemming algorithm.
after these lexical pre processings we get the keyword set of the document.
for each keyword we define its candidate api entities as api entities whose identifiers contain the keyword.
the number of candidate api entities for a keyword may be large.
for example there are candidate api entities for keyword english such as class englishstemmer class englishanalyzer class englishpossessivefilterfactory etc.
taking such a large number of api entities into the consideration of conceptualdocument representation will not only increase the time cost but also hinder the performance of subsequent processes.
therefore we propose several heuristic rules based on our observations to reduce the number of candidate api entities including filter some api entities out according to keywords in their identifiers.
for example filter is a keyword in a document and class tokenfilter and class stopfilter are two of its candidate api entities.
the document contains another keyword token as well but it does not contain keyword stop .
the percentage of matched keywords in tokenfilter is higher than that in stopfilter.
therefore we filter class stopfilter out and keep class tokenfilter.
filter some api entities out according to their types.
we regard that concepts in the software project are mainly defined in classes interfaces.
therefore if the candidate api entity collection contains both classes interfaces and methods fields we will filter methods and fields out from it.
filter some api entities out according to the lengths of their identifiers.
for example class englishanalyzer and class englishminimalstemfilterfactory are two candidate api entities for keyword english .
we filter class englishminimalstemfilterfactory because its identifier is much longer than class englishanalyzer.
in our approach we define much longer as five letters longer or twice longer.
for each keyword in the document we obtain a collection of remaining candidate api entities.
we define the final collection of api entities for the document as the union of these collections.
api entity weighting we assign weights to keywords in the document using tf idf.
after that we re assign the weights of keywords to api entities corresponding to them wc w0 cp c02cw0 c0 w0 c x t2tctfidf t jctj in this equation wcrepresents the weight assigned to api entityc crepresents all api entities discovered in the document tcrepresents all keywords correspond to api entity c ctrepresents all api entities correspond to keyword t. after api entity discovery and api entity weighting each document is represented as a weighted collection of api entities.
we use the weighted collection as the conceptual representation of the document.
c. document relatedness calculation this phase is used to score the semantic relatedness between a query and a document using their conceptual document representations so as to re rank search results in software text retrieval systems.
it is composed of two parts api entity embedding and conceptual relatedness calculation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
api entity embedding this step aims to measure pairwise similarity between different api entities in the api graph.
it is an off line step.
in different software engineering tasks the pairwise similarity between api entities has different definitions.
for example in code clone detection the pairwise similarity between two api entities means how code fragments in them are similar to each other in concept location the pairwise similarity between two api entities means how they are structurally dependent to each other etc.
in our approach we define the pairwise similarity between two api entities as how their contexts in the api graph are similar to each other.
in this definition context means how the api entity is linked to other api entities in the api graph.
for example if class aand class b are both sub classes of class c the pairwise similarity between class aand class bwill increase if these two classes are both used as parameter types of method mtogether the pairwise similarity between them will increase as well.
moreover we regard that the pairwise similarity between api entities should be transitive in the api graph.
for example suppose that the pairwise similarity between class aand class bis high and methodm1and method m2uses them respectively.
in this case we think method the pairwise similarity between method m1and method m2should gain some increase.
given two api entities our approach calculates how their contexts in the api graph are similar to each other based on multi relational data embedding technology.
multi relational data embedding is a technology for learning vector representations of entities from multi relational data i.e.
data consisting of entities and different types of relationships between them .
first each entity is represented as a random vector in a shared low dimensional continuous vector space.
then gradient descent methods are used to optimize these vectors if two entities have the same kind of relationships to the same entity their vectors will be drawn closer to each other otherwise their vectors will pull away from each other.
after the global optimization these vectors of entities are finally adjusted to proper positions in the shared vector space and we can measure the pairwise similarity between two entities based on the distance between their vectors.
in our approach the api graph is a form of multi relational data thus we leverage multi relational data embedding methods to learn vector representations of api entities from it.
more specifically we use transr a widely used multi relational data embedding method for api entity embedding.
we normalize the distance between two api entities to the range of and denote it as dist e e2 .
then we define the pairwise similarity between these two api entities sim e e2 as dist e e2 .
conceptual relatedness calculation to compute the conceptual relatedness between two weighted collections of api entities cq i.e.
the query and cd i.e.
the document we modify the text to text similarity measure introduced by mihalcea et al .
the conceptual relatedness between an api entitycand a weighted collection of api entities cis defined as the maximum similarity between cand any api entityc0inc sim c c max c02csim c c0 an asymmetric similarity sim c q!cd is defined as weighted sum of similarities between api entities in cqand the entire collection of api entities in cd sim c q!cd x c2cqsim c c d wc the asymmetric similarity sim c d!cq is defined analogously by swapping cqandcdin the formula above.
finally the symmetric similarity sim c q cd betweem two weighted collections of api entities cqandcdis defined as the sum of the two asymmetric similarities sim c q cd sim c q!cd sim c d!cq we usesim c q cd to represent the conceptual relatedness between query qand document d and then use it to rescore search results in software text retrieval systems.
the new document scoring function is defined as s d s0 d sim c q cd in this document scoring function s0 d represents the original document scoring function in the software text retrieval system.
usually s0 d is defined as the lexical similarity between the query and the document for example the cosine similarity between tf idf vectors representing the query and the document in a vector space model.
we normalize s0 d to the range of and then combine it with sim c q cd linearly.
v. e mpirical evaluation to evaluate our approach we have conducted a set of quantitative experiments.
these experiments address the following questions rq1.
does our approach improve the retrieval of api learning resources?
the objective of our approach is to improve the retrieval of api learning resources.
we are concerned about how well our approach works.
to evaluate it firstly we study the effect of varying hyper parameters in our approach.
after that we make some comparisons between our approach and existing text retrieval approaches.
rq2.
how does the conceptual document representation component work in our approach?
in our approach we represent each document as a weighted collection of api entities.
we are concerned about the effectiveness of these conceptual document representations.
therefore we conduct experiments to study how different conceptual document modeling strategies will impact the reranking results.
rq3.
how does the semantic relatedness calculation component work in our approach?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii v arious statistics from testdata project name source code version api entities relationships documents keywords doc entities doc apache lucene .
.
.
.
.
apache poi .
.
.
.
jfreechart .
.
.
.
.
in our approach we embed api entities in the api graph as low dimensional continuous vectors then we calculate pairwise similarity between two different api entities.
we are concerned about the effectiveness of these pairwise similarities.
therefore we conduct experiments to study how different methods for measuring pairwise similarity between two different api entities will impact the re ranking results.
a. evaluation setting up we evaluate our approach in a real world software text retrieval task for three popular open source java projects apache lucene9 apache poi10andjfreechart11 respectively.
document collections stackoverflow provides good api learning resources for different software projects thus we build the document collections for the evaluation using stackoverflow data dump.
for each of the three projects we download all questions and answers about it from stackoverflow according to their tags.
among these data we model each question with its accepted answer as a document.
therefore we build a document collection for each of the three software projects respectively.
table iii shows the number of documents each software project contains the documents column and the average number of keywords the the document collection of each software project contains the keywords doc column .
api graph preparation for each project our approach extracts an api graph from its source code.
the project have many versions of source code while we select the newest version among them for api graph extraction.
table iii shows the source code version number the source code version column the number of extracted api entities the api entities column and the number of extracted relationships among these api entities the relationships column .
in our approach we represent each document as a weighted collection of api entities.
the entities doc column in table iii shows the average size of these collections.
for example .
.
means that on average the conceptual representation of each document contains .
api entities and .
of them are directly mentioned in the document.
test queries to evaluate our approach we conduct text retrieval process with real world queries from stackoverflow questions.
each project has test queries.
since our goal is to help software developers reuse apis we restrict that these questions should be about the project s api.
it means that we rule out questions like how to install this software?
or should i use this software or that software when selecting these questions.
for each of these questions we split it fig.
the distribution of the number of ground truth related documents fig.
the effect of varying on the map of our approach into words stem them filter stop words out and finally get keywords in it.
these keywords are regarded as a test query for searching the document collection.
ground truth related documents we build a standard text retrieval system based on the cosine similarity between tf idf vectors representing the query and the document in a vector space model.
for each test query we use the text retrieval system to obtain the top documents related to it documents corresponding to these test queries are excluded from the document collections .
the evaluation task is to re rank these documents.
we ask three undergraduate students majoring in computer science to annotate these querydocument pairs.
they are all familiar with java programming and these three software projects.
for each test query we show the corresponding question and its accepted answer to annotators.
then each annotator judges whether each of the retrieved documents is helpful to answer the question mainly according to the comparison of the accepted answer and the retrieved document .
if a query document pair is annotated authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
yes by all the three annotators we regard the document as a ground truth related document to the query.
fig.
shows the distribution of the number of ground truth related documents.
on average each test query has .
ground truth documents related to it.
baselines we denote our approach as ck i.e.
conceptual knowledge and compare it against four text retrieval methods tf idf.
this is the baseline method.
it ranks documents based on the cosine similarity between tf idf vectors representing the query and the document in a vector space model .
okapi bm25.
okapi bm25 is a widely used ranking function that extends tf idf in a probabilistic way .
we use the okapi bm25 function provided by apache lucene to compute okapi bm25 scores.
latent dirichlet allocation lda .
lda is a popular generative model for text documents that learns representations for documents as distributions over word topics .
we use the lda library provided by mallet12to train lda models.
then we score a query document pair using the cosine similarity of their latent topic distributions.
word embedding word2vec .
this method learns word embeddings from document corpora and uses the distance between word embeddings as natural building blocks to score a query document pair .
many supervised learning based approaches have been proposed for improving software text retrieval such as and .
we do not compare our approach against these approaches because they need large annotated datasets for training.
in practice it is usually difficult to collect and annotate enough query document pairs for a given software project.
evaluation metric we use the mean average precision map and the mean reciprocal rank mrr as our evaluation metrics.
map and mrr are both standard evaluation metrics in text retrieval.
mean average precision for a set of queries is the mean of the average precision scores for each query as shown in equation and .
map qp q 1avep q q avep q np k p k rel k number of related documents wherekis the rank in the sequence of retrieved documents p k is the precision at cut off kin the list and rel k is an indicator function equaling if the item at rank kis a related document zero otherwise.
for example consider that query has two ground truth documents related to it.
they are ranked at the 2nd and the 5th places respectively.
the average precision score avep of this query is .
mean reciprocal rank for a set of queries is the harmonic mean of ranks of the first related documents as shown in equation .
mrr qp q first q q b. rq1 the comparison with other approaches in our approach we combines the conceptual document relatedness with the original document scoring function linearly to rank search results.
is a hyper parameter that represents the weight of the original document scoring function.
for our approach uses only the conceptual document relatedness for our approach amounts to the standard tf idf based text retrieval approach.
fig.
presents the effect of varing on the map of our approach.
our first observation is that our approach outperforms the standard tfidf based text retrieval approach significantly.
take the dataset ofapache lucene as an example.
we get the highest map value .
when we set while the map value of the standard tf idf based text retrieval approach is .
.
furthermore fig.
shows that it is necessary to combine the conceptual document relatedness with the original lexical document similarity.
if we re rank retrieved documents using only the conceptual document relatedness i.e.
the map results will be substantially lower down.
for the three software projects the best performed settings are .
.
.
respectively.
in our approach there is another hyper parameter the vector dimensiond in the api entity embedding step.
we vary dfrom to and the experimental results show that this hyper parameter has limited effect on the performance of our approach.
therefore we set d in the evaluation.
fig.
compares our approach ck with four other text retrieval approaches tf idf okapi bm25 lda and word2vec.
table iv shows the average map and mrr results of these approaches across all the three software projects as well as average map and mrr improvement results relative to tf idf.
in fig.
and table iv all these approaches are ranked according to their performance.
comparing to the standard text retrieval system tf idf our approach obtains an improvement of .
in map while the second best performed approach word2vec obtains an improvement of .
in map.
therefore for rq1.
we can conclude that our approach outperforms the state of the arts.
c. rq2 conceptual document representation evaluation we compare the conceptual document representation component in our approach with the following two alternatives ck recodoconly.
this approach is the same as ck except that in this approach the weighted collection of api entities for each document contains only api entities authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a map b mrr fig.
average map and mrr comparison results table iv a verage map and mrr c omparison results across all the three software projects ck word2vec tf idf okapi bm25 lda map .
.
.
.
.
map improvement w.r.t.
tf idf .
.
.
.
mrr .
.
.
.
.
mrr improvement w.r.t.
tf idf .
.
.
.
directly mentioned in the document discovered by recodoc.
we compare our approach with ck recodoconly to study the effectiveness of the keyword matching based api entity expansion step in our approach.
ck unweighted.
this approach is the same as ck except that in this approach api entities discovered for each document are not weighted.
we compare our approach with ck unweighted to study the effectiveness of the tf idf based api entity weighting step in our approach.
fig.
shows the comparison results.
our approach outperforms ck recodoconly andck unweighted significantly in all the three software projects.
for example in apache lucene our approach gets the highest map value .
when while the highest map value is .
for ck recodoconly and .
for ck unweighted .
these experimental results prove that api entities directly mentioned in documents discovered byrecodoc are not enough to represent concepts involved in these documents.
it is necessary to expand the collection of api entities through keyword matching.
api entities involved in documents should be weighted based on tf idf so that the conceptual document representations can be more effective.
d. rq3 semantic document relatedness evalution we compare the semantic document relatedness component in our approach with the following two alternatives ck coupling.
this approach is the same as ck except that in this approach we calculate the pairwise similarity between two different api entities using the coupling measurement suit proposed by briand et al.
.
ck shortestpath.
this approach is the same as ck except that in this approach we calculate the pairwise similarity between two different api entities based on thedijkstra shortest path distance between them in the api graph .
fig.
shows the comparison results.
our approach outperforms ck coupling andck shortestpath significantly in all the three software projects.
these two alternatives perform poorly.
for example in apache lucene ck shortestpath gets the highest map value .
when which is just slightly better than the standard tf idf based text retrieval approach.
for coupling the best performed setting is which means that the re ranked search results are even worse than the original search results.
these experimental results prove that to capture the semantic document relatedness the pairwise api entity similarity should be defined as how their contexts in the api graph are similar to each other ck rather than how they are structurally dependent to each other ckcoupling andck shortestpath .
e. threats to validity there are a number of threats to the validity of our results.
construct validity are the evaluation metrics suitable?
we evaluate our approach using map and mrr which are both standard evaluation metrics in text retrieval.
we do not use precision at some cutoff precision k since the number of ground truth related documents of a test query may be less thank.
internal validity are there any experimental biases?
first the test query construction and ground truth related document annotation.
we extract test queries from real world authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a apache lucene b apache poi c jfreechart fig.
the comparison between different conceptual document representation strategies a apache lucene b apache poi c jfreechart fig.
the comparison between different api entity pairwise similarity calculation strategies questions about software apis in stackoverflow rather than use designed test queries.
we ask three undergraduate students majoring in computer science to annotate ground truth documents related to test queries.
a document is regarded as a ground truth related document to a test query only if all the three annotators admit it.
second the parameter settings in our approach.
we conduct detailed experiments in which the main hyper parameter varies from to with the step size of .
.
the bias is minimized by these efforts.
external validity could the results be generalized?
first we conduct experiments in three popular open source software projects respectively to ensure that the results can be generalized in different software projects.
second we construct the evaluation dataset using real world api learning resources extracted from stackoverflow.
for each project we evaluate our approach using test queries.
the sizes of the document collections for the three projects are respectively.
these queries and documents are written by many different people and they are all free text without special formatting rules.
therefore the results can be generalized in different document writing styles.
vi.
c onclusion and future work in this paper we present a novel approach for improving software text retrieval using software specific conceptual knowledge in software source code.
we extract an api graphfrom software source code and use it as software specific conceptual knowledge of the software project.
then we represent each query or document as a weighted collection of api entities in the api graph.
after that we calculate the semantic relatedness between queries and documents through leverage multi relational data embedding technology.
finally we use the semantic relatedness to re rank search results in software text retrieval systems.
detailed experiments are conducted on stackoverflow dataset to evaluate the effectiveness of our approach.
comparing to the state of the art text retrieval approaches our approach leads to at least .
improvement with respect to mean average precision map .
in the future we plan to further study how to improve conceptual document representation and api entity embedding in the proposed approach.
in parallel we will explore how this approach could be adapted to more types of textual artifacts in software such as bug reports source code comments requirement specifications etc.