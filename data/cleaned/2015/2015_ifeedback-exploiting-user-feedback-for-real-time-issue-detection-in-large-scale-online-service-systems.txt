ifeedback exploiting user feedback for real time issue detection in large scale online service systems wujie zheng haochuan lu yangfan zhou jianming liang haibing zheng y uetang deng school of computer science fudan university shanghai china shanghai key laboratory of intelligent information processing shanghai china tencent inc. shenzhen china abstract large scale online systems are complex fastevolving and hardly bug free despite the testing efforts.
backend system monitoring cannot detect many types of issues such as ui related bugs bugs with small impact on backend system indicators or errors from third party co operating systems etc.
however users are good informers of such issues they will provide their feedback for any types of issues.
this experience paper discusses our design of ifeedback a tool to perform real time issue detection based on user feedback texts.
unlike traditional approaches that analyze user feedback with computation intensive natural language processing algorithms ifeedback is focusing on fast issue detection which can serve as a system life condition monitor .
in particular ifeedback extracts word combination based indicators from feedback texts.
this allows ifeedback to perform fast system anomaly detection with sophisticated machine learning algorithms.
ifeedback then further summarizes the texts with an aim to effectively present the anomaly to the developers for root cause analysis.
we present our representative experiences in successfully applying ifeedback in tens of large scale production online service systems in ten months.
i. i ntroduction online service systems especially those of large service providers e.g.
microsoft facebook and google are getting larger in scale and more complex in its functionality.
they typically provide various services to different applications serving millions of users concurrently1.
serving huge amount of users is a huge challenge to the reliability of such systems.
in typical industry practice e.g.
that in microsoft service developing teams usually test services in a smaller in house scale.
in the production environments massive users will exercise the service with unpredictable circumstances that may not be expected during development.
such a gap between the environments of development and deployment is a notorious typical source of bugs .
it has been well accepted that testing can also be conducted in the wild after a software artifact is released to the users e.g.
especially for online service systems.
in a testing prospective the large amount of user requests to a 1for instance services provided by azure can be found at actually exercise the system extensively.
specificallytailored system indicators e.g.
performance indicators such as the latency of critical method calls collected during system operations can facilitate developers in fault removal and further service upgrade .
however defining such specific indicators is labor intensive which largely depends on a deep understanding to the target service.
moreover a monitoring on such conventional indicators can only discover issues that greatly affect the backend performance.
those issues related with frontend user interface bugs with few backend impact or errors from third party co operating systems can hardly be successfully detected not to mention the newly occurring issues that have never been encountered before.
unlike existing efforts that rely on monitoring servicespecific system indicators we show that with a proper text processing natural language texts in user feedback can also serve as a good timely indicator of service runtime issues.
this experience paper introduces our efforts to analyze user feedback automatically generate key performance indicators and accordingly achieve real time identification on runtime issues in large scale online service systems.
we present such a system namely ifeedback as well as our experiences in applying ifeedback in tens of online production services each serving tens of millions of users concurrently.
user feedback typically written in natural language texts by end users are usually complaints of bad experiences e.g.
buggy behaviors and annoying features.
usually such feedback may be given via certain channels provided by each service e.g.
app built in user response interface when a user encounter service issues.
it is long been proposed that user feedback can assist software maintenance .
however existing work in general focuses on conducting exhaustive text mining to provide an offline report that helps manual analysis of software issues.
ifeedback in contrast shows that it is feasible to perform a real time issue detection by generating key performance indicators automatically based on user feedback which in turn can be monitored so as to efficiently identify service runtime issues online in a timely manner.
however for real world production service systems to enable a fine grained identification of service runtime issues 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the number of such key performance indicators should be huge.
ifeedback faces huge challenges includeing how to properly generate a large amount of effective indicators that can best describe potential issues automatically h o wt o identify service issues by monitoring such massive indicators efficiently in a timely manner.
to address these challenges ifeedback specifically extracts word combinations to automatically generate such indicators from tremendous user feedback text.
it wipes off extensive noisy non informative information.
then to automatically identify issues ifeedback collect features from the indicators historical trend and the corresponding text content.
such features are further used to achieve the issue detection via a machine learning approach i.e.
a class classification model determining whether or not an issue occurs.
conventional approaches in detecting issues in a set of indicators typically resort to unsupervised statistical methods which locate outliers in time series trends.
in contrast ifeedback models the problem as a class classification problem.
this also allows us to take advantage of other well designed features in the feedback e.g.
text diversity to train the classifier with high accuracy.
our experiences show that ifeedback can achieve very satisfactory results in detecting service issues comparing with traditional unsupervised methods.
we summarize the contributions of this paper as follows.
we present ifeedback a tool that can generate key performance indicators automatically based on user feedback.
it can provide real time issues detection for large scale online services based on the indicators.
we propose an effective detection approach via a machine learning based classifier.
unlike traditional regressionbased approaches it is more suitable to handle the noisy nature of feedback data via a set of hybrid features from indicator values and feedback text.
thus it can achieve satisfactory results in both efficiency and accuracy.
such a classification based perspective can shed light to other system operation maintenance tasks.
we show that ifeedback is an effective tool via presenting our successful experiences in applying ifeedback to tens of online production services each serving tens of millions of users concurrently.
we expect such experiences can inspire aiops in real world production service systems.
the rest of the paper is organized as follows.
section ii introduces the related work.
in section iii we discuss some preliminaries and our design motivations.
sections iv overviews our ifeedback design.
section v elaborates our detailed design considerations in ifeedback .
we present our experiences in applying ifeedback in section vi to show its effectiveness.
section vii provides some further discussions on the future work and limitations of this work and we conclude the paper in section viii.
ii.
r ela ted work artificial intelligence for it operations aiops is a concept that introduces intelligent algorithmic mechanisms e.g.machine learning into it operations .
it helps realize automatic it operations with an aim to reduce human efforts.
extensive research work has suggested aiops in solving different practical problems e.g.
in failure handling in log analysis and in resource arrangement .
ifeedback similarly is a tool designed for aiops which aims at automatically detecting emerging software system issues in large scale online service systems.
ifeedback mainly considers user feedback texts regarding each specific online service as data source and detects reported issues accordingly.
it has long been suggested that user feedback texts contain significant information and can be of great value in supporting software development and maintenance .
for instance di sorbo et al.
propose surf a system that captures user requirements in user feedback which can in turn facilitate developers in performing software maintenance tasks.
it relies on machine learning techniques to conduct topic classification and then summaries user requirements.
vu et al.
propose puma that uses a phrase based clustering approach to extract user opinions in app reviews.
however existing work generally focuses solely on generating specific topics i.e.
unveiling user requirements .
gao et al.
propose idea in which a topic modeling algorithm is introduced to automatically interpret topics in user reviews and perform issue mining accordingly.
issue mining is a difficult task.
current approaches in general consider user feedback as static data and perform offline issue mining without considering the time related information e.g.
the topic trend in the feedback texts.
they are not capable of monitoring issues in a real time manner and raising alarms timely.
in contrast real time issue detection critical to largescale online service systems is what ifeedback focuses on.
one key functionality of ifeedback is system runtime anomaly detection.
it is also a critical task in aiops attracting lots of efforts in both industrial practitioners and academic researchers .
specifically anomaly detection in key performance indicators kpis has been widely used to monitor systems and indicate issues .
for example laptev et al.
propose egads which achieves both accuracy and scalability in anomaly detection on time series kpis with a collection of anomaly detection and forecasting models.
liu et al.
propose opprentice.
it adopts machine learning techniques to classify kpis anomaly within a given time window.
xu et al.
introduce v ariational auto encoder v ae to realize unsupervised anomaly detection.
such a mechanism can be successfully deployed to monitor periodic kpis in web applications.
however building proper kpis is generally labor intensive which requires domain specific expertise.
no experience on building such kpis on user feedback texts has been introduced before.
in contrast ifeedback takes an automatic approach.
ifeedback is the first approach that transforms real time issue detection automatically from a feedback processing problem to a kpi monitoring problem.
it automatically generates a large amount of indicators from user feedback texts where system anomaly is detected with a set authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of specific tailored machine learning techniques.
in this way ifeedback realizes real time issue detection in user feedback.
it is worth noting that there is also a body of work which conducts bug detection bug classification and bug prediction in bug or issue reports.
examples include idice proposed by lin et al.
which monitors attributes within issue reports and identifying particular attribute combinations to characterize emerging issues.
however such reports require to contain a strict structural data form which directly provides the required attributes.
such forms may vary among different services.
user feedback texts in contrast are written in non structural natural languages.
ifeedback is designed to deal with such non structural feedback texts allowing it to be adaptive to different types of services.
iii.
p reliminaries and motiv a tions we will first illustrate the target systems i.e large scale online service systems in which ifeedback is specifically designed to conduct emerging issue monitoring and detection.
since ifeedback is based on user feedback to perform such a task we will then introduce typical user feedback in our target services.
a. large scale online service systems and their defects in recent years many service providers have launched online cloud based services for the public.
such online services usually serve a large amount of users.
for example the target service system of this work has billions of monthly active users.
typically the online services are implemented with complicated programs and deployed in an extra large scale i.e.
on a cloud consisting of hundreds of servers.
online service systems typically work in a manner.
although in industrial practice a service must undergo a careful testing phase not all system defects can be removed before online deployment especially due to its complications.
for example defects may manifest when there is a sudden boosting of concurrent requests or when a corner use case is encountered.
when a defect manifests and causes bad user experiences it is a critical task for service developers to fix the defect in a timely manner to avoid a broader influence.
however our experiences in operating large scale cloudbased service systems reveal that it usually takes quite a long time before the developers are aware of the occurrence of an issue.
in this regard backend systems can adopt an indicator mechanism where system runtime metrics i.e.
kpis can be designed to reflect the conditions of service.
for example the faulty behavior of a service caused by a sudden increase of concurrent user requests can be detected by an indicator built to reflect backend workload.
however such mechanisms can only cope with system defects that have significant impact on system runtime metrics.
faulty behaviours of the services at the user end for example those related to user interface may not manifest as distinct abnormal behaviors of the backend systems.
hence it is a more efficient way to detect user end faulty behaviors in the user end.
user feedback is then a natural data source to such a task.
in this work we will show how weuser a i met some problems when purchasing with wechat pay user b payment failure when using payment code user c why can t i transfer money?
user d what s wrong with my wechat pay account?
there should be some money.
but i can t find it.
fig.
.
examples of a major form of feedback build system indicators from user feedback and how we can rapidly identify different types of defects occurred in largescale cloud based service systems.
b. user feedback user feedback of a service is a direct reflection of user s experience on using the service.
in modern online service systems especially those for mobile applications when a user encounter unexpected troubles she may conveniently report the issues through a feedback interface.
naturally not all users would like to report issues.
but given the large number of users typically millions of users of an online service system the daily user feedback is still huge in volume.
user feedback is typically in non structural raw texts.
figure shows an example of a list of feedback texts for an online service product2.
each text is provided by an individual user.
it is long been suggested that user feedback can be used to evaluate the usability of services .
but retrieving useful information from such non structural raw texts and detect issues accordingly is a quite challenging task.
in our practical experiences we found that over of the user feedback items collected from over tens of online services are on bad user experiences.
although it seems promising that we can perform issue mining from the tremendous bad userexperience feedback.
however our analysis shows that most of reported bad experiences are caused simply by the incorrect user interface operations which can be solved easily by a better user guidance.
in other words they are not related to an emerging system issue.
solving such a challenging task is the major aim of ifeedback .
iv .
ifeedback overview figure shows the overall system design of ifeedback .
the workflow of ifeedback mainly consists of two phases service runtime indicator building phase and machine learning based issue detection phase.
ifeedback takes real time user feedback texts as input.
it performs a specific tailored natural language processing approach to automatically generate tremendous fine grained system indicators.
ifeedback then applies a rule based filter to select possible indicators of faulty behaviors as candidates for the issue detection phase.
in the issue detection phase we focus on these candidate indicators and accordingly perform feature engineering on them.
in this way ifeedback generates vectorized features to describe the service runtime condition.
then a machine learning based classifier is applied on these features to quickly 2the original feedback texts are written in chinese.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
ifeedback overview perform anomaly detection.
its results are fed into a clustering algorithm that further summarizes similar anomalies inorder to minimize human inspection efforts.
the clusters ofanomalies are then reported as a clear summary of potentialissues detected in the user feedback.
they can facilitate furtheranalysis of potential service issues.
we summarize the key steps shown in fig as follows where their technical details and practical concerns will be elaborated in section v. indicator generation extracting millions of indicators automatically from non structural user feedback texts isone key concern of ifeedback .
user feedback texts are noisy in nature which contain much useless informationin practice.
we synthesize several natural language pro cessing techniques to combat these noisy data.
ifeedback can automatically extract meaningful word combinationsas indicators.
details will be described in section v.a.
indicator filtering the indicators generated with user feedback texts can be large in volume.
it is not cost effective to monitor every indicator.
ifeedback then performs a rule based filtering strategy based on historicaldata.
the filtering approach can remove most indicatorsand retain possible indicators of faulty behaviors.
ifeedback considers the occurrences of these candidate indicators as metrics to be monitored.
section v.b elaboratesuch a filtering approach.
anomaly detection a conventional approach to performanomaly detection is to model the problem as a regressionproblem.
the measurement values i.e.
the metrics of theindicators are compared against their prediction valuesin each given time interval of interest.
in contrast wemodel the problem as a classification problem where the features constructed from the candidate indicators historical trends as well as the content information collectedfrom corresponding feedback texts.
in this way previoushuman experiences can be considered as labels to trainthe classifier.
as a result it can be more informative.the anomaly detection scheme as well as the prerequisitefeature generation approach will be described in sectionv.c.
issue clustering several different anomalous cases mayreflect the same service issue.
a summary after anomalydetection is necessary to minimize human inspectionefforts.
we first take the corresponding feedback texts foreach anomalous case.
they are vectorized as auxiliarydata based on the context of the indicators i.e.
the word phases .
a distant based clustering algorithm is then designed to group the anomalous cases based onthe vectorized data.
each cluster is then deemed as asummary of a detected issue in the user feedback.
suchan issue clustering process can improve the efficiency ofhuman inspection in analyzing potential service issues.secton v discusses the details.
v. i mplementa tion details in this section we will elaborate the ifeedback design together with its technical details.
a. indicator generation as illustrated in section iii user feedback texts are written by end users on their experiences when using a service.
unfortunately end users are typically not well trained to provide tidy informative feedback.
in fact our field experiences show that there is large amount of irrelevant information in thefeedback texts.
for example we have found a lot of uselessfeedback such as irrelevant descriptions.
some users wouldeven complain about their own life in their feedback.
suchfeedback will result in unnecessary indicators and misleadthe issue detection task.
as generating indicators from userfeedback texts is critical to ifeedback performance in detecting issues we should first minimize the negative impact of noisy useless feedback.
moreover the indicators generated from the feedback texts should help ifeedback to achieve both high accuracy and wide coverage towards detecting service issues.
in other words theindicators should contain enough information that can leadto effective issue detection.
in addition they should also becapable of covering issues that are rarely occurring or newly appearing.
finally we should design good metrics for the indicators that can encode enough information for the issue detection task.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
user feedback texts on wechat moments we illustrate the measures ifeedback takes as follows to combat those challenges.
preprocessing feedback texts feedback texts are first preprocessed similarly as in conventional natural language processing.
for example for languages like chinese which does not have explicit word segmentation we perform word segmentation with a state of the art word segmentation approach .
moreover stop words are removed.
then ifeedback starts to extract informative feedback.
word combination based indicators wci we propose the word combination based indicators wcis which build indicators with a combination of words.
specifically for each feedback text we combine every two words together as a wci after text preprocessing.
wcis provide more information.
for example the wci may indicate something goes wrong in the payment process.
may present an issue when displaying pictures.
may infer that the user interface encounters problems.
as user feedback are usually written as short texts wci shows highly recapitulative indications toward issues.
for comparison a single word may be a commonly used one.
its occurrence data in the user feedback can hardly tell that something is specially concerned by the users.
for example the word payment may appear a lot in user feedback throughout the runtime of the service.
in contrast the combination of words may occur far less frequently which can greatly help issue detection.
for example occurs far less frequently than payment does.
if it appears more often i.e.
in case that its number of occurrence increases dramatically we can easily detect such an anomaly and infer that fingerprint based payment process may contain issues.
wcis can also achieve a wide coverage of issues.
we can build billions of indicators with every combination of words we may easily identify and distinguish different sources of issues.
comparing with topic clustering based methods such a huge amount of fine grained indicators built from word combinations are more capable to capture issues in fig.
.
example historical occurrence trend more details.
figure shows an example where complaints like videos in wechat moments get stuck appear with the overwhelming complaints like can t see pictures in wechatmoments .
summarizing them with the topic problems with wechat moments is too general and provide less information for issue detection.
but if we use wcis we can identify the first issue with combinations like and and the second issue with the combinations like .
as a result wcis can capture both issues.
wcis can automatically adapt to newly occurring issues.
traditional approaches based on static text analysis models usually fail to deal with unseen new issues in feedback texts or require extra cost to achieve such adaption e.g.
b y retraining the model .
in contrast wcis can be automatically generated from any texts by combining words.
a new issue may automatically result in a new set of wcis which can in turn be monitored.
note that an important concern is the cold start problem caused by the lack of historical data for newly occurring issues.
we will further discuss how we solve it in section v c. metrics for wcis how to quantify wcis so as to monitor their values is important to ifeedback .
we choose the occurrence of each wci among feedback texts as a metric to describe the wci.
we count the occurrence number within a certain time period.
the results form a historical occurrence trend hot curve.
figure shows an example.
as we can see every wci is corresponding to a specific hot.
the hot curve of a wci clearly shows its occurrence numbers in each time interval and their changes.
thus we can distinguish dramatic changes from reasonable changes and identify the occurrence of an issue.
we will discuss how we extract such features from the hot and form feature vectors for each interval of the service runtime.
such vectorized data can then be analyzed by our machine learning based anomaly detection approach.
with the wcis and the corresponding hots ifeedback can successfully transform issue detection problem from a text analysis problem to an anomaly detection problem in a set of vectorized data.
we will show how this problem can be solved authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
effectively with a rapid machine learning based approach.
this is more practical for large scale online service systems.
b. indicator filtering according to the indicator generation approach introduced above billions of wcis along with their hots can be built for further anomaly detection.
however though generating wcis can be an efficient process since few computation is needed applying machine learning based anomaly detection on billions of hots can be an extremely time consuming task.
in our field experience we have found that hots of most wcis are usually stable.
only those with dramatic changes may indicate potential issues.
therefore a rapid algorithm to filter out those wcis with stable hots can significantly reduce the numbers of wci candidates to be analyzed by the machine learning based anomaly detection and consequently improve its efficiency.
in particular we propose the filtering strategy as follows.
r wci t h o t t 1and hot t 2 hot o therwise wherer wci t evaluates whether a wci should be retained or not after filtering given a specific time interval t. it should be filtered out if r wci t is .
herehot tdenotes the occurrence number of the wci in the time interval t wheretis typically one hour in our practice.hot denotes the average occurrence number of the wci in the past given the same length of time intervals.
we calculate hot with the data in the past seven days before t. we consider the manifest of an issue will result in more complaints in the user feedback texts.
therefore we first considerhot t 1as a condition that an wci should be of concern where 1is a given threshold.
1is typically set to based on our experiences if the interval is one hour.
such a value may vary according to the number of service users.
we also consider hot t 2 hot so as to capture a substantial increase of the hot value.
2is typically set to two in our practice.
withr wci t ifeedback then removes most wcis and those remained are considered as suspicious indicators of potential issues which we name candidate wcis.
such a filtering criterion is quite helpful in cutting down the number of candidate wcis for further anomaly detection.
in fact according to our statistics of our production system more than .
of wcis will be filtered out.
our weekly data show that typically only tens of thousands of wcis are candidate wcis comparing to billions of wcis in origin.
finally it is worth noting that such a filtering approach is based on simple calculation.
it can be efficiently implemented which is specifically suitable for our problem domain since efficiency is our major concern towards handling a largescale online service system that serves millions of users concurrently.c.
feature extraction and anomaly detection we now have the candidate wcis and their corresponding hots for anomaly detection.
a traditional anomaly detection approach to deal with such time series data is to model it as a regression problem which can be applied with several forecasting techniques including arima and lstm .
unfortunately such a regression based method can only leverage information from time series aspect which turns out to be quite unstable in our problem scenario.
and the additional content based features are not considered.
hence ifeedback models the anomaly detection problem as a classification problem.
before we elaborate our classification approach let us discuss how we conduct proper feature engineering as input for classification task.
on the one hand as the historical data curve describes the trend of how wci changes.
we should extract time series features from the hots to describe the service condition in each time interval one hour in our practical implementation .
on the other hand we consider those feedback corresponding to an anomalous wci should share centralized content indicating a certain issue.
hence content based feature on text diversity is another important component of the vectorized feature.
to better collect features we propose a long short term sliding windows strategy.
the long term sliding window is seven days while the short term sliding window is hours in our practical implementation.
given the current hour and a wci it selects the occurrence numbers of the wci in the previous hours i.e.
in the short term window as well as the occurrence numbers of the wci in the previous seven days i.e.
in the long term window to construct time series features.
as the target time interval moves the two windows slide and follow which capture the features of the occurrence numbers of wci within them respectively.
figure shows two examples.
in particular we collect the occurrence numbers of a wci among all the feedback texts in every hour within the short term window as well as those within the longterm window.
moreover we also take the ratio of occurrence numbers of wci to the total number of feedback in the time window as features.
similarly we compute the text diversity via jaccard distance on bag of words bow within the same periods marked by time windows to construct content based feature.
such feature checks the unique number of feedback related to a wci.
our experiences prove this to be a vital feature in recalling severe issues as users tend to give the similar feedback text urgently when they occur.
we rely on historical human efforts in inspecting whether an issue exists to label some of such vectorized data.
in this way previous human efforts can be efficiently exploited to our anomaly detection task.
we can then train an xgboost based class classification model .
the model can then be applied to classify whether current time interval contains an issue.
for the interval reported as anomalous one the corresponding wcis can be considered as those associated authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a hot and sliding windows of an existing wci b hot and sliding windows of a newly generated wci fig.
.
long short term time window for existed and new indicators with potential issues and in turn presented for further analysis.
finally it is worth noting that our sliding windows based approach can solve the cold start problem mentioned in section v a. figure b shows the mechanism.
when a new wci is generated the monitoring process for the corresponding data of the wci starts from the initial state.
it simply considers the historical occurrence numbers before the generation of the new wci as zero.
a hot can be formed accordingly and the feature vector of current time interval can also be extracted.
if a newly added wci exhibits a substantial increase in its hot comparing with the zero values in the historical data it is reasonable if it is classified as an anomaly.
d. bug clustering as discussed in section v c ifeedback can produce a set of anomalous time intervals with corresponding wcis and user feedback texts.
unfortunately for each issue there are many anomalous wcis ifeedback detects.
they are also associated with tremendous user feedback texts.
although such outputs can direct human inspection of issues our experiences show that each engineer can only inspect and handle a few of such cases per day.
we need to minimize such human efforts.
we notice that typically wcis may share similar words.
moreover similar user feedback texts may describe the same issue as shown in fig .
therefore we propose to cluster the wcis and the feedback texts of the anomalous cases in order to obtain better summaries of the potential issues.
again a proper vectorization approach for the wcis is required to facilitate the clustering process through which the distance between the corresponding vectors can correctly measure the similarity between wcis.
however the design of such a vectorization approach can be hard due to the diverse ways of natural language expression in the user feedback.
moreover a specific word may havewci related feedback i scanned the qr code while paying but a failure message showed up.
related feedback my paying by scanning qr code resulted in failure!
wci related feedback i scanned the payment code from my customers but it failed.
related feedback why it failed all the time when paying with the payment code?
fig.
.
examples of feedback texts that can be clustered fig.
.
example of context based vectorization of texts where indicates a word appears in the context and otherwise.
different meanings under different scenarios and should be embedded as different vectors.
therefore we adopt a dynamic vectorization approach which is based on the contexts of the wcis.
the underlying idea is that two words with similar contexts share similar meanings .
we generate the vector by collecting the bag of words on the context of wci words and counting the occurrence on each word in bow to form the vector.
an example is shown in figure .
note that such an approach requires no training process when applying to different text sets.
ifeedback can hence maintain its ability to be adaptive to monitoring different online services with different forms of feedback.
we can then apply a hierarchical clustering algorithm which can automatically decide the number of clusters on the generated vectors of the feedback text associated with the anomalous wcis.
such clustering is based on cosine distance to gather the similar vectors.
each cluster of texts are then considered as those describing the same issue.
in this way the number of reported issues are below ten for each service which is a reasonable amount of workload for one engineer to efficiently perform further inspection.
vi.
e xperiences in applying ifeedback in this section we will describe how effective ifeedback performs in detecting real life service issues through our real practical experiences in applying ifeedback to different largescale online services.
a. overview of field deployment ifeedback has been deployed in tens of large scale online service systems for ten months.
example systems include a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a issue detected in wechat reading b issue detected in wechat official accounts fig.
.
examples of reporting issues social media application wechat a business communication application wechat work an online payment system wechat pay a mini game hosting platform wechat games a searching engine wechat search a reading application wechat reading a music player qq music a news reader wechat news a browser qq browser a video player qqvideo and a game tencent royal .
a system can has one to several ifeedback instances monitoring the user feedback of different functions of the same system.
each product line typically serves tens of millions of users is maintained by over one hundred developers in general and contains tens of online services which use ifeedback to perform real time issue detection from user feedback.
such large scale production systems are complicated both in its codes and in how it works to serve a large amount of users.
they require significant workload in their daily operations.
on the other hand the large amount of users can contribute to a rich source of feedback every day.
about two million feedback text items will be collected by ifeedback daily.
the total historical data volume has reached the scale of hundreds of gigabytes gbs in its ten month operation.
ifeedback has a user friendly frontend to facilitate developers in processing a reported issue.
figure shows two examples of the ifeedback mobile end user interface.
we can see that the detected issue will be reported together with the time information the clustered anomalous wcis the number of relevant feedback text items and the corresponding feedback texts provided as a list.
meanwhile a hot is shown as a red curve.
as a reference a blue curve which is the day moving average of the hot is also shown.
in the past ten months we have witnessed the success of ifeedback in helping aiops.
we have interviewed many core engineers from different product lines who rely on ifeedback to perform issue detection.
we found that ifeedback are embraced by the product lines which has greatly reduce human efforts in issue detection and debugging.
the success of ifeedback in diverse services from different product lines shows that it a ui related issue b ui related issue fig.
.
examples for ui related issue can well adapt to different types of services.
it is suitable for general large scale online service systems.
our statistics show that ifeedback has reported around issues in each week.
among them are those that do not manifest in the backend system.
in other words they do not leave distinct traces in backend system monitors.
specially for some specific services such as those in wechat pay such a ratio can reach .
these results indicate that without ifeedback it would be quite labor intensive if not infeasible to detect such issues.
ifeedback can achieve a significant improvement in detecting issues during system operations.
in our following discussions we will provide several representative cases where ifeedback successfully detect the issues which are not able to be detected by the backend system monitors.
b. case detecting user interface ui defects ui is critical to user experiences when accessing a service.
ui may encounter problems.
for example contents are not displayed correctly in some smartphone models.
it is hard if not impossible for a backend system monitor to be aware of such problems since they typically do not manifest in the backend system.
during our field deployment ifeedback has successfully found many ui issues from user feedback.
it meanwhile brings informative reports that help debugging.
figure shows example reports of such issues.
in figure a the reported issue is on a mistakenly set font size which incurs complaints from several users.
with the reports ifeedback generates such an issue can be easily located and fixed.
also as shown in figure b ifeedback reports an issue where we can easily find that the advertisement content mistakenly covers a button in a service in wechat games.
we can see that such ui related issues may severely annoy the users.
unfortunately nothing can be found from the backend system monitors.
in such cases user feedback is the sole way for developers to be aware of such issues.
ifeedback effectively detects such issues in a timely manner.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a silent issue b silent issue fig.
.
examples for silent issues c. case detecting silent backend issues in contrast to ui issues that may hardly manifest in backend systems some issues are caused by the backend logic but are still hard to be detected by the backend system monitor.
in other words they may have little impact on the indicators monitored by the backend systems.
we name such issues silent ones.
such issues may deteriorate user experiences.
however they are particularly difficult to detect without ifeedback .
they do not cause alarms by their corresponding backend system indicators.
as a result they tend to be neglected by developers.
in contrast we have found that ifeedback can perform quite satisfactorily in detecting such silent issues since users are good informers of them.
figure shows two examples of such issues reported by ifeedback .
the first reported issue shows that the messages received in the android end of wechat cannot be successfully synchronized to a windows client.
the issue is due to the backend logic.
but it affects only a small portion of the users.
although there are designed backend system indicators for the relevant backend logic the indicators receive no significant influence to signal an alarm.
the second reported issue indicates users can still receive notifications even after they stop following some official accounts in wechat.
its cause is that the stop following instruction is mistakenly intercepted in the backend system.
although there exists certain backend system indicators for such interceptions the indicator records the total amount of such interceptions which is unable to detect the issue.
ifeedback in contrast can capture user complaints on such issues.
it then provides informative reports which can instantly direct human inspection towards bug removal.
d. case detecting newly occurring issues online services are typically experiencing rapid upgrading to fulfill new user requirements.
however rapid upgrading may incur new bugs.
unfortunately developers can hardly predict possible problems that may encounter in the future.
they may not be able to prepare good backend system a newly occurring issue b newly occurring issue fig.
.
examples for newly occurring bugs indicators for all potential problems especially those caused by the newly added functionalities.
again users are good informers of such newly occurring issues.
with ifeedback developers can conveniently detect newly occurring issues reported by the users.
figure gives two example cases.
in figure a .
a it is reported that users cannot participate in the sailing competition activity in a minigame hosted by wechat games.
such functionality is newly added.
its corresponding information has never been collected before e.g the word sailing has never appeared before .
therefore this issue can never be captured using the existing indicators however with the ability of automatically constructing news wcis ifeedback can successfully detect and report such an issue and helps developers rapidly locate and fix it.
figure b shows another case where the comments in the wechat official accounts are not shown correctly.
the issue is actually caused by the new advertisement policy enforced in the backend logic.
it unexpectedly results in blocking the comments.
again we can see that ifeedback can successfully detect and report such issues with helpful information in debugging.
vii.
f urther discussions in our previous section we present that ifeedback has successfully detected service issues that are hard to be found by traditional backend system monitor based approaches.
next we presented a further evaluation of ifeedback in terms of its accuracy and effectiveness in issue detection.
we then discuss several possible threats to validity together with how we address them.
the lessons learnt from our past ten month operation experiences are finally provided.
a. accuracy and effectiveness first for an issue detection tool an important consideration is its detection accuracy.
in other words how many detected issues are confirmed as true positives i.ethe precision and how many real issues are missed as false negatives i.e.
the recall .
we present some of our statistics.
for precision we count their numbers from a two month operations of four authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i accuracy on four services wechatwechatgameswechatreadingwechatworktotal detected confirmed precision .
.
.
.
.
table ii recall on severe issues total detected missed recall severe issues .
different product lines.
the ratio of the confirmed issues is the precision.
the results are shown in table i. we can see ifeedback has reached a satisfactory precision.
note that the number of the original feedback texts are in millions.
with ifeedback developers can be directed to analyze a small number of potential issues where over in some product line such a ratio can reach over are true positive issues.
as for recall we collect results for severe issues as they will be reported eventually if ifeedback fails to detect them.
results through ten months operation are shown in table ii.
along with such a high recall value most issues are successfully detected in advance of traditional backend monitoring.
moreover we also use more training data to improve our classifier which can successfully detect the missed three bugs in postmortem backtesting.
we further study the efficiency improvement introduced by ifeedback in issue detection.
our past practice of debugging from user feedback without ifeedback is relying heavily on human efforts.
frequent keywords will be reported by a feedback management system.
engineers will perform human inspection of such keywords.
they may accordingly search relevant feedback texts of suspicious keywords in the system.
by reading the texts they may infer possible issues.
in contrast ifeedback improves such a process with an automatic approach.
as an example we focus on a one week feedback data set of a product line.
we examine how many feedback texts are filtered in each steps of ifeedback as a reference to see how ifeedback can reduce possible human efforts.
in the beginning we have collected feedback text items in total along with wcis generated from them.
after the indicator filtering process of ifeedback there are remaining candidate wcis.
after the machine learningbased anomaly detection ifeedback has selected wcis that are considered to be related to system issues.
finally the clustering process has eventually produced potential issues which means that less than potential issues will be produced as report for each single application in average.
we can see that ifeedback is able to locate hundreds of issues out of tens of millions of feedback texts automatically which can greatly reduce human efforts in handling the feedback texts.
b. threat to v alidity first of all we design ifeedback for our production service systems and shows its effectiveness.
one may concern thatwhether ifeedback works only for a specific application scenario.
actually in our ifeedback design we aim at diverse general production lines.
these production lines are all in nature large scale online service systems.
but they do not share much similarity they are developed for different purposes by different teams.
they possess various code logic implementation languages and architectures.
ifeedback has shown its effectiveness in all these product lines which can prove its adaptability to different software service systems.
as we have discussed ifeedback relies on light configurations e.g.
the proper lengths of the long short time windows that may be based on service specifics.
our experiences show that such efforts are minor.
it is worth noting that in ifeedback design we focus on a general way in building wcis from user feedback text.
we rely on the wcis together with their features e.g.
their hots to detect issues instead of using service specific information.
it is why ifeedback can preserve its good adaptability to divers service scenarios.
ifeedback relies heavily on the quality of user feedback.
a second possible threat is whether the feedback that ifeedback uses is specially with high quality comparing with other user feedback in general service systems.
actually our service users are internet service users from the general public.
they are representative users of large scale online service systems.
the feedback that ifeedback uses is not with relatively high quality the feedback texts are with tremendous irrelevant information and inexperience expressions on user experiences.
we carefully design methods to handle noisy information generally existed in the feedback texts in ifeedback .
hence it can work for feedback from general users of large scale online service systems.
viii.
c onclusions this paper discusses our design of ifeedback a tool to perform real time issue detection in large scale online systems based on the user feedback texts.
we also presents our representative experiences in successfully applying ifeedback in tens of large scale online service systems for ten months.
it s a relatively new attempt in aiops for large scale online service systems to detect potential issues in real time based solely on user feedback texts.
in contrast to the existing work that considers user feedback as a static text based dataset ifeedback proposes a dynamic adaptive approach to capture the historical trends of user feedback.
indicators in particular wcis are built to facilitate feature extraction of the trends.
machine learning based approaches can then be applied towards a successful issue detection.
such a prospective to exploit information in user feedback can shed light to other tasks based on user experiences.
examples include dynamic service adaptation to user requirements and user profiling.
acknowledgement this work was supported by the national natural science foundation of china project no.
.
yangfan zhou is the corresponding author.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.