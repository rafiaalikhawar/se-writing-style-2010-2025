speedoo prioritizing performance optimization opportunities zhifei chen state key laboratory for novel software technology nanjing university chinabihuan chen school of computer science shanghai key laboratory of data science and shanghai institute of intelligent electronics systems fudan university chinalu xiao xiao wang school of systems and enterprises stevens institute of technology united states lin chen state key laboratory for novel software technology nanjing university chinayang liu school of computer science and engineering nanyang technological university singaporebaowen xu state key laboratory for novel software technology nanjing university china abstract performance problems widely exist in modern software systems.
existing performance optimization techniques including profilingbased and pattern based techniques usually fail to consider the architectural impacts among methods that easily slow down the overallsystemperformance.thispapercontributesanewapproach namedspeedoo toidentifygroupsofmethodsthatshouldbetreated together and deserve high priorities for performance optimization.
the uniqueness of speedoo is to measure and rank the performance optimization opportunities of a method based on the architectural impact and the optimization potential.
for each highlyrankedmethod welocatearespective optimizationspace based on performance patterns generalized from empirical ob servations.
the top ranked optimization spaces are suggested to developers as potential optimization opportunities.
our evaluation onthreereal lifeprojectshasdemonstratedthat18.
to42.
ofmethodsinthetoprankedoptimizationspacesindeedundertook performance optimization in the projects.
this outperforms oneof the state of the art profiling tools yourkit by to times.
an important implication of this study is that developers should treat methodsinanoptimizationspacetogetherasagroupratherthan asindividualsinperformanceoptimization.theproposedapproach can provide guidelines and reduce developers manual effort.
ccs concepts software and its engineering software performance keywords performance metrics architecture corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
reference format zhifeichen bihuanchen luxiao xiaowang linchen yangliu andbaowen xu.
.speed oo prioritizingperformanceoptimizationopportunities.
inicse 40th international conference on software engineering may june3 gothenburg sweden.
acm newyork ny usa 11pages.
introduction performanceproblems1widelyexistinmodernsoftwaresystems due to the high complexity of source code .
they can slow down production runs hurt user experience or even cause system failures.
therefore the first task for software performance optimization is to find performance optimization opportunities and then conduct performance refactoring.
theremainly existtwotypesof techniquesforfindingperformanceoptimizationopportunities.profiling basedtechniqueslever ageinstrumentationtocollectprofilesorexecutiontraces.thepurposeistolocatethecoderegions or hotspots thatconsumemostresources e.g.
memoryandtime toidentifythefrequentlyexecutedprogrampaths or hotpaths ortofitaperformance function .suchtechniquesuseconsumedresourcesor executionfrequenciesastheonlyperformanceindicator anddonot consider the performance impact due to architectural connections amongsoftwareelements e.g.modules sourcefiles methods etc.
thus theyprovideanarrowviewforidentifyingperformanceproblems anddevelopershavetospendalargeamountofmanualeffort to locate the root causes that are not even in the ranked list of profilers .moreover themanifestationofthecodewithperformance problems heavilyrelies on the qualityof the test inputs.
despite advances on producing performance test inputs important performance problems still remain unrevealed.
anothertypeofperformanceproblemdetectiontechniquesis pattern basedtechniques.suchtechniquesleveragestaticand or dynamic program analysis to identify code regions that match specificpatterns e.g.
inefficientloops inefficientcontainers inefficient synchronizations or redundant collection traversals .
while they are effective at identifying specifictypesofperformanceproblems theyfailtobeapplicable 1performanceproblemsaresometimesalsocalledperformancebugsorperformance issues.
for consistency we use performance problems throughout the paper.
acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden z. chen et al.
to a wider range of performance problems.
moreover they are not designedandthuswillnotprovideanysuggestionsonthebenefited code regions after the performance optimization.
inthispaper weproposeanewapproach named speedoo2 toprioritize performance optimization opportunities based on their impact on the overall system performance as well as their local potential of being optimized.
thus the identified optimization opportunitieshaveahighchanceofbeingoptimized andonceoptimized they can benefit to the overall system performance.
a main insightisthattheinvestigationofperformanceoptimizationshould consider the architectural connections among methods instead of treating each method in isolation because the performance of amethod can affect or be affected by the behaviors of other meth ods e.g.
the methods that it calls .
hence each opportunity is a set of architecturally related methods with respect to both architecture and performance e.g.
the performance of a methodis improved by optimizing the methods it calls .
different from profiling based techniques s peedoo furtherr elies on the architectural knowledge to prioritize potential optimization opportunities more accurately.
different from pattern based techniques speedoo is more generic and considers the overall architecture of methods.
our approachworks inthree stepsto identifythe optimization opportunitiesthatshouldbegivenhighprioritiestoimprovethe overallperformance.first wecomputeasetofmetricsthatmeasurethearchitecturalimpactandoptimizationpotentialofeachmethod.
the metrics include architectural metrics e.g.
the number of methods called by a method the larger the number the more impact theoptimizationhas dynamicexecutionmetrics e.g.
thetimea method consumes the longer the time the more potentially it can beoptimized andstaticcomplexitymetrics e.g.
thecomplexityof the method the more complex the code structure the more potentiallyitcanbeoptimized .then wecomputean optimizationpriorityscoreforeachmethodtoindicatethepriorityforoptimization basedonthemetrics andrankthemethods.finally foreachhighlyrankedmethod i.e.
candidate welocatearespective optimization spacecomposedofthemethodsthatcontributetothedynamicperformance of the candidate which is based on five performance patterns summarized from real performance optimization cases.
each methodinoptimizationspacecouldbeoptimizedtoimprovethe overall system performance.
such optimization space is suggested todevelopersaspotentialoptimizationopportunities whichcan provide guidelines and reduce developers manual effort.
wehaveimplementedtheproposedapproach andconductedan experimental study on three real life java projects to demonstrate the effectiveness and performance of our approach.
the resultsshow that .
to .
of methods in the top ranked optimization spaces calculated by speedoo indeed undertook performance optimization during software evolution.
we also compared speedoowith thestate of the artprofileryourkit anddiscovered speedoo covers more refactored methods.
the results have demonstrated that speedoo effectiv ely prioritizes potential performance optimization opportunities significantly outperforms yourkit and scales well to large scale projects.
in summary our work makes the following contributions.
2thespeed optimization opportunities that we are pursuing with our approach.
bnf.node 2i o .
p i p e 3a s t .
n o d e filter io.inputpipe ext 6i o .
o u t p u t p i p e ext io.writeroutputpipe impl io.memoryoutputpipe impl io.readerinputpipe impl io.memoryinputpipe impl ast.treevisitor interpreter impl impl parse.parser impl lex.lexer impl parse.convert impl ast.variable ext ast.number ext ast.operexpr ext ast.funcexpr ext ast.unaryoperexpr ext figure an example of drh weproposedanewapproachtoprioritizingperformanceoptimization opportunities based on their architectural impact on the overall performance and their potential of being optimized.
we implementedtheproposedapproachandconductedexperimentsonthreereal lifeprojectstodemonstratetheeffectiveness and performance of our approach.
preliminaries on architecture to understand the architectural roles of different elements in a system design rule hierarchy drh algorithm was proposed toclustersoftwareelementsintohierarchicallayers.thisalgorithm isbasedonthe designruletheory whichindicatesthatamodular structureiscomposedof designrules i.e.
architecturallyimportant elements and modules i.e.
elements depending on and decoupled by thedesign rule elements .
the drhcaptures the roles of software elements as design rules andmodulesin hierarchical layers based on the directed dependency graph of the software elements.
eachlayercontainsasubsetofelements.thelayersmanifesttwo key features the elements in the lower layers depend on the elementsintheupperlayers but notviceversa and elements inthesamelayerareclusteredintomutuallyindependentmodules.
ingeneral elementsintheupperlayersarearchitecturallymore important than elements in the lower layers because the latter dependontheformer.forexample fig.1illustratesa drhcalculated fromthe extend and implement dependenciesinamathcalculatorprogram.the drhisdepictedasa designstructurematrix a n nmatrix.
the rowsand columns represent software elements i.e.
source files in this example.
each cell indicates the dependency from the file on the row to the file on the column.
the internalrectangles in the dsm represent layers and modules.
this drh capturesthekeybaseclassesorinterfacesasthehigherleveldesign rules and captures the concrete classes as two lower layers.
files to4formlayer1onthetop whicharethekeyinterfacesandparent classes.
files to form layer in the middle whose elements extend the elements in the top layer.
files to form layer in the bottom which contains mutually independent modules.
each module extend or implement a design rule element in the upper layers.forexample files16to20formamoduleastheyall extend the design rule mij.ast.node which is a parent class.
to achieve different analysis goals the elements could be at differentgranularities e.g.methodlevel.inthispaper weapplythe drhalgorithm at method level to capture the architectural importanceofeachmethodandtounderstandthepotentialbenefitsof authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
speedoo prioritizing performance optimization opportunities icse may june gothenburg sweden titan understand yourkit dynamic execution metricsstatic complexity metricsarchitectural metrics normalizing metricscomputing priority scoreranking methods detecting performance patternscomputing optimization spaceranking optimization spaces performance optimization opportunitiessource code of target system step metric computation step method prioritization step optimization space localization figure approach ov erviewofspeedoo improvingitfortheoverallsystemperformance.wewillintroduce this in details in section .
.
methodology fig.2showstheoverviewofspeedoo.
basically ourapproachworks inthreesteps computingmetricsforeachmethodinthetargetsystem section3.
prioritizingmethodsbasedonmetric valuesto indicate their optimization priorities section .
and locating the optimizationspaceforeachhighly rankedmethod section3.
.the prioritizedoptimizationspacesarereportedtodevelopersaspotential optimization opportunities and every methodin an optimizationspacehasthepotentialofbeingoptimizedtoimprovetheoverall system performance.
we elaborate each step as follows.
.
metric computation inthefirststep wecomputethevaluesofmetricsthatmeasureeach method sarchitecturalimpactandoptimizationpotential.before divingintothedetails wefirstexplaintherationalityofmeasuring architectural impact and optimization potential.rationality.
givenalistofhotspotmethods developersusually treat them equally by considering whether any optimization could beappliedtoimprovethesystemperformance.however theimpactofaperformanceoptimizationvariesdependingonthearchitecturerole of the optimized method in a system.
for example if a method is invoked by many methods the performance optimization of this method will greatly improve the system performance.
it is desired thatsuchmethodsshouldbegivenhigherprioritiesforperformance optimization.
thus we analyze the architectural impact of each method to estimate the impact scope of performance optimization.
in addition considering the functionality of each method not everymethodhasthesamepotentialtobeimprovedwithrespect toitsperformance.intuitively ifamethodhashighercomplexityor consumes more runtime it has more potential to be optimized.
for example ifamethodfrequentlyinvokesatime consumingmethodthroughaloopstructure wecaneitherreplacethetime consuming method or avoid unnecessary invocations.
it is desired that such methodsshouldobtainahigherpriorityforoptimization.hence weanalyzetheoptimizationpotentialofeachmethodtoestimate the potential space of performance optimization.based on the previous understanding we derive a set of metrics as shown in table to represent and estimate the architectural impact and optimization potential of a method.
architectural impact.
we propose an architectural role based metricandthreecaller callee scope basedmetrics whosevalues are computed from the output of titan .
.
role based metric layer.
as briefly discussed in section we apply the drhalgorithm to the method level call graph of a systemto capturethearchitectural importanceofeach method.each methodresidesinauniquelayerofthedrh andthelayernumber reflects its architectural role.
the top layers usually contain infrastructurallevelmethods whicharecalledbymanyothermethodsin the system while the bottom layers usually contain main methods that call many other methods.
the methods in the middle layers gradually transitfrom the infrastructuralroles to controlroles.
in addition the naming conventions of the methods are generally consistentwiththearchitecturalrolessuggestedbythedrh layer.
for example methods in the top layers are likely to be named with util and commons etc..incomparison methodsinthebottom layers are likely to be named with execute and main etc.. hence weusedrh layerasametrictodescribethearchitectural importance of a method.
a method in an upper layer is architecturallymoreimportantthanamethodinalowerlayer astheperformance problems with methods in top layers tend to produce larger impactsonthewholesystem.thehigherthedrh layer thesmaller the metric value but the higher the architectural importance.
.
scope based metrics size depth and width.
we calculate two scopes related to each method f the caller scope containing the methods directly or transitively calling f and the calleescopecontainingthemethodsdirectlyortransitivelycalledby f. iffhas a performance problem the methods in the caller scope also perform poorly as their execution time contains f s execution time.iffhasaperformanceproblem e.g.
aninefficientloop the methods called by it may also consume more time due to frequent invocations by f. the actual impact of fon other methods may dependontherun timeenvironment butthetwoscopescontainall the potentially impacted methods if fhas a performance problem.
thecaller scopeandthecallee scopecanbecalculatedastwo sub treesbytransversingfrom fthroughstatic call and called by relationsrespectively.weextractthreemetricsfromthetwo sub trees of fto measure its importance size thetotalnumberofmethodsinthecaller scopeandthe callee scope.thelargerthevalue thelargertheimpactsof f and thus the more important is f. depth thesumofthedepthofthetwosub trees.thelargerthe value the deeper the impact of f and thus the more important isf.
width thesumofthewidthofthetwosub trees.thelargerthe value thebroadertheimpactof f andthusthemoreimportant isf.
optimization potential.
we measure both the static complexityanddynamicexecutionofamethodtoestimatetheoptimization potential.
for the complexity of a method we choose five metrics based on the internal structure of the method and the external relationshipwithothermethods asshowninthesixthtotenthrows of table .
cccaptures the cyclomatic complexity of a method.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden z. chen et al.
table metrics used for measuring architectural impact and optimization potential of a method category metric description architectural metrics architectural impact layer the residing drh layer number of a method size the total number of methods in the caller callee scope of a method depth the sum of the depth of the caller and callee scope of a method width the sum of the width of the caller and callee scope of a method static complexity metrics optimization potential cc cyclomatic complexity i.e.
the number of linearly independent paths in a method sloc the source lines of code in a method fanin the number of methods directly calling a methodfanout the number of methods directly called by a method.
loop the number of loops in a method dynamic execution metrics optimization potential time the cpu time consumed by a method owntime the cpu time consumed by a method itself without subcalls counts the invocation counts of a method sloc faninandfanoutcharacterizethesizeandcomplexityof thefunctionalresponsibilityofamethod.thehigherthesemetrics the more opportunities to optimize the structure or refactor the coupling relationships to improve performance.
loopis selected becauseinefficientorredundantloopsarecommonperformance problems .
if a method contains many loops there are potential optimization opportunities in it.
note that these metrics are all widely used in the literature as useful indicators for quality problems .
here we leverage them to reflect the potential of performance optimization.
we use understand in our implementation to compute these metrics.
moreover consideringthedynamicexecutionofamethod we usetime owntime andcounts as listed in the last three rows of table to measure the performance level of a method during concreteexecutions.ahighervalueofthesemetricsindicatesaseverer performance problem of a method and thus a higher possibility to improve its performance.
these metrics are also widely used in the existingprofilingtools toidentify hot spot methods.
in ourcurrentimplementation weobtainthesemetricsbyrunning profiling tools i.e.
yourkit against the test cases.
as will be discussed in section our approach can be extended with additional metrics to prioritize optimization opportunities.
.
method prioritization in the second step based on the metric values we compute an optimizationpriorityscore foreachmethod f f wherefisthe set of all methods in a system to indicate its optimization priority and rankall methods in faccording totheir optimization priority scores.
this step is achieved in the following three sub steps.normalizing metrics.
to allow a unified measurement of these metricsindependentoftheirunitsandranges wenormalizeeach metricintoavaluebetween0and1bycomparingitwiththemaxi mumandminimumvalueofthemetric asformulatedineq.1and2.
m prime f mf min i fmi max i fmi min i fmi m prime f max i fmi mf max i fmi min i fmi mfrepresentsthevalueofametric mofamethod f andm prime fisthe normalized value of mf.
except for layer all themetrics are positive metrics i.e.
the higher the metric value the higher the optimizationpriority.thus layerisnormalizedbyeq.
andtheother metrics are normalized by eq.
.computing priorityscore.
basedonthenormalizedmetricvalues wecomputeanoptimizationpriorityscore opsforeachmethod fto estimate the optimization priority as formulated in eq.
.
ops f aif opf the first term aif see eq.
represents the architectural impact of amethod andthesecondterm opf seeeq.
indicatestheoptimizationpotentialofamethod.themultiplicationrepresentsthe globalpotentialperformanceimprovementontheoverallsystem by optimizing the method f. aif layer prime f size prime f depth prime f width prime f opf scf def noticethat opfconsidersboththestaticcomplexity seeeq.
and the dynamic execution see eq.
of a method.
scf cc prime f sloc prime f fanin prime f fanout prime f loop prime f def time prime f owntime prime f counts prime f here we assign equal weights to the architectural metrics in eq4 tothestaticcomplexityanddynamicexecutionineq.
tothe static complexity metrics in eq.
and to the dynamic execution metricsineq.
becauseweconsiderdifferentfactorstobeequally important.ranking methods.
once the priority scores of all the methods are computed we rank these methods in decreasing order of their scores.thus thelargertheoptimizationpriorityscoreofamethod thehigherpriorityitshouldbegivenforperformanceoptimization.
.
optimization space localization if a highly ranked method manifests performance problems the optimization opportunities reside not only in the method itself butalsointhemethodsthatcontributetoitsdynamicperformance e.g.
itscallees .however suchcontributingmethodsmaynotrankhigh even if they are potentially important for solving the performance problem.hence welocatearespective optimizationspace foreach highlyrankedmethod i.e.
candidate byfindingitscontributing methods.
each method in the space could be an optimization opportunity to improve the performance of the candidate.
based on our empirical analysis of performance problems fixed bydevelopers wefindthattheoptimizationspaceofacandidate isoftendeterminedbytheperformancepatternsofthecandidate.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
speedoo prioritizing performance optimization opportunities icse may june gothenburg sweden table summarized performance patterns from four apache systems performance pattern symptom description optimization strategies issues cyclic invocationa slow or frequently executed method is in an invocation cycleimprove any method in the cycle change dependency relations to break the circle expensive recursion a slow method calls itself repeatedlyimprove the method avoid or reduce the calls in its callers frequent invocation a method is frequently executedimprove the method cache the returned values to reduce calls add or update the call conditions in its callers avoid unnecessary or duplicated calls in the callers inefficient method a method is slow in executing its own codeimprove the method reduce its calls in its callers replace its calls with cheaper callees in its callers expensive callee a method is slow in executing its callees codeimprove the expensive callees reduce the calls to expensive callees replace the calls to expensive callees with cheaper callees others all the issues not belonging to the above commit ivy 6f8302fe candidate xmlsettingsparser.startelement string string string attributes private void includestarted map attributes throws ioexception parseexception ... new xmlsettingsparser ivy .parse configurator setting surl ... private void parse configurator configurator url configuration throws ioexception parseexception ... doparse configuration ... public void startelement string uri string localname string q name attributes att throws saxexception ... includestarted attributes optimization improve includestarted map by removing unnecessary cast figure a case for cyclic invocation thus we first present the performance patterns we summarized and then introduce the steps to locate optimization spaces.
performance patterns.
we manually analyzed real performance problemsfromfourapachesystems i.e.
cxf avro ivyandpdfbox which are all mature and industrial scale software projects.
we searched the issue databases using a set of performance related keywords aswassimilarlydoneintheliterature .finally werandomlysampled75fixedperformanceissues andanalyzed how the problems were fixed i.e.
optimization strategies .
table describesthesummarizedperformancepatternswiththeirsymptomdescription optimizationstrategiesandthenumberofcorre spondingissues.theseperformancepatternsappearnearlyinall the systems.
notice that although performance patterns were also summarized in the literature e.g.
their patterns focus on root causes of performance problems and are local to a method andthusareoftenspecific.ourpatternsemphasizetheoptimization locations of performance problems from the perspective of symptomsandinfluence andarehenceglobalandgeneric.inthe following we illustrate each performance pattern.
.
cyclic invocation.
the invocation of a set of methods forms a cycle.anyinefficientorfrequentlyexecutedmethodinthecircle would slow down the execution of all involved methods.
for example infig.
themethod startelement was reportedto be slow becauseit sinaninvocationcirclecontainingaslowmethod includestarted.
this issue was solved by improving includestarted.
note that the performance problem would worsen if the callingcommit pdfbox 7929477d candidate pdfstreamparser.parsenexttoken private object parsenexttoken throws ioexception ... while nexttoken parsenexttoken instanceof cosname object value parsenexttoken ... optimization improve this method by using per image color conversion figure a case for expensive recursion commit pdfbox candidate cosdictionary.getdictionaryobject cosname public map string pdfont getfonts throws ioexception ... cosbase font fontsdictionary.getdictionaryobject fontname ... public map string pdcolorspace getcolorspaces ... cosdictionary csdictionary cosdictionary resources.getdictionaryobject cosname.colorspace ... optimization reduce the calls to getdictionaryobject cosname in getfonts and getco lorsp aces thro ugh caching figure a case for frequent invocation circleisrepeatedlyexecuted.analternativesolutionistobreakthe cycle evidenced by real performance issues.
.
expensive recursion.
recursion allows a method to repeatedly callitself.ifthemethodisexpensive theperformanceproblemwill bedramaticallyamplified.forexample themethod parsenexttoken infig.4repeatedlycallsitselfbyrecursion.thereasonwasthatthe pdf parser converted the color spaces of images for every pixel.
finally developers improved this method by performing color conversioninoneoperationforeachimage.performanceproblemsdue toexpensiverecursioncanbeoptimizedbyimprovingtherecursion method itself or by reducing calling such methods.
.
frequent invocation.
frequently executed methods especially the most influential methods in low drhlayers can be optimized to produce a significant improvement of system performance even with minor optimization.
for example in fig.
the method getdictionaryobject iscalledfrequentlyby pdresource forobtainingresources.incommit54037862 pdresources wasrefactoredtoreduce theinvocationsof getdictionaryobject.thisisthemostcommon patterninthestudiedissues.itcanalsobereducedbyupdatingthe call conditions or avoiding unnecessary duplicated invocations.
.
inefficient method.
a method manifests poor performance due totheinefficiencyinitself.fig.6showsanexamplein avro.the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden z. chen et al.
table detection conditions and optimization spaces of performance patterns performance pattern detection condition optimization space cyclic invocation iscallcycle and ishigh time and ishigh counts os f all the methods in the invocation cycle expensive recursion isrecursion and ishigh time and ishigh counts os f f o s each method in getcallers f that satisfies ishigh time frequent invocation ishigh counts os f f o s each method in getcallers f that satisfies ishigh invocations inefficient method ishigh time and ishigh owntime os f f o s each method in getcallers f that satisfies ishigh time expensive callee ishigh time and not ishigh owntime os f f o s each method in getcallees f that satisfies ishigh time others all the conditions not belonging to the above os f f commit avro e0966a1e candidate genericdata.deepcopy schema t public t t deepcopy schema schema t value ... case float return t new float float value case int return t new integer integer value case long return t new long long value ... optimization improve this method by avoiding creating new instances figure a case for inefficient method commit pdfbox 4746da78candidate preflightparser.parseobjectdynamically long int boolean protected cosbase parseobjectdynamically long objnr int objgen nr boolean requireexistingnotcompressedobj throws ioexception ... final set long refobjnrs xreftrailerresolver.getcontainedobjectnumbers objstmobjnr ... optimization replace xreftrailerresolver.getcontainedobjectnumbers int with xreftrailerresolver.getxreftable figure a case for expensive callee methoddeepcopy createdmanynewinstancesforprimitives which is unnecessary.
this method was optimized in commit e0966a1e by removingunnecessary instancecreations.ifan inefficientmethod canhardlybeoptimized theoverallperformanceofasystemcan still be improved by reducing the invocations of this method.
.expensivecallee.
theperformanceofamethodisalsodeterminedbytheperformanceofitscallees.expensivecalleeisaperformancepatternwhereamethodisslowinexecutingitscallees code.
theoptimizationstrategiesincludeimprovingtheexpensivecallees and reducing or avoiding the invocations of expensive callees.
the example in fig.
illustrates the performance optimization of the methodparseobjectdynamically.asreported themethodtookquite some time which was mostly spent in getcontainedobjectnumbers.
thereasonwasthatmanyfullscanswereperformedin getcontainedobjectnumbers.developers fixedthis issueby replacingthe calls togetcontainedobjectnumbers with the calls to getxreftable.
notethattheoptimizationofaninefficientmethodcanbetreated eitherasthe inefficientmethod pattern i.e.
toimprovetheinefficientmethod orasthe expensivecallee pattern i.e.
toimprovethe caller of the inefficient method .
thus we counted the numberof corresponding issues equally for these two patterns.
we separatelydefinethesetwopatternsforlocatingindividualoptimization opportunities for each target method.
there were issues thatexhibit no specific patterns e.g.
moving a generic method from other classes to a target class avoiding using type parameters andpackage reorganization.
current patterns are defined among architecturallyconnectedmethods whichcanbeextendedtosupport high level structures like packages and modules.detectingperformancepatterns.
asshownabove theoptimizationopportunitiesofamethodresideinallthemethodsthatcontribute to its performance and the scope is determined by the performancepatternsitbelongsto.therefore beforecomputingthe optimizationspaceofeachcandidate wefirstdetectitsperformance patterns using the dynamic execution metrics time owntime andcounts as well as its call relations with other methods.
generally themethodsindifferentdrhlayersareexpectedto exhibit different level of performance.
the dynamic execution metrics of methods are influenced by the architectural roles suggested by the drh layer.
intuitively since the infrastructural methods provide basic functions to support other methods they usually havelowermethodexecutiontimebuthigher invocationcounts.
thecontrolmethodsoftenprovidehigherlevelfunctionsbycalling manyother methods thus theyusually havelower invocation countsbuthigherexecutiontime.inthatsense theperformanceof a method is only comparable to the methods within the same drh layer.
hence our general idea of detecting performance patterns is tocheckwhetherthedynamicperformanceofamethodactsasa outlier among all the methods in the same drh layer.
specifically theconditionsfordetectingperformancepatternsof acandidate faredefinedinthesecondcolumnoftable3basedon their symptoms in table .
here ishighreturns whether the metric valueoffisastatisticaloutlieramongthemethodsinthesamedrh layer.forcallrelations iscallcycle returnswhether fisinvolvedin acyclicinvocationand isrecursion returnswhether finvokesitself.
following the detection conditions a method can be matched in morethanoneperformancepatterns.forexample amethodmatchesbothfrequentinvocation andexpensivecallee whichmeansthatthe optimization opportunities reside in the two optimization spaces.computingoptimizationspace.
basedontheperformancepatternsofacandidate f wecomputetheoptimizationspace os f tolocatethepotentialoptimizationopportunities.guidedbythe optimizationstrategiesintable2 welocatetheoptimizationspace foreachpatterninthethirdcolumnoftable3.
getcallers returns the list of callers of f andgetcallees returns the list of callees.
ranking optimization spaces.
the optimization space consists ofallthecontributingmethodstoacertainperformanceproblem and should be investigated by the developers as a whole.
thus we computetheoptimizationpriorityscoreofanoptimizationspace as the average priority score of all the methods in the space to represent its optimization priority.
finally the optimization spaces arerankedbasedontheirpriorityscores.onlythosehighly ranked optimizationspacesarerecommendedtothedevelopersaspotential optimization opportunities.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
speedoo prioritizing performance optimization opportunities icse may june gothenburg sweden table subject projects projectanalyzed release refactored release ver.
date loc meth.
ver.
date avro1.
.
.
.
ivy2.
.
.
.
pdfbox .
.
.
.
evaluation to evaluate the effectiveness and performance of our approach we conducted an experimental study on three real life java projects.
.
evaluation setup subjectprojects.
table4describesthesubjectprojectsusedinour evaluation.threewell knownapacheprojectsareselected avrois a data serialization system ivyis a popular dependency manager andpdfboxis a tool for working with pdf documents.
they are selectedduetothefollowingreasons.first wecancollectsufficient performance problems from the formal issue reports documented in jira and the entire commits history traced through github see below .second theyarenon trivialprojectsthatcontainthousands of methods.
third theseprojects belong to different domains and have high performance requirements.
notice that cxf u s edi ns ec tion3.
isnotusedintheevaluationaswefailedtorunitsoriginal tests.tworeleasesareselectedfromeachproject namely analyzed release column2 and refactoredrelease column6 .weapply speedooonthe analyzedrelease andevaluatewhetherthemethods in highly ranked optimization spaces are actually optimized in the refactored release.
generally the studied time frame should be long enoughtoallowplentyofperformanceproblemsreportedandfixed.
consideringtheprojects updatefrequencies thestudiedinterval is approximately five years in avroandivy and three years in pdfbox.thentworeleasesintable4wereselectedtocovermost performance problems within this time frame.ranked methods to optimize.
we apply our approach on the analyzedreleaseofeachprojecttoidentifyhighly rankedoptimiza tionspaces.themethodsintheseoptimizationspacesaresuggested for optimization to developers.
the suggested methods all suffer from performance problems running slowly or frequently called .
table5reportsthedistributionofperformancepatternsaggregating the identified methods.
for each pattern we list the numberofpatterninstancesdetectedintheprojectundercolumn count aswellastheaveragesize i.e.numberofmethods intheoptimization spaces under column avg.
size.
we can observe that frequentinvocationandexpensivecalleearethemostprevalent performancepatterns implyingdevelopersshouldpayspecific attention to frequently executed and slow methods.
theimpactscopeofcyclicinvocationandfrequentinvocationis larger than other performance patterns with the average size of optimization spaces ranging from to .
therefore it may take more time for developers to solve these performance problems.
actually optimizedmethods.
wecollecttheperformanceproblems fixed during the two selected releases as the ground truth.
foreachproject wefirstidentifiedalltheperformanceissuesdocumented in jira by matching performance related keywords e.g.
performance and optimization in the issue descriptions as was similarlydoneintheliterature .second wescannedthecommitsbetweentheanalyzedandrefactoredreleasestoextractthe matched commits that mention performance issue ids in the commitmessages.further toavoidmissingundocumentedperformance issues we also scanned those unmatched commits to extract the commits thatmention performance related keywordsin the commit messages.
finally the resulting performance issues and the correspondinglinkedcommitsweremanuallyanalyzedtofilterout non performanceoptimization andthemethodsparticipatingin performance optimization were identified as the ground truth.researchquestions.
wedesignedtheexperimentstoanswerthe following three research questions.
rq1 howistheeffectivenessofourapproachinprioritizingperformance optimization opportunities?
rq2 howisthesensitivityofthemetricsontheeffectivenessof our approach?
rq3 how is the performance overhead of our approach?
.
effectiveness evaluation rq1 to evaluate the effectiveness of the proposed approach we further breakdown the evaluation into sub questions below rq1 howmanyoftherankedmethodstooptimizefromthe analyzedreleaseareactuallyoptimizedintherefactoredrelease?
how does our approach compare to random search assuming developers have no knowledge what to do and yourkit?
this question evaluates whether speedoo can truly identify worthwhile optimization opportunities.
toanswerthisquestion weusethedensity i.e.
thepercentage of actually optimized methods in highly ranked optimization spacestoevaluatetheeffectiveness ofspeedoo .ascomparison wealsocalculatethedensityofactuallyoptimizedmethodsinthewholesystem whichapproximatestheeffectivenessofarandom search and the density of actually optimized methods in the hot spots reported by yourkit.
we do not compar e speedoo with pattern based techniques because each pattern based technique oftendetectsonlyonekindofperformanceproblemsandmost of them are not publicly available.
rq1 howthebefore afteroptimizationpriorityoftheactually optimized method and of the actually optimized optimization space change?
presumably after performance optimization the performanceofthemethodand ormethodsinitsoptimization spaceshouldimprove.consequently theoptimizationpriority score offered by our approach should captur e such improvement to show its effectiveness.
otherwise it implies that the proposed optimization priority score may not be effective the another possibility is that the optimization is not successful .
toanswerthisquestion weusewilcoxontest tocompare theoptimizationpriorityscoresoftheactuallyoptimizedmethods and of the actually optimized optimization spaces in two releases.
wilcoxon test is a non parametric statistical hypothesistest whichisusedtocomparetwogroupsofindependentsamples to assess whether their population mean ranks differ.
without assumingthatthedatahasanormaldistribution wetestatthe significancelevelof0.001toinvestigatewhethertheoptimization authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden z. chen et al.
table distribution of detected performance patterns projectcyclic invocation expensive recursion frequent invocation inefficient method expensive callee others count avg.
size count avg.
size count avg.
size count avg.
size count avg.
size count avro ivy pdfbox table effectiveness of optimization space ranks projectwhole system top spaces top spaces top spaces yourkit hot spots opt.
total den.
opt.
total den.
opt.
total den.
opt.
total den.
opt.
total den.
avro .
.
.
.
.
ivy .
.
.
.
.
pdfbox .
.
.
.
.
table change of optimization priority scores after performance optimization projectoptimized methods methods in optimized optimization spaces total higher rank lower rank p value dtotal higher rank lower rank p value d avro .
.
ivy .
.431e .
pdfbox .
.940e .
of methods in top optimization spaces of actually optimized methodsour approach our approach w o sc our approach w o deour approach w o ai our approach w o gen. tests a avro05101520 of methods in top optimization spaces of actually optimized methodsour approach our approach w o sc our approach w o deour approach w o ai our approach w o gen. tests b ivy0204060 of methods in top optimization spaces of actually optimized methodsour approach our approach w o sc our approach w o deour approach w o ai our approach w o gen. tests c pdfbox figure metric sensitivity in subject projects priorityscoresofthemethodsandtheoptimizationspacessignificantlybecomelowerafterperformanceoptimization.furthermore we use cliff s delta effect size to measure the magnitudeofthedifferenceifwilcoxontestindicatesasignificantdifference.cliff sdelta i.e.
d measureshowoftenthevaluesinadistributionarelargerthanthevaluesinanotherdistribution.theef fect size is negligible for d .
smallfor .
d .
mediumfor .
d .
andlargefor d .
.
answer rq1 .
table compares five groups of methods the whole methods in a system column the methods in top column5 column8 and100 column11 optimization spaces andthehotspotsreportedbyyourkit column14 .for eachgroupofmethods wereportthenumberofactuallyoptimizedmethods column opt.
thetotalnumberofmethods column total and the density i.e.
opt.
total in this group column den.
.
the table shows to of methods in top spaces are actually optimized.
when the investigated optimization spaces adjustfromtop25totop100ones thenumberofmethodsincreasesdrasticallyandthedensityofactuallyoptimizedmethodsdecreasesslightly.itsuggestsdeveloperstofocusonthetopfewoptimization spaces to narrow down the methods as candidates to optimize.
in comparison the density of actually optimized methods in thewholesystemrangesfrom1.
to7.
.thedensitybyourapproachisseveralordersofmagnitudehigherthanrandomsearch when developers have no clue .
yourkit reports a list of hot spots andsuggestseachoneasapotentialoptimizationopportunity.in ordertocomparespeedoowithyourkit we selectedthesamenumber of hot spots excluding library methods with the number of methodsintop100optimizationspaces.notingthatyourkitonly reported hot spots in pdfbox.
the density shown in column and column indicate that our approach outperforms yourkit by times.
we can conclude that our approach provides a significantlymoreeffectiveprioritizationofoptimizationopportunities comparedwithrandomsearchandyourkit andthusisworthwhile.
answer rq1 .
table reports the change of optimization priority scores in the actually optimized methods and the methodsin optimized optimization spaces.
for each group of methods itreports the total number of target methods column total the number of methods whose priority scores increase column higher rank the number of methods whose priority scores decrease col umnlower rank the p value of wilcoxon test and cliff s delta d if wilcoxon test shows significant lower values p value .
.
the table reveals that the optimization priorities of actually optimizedmethodsarenotsignificantlylowerafterperformance optimization incontrast theoptimizationprioritiesofthemethods inactuallyoptimizedoptimizationspacesaresignificantlylower.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
speedoo prioritizing performance optimization opportunities icse may june gothenburg sweden table performance ov erheadofspeedoo projectmetric computationmethod prioritization optimization space localization architectural metrics static complexity metrics dynamic execution metrics avro .
s .
s .
h .
h .
s .
s ivy .
s .
s .
h .
h .
s .
s pdfbox .
s .
s .
h .
h .
s .
s wilcoxon test does not indicate a significant difference in avro be causethenumberoftargetmethodsissmall i.e.
only48methods but most of the methods in actually optimized optimization spaces i.e.
34outof48methods arerankedlowerintherefactoredrelease.
these results indicate that even if the performance of optimized methodsdoesnotshowsignificantimprovements theperformance ofthemethodsintheirspaceswouldbenefitfromtheoptimization.
thisalsodemonstratestherationalityofourprioritizationonthe level of optimization spaces but not on the level of methods.
thesmalleffectsizesintable7indicatethattheperformanceimprovements are limited in most cases.
in fact we find performance optimizationperformedbydevelopersisusuallyminor e.g.
removing an unnecessary type cast.
we also find some highly rankedmethods were optimized many times in history.
it implies that performance optimization is a long way in software evolution.summary.
speedoo is effective i n prioritizing potential performance optimization opportunities and outperforms a state of theart profiling tool yourkit and the prioritization is advisable as the priorities of the methods in actually optimized optimization spaces become significantly lower after performance optimization.
.
sensitivity evaluation rq2 three groups of metrics are used in our approach i.e.
architectural impact metrics static complexity metrics and dynamic executionmetrics thequestionis whatisthesensitivityofthesemetricsontheeffectivenessofourapproach?toanswerit weremoveeach group of metrics from our approach and then estimate the metricsensitivity by comparing the effectivenessof different approaches.
answerrq2.
theresultsarevisuallyillustratedinfig.
wherethe x axisrepresentsthenumberofmethodsintopoptimizationspaces andthey axisrepresentsthenumberofactuallyoptimizedmethods.
generally the curves of our approach without architectural impact metrics ai static complexity metrics sc and dynamic execution metrics de are lower than the curve of our approach with all the metrics especiallysignificantfor avroandivy.theresultsindicate thateachgroupofmetricscontributestotheeffectivenessofour approach and thus a combination of them is reasonable.
similarly wealsoevaluatethesensitivity ofeachmetric.after removing any metric in table from our approach the density in top25spacesdecreasesto14.
.
.
.
and36.
.
for three projects respectively.
the density is lower than or samewithspeedoo withonlyoneexceptioncomingfromremoving ccforpdfbox.itindicatesthateachusedmetric isvitaltospeedoo.
besides wetriedtoassigndifferentweightstothesemetricsand found that using the same weights performed best in general.
in order to cover more methods when collecting dynamic executionmetrics werunboththeoriginaltestsinsystemsaswellasthetestsautomaticallygeneratedbytheunittestingtoolevosuite .in comparison we show the effectiveness of our approach with out generated tests in fig.
our approach w o gen. tests .
the approach without generated tests turns out to be as effective as the whole approach.
this is because the execution behaviors in automatically generated tests are different from the real behaviors in manually written tests.
this suggests that the effectiveness of approachisalsoaffectedbywhetherthecollecteddynamicmetrics can reflect real executions that manifest performance problems.
summary.
all the metrics used in this study have contribution to theeffectivenessof speedoo.besides dynamicexecutionmetrics should be obtained through real execution behaviors.
.
performance evaluation rq3 thisresearchquestionaddresseshowtheperformanceoverhead ofspeedoois.toanswerthisquestion wecollectthetimespentineachstepoverthethreeprojects consistingofmetriccomputation method prioritization and optimization space localization.
answerrq3.
table presents the time consumed in each step of theproposedapproach.wecanseethatcomputingdynamicexecu tionmetricstakesthemosttime.notethatthisstepincludesthetime for running original tests with yourkit as well as the time for generatingtestsbyevosuiteandrunningthemwithyourkit whicharerespectivelyreportedintable8.astheresultofrq2indicatesthatourapproachwithoutgeneratedtestsisaseffectiveasourapproach withgeneratedtests theperformanceoverheadwouldbesignificantlyimprovedwhenapplyingspeedoowithoutgeneratedtests.
generally our approach scales to large scale project like pdfbox.
summary.
speedooscales tolarge scaleprojects andcomputing dynamic metrics takes most of the time as it needs to run tests.
discussion threatstovalidity.
first thevaluesofdynamicexecutionmetrics are affected by the tests that produce them.
basically speedoocan beimprovedthroughtheteststhatcovermoresuspectmethodsand reflect real execution behaviors.
hence in this study the dynamic executionmetricswereobtainedbyexecutingnotonlytheoriginal testsbutalsotheautomaticallygeneratedtests althoughitturns outthatautomaticallygeneratedtestsarenotveryhelpfulasshown insection4.
.inthefuture weplantofurtherinvestigatehowto generateadvancedtests andintegrateitintoourapproach.
second thegroundtruthwascollectedfromthedocumentedperformanceissuesandcommitlogsofsubjectprojects.insomecases developersdidnotdocumentoptimizationoperationsinissuereportswhencommittingchanges.wemitigatedthisproblembyalso matching performance keywords in commit messages in case of lacking issue records.
the second problem is the informal descriptionofperformanceproblemsinissuesandcommitlogs.tomitigateit weadoptedalistofperformancekeywordsthatiswidelyusedin literature and manually checked the matched commits and issues.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden z. chen et al.
analysis of top optimization spaces.
we manually checked somereallyoptimizedmethodsintopoptimizationspaces.wefind that performance optimization tends to be conducted on the methodsinhigherdrhlayers.forexample getdictionaryobject infig.
is in the highest drh layer and its optimization space is ranked .wealsofindthatthemethodswereoptimizedmostlybyminor changes.thisindicatesthattimeisnoteverythinginmotivating performanceoptimization andarchitecturalconsiderationsarealso important.
this also explains why speedoooutp erforms yourkit.
we also checkedthose highly ranked methodswhich were not optimized.
we can discover that some methods were not optimized although users reported a performance issue because it wouldincurhighmaintenancecost e.g.
issueavro orthe code is kept remained for future extensions e.g.
issue avro and some methods have been optimized before the analyzed release but the previous optimization only reduces the performance problem and cannot totally fix it e.g.
issue avro .extensions.
speedoo provides a ranked list of optimization opportunitiestodevelopers.onepotentialextensionistointegrate pattern basedtechniquesintospeedoo.oncepattern basedtechniques locate a pattern in the system we can extend speedoo to determineoptimizationspacesofthemethodsinthepattern which indicates the impact of the pattern on other methods as well as the potential improvement that can be achieved by performance opti mization.anotherpossibleextensionistointegratemoremetrics intospeedootob etter reveal performance problems.
for example we can give higher priorities to the methods with more expensive api calls.
furthermore this study assigns equal weights to the factors in eq.
eq.
and eq.
.
if additional metrics are integrated intoourapproach thefactorscanbeassignedindividualweights which are tuned with different combinations of weights.
related work performanceunderstanding.
severalempiricalstudieshavebeen conducted to understand the characteristics ofperformance problemsfrom differentperspectives .they investigatedrootcausesofperformanceproblemsaswellashowperformance problems are introduced discovered reported and fixed which providesvaluable insights and guidances fordesigning performanceprofilingandperformanceproblemdetectionapproaches.
profiling basedperformanceanalysis.
profilingtools are widelyusedtolocatehotspotmethodsthatconsumemostresources e.g.
memory and time .
besides several path sensitive profiling techniques e.g.
have been proposedtoanalyzeexecution traces of an instrumented program for predicting execution frequencyofprogrampathsandidentifyinghotpaths.further there has been some recent work on input sensitive profiling techniques e.g.
.theyexecuteaninstrumentedprogramwithaset ofdifferentinputsizes measuretheperformanceofeachbasicblock andfitaperformancefunctionwithrespecttoinputsizes.moreover mudduluru and ramanathan proposed an efficient flow profilingtechniquefordetectingmemory relatedperformanceproblems.chenetal.
appliedprobabilisticsymbolicexecutiontogenerate performance distributions while br nink and rosenblum used in field data to extract performance specifications.
these profiling techniquesaremorehelpfultocomprehendperformancethantopinpointperformance problems becausetheyuseresourcesor execution frequencies as the only performance indicator and do not consider the performance impact among architecturally connected code.
thus developers have to waste many manual effort to locate rootcauses.
differently our approachtries torelieve suchburden from developers by prioritizing optimization opportunities.
besides severaladvances havebeenmadetofurther analyze the profiles or execution traces for performance problem detection.
specifically ammons et al.
find expensive call sequencesincall treeprofiles andthecallsequencesthataresignificantly more expensive in one call tree profile than in another calltree profile.
han et al.
and yu et al.
mine performance behavioralpatternsfromstacktracesandexecutiontraces.these approachesheavilyrelyonthetestinputsthatgeneratetheprofiles i.e.
performanceproblemsmightstayunrevealedastheyarenot manifested by those test inputs.
instead our approach also con siders architectural impact and static complexity to avoid solely depending on dynamic executions.pattern basedperformanceproblemdetection.
alargebody ofworkhasbeendoneonthedetectionofspecificperformanceproblems.
one line of work focuses on loop related performance problems e.g.
inefficientloops redundantloops andredundantcollectiontraversals .anotherlineofworkfocuseson memory relatedperformanceproblems e.g.
under utilizedoroverutilizedcontainers inefficientusageoftemporary structuresorobjects andreusableorcacheabledata .besidesthesetwomainlinesofwork researchershave also investigated performance problems caused by inefficient or incorrect synchronization slow responsiveness of user interfaces heavy input workloads inefficient order of evaluatingsubexpressions andanomalousexecutions .inaddition performanceanti patterns areusedtodetectperformance problems satisfying the anti patterns .
while they are effective at detecting specific types of performanceproblems they fail to be applicable to a wider range of performance problem types as these patterns focus on the root causes of specific performance problems.
differently thepatterns inthis studyconsider theoptimizationlocationsfromexecutionsymptoms andaremoregeneric.
conclusions weproposedandimplementedanovelapproach named speedoo to identify the optimization opportunities that should be givenhigh priorities to performance critical methods to improve theoverall system performance.
our evaluation on real life projectshas indicated that our approach effectively prioritizes potential performanceoptimizationopportunitiestoreducethemanualeffort ofdevelopers andsignificantly outperformsrandomsearch andastate of the art profiling tool yourkit.