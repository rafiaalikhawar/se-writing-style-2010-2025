experiences applying automated architecture analysis tool suites ran mo central china normal university wuhan china moran mail.ccnu.edu.cnwill snipes abb corporate research raleigh nc usa will.snipes us.abb.comyuanfang cai drexel university philadelphia pa usa yc349 drexel.edu srini ramaswamy abb inc. cleveland oh usa srini ieee.orgrick kazman seu cmu u. of hawaii honolulu hi usa kazman hawaii.edumartin naedele abb inc. baden switzerland martin.naedele ch.abb.com abstract in this paper we report our experiences of applying three complementaryautomated software architectureanalysis techniques supportedbyatoolsuite calleddv8 to8industrialprojectswithin a large company.
dv8 includes two state of the art architecturelevel maintainabilitymetrics decouplingleveland propagation cost anarchitecture flawdetectiontool andanarchitectureroot detection tool.
we collected development process data from the project teams as input to these tools reported the results back tothepractitioners andfollowedupwithtelephoneconferences andinterviews.ourexperiencesrevealedthatthemetricsscores quantitativedebt analysis andarchitectureflaw visualizationcan effectively bridge the gap between management and development help them decide if when and where to refactor.
in particular the metrics scores compared against industrial benchmarks faithfully reflectedthepractitioners intuitionsaboutthemaintainabilityof their projects and enabled them to better understand the maintainability relative to other projects internal to their company and tootherindustrialproducts.theautomaticallydetectedarchitectureflawsandrootsenabledthepractitionerstopreciselypinpoint visualize andquantifythe hotspots withinthesystemsthatare responsibleforhighmaintenancecosts.exceptforthetwosmallest projectsforwhichbotharchitecturemetricsindicatedhighmaintainability all other projects are planning or have already begun refactorings to address the problems detected by our analyses.
we areworkingonfurtherautomatingthetoolchain andtransformingtheanalysis suiteinto deployableservices accessibleby allprojects within the company.
ccs concepts software and its engineering software architectures permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
software architecture software quality software maintenance acm reference format ranmo willsnipes yuanfangcai sriniramaswamy rickkazman andmartin naedele.
.
experiences applying automated architecture analysis tool suites.
in proceedings of the 33rd acm ieee international conference on automated software engineering ase september montpellier france.
acm new york ny usa 11pages.https introduction although software measurement and source code analysis techniques have been researched for decades making project decisions that have significant economic impact especially decisions about technical debt and refactoring is still a challenge for management and development teams.
development teams feel the increasing challenges of maintenance as the architecture degrades and of ten have intuitions about where the problems are but have difficulty pinpointing which files are problematic and why.
it is still achallengeforthedevelopmentteamstoquantifytheirprojects maintenanceproblems theirdebts asawayofjustifyingtheinvestmentinrefactoring.inthispaper wepresentourexperience ofapplyingthreeautomatedarchitectureanalysisandquantification techniques supported by a tool suite called dv81 on eight large scale projects within abb.
the first technique is measuring architecture maintainability usingapairofarchitecture levelmaintainabilitymetrics decoupling level dl andpropagationcost pc .dlmeasureshowwell asoftwaresystemis decoupled intosmallandindependentmodules thatcanbedevelopedandmaintainedinparallel.pcmeasureshow tightlythefilesinasoftwaresystemare coupled whichindicates theprobabilitythatchangestoonefilepropagatetootherfiles.both metrics were proposed recently by different researchers.
applying both to the same projects could help us evaluate which metric is moreeffectiveandreliable andifandhowtheycanrevealdifferent aspects of the same project.
thesecondtechniqueis architectureflaw detection.moetal.
formally defined a set of architecture flaws that incur high maintenancecosts.filesinvolvedinsuchflawssufferfromoneormore architecturedesignmistakes theseflawshavesignificantimpacton the bug proneness and change proneness ofthe system.
to make authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france r. mo w. snipes y. cai s. ramaswamy r. kazman and m. naedele thepenaltyincurredbytheseflawsexplicit wequantifythenumber ofbugsandchanges aswellasthebugchurnandchangechurn for eachflawusingprojecthistorydata.theuserscanalsovisualize each flaw as a design structure matrix dsm .
the third technique we applied is architecture root analysis proposed by xiao et al.
.
they proposed the notion of a design rule space drspace a set of architecturally connected files to implementapattern afeature orotherimportantsystemconcerns.
theyalsoproposedtheconceptof architectureroots designrule spacesthatclustertogetherthemosterror pronefilesinthesystem.
asreportedbyxiaoetal.
fivearchitecturerootsinaproject almostalwayscover50 to90 ofthemosterror pronefileswithin a project.
inabbwe assembled and integrated these tools to create our automated architecture analysis framework.
using this framework we analyzed eight projects within the company.
these projects aredevelopedatmultiplelocations india usa andswitzerland and differ in their ages domains and sizes.
our study had thefollowing steps first the development teams of abbgranted us access to their code repository from which we collected file dependencyinformation historydata andworkitems.usingthese data as input we ran dv8 which automatically generated metrics scores and visualizable architecture flaws and roots along with supportingquantitativedata.finally wecombinedtheoutputfromthesetoolsintoareportforeachproject andpresentedthesetothe development teams.
after we ensured that the development teams understood the reports we conducted a phoneor email interview with each team to collect their feedback and most importantly toseeifthesetechniqueshelpedthemtodetermineif when and where to refactor.
our experiences have shown that the two metrics pc and dl canfaithfullyreflecttheextenttowhichaprojectisexperiencing maintenancedifficulty.thecomplementarynatureofpcanddl can provide useful insights as we will show.
the architecture flaw detector can effectively pinpoint which files are suffering from which specific design problems.
this visualization and quantification has effectively bridged the gap between management and developmentteams.exceptforthetwosmallestprojects containing justafewhundredsoffileseach andthehighestmetricsscores allotherprojectsarenowundergoingmajorrefactoringstoaddressthedetectedflaws.finally thefeedbackwereceivedregardingroot analysis isdivided someteams foundthat it providedan effective way to detect architectural problems since they only needed to examinefivegroupsofrelatedfiles.butotherteamsfoundthata rootcanbemisleadingwhentherearehighlyinfluentialutilityfiles that may distort their results.
research questions ourgoalinemployingtheseanalysistechniqueswithin abbwas to investigate the following research questions rq1 does dv8 help to close the gap between management and development?
that is does it help them to decide if when and where to refactor?
rq2 doesdv8 help practitionersunderstand the maintainability of their systems relative to other projects internal tothecompany andrelativetoamorebroad basedbenchmark suite?
rq3 doesdv8helpdeveloperspinpointthe hotspotsoftheir systems thatis thegroupsoffileswithseveredesignflaws?
we investigated these questions by interviewing the developmentteams analyzingtheirexperienceswiththeprovidedtools andsolicitingfeedback.thisinterviewprocessalsoallowedusto understand the limitations of dv8.
procedure in this section we present the projects where we applied the architectureanalysisframework thedataneededtorunthetoolsuite and the overall structure of the analysis framework as shown in figure1.
subjects.
table1presentstheeightprojectsthatweanalyzed.
the lan .
column shows the main language used.
the files column shows the number of files in the project.
for proj chand proj ec we measured multiple releases of each so we listed the rangein the number of files in these projects.
the column com .
presents the number of commits over the time period studied.
the column period shows the period of time we analyzed for each project.thecolumn p presentsthenumberofpeoplewhomade commits during this time period.
consider proj bmas an example it contains source files and the history studied is from april 2015tojuly2017.itwasmaintainedby8full timedevelopers whomade536commits.therealprojectandfilenamesareanonymized in this paper.
table studied projects lang.
files com.
period p proj eo c proj bm c c proj ch c c proj ep c proj ss c c proj op c c proj co c proj ec c c n a data needed for the tool suite dv8 .
once a development teamagreedtoparticipateinourstudy theygrantedusaccessto their code repository and the specific version s they wanted toanalyze.fromtherepository weextractedthreetypesofdataas the input to dv8 as shown in figure thedependencyinformationamongsourcefiles.givenaproject sourcecode wefirstused understand2 acommercialstaticanalysis tool togenerate a file leveldependency report in xmlformat for each project version to be analyzed.
therevisionhistoryoftheproject.weextractedtherevision history of a project from the team foundation server3 tfs system usedinabb.tfsrecordseachcommitasachangesetandprovides console commands through which we can export the changesetintoaplaintextfile.thisfilerecords foraspecifiedtimeperiod whichfileswerechangedtogetherinwhichcommit andhowmany lines of code were changed in each file.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
experiences applying automated architecture analysis tool suites ase september montpellier france bug records.
tfs also has a work item tracking system where thedeveloperscanrecordbugsorothertasks suchasaddinganew feature.
when a developer makes a commit he she can link the commit changeset to a workitem.
from the work item tracking system wecandownloadallthebugreportsandtheirassociated changesets in xml format.
the xml files recording dependency information among source files the plain text file recording all changesets in the revision history andthexmlfilerecordingbugfixesweretheinputneeded by the automated tool suite as shown in figure .
automated architecture analysis framework .
this framework contains the following components sdsm generator.
the representation used by dv8 is a design structure matrix dsm first proposed by baldwin and clark .
we will elaborate the model using examples in section v. the sdsm shortfor structuredsm generatortransformsaxmlfile containing file dependency information into a dsm.
hdsm generator.
this component transfers the plain text file recording changeset information into a dsm format which we callahdsm shortfor historydsm sothatbothstructuralandhistory information can be processed simultaneously.
dl pc calculators.
these two calculators take a sdsm as input and output the decoupling level and propagation cost scoresrespectively calculatedusingthefiledependenciesoutput byunderstand.
flaw detector.
this component takes both a sdsm and a hdsm as input and generates a set of new dsm files each of which contains an architecture flaw which we will describe in section v as defined in mo et al.
s work .
each flaw dsm can alsobeexportedintoaspreadsheetforfurtheranalysisandbroad dissemination.
rootdetector.
thiscomponenttakesasdsm ahdsm andthe changesetfileasinput andgeneratesasetofdsms eachcontaining an architecture root capturing the most change prone or bugprone files in the system.
the change proneness or bug proneness of files can be ranked by the number of changes bugs to any given file which can be calculated from the project s changeset file.
a dsm containing the root can also be exported into a spreadsheet.
flawcostcalculator.
thiscomponentextendsthe flawdetectorcomponentbycalculatingthenumberofloc thenumberof changes andthenumberofbugfixesincurredbyeachflaw.ittakes the output of the flaw detector as input and uses the changeset file and bug records to calculate the maintenance costs related to each flaw.
it outputs the results into a spreadsheet as we will elaborate later.
debt calculator.
inspired by the experiences reported in this component calculates the expected number of additional bugs changes and churn incurred by a root or a flaw as compared to systemaverages.theoutputofthiscomponentisalsoaspreadsheet.
reportgenerator.
thiscomponentautomaticallyputstogether areportbysummarizingtheoutputoftheothercomponents includingthemetricsscores thesummarizationofflawsandroots andthecostsanddebts.wecanmanuallyaddproject specificprose as needed to the report before sending it back to the development team along with supporting data.
for each project the report contains its basic facts dl and pc values with the corresponding percentile rankings detected flaws and roots with the involvedfiles andthecostsanddebts.wethenpresentthereportandthe associated data to the development team to help them properly interpret the results.
ofallthe8componentsindv8 thefirstfivewereobtainedfrom theresearcherswhooriginallycreatedthesetechnologies.thefinal three components are extensions newly developed for abb.
all these components have been integrated into our framework to automate the architecture analysis.
afterthedevelopmentteamhadtime atleastoneweek tofully digestthereport wesetupafollow upinterviewtoevaluatethe effectiveness of the tools and their results.
we asked the architects andprojectmanagersasetofpre definedinterviewquestions as explained in section so that we could confidently answer the research questions posed in section .
figure analysis framework architecture measurement we first measured the maintainability of each project using the dl and pc calculators as shown in figure and compared each project s scores with an industrial benchmark suite so that both the management and development team can understand how their projectcomparestoothersinindustry.forprojectswheremultiple releases were available to us we also calculated the variation of dl and pc over time to see if the trend matched the practitioners intuitions.
next we first introduce these two metrics and then present the results.
.
two architecture level metrics decouplinglevel dl wasproposedbymoetal.
tomeasure howwellasoftwaresystemis decoupled intoindependentmodules.
thetheoreticalfoundationbehinddlisbaldwinandclark sdesign ruletheory themoreindependent small andactivemodules there are the higher option values the system can produce.
based on this rationale their algorithm first clusters all source files into a designrulehierarchy drh ahierarchicalstructurewith two features files in lower layers only depend on files in higher layers files within the same layer are grouped into mutually authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france r. mo w. snipes y. cai s. ramaswamy r. kazman and m. naedele independentmodules.basedondrh dliscalculatedasfollows the more independent modules there are the higher the dl the largeramoduleis theloweritsdl.foramodulethatinfluences others the more dependents it has the lower its dl.
propagationcost pc wasproposedbymaccormacketal to measure how tightly coupleda system is.
the calculation of a system spc valueisbased onadesign rulematrix whose rows and columns are labeled with the files in the same order.
based on this matrix their algorithmfirst representsthe directdependency relations among files in a system and then calculates its transitive closure to add indirect dependencies to the matrix until no more dependenciescanbeadded.anonemptycellinthematrixindicates anindirectordirectdependencybetweenthefileontherowandthe file on the column.
given the final matrix containing all direct and indirectdependencies pciscalculatedasthenumberofnonempty cells divided by the total number of cells.
pc has been used to analyze large projects with similar domains and sizes .
mo et al.
calculated the pc and dl of projects and published the data as an industrial benchmark.
figure 2depicts the cumulative distributions of benchmark dl and pc values.
the red solid line represents the probability that a random dl valueis less than or equal to a specific value.
for example the mark 80th .
on the red solid line indicates that of the projects in the benchmarkdata have dl scores less thanor equal to .
.
thebluddashedlinerepresentstheprobabilitythatarandompc value is larger than or equal to a specific value.
for example the mark 80th .
atthebluedashedlineindicatesthat80 ofthe projectsinthebenchmarkdatahavepcscoreslargerthanorequal to7.
.thefigureshowsthatdlandpccomplementwitheach other ahighdl and a lowpc indicate bettermodularity and maintainability and a lowdl is usually associated with highpc indicating lowerlevel of modularity and maintainability.
figure cumulative probability distribution of dl and pc .
measuring and ranking of maintainability foreachproject wefirstcalculateditsdlandpcscores thencalculated itspercentile ranking as comparedwith the benchmarkdata.
forexample thedlandpcofproj eoare78 and6 respectively ranked the 85th among the projects for both metrics meaning that its modular structure is better than of the benchmarkprojects.
table 2summarizes the dl and pc scores for the latest version of each project showing their metric values percentile rankings and the number of files.
therowfor proj epshowsthatitsdlis72 betterthan74 of all benchmark projects.
the pc of this project is lower better than of the benchmark projects.
as we can see from the table in general a higher dl is associated with a lower pc and their percentile rankings are largely consistent differing by less than 10percentile points.
there are exceptions however such as proj ch itsdlis76 ranking81stofallprojects butitspcis16 only ranking the 54th.
considering that this project has files changes to one file may affect directly or indirectly approximately files suggestingthatthispartofthesystemwillsufferfromconsiderable maintenance difficulty even though the majority of the systemis reasonably decoupled.
this observation was confirmed by thedevelopment team.
as we discuss later using architecture flaw androotanalyses we were able to pinpoint the file groups and their architecture flaws that are responsible for this maintenance difficulty.
for two of the eight projects the product organization requested that we calculate the dl and pc values of multiple snapshots to assesswhetherthearchitectureisdegradingornot andiftheassessmentisconsistentwiththepractitioners intuitions.for proj ch thedlofitslatestreleasewas76 .sincethen thesystemhasbeen changed considerably.
the development team asked us to measure a more recent working version and obtained a dl of showing thatthearchitecturehasdegradedsincethelastrelease.itsarchitectconfirmedthatthedegradationwasexpectedasthereleasewasstill inalphatestingafterimplementingamajortechnologyimprovement.theprojectteamplanstocontinueworkingonrefactoring the code to improve the architecture.
we also calculated the scores for six releases of proj ec.
all these releases have very low dl scores and high pc scores .thepractitionersconfirmedthatthisprojecthas always been difficult to maintain a seemingly simple change often causedalargenumberofunexpectedchangesinvolvingmultiple files.
forthe latestversionof proj ec its dlis only28 ranking in the 5th percentile and is the worst of all eight projects.
onepractitioner commented ... revising even two lines of code would require double digit man months indicating that the project has been extremely difficult to understand and maintain.
wereportedourresultstothearchitectofeachprojectandasked whetherdlandpcanalysesfaithfullyreflectthemaintainability oftheirprojects.allthearchitectsconfirmedthatthedlandpc scoresindeedreflectedtheirknowledgeofthesoftwarearchitecture andfurtherhelpedthemtopresentthearchitecturequalityissues to managementin a quantitative way.all four projects whosedl valuesrankedlowerthanthe50thpercentilereportedthattheywere experiencingseveremaintenancedifficulty.oftheeightprojects other than proj eo and proj bm that are small and have high pc and dl values the other six project teams have decided or have alreadybegunrefactoringtoaddresstheproblemspresentedinour reports.
the architects also expressed their willingness to leverage dlandpcmetricstocontinuouslymeasure quarterlyorevenat every release the architecture of their projects.
by tracking the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
experiences applying automated architecture analysis tool suites ase september montpellier france variations of dl and pc values regularly they believe that they could monitor whether their architecture is improving or decaying.
table data of each project s dl and pc dlpercentile pcpercentile files proj eo 85th 85th proj bm 85th 98th proj ch 81st 54th proj ep 74th 83th proj ss 49th 45th proj op 49th 41th proj co 43rd 52th proj ec 5th 2nd architecture flaw analysis thedlandpcscoresonlyprovideacoarseassessmentofaproject s overall modularity.
but a development team needs to precisely determinewhereandhowthesystemshouldbeimproved.in mo et al.
formally defined a set of architecture design flaws4 shown to behighlycorrelatedwitherror pronenessandchange proneness.
we applied their flaw detector to the eight abbprojects and created an extension of the tool to calculate the maintenance costs of each flaw which we call the flaw cost calculator.
we reported theresultstopractitionerstoseeifthisanalysiscouldhelpthem pinpointthe filegroups responsiblefor maintenanceproblems to visualizearchitectureflaws andtomakerefactoringdecisions.in thissection we introducetheconceptofarchitectureflaws and then report the flaws detected in abbprojects.
.
six types of architecture design flaws moetal.
firstdefinedfivetypesofarchitecturedesignflaws that were repeatedly observed from many software systems in cluding unstable interface where an influential file changes frequently with its dependents as recorded in the revision history modularity violation where structurally decoupled modules frequently change together unhealthy inheritance where a base classdependsonitssubclassesoraclientclassdependsonboththebaseclassandoneormoreofitssubclasses cyclicdependency or clique whereagroupoffilesformastronglyconnectedgraph and5 packagecycle wheretwopackagesdependoneachother rather than forming a hierarchical structure as it should .
their tool was recentlyextendedtodetecta6thtypeofflaw crossing where a file with both high fan in and high fan out changed frequently with its dependents and the files it depends on.
a system may have multiple instances of any flaw and each can be visualized as a dsm using existing tools such as titan .
as exemplifiedinfigure adsmisasquarematrixinwhichcolumns androwsarelabeledusingthesamesetoffilesinthesameorder.
theannotationsineachcellindicatethestructuralandevolutionary relations between the file on the row and the file on the column.
for example the cell in row column cell r3 c1 contains d which means that path1.file3 cpp depends on path1.file1 cpp and they have changed together times as recorded in the revision history.cellswithnumbersonlyindicatethattherearenostructural dependencies between the files but they were changed together.
cellswithlettersonlyindicatethatthefileontherowsyntactically 4which are also called as hotspots o r issues depends on the file on the column and they were not changedtogether.
the cells along the diagonal indicate self dependency.
figure3depicts two dsms each representing a flaw instance from aabbproject.
figure 3apresents the dsm of an instance of clique theactualfilesnamesareanonymized whichshowsthatthereare 16filesintheclique changestoanyoneofthemcouldinfluence the other .
.
architecture flaws detected in practice theflawdetector component asdepictedinfigure automatically detectedtheflawswithineachproject andoutputadsmfilefor each flaw instance which became the input of the flaw cost calculatorcomponentthatquantifiedthemaintenancecostsofeach flaw so that the users could better prioritize and rank them.
in this component four measures extracted from revision history were used to quantify maintenance costs change frequency cf the number of commits a file is involved in change churn cc the number of lines of code loc committed to make changes bug frequency bf thenumberofbugfixesafileisinvolvedin bug churn bc thenumberofloccommittedforbugfixes.notall projectshaveallthedataneeded.ifthecommitsofaprojectdonot linktoissues orissueswerenotcategorizedintobugs features etc.
thiscomponentwillonlycalculatecfandcc.somelegacysystems do not keep records about the loc changed in each commit.
in thesecases thiscomponentonlycalculatesbforcf.theoutput generated by these two components has three parts flaw summary.
as an example table 3summarizes the flaws detectedin proj ep theirscopesandmaintenancecosts.
thefirst lineshowsthatthereare322files ofallthefiles involvedin 26cliqueinstances.thesefileswerechanged1 790timesinvolving loc of all the loc changed in the revision history.
of the changes are for bug fixing involving loc which is of all the loc spent for bug fixing.
this table shows that cliques are very expensive to maintain in this project.
flaw costs.
as an example table 4shows that clique1 involves files and incurred the most maintenance costs definitely worth attention.
clique5 although it contains just files also appearstobeverycostly.usingthistable thedevelopmentteam can prioritize which flaws need to be addressed in which order.
bycomparingwithsystemaveragebugandchangerates wecan see that files involved in these flaws are causing high maintenance difficulty.
flaw dsm.
for each instance of each flaw the tool generated adsmwhichweexportedasaspreadsheetsothatthedevelopment teamcouldvisualizetherelevantstructure.figures 3a 3bpresent thedsmsofseveraltypicalflawsthatwereportedtothearchitects together with their maintenance effort.
figure3ashowsacliquewherefilesarehighlycoupledby cyclic dependencies.
for example path1.file1 cpp path1.file2 cpp and path1.file3 cpp formtwodependency cycles and these files changed together frequently.
figure3bshowsacrossing centeredon path2.file2 h which dependsonfourfiles andinfluenceseightother files.this filechangesfrequentlywithallthese12files andrankedthe 8thmosterror prone e.g.
changedforbugfixes11times and change prone among all files in proj ch.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france r. mo w. snipes y. cai s. ramaswamy r. kazman and m. naedele table architecture flaws in proj ep pt.
percentage flaw cf bc maintenance costs quantified by cf cc bf and bc of the files in each flaw instances files pt.flaw cf pt.flaw cc pt.flaw bf pt.flaw bc pt.
clique crossing modularityviolation packagecycle unstableinterface unhealthyinheritance table maintenance costs of clique instances tot.
cf bc the total cf bc of all files in each clique instance instance name sizetot.
cf tot.
cc tot.
bf tot.
bc clique1 clique2 clique3 clique4 clique5 a a clique highlighted cells form the dependency cycles.
effort tot.
cf tot.
cc tot.
bf tot.
bc .
b a crossing the cell in red is the crossing file blue cells show dependencies and their co changes.
effort tot.
cf tot.
cc tot.
bf tot.
bc .
figure example dsms of architecture flaws d depend number co changes aftersubmittingthereporttothedevelopmentteams wealso conducted a telephone conference to review the results in case the developerswerenotfamiliarwithdsms.duringthepresentation andinteraction thedevelopmentteamsallcommentedthatthese architectureflaws revealedkey problemsthat theyhadsuspected but had no way to specify or quantify before.
architecture root analysis unlike the flaw detectors that identify many file groups the root detector asshowninfigure generates10 orfewer filegroups that typically capture the majority of a system s most error prone files.ourobjectiveistoassesswhetherrootanalysiscanhelpdevelopment teams pinpoint architectural problems more effectively.
.
architecture roots xiao et al.
proposed that software architecture can be modeled as multiple overlapping design rule spaces drspaces each containinganarchitecturallyconnectedgroupoffiles.theyalso define the top few drspaces that capture most error prone files asarchitecture roots orrootsfor short .
they have shown that five roots can typically cover to of all the error prone files in a system anobservationvalidatedoverdozensofindustrialandopen source software systems.
the implication is that most error prone filesarearchitecturallyconnected andthatthemoreerror prone thefilesare themorelikelythattheyarearchitecturallyconnected and that errors propagate through the connections.
a root can also be modeled using a dsm.
figure 4presents a root detected from one of the projects.
this dsm reveals mul tiple architecture design flaws.
for example p1.f1 an unstable interface is depended upon by most of the files and most of thesedependents changed togetherwithitfrequently multiple dependency cycles are identified such as p1.f5 p2.f2 and p2.f2 p2.f1 p1.f6 p1.f5 p2.f2 p1.f1 depends onitschild whichisunhealthyinheritance manymodularity violationsarehighlightedinred structurallyindependentmodules thathave changedtogetherfrequently.wealsoshowed thechange rateandrankingofeachfileinthefirsttwocolumnsofthedsm.
for example the file p4.f3 in row was changed times andrankedthemostchange proneamongall2 403changedfiles inproj ss.inthisroot ofthefilesrankedwithintop10percentile most change prone and out of the files ranked within top1percentile whichindicatesthatthisrootisarealmaintenance hotspot.
.
root analysis in practice we useproj epas an example to illustrate the roots detected in theseeightprojects.in proj ep the rootdetector detected4roots and generated the following data the scope and cost of each root as shown in table .
for example the first row shows that the first root involves files.
thesefileswerechanged1 109times consuming13 487loc.of these changes were bug fixes involving loc.
as we can see from the table even though a root only covers a small portion authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
experiences applying automated architecture analysis tool suites ase september montpellier france figure drh clustered architecture root d depend i inherit cf change frequency top percentile rank ofthe system itisa hotspotwheremuchmaintenance effortwas spent.
table data of each detected architecture root percentage rt.
cf bc the total cf bc of all files in each root size rt.
cf rt.
cc rt.
bf rt.
bc root1 root2 root3 root4 thecumulativedatafor allroots.afilemayparticipatein more than one architecture root that is roots overlap with each other.dv8alsocalculatestheircumulativedata asshownintable .
inthistable size meansthenumberofdistinctfilesinthefirst nroots where n ... .
the size column presents the percentage of the root size compared with the total number of files in the project.
for example in the second row meansthat root1 androot2 the first roots contain distinct files whichcovers14 ofallfilesintheproject.the covera e column presents the cumulative coverage of change prone or bug prone files by these roots.
the fourth row of this table indicates all these roots contain only of all the files in this project but cover of all change prone files and of all bug prone files.
filesin each root are architecturally connected hence it appears thatchange proneness or bug proneness may be propagated among these files.
moreover followingtheexperiencereportedin andconsideringeacharchitecturerootasa debt wecreatedthe debtcalculator to compute the penalty incurred by these roots as the difference betweentheactualmaintenanceeffortspentandthe expectedmaintenance effort spent on them.
we use the average change bug rate of all the files in each project as the expectedmaintenance effort following .
theexpectedeffort columns extracf extrabc represent the cumulative maintenance penalty from the roots.
for example in the second row of extrabf column indicates that the files in root1 androot2 are involved in bug fixes 615timesmoreoftenthanaveragefiles.the percenta e rowpresents the percentage of the extra maintenance effort to whole project.
the last row indicates that of all the changes of all the loc spent of bug fixing changes and of bug fixing loc spentontheentireprojectareincurred orpenalties bytheseroots.
table cumulative data of architecture roots proj ep coverage root size size change bug root1 root2 root3 root4 penalty of architecture roots extra cf extra cc extra bf extra bc root1 root2 root3 root4 percentage wehaveobservedconsistentresultsfromalleightprojects as summarized in table .
column all roots tot .size shows that for all the projects their detected architecture roots contain ofallfilesineachproject butcoveramuchlargerportionofthe project schange prone orbug pronefiles asshownincolumn tot.covera e .duetothelackoftraceability ofthebugs wedidnotconductthebug relatedanalysisonthelast three projects.
the penalty columns show that for all projects a large portion of maintenance effort spent on a project is generated from the detected architecture roots.
aswewillelaboratelater thefeedbackregardingthedetected rootswasdivided.someteamsfoundthatrootscancapturehotspots more effectively than the flaw analysis since they only need to examine a few file groups but other teams found that the detected roots can be distorted.
table architecture root analysis results of all projects tot.
ext cf bc showing the ratio of penalty from all roots in each project to the total maintenance effort spent on the project project rootall roots tot.
coverage tot.
size change bug proj ch proj co proj ep proj op proj ec proj ss proj bm proj eo penalty percentage of all architecture roots tot.
ext cf tot.
ext cc tot.
ext bf tot.
ext.
bc proj ch proj co proj ep proj op proj ec proj ss proj bm proj eo interviews and feedback tobetterunderstandhowtheinformationpresentedbytheanalysis framework was understood and used we formulated a set of questionsforthepilotparticipantstoansweraftertheyhadreviewed authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france r. mo w. snipes y. cai s. ramaswamy r. kazman and m. naedele the results.
participants were provided with the questions in advanceofa telephoneconferenceconductedto record theiranswers.
thequestionsweredesignedtofollowthe keydeliverablesofthe report themetrics architectureflaws andarchitectureroots.we posed the questions to five participants who represented one or more of the projects we analyzed.
three of the participants were software architects and two were r d managers.
our objective was to explore how the development teams intended to react to andusethereporttoimprovetheirarchitecturequality.nextare the questions and the summary of their responses q1 what did the report reveal that you didn t know about your software?
prior to having this report the participants had intuitive understandingoftheirarchitectures.oneofthemcommented we understandintuitivelyhowourcodeisstructured.
thedlandpc scoresweresurprisingforthearchitectsofproj opandproj ss whothoughtthereportindicatedtheircodewasbetterthantheir opinion of it.
for proj op their maintenance effort was higher thantheythoughtthemetricsindicateditshouldbe.forproj ss some programming languages used in the project were not fully supported by understand therefore the scores were better than they should be due to missing elements.
q2 are the metrics useful for reflecting the architecture of your software?
all the participants commented that the report provided them quantifiable results and actionable items to improve theirarchitecture as well as a way to discuss the importance of refactoring their architecture with managers.
for proj ch the trend of metrics between releases was useful to understand whether the architecturewasimprovingordegrading.forproj ecandproj ss the participant thought the scores were useful and would like to rundailyanalysestocomparethevariationsofthescoresovertime.
theneedforupdatedreportswasalsoexpressedbyprojectsbm co andep.proj eohadaveryhighdlandalowpcscore but they still had more rework effort than expected as they attempted to evolve their product which indicates that metrics derived from syntactical relations only may not be sufficient.
q3 what did the architecture design flaws reveal about your software?proj eo was surprised by their architecture design flaws and found a few false positives.
all other projects reported that the detected flaws confirmed what they intuitively knew about the structure of their software and made their intuitive knowledge visualizable and quantifiable.
q4 what actions have you planned as a result of the architecture design flaws report?
six of the projects said they planned to perform refactoring to address the detected architecture design flaws.
proj bm and proj eo do not plan to refactor and indeed their dl andpcscoreswerehigh.thearchitectofproj chcommentedthatthearchitectureflawreportprovidedthemquantitativedatatohelp prioritize where to refactor the code.
proj ec and proj ss which are already actively refactoring desired more specific guidance towards how they should refactor.
q5 what did the architecture roots reveal about your software?
proj op reported that roots in a particularcomponent were unexpected and probably indicated a higher amount of development activity in normally stable code.
the participant commented we may have underestimated the risk of changing this part of the code.
for proj ch and proj op the architecture roots pointed to utility files that should not be considered as roots of the structure.
thesemoduleswereusedbymanyothercomponentsthustheanalysis consideredthemasroots.proj ecandproj ssfoundthefilesreportedasrootswereexpectedbecausetheycontaindefinitionsthat change frequently.
q6 whatactionsdoyouplantotaketoaddressarchitectureroots?
proj op had a plan to componentize the architecture which is expectedtoaddresstherootsandimprovethemetricsscores.they commented we plan to monitor our progress in architecture decouplingusingthese metricsovertime.
proj ecandproj ss reported being unsure about how to proceed with improving their architecturerootsandfelttheylackedanaccuratementalmodelofwhat anarchitecturerootis.proj chexpressedasimilarsentimentbecause utility files were identified as roots and they did not fully understand the purpose of root identification.
by contrast proj ep confirmed that the detected roots are composed of defect prone files and they were planning to refactor them to improve quality.
answers to research questions we summarize the feedback provided by the development teams to answer each research question below.
rq1 does dv8 help to close the gap between management and development?thatis doesithelpthemtodecideif when andwhereto refactor?threeoftheparticipantsinchargeof5projects proj bm proj co proj ep proj ch andproj op verifiedthattheinformation provided was useful in closing the understanding gap with management.
even though the other two participants didn t explicitlycommentonthisaspect thefactthatsixofeightprojects planned or had already begun refactoring their code to address the flawsandrootssuggeststhatourreportplayedaroleinreaching these refactoring decisions.
rq2 does dv8 help practitioners understand the maintainability oftheirsystemsrelativetootherprojectsinternaltothecompany andrelativetoamorebroad basedbenchmarksuite?
alltheparticipants said the report gave them quantifiable results with which to judge theircodebase.twoweresurprisedonhowgoodtheirproducts wererated thatis closetothe50thpercentilewithintheindustrial benchmark giventheirintuitiveunderstandingofthemaintenance effortinvolvedfortheirproducts.thecomparisonwithindustrial benchmark makes it clear that maintenance difficulty caused by degrading architecture is very common.
rq3 does dv8 help developers pinpoint the hotspots of their systems thatis thegroupsoffileswithseveredesignflaws?
based on the feedback from all participants the answer to this ques tion is clearly yes.
six of the eight projects planned to or already started refactoring to address the detected flaws.
the project with thelowestdlscoreisundergoingamajorrewrite.onepractitioner expressedtheneedformoredetailedguidanceonhowtorefactor the detected flaws.
in summary we can answer all three questions positively.
lessons learned inthissection wediscussthelessonslearnedinterms theeffectivenessofthesetechniques thedataquality andthelimitations that lead to future work.
usingdlandpc.
the practitioners adopted dl and pc easily andexpressedtheneedtocomparemultipleprojectsandanalyze authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
experiences applying automated architecture analysis tool suites ase september montpellier france multiplereleasesofthesameprojectusingjustafewmetricssothat theycanmonitorthequalityofthearchitecture.wesummarized the following lessons regarding how dl and pc should be used in a complementary way if a system has a low dl and high pc score it means that maintenancedifficultyisinevitable andthisconclusionisconsistent with the experience of practitioners so far we have seen no exceptions.
if the dl and pc scores are both highly ranked it means the system is likely not experiencing severe problems.
if the developmentteamisexperiencing maintenancedifficulty thissuggests thatthesystemhasalargenumberofimplicitdependencies.inthis case architecture flaw detector should be executed to pinpoint the problematic file groups.
ifthesystemhasahighlyrankeddl butamuchlowerranked pc such as proj ch where dl is ranked the 81st percentile but pc is only ranked the 54th percentile it means that there couldbe a small portion of the system that is highly coupled which isconfirmed by the development team.
this result implies that an overalldlscoreonlymaynotbeabletoreflecttheexistenceofa high maintenance subsystem.
if both scores are ranked medium e.g.
the dl of proj co ranked the 43rd and its pc ranked the 52nd then the project is likely experiencing maintenancedifficulties already as confirmed by the development teams.
we have not observed a case where the system has a highly rankedpcscore butitsdlrankingisverylow.sincepcisvery sensitivetothesizeoftheproject asreportedin wesuggest that these two scores should be used together.
another lessonis that a good dl and pc score does not necessarily mean that a systemishealthy.ifthedevelopmentteamisexperiencingdifficultydespitegoodscores itisworthwhiletouseflawdetectorstofurther pinpoint where the difficulty comes from.
usingflawsandroots.
inadditiontoquantifyingflaws visualization of each flaw augments the intuition of developers bridging the gap between development and management.
currently we exportflawdsmsintospreadsheets andmarkthefilesinvolvedin each flaw manually.
in the future we will further automate this process.
the experiences of root analysis are divided.
some teams like thefactthatusingroots theyonlyneedtoinspectafewfilegroups andobservehowmosterror pronefilesareconnected.otherteams foundthatsomerootsarecausedbyhigh impactutilityfiles.because these files have a large number of dependents they can form a large drspace that captures a large number of files including both error proneand healthyones thusdistorting theresult.
the lesson here is that the high impact utility files should be excluded from this analysis.
we also learned that the pilot participants required comparison reports which showed the evolution of a product between releases.
these reports were usefulbecausethe teamcould understandthe trend of whether their architecture was degrading or improving.
the report contained comparisons of the key metrics plus comparisons of the architecture flaws and which classes were involved in architectureroots.wefoundsomerootspersistedbetweenreleases which means that they were a consistent source of development effort across releases.one comment since the guidance of the report is towards refactoring it makes sense that a release to release comparison would be a useful way to integrate it into the software development life cycle... i wouldlovetohavethisintegratedintothedailybuild withautomatic production of guidance to architects to help them with day to day architectural governance.
dataquality.
anotherlessonwastoconsidermultiplebranches when collecting history data.
what the participants provided as a known good source code branch frequently was a branch with veryfewactualdevelopercommits theyweremostlymerges thus littlehistoryofeachfilecouldbecollected.byconsideringmultiplebranchesforhistoricalchanges weexpandedthesetofcommitsin theanalysisandhadamorecompleteviewofthehistoryofeach file.
ofallthethreetypesofanalysisprovidedbydv8 thedland pc are structure metrics calculated based on syntactic relationsonly.
three architecture flaws require structural relations only clique improper inheritance and package cycles.
the other three flaws aswellasrootanalysisrelyonrevisionhistorydata.ifthe issue tracking data is available issues can be categorized e.g.
into bug fixing featureaddition etc.
andeachcommitislinkedwith anissue thenwecancalculatebothchangerelatedcosts change frequencyandchangechurn andbug relatedcosts bugchurnand bug frequency .
if the issue tracking data is not available or notlinked with commits then we can only calculate change related costs.
consistent with our observations with other industrial projects mostofthecomany sprojectsdonothavethedataneededforall analyses.
some projects have a long history back to the time when modern version control systems were not available.
other projects used their own issue tracking systems that do not support issue categorization.
there are also projects in which the commits were not linked to issues.
aftertalkingtothedevelopmentteams werealizedthatinsome projects the developers were not required to link commits with issuessincetheydidn tenvisionthepossibleusageofthedata.now that they have seen the benefits of these analyses a more rigorous management process was being discussed.
limitationsandfuturework.
theprocessofanalyzingthe8 projectsalsorevealedseverallimitationsofdv8andtheunderlyingtechnologies.firstofall thedefinitionofcertainarchitectureflaws should be further refined.
for example the tool may detect a large numberofmodularityviolations mv thatoverlapwitheachother.
sometimesthe numberof filesinvolved inmv isso largethat the instances were just ignored all together.
we are exploring the possiblewaystofurtherrefinethedefinitionofmvanditsdetection algorithmtoobtainmorefocusedresults.second basedonthefeedback we received about root analysis we plan to further refine the root detectionmethods sothat utilityfiles canbe excluded.third dv8 is limited to c c c and java the mainstream program minglanguagesthatcanbeaccuratelyprocessedby understand .
processing multi language systems is a future direction of the tool suite.
finally we will further increase the number of projects inthe benchmark database so that the users can compare projects withsimilarsizesanddomains.eventhoughwehavenotobserved that dl is affected by these characteristics pc can be significantly affected by the number of files in the project.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france r. mo w. snipes y. cai s. ramaswamy r. kazman and m. naedele findings with respect to the research questions have a threat to external validity in that they are based on the opinions of only practitioners and the set of practitioners is focused on architects and managers not developers.
during this process we have observed the need to further automatetheprocesstoenabletheanalysisofmorecomplexcodebases forexample asystemmaycontainmultiplecomponents.eachcomponent should be analyzed individually and the system should be analyzed as a whole.
we have worked to automate the analysis by addingconfigurationmanagementtotheinterimdataandapplyingscriptstoautomatethegenerationofreportdata.thepractitioners alsosuggestedthattheywouldliketoknowwhetheraproduct s dl andpc values have gotten worse over night.if so theywould liketoknowhowtolocatethespecificproblemarea.theyenvision avisualdiffofthedsm.thentheywouldneedautomatedguidance on what to change which file and which refactoring to repair the situation before problems accumulate.
so far our analysis is at the filelevel oneofthepractitionerssuggestedthatwecoulddigmore details at the method level.
if so they would like to know which methods or attributes are responsible to the specific flaws or roots.
related work in this section we compared with the related work in the areas of software metrics defect prediction and technical debt.
softwaremetrics.numerousresearchhasbeenconductedtomeasure software systems.
mccabe measures code complexity by calculating the number of linearly independent paths in the source code.
various metrics were proposed to measure oo projects such as c k metrics and mood metrics .
yu at.
el s proposed multiple coupling metrics and reported that they were correlated to history changes reuse effort and software performance respectively.
bouwers et.
al.
showed that the measuring results from two architecture metrics matched practitioner s intuitions and could help in the decision making process.
there is no substantial evidence showing that these metrics can be used toeffectivelycompareandcontrastdifferentprojectsormultiple versionsofthesameproject andthusformaneffectivebenchmark to bridge the gap between management and development teams.
sahraouiet.al.
presentedameasurementprogram mql.their results showed that using mql could significantly impact the quality of software systems such as maintainability evolvability code complexity etc.
but they didn t have a benchmark to follow anddidn t clearly present how to guide the development teams for further refactorings.
defectprediction.
defectpredictionhasalsobeenwidelystudied.
codemetrics historymeasuresorbothwereusedfordefectprediction.
nagappan et al.
presented a combination of code metrics used for defect prediction.
however they also reported that the bestcombinationofmetricsvariesindifferentprojects.selbyand basili presented that dependency structure is a good indicator of software defects.
cataldo et al.
s showed that the density of change coupling is strongly correlated with failure proneness.
ostrand et al.
demonstrated that a combination of file metrics andfilechangehistorycanbeusedtoeffectivelypredictdefects.all the above studies treat files individually in the analysis not takingarchitecturalconnectionsamongfilesintoconsideration.bycontrast therootandflawdetectionweappliedcanrevealarchitecture problems that propagate errors among multiple files.
schwanke et.
at.
reported their experience of using structure dependency and history measures to predict defects and detect architecture issues buttheirexperiencewasbasedononeindustrialcaseand focused on the detection of molecularity violation.
by contrast we report our experiences of applying software measurement flaw and root detection comprehensively.
technical debt analysis.
in the past decade a number of heuristicshavebeenproposedtoanalyzetechnicaldebt ofsoftware systems.
kazman et.
al.
presented their experience of using economicmodelstoassessthecostsandbenefitsofrefactoringsoftwarearchitecturedebts inwhichtheyonlyreportedtheexperience from one system without the application of dl pc benchmark flawdetection orthe automatedcalculationof maintenancecosts of each flaw.
carriere et.
al.
used a cost benefit model to estimate the effort and benefits of applying refactoring to decouple the architecture.
their study only considered the coupling level of architectureinonecase anditdidnotprovideinformationabout when and where to refactor.
curtis et.
al.
presented a model to estimate technical debt principal in terms of cost which is determined by static analysis of source code.
nord et.
al.
developed a formula to assess the impact of technical debt in architecture and presented that their approach could be used to optimize the long termevolutionofaproduct.thesestudiesonlyreportedinformationforassessingtechnicaldebt butdidn treportinformation about where to refactor.
conclusions in this paper we reported our experiences of applying three ar chitecture analysis techniques supported by an automated toolsuite with components to projects in abb.
our experiences demonstrated that dl and pc could effectively reflect the maintainability of a software project by comparing with a publishedindustrial benchmark architecture flaw analysis enables prac titionerstopinpointandvisualizeseveredesignflaws aswellasto quantify their maintenance costs so that developers can target refactoring actions towards the most severe architecture flaws and3 architecturerootanalysiscouldrevealhowbug proneand change pronefilesareconnectedmoreeffectively.thesetechniques and our tools have been adopted within abb we are now working on integrating thethree techniques into adeployable service that can be used by all projects in the company.
we will also create a commandlineversionofthetools sothatthekeyanalyses suchasdlandpccalculations canbemoreeasilyintegratedintoexistingsoftwarequalitycontroltools suchassonarqube .ourobjectiveis tomeasureprojectswitheachbuildsothatanyqualitydegradation can be detected immediately.