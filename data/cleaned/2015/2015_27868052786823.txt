efficient dependency detection for safe java test acceleration jonathan bell gail kaiser columbia university west 120th st new y ork ny usa jbell kaiser cs.columbia.edueric melski mohan dattatreya electric cloud inc s market street san jose ca usa ericm mohan electric cloud.com abstract slow builds remain a plague for software developers.
the frequency with which code can be built compiled tested and packaged directly impacts the productivity of developers longer build times mean a longer wait before determining if a change to the application being built was successful.
we have discovered that in the case of some languages such as java the majority of build time is spent running tests where dependencies between individual tests are complicated to discover making many existing test acceleration techniques unsound to deploy in practice.
without knowledge of which tests are dependent on others we cannot safely parallelize the execution of the tests nor can we perform incremental testing i.e.
execute only a subset of an application s tests for each build .
the previous techniques for detecting these dependencies did not scale to large test suites given a test suite that normally ran in two hours the best case running scenario for the previous tool would have taken over cpu days to nd dependencies between all test methods and would not soundly nd all dependencies on the same project the exhaustive technique to nd all dependencies would have taken over 10300years.
we present a novel approach to detecting all dependencies between test cases in large projects that can enable safe exploitation of parallelism and test selection with a modest analysis cost.
categories and subject descriptors d. .
testing and debugging keywords test dependence detection algorithms empirical studies .
introduction slow builds remain a hinderance to continuous integration and deployment processes with the majority of building time often spent in the testing phase.
our industry partners con rm previous results reported in literature test suites frequently take several hours to run often over a day making it hard to run them with the desired frequency.
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for thirdparty components of this work must be honored.
for all other uses contact the owner author.
copyright is held by the owner author s .
esec fse august september bergamo italy acm .our study of popular open source java programs echoes these results nding projects that take hours to build with most of that time spent testing.
even in cases of projects that build in a manageable amount of time for example ve to ten minutes faster builds can result in a signi cant increase in productivity due to less lag time for test results.
to make testing faster developers may turn to techniques such as test suite minimization which reduce the size of a test suite for instance by removing tests that duplicate others test suite prioritization which reorders tests to run those most relevant to recent changes rst or test selection which selects tests to execute that are impacted by recent changes .
alternatively given a su cient quantity of cheap computational resources e.g.
amazon s ec2 we might hope that we could reduce the amount of wall time needed to run a given test suite even further by parallelizing it.
all of these techniques involve executing tests out of order compared to their typical execution which may be random but is almost always alphabetically making the assumption that individual test cases are independent .
if some test case t1writes to some persistent state and t2depends on that state to execute properly we would be unable to safely apply previous work in test parallelization selection minimization or prioritization without knowledge of this dependency.
previous work by zhang et al.
has found that these dependencies often come as a surprise and can cause unpredictable results when using common test prioritization algorithms .
this assumption is part of the controlled regression testing assumption given a program pand new version p0 when p0is tested with test case t all factors that may in uence the outcome of this test except for the modi ed code in p0 remain constant .
this assumption is key to maintaining the soundness of techniques that reorder or remove tests from a suite.
in the case of test dependence we speci cally assume that by executing only some tests or executing them in a di erent order we are not e ecting their outcome i.e.
that they are independent .
one simple approach to accelerating these test suites is to ignore these dependencies or hope that developers specify them manually.
however previous work has shown that inadvertently dependent tests exist in real projects can take signi cant time to identify and pose a threat to test suite correctness when applying test acceleration techniques .
zhang et al.
show that dependent tests are a serious problem nding in a study of ve open source applications tests that depend on other tests .
in our own study we permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author.
copyright is held by the owner author s .
esec fse august september bergamo italy acm.
found many large test suites in popular open source software do not isolate their tests and hence may potentially have dependencies.
our new approach and tool electrictest detects dependencies between test cases in both small and large realworld test suites.
electrictest monitors test execution detecting dependencies between tests adding on average a 20x slowdown to test execution when soundly detecting dependencies.
in comparison we found that the previous state of the art approach applied to these same projects showed an average slowdown of 276x using an unsound heuristic not guaranteed to nd all dependencies often requiring more than 10308times the amount of time needed to run the test suite normally in order to exhaustively nd all dependencies.
moreover the existing technique does not point developers to the speci c code causing dependencies making inspection and analysis of these dependencies costly.
with electrictest it becomes feasible to soundly perform test parallelization and selection on large test suites.
rather than detect manifest dependencies i.e.
a dependency that changes the outcome of a test case the de nition in previous work by zhang et al dtdetector electrictest detects simple data dependencies and anti dependencies i.e.
readover write and write over read .
since not all data dependencies will result in manifest dependencies our approach is inherently less precise than dtdetector at reporting true dependencies between tests though it will never miss a dependency that dtdetector would have detected.
however in the case of long running test suites e.g.
over one hour the dtdetector approach is not feasible.
on popular open source software we found that the number and type of dependencies reported by electrictest allow for up to 16x speedups in test parallelization.
our key insight is that for memory managed languages we can e ciently detect data dependencies between tests by leveraging existing e cient heap traversal mechanisms like those used by garbage collectors combined with lesystem and network monitoring.
for electrictest test t2depends on test t1ift2reads some data that was last written by t1.
a system that logs all data dependencies will always report at least as many dependencies as a system that searches for manifest dependencies.
our approach also provides additional bene ts to developers it can report the exact line of code with stack trace that causes a dependency between tests greatly simplifying test debugging and analysis.
electrictest instruments all classes in the system under test including those provided in the core java system library to support a heap walking analysis.
during test execution electrictest observes the heap values les and network resources that are read and written collecting these results and analyzing them to determine if a dependency occurred.
after this learning phase we can safely perform test parallelization or selection using the resulting dependency chains produced by electrictest .
we applied electrictest to the test suites of popular free open source applications studying the dependencies detected and the runtime overhead imposed by the detection process.
we found that electrictest nds at least as many dependencies as the state of the art tool in many orders of magnitude less time.
we have studied the impact of electrictest on test parallelization and test selection techniques nding that its test dependence analysis technique can allow for sound test acceleration.
while we im plement electrictest in java we believe that we present a su ciently general approach that should be applicable to other memory managed languages.
.
motivation to motivate our work we set out to answer three motivating questions to ground our approach mq1 for those projects that take a long time to build what component of the build dominates that time?
mq2 are existing test acceleration approaches safe to apply to real world long running test suites?
mq3 can the state of the art in test dependency detection be practically used to safely apply test acceleration to these long running test suites?
.
a study of java build times in our previous work we studied open source java applications to determine the relative amount of build time spent testing nding testing to consume on average of build time.
the longest of these projects took approximately minutes to build while the shortest completed in under one minute.
given a desire to target projects with very long build times we wanted to make sure that those very long running builds were also spending most of their time in tests.
if we are sure that most of the time spent building these projects is in the testing phase then we can be con dent that a reduction in testing time will have a strong impact in reducing overall build time.
for this study we downloaded the largest and most popular java projects from the open source repository site github those with the most forks and stars overall and those with the most forks over mb as of december 23rd .
from these projects we searched for only those with tests i.e.
had les that had the word test in their name bringing our list to projects.
next we looked at the di erent build management systems used by each project there are several popular build systems for java such as ant maven and gradle.
to measure the per step timing of building each of these projects we had to instrument the build system and hence we selected the most commonly used system in this dataset.
we looked for build les for ve build systems ant maven gradle set and regular make les.
of these projects the majority used maven and hence we focused our study on only those projects using maven due to resource limitations creating and running experiments.
we utilized amazon s ec2 m3.medium instances each running ubuntu .
.
and maven .
.
with .75gb of ram gb of ssd disk space and a one core .5ghz xeon processor.
we tried to build each project rst with java .
.
and then fell back to java .
.
if the newer version did not work some projects required the latest version while others didn t support it .
for each project we rst built it in its entirety without any instrumentation and then we built it again from a clean checkout with our instrumented version of maven in o ine mode with external dependencies already downloaded and cached locally .
if a project contained multiple maven build les we executed maven on the build le nearest the root of the repository and we did not perform any per project con guration.
of these projects we could successfully build .
table shows the three longest build phases rst for all of these projects and then ltering to only those projects that took more than minutes to build projects and 771table top three phases in java builds.
phase all only projects building in projects min hour test .
.
.
compile .
.
.
package .
.
pre test .
those that took more than one hour to build projects .
when looking across all projects of the build time per project was spent testing and testing was the single most time consuming build step.
when eliminating the cases of projects with particularly short build times those taking less than minutes to execute all phases of the build the average testing time increased signi cantly to nearly .
in the eight cases of projects that took more than an hour to build nearly all time is spent testing.
therefore to answer mq1 we nd that testing dominates build times especially in long running builds.
this conclusion underscores the importance of accelerating testing.
.
danger of dependent tests any test acceleration technique that executes only a subset of tests or executes them out of order e.g.
test parallelization or test selection is unsound in the presence of test dependencies.
if the result of one test depends on the execution of a previous test then these techniques may cause false positives tests that should fail but pass or false negatives tests that should pass but fail .
zhang et al.
studied the issue trackers of ve popular open source applications to determine if dependent tests truly exist and cause problems for developers .
they found a total of dependent tests of which would result in a false negative when executed out of order causing a test to fail although it should pass and one which produced a false positive when executed out of order causing a test to pass when it should fail .
given that test dependencies exist and can cause tests to behave incorrectly when executed out of order we conclude that yes dependent tests pose a risk to existing test acceleration techniques.
if we isolate the execution of each of our test cases then dependencies would not be possible.
in practice tests are typically written as single test methods which are grouped into test classes which are batched together into modules.
typically each test method represents an atomic test while test classes represent groups of tests that test the same component.
the module separation occurs when a project is split into modules with a test suite for each module.
since they are typically testing the same component individual test methods are never isolated although sometimes test classes are isolated.
since they represent di erent modules of code that must compile separately test modules are always isolated in our experience.
we are interested in detecting dependencies both at the level of individual test methods and also test classes which also are the same granularity used by test selection and parallelization techniques.
for the remainder of this paper when we refer to individual tests we will refer to test classes and test modules.
one approach to solving the dependent test problem is to simply isolate each test to ensure that no dependencies could occur e.g.
by executing each test in its own process or by using our e cient isolation system vmvm .
however if the application does not isolate its tests and testscurrently depend on each other then tests may present false negatives or false positives albeit deterministically between executions when isolated.
we examined the java projects that we built nding that or isolated all of their test classes and or isolated at least some of their test classes i.e.
some classes were isolated and others were grouped together and executed without isolation .
the majority of projects did not isolate their tests at all and therefore are prone to test dependencies occurring posing a risk to test acceleration.
this result di ers from our study which showed of java projects isolated their tests .
this study examined only projects that built with maven while our previous study which was performed through a static analysis of build scripts examined both maven and ant building projects.
in our previous study we found that of our projects only approximately of those that used maven to build and run their tests isolated some or all of their tests a number much more similar to what we found here.
due to the risks that they impose and ability to occur when tests aren t isolated our goal is to detect dependencies between test classes so that we can inform existing test acceleration techniques of the dependencies to ensure sound acceleration and provide feedback to developers so that they are aware of dependencies that exist.
.
feasibility of existing approaches finally we study the existing state of the art approach for detecting dependencies between test cases to determine if it is feasible to apply to long running test suites.
if we de ne a test dependence as the case where executing some set of tests tin a di erent order changes the result of the test s then identifying test dependencies is npcomplete .
this de nition for dependence henceforth referred to as a manifest dependence is more narrow than ours a distinction described later in x3 but is the de nition used in the state of the art work by zhang et al.
.
to identify all manifest test dependencies in a suite of n tests we would have to execute every permutation of those n tests requiring o n!
test executions clearly infeasible for any reasonably large test suite.
moreover such a technique would only identify that tests are dependent and not the speci c resource or lines of code causing the dependence making it di cult for developers who wish to examine or remove the dependency.
in our study that follows we estimated that this exhaustive process often would take more than 10308times longer than running the test suite normally.
zhang et al.
propose two techniques to reduce the number of test executions needed to detect manifest dependent tests both of which they acknowledge may not scale to large test suites .
in one approach they reduce the search space to o n2 by suggesting that most dependencies manifest between only two tests with no need to consider every possible nsize permutation.
however this is incomplete there may be dependencies that only manifest when more than two tests interact.
they further reduce the search space by a constant factor it is still an o n2 algorithm by only checking test combinations that share common resources de ned to be static elds and les .
if two tests access read or write the same le or static eld then they are marked as sharing a common resource regardless of whether a true data dependency exists or not.
since this very coarse dependency 772table testing time and statistics for the longest running test suites studied with unisolated tests plus the projects studied in previous work by zhang et al.
.
in addition to the normal testing time we estimate the time that needed to run all pairwise combinations of tests and the time needed to exhaustively run all combinations.
indicates a slowdown greater than .
project test classes test methodstesting time mins pairwise test slowdownexhaustive test slowdown class method class methodprojects selected in x2.3camel .
865x 045x 1e 308x 1e 308x crunch .
65x 298x 54e 82x 54e 82x hazelcast .
147x 536x 1e 308x 1e 308x jetty.project .
35x 555x 2e 60x 2e 60x mongo java driver .
58x 649x 4e 76x 4e 76x mule .
250x 438x 1e 308x 1e 308x netty .
11x 725x 62e 82x 62e 82x spring data mongodb .
136x 715x 3e 230x 3e 230x tachyon .
47x 397x 56e 56x 56e 56x titan .
181x 398x 1e 308x 1e 308x average .
279x 276x 1e 308x 1e 308xzhang joda time .
627x 016x 3e 204x 1e 308x xml security .
59x 316x 47e 16x 15e 172x crystal .
37x 763x 3e 8x 3e 108x synoptic .
183x 497x 5e 28x 1e 194x average .
226x 898x 70e 202x 1e 308x detection will likely result in many false positives zhang et al.
manually inspect each resource to determine if it is likely to cause a dependence and if not ignore it in this process.
this heuristic can limit the search space but it still can remain large and requires manual e ort to rule out some resource accesses that will not cause manifest dependencies.
table shows the estimated cpu time needed to detect the dependent tests in each of the ten longest building projects from our dataset from x2.
with unisolated tests along with the four projects studied by zhang et al.
previously .
this experiment was performed on amazon ec2 r3.xlarge instances each running ubuntu .
.
and maven .
.
with virtualized intel xeon x5 v2 .5ghz cpus .
gb of ram and gb of ssd storage.
subjects jetty titan and crunch were evaluated on openjdk java the most recent version supported by the projects while the others were evaluated on openjdk java .
in the case of the four small projects previously studied by zhang et al we used their publicly available tool to calculate the pairwise testing time and estimated the exhaustive testing time.
in the case of our ten projects we estimate all times due to scaling limitations of the dtdetector tool.
we estimated all times using the following approach rst we measured the time to run each test normally and then we calculated the permutations of tests to run for each module of each project most of these projects had many modules with tests and since tests from di erent modules were isolated there was no need to include permutations cross module .
we added a constant time of second to each combination of tests executed to account for the time needed to start and stop the jvm and system under test a conservative estimate based on our prior results .
we compare this projected time to the actual time needed to run the test suite in its normal con guration presenting the slowdown as tdtdetector tnormal .
even the pairwise heuristic examining only every pair of tests rather than all possible permutations can be cost prohibitive adding an overhead of up to 016x minimum 298x for test meth ods even though there is no guarantee of its correctness.
a large slowdown appears in both long building and fast building projects.
for the four projects previously studied by zhang et al.
the dependence aware approach showed approximately one order of magnitude less overhead.
however we were unable to evaluate the dependence aware technique on our ten projects due to technical limitations of the dtdetector implementation running it requires manual enumeration and con guration of each test to run in the dtdetector test runner.
given the manual e ort required and that this heuristic is unsound we chose not to implement it for our ten projects.
as expected there is no situation in the projects that we studied where the fully exhaustive method testing all possible permutations is feasible.
even in the cases of the more modest length test suites the overhead of dtdetector is very high.
we answer mq3 and conclude that the existing state of the art approach for detecting dependencies between tests can not scale to detect dependencies in the wild except when using unsound heuristics on the very smallest of test suites that took less than a minute to execute normally.
.
detecting test dependencies while previous work in the area has focused on detecting manifest dependencies between tests we focus instead on a more general de nition of dependence.
for our purposes if t2reads some value that was last written by t1 then we say that t2depends on t1 i.e.
there is a data dependence .
if some later test t3writes over that same data then we say that there is an anti dependence between tests t2andt3 t3must never run between t1andt2.
note that any two tests that are manifest dependent will also be dependent by our de nition but two tests that have a data dependence may not have a manifest dependence.
consider the case of a simple utility function that caches the current formatted timestamp at the resolution of seconds so that multiple invocations of the method in the same second returns the same formatted string.
if the date formatter has no side e ects we can surmise that if several tests call 773this method while there is a data dependency between them since the cache is reused this dependence won t in and of itself in uence the outcome of any tests.
hence there will be no manifest dependence between these tests even though there is a data dependence.
while detecting manifest dependencies between tests may require executing every possible permutation of all tests detecting data dependencies that may or may not result in manifest dependencies requires that each test is executed only once.
electrictest detects dependencies by observing global resources read and written by each test and reports any test tjthat reads a value last written by test tias dependent.
electrictest also reports anti dependencies that is other tests tkthat write that same data after tj to ensure that tkis not executed between tiandtj.
electrictest consists of a static analyzer instrumenter and a runtime library.
before tests are run with electrictest all classes in the system under test including its libraries are instrumented with heap tracking code at the bytecode level no access to source code is required .
in principle this instrumentation could occur on they during testing as classes are loaded into jvm however we perform the instrumentation o ine for increased performance as many external library classes may remain constant between different versions of the same project.
this process is fairly fast though analyzing and instrumenting the classes in the java .
jdk took approximately minutes on our commodity server.
electrictest detects dynamically generated classes that are loaded when testing which were not statically instrumented and instruments them on the y. during test execution the electrictest runtime monitors heap accesses to detect dependencies between tests.
dependencies between tests can arise due to shared memory shared les on a lesystem or shared external resources e.g.
on a network .
electrictest s approach for le and network dependency detection is simple it maintains a list of les and network socket addresses that are read and written during each test.
electrictest leverages java s built in iotrace support to track le and network access.
e ciently detecting in memory dependencies is much more complex and we focus our discussion to this technique next.
.
detecting in memory dependencies to detect dependencies in memory between test cases electrictest carefully examines reads and writes to heap memory.
recall that java is a memory managed language where it is impossible to directly address memory.
simply put the heap can be accessed through pointers to it that already exist on the stack or via static elds which reside in the heap and can be directly referenced .
at the start of each test we ll assume that the test runner which is creating these tests does not pass on the stack