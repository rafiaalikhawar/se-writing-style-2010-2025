program synthesis using natural language aditya desai iit kanpur adityapd cse.iitk.ac.insumit gulwani msr redmond sumitg microsoft.comvineet hingorani nidhi jain iit kanpur viner nidhij cse.iitk.ac.in amey karkare iit kanpur karkare cse.iitk.ac.inmark marron msr redmond marron microsoft.comsailesh r subhajit roy iit kanpur sairaj subhajit cse.iitk.ac.in abstract interacting with computers is a ubiquitous activity for millions of people.
repetitive or specialized tasks often require creation of small often one off programs.
end users struggle with learning and using the myriad of domain specific languages dsls to effectively accomplish these tasks.
we present a general framework for constructing program synthesizers that take natural language nl inputs and produce expressions in a target dsl.
the framework takes as input a dsl definition and training data consisting of nl dsl pairs.
from these it constructs a synthesizer by learning optimal weights and classifiers using nlp features that rank the outputs of a keywordprogramming based translation.
we applied our framework to three domains repetitive text editing an intelligent tutoring system and flight information queries.
on english descriptions the respective synthesizers rank the desired program as the top and top3 for and descriptions respectively.
.
introduction program synthesis is the task of automatically synthesizing a program in some underlying domain specific language dsl from a given specification .
traditional program synthesis synthesizing programs from complete specifications has not yet seen wide adoption due to the difficultly of writing such specifications and verifying the synthesized program satisfies this specification.
recent work has experimented with another class of possibly incomplete specifications namely examples .
programming by example pbe systems have seen much wider adoption thanks to the ease of providing such a specification.
however they can suffer in cases where many examples are required to accurately specify intent or where examples are difficult to construct.
the classic l algorithm a pbe system for describing a regular language has the well known drawback of requiring too many examples.
a domain such as atis queries air travel information system is a case where constructing an example input output pair is a non trivial task for an end user.
however tasks in these permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c acm.
isbn .
.
.
.
can easily be specified natural language as as we show in this work can be used to reliably synthesize the desired program.
in this paper we address the problem of synthesizing programs in an underlying domain specific language dsl from natural language nl .
nl is inherently imprecise hence it may not be possible to guarantee the correctness of the synthesized program.
instead we aim to generate a ranked set of programs and let the user possibly select one of those programs by either inspecting the source code of the program or the result of executing them on some test inputs .
the synthesis algorithm in this paper is able to consistently produce and rank the desired result program in the top spot over of the time or in the top spots over of the time in our benchmarks.
to give users confidence in the program they choose we show both the translation of the code into disambiguated english and or run it to show the result as a preview.
unlike most existing synthesis techniques that specialize to a specific dsl such as our approach can be applied to a variety of dsls.
our approach requires two inputs from the synthesis designer i the dsl definition which we assume provides a set of operations that are similar to the concepts an enduser might express in nl ii training data consisting of example pairs of english sentences and corresponding intended programs in the dsl.
a training phase infers a dictionary relation over pairs of english words and dsl terminals in a semi automated interactive manner and optimal weights classifiers in a completely automated manner for use by the generic synthesis algorithm.
our approach can be seen as a meta synthesis framework for constructing nl to dsl synthesizers.
the generic synthesis algorithm alg.
takes as input an english sentence and generates a ranked set of likely programs.
first it uses a bag algorithm alg.
to efficiently compute the set of all well typed dsl programs whose terminals are related to the words that occur in the sentence.
for this it uses a dictionary learned during the training phase that is a relation over english words and dsl terminals.
then it ranks these programs based on a set of scoring functions .
inspired by our view of the abstract syntax tree ast of a program as involving two constituents the set of terminals in the program and the tree structure between those terminals.
a weighted linear combination of scores determines the rank of each program i a coverage score that captures the intuition that results that ignore many words in the user input are unlikely to be correct ii a mapping score that captures the intuition that english words can have multiple meanings wrt.
the dsl but we prefer the more probable interpretations iii a structure score that uses the insight that natural and programming languages have common idiomatic structures and prefer more natural results.
ieee acm 38th ieee international conference on software engineering a grammar s command seq command command command replacecmd removecmd insertcmd printcmd replacecmd replace selectstr newstring iterscope removecmd remove selectstr iterscope insertcmd insert pstring position iterscope printcmd print selectstr iterscope selectstr token bcond occurrence iterscope scope bcond occurrence document token pstring line word number ... bcond atomiccond not atomiccond and atomiccond atomiccond ... atomiccond startswith token contains token commoncond commoncond between token token after token ... occurrence all atomicoccurrence ... atomicoccurrence intsetbyand firstfew integer ... intsetbyand integer intset integerset scope linescope wordscope position start end ... newstring by pstring b sample benchmarks .remo ve the first word of lines which start with number.
.replace with unless it is inside .
.add at the beginning of those lines that do not already start with .
.add ..???
at the last of every 2nd statement.
.in every line delete the text after .
.remove 1st from every line.
.add the suffix idm to the word right after idiom .
.delete all but the 1st occurrence of cook .
.delete the word the wherever it comes after all .
c variations in nl for description of the same task.
.
prepend the line containing p.o.
box with .
add a at the beginning of the line in which the string p.o.
box occurs .
put a before each line that has p.o.
box in it .
put in front of lines with p.o.
box as a substring .
insert at the start of every line which has p.o.
box word table grammar and sample benchmarks for the text editing domain.
the classifiers to compute these scores as well as the weights for combining the scores are learned during the training phase using off the shelf machine learning algorithms.
the novelty of of our approach lies in the generation of training data for classifier learning from the top level training data alg.
3and4 and in smoothing a discrete scoring metric into a continuous and differentiable loss function for effective learning of weights .
.
this paper makes the following contributions we describe a meta synthesis framework for constructing nlto dsl synthesizers consisting of a synthesis algorithm for translating english sentences into corresponding programs in the underlying dsl and a training phase for learning a dictionary and weights that are used by the synthesis algorithm .
our method can be applied to new dsls and requires only the dsl definition along with translation pair training data.
we apply our generic framework to three different domains namely automating end user data manipulation .
generating problem descriptions in intelligent tutoring systems .
and database querying .
.
in cases where comparisons can be made with state of the art nlp based approaches the results of the approach presented in this paper are competitive.
we gather an extensive corpus of data consisting of pairs of english descriptions and corresponding programs.
we use this data for evaluation in this paper and provide it as a resource for researchers in the community.
of these english descriptions come from the air travel information system atis benchmark suite come from another corpus while english descriptions were collected by us from various online sources including help forums and course materials textbooks and user studies.
we evaluate the effectiveness of our approach on three different dsls .
the synthesizers produced by our framework run in seconds on average per benchmark and produce a ranked set of candidate programs with the correct result in the top top choices for over benchmarks respectively.
.
motiv ating scenarios we describe different domains where a nl to dsl synthesizer is useful text editing .
automata construction problems for intelligent tutoring .
and answering queries for an air travel information systems .
.
.
text editing end user programming through a study of help forums for office suite applications like microsoft excel and word we observed that users frequently request help with repetitive text editing operations such as insertion deletion replacement or extraction in text files.
these operations table b are more complicated than simple search and replace of a constant string by another in two ways.
first the string being searched for is often not constant and instead requires regular expression matching.
second the editing is often conditional on the surrounding context.
programming of even such relatively simple tasks requires the user to understand syntax and semantics of regular expressions conditionals and loops which are beyond the ability of most end users.
this inspired us to design a command language for text editing a subset of the grammar is shown in table a that includes key commands insert remove print andreplace.
each of these commands relies on an iterscope expression that specifies the region a set of lines a set of words or the entire word document that the text editing operation is on.
the selectstr production includes a token which allows for limited wild card matching e.g.
an entire word number or a pattern specified by pstring a boolean condition bcond that acts as an additional local filter on the matched value and an occurrence value that performs an index based selection from the resultant matches.
use of the occurrence values like firstfew n from atomicoccurrence when performing a remove results in the removal of only the first n items here n is a positive integer that match the condition while use of all will instead result in all matches of the condition being removed.
the boolean conditions bcond cover the standard range of string matching predicates contains startswith etc.
and allow conjunction of conditions and notetc.
.
the commoncond production specifies the position relative to the string token s that occurs after it after before it before or around it between acts as another global filter.
example .for text editing task in table b our system produces the following translation remove selectstr word always iterscope linescope startswith number all example .for text editing task in table b our system produces the following translation replace selectstr not between all by document .
consider the set of all binary strings where the difference between the number of and the number of is even.
.
the set of strings of and such that at least one of the last positions is a .
.
the set of strings w such that the symbol at every odd position in w is a .
.
let l1 be the set of words w that contain an even number of a let l2 be the set of words w that end with b let l3 l1 intersect l2.
.
the set of strings over alphabet to such that the final digit has not appeared before.
table sample benchmarks for the automata domain.
.
i would like the time of your earliest flight in the morning from philadelphia to washington on american airlines.
.
i need information on a flight from san francisco to atlanta that would stop in fort worth.
.
what is the earliest flight from washington to atlanta leaving on wednesday september fourth.
.
okay we re going from washington to denver first class ticket i would like to know the cost of a first class ticket.
.
what ground transportation is there from the airport in atlanta to downtown.
table sample benchmarks for the atis domain.
table c describes a sample of the variations that our system can handle for description of a task that is expressible in our dsl.
our belief is that once users are able to accomplish these types of smaller conditional and repetitive tasks they can accomplish complex tasks by reducing them to a sequence of smaller tasks using end user programming environments .
.
automata theory intelligent tutoring results from formal methods research have been used in many parts of intelligent tutoring systems including problem generation solution generation and especially feedback generation for a variety of subject domains including geometry and automata theory .
each of these domains involves a specialized dsl that is used by a problem generator tool to create new problems a solution generation tool to produce solutions and more significantly a feedback generation tool to provide feedback on student solutions.
consider the domain of automata constructions where students are asked to construct an automaton that accepts a language whose description is provided in english for some examples see table .
we designed a dsl based on the description provided by alur et.al.
on constructs required to formally specify such languages.
this dsl contains predicates over strings boolean connectives functions that return positions of substrings and universal existential quantification over string positions.
as stated in such a language is used to generate feedback for students incorrect attempts in two ways i it is used by a solution generation tool to generate correct solutions against which a student s attempts are graded ii it is also used to provide feedback and generate problem variations consistent with a student s attempt.
this feedback generation tool has been deployed in the classroom and has been able to assign grades and generate feedback in a meaningful way while being both faster and more consistent than a human.
example .specification in table is translated as iseven diff occurcount occurcount example .specification in table is translated as exists lastk strequals symbolatpos .
air travel information systems atis atis is a standard benchmark for querying air travel information consisting of english queries and an associated databasecontaining flight information.
it has long been used as a standard benchmark in both natural language processing and speech processing communities.
table shows some sample queries from the atis suite.
for atis we designed a dsl that is based around sql style row column operations and provided support for predicates expressions that correspond to important concepts in air travel queries arrival departure locations times dates prices etc.
example .the first query in the atis examples table is translated into our dsl as colselect dep time rowmin dep time rowpred eqdepart philadelphia time morning eqarrive washington time any eqairline american .
problem definition we study the problem of synthesizing nl to dsl synthesizers given a dsl definition and training data.
a dsl l g tc consists of a context free grammar g with terminal symbols denoted bygtand production rules by gr and a type semantic checker tcthat can check if a given program is well typed.
the training data consists of a set of pairs s p where sis an english sentence andpis the corresponding intended program from the dsl l. a sentence is simply a sequence of words .
the goal of the generated nl to dsl synthesizer is to translate an english sentence to a ranked set of programs inl.
.
nl to dsl synthesis algorithm our synthesis algorithm alg.
takes a natural language command from the user and creates a ranked list of candidate dsl programs.
the first step loop on line is to convert each of the words in the user input into one or more terminals function names or values using the nl to program terminal dictionary nldict.
this loop ranges over the length of the input sentence and for each index looks up the set of terminals in the dsl that are associated with the word at that index in nldict.
fundamentally nldict encodes for each terminal which english language words are likely to indicate the presence of that terminal in the desired result program.
this map can be constructed in a semi automated manner .
.
once this association has been made for a terminal twe store a tuple of the terminal and a singleton map relating the index of the word to a terminal that was produced into the set r0 line .
for each natural language word the dictionary nldict associates a set of terminals with it.
the terminals may be constant values or function applications with holes as arguments.
thus alg.
when applying nldict on line can create incomplete programs where some arguments to functions are missing.
for example consider the sentence print all lines that do not contain .
since the grammar contains printcmd print selectstr iterscope as a production and the dictionary relates the word print to the function print the partial program print will be generated.
these holes are later replaced by other programs that match the argument types selectstr and iterscope.
once the base set of terminals has been constructed the algorithm uses the bagalgorithm alg.
to generate the set of all consistent programs rest that can be constructed from it line .
the final step is to rank .
this set of programs using a combination of scores and weights in the loop on line .
.
synthesizing consistent programs a program pin the dsl is either an atomic value i.e.
a terminal in g or a function operator applied to a list of arguments.
by convention we represent function application as s expressions where a function fapplied to karguments is written f p1 pk .
347algorithm top level nl to dsl synthesis algorithm input nl sentence s word to terminal dictionary nldict output ranked set of programs 1r0 2fori2 do t nldict s foreach t2tdor0 r0 t singletonmap i t 5res t bag s r0 6res p fpj9 ms.t.
p m 2res tg 7foreach program p2res pdoscore p 8foreach program p m 2res tdo scov coveragescore p s m wcov smap mappingscore p s m wmap sstr structurescore p s m wstr score p max score p scov smap sstr 13return set of programs in res pordered by score consistent programs and witness maps.
given a dsl l g tc we say a program pin language l is consistent with a sentence sif there exists a map mthat maps some word occurrences in sto terminals in gtsuch that the range ofmequals the set of terminals in the program p.1we refer to such a map mas awitness map and use the notation witnessmaps p s to denote the set of all such maps.
usable and used words.
letsbe an english sentence pbe a program consistent with s and mbe any witness map.
usablewords s are those word occurrences in sthat are mapped to some grammar terminal and hence might be useful in translation.
usedwords s m is the set of usable word occurrences in sthat are used as part of the map m. usablewords s fijs 2domain nldict g usedwords s m usablewords s domain m partial programs.
a partial program extends the notion of a program to also allow for a hole as an argument.
a hole is a symbolic placeholder where another complete program program without any hole can be placed to form a larger program.
to avoid verbosity we often refer to a partial program as simply a program.
given a partial program p f with a hole we can substitute a complete program p0to fill the hole p f p0 iftc f p0 ?
otherwise the validity check tc ensures that all synthesized programs are well defined in terms of the dsl grammar and type system otherwise we return the invalid program?
.
combination.
the combination operator suball generates the set of all valid programs that can be obtained by substituting a complete program p0in some hole of a partial program p. this is done by going over all the arguments of pand producing substitutions for argument positions with holes.
given partial program p f p1 pk and complete program p0 we have suball p p0 fp jpi 2ig 1since the same english word can occur at different positions in s having different meanings any map mmust take the position information also as an argument.
we ignore this in the paper.algorithm bag synthesis sub algorithm input nl sentence s initial tuple set b0 1result b0 2repeat oldresult result foreach p1 m1 p2 m2 2result do okpc p1is partial p2is complete disjoint usedwords s m1 usedwords s m2 ifokpc disjoint then combs suball p1 p2 f?g new f pr m1 m2 jpr2combsg result result new 11until oldresult result 12return result bag algorithm.
thebagalgorithm alg.
is based on computing the closure of a set of programs by enumerating all possible well typed combinations of the programs in the set.
the main loop line is a fixpoint iteration on the result set of programs that have been constructed.
the requirement that p2is a complete program line when applying the suball function ensures that the only holes in the result programs are holes that were originally in p1.
we restrict the initialization of b0to include only complete programs and partial programs with holes at the top level only.
using this restriction we can inductively show that at each step all partial programs only have holes at the top level.
thus we can efficiently compute the fixpoint of all possible programs in a bottom up manner.
the condition usedwords s m1 usedwords s m2 line ensures that the two programs do not use overlapping sets of words from the user input.
this ensures that the final program cannot create multiple sub programs with different meanings from the same part of the user input.
this also ensures that the set of possible combinations has a finite bound based on the number of words in the input.
line constructs the set of all possible substitutions of p2into holes in p1 ignoring any invalid results .
for each of the possible substitutions we add the result and the union of the mmaps to the newprogram set line .
since the domains of the maps were disjoint the union operation is well defined.
thebagalgorithm has a high recall but in practice it may generate spurious programs that arise as a result of arbitrary rearrangement of the words in the english sentence.
to fix this the correct translation is reported by selecting the top most rank program based on features of the program and the parse tree of the sentence.
.
ranking consistent programs we view the abstract syntax tree of the synthesized program as consisting of two important constituents the set of terminals in the program and the tree structure between those terminals.
we use these constituents to compute the following three scores to determine the rank of a consistent program i a coverage score that reflects how many words in the english sentence were mapped to some operation or value in the program ii a mapping score that reflects the likelihood that a word to terminal mapping is capturing the user intent iii a structure score that captures the naturalness of the tree program structure and the connections between parts of the program and the parts of the sentence that generated them.
.
.
coverage score for a given sentence s a candidate translation p and a witness map m the coverage score is defined as coveragescore p s m jusedwords s m j jusablewords s j 348thecoveragescore p s m denotes the fraction of available information in sthat is actually used to generate p. intuitively we want to prefer programs that make more use of the input information.
example .consider possible translations for an input s s find the cheapest flight from washington to atlanta p1 rowmin fare rowpred eqdepart washington eqarrive atlanta p2 rowpred eqdepart washington eqarrive atlanta the first program p 1makes use of all parts of the user input including the desired cheapest fare while the second program p 2ignores this information.
the coverage score ranks p 1higher than p .
.
.
mapping score for any word wthere may be multiple terminals functions or values in the set nldict w each of which corresponds to a different interpretation of w. we use machine learning techniques to obtain a classifier cmapbased on the part of speech pos tag provided for the word by the stanford nlp engine .
cmap predict function of the classifier predicts the probability of each word toterminal mapping being correct.
we use predictions from cmapto compute the mappingscore the likelihood that terminals in pare correct interpretation of corresponding words in s. mappingscore p s m w2usablewords s cmap predict w pos w s m w a limitation of the mappingscore score is that it looks only at the mapping of a word but not its relation to other words and how they are are mapped by the translation.
thus interchanging a pair of terminals in a correct translation gives us an incorrect translation which has the same score.
example .consider an input s and possible translations s if xyz is at the beginning of the line replace xyz with abc p1 replace selectstr xyz always all by abc iterscope linescope startswith xyz all p2 replace selectstr xyz always all by abc iterscope linescope before xyz all both the programs use same sets words so they have the same coverage score.
the only difference is that the word beginning is mapped to startswith pos verb phrase in p and to before pos prepositional phrase in p .
mapping score helps in identifying p 1as the correct choice.
.
.
structure score structure score captures the notion of naturalness in the placement of sub programs.
we use connection features obtained from the sentence s the natural language parse tree for the sentence nlparse s and the corresponding program pto define the overall structure score.
these features are used to produce the classifier cstrwhich computes the probability that each of the combinations inpis correct.
definition c onnection .for a production r 2grof the form n!n1 ni nj nk the tuple r i j where i j k and i6 j is called a connection.definition c ombination .consider the program p p1 p2 pk generated using the production r n!n1n2 nk such that n igenerates p ifor1 i k. we say the pair of subprograms pi pj is combined via connection r i j and this combination is denoted as conn pi pj .
the overall structurescore is obtained by taking the geometric mean of the various connection probabilities of the scores for the program p this normalizes the score to account for programs with differing numbers of connections.
structurescore p s m geometricmean connprobs p s m connprobs p s m conn pi pj inpfcstr predict f g where f hfpos1 fpos2 flca1 flca2 forder fover fdisti computed for piandpjusing p s and m. we obtain separate classifier cstr for each connection conn. the function cstr .predict asks the classifier to predict the probability that f vec belongs to class i.e.
present in correct translation .
the other class is .
given a program pand input sentence sthat are related by a witness map mand the parse tree nlparse s the following functions define several useful relationships treecover p s m minimal sub tree tsubof nlparse s s.t.usedwords s m usablewords tsub root p s m root node of treecover p s m span p m in the rest of the section we assume that p1andp2denote two sub programs of p. the following features determine the naturalness of the connections between p p1 p2 and s definition r oot pos t ags .part of speech features are the pos tags assigned by the nl parser to the root nodes of the sub trees associated with p 1and p 2respectively fpos1 pos root p1 s m fpos2 pos root p2 s m the features fpos1 and fpos2 help to learn the phrases that are commonly combined using a particular connection.
definition lca d istances .let lca be the least common ancestor of root p1 s m and root p2 s m .
the lca distance features are the tree distances from lca to the root nodes of the sub trees associated with p 1and p 2respectively flca1 treedistance lca root p1 s m flca2 treedistance lca root p2 s m definition o rder .the order feature is determined by the positions of the roots of the sub tree roots associated with p and p 2in the in order traversal of nlparse s .
forder 1if root p1 s m occurs before root p2 s m in in order traversal of nlparse s 1otherwise features flca1 flca2and forder are used to learn the correspondence between the parse tree structure and the program structure.
we use these to maintain the structure of translation close to the structure of the parse tree.
349definition o verlap .the overlap feature captures the possibility that two programs are constructed from mixtures of two subtrees in the nl parse tree fover 1if span p1 m1 end span p2 m2 start 1if span p1 m1 start span p2 m2 end 0otherwise definition d istance .given two programs p 1and p we define the distance feature for programs by looking at the distance between the word spans used in the programs fdist span p2 m2 start span p1 m1 end if f over span p1 m1 start span p2 m2 end if f over otherwise the features foverandfdistcapture the proximity information of words and are useful because related words often occur together in the input sentence.
example .consider possible translations for an input s s print all lines that do not contain p1 print selectstr line not contains all document p2 print selectstr not contains line all document in the parse tree nlparse s print will have two arguments what to print lines and when to print not contain .
we observe the following for the candidate programs a the word lines is closer to print while the word is farther in nlparse s .
the same structure is observed for p but not for p .
this is captured by lca distances.
b the order of the words in nlparse s matches the order in p 1better than in p .
this is captured by the order feature.
c the phrase do not contain is kept intact in p but is split apart in p .
overlap and distance features will capture this splitting and reordering.
both the programs use the same set of words and the same word to terminal mappings resulting in the same coverage score and the same mapping scores.
however the program p 1is correct and our choice of features rank it higher.
.
combined score example to provide some intuition into the complementary strengths and weaknesses of the various scores we examine how they behave on a subset of the programs generated by the bagalgorithm for the following text editing task add a at the beginning of the line in which the string p.o.
box occurs.
table shows some of the consistent programs generated by the bag algorithm.
the first program p is the intended translation.
let us look at the performance of each of the component scores coverage score both p1andp4use the maximum number of words from the sentence and are tied on top score.
p4is wrong as it adds p. o. box at the beginning of the line containing .
mapping score the classifier learnt by our system maps the word beginning to the terminal startswith with a high probability but to the terminal start with a lower probability.
further it maps occurs to the terminal contains with a still lower probability.
p2 does not use the word occur otherwise it has same mappings as p1.
as a result it has higher mapping score than p1 but suffers on coverage.
p3maps beginning to startswith and does not use the word occurs .
as a result it has a mapping score lower than p2 but higher than p1.
if we had used the mapping score alone we would not have been able to rank the desired program p1above the incorrect programs p3andp4.algorithm learning mapping score classifier cmap input training data t foreach training pair s p 2tdo m witnessmaps p s m argmaxm02 m likeability p s m0 foreach w t 2mdo cmap.train w pos w s t return cmap structure score coverage score and mapping score look only at the mapping of a word but not its relation to other mappings and their placement with respect to the original sentence.
structure score fixes this by considering structural information parse tree ordering of words and distance among words from the sentence.
p4has poor structure score because it swaps the sentence ordering for strings and p.o.
box .
p3also suffers as it moves beginning mapped to startswith away from add mapped to insert .
p1gets a high structure score as it maintains the parse tree structure of the input text.
note that p2andp5have high structure score as well.
this is because structure score does not take into account the fraction of used words or word to terminal mappings.
so an incomplete translation that uses very few words but maps them to correct terminals and places them correctly is likely to have a high value.
the desired program p1 is only top ranked by one of the scores and even in that case the score is tied with another incorrect result.
however a combination of the scores with appropriate weights ranks p1as the clear winner!
.
training phase this section describes the learning of classifiers weights and the word to terminal mapping used by the synthesis algorithm described in .
the key aspects in this process are i deciding which machine learning algorithm to use and ii generation of lower level training data for that machine learning algorithm from the top level training data provided by the dsl designer.
.
mapping score classifier cmap the goal of the cmapclassifier is to predict the likelihood of a word wmapping to a terminal t2gtusing the pos tag of the word w. the learning of this classifier is performed using an offthe shelf implementation of a naive bayesian classifier .
the training data for this classifier is generated as shown in alg.
.
the key idea is to first construct the set mof all witness maps that can yield program pfrom natural language input s. we then select the most likely map mout of these witness maps based on the partial lexicographic order given by the likeability score tuples.
likeability p s m usedwords s m disjointness p s m disjointness p s m p02subprogs p s p0 where s p1 pn if8pi pj pi pj otherwise the likeability tuples serve two purposes first via the usedwords they guide the system to prefer mappings that use all parts of the input sentence.
second via the disjointness they guide the system to prefer mappings that penalize the use of a single part of a sentence to construct multiple different subprograms.
.
structure score classifiers cstr we now describe how the classifiers used in structure score cstr for each connection conn are learned.
the goal of each 350pr ogram generated co verage scoremapping scor estructur e scorefinal scor e p1insert start iterscope linescope contains p.o.
box all .
.
.
.
p2insert start iterscope linescope always all .
.
.
.
p3insert start iterscope linescope startswith p.o.
box all .
.
.
.
p4insert p .o.
box start iterscope linescope contains all .
.
.
.
p5insert start document .
.
.
.
table ranking the set of consistent programs generated by the bagalgorithm.
algorithm learning structure score classifiers cstr input training data t foreach training pair s p 2tdo allopts synthnoscore s foreach program p02allopts do foreach combination c that occurs in p0do if c occurs in p then class else class conn connection used by c f hfpos1 fpos2 flca1 flca2 forder fover fdisti cstr .train f class return cstr classifier cstr is to predict the likelihood that a combination cis an instance of connection conn using the features of cfrom .
.
we use an off the shelf implementation of a naive bayesian classifier and generate the training data for it as shown in alg.
.
the key idea is to run the synthesis algorithm without the scoring step synthnoscore to construct the set of all possible programs allopts from the english sentence s. any combination present in a program in p0inallopts but not present in pis used as a negative example while that present in pis used as a positive example.
.
dictionary construction we construct the dictionary nldict in a semi automated manner using the names of the terminals functions and arguments in the dsl.
if the name of an operation is a proper english word such as insert we use the wordnet synonym list to gather commonly used words which are associated with the action.
cases where the name is not a simple word but instead concatenations of or abbreviations of several words such as startswith are handled by splitting the name and resolving the synonyms of each sub component word.
it is possible that the general purpose synonym sets provided by wordnet contain english words that are not useful for the particular domain we are constructing the translator for.
however the mapping score learning in .
will simply assign these words low scores.
once the learning algorithm for the mappings has finished assigning weights to each word terminal we discard all mappings below a certain threshold.
conversely it is also possible that an important domain specific synonym will not be provided by the wordnet sets or that the names in the dsl are not well matched with proper english words.
our system automatically detects these cases as a result of being unable to find witness maps for programs in the training data involving certain dsl terminals.
in these cases it prompts the user to identify a word in an input sentence that corresponds to an unmapped terminal in a program.
these new seed words are then further used to build a more extensive synonym set using wordnet.
.
learning combination weights in the previous section we defined component scores for a translation.
a standard mechanism for combining multiple scoresinto a single final score is to use a weighted sum of the component scores.
in this section we describe a novel method for learning the required weights to use to maximize the following function.
optimization function number of benchmarks in the training set for which the correct translation is assigned rank .
in numerical optimization maximization of an optimization function is a standard problem which can be solved using stochastic gradient descent .
in order to use gradient descent to find the weight values that maximize our optimization function we need to define a continuous and differentiable loss function floss.
this loss function is used to guide the iterative search for a set of weights that maximizes the value of the optimization function as follows wn wn g 5floss wn n where 5denotes the gradient and gis a positive constant.
at each step wmoves in the direction in which the value of flossdecreases and the process is stopped when the change in the function value in successive steps drops below a specified threshold e. a common form for loss functions is a sigmoid.
we can convert our ill behaved optimization function into a loss function that is closer to what is needed to perform gradient descent by basing the sigmoid on the ratio score given to the best incorrect result and the score given to the desired rate via the following construction floss w 8training sf w s f w s e c l where l vwrong score pdesired c pdesired correct translation of s vwrong max fscore p jp2bag s p6 pdesiredg although the above transformation results in a loss function which is mostly well behaved it saturates appropriately and is piecewise continuous and differentiable there are still points were the function is not continuous.
in particular the presence of the max function in the definition of vwrong creates discontinuous points in floss.
however the following insight enables us to replace the discontinuous max operation with a continuous approximation max a b log eca ecb cwhere c ifa b b a thus we can replace the max operator with this function extended in the natural way to karguments in the computation of vwrong to produce a globally continuous and differentiable loss function.
the cases where there are several incorrect results which are given very similar scores are minimized by the selection of a large value for c which amplifies small differences.
additionally in the worst case where two scores are extremely close the impact of the approximation is to drive the gradient descent to increase the ratio between vwrong and score pdesired .
thus the correctness of the gradient descent algorithm is not impacted.
in addition to satisfying the basic requirements for performing gradient descent our loss function floss saturates for large values ofl.
this implies that if an input s hasvwrong score pdesired it will 351not dominate the gradient descent causing it to improve the ranking results for a single benchmark at the expense of rank quality on a large number of other benchmarks.
the saturation also implies that the descent will not become stuck trying to find weights for an input where there is no assignment to the weights that will improve the ranking i.e.
there is an incorrect result program piwhere every component score has a higher value than the desired program pd.
.
experimental ev aluation the online synthesis algorithm consisting of the bagalgorithm and feature extraction for ranking was implemented in c and used the stanford nlp engine version .
.
for pos tagging and extracting other nl features.
the offline gradient decent was implemented in c while the classifiers used for training the component features were built using matlab.
a major goal of this research is the production of a generic framework for synthesizing programs in a given dsl from english sentences.
thus we selected different categories of tasks question answering atis constraint based model construction automata theory tutoring and command execution on unstructured data repetitive text editing .
these domains described in detail in present a variety of structure in the underlying dsl the language idioms that are used and the complexity of the english sentences that are seen.
for benchmarks automata descriptions are taken verbatim from textbooks and online assignments.
text editing descriptions are taken verbatim from help forums and user studies.
atis descriptions are part of a standard suite.
tables b c and 3describe a sample of the benchmarks.
the details of the benchmarks and their sources can be obtained from companion website .
air travel information system atis .
we selected queries at random from the full atis suite which consists of few thousand queries and by hand constructed the corresponding program in our dsl to realize the query.
each task in atis domain is a query over flight related information.
automata theory tutoring.
we collected natural language specifications accepting conditions of finite state automata from books and online courses on automata theory.
repetitive text editing.
we collected a description of text editing tasks from excel books and help forums.
we collected english descriptions for these tasks via a user study which involved participants who were first and second year undergraduate students .
the large number of participants ensured variety in the english descriptions e.g.
seetable c .
in order to remove any description bias each of these tasks was described not using english but using representative pairs of input and output examples.
additionally we obtained english descriptions for text editing tasks one for each task from an independent corpus .
.
precision recall and computational cost in this study we used standard fold cross validation to evaluate the precision and recall of the translators on each of the domains.
thus we select of the data at random to use for learning the classifiers weights and then evaluate the system on the remaining of data which was held back and not seen during training .
in the ranking we handle ties in the scores assigned to an element using a ranking scheme .
in ranking in figure ranking precision of algorithm on all domains.
the case of tied scores each element in the tied group is assigned a rank corresponding to the lowest position in the ordered result list as opposed to the highest .
this ensures that the reported results represent the worst case number of items that may appear in a ranked list before the desired program is found.
precision.
fig.
shows the percentage of inputs for which the desired program in the dsl is the top ranked result and the percentage of inputs where the desired result is in the first three results.
as shown in the figure for every domain on over of the inputs the desired program is unambiguously identified as the top ranked result.
further for the atis domain the desired result is the top ranked result for of the natural language inputs.
given the size of our sample from the full atis suite we can infer that the desired program will be the top ranked result for of the natural language inputs at a confidence interval.
these results show that our novel program synthesis based translation approach is competitive with the state of the art natural language processingsystems in in and in .
recall.
in addition to consistently producing the desired program as the top ranked result for most inputs the ranking algorithm places the desired program in the top results an additional of the time.
thus across all three of the domains for over of the natural language inputs the desired program is one of the three top ranked results.
this leaves less than of the inputs for any of the domains and only in the case of the text editing domain where the synthesizer was unable to produce and place the desired program in the top three spots.
computational cost.
fig.
shows the distribution of the time required to run the synthesis algorithm and perform the ranking on a .
ghz intel r core tm i7 cpu with gb ram.
on average translation takes seconds for text editing seconds for automata and seconds for the atis inputs.
further the distribution of times is heavily skewed with more than of the inputs taking under second and very few taking more than seconds.
the outliers tend to be inputs in which the user has specified an action in an exceptionally redundant manner.
as with any program synthesis technique which fundamentally involve search over exponential spaces the cost of our technique is also worst case exponential in the size of the dsl.
however the key issue is this efficiently for practical cases.
our synthesis works efficiently usually under second for a range of useful dsls.
the size of the dictionary has minimal impact on the run352figure timing performance of algorithm on all domains.
top rank domain coveragescore mappingscore structurescore atis .
.
.
automata .
.
.
text editing .
.
.
table performance of individual component scores in ranking the desired program as top result.
time as the translation only depends on the subset of the dictionary corresponding to the words in the input sentence.
.
individual component evaluation in we defined various components for ranking and provided intuition into their usefulness.
to validate that these component scores are important to achieving good results we evaluated our choices by using various subsets of the component scores learning the best weights for the subset and re ranking the programs.
performance of individual scores.
the results of using each component in isolation are presented in table .
this table shows that when identifying the top ranked program the best performance for using only coveragescore is using mappingscore is and for structurescore is .
this is far worse than the result obtained by using the combined ranking which placed the desired program as the top result for of the inputs.
thus we conclude that the components are not individually sufficient.
score independence.
although these results show that independently none of the components are sufficient for the program ranking it may be the case that one of the components is effectively a combination of the other two.
table shows the results of ranking the programs when dropping one of the components.
dropping structurescore results in the largest decrease as high as in the worst case and even the best case has a decrease of .
dropping coveragescore also results in substantial degradation although not as high as for structurescore.
the impact of dropping mappingscore is much smaller between and .
however the consistent positive contribution of mappingscore shows that it still provides useful information for the ranking.
thus all of the components provide distinct and useful information.
dictionary construction.
in practice the semi automated approach makes dictionary construction a task that while usually requiring manual assistance does not require expertise in natural language processing or program synthesis.
on average the dictionaries for each domain contained english words averaging .
words terminal and .
terminals word.
the user was prompted to provide .
map change on dropping domain coveragescore mappingscore structurescore atis .
.
.
automata .
.
.
text editing .
.
.
table impact of dropping individual component scores on top rank percentage.
domain total equal wt.
rank boost gradient count top top top top top top atis .
.
.
.
.
.
automata .
.
.
.
.
.
text editing .
.
.
.
.
.
table comparison of ranking using equal weights gradient descent and rankboost.
top top shows the percentage of benchmarks where the correct translation is ranked in top .
pings on average across the three dsls.
although beyond the scope of this work as it requires a larger corpus of training data the amount of user intervention can be further reduced by using statistical alignment to automatically extract the domain specific synonyms from the training data.
score combination weights.
we used gradient descent to learn how much to weight each score in the computation of the final rank of a program.
to evaluate the quality of the weights identified via the gradient descent we compared them with a naive selection of equal weights for all the component scores and with the results of boosting.
boosting is a frequently technique which combines a set of weaker rankings such as the individual component scores to produce a single strong ranking.
table shows the results of the rankings obtained with the three approaches.
using gradient descent has improved the number of top ranked benchmarks significantly over the naive weight selection as large as .
however the improvement in the top ranked benchmarks is much smaller.
similarly the gradient descent approach produces substantially better results than rankboost with an average difference of in the top ranked benchmarks.
thus we can conclude that the use of gradient descent for learning the combination weights is important for the overall quality of the results.
our choice of the ranking functions is critical to the quality of results.
as shown in table dropping any of the component functions results in a substantial loss of precision.
also using a simpler method such as equal weights or boosting to compute the combination weights results in a loss of in precision when compared to the use of gradient descent .
in our system most failures i.e.
the correct solution failing to rank in the top three solutions arise because some key information is left implicit in the english description e.g.
i want to fly to chicago on august .
in this case the departing city should default to current city and the time should default to any .
such issues might be fixed either by having orders of magnitude larger training data or by building some specialized support for handling implicit contextual information in various domains.
as part of learning the weights for the component scores we used a shifted variant of the logistic function as our loss function .
fig.
shows how the value of loss changes with iteration index and the corresponding number of top ranked benchmarks.
it can be seen that as the loss value decreases the number of top ranked benchmarks increases and vice a versa.
thus as these values are negatively correlated as needed for optimal performance of the gradient descent algorithm and even though our loss function contains 353figure behavior of loss function and top ranks.
change on using weights learnt for domain atis automata text editing atis .
.
.
automata .
.
.
text editing .
.
.
table generalization of learning across domains.
the log exponential approximation of the max operation it is well behaved for the gradient descent algorithm.
since the weights learned in .
are general purpose we expect that weights learned from one domain are applicable to other domains eliminating the the time and effort required to re learn these values on each new domain.
the results in table 8show that the weight vectors that are learned for one domain perform well when used to rank the results for a new domain.
the average decrease in the number of top ranked programs is only with a maximum decrease of .
for the number of top ranked programs the change is insignificant with a maximum decrease of less than and thus we do not include them here.
this result demonstrates that the learning of the component weights is highly domain independent and generalizes well allowing it to be reused or used as a starting point for new domains.
.
related work programming by demonstration pbd systems which use a trace of a task performed by a user and programming by example pbe systems which learn from a set of input output examples have been used to enable end user programming for a variety of domains.
for pbd these domains include text manipulation and table transformations among others .
recent work on pbe by gulwani et.
al.
has included domains for manipulating strings numbers and tables .
as mentioned earlier both pbd and pbe based techniques struggle when the desired transformations involve conditional operations.
in contrast the natural language based approach in this work performs well for both simple and conditional operations.
keyword programming refers to the process of translating a set or sequence of keywords into function calls over some api.
this api may consist either of operations in an existing programming language or a dsl constructed for a specific class of tasks .
these techniques use various program synthesis approaches to build expression trees from the elements of the underlying api similar to the bagalgorithm in and then use simple heuristics such as words used and keyword to terminal weights to rank the resulting expression trees.
these systems have low precision when used on inputs with complex intents accuracy in and will frequently suggest incorrect programs.
conversely due to the ranking methodology the synthesizer in this paper is able to maintain high accuracy even on complex domains such as atis where we achieve accuracy .semantic parsing is a sophisticated means of constructing a program from natural language using a specialized language parser.
several approaches including syntax directed nlp parse trees svm driven combinatory categorical grammars and dependency based semantics have been proposed.
these systems have high precision usually suggesting the correct program but recall rates below in atis.
in contrast the technique in this paper achieves similar or higher levels of precision while providing a recall rate near .
a number of natural language programming systems have been built around grammars nlc or templates naturaljava which impose various constraints on input expressions.
such systems are sensitive to grammatical errors or extraneous words.
there has been extensive research on developing natural language interfaces to databases nlidb .
while early systems were based on pattern matching the user s input to one of the known patterns precise translates semantically tractable nl questions into corresponding sql queries.
these systems depend heavily on the underlying data having a known schema which makes them impractical when the underlying data structure is unknown or non existent as in the text editing domain used in this work.
smartsynth is a system for synthesizing smartphone scripts from nl.
the synthesis technique in smartsynth is highly specialized to the underlying smartphone domain and uses a simple the ranking strategy for the programs that it produces.
similarly the nlyze system synthesizes spreadsheet formulas from nl.
again nlyze is designed for a specific domain spreadsheet formula and uses a relatively simple ranking system consisting of only the equivalent of the coverage mapping and overlap features presented in our paper.
in contrast our technique is agnostic to the specifics of the target dsl the ranking features are independent of the underlying dsl and we automatically learn appropriate weights for the features.
in addition as the experimental results intable demonstrate the use of the ranking methodologies described in smartsynth or nlyze results in substantial reductions in recall precision.
thus the ranking methodology in this paper presents an improvement on those use in nlyze smartsynth and an opportunity to further improve their performance by integrating the advancements.
the work in leverages natural language to enable compositional pbe programming by examples .
it does not apply any natural language learning techniques used in pbnl approaches but only utilizes the natural language decomposition into phrases followed by asking the user to provide example based interpretation of those phrases.
.
conclusion this paper develops a meta approach for synthesizing programs from natural language descriptions that can be instantiated for a range of interesting dsl s including text processing automata construction and information retrieval queries.
our approach takes three inputs from the synthesis designer a suitable dsl definition a basic training data set and assistance in construction of a wordsto token dictionary and from these inputs constructs a corresponding high precision and high recall nl to dsl synthesizer.
we aim to further generalize the framework to allow construction of synthesizers for a wider variety of domains.
another area of investigation is the addition of context awareness when translating single line intents like ours to provide interactive programming environment for programming in larger dsls and accomplishing more complex tasks.
354references r. alur l. d antoni s. gulwani d. kini and m. viswanathan.
automated grading of dfa constructions.
inijcai .
i. androutsopoulos g. ritchie and p. thanisch.
natural language interfaces to databases an introduction.
corr .
d. angluin.
learning regular sets from queries and counterexamples.
inf.
comput.
.
b. w. ballard and a. w. biermann.
programming in natural language nlc as a prototype.
in annual conference acm .
d. p. bertsekas.
nonlinear programming 2ndedition.
athena press .
c. m. bishop and n. m. nasrabadi.
pattern recognition and machine learning volume .
springer new york .
a. cypher.
watch what i do programming by demonstration.
mit press .
d. a. dahl m. bates m. brown w. fisher k. hunickesmith d. pallett c. pao a. rudnicky and e. shriberg.
expanding the scope of the atis task the atis corpus.
in hlt .
a. desai s. gulwani v .
hingorani n. jain a. karkare m. marron r. sailesh and s. roy.
benchmarks for program synthesis using natural language january .
https sites.google.com site nl2pgm .
y .
freund r. iyer r. e. schapire and y .
singer.
an efficient boosting algorithm for combining preferences.
j. mach.
learn.
res.
dec. .
r. ge and r. j. mooney.
a statistical semantic parser that integrates syntax and semantics.
in conll .
s. gulwani.
dimensions in program synthesis.
in ppdp .
s. gulwani.
automating string processing in spreadsheets using input output examples.
in popl .
s. gulwani.
synthesis from examples interaction models and algorithms.
in synasc .
s. gulwani.
flash fill excel feature .
flashfill.html.
s. gulwani.
example based learning in computer aided stem education.
in cacm .
s. gulwani w. harris and r. singh.
spreadsheet data manipulation using examples.
cacm .
s. gulwani s. jha a. tiwari and r. venkatesan.
synthesis of loop free programs.
in pldi .
s. gulwani v .
a. korthikanti and a. tiwari.
synthesizing geometry constructions.
in pldi .
s. gulwani and m. marron.
nlyze interactive programming by natural language for spreadsheet data analysis and manipulation.
in sigmod .
w. r. harris and s. gulwani.
spreadsheet table transformations from examples.
in pldi .
a. hindle e. t. barr z. su m. gabel and p. devanbu.
on the naturalness of software.
in icse .
s. kandel a. paepcke j. hellerstein and j. heer.
wrangler interactive visual specification of data transformation scripts.
inchi .
r. j. kate and r. j. mooney.
using string kernels for learning semantic parsers.
in acl .
r. j. kate y .
w. wong and r. j. mooney.
learning to transform natural to formal languages.
in aaai .
d. klein and c. manning.
accurate unlexicalized parsing.
in acl .
t. kwiatkowski l. zettlemoyer s. goldwater and m. steedman.
lexical generalization in ccg grammar induction for semantic parsing.
in emnlp .
t. lau s. wolfman p. domingos and d. weld.
programming by demonstration using version space algebra.
machine learning .
v .
le s. gulwani and z. su.
smartsynth synthesizing smartphone automation scripts from natural language.
in mobisys .
p. liang m. i. jordan and d. klein.
learning dependencybased compositional semantics.
in acl .
h. lieberman.
your wish is my command programming by example.
morgan kaufmann .
g. little t. a. lau a. cypher j. lin e. m. haber and e. kandogan.
koala capture share automate personalize business processes on the web.
in chi .
g. little and r. c. miller.
translating keyword commands into executable code.
in uist .
g. little and r. c. miller.
keyword programming in java.
autom.
softw.
eng.
.
z. manna and r. j. waldinger.
a deductive approach to program synthesis.
acm toplas .
m. manshadi j. f. allen and m. d. swift.
a corpus of scopedisambiguated english text.
in acl short papers .
m. mayer g. soares m. grechkin v .
le m. marron o. polozov r. singh b. g. zorn and s. gulwani.
user interaction models for disambiguation in programming by example.
in uist .
g. a. miller.
wordnet a lexical database for english.
cacm .
r. j. mooney.
learning for semantic parsing.
in cicling .
n. nihalani s. silakari and m. motwani.
natural language interfce for database a brief review.
ijcsi .
d. perelman s. gulwani t. ball and d. grossman.
typedirected completion of partial expressions.
in pldi .
h. poon.
grounded unsupervised semantic parsing.
in acl .
a. m. popescu a. armanasu o. etzioni d. ko and a. yates.
modern natural language interfaces to databases composing statistical parsing with semantic tractability.
in coling .
a. m. popescu o. etzioni and h. a. kautz.
towards a theory of natural language interfaces to databases.
in iui .
d. price e. riloff j. l. zachary and b. harvey.
naturaljava a natural language interface for programming in java.
in iui .
ranking.
modified competition ranking ranking .
.
m. raza s. gulwani and n. milic frayling.
compositional program synthesis from natural language and examples.
in ijcai .
r. singh and s. gulwani.
learning semantic string transformations from examples.
pvldb .
r. singh and s. gulwani.
synthesizing number transformations from input output examples.
in cav .
a. solar lezama.
program synthesis by sketching .
a. solar lezama l. tancau r. bod k s. a. seshia and v .
a. saraswat.
combinatorial sketching for finite programs.
in asplos .
s. srivastava s. gulwani and j. foster.
from program verification to program synthesis.
in popl .
n. g. stanford.
stanford parser .
.
.
edu software lex parser.shtml .
d. m. l. xu r. bod k and d. kimelman.
jungloid mining helping to navigate the api jungle.
in popl .
k. yessenov s. tulsiani a. menon r. c. miller s. gulwani b. lampson and a. kalai.
a colorful approach to text processing by example.
in uist .
l. zettlemoyer and m. collins.
learning to map sentences to logical form structured classification with probabilistic categorial grammars.
in uai .
l. s. zettlemoyer and m. collins.
online learning of relaxed ccg grammars for parsing to logical form.
in emnlpconll .