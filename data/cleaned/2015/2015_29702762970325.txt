applying combinatorial test data generation to big data applications nan li research and development medidata solutions new y ork ny usa nli mdsol.comyu lei dept.
of computer science and engineering the university of texas at arlington arlington tx usa ylei cse.uta.eduhaider riaz khan research and development medidata solutions new y ork ny usa hriaz mdsol.com jingshu liu research and development medidata solutions new y ork ny usa jliu mdsol.comyun guo dept.
of computer science george mason university fairfax va usa yguo7 gmu.edu abstract big data applications e.g.
extract transform and load etl applications are designed to handle great volumes of data.
however processing such great volumes of data is time consuming.
there is a need to construct small yet effective test data sets during agile development of big data applications.
in this paper we apply a combinatorial test data generation approach to two real world etl applications at medidata.
in our approach we first create input domain models idms automatically by analyzing the original data source and incorporating constraints manually derived from requirements.
next the idms are used to create test data sets that achieve t way coverage which has shown to be very effective in detecting software faults.
the generated test data sets also satisfy all the constraints identified in the first step.
to avoid creating idms from scratch when there is a change to the original data source or constraints our approach extends the original idms with additional information.
the new idms which we refer to as adaptive idms aidms are updated by comparing the changes against the additional information and are then used to generate new test data sets.
we implement our approach in a tool called combinatorial big data test data generator bit tag .
our experience shows that combinatorial testing can be effectively applied to big data applications.
in particular the test data sets created using our approach for the two etl applications are only a small fraction of the original data source but we were able to detect all the faults found with the original data source.
ccs concepts software and its engineering !software testing and debugging keywords big data testing combinatorial testing input domain model adaptive input domain model test data generation .
introduction and challenges one important characteristic of the big data concept is high volume .
the datasets used in industry are often measured in terabytes or higher orders of magnitude.
for example at medidata we deal with a large amount of data from various clinical trial sites studies and subjects.
also as more clients are using internet of things iot 1such as vital connect and actigraph apps the amount of data to process is expected to grow quickly and tremendously in the future.
even with latest advances in computing technologies such as hadoop mapreduce2 processing large amounts of data can easily take days weeks or even months.
since processing high volumes of data can be time consuming small yet effective test data sets need to be constructed for testing during development of big data applications.
this is particularly so for agile software development where testing is performed frequently.
in this paper we deal with one common type of big data application called extract transform and load etl application.
etl application developers write sql hive or pig3scripts for reporting and analytics purposes.
in a typical etl process data is extracted from an original data source e.g.
a microsoft sqlserver database and is then transformed into a structured format e.g.
a flat file on amazon simple storage service s3 to support queries and analysis.
finally the data is loaded into a final target e.g.
a postgresql database for customers to view.
the original data source is called the source.
the final target that loads data is called the target.
for example at medidata we compute store and analyze dozens of terabytes of clinical trial data through etl processes using amazon web services aws .
in industry practitioners often manually generate small test data sets for testing etl applications.
manual test data generation can be time consuming labor intensive and error prone.
on the one 1the iot inter connects embedded devices on the internet.
2apache hadoop mapreduce processes large data sets over clusters of computers using hadoop distributed file system hdfs .
3apache hive and pig scripts are transformed to mapreduce programs that run on top of hdfs.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
hand it may be difficult to ensure validity of test data when various business and or structural constraints must be satisfied.
on the other hand the quality of test data heavily depends on human judgment and can vary significantly.
in this paper we report our experience on applying a combinatorial testing approach to two real world etl applications.
our approach consists of two major steps creating input domain models idms and applying combinatorial testing to the idms.
in the first step we automatically create a set of idms one for each database table from the original data source .
we consider each column of a database table to be an input parameter.
for each input parameter important test values are derived from constraints that are either automatically extracted from database schema or manually specified by the user.
for example the user may specify a statisticsbased constraint that considers distinct values with the most or least appearances in the original source to be important test values.
in the second step we apply combinatorial testing to the idms to generate test data.
combinatorial testing has been shown to be a very effective software testing strategy.
the key observation is that most faults are caused by interactions involving only a few parameters .
a t way combinatorial test set is designed to cover all the t way interactions i.e.
interactions involving no more than t input parameters.
when input parameters are properly modeled a t way test set is guaranteed to expose faults that are caused by interactions involving no more than t parameters .
one problem arises when there are changes in the original data source or constraints.
since the original data source can be large for big data applications re creating the idms from scratch whenever there is a change is very inefficient.
to address this problem we extend the original idms with additional information.
the new idms which we refer to as adaptive idms aidms include the original idms and some analytical results of the original data source and constraints.
for example the analytical results may include the statistical distribution of the original data source.
when the original data source or constraints change our approach does not need to re process the original data source and constraints.
instead it quickly updates the aidm by comparing the changes against the analytical results saved in the aidm and then generates new test data sets to reflect the latest changes.
because we process the original source data only once frequent test generation is made possible which is necessary to support agile development.
aidms will be discussed in section .
in detail.
we point out that our approach reuses the real data in the original source as much as possible even though new data may also be generated.
this helps practitioners to generate expected outcomes and also facilitates debugging because real data is more meaningful and thus easier to understand by practitioners compared to using data that is entirely synthetic.
we have implemented our combinatorial testing approach in a tool called combinatorial big data test data generator bit tag .
we have used bit tag to test two real world etl applications at medidata.
the applications contain nearly lines of code processing a total of over .
tb data with .
billion rows.
the experimental results show that the test data sets created using our approach are only a small fraction of the original data source but still detect all the faults that are detected by the original data sources.
in the rest of the paper we use bit tag to refer to our combinatorial approach and tool when there is no ambiguity.
we highlight the following major contributions of this paper.
.
we present a systematic approach that applies combinatorial testing to big data applications.
.
we implemented our approach in an enterprise strength tool i.e.
bit tag and evaluated the effectiveness and efficiency of bit tag using two real world industrial etl products.
.
we report several compelling insights gained from our experience.
these lessons could help practitioners better understand and improve the process of applying combinatorial testing to big data applications.
the paper is organized as follows.
sections and present the test data generation approach and implementation.
section gives the experiment.
section discusses the related work.
section concludes this paper and discusses the future work.
.
the approach this section presents in detail our combinatorial approach to generate test data.
section .
provides a brief introduction to combinatorial testing.
section .
discusses how we create idms with the original data source and constraints.
section .
discusses how we generate test data from idms.
section .
shows how bit tag creates and updates aidms adaptively when there are changes.
.
combinatorial testing let m be a program with n parameters.
combinatorial t way testing requires that for any t out of n parameters of m every combination of values of these t parameters be covered at least once.
the value of t is referred to as the testing strength.
consider a program that has three parameters p1 p2 and p3 each parameter having two values and .
a way or pairwise tests set for the three parameters p1 p2 p3 could be and .
an important property of this test set is that if we pick any two columns i.e.
columns p1 and p2 columns p1 and p3 or columns p2 and p3 they contain all four possible pairs of values of the corresponding parameters i.e.
.
an exhaustive test set for these parameters consist of tests.
a number of algorithms have been developed for combinatorial test generation .
in particular lei et al.
reported on a combinatorial testing strategy called ipog .
the ipog strategy generates a t way test set to cover the first t parameters and extends this test set to cover the first t parameters.
this process is repeated until this test set covers all the parameters.
lei et al.
also reported a tool called acts that implements the ipog strategy.
in this paper bit tag uses acts to generate combinatorial test sets.
combinatorial testing can significantly reduce the number of tests.
for example a pairwise test set for boolean parameters only needs as few as tests whereas an exhaustive test set consists of tests .
despite this dramatic decrease in the number of tests combinatorial testing has been shown to be very effective for general software testing .
.
input domain model to apply combinatorial testing we need to create idms consisting of parameters and important test values.
unlike previous research where idms are often created manually bit tag automatically creates idms.
bit tag analyzes constrains derived from requirements and database schemas.
some constraints are automatically derived while others are manually specified.
next bit tag decides which test values are extracted from the original data and then creates idms with the extracted test values.
data in various formats e.g.
videos and graphs could be saved in different kinds of databases e.g.
no sql database .
in the context of this paper we study how to generate idms from relational databases which are widely used by many data analytical products nowadays.
we will discuss how to create traditional idms in this section and extending idms to aidms in section .
.
638creating an idm consists of the following three steps.
first we identify all the parameters for which we need to generate test values in the idm.
second we select a characteristic for each parameter to create a partition over the domain of this parameter.
the partition should be complete i.e.
it should cover the whole domain and disjoint i.e.
the partitioned blocks must not overlap.
for example a characteristic for a parameter of the string type may be if this parameter is null.
the domain of this parameter is divided into two blocks null value and values that are not equal to null.
third we can select null from the first block and a random value such as test from the second block since values in the same block are considered to be equivalent.
as mentioned in section we create one idm for each table in a database.
each column is considered as a parameter of the idm.
to create partitions we extract important values from the constraints the types of constraints are shown below that are applied to each parameter.
all values derived from the constraints are used as boundaries to create blocks.
for example if we derive from constraints the minimum integer min int and the maximum integer max int for an integer parameter the domain is partitioned into three blocks min int max int .
then we can select the max value from each block.
next we will discuss how to collect the constraints and how to derive test values from the constraints to create partitions for each data type.
we collect constraints from three sources.
first constraints e.g.
foreign key constraints are derived from database schema.
second users may specify constraints that are derived from requirements.
for example users may specify some important test values to include in specific value constraints because these values are required to be queried in the requirements.
it is important for testers to get independent requirements and avoid interpreting source code sql hive or pig scripts developed by programmers.
third bittag provides built in edge case test values for different data types included as specific value constraints.
bit tag supports ten general types of constraints.
in this section we discuss five constraints related to creating idms below check default specific value logic and statistics based constraints.
check and default constraints are derived from database schema and the other three are specified by users.
the other five constraints used for test data generation will be discussed in section .
.
a check constraint is a logical expression specified for a column specified in database schema e.g.
age and all the data values in this column must satisfy this expression.
we can derive important boundary values from logical expressions such as and .
a default constraint provides a default value to insert if no values are explicitly specified.
default values are identified as important test values.
specific value constraints add special values to test data sets.
for instance requirements may stipulate objects whose statuses are equal to and .
then for the status column we can add as specific values.
bit tag derives unicode characters such as chinese characters if no unicode characters are included in the original data source.
this is important for testing products that have worldwide customers.
statistics based constraints are used to derive important test values by performing statistical analysis over the original data source.
bit tag counts the numbers of appearances frequencies of distinct test values for each input.
by default bit tag selects the most frequent value the least fre quent value and a value with a random frequency between the lowest and highest frequency values.
logic constraints are additional logical expressions derived from requirements from sql where clauses.
similar to check constraints bit tag extracts values from logical expressions to satisfy the where clauses.
for example if all active projects are required to be selected in a script and the where clause is isactive then is selected.
it is crucial to write constraints in a format that allows people to easily specify them and bit tag to easily parse them.
we use json4as the basic format to design the data structure for each constraint.
json is a lightweight data format and has been widely used in industry.
we specify the constraints for each table in a separate file.
the detailed specification for each constraint will be presented in section .
.
we consider database data types as three general categories numeric date time and string.
numeric types include boolean integer float double etc.
date time types include date time datetime timestamp etc.
string types include char varchar text etc.
we create partitions for the general categories of data type as follows.
if an input domain is numeric the range from the minimum value to the smallest derived test value forms a block and the range of the smallest derived test value to the second smallest derived test value forms another block and so on.
if an input domain is of a date and time data type the partition is created similar to the process for a numeric data type.
the only difference is that there could be a separate block that has a null value which is not between any two dates.
an empty string is automatically converted to a default date.
if an input domain is of string data type each distinct test value is a block and the rest of the domain is another block.
if a statistics based constraint derives additional values and on top on the aforementioned blocks over an integer domain min int and max int then max int is further divided into and max int .
bit tag does not derive more values if and are already selected before the statistics based constraint is applied.
however this situation is rare for non binary types in practice.
for example we may create a partition min int and max int and given the min int and the max int.
if the data type of a column e.g.
employee role is string and the derived test values are manager engineer and director then these three values are three blocks and all other string values form another block.
after idms are constructed we select test values from each block.
if the data type of the domain is numeric we select the max value of each block.
if the data type of the domain is date and time we select the max value of each block as well as the null value if it exists.
if the data type is string we choose the sole value of each block since each derived test value is a block.
for the remainder of the domain we select the empty string from it if this value has not been selected.
if the empty string has been selected we select a random string.
note that other strategies could be used to select test values from each block.
.
test data generation in this section we discuss foreign key logic density unique key combinatorial coverage and test set size constraints used for test data generation.
section .
.
shows the constraints.
section .
.
describes an algorithm to generate test data with the constraints.
.
.
test data generation constraints we use density constraints to control the size between table objects foreign key and unique key constraints to ensure data validity and use logic and combinatorial coverage constraints to achieve a well defined test data coverage.
the five constraints about test generation are explained as follows.
foreign key constraints define referential relationships between tables.
bit tag automatically extracts foreign key constraints from database schema.
sometimes foreign key constraints are purposely omitted due to performance concerns then they can be manually specified in json.
density constraints apply when tables have foreign key constraints.
in a table a foreign key column can be referred to as a parent column and a primary key column can be referred to as a child column.
for example if the table studies has a primary key column studyid and a foreign key column projectid projectid is a parent column and studyid is a child column.
this indicates that a project could have one or more studies.
if each project has only a few studies the density is low.
if each project has thousands of studies the density is relatively high.
if we have such constraints across multiple table objects at different hierarchies then the constraints determine the size of the test data sets to be generated.
for example we can use this constraint to define how many projects are there how many studies are in each project how many subjects are in each study and how many records are in each subject etc.
usually we do not want to apply combinatorial coverage to all the columns in a table since so could generate too many rows.
we usually specify a combinatorial coverage criterion such as pair wise pw coverage to two or three critical columns that are used in join or filter conditions of sql scripts.
pw requires one value from each block for a characteristic to be combined with a value from each block for another characteristic.
logic constraints are additional logical expressions derived from requirements about sql where clauses.
if a logical coverage criterion such as predicate coverage pc andclause coverage cc is specified in constraint configuration files bit tag analyzes the expressions to generate test values to satisfy the coverage.
note that pc requires a test set to evaluate the predicate to true and false.
cc requires a test set to evaluate every clause to true and false.
the test set size constraint specifies a number of rows to be generated in a test set of a table.
users may or may not add density combinatorial coverage logic or test set size constraints for a table.
if they do not specify any of these constraints bit tag just uses the test values derived from idms.
in this case each choice coverage ec is applied.
ec requires each value from each block for each characteristic of the idms to appear at least once in the test data.
since specific value and statistics based constraints may still be used a column to which a specific value constraint is applied may have more values than another column to which no constraints are applied.
to apply ec we may have to generate additional values for the columns that have few values from their idms.
if a column has a unique key constraint we generate new values randomly otherwise we re use the existing test values from the idm.table the studies table studyid active dc active no dc projectid s1 p1 s2 p1 s3 p2 s4 p2 .
.
test data generation algorithm when density combinatorial coverage logic or test set size constraints are applied we use the test data generation algorithm in algorithm to address the constraints.
first we consider foreign key constraints.
the row number for a table rows is set to the maximum integer.
we calculate an ordered list of tables using topological sorting of the foreign key constraints.
we start generating test data for a table without foreign key constraints or dependencies followed by the tables that have dependencies.
second we generate test values for density constraints before we consider combinatorial coverage and logic constraints.
otherwise we have to re process combinatorial coverage and logic constraints because density constraints could generate more test values which may not satisfy the combinatorial coverage and logic constraints.
we take values from the idms for the child and parent columns and generate test values for the parent columns first.
if there is more than one parent column bit tag uses acts to generate a data set to satisfy all combination coverage.
if users specify a specific number between and the number of total combinations the user specified number of instances are randomly selected from the complete set of combinations for the parent columns otherwise all the combinations are used.
then we generate values for the child columns which are usually primary key columns.
if the required ids for the child columns are more than what we have in the idms bit tag randomly generates more ids.
rows is updated with the number of child instances.
third we generate test values for columns to which combinatorial coverage is applied using acts.
bit tag understands the columns test values for each column and the coverage criterion and then passes them to acts to generate test values.
if a density constraint exists the combinatorial coverage is applied to all child objects of each parent instance.
it is important that the number of the child objects for each parent instance is greater than the number of the values generated by the combinatorial coverage otherwise extra values could be discarded.
rows is updated with the number of the combinations.
fourth we generate test values to satisfy logic constraints.
similar to combinatorial coverage constraints if a density constraint exists the logic constraint is applied to all child objects for each parent instance.
otherwise the logic constraint is applied to all existing rows of the table.
in the current implementation if a logic constraint specifies the predicate coverage the test data for a table is expected to have one half of the rows to evaluate the predicate to true and the other half to evaluate the predicate to false.
last if the test set size constraint specifies a larger number than rows we generate additional rows.
for each extra row we randomly re use test values from the aidms for each column.
if unique key constraint is applied to a column bit tag generates new values randomly.
one important decision in algorithm is to handle combinatorial coverage and logic constraints when density constraints exist.
we give an example to show how algorithm handles logic constraints with the presence of density constraints.
the same idea is 640applied to the handling of combinatorial coverage constraints.
assume we have a table that has a primary key column studyid a foreign key column projectid and another column active that indicates whether a study is active.
in addition we have a density constraint that requires each parent column projectid instance to have two children column studyid objects.
then the project p1 has studies s1ands2and the project p2has studies s3ands4 as shown in table .
if we have a logic constraint that specifies pc over column active bit tag generates test values to satisfy pc for each parent instance p1andp2 as shown in the second column in table dc means density constriants .
however if no density constraints are specified bit tag could generate test values in the third column.
all the tuples in the table satisfy pc but the tuples for each project do not.
this approach ensures pc is still satisfied no matter which project instance is filtered out in join operations.
algorithm test data generation algorithm require a set of aidms one for each table ensure a set of test data tdfor each table calculate an ordered list of tables tbased on topological order of the foreign key constraints between tables foreach table t2tdo rows int max ifa density constraint dc exists then generate test values for parent and child columns end if ifa combinatorial coverage constraint ccc exists then ifa density constraint exists then foreach parent instance p2dc do generate test data so that all tuples for psatisfy ccc end for else generate test data so that all tuples in tsatisfy ccc end if update rows with the current row number end if ifa logic constraint lcexists then ifa density constraint exists then foreach parent instance p2dc do generate test data so that all tuples for psatisfy the coverage in lc end for else generate test data so that all tuples in tsatisfy the coverage in lc end if update rows with the current row number end if ifa test set size constraint tssc exists then ifthe row number of tssc rows then add extra rows from aidms end if end if save the test data into td end for return td .
adaptive input domain model in this section we discuss how bit tag expands idms with additional information to create adaptive input domain models aidms .
aidms make it possible to update idms when a changearises without reprocessing the original data source.
the statisticsbased constraint is the only constraint that requires to process the original table data excluding database schema .
when applying this constraint we select test values based on their frequencies.
so bit tag saves distinct test values with their frequencies for each column for each table and these are referred to as the analytical results in this paper.
later on when new data is added or the statisticsbased constraint changes bit tag can re use the saved analytical results to select the new values.
in addition all constraints including the schema based constraints are saved in corresponding data structures.
thus we do not need to re process the original data source to get the database schema either.
therefore aidms include idms the analytical results and constraints.
all the data are saved in corresponding data structures in json files.
if data is changed we compare the new data with the saved analytical results.
medidata has an internal tool to automatically collect latest data changes and save them as special json data sets.
the syntax of the json structure includes key information such as type changes when .type specifies the type of a change update create or delete.
changes specifies the actual changes on a column including the column name the old value and the new value.
when gives the time for the change.
by analyzing these data sets bit tag knows what data is old and what data is new without reprocessing the original data source.
if a value is added and it does not exist in the analytical results this new value with the frequency of one is added.
if there is one change to a value that has existed in the analytical results the frequency of the value is modified or the value is deleted.
then bit tag may derive different test values to create idms.
for example if the values of the most frequent and least frequent distinct test values are affected the derived test values from statistics based constraints may differ.
when constraints change bit tag finds out which columns are affected by the changes and only updates the constraint data for the affected columns in aidms.
the idms for unaffected columns remain the same.
bit tag may derive different test values from the new constraints including check default specific value logic and statistics based constraints.
note that when statistics based constraints change we derive new test values from the saved analytical results.
then bit tag uses the newly derived test values to create idms.
when the schema of a table is changed bit tag has to re generate aidms for this table.
bit tag takes the same test data generation approach with the new foreign key density combinatorial coverage and test set size constraints.
testers often need to add extra foreign key constraints when the database is missing some because the testers were unaware of those foreign key constraints at the start.
.
implementation of bit tag section .
describes the architecture of bit tag.
section .
presents the specifications of constraints.
section .
discusses the most recent implementation and development of bit tag.
.
architecture figure shows the overall architecture of bit tag.
bit tag consists of two phases the initial cycle and subsequent cycles.
in the initial cycle bit tag collects and analyzes the original data source and initial constraints.
bit tag saves analytical results and derives important values to create idms for every input.
then bittag creates aidms by combining the idms with analytical results.
an effective test set is generated from the aidms by satisfying combinatorial coverage and other constraints.
in the subsequent cycles we deal with three types of change.
first new data coming from customers.
second existing con641figure the architecture of bit tag straints are modified or new constraints are added.
practitioners often need to update constraints in agile development as they understand more about requirements.
third reviewers return feedback about the previously generated effective test data sets.
reviewers could include product analysts architects developers and testers.
then the constraints are updated based on the feedback.
therefore when the data or constraints change bit tag collects and analyzes the changes and updates the existing aidms.
a new effective test data set is generated from the new aidms independent of the prior test data set.
.
specifications of constraints during agile development of etl applications we focus on analysis of requirements of data transformation.
while analyzing the requirements we need to understand how each column in the source is mapped to each column in the target.
we also need to understand the detailed transformation logic e.g.
how to transform a column of the integer type in the source to a column of the boolean type in the target?
in addition when we perform a join operation between two or more tables we need to understand the join and filter conditions i.e.
the conditions used in the onandwhere statements.
domain experts and architects usually construct the mapping information from the requirements.
bit tag users derive constraints from the requirements along with the mapping information.
next we explain each constraint in detail and discuss how to specify each constraint.
for statistics based constraints bit tag saves distinct values with their frequencies for each column.
if the original data source is very large e.g.
in terabytes it may take a long time to process the data and a lot of space to save the analytical results when there are a lot of long distinct values .
bit tag could save a small subset e.g.
whose values are chosen randomly from the original data source.
then the saved partial data should have similar statistical distribution as the original data source.
note that we do not need to record data of some columns such as uuids.
uuids in the original data source are not important for testing and we can generate new uuids in test data sets.
in addition saving them usually takes a lot of disk storage.
by default bit tag derives the most frequent least frequent test values and another value with a random frequency.
users can specify another number that is greater than three to derive more test values.
the extra values besides the default selection are chosen randomly.bit tag automatically extracts check and defaults constraints from database schema.
if they do not exist in database schema and users would like to specify them in json check constraints can be specified as logic constraints and default constraints can be specified as specific value constraints.
each table has a json file that specifies constraints.
the types of constraints are explained in the next paragraph.
statistics based specific value foreign key and unique constraints are specified in a column attribute which includes columnname referredtablename referredcolumnname specificvalues statisticsvalues and isunique .
a density constraint includes child parent maxchildren and numparentinstances .
the child attribute is the child column id the primary key .
the parent attribute shows the parent column ids with foreign keys .
the maxchildren attribute gives the maximal number of child instances included by each parent instance.
the numparentinstances attribute presents the number of parent instances.
a combinatorial coverage constraint includes columns andcovtype .covtype specifies a coverage criterion applied to columns .
a logic constraint includes expression andgentype which specifies a logic expression and a logical coverage criterion to apply.
a test set size constraints includes an integer to specify the number of rows of the test set to generated.
.
implementation and usage we have implemented the combinatorial test data generation approach in a java based software application bit tag.
some detailed information is listed in table .
currently bit tag is able to read database data and schema from microsoft sqlserver and mysql databases.
to generate test values to satisfy combinatorial coverage we integrate acts into bit tag.
with the efficient algorithm ipog acts can generate a test data set to satisfy t way coverage pretty quickly.
potentially satisfying t way coverage when t is large t and there are lots of test values for each input parameter could take a long time.
however in our usage we usually apply t way coverage to no more than four input parameters.
because we modestly and smartly select test values from constraints test values for each input parameter are not many.
satisfying t way coverage using bit tag does not take a long time.
detailed information can be found in section .
to improve the code quality we have used jacoco to measure the statement and branch coverage for bit tag.
to avoid potential faults and improve readability we applied checkstyle 642table bit tag language files lines java xml others total and sonarqube static analysis tools.
in addition we also created the continuous integration ci environment for bit tag on jenkins .
when a new test data set is generated expected values in test oracles are likely to change.
testers have to manually write new expected values.
to reduce the changes on the expected values bit tag uses the same seed for random methods.
for security and privacy reasons bit tag sanitizes identifiable information of real customers such as emails when extracting data from original data sources.
bit tag can be used for big data applications that use databases and have constraints and business rules on data objects.
in the current implementation of bit tag most of the constraints except foreign key constraints are used to describe data relationships and combinations between no more than one table.
currently we are not able to specify special business rules that involve more than two tables.
for example consider tables a and b which are joinable and undergo the join operation.
after the join of a and b tuples that have the same value for a column in b must have the same value for a column in a. to handle such special business rules we provide public apis to users.
the users can programmatically make bit tag api calls to access the aidms and generated test data for each table object.
therefore the users can implement the business rules by modifying the test data.
the problem of how to manually derive constraints from requirements in a systematic manner and optimize data structures of constraints is an ongoing research topic.
an in depth discussion about this topic is out of the scope of this paper.
.
experiments the objective of the experiments is to examine if we can use test data sets generated by bit tag to replace data sets generated by existing approaches.
currently there are three possible approaches including use the original data source select test data randomly from the original data source and generate test data sets manually.
we did not compare to manually generated data sets because the manual data generation is an ad hoc approach.
the manual data sets may differ to a great degree as different developers could generate different test data sets depending on their experience and domain knowledge.
the development phase also has an effect in that the data set used in a later phase is supposed to be more effective than the data set used in an earlier phase since developers keep updating the data set as they have better understanding of requirements and find faults or deficiencies in the program.
we call the original data source and random test data sets traditional data sets.
specifically we study the efficiency and effectiveness of bit tag compared to the traditional data sets.
the efficiency is defined in terms of time of executing etl programs and the effectiveness is defined in terms of faults found.
.
experiment design we show the subject systems first in section .
.
discuss three kinds of experimental test data and faults in section .
.
and present the experimental procedure in section .
.
.table the cloc report for the project a language files lines sql xml c others total table the cloc report for the project b language files lines sql java javascript xml others total .
.
experimental subjects to evaluate the effectiveness and efficiency of data sets generated by different approaches we used two real world enterprise level etl projects from medidata as our subjects.
due to non disclosure agreements we cannot show project names customer names or other detailed information about the products.
thus we use p a andp b to refer to these two projects.
p a extracts transforms and loads microsoft sqlserver datab ases.
for each customer we have a separate database.
in the experiments we selected three databases from hundreds of customer databases to represent three categories in size small medium and large.
these three database are named a db a db and adb and their sizes are gb 140gb and .4tb respectively.
these three databases are often used in development sandbox environments.
this project primarily uses microsoft sql server integration services ssis for data migration and transformation.
the development of the ssis packages are under the microsoft visual studio environment.
the project b extracts and transforms data from a mysql databa se and loads the transformed results into amazon simple storage service aws s3 in the csv format.
the original mysql source used for p b referred to as b db has gb data.
unlike p a that has separate databases for different customers p b has only one database for all customers.
this project uses pentaho as the business intelligence and analytics solution for data transformation.
pentaho provides various widgets for developers to quickly perform tasks for etl work flow jobs such as copying files validating xml files truncating tables writing logs sqoop exporting and pentaho mapreduce etc.
on top of the built in widgets developers can easily write sql scripts and programs in another language such as java to accomplish other general purpose operations.
table and table show the number of lines for p a andp b measured by a line counter cloc version .
.
.
the first three columns of table show the sizes and row counts of the core tables used for p a andp b. out of tables p a uses core tables which have a total of columns.
out of tables p b uses core tables which have a total of columns.
.
.
experimental data sets and faults we chose original data sources and randomly selected data i.e.
random data sets as traditional data sets to compare with bit tag 643table original and test databases databases size row count test database size row count size row count a db 14gb t a db 17mb a db 140gb t a db 17mb a db 1400gb t a db 17mb b db 3gb t b db 2mb data sets.
we will analyze each data source and give the reason that we did not use data sets generated manually.
developers started with manually generated data sets a common practise in industry as mentioned in the introduction but soon they abandoned these data sets because it was difficult to manually update data sets with frequent constraint changes.
they started using data sets generated by bit tag with the help of bit tag developers.
the original data sources used in the experiments are data in the sandbox environment used for development.
these data sources are replicas of the production data at one time.
though the original data sources may not have the latest customer data they are usually considered to be the most comprehensive in terms of constraints and relationships covered across different tables.
so using the original data sources is supposed to find nearly all potential faults.
the drawback is that the original data sources have high volumes of data and take a long time to process.
even though we will eventually deploy and run etl programs in production against the original data sources we do not want to run the original data source every time with frequent changes.
if we randomly select a small amount of data from a database it is likely that no results are returned after join operations when the selected data do not satisfy foreign key constraints.
thus the data set could not be very effective.
if we randomly select many data from each table the data could satisfy foreign key constraints but they may still take too long to process in agile development.
we would like to see if a random data set has the same effectiveness as the bit tag data set with the same size.
currently we do not have a tool that randomly selects data that satisfy foreign key constraints.
so we just randomly selected data from the original data sources with the same number of rows as those in the bit tag data set for each table.
for the effectiveness we identified faults based on the fault list of p a and history changes of p b.p a has nine documented faults butp b does not have faults recorded.
similar to many empirical studies that apply mutation analysis where synthetic changes to source code are treated as potential faults we treated the past changes in p b as potential faults.
the rationale is that we believe an effective test data set would cause developers to find faults in the existing code base and make changes to the source code.
ideally when developers make a new change to the core transformation running the test data would render a different result from a prior code commit.
so we checked the changes to the core transformation steps that have sql scripts on the github repository of p b. faults were counted by sql scripts by commits.
in each code commit all the changes made to a sql script are considered to be a fault.
we examined every fault to make sure it is not semantically equivalent to the original program.
we tested every faulty version to make sure executing it using the original data source rendered a different result from the original program.
we counted nonsemantically equivalent faults in the core transformation steps for p b. .
.
experimental procedure we ran the experiments as follows.
.
we read and understood the requirements of p a andp b with help from product analysts and developers.
.
we created the constraints for each table of the databases used in p a andp b. .
we generated test data sets using bit tag for p a andp b and recorded the sizes of the test data sets.
we used t a db1 t a db t a db t b db to refer to the test data sets generated by bit tag from the corresponding original data sources.
.
we generated random test data sets from the original data sources of p a andp b and recorded the sizes of both the original data sources and random data sets.
.
we ran the etl jobs in each project using the original data sources saved the results as the expected values and recorded the execution time.
.
we injected one fault into the corresponding project at one time.
ran the etl job with the injected fault using the original data source saved the results as actual values.
.
we compared the expected and actual values.
if they differ and if using the original data source detected this fault and recorded this fault.
.
we repeated the steps using the bit tag and random test data sets.
we conducted the experiments on a bit windows server r2 platform with intel i7 .40ghz processors gb ram and 5tb hard drive.
we used microsoft sqlserver and mysql .
for p a andp b respectively.
during agile development of etl programs developers could make frequent changes to the core transformation jobs as their understanding about the requirements evolves.
bit tag plays a critical role in such the agile development since it allows a new test data set to be quickly generated when such changes take place.
the key to specification of constraints is to develop a good understanding about the requirements and the product databases.
after such understanding is developed actual specification of constraints does not take much time.
this has been the case in our experiments.
note that business rules about a product database are relatively stable while the actual data may be frequently changed in the database.
in addition database schema related constraints are automatically extracted.
these factors help to reduce the time and effort needed for constraint specification.
.
experimental results table shows the constraints and business rules we manually specified and automatically derived from databases for p a and p b. we use to represent that it is not applicable to specify constraints.
the developers of bit tag specified constraints and generated test data sets but did not know the faults.
we added specifications for some of the foreign key constraints and unique key 644constraints because they are missing in a db a db a db andb db.
for statistics based constraints bit tag extracts three values by default.
we did not give extra specifications.
bit tag applied combinatorial coverage criteria when they are specified in the combinatorial coverage constraints.
for other columns where constraints are not specified ec is applied by default.
we also implemented some business rules using the apis of bit tag.
table constraints and business rules for each project constraintsmanual automated p a p b p a p b foreign key check default specific value logic statistics based density unique key combinatorial coverage test set size business rules we did not measure the time for translating requirements into constraints because most of the time was spent on understanding requirements and product business rules.
once we understood the requirements and business rules the translation did not take much time.
in addition it was difficult to separate the time for the translation from the time for understanding the requirements.
practitioners usually start coding while they are understanding requirements in agile development.
through a few agile development cycles the practitioners fully understand the requirements.
so it is hard to measure the time for learning the requirements.
table shows the sizes of the test data sets generated by bittag compared with the original data sources.
the last two columns show the percentages of the size of the test data set over the size of the original data source in terms of disk storage and row count.
t a db t a db and t a db have the same size even if their original data sources i.e.
a db a db and a db vary from gb to .
tb.
this is because we used the same constraints to generate the test data.
a db a db and a db have the same database schema.
for the same product with different customer databases we can use the same constraints for the test data generation.
the test values may vary in each test data set because statistic based constraints may extract different test values based on frequencies from each customer database.
the test data sets are very small compared to the original data sources.
t a db3 is only .
and .
of a db in terms of the disk storage size and row count respectively.
since random test data have the same row counts as the bit tag test data set we do not show them.
table shows the generation time for the test data sets by bittag.
m represents minutes.
s represents seconds.
h represents hours.
d represents days.
w represents weeks.
d and w are used in table .
we measured the time for generating initial test data sets and subsequent test data sets using bit tag.
we measured the time five times and calculated the averages.
generating the initial test data sets could take a long time because this needs to process original data sources.
even if generating t a db took up to hours from a db this happened for only one time.
generating subsequent test data only took minutes when we changed constraints.
we used the built in random method of sql to gen table time for test data generation test databases time first time second t a db 3m30s 2m t a db 40m 2m t a db 18h 2m t b db 2m 1m table faults detected project total faults bit tag original random a b total erate random test data sets for each table from every original data sources.
generating random test data sets took dozens of minutes from a db and took a few minutes from other databases.
table shows the faults detected by using the original data sources bit tag test data sets and random test data sets.
bit tag test data sets found all the faults detected by the original data sources.
we ran the etl jobs of p a and p b with the random test data sets and they did not find any faults because the random data did not satisfy the foreign key constraints.
table presents the execution time for running etl jobs of p a and p b using the original data sources the bit tag test data sets and random test data sets.
we can see running the etl jobs of p a and p b using the original data sources took a long time while using the bit tag test data sets took very little time.
in summary the bit tag test data sets found all the faults detected by the original data sources.
running the etl jobs in p a and p b using the bit tag test data sets only took a very small fraction of the time of using the original data sources.
bit tag is more efficient than using the original data source.
bit tag has the same effectiveness as using the original data source and is more effective than the random test data generation.
since bit tag is very effective in finding faults and efficient in generating and executing test sets for p a and p b we believe bit tag can be successfully used in agile development of big data applications.
the developers and stakeholders have given very positive feedback about bittag.
we are in a process of improving the usability of bit tag and guiding the developers to use bit tag.
we will continue using bit tag in future etl projects.
.
threats to validity as in most software engineering studies one external threat to validity is that the subject programs may not be representative.
we ameliorated this threat by selecting real world subjects from industry with a variety of sizes and kinds of databases.
another threat in the experiments could be that we translated the requirements into constraints.
other people may specify different constraints.
once testers fully understand requirements and product business rules they should come up with similar constraints even if they may diftable execution time original databases time test databases time a db 1d t a db 3m a db 3d t a db 3m a db 2w t a db 3m b db 3h t b db 30s 645fer sightly.
so this should not affect the experimental results much.
another external threat is that we used bit tag and acts to generate test data sets.
the faults in these two tools may affect the results.
as described in section we have applied best practices of java programming when developing bit tag.
acts has been used by many users for a long time.
one construct validity threat is that we used real faults and historical changes as faults.
using synthetic faults may generate different results.
however we believe using real faults and actual changes should better reflect the effectiveness of our approach.
.
lessons learned we summarize the following major lessons learned from our experience.
identification of constraints requires a good understanding about the requirements.
ideally all the important information should be clearly documented which is however not the case in practice.
in the course of this project we held many discussion sessions with subject experts to clarify our understanding.
we ran into situations in which an etl script didn t return any results because some requirements were not converted into constraints.
many constraints were discovered and or corrected during those discussion sessions as we obtained a better understanding about the requirements.
not all columns interact with other columns.
thus it is often not necessary to achieve uniform t way coverage among all the columns.
in our experiments we achieved pairwise coverage for columns that are involved in the same business rules and all combination coverage for columns that have many to many relationship.
for columns that are not involved in any business rule we covered them to achieve each choice coverage.
so has significantly reduced the size of the generated dataset without compromising test effectiveness.
when pc is specified for a logic constraint we generated half of the rows in a table that evaluate to true and the other half to false.
this seems redundant since pc only requires two rows one evaluating to true and the other to false.
however when tables are joined over with filtering i.e.
with a where clause this redundancy helps the remaining rows to still satisfy pc.
.
related work in this section we discuss previous research on database oriented test data generation and compare it to our work.
chay et al.
presented an approach that uses input space partition to generate test data for traditional database applications.
they use a manual approach to create idms where synthetic data is used.
in contrast our approach generates idms automatically and derives test values from the original data source.
olston et al.
presented an approach that generates example data sets for testing dataflow programs.
in their approach an example data set is first created by sampling the original database and then refined by propagation and pruning in multiple passes.
li et al.
improved the work in to deal with user defined functions.
specifically they derive symbolic constraints from dataflow operators and use concolic execution to solve those constraints.
in both approaches the user needs to provide equivalence classes for each dataflow operator.
this requires a detailed understanding about the semantics of each operator which may not be possible for general programs.
also both approaches require access to thesource code.
in contrast our work generates test data from the original data source and requirements and it does not require access to the source code.
li et al.
reported an approach that uses clustering to generate test data for data centric applications.
the main idea is to group similar records in a database together and compute a representative record for each group based on the notion of centroid object.
in order to preserve test coverage their approach performs static analysis to determine the weights of attributes in a database which is needed to compute the distance between different records.
as a result this work also requires access to the source code which is different from our work.
we note that many approaches generate large data sets to evaluate the performance of big data computing platforms .
these approaches are fundamentally different from test data generation approaches whose goal is to find faults that may exist in big data programs.
.
conclusions and future work in this paper we report our experience of applying a combinatorial testing approach to two real world etl applications.this approach automatically creates models i.e.
aidms from the original data source.
aidms extend traditional input domain models idms to include analytical results that capture important characteristics of the original data source and constraints.
based on the aidms our approach generates a data set to satisfy t way combinatorial coverage and other constraints.
when there is a change in the data source and constraints our approach updates the aidms by comparing the saved analytical data with the change and generates a new test data set.
our results show that the test data set generated by our approach detected the same faults as the original data source taking a fraction of the time executed by the original data sources.
this suggests that combinatorial testing can be effectively applied to big data applications.
in the future we plan to improve bit tag in the following directions with our own interests.
first we plan to optimize the performance of existing constraint handling and also include support for new types of constraints to better capture requirements.
second we will conduct experiments on more and even larger data sets and compare bit tag with other approaches described in section .
third we plan to use machine learning techniques to identify important test values from the original data source.
machine learning techniques could also be used to optimize aidm creation and test data generation by learning from previously generated data sets.
fourth we would like to refactor bit tag to support more data types such as spatial types.
fifth bit tag could be improved to reuse existing data sets to generate a new data set when constraints change.
sixth generating a minimal in terms of row counts test data set to satisfy all constraints is an interesting problem to study.
.