a language model for statements of software code yixiao yang yu jiang ming gu jiaguang sun jian gao han liu school of software tsinghua university tnlist kliss beijing china abstract building language models for source code enables a large set of improvements on traditional software engineering tasks.
one promising application is automatic code completion.
state of the art techniques capture code regularities at token level with lexical information.
such language models are more suitable for predicting short token sequences but become less effective with respect to long statement level predictions.
in this paper we have proposed pcc to optimize the tokenlevel based language modeling.
specifically pcc introduced an intermediate representation ir for source code which puts tokens into groups using lexeme and variable relative order.
in this way pcc is able to handle long token sequences i.e.
group sequences to suggest a complete statement with the precise synthesizer.
further more pcc employed a fuzzy matching technique which combined genetic and longest common subsequence algorithms to make the prediction more accurate.
we have implemented a code completion plugin for eclipse and evaluated it on open source java projects.
the results have demonstrated the potential of pcc in generating precise long statement level predictions.
in of the cases it can correctly suggest the complete statement with only six candidates and of the cases with ten candidates.
index terms code completion language model ir i. i ntroduction programs as natural languages are highly repetitive and predictable .
this observation opens a great opportunity to transfer well designed natural language processing nlp techniques to traditional software engineering tasks.
recent years have witnessed a class of researches on building language models for source code which enables a large set of promising applications.
in this paper we focus on automatic code completion i.e.
automatically generate a statement for code suggestion.
completing one token each time needs users to keep thinking and selecting tokens.
users need to perform many keyboard operations.
by comparison completing a statement is more user friendly.
previous works have used n gram language model to capture the regularity of source code .
the insight is to perform model training using grams i.e.
tokens of the source code.
the previous evaluations have shown the potential of n gram model in predicting short code i.e.
short token sequence.
however in terms of long predictions i.e.
long complete statement sequence they become less effective since they only consider token regularity within a specific bound i.e.
nas in n gram.
yu jiang is the corresponding author school of software tsinghua university email jy1989 mail.tsinghua.edu.cn our goal is to overcome such limitation by optimizing the long sequence prediction.
unfortunately to fulfill such a goal in practice is facing the following challenges.
challenge long sequence regularity.
the key of performing long complete statement predictions is to capture long sequence regularity.
in the n gram setting this requires increasing the value of n which may incur a large training overhead and make the model less predictive.
challenge large prediction space.
furthermore predicting long sequences leads to exploring on a large prediction space i.e.
all possible subsequent code.
the exploration often amounts to something like trying all feasible method calls of a given type which is quite expensive.
challenge complete statement synthesize.
finally synthesizing a complete statement leads to realtime compiling to check whether the suggested statement is legal or not which is quite time consumption and make the suggestion less effective.
we have proposed pcc in this paper to address the aforementioned challenges.
the first ingredient of pcc is an intermediate representation of source code.
the intuition of the ir is to group tokens together and capture regularity at group level.
in this way we can use groups to cover more tokens than techniques in the literature.
moreover pcc introduces a fuzzy matching algorithm to optimize the exploration on large prediction sequence.
specifically the contexts used to predict are not exactly same as the code contained in training data but the patterns may be similar.
through searching similar but not accurate matching contexts we could migrate potential patterns for synthesis.
then we develop a precise synthesizer to suggest the complete statement efficiently which adopts realtime compiling techniques to select context sensitive variables and recombines tokens into a legale complete statement.
for evaluation on open source java projects in of the cases it can correctly suggest the complete statement with only six candidates and of the cases with ten candidates and yield the precision improvement from .
to .
over the n gram approach.
the results demonstrated the potential of pcc in predicting precise statements.
ii.
r elated work the statistical n gram language model has been widely used in capturing patterns in source code.
hindle et al.
used n gram model on lexical tokens to suggest the next token.
in slamc they enhanced n gram by associating code tokens with roles data types and topics.
in cacheca they .
c ieeease urbana champaign il usa technical research new ideas682 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
improved n gram with caching for recently seen tokens in local files to improve next token suggestion accuracy.
allamanis et al.
and raychev et al.
captured common sequences of api calls with per object n grams to predict next call.
decision tree learning was applied to code suggestion based on which raychev et al.
presented an ast based hybrid language model for source code .
raychev et al.
abstracted the code into dsl and kept sampling and validating on a special kind of dsl until the good code suggestion was obtained.
nguyen et al.
tried to train code on graphs and combined naive bayes model and n gram model to suggest api usages.
recently deep learning techniques were applied to code suggestion .
they found that recurrent neural networks significantly outperform n grams for code suggestion.
given the amount of unstructured code available state of the art approaches such as recurrent neural networks can outperform existing code suggestion solutions.
previous works focused on suggesting short token sequence which often represents a method name or a field name.
our work makes a step forward to predict a long statement level sequence which represents a complete statement.
we work at the statement level rather than the lexical level or api level.
iii.
pcc a pproach the overall architecture of pcc is presented in figure .
first for both training or prediction the source code would be parsed into our predefined ir which is a sequence of tokens.
in previous works the token was defined by splitting the source code by white space or punctuation.
in the token of our ir there are many differences.
for example variable names are replaced by some symbols and lexical symbols such as or g are eliminated.
then the n gram trainer trains the ir into n gram model for statement prediction.
fig.
.
pcc architecture for predicting code the source code prior to the position where users invoke the code completion is taken as the context.
the context will be translated to ir.
in previous works using ngram model for prediction the last few tokens of ir of the context will be used to predict the next token and the information before the last few tokens is lost.
if a value never seen before is observed the whole probability may be invalid instantly making it hard for complete statement prediction.
for example in gram model p abcd p ajbc p bjcd p cjd p d if c is not observed in training data the whole probability can not be computed because the values of p ajbc p bjcd andp cjd are unknown.
a compromise approach to compute p abcd is that p abcd p ajb p b .
information of d is lost.
however the training data may containbed orbhd instead of bcd p abcd could be computed by p abcd p abed p ajbe p bjed p ejd p d orp abcd p abhd p ajbh p bjhd p hjd p d .
we design the context searcher to find substitutable contexts when some tokens in the origin context are not observed in training data as in the above example that bcdcould be substituted by bedorbhd.
the context searcher is a long term memory link to bond shattered token snippets.
with the context searcher ngram model could address the challenge of using long contexts to predict next tokens for statement completion.
finally when context searcher infers n tokens v1tovn from context h for each vi2f1 ng synthesizer judges whether vicould form a statement without compilation errors.
if so synthesizer pushes the generated statement onto the result set if not synthesizer replaces the context hwithvih searches from vihfor each next token zj2f1 mg and judges whether zjvicould form a statement or not.
then synthesizer replaces the context vihwithzjvihand does similar searches and a threshold is set to limit the traversal steps.
a. intermediate representation intermediate representation ir is generated by traversing the ast of source code in post order.
our ir generation algorithm is applied to the ast generated by eclipse jdt .
a simple example is presented in figure .
in eclipse jdt ast each node has a type which corresponds to the syntactic information of the node in the ast.
there are over types for nodes in eclipse jdt ast such as v ariable whilestatement methodinvocation .
we implement the getnodeidentifier node function to return the type of the node except for three types of nodes v ariable constant andmethodinvocation.
for type methodinvocation the function should return the name of the invoked method.
for typeconstant the function should return the concrete value of that constant.
for type v ariable the function should return the symbol for the variable.
fig.
.
an example of translating java source code to ir although we can select the suitable variable by type checking and compiling during the code completion stage we cannot directly give every variable name an unified symbol.
consider the following case.
if we decide to choose a variable of a specific type but there are more than one variable of that type in the context we can not decide which variable authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
to choose.
we observe that if a variable is recently declared there is a high probability that the variable will be used in the following code.
in another word whether this variable is recently declared could help us classify and choose variables.
based on the observation when translating a variable var to ir at each position the variable is used we give that variable a number which shows how recently the variable is declared compared to other variables with their types declared as same as the type of var.
the more recently the variable is declared the smaller the number is.
the minimum of that number is .
we give every variable symbol a prefix cto denote that this is a symbol for a variable.
figure shows the example of variable translations.
fig.
.
example of variable translation algorithm generates the ir token using post order traversal.
if we are generating the token token for a node node we initialize token to the string gotten by invoking getnodeidentifier .
we iterate child nodes of node from left to right.
if the encountered child node represents a variable or a constant append the separator and the ir token of that child to the end of token.
if the encountered child node does not represent a constant or a variable we think the node is complex and the token of that complex node should be stand alone without being grouped into any other tokens.
so we append the separator and the string pmk to the end oftoken.
pmk can be thought as a placeholder for the token of that complex ast node.
remember that we use postorder traversal to traverse the ast the tokens of child nodes are generated before the tokens of parent nodes.
therefore pmk must refer to a previously generated token.
the rules about how to identify the token to which pmk refers are as follows.
for convenience if a token contains pmk we name the token token with ref.
at the beginning we mark all tokens before token with refasun referred.
we iterate each pmk intoken with ref from right to left.
for each encountered pmk we find the nearest previous un referred token and mark the found token as referred.
the found token is the one to which the encountered pmk refers.
then the token with refand all referred tokens must be taken as one token conceptually.
b. model training we only make intra procedural analysis so each method in java files will be parsed into ir which is a token sequence.
the source code in java files in the training data will be parsed into many token sequences where each token sequence corresponds to a java method.
all token sequences will be trained into n gram model.
the model then serves as a serveralgorithm generateirtoken node input node output token token getnodeidentifier node ifisv ariable node jjisconstant node then return token end if children getchildrenfromlefttoright node forchild children do ifisv ariable child jjisconstant child then token token generateirtoken child else token token pmk00 end if end for return token to provide the service of searching inferring next tokens and the corresponding conditional probabilities given the token sequence with the length not exceeding n .
the length of a token sequence means the number of tokens in the sequence.
c. context searcher the source code prior to the position waiting to be code completed is taken as the context.
in order to obtain the ability to handle unseen tokens context searcher is to find similar contexts according to given context.
algorithm shows the details.
in algorithm for each token in the context we search for all token sequences which start with that token.
at last we could get a large number of token sequences possible contexts .
then we use the function sortandminimizecontexts to take similarities probabilities and lengths of token sequences into a comprehensive consideration to help select suitable token sequences contexts .
algorithm searchforcontexts given context input given context must be a token list output contexts contexts fortoken given context do first token token listset f g depth max depth given context length while depth max depth do new listset foronelist listset do next token set infernexttokens one list fornext token next token set do new list onelist next token contexts contexts new list new listset new listset new list end for end for listset new listset depth end while end for contexts sortandminimizecontexts contexts return contexts authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
function sortandminimizecontexts uses the longest common subsequence algorithm lcs to compute the length of common subsequences between the inferred contexts and the original context.
the longer the common subsequences the higher the priority.
if the priorities are same for two inferred contexts the context with smaller length wins.
if two contexts still cannot tell the difference using two metrics above the context with the higher probability in n gram model wins.
top ranked sequences will be retained.
fig.
.
example of searching contexts figure gives an intuitive graphical representation about this algorithm.
for simplicity complex ir tokens are replaced by english alphabets.
assume that the ir of the context are a c f g and k. in figure the algorithm starts at a keeps inferring next tokens of a to form multiple token sequences which is a path from the root to the leaves.
then we calculate the lcs between the generated sequence and the original context a c f g k .
the common subsequences has been marked orange in figure .
d. synthesizer every context found by context searcher is fed into a synthersizer .
given a context the synthesizer uses the algorithm to predict all possible token sequences and integrate those token sequences into statements.
algorithm is based on width first search.
the operator means concatenation.
if a statement could be generated from a sequence successfully the statement would be appended to final results and we stop exploring that sequence instantly.
if not we must keep predicting next tokens from that sequence to form new sequences to generate statements.
the threshold maxtry is set to avoid space explosions.
the key component in algorithm is generatestatement which is responsible for generating statements from a token sequence.
it is implemented in algorithm .
the whole mechanism is similar to the mechanism jvm uses to execute its byte code.
we use a stack to store the generated code.
we iterate each token and push the generated code onto stack.
a token contains the information of the corresponding ast node and the child nodes of that node.
these modules are separated by the separator .
for a token the function getinfofromrighttoleft extracts the modules which are separated by from right to left.
the uncertain modules are the symbol of variables and the placeholder pmk .
remember that the pmk in a token refers to the unreferred nearest previous token.
the code of previous tokensalgorithm synthesize context input context output statements statements max try try context set femptysequenceg while try max try do new set forone context set do next tokens infernexttokens context one fortoken next tokens do new one one token result generatestatement new one ifresult null then new set new set new one else appendresult result end if try end for end for context set new set end while return statements had been pushed onto stack one by one.
therefore the pmk refers to the top of the stack.
for each encountered pmk we pop the top of stack and replace pmk with the popped content.
function selectv ariables handles symbols of variables such as c0 c1 or c2.
we take c0as an example to explain the mechanism.
remember that c0represents the most recently declared variable for a type.
but in ir the symbol c0 contains no information about which type this symbol represents so for each type in the context the most recently declared variable should be selected.
there would be multiple variables selected at the same time.
after determining all modules the function combinetogether undertakes the tasks of integrating modules together into java code checking and compiling the generated code.
algorithm generatestatement material input material output statement stack new stack string fortoken material do executed parts new linkedlist string forpart getinfofromrighttoleft token do ifisv arablesymbol part then executed parts add selectv ariables part else if ispmk part then executed parts add stack pop else executed parts add part end if end for part stmt combinetogether executed parts stack push part stmt end for return stack pop authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
function combinetogether integrates modules of a token together according to the syntactic information of that token.
remember that the first module of a token is the syntactic information gotten by invoking getnodeidentifier .
that syntactic information decides how to generate the java code.
for example for the token n n the syntactic information shows that this token should be an infix expression and the operator is so the generated code is n n. for the token n n m1 the generated code could be m1 n n orn m1 n .
there are over kinds of the syntactic information and each kind corresponds to its own implementation about how to integrate modules together.
function combinetogether also contains a type checking system implemented by ourselves.
there are three kinds of checking.
the first kind is to check whether the variable is consistent with its involved arithmetic operator.
for example given an ir token c variable c0 must be of type int float double long or short.
the second kind is to check method specifications.
for example given an ir token substring c0 c1 our system checks whether substring can be found in local context.
if substring is a method declared in local file and c0 c1 are consistent with parameter types declared by method substring the check is passed and the form of the generated statement will be substring c0 c1 .
if the check is not passed our system takes substring as the member function of c0 and the form of the generated statement will be c0 substring c1 .
then our system checks c0 must be the variable that contains a method named substring c1 must be consistent with the corresponding parameter type.
the third kind of checking is to check whether the variable can be casted to the specific type.
our type checking system cannot ensure the completeness and soundness.
in order to ensure that there are no compilation errors in final results we furtively append the statements we generated to the tail of the java file being code completed if the whole java file can be compiled without errors the statement will be appended to final results.
when generating the code for variable declaration such as type t new combinetogether specifically checks whether the declared type is consistent with the type of the right operand of the assignment operator.
if not combinetogether will replace the declared type with the actual type of the right operand.
iv.
i mplementation the implementation consists of lines of codes.
we implement our system as an eclipse plug in named pcc which does not influence the original functionality of code completion in eclipse.
we train gram model and store the model in aerospike distributed database.
the antlr4 library is used to parse the special ir.
to use the plugin users just need to press the hot key to invoke the code completion.
figure shows the screenshots of the running eclipse with pcc installed.
the proposals prefixed by the apple icon are generated by pcc.
the lower the position the higher the priority.
the tool pcc is public on github.
fig.
.
screenshot of code completing v. e xperiment results experimental setup.
we have conducted the experiments to evaluate the accuracy of code completion at statement level.
we pick the n gram based technique as the comparison baseline.
all the experiments were performed on a laptop with intel i7 .4ghz processor and 16gb memory.
we have collected open source java projects from github which contain java files and lines of code in total.
the data is available at containing both training data files and testing data files .
evaluation process.
the code completion is performed at statement level.
specifically we have commented out each statement in test files and to check whether pcc and n gram model could accurately complete the statement.
an accurate completion must match the original code exactly.
for pcc we invoke the code completion engine described in the last section to complete a statement.
for n gram we iteratively predict tokens and record all the possibilities.
then we use the predicted tokens to form a set of sequences and check whether any one of them can match the original code.
results.
the results are public at and shown in table i. in table i stmt is the number of statements in the test file.topk means the completion matches the right code in first k predictions.
since n gram model is strongly dependent on plain text if all variable names in local context are unseen in training data n gram model cannot predict the right variable.
that is why n gram model gets accuracy in cookiecounter java .
consider top1 accuracy in cases pcc improves the accuracy form to .
the average improvement is .
.
in the remaining case deleteframe pcc gets lower accuracy than n gram model.
the reason is that the context and the predicted statement exist in training data.
if the context and the predicted statement are both exactly matched in training data the n gram model performs well.
meanwhile pcc predicts many possible statements including the expected statement but the priorities of those statements are slightly wrong.
this minor defect in priorities of statements could be eliminated when considering topn accuracy where authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i accuracy of code completion test file stmttop1 top3 top6 top10 n gram pcc n gram pcc n gram pcc n gram pcc arrayminvalue .
.
.
.
.
.
.
.
ballpanel .
.
.
.
.
.
.
.
classinfo .
.
.
.
.
.
cookiecounter .
.
.
.
deleteframe .
.
.
.
.
.
.
.
deleteutil .
.
.
.
.
.
.
.
drawsquareframe foo .
.
.
.
.
.
.
.
fullscreenframe .
.
.
.
.
.
.
.
jdbcconncommit .
.
.
.
.
.
.
nis greater than .
consider top3 accuracy in all cases pcc improves the accuracy form .
to .
.
the average improvement is .
.
consider top6 accuracy in all cases pcc improves the accuracy form .
to .
.
the average improvement is .
.
consider top10 accuracy in all cases pcc improves the accuracy form to .
.
the average improvement is .
.
the huge search space the handling of variable names and the elimination of trivial tokens contribute to the improvement.
for example if we want to predict code based on context a b the token sequence of our ir for this code is b a pmk which contains only tokens.
however the token sequence in n gram model for this code is a b which contains tokens.
if we use last two tokens to predict code in n gram model we could only use two tokens and which make little sense.
the two tokens b and a pmk contain more information than two tokens and comparatively.
our context searcher also contributes to the accuracy improvement through finding patterns from other code.
for example if we predict code from unseen unsees every word in the context is unseen before.
the context searcher finds the similar context file files and its following statement for file file files .
the synthesizer generates the right statement for unseen unsee unsees by selecting and generating suitable variables.
some statements in our experiments are generated in this way.
although our technology has done a lot of searching the speed of our system is fast the time consumed to show up the proposals are less than second in all positions waiting to be code completed.
vi.
t hreats tovalidity the right statements completed in our experiment are mainly jdk apis servlet apis common if judgements and for while loops.
for other data sets such as programs full of user defined functions the result might be different.
our solution to the problem of variable selection does not take the semantics or the topics of variables into consideration and this may produce false positive code which is compilable but semantically wrong.
our approach is based on the linear model with short term memory.
for programs full of branches and complex code relations our approach is hard to discover the co relationships of those programs.vii.
c onclusions in this paper we proposed pcc to optimize the tokenlevel based language modeling for the statement level code completion.
we introduced an intermediate representation ir for source code to handle long token sequences to suggest a complete statement with the precise synthesizer.
we also provide a simple solution to problems of variable selection.
in the future we will adopt the graph based model or deep learning to further improve the accuracy of variable selection.
program slicing techniques will also be adopted to analyze long contexts.