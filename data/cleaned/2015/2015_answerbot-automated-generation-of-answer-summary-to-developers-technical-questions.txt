answerbot automated generation of answer summary to developers technical questions bowen xu zhenchang xing xin xia david lo college of computer science and technology zhejiang university china school of engineering and computer science australian national university australia department of computer science university of british columbia canada school of information systems singapore management university singapore bowenxu.
phdis.smu.edu.sg zhenchang.xing anu.edu.au xxia02 cs.ubc.ca davidlo smu.edu.sg abstract the prevalence of questions and answers on domainspecific q a sites like stack overflow constitutes a core knowledge asset for software engineering domain.
although searchengines can return a list of questions relevant to a user query of some technical question the abundance of relevant posts and the sheer amount of information in them makes it difficult fordevelopers to digest them and find the most needed answers to their questions.
in this work we aim to help developers who want to quickly capture the key points of several answer postsrelevant to a technical question before they read the details of the posts.
we formulate our task as a query focused multianswer posts summarization task for a given technical question.
our proposed approach answerbot contains three main steps relevant question retrieval useful answer paragraph selection diverse answer summary generation.
to evaluate our approach we build a repository of java questions and their corresponding answers from stack overflow.
we conductuser studies with randomly selected java questions not in the question repository to evaluate the quality of the answer summaries generated by our approach and the effectiveness ofits relevant question retrieval and answer paragraph selection components.
the user study results demonstrate that answer summaries generated by our approach are relevant useful anddiverse moreover the two components are able to effectively retrieve relevant questions and select salient answer paragraphs for summarization.
index t erms summary generation question retrieval i. i ntroduction answers on stack overflow have become an important body of knowledge for solving developers technical questions.
typically developers formulate their questions as a query tosome search engine and the search engine returns a list ofrelevant posts that may contain answers.
then developers need to read the returned posts and digest the informationin them to find the answers to their questions.
information seeking is rendered difficult by the sheer amount of questionsand answers available on the q a site.
we survey developers in two it companies i.e.
hengtian and insigma global service with two questions whether you need a technique to provide direct answers when you post a question query online and why?
and what is your expectation of the automatically generated answers e.g.
must the answers be accurate?
all the developers agree corresponding author.that they need some automated technique to provide directanswers to a question query posted online.
the reasons theygive include sometimes it is hard to describe the problemthey meet so some hints would be useful there is toomuch noisy and redundant information online the answersin long posts are hard to find and even the answer theyfound may cover only one aspect of the problem.
developersexpect the answer generation tool to provide a succinct anddiverse summary of potential answers which can help themunderstand the problem and refine the queries questions.
our survey reveals a great need to provide improved techniques for information retrieval and exploration.
in this work we aim to develop an automated technique for generatinganswer summary to developers technical questions insteadof merely returning answer ports containing answers.
many developers technical questions are non factoid questions for example what are differences between hashtable and hashmap?
how do i write logs and display them realtime in java swing?
for such non factoid technical questions multiple sparse and diverse sentences may make up the answersummary together.
we formulate our task as a query focused multi answerposts summarization task for a given input question.
this task is closely related to question answering task which aims to find information from a huge text baseto answer a question.
an answer summary from the text baseshould provide related information with respect to the queryquestion.
however the answer sentences and the query ques tion are highly asymmetric on the information they convey.they may not share lexical units.
instead they may only besemantically related see examples in table i and table ii .the inherent lexical gap between the answer sentences andthe questions imposes a major challenge for the non factoidquestion answering task.
to tackle this challenge we develop a three stage framework to achieve the goal of generating an answer summaryfor a non factoid technical question.
in the first stage weretrieve a set of relevant questions based on the question titles relevance to a query question.
the question titles and the query question are parallel text whose relevance is easier todetermine.
however the question titles and the query questionoften still have lexical gaps.
inspired by the recent success of .
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research706 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
word embeddings in handling document lexical gaps we learn word embeddings from a large corpus of stack overflowposts to encode the question titles and the query question andmeasure their relevance.
in the second stage we collect all the answers of the relevant questions retrieved in the first stage.
we extract answer paragraphs from the collected answers.
we develop a multi criteria ranking method to select a small subset of relevant andsalient answer paragraphs for summarization based on query related user oriented and paragraph content features.
in thethird stage given a set of answer paragraphs to be summarized the generated answer summary needs to cover as much diverseinformation as possible.
to generate a diverse summary wemeasure novelty and diversity between the selected answerparagraphs by maximal marginal relevance mmr .
we build a question repository of java questions and their corresponding answers from stack overflow.
werandomly select another java questions as query questionsand use our approach to generate an answer summary foreach query question.
our user studies confirm the relevance usefulness and diversity of the generated summary and theeffectiveness of our approach s relevant question retrieval andanswer paragraphs selection components.
our error analysisidentifies four main challenges in generating high quality an swer summary vague queries lexical gap between query andquestion description missing long code snippet answers and erroneous answer paragraph splitting which reveal the future enhancements of our automated answer generation technique.
the main contributions of this paper are the following we conduct a formative study to assess the necessity ofautomated question answering techniques to provide answersummary to developers technical questions.
we formulate the problem of automated question answeringas a query focused multi answer posts summarization task for an input technical question.
we propose a three stage framework to solve the task i.e.
relevant question retrieval answer paragraphs selection answer summary generation.
we conduct user studies to evaluate the effectiveness of ourapproach and its components and identify several areas for future improvements.
paper organization.
section ii presents our formative study for automated question answering.
section iii describes our approach.
section iv reports our experimental methods andresults.
section v analyzes the strengths and improvementsof our approach and discusses threats to validity of ourexperiments.
section vi reviews related work.
section viiconcludes our work and presents future plan.
ii.
f ormative study of answer summary we contacted developers by emails in two it companies.
we received replies which help us understandthe developers difficulties in the current information retrieval ir practice and assess the necessity of automated questionanswering techniques.
some comments we received are listedas follows google will return a number of relevant links for a query and i have to click into these links and read a number of paragraphs ... it is really time consuming ... some links even contain viruses.
a tool which generates potential answers can save my time wasted on reading a lot of irrelevant content.i fthe generated answer accurately solves my question it is good.
but i think it would be difficult.
anyway i believe it is no harm to use an answering tool at least i can get some hints to solve my problem.
sometimes i cannot accurately describe my questions which made it hard to find the answer .
i have to browse a number ofposts online to learn how to refine my query and search again.thus i expect that the answer generation tool can help me understand the information space better so i can refine myquery faster without browsing those noisy posts.
i notice even the best answers in stack overflow often answer the questions only in one aspect.
sometimes i need to know a diversity of aspects to understand the problem better but they cannot be found in a single best answer .
thus i expect the tool should provide a diversity of potential answers even if some answers are not accurate.
... some questions received too many long answers and many of these answers have redundant content.
i expect the answer generation tool should return succinct answers which covers many aspects of potential solutions.
so i could have a highlevel understanding of the question i posted.
even if the accuracy of the tool is only i will still use it.in the worst case i will use your tool first and then search on google again to find the solutions.
we use a real world example to illustrate the difficulties mentioned in the developers replies and the desirable properties of automated answer generation tool.
assume adeveloper was interested in the differences between hashmap and hashtable in java.
he used google to search for stack overflow posts and table i lists the top ranked stackoverflow questions returned by google.
the question titlesare very relevant to the developer s information need and heshould be able to find the needed information in the answersto these questions.
however information overload can be detrimental to the developer.
there are answers which have words in total.
reading all these answers may take 30minutes based on the average readers reading speed of 200words per minute .
even just reading the best answers i.e.
the accepted answers or top voted answers may still takesome time.
it would be desirable to have a answer summary extracted from the answers to the top ranked questions as the oneshown in table ii.
this answer summary helps the developerquickly capture the key points of the answers relevant to histechnical question.
these points reveal the salient and diversedifferences between hashmap and hashtable.
they may help the developer decides which api is more appropriate for histask or provide information scents for guiding the developer performing further search or learning .
however manually generating this answer summary is not an easy task.
first there is much low quality and irrelevantinformation .
table iii shows two examples eight answersin total .
the first example discusses hashmap and hashtable in c. the second example discusses how to answer hashmap and hashtable related interview questions.
these answers are valid in a particular questions context but have nothing to dowith the developer s technical question.
another issue is information redundancy.
as shown in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i the top 5ranked stack overflow questions by google search engine for the query differences hashmaphashtable java no.
question id title answers words differences between hashmap and hashtable?
difference between hashtable and collections.synchronizedmap hashmap what are the differences between hashtable and hashmap?
not specific to java difference between hashmap and hashtable purely in data structures what are the differences between hashmap vs hashtable in theory?
table ii desirable answer summary with relev ant salient and diverse information no.
answer id content aspect hashmap is non synchronized whereas hashtable is synchronized.
synchronization thread safety hashmap can store one key as null.
hashtable can t store null.
null keys null values 22491742second important difference between hashtable and hashmap is performance since hashmap is not synchronized it perform better than hashtable.performance hashtable is a legacy class in the jdk that shouldn t be used anymore.
evolution history 20519518maps allows you to iterate and retrieve keys values and both key value pairs as well where hashtable don t have all this capability.iteration table iii examples of irrelev ant and low quality answer paragraphs no.
type answer id example irrelevant 42003464the explaination between hashmap and hashtable is quite correct as it also fits to the header of a string hash map implementated in strmap.
c where thestringmap is a hashtable for strings satisfying the properties of a key value structure.
here it says ...code... low quality 36325577the interviewer may have been looking for the insight that.
a hash table is a lower level concept that doesn t imply or necessarily support any distinction or separation of keys and values...even if in some implementations they re always stored side by side inmemory e.g.
members of the same structure std pair ... table iv redundant answer paragraphs aspect set of redundant answers id example hashmap is non synchronized whereas hashtable is synchronized.
synchronization thread safety17815037 hashtable is synchronized whereas hashmap isn t. that makes hashtable slower than hashmap.
hashtable is internally synchronized.
whereas hashmap is not internally synchronized.
hashmap can store one key as null.
hashtable can t store null.
null keys null values39785829 hashtable does not allow null keys or values.
hashmap allow sonenull key and any number of null values.
hashtable can only contain non null object as a key or as a value.hashmap can contain one null key and null values.
for threaded apps you can often get away with concurrenthashmapdepends on your performance requirements.
performance second important difference between hashtable and hashmap is performance since hashmap is not synchronized it perform better than hashtable.
as hashmap is not synchronized it is faster as compared to hashtable.
hashtable is a legacy class in the jdk that shouldn t be used anymore.
evolution history hashmap extends abstractmap class where as hashtable extendsdictionary class which is the legacy class in java.
second difference is hashmap extends map interface and whetherhashset dictionary interface.
iterating the values hashmap object values are iterated by using iterator.
hashtable is the only class other than vector which uses enumerator to iterate the values of hashtable object.
iteration maps allows you to iterate and retrieve keys values and both key value pairs as well where hashtable don t have all this capability.
iterator in hashmap is fail fast.
enumerator in hashtable is not fail fast.
table iv the same aspect may be mentioned in many answer posts.
the information redundancy and diversity creates adilemma for the developer.
if he reads every post he is likelyto come across the same information again and again whichis a waste of time.
if he skips some posts he risks missingsome important aspects he has not seen.
reading only the bestanswers can address the information overload issue but not theinformation redundancy and diversity.
for example the bestanswer 1to the 1st ranked question in table i discusses only three aspects synchronization or thread safety null keys and null v alues and iteration listed in table ii.
to tackle the information relevance redundancy and diversity issuesfor finding answers to developers technical questions we needan effective technique to generate an answer summary withrelevant salient and diverse information from unstructured textof answer posts.
iii.
p roposed approach as shown in figure our approach called answerbot takes as input a software engineering related technical ques tion as a query from the user and produces as output an answer summary for the question.
next we describe the threecomponents of our approach i.e.
relevant question retrieval authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
useful answer paragraphs selection and diverse answer summary generation.
a. relevant question retrieval the relevant question retrieval component takes a technical question as an input query qand ranks all questions qin a large question repository e.g.
questions from q a sites like stack overflow .
the questions that are ranked at the topare more likely to have answers that can answer the inputtechnical question.
we combine word embedding techniqueand traditional idf metric to measure the relevance betweenthe input query and the questions in the repository.
wordembedding has been shown to be robust in measuring textrelevance in the presence of lexical gap .
idf metric helpsto measure the importance of a word in the corpus.
to train the word embedding model and compute the word idf metrics we build a domain specific text corpus using thequestion title and body of stack overflow questions.
eachquestion is considered as a document in the corpus.
as the text is from the website we follow the text cleaning steps commonly used for preprocessing web content .
we pre serve textual content but remove html tags.
we remove longcode snippets enclosed in html tag angbracketleftpre angbracketright but not short code fragments in angbracketleftcode angbracketright in natural language paragraphs.
we use software specific tokenizer to tokenize the sentence.
thistokenizer can preserve the integrity of code like tokens and thesentence structure.
we use gensim a python implementationof the word2vec model to learn the word embeddingmodel on this domain specific text corpus.
to compute theword idf metrics we build a vocabulary from the text corpusby removing stop words based on the list of stop words forenglish text 2and using a popular stemming tool to reduce each word to its root form.
we then compute the idf metricof each word in the vocabulary over the text corpus.
given a query qand the title of a question qin the repository our relevance calculation algorithm computes their relevance based on an idf weighted word embedding similar ity between the query and the question title.
we use questiontitle in relevance calculation because query and question titleare parallel text .
the query and the question title are transformed into a bag of words respectively following thesame text preprocessing steps described above.
let w qbe the bag of words for the query qandwqbe the bag of words for the title of the question q. an asymmetric relevance rel wq wq is computed as rel wq wq summationtext wq wqrel wq wq id f wq summationtext wq wqid f wq where id f wq is the idf metric of the word wq rel wq wq ismax wq wqrel wq wq and rel wq wq is the cosine similarity of the two word embeddings wqandwq.
intuitively the word embedding similarity of a more important word in thequery and the words in the question title carries more weighttowards the relevance measurement between the query and question title.
an asymmetric relevance rel wq wq is computed in the same way.
then the symmetric relevancebetween the query qand the question qis the average of the two asymmetric relevance between w qandwq i.e.
rel q q rel wq wq rel wq wq .
based on the symmetric relevance between the query and each question in the repository the questions in the repositoryare ranked and the top nranked questions are returned as the relevant questions for the query.
b. useful answer paragraphs selection given a ranked list of relevant questions all the answer paragraphs split by html tag angbracketleftp angbracketright in the answers to these questions are collected.
we decide to use the granularity of answer paragraphs because they are the logical text units thatanswerers create when writing the posts.
to select relevant andsalient answer paragraphs for summarization our approachranks answer paragraphs based on three kinds of features i.e.
query related features paragraph content features and useroriented features.
query related features measure the relevance between an answer paragraph and the query.
relevance to query.
as the query and the answer paragraphs usually have lexical gap between the information theyconvey it is hard to directly measure their relevance.
inthis work we set the relevance between a query and ananswer paragraph as the relevance between the query andthe question from which the answer paragraph is extracted.
the underlying intuition is that the more relevant thequestion is to the query the more likely the answers tothe question contain relevant answer paragraphs.
entity overlap .
if an answer paragraph contains softwarespecific entities mentioned in the query it is very likelythat the paragraph is relevant to the query.
for exam ple all desirable answer paragraphs in table ii con tain hashmap and or hashtable mentioned in the query.
software specific entities can be programming languages libraries frameworks apis data format and domain specific concepts .
in this work we consider tags andtag synonyms on stack overflow as entities.
we identifyentity mentions in a query or answer paragraph by matchingwords in the query or answer paragraph with tag namesand tag synonyms.
let e qandeapbe the set of entities mentioned in the query and the answer paragraph respec tively.
the entity overlap between the query and the answerparagraph is computed as e q intersectiontexteap eq eq negationslash .
if the query does not mention any entities eq we set entity overlap at .
paragraph content features measure the salience of an answer paragraph s content.
information entropy.
salient answer paragraphs would con tain high entropy words.
a word with higher idf metricindicates that the word is less common in the corpus i.e.
3the relevance between the query and question is calculated during the relevant question retrieval process see section iii a. authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
fig.
.
the framework of our approach answerbot table v semantic patterns forsalient information no.
pattern no.
pattern please check xx in short xx pls check xx the most important is xx you should xx i d recommend xx you can try xx in summary xx you could try xx keep in mind that xx check out xx i suggest that xx higher entropy .
thus we sum the idf metrics of words after removing stop words and stemming in a paragraph to represent the paragraph s entropy.
using this feature manyparagraphs with low information entropy e.g.
i cannotagree more.
will be filtered out.
semantic patterns.
we observe that there are certain sentence patterns that often indicate recommendation or sum marization of salient information in stack overflow discus sions see table v for examples .
for example a question on stack overflow asks array or list in java.
which isfaster?
.
the best answer to this question is i suggest thatyou use a profiler to test which is faster .
.
in this work we summarize sentence patterns based on our empiricalobservations of randomly selected best answers onstack overflow.
if an answer paragraph contains at leastone of the sentence patterns we set the paragraph s patternvalue at otherwise .
format patterns.
we observe that html tags are often used to emphasize salient information in the discussions.for example angbracketleftstrong angbracketrighthighlights some text by bold font and angbracketleftstrike angbracketright points out some incorrect information.
if an answer paragraph contains such html tags we set its format pattern score at otherwise .
user oriented features select summary and high quality answer paragraphs based on user behavior patterns.
paragraph position.
we observe that when answerers writeanswer posts they usually start with some summary infor mation and then go into details.
for example a questionasks how do i compare strings in java?
the first threeparagraphs of the best answer of this question present for reference equality .equals for value equality and objects.equals checks for nulls .
therefore we set aparagraph s summary value to be inversely proportionalto the paragraph s position in the post for the first m paragraphs i.e.
summary pos pos m m in our current implementation .
the summary values of the subsequent beyond the m th paragraphs are set at .
v ote on answer.
answers with higher vote indicate that the community believes that they contain high quality informa tion to answer the corresponding question.
in this work we set an answer paragraph s vote as the vote on the answerpost from which the paragraph is extracted.
based on the above seven features an overall score is computed for each answer paragraph by multiplying the normalized value of each feature.
to avoid the feature scoresbeing all the feature scores are normalized to byadding a smooth factor .
.
answer paragraphs are ranked by their overall scores and the top mranked answer paragraphs are selected as candidate answer paragraphs forsummarization.
c. diverse answer summary generation as shown in table iv there are often many redundant answer paragraphs from the answers to relevant questions.
the generated answer summary should avoid such redundantinformation.
given a list of candidate answer paragraphs max imal marginal relevance mmr algorithm is used to select a subset of answer paragraphs in order to maximize novelty and diversity between the selected answer paragraphs .
mmrfirst builds a similarity matrix between each pair of candidateanswer paragraphs.
the similarity is computed as the symmet ric relevance between the two answer paragraphs as describedin section iii a. it then iteratively selects kcandidate answer paragraphs with maximal marginal relevance.
the selected answer paragraphs form an answer summary to the user stechnical question.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iv .
e xperiments r esults we conduct three user studies to answer the following three research questions respectively rq1 how effective is our approach in generating answer summaries with relevance useful and diverse information for developers technical questions?
rq2 how effective is our approach s relevant question retrieval component?
rq3 how effective is our approach s answer paragraph selection component?
in this section we first describe our repository of questions and answers and tool implementation.
we then describe our experimental query questions and how we select participantsand allocate tasks in our user studies.
finally we elaboratethe motivation approach and results for the three researchquestions.
a. question repository and tool implementation we collect java questions i.e.
questions tagged with java and their corresponding answers from stack overflow data dump of march .
these questions have atleast one answer.
to ensure the quality of question repository we require that at least one of the answers of the selectedquestions is the accepted answer or has vote .
when collecting questions we avoid duplicate questions of thealready selected questions because duplicate questions discussthe same question in different ways and can be answeredby the same answer.
we use these java questions and theiranswers as a repository for answering java related technicalquestions.
we build a text corpus using the title and body ofthese java questions to train the word embedding model andbuild the word idf vocabulary.
considering the conciseness ofthe generated answer summary and the fact that searchers tend to browse only the top ranked search results our currentimplementation returns top relevant questions for a query and selects top candidate answer paragraphs for summarization.the generated answer summary contains answer paragraphs.
b. experimental queries we randomly select another questions 4and use the titles of these questions as query questions.
we ensure that our question repository does not contain these query questionsand their duplicate questions.
the randomly selected query questions cover a diversity of aspects of java programming.for example some of them are related to language features such as multi threading e.g.
how does volatile actually work?
and i o e.g.
can bufferedreader read bytes?
while others are related to many third party libraries such as test assertions e.g.
testing api which returns multiple values with junit and rest e.g.
is there an equivalent to asp .net web api in java world?
.
some of the query questions areeasy to answer e.g.
how to convert string into dateformat in java?
while others are difficult e.g.
how does volatile actually work?
.
the diversity of these questions can 4see our replication package at vi task allocation to participants rqs group p1 d1 d2 d3 group p2 d4 d5 d6 rq1 q1 q50 q51 q100 rq2 q51 q100 q1 q50 rq3 q51 q100 q1 q50 improve the generality of our study and reduce the bias thatour approach might be only effective for a specific type ofquestions.
we index these questions as q1 to q100.
c. participant selection and task allocation we recruited participants through our school s mailing lists and select postdoctoral fellows p1 and p2 and phd students d1 to d6 to join our user study.
all the selectedparticipants have industrial experience on java development and they have used java to develop commercial projects intheir work before they went to graduate school.
the years oftheir working experience on java are vary from to years with an average .
years.
the diversity of these participants working experience on java can improve the generality of our results.
in practice our tool aims to help all levels ofdevelopers from novice to senior developers.
during our userstudy no participants report being unable to understand thequery questions and answers.
we divided the eight participants into two groups i.e.
p1 d1 d2 and d3 in group1 and p2 d4 d5 and d6in group2.
furthermore we divided the questions intotwo tasks i.e.
q1 q50 in task1 and q51 q100 in task2.table vi present the task allocation to the two participantgroups.
for rq1 group1 worked on task1 questions whilegroup2 worked on task2 questions.
for rq2 and rq3 group1 worked on task2 questions while group2 worked ontask1 questions.
all four participants in these two groups wererequired to review the answers of the assigned questionsindependently.
with this task allocation we have all queryquestions evaluated for the three research questions.
as rq1evaluates the overall performance of our approach and rq2and rq3 evaluates its components using the same set ofquery questions to evaluate rq1 and rq2 rq3 may bias theparticipants results.
however since rq2 and rq3 evaluatesthe two components independently and the two componentsdeal with completely different input output using the samequestions would have little impact on the participants results.we asked the participants to complete the study in three hoursessions the first session evaluates rq1 while the second andthird evaluate rq2 and rq3 respectively.
d. research questions rq1 effectiveness of the overall approach and the relevance usefulness and diversity of answer summary motivation.
in the current ir practice developers retrieve relevant questions by entering their technical questions to a search engine.
then they have to manually browse the answersof the returned relevant questions to find the needed infor mation.
in contrast our approach can automatically generatean answer summary of the key points in the answers of thereturned relevant questions.
we would like to investigate the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
overall performance of our approach in terms of the relevance usefulness and diversity of the generated summary comparedwith manual information seeking.approach.
we compare our approach against two baselines built based on google search engine and stack overflow search engine respectively.
we add site stackoverflow.com to the query of google search engine so that it searches onlyposts on stack overflow.
for a query question we use the firstranked stack overflow question returned by a search engineas the most relevant question.
we assume that a developerwould read the best answer i.e.
the accepted answer orthe answer with the highest vote if there is no acceptedanswer of the relevant question.
we refer to the collectedbest or highest vote answer of the two baseline approaches asthe baseline google answer summary and the baseline so answer summary respectively.
for each query question we provide the participants the answer summary generated by baseline google baseline so and our approach respectively.
the participants do not knowwhich answer summary is generated by which approach.
theyare asked to score the three answer summaries from threeaspects i.e.
relevance usefulness and diversity.
relevance refers to how relevant the generated summary is to the query.usefulness refers to how useful the generated summary is for guiding the developer s further search or learning.
diversity refers to whether the generated summary involves diverse aspects of information.
the score is a point likert scale with being highly irrelevant useless identical and being highly relevant useful diverse .
results.
table vii shows the mean of relevance usefulness and diversity scores of our approach and the two baselines.
theresult shows that our approach achieves the best performancein all three aspects while the baseline soachieves the worst performance.
our approach and baseline google have comparable relevance score but our approach has higher scorein usefulness and diversity especially diversity.
the averagenumber of paragraphs in baseline google and baseline so answer summaries are .
and .
respectively.
we use wilcoxon signed rank test to evaluate whether the differences between our approach and the baseline ap proaches are statistically significant.
the improvement of ourapproach over the baseline sois statistically significant on all three aspects at the confidence level of .
.
the improvement of our approach over the baseline google on usefulness and diversity is statistically significant at the confidence level of .
we use the best answer of the most relevant questionreturned by google as the baseline google s answer for the query.
considering the google s capability it is not surprisingthe difference in relevance is not statistically significant.
however our approach achieves statistically significant betterperformance on usefulness and diversity especially diversity .this indicates that the best or highest vote answers may not cover as diverse information as the developers need.
therefore it is worth reading more answer posts to summarize more com plete information.
our approach automates this summarization process for a diversity of answers.table vii mean of relev ance usefulness and diversity of ourapproach and the baseline approaches rq1 relevance usefulness diversity our approach .
.
.
baseline google .
.
.
baseline so .
.
.
p .
p .
p .
rq2 the effectiveness of relevant question retrieval motivation.
an answer summary is generated from the answers of some questions relevant to a query question.
if the retrieved questions are not relevant to the query question itis unlikely to generate a high quality answer summary forthe query question.
our question retrieval approach combinesword embeddings with traditional idf metrics.
we wouldlike to investigate the effectiveness of our method comparedwith the traditional tf idf based methods and other word ordocument embedding based methods.
approach.
we build three baseline approaches tf idf based ir word embedding based document retrieval and document to vector dov2vec based document retrieval .tf idf is a traditional ir metric that is often used to rank adocument s relevance to a user query in software engineering tasks such as question retrieval and code search .yang et al.
average word embeddings of words in adocument to obtain a document vector which can be used tomeasure document relevance.
dov2vec learns document embeddings together with the underlying word embeddings usinga neural network and is also applied to measure document relevance .
for each query question we collect the top ranked questions retrieved by our question retrieval approach or one of the baseline approaches.
we ask the participants to identifythe relevant questions in the top ranked questions.
theparticipants do not know which approach generates which listof top ranked questions.
we use the following metrics in comparison of different approaches.
top k accuracy given a query question q if at least one of the top k ranked questions is relevant we consider the retrieval to be successful and set the value success q top k to .
otherwise we consider the retrieval to be unsuccessful andset the value success q top k to .
given a set of queries denoted as qs its top k accuracy top k is computed as to p k qs summationtext q qssuccess q top k qs .
the higher the top k accuracy score is the better a relevant question retrievalapproach ranks the first relevant question.
in this paper wes e tk 5a n d1 .
mean reciprocal rank given a query question its reciprocal rank is the multiplicative inverse of the rank of the firstrelevant question in a ranked list of questions.
mean recipro cal rank mrr is the average of the reciprocal ranks of allqueries in a set of queries mrr qs qs summationtext q qs1 rank q .
we denote qsas the set of queries.
rank q refers to the position of the first relevant question in the ranked list ofquestions returned by a relevant question retrieval approach fora query.
the higher the mrr is the higher the first relevant questions is ranked for a set of queries.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table viii top kaccuracy and mrr ofourapproach and the baseline approaches in relev ant question retriev al rq2 top top top mrr our approach .
.
.
.
doc2vec .
.
.
.
word embeddings .
.
.
.
tf idf .
.
.
.
p .
p .
p .
results.
table viii shows the top k and mrr metrics of our approach and the three baseline approaches for retrieving relevant questions for the query questions.
we notice thatour approach achieves the best performance in all the evalu ated metrics especially for top and mrr.
the doc2v ec baseline achieves comparable performance as our approachon top and top but it has the worst performanceon top and mrr.
the word embedding baseline performs slightly better than the tf idf baseline in top k and mrr.
the differences between the three baseline approaches actuallyindicate that the traditional tf idf based method and the wordor document embedding based methods can complement eachother.
in fact our approach that combines word embeddingsand idf metrics achieves the best performance than eithertf idf or word document embedding alone.
we apply wilcoxon signed rank test to test the statistical significance of the differences between our approach and the baseline approaches.
the improvement of our approach over the tf idf baseline is statistically significant on all the metrics at the confidence level of .
compared withthe word embedding and doc2v ec baselines our approach is statistically significant better on top and mrr at the confidence level of .
although the differences between our approach and the word embeddings and doc2v ec baselines on top and top are not statistically significant thesignificantly better mrr indicates that our approach can rankrelevant questions higher than the other two baselines.
rq3 the effectiveness of answer paragraph selection motivation.
to select the most relevant and salient answer paragraphs for summarization our approach considers three types of features of answer paragraphs i.e.
query related user oriented and paragraph content features.
we would like to investigate the impact of different types of features on the results of answer paragraphs selection.
approach.
we remove one type of features at a time from the full feature set and reserve the other two types.
thus three baseline approaches are built i.e.
without w o query relatedfeatures w o user oriented features and w o paragraph contentfeatures.
we let each approach output a ranked list of 10answer paragraphs with the highest overall score.
we take the union of the answer paragraphs selected by our approach with all features and the three baselines.
participants areasked to judge whether the selected paragraphs contain salientinformation relevant to the query question.
they do not know which answer paragraph is selected by which approach.
we use top k accuracy and mrr in the top ranked candidateanswer paragraphs to evaluate the performance of answerparagraph selection with and without certain type of features.table ix top kaccuracy and mrr offeature ablation for answer paragraph selection rq3 top top top mrr all features .
.
.
.
w o query related .
.
.
.
w o user oriented .
.
.
.
w o paragraph content .
.
.
.
p .
p .
p .
results.
table ix presents top k and mrr metrics of using all features or adopting certain type of features for answerparagraph selection.
using all features achieves the best performance in all metrics compared with adopting certain typeof features.
this suggests that all types of features are usefulfor answer paragraph selection.
using query related featuresachieves better performance than adopting the other two types of features and using user oriented features achieves the worst performance.
this suggests that user oriented features play themost important role for answer paragraph selection paragraphcontent features take a second place and query related featuresare relatively less important.
wilcoxon signed rank test shows that the improvement of using all features over adopting certain type of features isstatistically significant on top and mmr at the confidencelevel of .
top and top results in table ix showthat there is almost always at least one relevant answerparagraph in the top or ranked list of candidate answerparagraphs.
this demonstrates the general effectiveness of ouranswer paragraph selection features.
therefore the differencesbetween using all features and adopting certain type of featureson top and top are not statistically significant.
v. d iscussion in this section we qualitatively compare our question answering approach with community question answering prac tice.
we then discuss cases where our approach is ineffective followed by some threats to validity.
a. comparison with community question answering in community question answering developers post their questions on a q a site like stack overflow and wait for answers from the community of developers.
to understandthe differences between the community provided answers to atechnical question and the answer summary that our approachgenerates from answers of relevant questions we manuallycompare the best answers of the questions we use as queriesand the answer summary that our approach generates for thesequestions.
table x presents two examples.
the query question in first example is calculating time difference in which the developer runs into some errors in using gettime to calculate time difference.
the best answer of this question suggests to use long to store gettime s return value rather than casting it to int.
for this query question our generated answer summary consists of five paragraphs fromthe four answers of two relevant questions.
except for the fifth paragraph you can try this due to the limi tation of paragraph splitting see section v b the other authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
four paragraphs provide two alternative solutions using system.nanot ime orcurrentt imemillis to calculate time difference.
they also describe the reason and cautions of using the two apis such as system.nanotime is more precise system.nanot ime must not be used as a wall clock currentt imemillis may not be a good method for time due to method overhead.
the query question in the second example is about hostname mismatch problem in httpcli e n t .
the best answer provides a solution and explains the advantage of the provided solution.
our answer summary consists of five paragraphs from the five answers of four relevant questions.
except forthe fifth paragraph from the fourth question about a factof amazon aws service the other four paragraphs providevaluable information which can complement the best answerof the query question.
the third and fourth paragraphs provide two different solutions for solving the problem i.e.
usinghttpclientbuilder.create .build or using a fixed version ofhttpcli e n t instead of the buggy version.
although the first and second paragraphs do not provide direct solutions they point out some important aspects related to the hostnamemismatch problem such as avoiding the action which allows all hosts without any verification checking the dns name ofthe certificate presented by the server.
as the above two examples show community answers to a technical question usually focus on a specific aspect technicalissue in the question.
our generated answer summary derivedfrom answers of several relevant questions can well comple ment community answers to a technical question by providing more alternative solutions and or broader information usefulfor further search and learning.
b. error analysis through the analysis of the cases in which our approach generates a low quality even unrelated answer summary we identify four main challenges in generating high quality an swer summary vague queries lexical gap between query andquestion description answers involving long code snippet anderroneous answer paragraph splitting.
in our study we randomly select stack overflow questions and use their titles as queries to search the questionrepository.
however question titles are short and some ofthem are vague e.g.
why is this code working withoutvolatile?
fast processing of data .
it is hard to understand such question titles without looking into the question bodies.furthermore the lexical gap between query question and titlesof questions in the repository makes it difficult to measure theirsemantic relevance.
due to these two challenges our relevant question retrieval component fails to retrieve at least onerelevant question in the top results for of the queryquestions.
the low quality question retrieval results directlyresult in the low quality of the generated answer summary.
to improve relevant question retrieval we will consider questionbodies which contain richer information and adopt deep neural network which are more robust for handling lexical gapsin text.our current approach keeps short code fragments enclosed in html tag angbracketleftcode angbracketright in natural language paragraphs but removes long code snippets enclosed in html tag angbracketleftpre angbracketright .
however for some query questions such as how to send an image from web service in spring how to implement a db listener in java and xml to json using json lib relevant answers are code snippets rather than natural language para graphs.
thus to generate high quality answer summary forthis type of questions we must take into account long codesnippets.
our current approach splits the text in an answer post into answer paragraphs by html tag angbracketleftp angbracketright.
this simple strategy may sometimes break the logic relationships between several consecutive physical paragraphs.
for example people often writesome introductory paragraph like try with the following from here you can go to and there are many solutions to this problem followed by a separate paragraph explainingthe details.
to generate more sensible answer summary anintroductory paragraph and the following detailed explanationshould be treated as a whole using more robust paragraphsplitting method.
c. threats to v alidity threats to internal validity are related to experimental bias of participants in manual examination of relevant questions and answer summaries.
first the participants lack of knowledgeon java may affect their judgements about question answer srelevance and usefulness.
this threat is limited by selectingonly participants who have at least years industrial ex perience on java development.
still there could be errorsbecause an experienced java developer is not necessarilyfamiliar with all java frameworks and apis.
on the otherhand the participants degree of carefulness and effort inmanual examination may affect the validity of judgements.we minimize this threat by choosing participants who expressinterests in our research and giving the participants enoughtime to complete the evaluation tasks.
threats to external validity are related to the generalizability of our research and experiments.
stack overflow questions are related to many domains other than java e.g.
python eclipse database or combinations of multiple domains.
our approachis general but considering the background knowledge ofavailable participants for the user studies we use only java questions in this work.
furthermore as user studies require significant human efforts we only use query questionsin this study.
in the future we will use more queries largerquestion repository and questions of more domains to reducethese threats.
threats to construct validity are related to the suitability of our evaluation metrics.
relevance usefulness and diversity are widely used to evaluate summarization tasks in software engineering both relevant question retrieval and answer paragraphs selection are ranking problems.
thus we use top k accuracy k and mrr.
these twometrics are the most widely used metrics for evaluating irtechniques .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table x comparison of the bestcommunity provided answers and the answer summary generated by ourapproach query question calculating time diffrence relevant questionsbest answer of query question id answer summary generated by our approachid title try this long start time system.nanotime resp geolocationservice.getlocationbyip ipaddress long end time system.nanotime double difference end time start time 1e6 try changingint diff int d2.gettime int d1.gettime to long diff d2.gettime d1.gettime explicit typecasting from long to int no it doesn t mean it s taking 0ms it shows it s takinga smaller amount of time than you can measure with currenttimemillis .that may well be 10ms or 15ms.
it s not a good method to call for timing it s more appropriate for getting the current time.
calculating time difference in milliseconds will cause precision loss and may result in a negative value on subtraction.
long a 90000000000000l long b 10000001000000l a b int a int b negative value to measure how long something takes consider using system.nanotime instead.
the important point here isn t that the precision is greater but that the resolution will be greater... but only when used to measure the time between two calls.
it must not be used as a wall clock .
since java .
you can get a more precise time value with system.nanotime which obviously returns nanoseconds instead.
you can try this calculating time difference in java query question android httpclient hostname in certificate didn t match example.com !
.example.com relevant questionsbest answer of query question id answer summary generated by our approachid title important if you are allowing all hosts that is disabling host name verification then it is certainly not safe.
you shouldn t be this in production.15497372java http post using https confusion javax.net.ssl.sslexception hostname in certificate didn t match this is my edited solution ...code... it has the advantage of not changing the the certificate verification process will always verify the dns name of the certificate presented by the server with the hostname of the server in the url used by the client.7256955java sslexception hostname incertificate didn t match default behavior unless there is a wildcard domain and in that case it revalidates as though the part domain e.g.
someurl.com this httpclientbuilder.create .build will return org.apache.http.impl.client.internalhttpclient.
it can handle the this hostname in certificate didn t match issue.
were part of the certificate otherwise theoriginal exception is rethrown.
that meanstruly invalid certs will still fail.
this problem is described in apache httpclient resolvingdomain to ip address and not matching certificate.
it appears to be abug in the version of httpclient you are using where it compares the target ip instead of the target hostname with the subject certificate.
please use a fixed version of httpclient instead.34493872sslexception hostname in certificatedidn t match .
.
.
!
.heroku.com buckets whose name contains periods can now be correctlyaddressed again over https.12755038ssl problems with s3 aws using the javaapi hostname in certificate didn t match vi.
r elated work many text summarization approaches have been applied in different software engineering tasks aiming to reduce the developers effort to read an immense quantity of information.rastkar et al.
propose an extractive approach for automatic bugreport summarization .
their approach selects a subset ofbug report comments by training a binary classifier to deter mine whether a comment should be selected or not.
andreaet al.
propose an approach surf to produce a summary for user reviews .
surf classifies each user review sentence to one of the user intention categories and groups together sentences covering the same topic.
it then uses a sentence selection and scoring mechanism to generate the user reviewsummary.
different from the above studies we focus onanswer summarization on online q a sites which requiresa different set of features.
additionally we formulate ourproblem as a ranking problem instead of a classification one.
a number of studies have also proposed methods to identify relevant or high quality posts in question and answering sites.gottipati et al.
propose a semantic search engine framework to process posts in discussion threads to recover relevant answers to a user query .
they classify posts in the discussionthreads into categories e.g.
questions answers clarifyingquestion junk etc.
and use the category labels to filter lessuseful information to improve the search experience.
yao et al.
propose an approach to detect high quality posts incommunity question answering sites .
different from the above studies we not only identify relevant posts but alsocreate summaries of these posts.
popular search engines like google can provide directanswers for some types of queries.
for example googlecan extract a paragraph from the best answer of the stackoverflow question what are the differences between hashtableand hashmap?
as the answer for a query like differencesbetween hashtable and hashmap .
however google does thisonly for specific kinds of 5w 1h who what where when why how questions and it does not generate direct answers to all 5w 1h questions e.g.
what are the differences be tween quick sort and bubble sort?
.
more importantly googleextracts only a paragraph from one answer of one question while our approach is multi answer posts summarization.
vii.
c onclusion our formative study indicates that developers need some automated answer generation tools to extract a succinct anddiverse summary of potential answers to their technical ques tions from the sheer amount of information in q a discus sions.
to meet this need we propose a three stage frameworkfor automated generation of answer summary.
our user studiesdemonstrate the relevance usefulness and diversity of our automated generated answer summaries.
in the future we will investigate more robust relevant question retrieval algorithms and explore more features for answer paragraph selection.
wewill develop our approach into practical tools e.g.
browserplugins to provide developers with direct answers and ex tended suggestions to help them find the needed information more efficiently on the internet.
a cknowledgment this work was partially supported by nsfc program no.
and .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.