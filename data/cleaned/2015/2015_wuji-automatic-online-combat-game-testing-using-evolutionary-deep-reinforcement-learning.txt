wuji automatic online combat game testing using evolutionary deep reinforcement learning yan zheng1 dividemultiply xiaofei xie2 dividemultiply ting su2 lei ma3 jianye hao1 zhaopeng meng1 yang liu2 ruimin shen4 yingfeng chen4 changjie fan4 1college of intelligence and computing tianjin university china.
2nanyang technological university singapore.
3kyushu university japan.
4fuxi ai lab netease inc. hangzhou china.
abstract game testing has been long recognized as a notoriously challenging task which mainly relies on manual playing and scripting based testing in game industry.
even until recently automated game testing still remains to be largely untouched niche.
a key challenge is that game testing often requires to play the game as a sequential decision process.
a bug may only be triggered until completing certain difficult intermediate tasks which requires a certain level of intelligence.
the recent success of deep reinforcement learning drl sheds light on advancing automated game testing without human competitive intelligent support.
however the existing drls mostly focus on winning the game rather than game testing.
to bridge the gap in this paper we first perform an in depth analysis of real bugs from four real world commercial game products.
based on this we propose four oracles to support automated game testing and further propose wuji an on the fly game testing framework which leverages evolutionary algorithms drl and multi objective optimization to perform automatic game testing.
wuji balances between winning the game and exploring the space of the game.
winning the game allows the agent to make progress in the game while space exploration increases the possibility of discovering bugs.
we conduct a large scale evaluation on a simple game and two popular commercial games.
the results demonstrate the effectiveness of wuji in exploring space and detecting bugs.
moreover wuji found previously unknown bugs1 which have been confirmed by the developers in the commercial games.
index terms game testing artificial intelligence deep reinforcement learning evolutionary multi objective optimization.
i. i ntroduction gaming has evolved into an all around entertainment phenomenon.
the game market is growing rapidly.
according to global games market report from newzoo in there are more than .
billion active gamers in the world of which spend money on games.
the game market around the world reaches around billion and is expected to more than billion in .
with so many users the quality of games becomes especially important.
a game bug may bring bad gaming experience even cause financial losses for gamers or game companies.
game companies not only put a lot of efforts to detect and fix the bugs but also suffer from losing dividemultiplyequal contribution.
corresponding author.
1the video reproduction of the bugs can be found in our website .
fig.
block maze with bugs red green and yellow dots users.
hence an early stage testing is of great importance in discovering bugs and guaranteeing the game quality.
due to the complexity and heavy user interactions of games currently game testing is mainly dependent on human testers.
most game companies adopt some ad hoc manual testing without using systematic and automated testing solutions .
the ad hoc manual testing is costly and is inefficient in discovering bugs for large games.
as a result many bugs are still discovered long after the official release.
unfortunately most of these bugs are discovered by gamers.
a study shows that popular games receive a large number of reviews each day making it very time consuming for developers to handle them .
semi automatic script based testing can improve efficiency.
however it still requires substantial manual efforts to develop the scripts.
in addition the ad hoc scripting is also costly and usually focuses on main scenarios of the game following a pre defined strategy.
from the internal report of thenetease inc. there are testers for testing even one game which was used as the benchmark in our evaluation .
the direct loss caused by the bugs in this game is about million each year.
a systematic and automated testing technique towards better quality assurance is urgent in the game industry .
automatic testing techniques have been widely studied such as search based testing coverage guided fuzzing and symbolic execution .
however such techniques are challenging in testing games as the game playing is a continuous interaction process with rich graphical user interfaces guis between the gamer and the game.
some techniques such as random based testing monkey and 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
model based testing stoat have been proposed for testing gui applications.
these techniques improve the code coverage by covering the sequences of different events during ui change.
however in a game events in each ui are usually fixed i.e.
the skills and it is very easy to cover all events inside game uis.
these techniques are not effective in game testing since they are not smart to accomplish the complicated goal of the game.
for example fig.
shows the block maze game in which a robot hunts for gold.
bugs will be triggered if the robot reaches one of the dots.
the yellow dots are easily reachable by monkey and stoat since they are near the initial place.
however the red and green dots are more difficult to reach as the robot needs to find a better path.
recently deep reinforcement learning drl has achieved great success in automatic game playing e.g.
alphago on the board game openai on atari games and openai five on the large scale real time strategy games dota2 .
this brings an opportunity for automated game testing with the intelligent support of drl.
however the challenge is that the current dlr focuses on finding a better solution to accomplish the mission e.g.
obtaining higher rewards rather than testing games .
for example in fig.
drl will learn a better strategy the dotted arrow that will earn the gold quickly.
hence the green dots can be reached but other dots e.g.
the red dots are hard to cover.
in addition to capture various types of bugs e.g.
glitches gaming balance during playing the game the test oracle is needed.
this paper performs a two stage study towards addressing the game testing challenges.
in the first stage we conduct an empirical study towards understanding the characteristics of bugs in real world games.
we analyzed real bugs from four commercial online games and classify the bugs into five categories based on their manifestation i.e.
crash bugs stuck bugs logical bugs game balance bugs anduser experience bugs .
based on the manifestation of different types of bugs we proposed four oracles that facilitate the detection of such bugs.
at the second stage we develop an automated game testing framework named wuji to test the games and discover different types of bugs.
the objective of wuji is to detect game bugs by exploring states as much as possible.
to achieve the goal wuji not only considers accomplishing the mission but also considers searching different directions.
these two strategies complement each other.
consider fig.
the strategy of accomplishing mission helps reach the green dots that are difficult to cover by the general random strategy.
the strategy of exploration will help reach the red dots that may not be located in the mainline of the game.
to test a game wuji first trains a set of agents i.e.
deep neuron networks that can play the game automatically.
the agent policies will always be updating towards exploring the states and accomplishing the mission.
based on proposed oracles wuji performs an on the fly testing while constantly training the agent policies rather than the usual ai solutions that can be used only after training .
we propose a technique by a combination of evolutionary algorithms ea multi objective optimization moo and deep reinforcement learning drl .
ea and moo mainly contribute to exploring game space while drl contributes to accomplishing the mission .
specifically in each evolution iteration the population first performs crossover and mutation considering the two objectives i.e.
exploring states and accomplishing mission .
then each policy in the population will be separately trained by the drl for enhancing the capability of accomplishing the mission.
next the moo together with a non dominated sorting is used to select a better set of policies as the seed population of the next evolution.
by optimizing the population iteratively more states of the game could be explored and tested.
we implemented wuji and evaluated the usefulness by applying it to one simple game and two commercial massively multiplayer online games mmogs including a mobile game and a pc game.
mmog is one of the most popular games nowadays where thousands of players join together.
the two commercial games have more than million and thousand peak daily players respectively.
the results demonstrated that wuji significantly outperforms random testing ea alone and drl alone in terms of bug detection and state coverage.
moreover wuji also successfully discovered previously unknown bugs in the two active commercial games.
to the best of our knowledge this is the first automated game testing work on real world online combat games.
the main contributions of this paper are summarized as follows we perform an empirical study to characterize game bugs by analyzing real bugs from four industrial games.
we propose four test oracles for four types of bugs.
we propose an on the fly automated testing framework based on evolutionary deep reinforcement learning.
we implement wuji and evaluate its effectiveness in two real world games.
moreover we find previously unknown bugs which are confirmed by the developers.
ii.
b ackground and problem definition a. game playing game playing is a sequential decision making process where a player needs to continuously make decisions and take actions based on received observations before the game ends.
such a problem can be modeled as a markov decision process mdp that is a classical formalization of sequential decision making .
fig.
depicts an mdp which shows a typical overall interaction of playing the game.
in general the mdp consists of elements agent environment state action and reward .
an environment is what an agent interacts with e.g.
the game .
the agent utilizes a specific policy for interacting with the environment.
the state is the observation of the environment which is usually different at different timestamps.
the action is a set of possible decision e.g.
moves fire the agent can make.
the reward e.g.
scalar value is the feedback by which we measure the success or failure of an agents actions with regard to achieving some goals e.g.
winning the fight or achieving points .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
the interaction of an agent and game environment.
for example to be specific given the state stat time t the agent selects an action atto interact with the game environment and receives a reward rtfrom the environment.
the environment turns into a new state st affecting the agent selection of the next action.
the environment and agent interact continually until the game ends and gives rise to a trajectory as follows s0 a0 r0 si ai ri b. problem definition a large online combat game ocm usually contains many instance dungeons or scenarios.
it is challenging to test the whole game one time the common way is to test each scenario separately.
another overwhelming challenge is that the ocm is usually played by multiple players.
in this initial game testing work we mainly focus on game testing with one agent.
based on the equation we define a game as follows definition game .a gamegis a function s r g a whereais a sequence of actions from an agent sis a sequence of states and ris a sequence of rewards.
for simplicity we also use bardblg a bardblsand bardblg a bardblrto represent the outputs of the game g i.e.
sandr respectively.
a test case of the game gis a sequence of actions a which explore and drive the game into different states.
note that gis often a non deterministic function because of the randomness in the game e.g.
the randomness in the environment the randomness effect of an action .
suppose bardblg a bardblr r1 ... r n the rewards of aon the gameg denoted as rw a g is computed as follows rw a g summationdisplay t ...n trt where is a discount factor.
definition policy .a policy is a function a s that maps a state to an action where sandarepresent a state and an action respectively.
the policy is the strategy that an agent adopts to decide the next action based on the current state.
for example a policy can be the strategy from the human or a trained deep neural network that can make the decision.definition game state .a statesof the game gis a fixedlength vector angbracketleftv0 v1 ... v n angbracketright where each value of the vector represents a certain aspect of the state of the game.
the vector of a state represents the current status of the game and it differs for different games.
for example the position of the player character the health points etc.
definition game testing .f o rag a m e g a game tester t can be defined as a function s b t g where is a set of policies to interact with g sis a set of states explored bytandbis a set of bugs by t. a game tester employs different policies and tries to explore different states of the game where a bug might hide.
the goal of the game testing is to generate test cases i.e.
valid actions that can reach the diverse states on which bugs will be triggered.
to check whether a specific state triggers a bug the test oracle is usually needed.
however due to the uniqueness of game software the test oracle also quite differs from other kinds of software which is yet not well studied.
thus our first goal is to perform an empirical study and investigate the potential useful test oracles for checking game bugs.
then we propose effective testing policies i.e.
for effective state exploration to cover those bugs might hide which is then captured by our proposed test oracle.
iii.
e mpirical study massively multi player online game mmog is one of the most popular games nowadays where thousands of players join together.
to achieve better user experience game companies strive to detect and fix as many bugs as possible before each version release.
online combat is among the core and most complicated elements of mmogs which is the key challenge for mmog testing.
in this paper we mainly focus on such scenarios but our approach is general to other types of games.
to better understand the characteristics of existing game bugs we first perform an empirical study that analyzes bugs from the bug lists of four popular commercial ocms that are actively used by over million peak daily players in total.
all these games are developed and maintained bynetease inc. .
informed by the knowledge of game developers we categorize them into different types according to their consequences.
we order them by the fixing priority from highest to least as follows crash bugs which leads the entire game to crash and exit.
for example the zero dividing problem memory leak issue or recursive function calls will result in game crash.
crash bugs are severe defects and should be fixed first.
stuck bugs which freezes the game.
game players are unable to continue the interactions.
such bugs may be caused when the player enters into an abnormal state getting stuck the whole game.
although there is no crash the game cannot response to any player s actions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i bug distribution in four commercial games.
games l10 nsh l12 lh01 total crash .
stuck .
logical .
balance .
experience .
total .
logical bugs which often do not break the game like the previous two types of bugs but lead to unexpected results e.g.
errors on score computation .
this type of bugs are usually caused by incorrect implementation of game logic.
gaming balance bugs which disturb the gaming balance between human and computer aided players e.g.
too strong or too weak and thus violating the designers original expectation.
this type of bug is specific but critical to combat games.
user experience bugs which downgrade user experience e.g.
missing video e.g.
ray tracing and audio effects failing to give proper hints text typos etc.
among these types of bugs game developers give higher priority for the first four types of bugs since they directly affect the business logic stability and correctness of games.
these bugs are often strived to be fixed before releasing games.
table i shows the detailed classification results of bugs.
we can observe that these four critical types of bugs account for half of all bugs in studied games.
based on these in this paper we define four types of oracles with the intention to detect these four typical bugs respectively which are urgent to be fixed with a higher priority oracle crash the crash bugs can be automatically captured by monitoring whether the game process is terminated.
oracle stuck the stuck bugs can be automatically captured by monitoring the change of the game screen or monitoring the states of the game i.e.
comparing the difference between the current state and the previous nconsecutive states.
in this paper we used the state level check since monitoring the game screen is too heavy and not scalable.
specifically during game playing the moving averaged state s summationtext i t n ... tsi n is calculated by averaging nprevious consecutive states.
if the difference between current observed state sand sis less than a threshold ts a stuck potential bug is discovered.
oracle logical the logical bugs are difficult to be discovered automatically.
it requires specific assertions to reveal logical bugs.
oracle gaming balance the balance bugs can be captured by checking whether one player achieves unexpected performance e.g.
too strong or weak beyond a predefined threshold.
gaming balance bugs are subjective and need the confirmation of game designers.
we will not consider this type of bugs in this paper.
iv .
a utoma ted game testing in this section we first describe the overview of our approach.
then we present our detailed techniques and al gorithms for automated game testing.
a. overview the general idea of wuji is to generate effective policies that explore more states of the game where the bugs potentially hide.
in company with the proposed oracle such bugs are more likely to be detected.
fig.
a shows the simplified workflow ofwuji .
given a game wuji randomly initializes a population of policies p i.e.
a specified number of dnns after which the main testing iteration starts on the basis of an evolutionary process.
notably all individuals of the first generation are randomly initialized dnns with random weights .
from the high level in each iteration wuji adopts the evolutionary multi objective optimization emoo and deep reinforcement learning drl to achieve efficient game testing.
emoo is used for improving the diverse state exploration while drl facilitates improving the capability of accomplishing the mission of the game.
the combination of emoo and drl altogether makes wuji potentially be able to complete more missions and explore more states of the game.
the policies i.e.
dnns pare randomly selected by the binary tournament selection to perform crossover and mutation to evolve the population to p prime towards generating policies that are capable of exploring more states section iv c .
the fitness score of each policy is computed by playing and interacting with the game.
a challenge is that if the policy is not smart and is always unable to complete the mission of the game then the policy would not explore the states after the mission.
to mitigate the problem wuji leverages a drlbased technique which aims at improving the capability of accomplishing the mission of the game section iv d .
each obtained dnn policy in p primewill be further trained by drl and an advanced population qthat is good at completing missions is obtained.
afterwards wuji performs a multi objective based selection form the population p p prime andqthat could survive in the next iteration section iv e .
in particular the non dominated sorting algorithm and crowding distance sorting algorithm are used to sort the population p p prime q. wuji continually evolves its testing policy by emoo and drl iteration by iteration.
at the same time the policies obtained from both procedures are leveraged to perform onthe fly game testing until the given testing time or resource budge exhausts.
wuji eventually outputs a set of failed traces that facilitate reproducing the bugs and a set of obtained competitive testing policies.
b. multiple objectives to be specific wuji iteratively evolves the population towards achieving two objectives exploring more states and winning the game i.e.
complete the mission .
we capture these two objectives by exploration score and the winning score respectively which are measured as follows.
for the gameg we use the policy to play the game for mtimes after which msequence of actions i.e.
a0 a1 ... a m will be generated.
each of these sequences represents the actions authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
!
!
!
!
!
!
!
initializationrandom a the workflow of wuji b visualization of moo based selection fig.
the overview and workflow of wuji and moo based selection.
taken by an agent and the corresponding game environment state changes step by step.
then the exploration score of on the game gis computed as follows es g mm summationdisplay i size bardblg ai bardbls the wining score of on the game gis calculated as rs g mm summationdisplay i rw g ai intuitively the exploration score is computed as the average of the total number of states in mruns of the game.
the winning score is computed as the average of the total rewards inmruns of the game.
the higher the exploration score the more states the policy will potentially explore.
the higher winning score the policy has a higher chance to accomplish more intermediate missions.
to better capture the capability relation of different policies we define the policy domination relation as follows definition pareto domination .a policy 0dominates another policy 1 denoted as 0 follows 1 o ng a m e gif es 0 g es 1 g rs 0 g r s 1 g o r es 0 g es 1 g rs 0 g rs 1 g .
c. crossover and mutation algorithm shows the crossover and mutation on a populationp.
p represents the total number of the policies in p. wuji adopts a binary tournament selection to select two policies i.e.
dnns fand mfor the crossover line .
specifically two candidates 1and 2are picked randomly.
if 1dominates 2 1will be selected line .
if 2dominates 2 2will be selected line .
if they do not dominate each other we randomly select one of them line .
for the two selected dnns single point crossover spx is performed on the weights of dnn line .
after that random mutation is further used by applying additive gaussian noise gn to each weights of dnns in p. by perturbation diverse new policies with different strategies are generated.
consequently more states may be explored by utilizing such policies.algorithm cross mutate input p the population g the game output p prime a new population 1p prime 2fori ... p do randomly select two policies 1 2fromp if 1 follows 2then f 1 else if 2 follows 1then f 2 else f random 1 2 similarly select another candidate m 11 c weights cross f m see spx in 12 weights mutate c see gn in 13p prime p prime 14returnp prime d. drl training deep reinforcement learning drl is an effective approach that interacts with the real environment and learns the optimal policy from the reward signal enabling the agent to play really complex games automatically and achieving a certain level of human competitive performance .
wuji adopts a standard drl algorithm named advantage actor critic a2c which tries to find a policy to maximize the rewards in terms of accomplishing the mission.
specifically all perturbed individuals in offspring p primeare further evolved by leveraging the gradient based optimization a2c .
the optimization continuous until either of the following criteria is satisfied reaching maximum training time or none performance improvement after certain consecutive training rounds.
these criteria are referred to as the early stop which achieves a good trade off between improving performance and saving computational resources.
it is worth mentioning that to be effective wuji adopts a parallel approach for training all dnns in the offspring asynchronously and simultaneously.
the choices of hyper authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm nd sort input p the population output p prime a sorted set of pareto frontiers.
1p prime 2while p 0do 3f p prime p prime follows 4p prime p prime f 5p p f 6returnp prime algorithm pop select input p the population n p the number of agents to be selected output p prime a selected population 1p prime f1 ... f m nd sort p 3foreachfiin f1 ... f m do if p prime fi nthen p prime p prime fi else f prime cd select fi n p prime p prime p prime f prime break 10returnp prime parameters of drl training and early stop will be detailed in the later section v a4 respectively.
e. population selection after the evolution we obtain two new strategy populations p primeandq.
the policies in p primeare evolved via crossover and mutation which pay more attention to the exploration.
on this basis population qare obtained via drl training which focuses on completing the mission.
given all candidate policies i.e.
p p prime q wuji utilizes the non dominated sorting and crowding distance sorting to dynamically select the better policies in terms of the two objectives see section iv b .
algorithm shows the non dominated sorting algorithm.
given a population p it identifies a number of prioritized pareto frontiers a subset of the policies by a repeating process.
in each iteration a policy which cannot be dominated by other policies is added into the current selected pareto frontier line .
then the pareto frontier will be added to the new population line and be removed from the original population line .
in the next iteration it continues to select next pareto frontier from the remaining agent policies.
the intuition of our selection strategy is that if a policy is not dominated by others it is at least no worse than any other one in terms of the multi objectives.
fig.
b shows an example of the non dominated sorting that outputs a sequence of pareto frontiers f1 f2 f3 ... .
each point is a policy within a pareto frontier.
we could see that f1generally exceedsf2because f1.
prime f2.
prime follows .algorithm cd select input f 1 2 l a pareto frontier n f the number of polices to be selected output p prime a set of selected policies i l dist i 2e sorte f 3w sortw f 4dist e 5dist w 6foreach f e w do 7i indexof e 8j indexof w 9dist ese ese rsw rsw 10z sortdist f 11return z ... z algorithm presents the procedure that takes policies p and a number nas the input and outputs a better population p prime.
we first perform a non dominated sorting to prioritize the policies.
based on the prioritization the pareto frontiers will be added to the new population in order line .
if the total number of the selected population p primeand the current pareto frontierfiexceedsn line we have to select part of the polices whose size is n p prime fromfisuch that we can exactly select npolicies.
crowding distance sorting strategy is then adopted to select the remaining policies line .
algorithm shows the crowding distance sorting algorithm.
crowding distance cd measures the density of the policies andwuji tends to select the sparse points for constructing a more diverse population.
the cd of each policy is initialized as zero line .
then we sort the policies fin descending order based on the exploration score and winning score respectively line .
the cd of the policy which has the largest winning score or the exploration score is set as the max distance value line .
the two policies are the best ones in terms of exploring winning and we will always select them.
for each of the other policies line we compute the cd by the distances from the surrounding polices.
we get the indexes in the ordered sequences eandw line .
the cd is computed based on the distance between two surrounding policies in terms of exploration score and winning score i.e.
ese ese andrsw rsw .
afterward the polices are sorted in descending order based on the cd and the first npolicies are returned line .
consider the two policies 1and 2in fig b the cd of 1is computed as d1 d2 d3 d4 which is larger than the cd of 2. intuitively the polices around 2are very dense while the polices around 1are sparse.
v. e v alua tion we have implemented wuji lines of code based on pytorch .
to demonstrate the effectiveness of wuji w e investigate the following research questions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
examples of combat scenarios for l10 left and nsh.
rq1 bug detection how effective is wuji in detecting game bugs?
how do the key components of wuji i.e.
ea drl andmoo affect the performance when functioning separately?
rq2 coverage how is the exploration capability of wuji in terms of code coverage and state coverage?
rq3 bug analysis what are the root cause and characteristics of those bugs found by wuji ?
a. experimental design and settings datasets a simple game block maze and two commercial online combat games l103and nsh4 are selected in the evaluation.
since l10 and nsh are quite complex real world commercial games.
to make our study feasible for each game we only selected one instance i.e.
a combat scenario for testing.
fig.
depicts the online combat scenarios for the two commercial games.
table ii summarizes the detailed information about the games.
column code shows the number of lines of source code for each game.
in particular for l10 and nsh the left number shows the code size of the selected scenario while the right number is the code size of the whole game.
column inst.size represents the installation size and column players shows daily peak player count for l10 and for nsh from which we could see the selected games are indeed large scale and practical.
block maze the agent controls the robot to reach the goal coin.
the player has actions to choose north south east west .
every movement leads the agent to move into a neighbor grid in the corresponding direction except that a collision on the block dark green results in no movement.
l10and nsh the two games are online combat games where a game character fights against an opponent in the game.
to be specific l10 is dimensional scenario running on the mobile platform.
nsh is a dimensional game running on the pc platform.
in l10 the game character is equipped with skills while it has skills in nsh.
also the game character can move in four directions.
the goals of the games are to win the fighting by discovering a combination of skills that damage the opponent to the largest extent.
note that all three games are tested on the server even if l10 is released for mobile devices.
guis of the games are removed as we mainly focus on functional testing.
in industry game development and testing of netease inc. this is a typical method for maximizing the testing efficiency.
3a chinese ghost story l10 4treacherous water online nsh ii three games used in the evaluation name code kloc inst.size players block maze .
mb l10 .
.
gb million nsh .
.
gb thousand game bugs to the best of our knowledge it still lacks standard benchmarks for game testing at this moment.
in our study to evaluate the usefulness of wuji the three games are injected with history bugs for controlled study as follows.
for block maze we injected bugs that are randomly distributed within the environment.
the bugs will be triggered if the robot has reached the specific location of the map see fig.
.
forl10 and nsh the game developers of our collaborated company help to inject crash bugs stuck bugs and logical bugs and crash bugs stuck bugs and logical bugs previously known bugs in the selected scenarios based on their experience.
these bugs actually appeared in previous different versions of the game and are now injected in the same game version for our evaluation purpose.
furthermore developers also insert the assertions for detecting logical bugs.
baseline approaches to the best of our knowledge there still does not exist any automated testing technique on large ocms at this moment.
as mentioned in section v a1 our testing environment is deployed on the server and does not contain the gui part.
hence the existing gui testing tools such as monkey sapienz stoat are not applicable.
we tried our best to compare wuji with the five strategies including one baseline and four variants of wuji to demonstrate the effectiveness of wuji and evaluate the effectiveness of each strategy of wuji in terms of bug detection and coverage random during playing the games it randomly selects one of the available actions.
we followed the idea of monkey and implemented this random strategy as the baseline.
eas the evolutionary algorithm based strategy whose fitness score only considers the winning score.
eam the evolutionary algorithm based strategy that uses a moo based fitness score i.e.
algorithm .
drl test the games by only considering the winning score with drl.
eas drl the combination of ea and drl but they only consider the winning score.
compared with eas drl wuji eam drl adopts a multi objective optimization.
configurations and implementations we select the dnn model in the previous work a2c as the seeding model for all of the three games with the input and output dimensions adjusted for different games.
mean square error mse is adopted as the loss function in drl training which is minimized by leveraging the adam optimizer.
to be specific due to the stripping of guis the inputs of the dnns models are scalar based vectors.
consequently we use three fully connected linear layers with units as the hidden layers connected with the output layer.
the output authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
... 1m ... 30m ... 1h0510152025 bugsrandom ea sea m drlea s drl wuji a block maze0 15m 30m 1h 3h 8h 10h0246810random ea sea m drlea s drl wuji b l100 ... 5h ... 10h048121620random ea sea m drlea s drl wuji c nsh fig.
comparison of different strategies regarding the average number of bugs found in three games respectively random ea s ea m drl ea s drl wuji0510152025 bugs a averaged bugs found in block mazerandom ea s ea m drl ea s drl wuji0246810 b averaged bugs found in l10random ea s ea m drl ea s drl wuji048121620 c averaged bugs found in nsh fig.
the number of bugs discovered using different strategies after hour for block maze and hours for l10 and nsh.
layer consists of each valid actions for three games with the corresponding size of .
the output of each hidden layeris normalized using batch normalization and the activation function relu is adopted between two consecutive layers.
on the other hand to ensure sufficient exploration we set the weight of the entropy item in the loss functionof the actor to and ensure at least possibility of each valid actions being selected for executing.
the epoch fordrl training is set to blockmaze l10 and nsh respectively.
the learning rate is set to .
3in the adam optimizer.
in addition the maximum training timeand threshold of the consecutive training period in early stop are set to minutes and rounds respectively.
for the eaalgorithm the size of the population and offspring are set to30.
the crossover rate and mutation rate are set to and respectively.
notably a random noise sampled from a gaussian distribution n .
is adopted for mutating the dnns.
all the experiment were run on three powerful serverswith cpu intel r xeon r gold cpu .00ghz with100 cores and 256gb ram.
from a statistical perspective in rq1 and rq2 each strategy is used to test the games for a certain amount of timebased on the complexity.
for the simple game block maze weallocate one hour time budget.
for l10 and nsh we allocate10 hours for each.
to counter the randomness effect duringtesting we repeat each strategy times to average the results.to mitigate the potential bias of bug injection during eachtest run of block maze the bugs and the gold position are randomly injected.
note that even though we only selected two scenarios in the two commercial games it still takes quite a lot of human efforts the game developers spent about man days and manpower cost to inject these known bugs carefully and we ran different strategies on three games with 10repetitions and it overall took machine hours.
b. evaluation on bug detection rq1 fig.
shows the average number of bugs discovered over time.
the statistical results of the number of bugs discovered during the allocated testing time see section v a4 are shownin fig.
.
overall we have the following findings.
first compared with the baseline we can see that random achieves better results in the smaller game i.e.
block maze .the reason is that small game has a relatively small searchspace.
in the small space random can explore the states given enough time budge.
for example in block maze random can find more bugs than ea sanddrl quickly.
the reason is that easanddrl focus on how to accomplish the mission.
in a smaller game they can find a better policy quickly and stopexploring more states.
this prevents them from discoveringmore bugs.
in commercial games the state space becomesquite larger.
due to lacking the intelligence random cannot authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii the average number of different types of bugs l10 nsh wuji e s drl drl e me s ran.
wuji e s drl drl e me s ran.
crash .
.
.
.
.
stuck .
.
.
.
.
.
.
.
logical .
.
.
.
.
.
.
.
.
.
new .
.
.
total .
.
.
.
.
.
.
.
.
.
.
.
accomplish the mission well.
it only explores a small number of states and has the worst performance.
the results indicate that random testing performs badly in the large games which is consistent with our intuition that traditional random testing is less effective in testing large game testing because it cannot effectively accomplish the game intermediate missions at the first hand.
the ai based approach e.g.
drl can mitigate such a problem by learning a smart policy.
second the results also show the effect of ea drl and moo on training smart policies for bug detection.
the combination of easanddrl can achieve much better results than using one of them separately.
the reason is that easmainly considers the winning score and it provides various searching directions by leveraging the advantage of population while the drl algorithm improves the searching speed in a particular direction through gradient based optimization.
the results also indicate that both gradient based drl and search based techniques eas contribute to discovering more bugs by complementing each other.
further wuji i.e.
eam drl achieves better results than eas drl and eamachieves better results than eas.
this suggests that the multi objective optimization is helpful for discovering more bugs efficiently.
even though the population can contribute to the exploration e.g.
eas drl performs well in all three games the performance can be further improved by involving the exploration score in the fitness score i.e.
eam drl forwuji .
in a large game like nsh the improvement is even more obvious.
we also found that eamis slightly better than easwhile eas drl exceeds easeven further.
intuitively adding drl achieves larger improvement than adding exploration score.
we perform an in depth analysis and found that even if we have added the winning score in ea the performance of accomplishing the mission is still not satisfying since ea is gradient free and it is not effective to improve the winning score.
as a result if it cannot accomplish the mission the exploration score cannot be improved.
the results further demonstrate why the benefits of including drl gradientbased in wuji which facilities the mission accomplishment.
table iii shows the average number of different types of bugs including known injected bugs and newly discovered bugs in l10 and nsh.
during the rounds of testing wuji detects new bug in l10 and new bugs in nsh.
as for the other strategies only eas drl finds one new bug in nsh.
these three new bugs have been confirmed by the game developers and will be fixed in the next update.
more details about the bugs will be discussed in section v d. consideringtable iv the results of line coverage with each strategy blockmaze l10 nsh coverage table v the results of state coverage with each strategy game wuji eas drl eam eas drl rand.
b.m .
.
.
.
.
.
l10 .
.
.
.
.
.
nsh .
.
.
.
.
these unknown bugs are detected from the stable release version of these games it further confirms the usefulness of wuji for detection unknown bugs.
to summarize wuji enhances the exploration with the moo based eamand improves the capability of accomplishing the mission with drl which complement each other and achieve the best performance with combination.
answer to rq1 wuji is much more effective in detecting game bugs than random testing which performs poorly on large games.
ea drl andmoo complement each other in exploration of diverse states and accomplish the game mission.
their combination i.e.
wuji achieves better performance.
c. evaluation on coverage increase rq2 we continue to analyze the coverage obtained by each strategy on the game.
surprisingly the line coverage of different strategies is exactly the same for each game despite the results of bug detection are quite different see table iv .
the results indicate that these strategies achieve high line coverage easily.
this can be explained by the characteristics of game i.e.
the code related to the function of the game is easily covered by playing games.
hence it seems to be ineffective to detect game bugs by only improving code coverage.
furthermore we also analyze state coverage.
the state coverage is computed as follows each state see definition contains a set of features.
we adopt an interval approximation to partition each feature into equal buckets.
based on the discretization buckets of the features we can count covered buckets to estimate the state coverage.
for example if the state has nfeatures and each feature is partitioned into m intervals the maximum number of states i.e.
buckets is mn.
table v shows the number of states that are covered by each strategy on each game.
compared with the other strategies wuji cover more states in each game the bold number .
for large games such as nsh the state coverage is relatively low because large games might have more challenging resp.
infeasible states that are difficult reps. unable to reach.
for example if the player is not close to the boss then the boss cannot be attacked.
we further found that the results of state coverage are consistent with the results of bug detection inrq1 .
in the small game random testing can achieve competitive results while it is the worst one in the larger games indicating that it is more useful in detecting bugs authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a diversity of policy b trajectories fig.
a the final distribution of policies in the population and b the trajectories of executing each policy.
by maximizing state coverage than code coverage for game testing.
answer to rq2 the code of games is relatively easy to cover.
for state coverage random testing is limited in increasing state coverage on large games while wuji is more effective than the other strategies by a combination ofea drl and moo .
d. bug analysis rq3 in this section we analyze some specific cases towards better understanding the coverage and bugs of games.
more analysis and bug reproducing videos can be found on our website .
polices in block maze.
fig.
shows the final distribution of policies with eas drl and wuji in one run of block maze with the difference between single objective and multiobjective depicted.
with moo the policies in the population ofwuji are more diverse in terms of exploration score and winning score while the polices in eas drl only favors in winning i.e.
closely distributed at the bottom right .
the right figure shows the trajectories of executing each policy.
intuitively the policies in wuji can explore more directions yellow arrows while the policies in eas drl only move towards the gold green arrow .
line state coverage.
the following example is a simplified code segment including a bug in l10.
the bug will be triggered once the variable distance is .
intuitively the code is very easy to cover as long as the skill i.e.
skill id is used.
for state coverage distance is a feature in the state new states will be covered when distance changes.
the example shows that line coverage is relatively easy to cover and it cannot capture the state change.
if player.buff time distance compute distance player boss will crash if distance attack player.use skill skill id distance damage boss.get damage attack ... ... logical bug in l10 the following code shows the calculation of damage from the enemy to the player character.
specifically the damage from the boss is calculated based on the current damage reduction buffs e.g.
line and each buff has a time limit.
a logical bug is that the damage i.e.
variable hurt can be negative after some operators and finally the damage from the boss becomes the treatment for the player line .
we investigate the bug and found that the variablehurt becomes negative only when the initial damage line is within a certain range and three specific buffs i.e.
exist simultaneously.
the state is difficult to reach butwuji discovered it by adopting an effective exploration.
.
hurt boss.atk .if boss.buf true .
hurt boss.buf .reduce val .if boss.buf true .
hurt boss.buf .reduce val .if boss.buf true .
hurt boss.buf .reduce val ... .
fix hurt max hurt .
player.hp hurt crash bug in nsh a crash bug discovered by wuji andma s drl .
generally the bug is triggered when some properties of the game are reset with invalid values which are generated only when the boss is dead with multiple specific buffs.
this a very hard corner case to reach by manual testing e.g.
manual playing or scripting .
however it could be triggered during playing by a large number of game players.
wuji found the bug by learning diverse policies in favoring to explore more states of the game.
e. threats to validity randomness is a major factor during testing and the game playing.
we counter this issue by repeating times for each testing task and averaging the results.
in addition the oracle we proposed may not be complete and thus wuji may miss some other unknown bugs like logical bugs and other types of bugs.
the selection of the games could be biased.
in this paper we try to counter this issue by using games with different sizes where two of them are active commercial games.
the selection of the dnns used for training policies could be biased.
we counter this problem by selecting models from the state of the art for game playing.
the bias of bug injection may be also a threat.
we counter this issue by randomly injecting bugs of block maze in each testing round and ask helps from game developers to inject bugs in the commercial games.
on the other hand false alarms reported by wuji may be a potential threat.
specifically the thresholds in stuck and balance oracles directly determinate the sensitivity of wuji reporting a bug.
the choice of such hyper parameters are not only highly dependent on the domain knowledge but may also be biased.
to address this for each discovered bugs wuji will record every necessary information during game playing including every taken action the opponent s hp value attack and defensive value etc.
.
moreover an additional replay suit is also developed for replaying the game automatically.
on this basis despite unable to achieve detection accuracy wuji however provides an effective way for testers to discern real bugs efficiently.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
vi.
r ela ted work this section surveys the related work in three aspects.
game testing .
as games become increasingly popular and complex testing games is now an important challenge and becomes the key vehicle to improving quality.
however existing testing practice and techniques still need more significant advances.
as pointed by lin et al.
even the most popular games on the market show the lack of sufficient testing.
alemm et al.
conduct a quantitative survey to identify key developers factors for a successful game development process.
they find that manual testing and ad hoc exploratory testing are still predominant than systematic and automated testing.
iftikhar et al.
proposes a uml based modeling methodology to support automated system level game testing of platform games.
they manually construct the model and use the model to generate and execute test cases with oracles.
however the manual model construction process still requires a lot of efforts.
as for mobile games existing research is also preliminary.
lovreto et al.
first reports their experience of using exploratory testing to test mobile games.
they manually write test scripts for functional testing and find existing testing techniques still have a lot of limitations.
to prioritize testing efforts khalid et al.
mine the user reviews from free mobile game apps and find that most negative reviews come from a small subset of devices which game developers should emphasize during testing.
game apps are gui applications which extensively interact with human players.
although various research work exists for testing of gui applications these techniques only target at general purpose guis.
they do not work for testing game apps because game apps usually use a game engine to render guis instead of using traditional gui widgets.
to our knowledge our technique wuji is the first to achieve systematic and automated testing for real world games and makes great improvements over state of the practice.
reinforcement learning for testing .wuji applies deep reinforcement learning drl to facilitate automatic game testing.
v arious research work have also applied deep reinforcement learning for their own purposes.
for example qbe uses q learning one type of reinforcement learning technique to automatically test mobile apps.
specifically it explores the guis of a set of apps to generate the corresponding behavior models and uses these models to train the transition prioritization matrix with two optimization goals i.e.
activity coverage and the number of crashes respectively.
qbe focuses on improving code coverage and the number of detected crashes for android apps.
similarly adamo et al.
and vuong et al.
also use q learning to improve gui testing of android apps.
however these work only focus on general apps instead of game apps.
b ottinger et al.
introduces the first program fuzzer that uses reinforcement learning to learn high reward seed mutations for testing traditional software.
specifically by automatically rewarding runtime characteristics of the target program this technique obtains new inputs that likely drive program execution towards a predefined goal e.g.
maximizing code coverage.
reinforcement learning is also applied to facilitate test prioritization and selection in regression testing .
different from these work wuji combines drl with evolutionary algorithms to improve testing of game apps which have not been tackled before.
evolutionary testing .
many evolutionary algorithms have been proposed and widely used to solve the test generation problem in software testing.
coverage guided fuzzing is a popular testing technique which has been proposed for detecting vulnerabilities in traditional software .
moreover evolutionary testing based techniques have also been applied to test deep neural networks .
compared with them wuji is designed to explore more states in the game instead of maximizing code neural coverage.
in addition there are some techniques using evolutionary testing with multi objective optimization.
in the field of gui testing a number of research work exploit multi objective optimization to improve testing effectiveness.
sapienz uses multi objective search based testing to automatically finding bugs in android apps.
it optimizes test sequences by minimizing length while simultaneously maximizing coverage and fault revelation.
similarly stoat use the gibbs sampling technique to optimize test generation for android apps by maximizing coverage at both model and code level while increasing test diversity.
in our work wuji uses a combined evolutionary algorithm and dlr for effective game testing towards covering diverse states and completing more intermediate missions.
vii.
c onclusion in this paper we performed a study on the characteristics of bugs in real world commercial games.
we classified the bugs into five categories and proposed four test oracles.
based on the oracles we developed an automated testing framework for games.
by combing the evolutionary multi object optimization and drl wuji can explore more states such that bugs can be more likely to be detected.
we demonstrated the effectiveness ofwuji on one simple game and two large commercial games.
in the future we plan to extend wuji to support multiplayer games based on multi agent reinforcement learning and apply wuji on more types of games.
acknowledgment the work is supported by the national key r d program of china grant nos.
2018yfb1701700 national natural science foundation of china grant nos.
u1836214 the national research foundation prime ministers office singapore under its national cybersecurity r d program award no.
nrf2018ncr ncr005 national satellite of excellence in trustworthy software system award no.
nrf2018ncr nsoe003 administered by the national cybersecurity r d directorate singapore.
we thank our industrial research partner netease inc. especially the fuxi ai lab and game testing department of leihuo business groups for their discussion and support with the experiments.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.