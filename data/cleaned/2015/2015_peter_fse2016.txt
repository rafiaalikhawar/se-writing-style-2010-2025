cacheoptimizer helping developers configure caching frameworks for hibernate based database centric web applications tse hsun chen queen s university ontario canada tsehsun cs.queensu.caweiyi shang concordia university quebec canada shang encs.concordia.caahmed e. hassan queen s university ontario canada ahmed cs.queensu.ca mohamed nasser blackberry canadaparminder flora blackberry canada abstract to help improve the performance of database centric cloudbased web applications developers usually use caching frameworks to speed up database accesses.
such caching frameworks require extensive knowledge of the application to operate e ectively.
however all too often developers have limited knowledge about the intricate details of their own application.
hence most developers nd con guring caching frameworks a challenging and time consuming task that requires extensive and scattered code changes.
furthermore developers may also need to frequently change such con gurations to accommodate the ever changing workload.
in this paper we propose cacheoptimizer a lightweight approach that helps developers optimize the con guration of caching frameworks for web applications that are implemented using hibernate.
cacheoptimizer leverages readilyavailable web logs to create mappings between a workload and database accesses.
given the mappings cacheoptimizer discovers the optimal cache con guration using coloured petri nets and automatically adds the appropriate cache con gurations to the application.
we evaluate cacheoptimizer on three open source web applications.
we nd that i cacheoptimizer improves the throughput by and ii after considering both the memory cost and throughput improvement cacheoptimizer still brings statistically signi cant gains with mostly large e ect sizes in comparison to the application s default cache con guration and to blindly enabling all possible caches.
.
introduction web applications are widely used by millions of users worldwide.
thus any performance problems in such applications can often cost billions of dollars.
for example a report published in shows that a one second page load permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa copyright held by the owner author s .
publication rights licensed to acm.
isbn .
.
.
.
of the amazon web applications can cost an average of .
billion dollars in sales each year .
the complexity and scale of modern database centric web applications complicate things further.
as much as of developers nd their applications performance is deeply impacted by their reliance on databases .
application level caching frameworks such as ehcache and memcached are commonly used nowadays to speed up database accesses in large scale web applications.
unlike traditional lower level caches e.g.
hardware or web proxies these application level caching frameworks require developers to instruct them about what to cache otherwise these frameworks are not able to provide any bene t to the application.
deciding what should be cached can be a very di cult and time consuming task for developers which requires in depth knowledge of the applications and workload.
for example to decide that the results of a query should be cached developers must rst know that the query will be frequently executed and that the fetched data is rarely modi ed.
furthermore since caching frameworks are highly integrated with the application these frameworks are con gured in a very granular fashion with cache api calls that are scattered throughout the code.
hence developers must manually examine and decide on hundreds of caching decisions in their application.
even worse a recent study nds that most database related code is undocumented which makes manual con guration even harder.
developers must continuously revisit their cache con guration as the workload of their application changes .
outdated cache con gurations may not provide as much performance improvement and they might even lead to performance degradation.
however identifying workload changes is di cult in practice for large applications .
even knowing the workload changes developers still need to spend great e ort to understand the new workload and manually re con gure the caching framework.
in this paper we propose cacheoptimizer a lightweight approach that automatically helps developers decide what should be cached and also automatically places the cache con guration code in web applications that are implemented using hibernate in order to optimize the con guration of caching frameworks.
using cacheoptimizer developers can better manage the cost of their database accesses greatly improving application performance .cacheoptimizer rst recovers the workload of a web application by mining the web server access logs.
such logs are typically readily available even for large scale applications that are deployed in production environments.
cacheoptimizer further analyzes the source code statically to identify the database accesses that are associated with the recovered workloads.
to identify detail information about the recovered database accesses such as the types of the access and the accessed data cacheoptimizer leverages static taint analysis to map the input variables of the web requests to the exact database accesses.
combining the recovered workload and the corresponding database accesses cacheoptimizer models the workload the database accesses and the possible cache con gurations as a coloured petri net.
by analyzing the petri net cacheoptimizer is able to determine an optimal cache con guration i.e.
given a workload which objects or queries should be cached by the caching frameworks .
finally cacheoptimizer automatically adds the appropriate con guration calls to the caching framework api into the source code of the application.
we have implemented our approach as a prototype tool and evaluated it on three representative open source databasecentric web applications pet clinic cloud store and openmrs that are based on hibernate .
the choice of hibernate is due to it being one of the most used java platforms for database centric applications in practice today .
however our general idea of automatically conguring a caching framework should be extensible to other database abstraction technologies.
we nd that after applyingcacheoptimizer to con gure the caching frameworks on the three studied applications we can improve the throughput of the entire application by .
the main contributions of this paper are .
we propose an approach called cacheoptimizer which helps developers in automatically optimizing the conguration of caching frameworks for hibernate based web applications.
cacheoptimizer does not require modi cation to existing applications for recovering the workload and does not introduce extra performance overhead.
.
we nd that the default cache con guration may not enable any cache or may lead to sub optimal performance which shows that developers are often unaware of the optimal cache con guration.
.
compared to having no cache nocache the default cache con gurations defaultcache and enabling all caches cacheall cacheoptimizer provides a better throughput improvement at a lower memory cost.
paper organization.
the rest of the paper is organized as follows.
section rst discusses related work to our paper.
section introduces background knowledge for common caching frameworks.
section describes the design details of cacheoptimizer .
section evaluates the bene ts and costs of cacheoptimizer .
section discusses threats to validity of our study.
finally section concludes the paper.
.
related work and background in this section we discuss related work to cacheoptimizer .
we focus on three closely related areas software engineering research on software con guration optimizing the performance of database centric applications and caching frameworks.
.
software configuration improving software con gurations.
software con gurations are essential for the proper and optimal operation of software applications.
several prior software engineering studies have proposed approaches to analyze the congurations of software applications.
for example rabkin et al use static analysis to extract the con guration options of an application and infer the types of these congurations.
xu et al conduct an empirical study on the con guration parameters in four open source applications in order to help developers design the appropriate amount of con gurability for their application.
liu et al focus on con guring client slide browser caches for mobile devices.
detecting and fixing software con guration problems.
rabkin et al use data ow analysis to detect con guration related functional errors.
zhang et al propose a tool to identify the root causes of con guration errors.
in another work zhang et al propose an approach that helps developers con gure an application such that the application s behaviour does not change as the application evolves.
chen et al propose an analysis framework to automatically tune con gurations to reduce energy consumption for web applications.
xiong et al automatically generate xes for con guration errors using a constraint based approach.
prior research on software con guration illustrates that optimizing con gurations is a challenging task.
in this paper we propose cacheoptimizer which particularly focuses on helping developers optimize the cache con gurations to improve the performance of large scale web applications.
.
improving application performance by reducing the overhead of database accesses most studies in literature propose frameworks to reduce the overhead of database accesses by batching or reordering database accesses.
ramachandra et al propose a framework for pre fetching data from the database management system dbms in batches in order to reduce database access overheads.
similarly cheung et al propose a framework for delaying database accesses as late as possible and sending database access requests only when the data is needed in the application.
chavan et al propose a framework for sending queries asynchronously in order to improve application performance.
several proposed frameworks improve application performance by analyzing the source code.
in our prior work we propose a static analysis framework to detect and rank database access performance anti patterns.
developers can address these anti patterns based on their priorities.
cheung et al leverage static analysis and code synthesis to automatically generate optimal sql queries according to post conditions and loop invariants.
grechanik et al propose a framework that combines both static and dynamic analysis to prevent database deadlocks.
chaudhuri et al use instrumented database access information to nd database related performance problems in the code.
compared to prior studies we not do propose a new framework.
instead cacheoptimizer helps developers optimize the con guration of the frameworks in particular caching frameworks that are already in use in practice today.
indepth knowledge of a software application is needed for software developers to optimally con gure such frameworks.
entity cachable table name user public class user id column name user id private long userid column name user name private string username ... other instance variables ... setter and getter methods user.java user user finduserbyid main.java ormcache layer database11 user updateuserbyid user user users execute select from user .cache user user id name peter user id name jack updateuserbyid user 4user user figure an example of simpli ed hibernate code hibernate cache con guration code and hibernate cache mechanism.
the numbers and the arrows indicate the ow for di erent workloads.
the grey userobjects in the cache layer means the objects are invalidated in the cache layer.
.
caching frameworks there are many prior studies on cache algorithms and frameworks.
many cache algorithms such as least recently used lru and most recently used mru are widely used in practice for scheduling lower level caches.
for example such algorithms are used to improve the performance of web applications by caching web pages through proxies .
most of these caching algorithms operate in an unsupervised dumb fashion i.e.
these low level caching algorithms do not require any application level knowledge to operate.
many modern applications generate dynamic content which may be highly variable and large in size based on data in the dbms.
therefore many low level cache frameworks are becoming less e ective.
many recent caching frameworks cache database accesses at the application level .
when using these application level caching frameworks developers have full control of what should be cached in an application.
however to leverage these caching frameworks e ectively they must be con gured properly.
unlike most prior studies cacheoptimizer does not try to manage cache scheduling.
instead cacheoptimizer is designed to help developers optimize the con guration of application level caching frameworks which must be con gured correctly for developers to fully leverage their bene ts.
.
hibernate and caching mechanisms .
hibernate cacheoptimizer automatically con gure the caching framework for hibernate based web applications .
hibernate is one of the most popular java frameworks for abstracting database operations.
hibernate abstracts database accesses as object calls in java instead of using sql or jdbc directly.
hibernate is very popular among developers because it helps reduce the amount of boilerplate code and development time .
for instance a recent survey shows that among the surveyed java developers .
use hibernate instead of other database abstraction frameworks including jdbc .
figure shows an example of using hibernate to abstract database accesses in java.
in this example annotations e.g.
entity table and column are added to user.java to specify the mapping between tables in a rela tional database and objects in java.
based on such annotations hibernate automatically transforms user records to user objects and vice versa and automatically translates the manipulation of the user object to the corresponding sql queries.
hibernate is often used along caching frameworks like ehcache .
these application level caching frameworks aim to improve the performance of database centric applications by reducing the number of database accesses.
.
hibernate caching mechanism most caching frameworks act like an in memory key value store.
when using hibernate these caching frameworks would store the database entity objects objects that have corresponding records in the dbms in memory and assign each object a unique id i.e.
the primary key .
there are two types of caches in hibernate object cache.
as shown in work ow figure if the requested user object is not in the cache layer the object will be fetched from the dbms.
then the user object will be stored in the cache layer and can be accessed as a key value pair using its id e.g.
fid user objg .
if the object is updated the cached data would be evicted to prevent a stale read work ow .
to cache database entity objects developers must add an annotation cachable at the class declaration as shown in user.java in figure .
then alldatabase entity object retrieved by id e.g.
retrieved using nduserbyid would be cached.
these annotations congure the underlying caching frameworks.
query cache.
the cache mechanism for query cache is slightly di erent from object cache.
for example the cached data for a select all query on the user table work ow would look as follows select from user !fid id 2g fid id 2g!f id user objg fid user objg the cache layer stores the idsof the objects i.e.
id and that are retrieved by the query and uses the ids to nd the cached objects the corresponding user obj .
thus the object cache must be enabled to use a query cache.
when a user object is updated workow the query cache needs to retrieve the updated object from the dbms to prevent a stale read.
thus if the queried entity objects are frequently modi ed using a query cache may not be bene cial and may even hinder performance .
to cache query results developers must call a method like cache before executing the query main.java in figure .
such method is used to con gure the underlying caching frameworks.
adding caches incorrectly can introduce overhead to the application.
caching a frequently modi ed object or query will cause the caching framework to constantly evict and renew the cache which not only causes cache renewal overhead but may also result in executing extra sql queries.
therefore blindly adding caches without understanding the workload may lead to performance degradation .
requestmapping value user id method get public user getuserbyid int id return finduserbyid id .
.
.
get user http .
.
.
.
get user http .
.
.
.
get user http .
.
.
.
get user http .
.
.
.
get user http .
.
.
.
get user http .
cachable entity public class user ... public user finduserbyid int id return entitymanager.find user.class id source code web access logsfigure a working example of cacheoptimizer .
the sign in front of the cachable line indicates that the caching con guration is added by cacheoptimizer .
.
cache optimizer cacheoptimizer optimizes the con guration of caches that are associated with database accesses that occur for a given workload.
hence our approach needs to recover the workload of an application then to identify which database access occurs within that particular workload.
in the following subsections we explain each step of the inner workings of cacheoptimizer in detail using a working example.
the input of the working example shown in figure consists of two parts source code of the application and web access logs.
figure shows an overview of cacheoptimizer .
.
recovering control and data flow graphs we rst need to understand the calling and data ow relationships among methods and determine which applicationlevel methods are impacted by database caching i.e.
which methods eventually lead to a database access .
we therefore extract the call and data ow graphs of the application by parsing the source code of the application using the eclipse jdt.
we opt to parse the source code instead of analyzing the binary since we need to locate the hibernate annotations in the source code such annotations are lost after compiling the source code to java byte code.
we mark all hibernate methods that access the dbms e.g.
query.execute in the call and data ow graphs.
such methods are easy to identify since they are implemented in the same class i.e.
in theentitymanager and the query class of hibernate .
once such methods are marked we are able to uncover all the application level methods that are likely to be impacted by optimizing the database cache.
in our working example after generating the call and data ow graphs and identifying the hibernate database access methods we would know that the method getuserbyid contains one database access and the parameter is passed in through a web request.
.
linking logs to application level methods we recover the workload of the application by mining its web access logs.
we leverage web access logs because of the following reasons.
first web access logs are typically readily available without needing additional instrumentation since many database centric applications rely on restful web service based on http web requests to accept requests from users .
for example large companies like ibm oracle facebook and twitter all provide restfulapis1.
second unlike application logs web access logs have a universal structure the format of all log lines are the same .
hence compared to application logs web access logs are easier to analyze and do not usually change as an application evolves .
web access logs may contain information such as the requestor s ip timestamp time taken to process the request requested method e.g.
get and status of the response.
an example web access log may look like .
.
.
get user http .
this web access log shows that a request is sent from the local host at august to get the information of the user whose id is .
the status of the response is and the application took milliseconds to respond to the request.
in order to know which application level methods will be executed for each web request we use static analysis to match the web access logs to application level methods.
cacheoptimizer parses the standard restful web services jax rs speci cations in order to nd the handler method for each web request .
an example of jax rs code is shown below requestmapping value user id method get public user getuserbyid int id return finduserbyid id in this example based on the jax rs annotations we know that all get requests with the url of form user fidg will be handled by the getuserbyid method.
for every line of web access log cacheoptimizer looks for the corresponding method that handles that web request.
after analyzing all the lines of web access logs cacheoptimizer generates a list of methods and their frequencies that are executed during the run of the application.
in our working example we map every line of web access log to a corresponding web request handling method i.e.
getuserbyid method.
.
database access workload recovery we want to determine which database accesses are executed for the workload.
since application level cache highly depends on the details of the database accesses we need to recover the types of the database access e.g.
a query versus a select insert update delete of a database entity object by id and the data that is associated with the database access e.g.
accessed tables and parameters .
such detailed information of database accesses helps us in determining the optimal cache con gurations.
we rst link each web access log to its request handler method in the code as described in section .
.
therefore for each workload we know the list of request handler methods that are executed i.e.
entry points into the application .
then we conduct a call graph and static ow insensitive interprocedural taint analysis on each web request handler method using the generated call and data ow graphs as describe in section .
.
our algorithm for recovering the database access workload is shown in algorithm .
for each web request handlermethod we identify all possible database accesses by travers1 caching locations source code web access logsfinding database access codecall and data flow graphs1.
recovering control and data flow graphs3.
db access workload recoverydb access tables db access params db access types4.
identifying possible cache locations possible caching locations .
evaluating potential cache benefit using coloured petri net miss ratio threshold add cacheyesfor each cache locationanalysis data condition in a loop6.
configuring the caching frameworks2.
linking logs to application level methodsrecovered request handler methodpossible cache locationsfigure overview of cacheoptimizer .
algorithm our algorithm for recovering database accesses.
input cg dg mthd call graph data ow graph the request handler method output accessinfo params accessed db tables and db func type query or key value lookup and parameter of the request 1accessinfo params traverse the call graph from mthd 3foreach path2cg.
ndallpathfrom mthd do foreach call2pathdo ifisdbcall call then accessinfo accessinfo getaccessedtable call getmthdtype call end end 9end track the usage of the input params 11foreach param2mthd.getparams do foreach path2dg.
ndallpathfrom param do foreach node2pathdo node pointtoanalysis node ifusedindbaccesscall node then params params dbaccesscall node end end end 20end ing all paths in the call graph and recording the type of the database access.
after recovering the database access we traverse the data ow graph of each web request handler method to track the usage of the parameters that are passed in through the web requests.
we want to see if the parameters are used for retrieving modifying the data in the dbms.
such information helps us better calculate the optimized cache con guration.
for example we would be able to count the number of times a database entity object is retrieved e.g.
according to the id that is speci ed in the web requests or how many times a query is executed e.g.
according to the search term that is speci ed in the web request .
for post put and delete requests we track the url e.g.
post newuser to which the request is sent which usually speci es which object the request is updating.
if there is no parameter speci ed then we assume that the request may modify any of the objects to be conservative on our advice on enabling the cache.
in our working example we recover a list of database accesses.
all of the accesses read data from the user table.
in ve of the accesses the parameter is and in one of the accesses the parameter is .
.
identifying possible caching locationsafter our static analysis step we recover the location of all the database access methods in the code and the mapping between java classes and tables in the dbms.
namely we obtain all potential locations for adding calls to the cache con guration apis.
thus if a query needs to be cached we can easily nd the methods in the code that execute the query.
if we need to add object caches we can easily nd the class that maps to the object s corresponding table in the dbms.
in our example we identify that the class user is a possible location to place an object cache.
our static analysis step is very fast seconds on a machine with 16g ram and intel i5 .3ghz cpu for our studied applications see table and is only required when deploying a new release.
thus the execution time has minimal impact.
we use ow insensitive static analysis approaches to identify possible caching locations because it is extremely di cult to recover precise dynamic code execution paths without introducing additional overhead to the application e.g.
using instrumentation .
during our static analysis step if we choose to assign di erent probabilities to code branches we may under count or over count reads and writes to the dbms.
under counting reads may result in failing to cache frequently read objects which has little or no negative performance impact i.e.
the same as not adding a cache .
however under counting writes may result in caching frequently modi ed objects and thus has signi cant negative e ects on performance.
in contrast we choose a conservative approach by considering all possible code execution paths over counting to avoid under counting reads and writes.
we may over count reads and writes to the dbms but over counting reads has minimal performance impact since in such cases we would only place cache con guring apis on objects that are rarely read from the dbms overcounting writes means that we may miss some objects that should have been cached but will not a ect the system performance the same as adding no cache .
hence our conservative choice by intentionally considering all possible code execution paths over counting ensures that the caching suggestions would not have negative performance impact after placing the suggested caches.
note that there may be some memory costs when turning on the cache i.e.
use more memory and in rq2 we evaluate the gain of our approach when considering such costs.
.
evaluating potential cache benefits using coloured petri net after linking the logs to handler methods and recovering the database accesses cacheoptimizer then calculates the potential bene ts of placing a cache on each database ac p1p2 p3p4 p5p1p2 p3p4 p5 p1p2 p3p4 p51 t1 t1 t1t2 t2 t2figure an example of modeling potential cache bene ts using a coloured petri net.
a red token represents a read to a speci c database entity object e.g.
nduserbyid and a blue token represents write to a speci c database entity object updateuserbyid .
cess call.
we use petri nets a mathematical modeling languages for distributed applications to model the activity of caches such as cache renewal and invalidation.
petri nets allow us to model the interdependencies so the reached caching decisions are global optimal instead of focusing on top cache accesses greedy .
petri nets model the transition of states in an application and a net contains places transitions and arcs.
places represent conditions in the model transitions represent events and arcs represent the ow relations among places.
formally a petri net ncan be de ned as n p t a and a p t t p where pis the set of places tis the set of transitions and a is the set of arcs.
places may contain tokens which represent the execution of the net.
any distributions of the tokens in the places of a net represent a set of con gurations .
a limitation of petri nets is that there is no distinction between tokens.
however to use petri nets to evaluate potential cache bene ts we need to model di erent data types e.g.
a hibernate query versus an entity lookup by id and values e.g.
query parameter .
thus we use an extension of petri nets called coloured petri net cpn .
in a cpn tokens can have di erent values and the values are represented using colours.
formally a cpn can be de ned as cpn p t a c n e g i where p t and aare the same as in petri nets.
represents the set of all possible colours all possible tokens c maps pto colours in e.g.
specify the types of tokens that can be in a place and nis a node function that maps a into p t t p .eis the arc expression function g is the guard function that maps each transition into guard expressions e.g.
boolean and nally irepresents an initialization function that maps each place to a multi set of token colours.
in our cpn shown in figure we de ne pto be the states of the data in the cache.
p3 is a repository that stores the total number of database accesses p4 stores the total number of cache hits and p5 stores the number of invalidated caches.
p2 is an intermediate place for determining whether the data would be cached or invalidated.
we de ne tto be all database accesses that are recovered from the logs.
we de ne to distinguish the type of the database access call e.g.
read write using ids or queries and the parameters used for the access obtained using algorithm .
thus our cde nes that p4 can only have colours of database access calls that are reads and p1 p2 p3 and p5 may contain all colours in .
the transition function on t1 always forwards the tokens in the initial place p1 to p2 and p3.
there are two guard functions on t2 where one allows a token to be moved to p4 if there are two or more tokens of the same colour in p2 i.e.
multiple reads to the same data so a cache hit and another guard function makes sure that if there is a write in p2 all the same write tokens and the corresponding read tokens are moved to p5 e.g.
the cache is invalidated .
in our example figure we let red tokens represent the database access call nduserbyid and blue tokens represent updateuserbyid .
in there are two red tokens and t1 is triggered so the two red tokens are stored in p2 and p3.
since there are two red tokens in p2 t2 is triggered and moves one red token to p4 a cache hit .
the resulting cpn is shown in .
when a blue token appears in p1 t1 is triggered and moves the blue token to both p2 and p3.
since there is a blue token in p2 t2 is triggered and we move both the red and blue token to p5 cache invalidation .
the nal resulting petri net is shown in .
note that t2 acts slightly di erent for tokens that represent query calls.
when an object is updated the query cache needs to retrieve the updated object from the dbms to prevent a stale read.
thus to model the behaviour t2 would be triggered to move the query token to p5 from p2 if we see any token that represents a modi cation to the query table.
we use the recovered database accesses of the workload to execute the cpn.
for all tokens that represent the database access to the same data e.g.
a read and write to user by id we examine their total counts in p3 and p4 to calculate the miss ratio mr of the cache.
mr can be calculated as one minus the total number of cache hits in p4 divided by the total number of calls in p3.
we choose mr because it is used in many prior studies to evaluate the e ectiveness of caching e.g.
.
if mr is too high caching the data would not give any bene t. for example if a table is constantly updated then data in that table should not be cached.
thus we de ne a threshold to decide whether a database access call should be cached.
in our cpn if mr is smaller than then we place the cache con guration code for the corresponding query query cache or table object cache .
since object cache must be turned on to utilize query cache we enable query cache only if the mr of the object cache is under the threshold.
such that there would not exist con icting decisions for object and query cache.
we choose to be more conservative on enabling caches so that we know the cached data would be invalidated less frequently lower cache renewal cost .
we also vary mr to and do not see any di erence in terms of the suggested cache con gurations.
however future work should further investigate the impact of mr. .
configuring the caching frameworks cacheoptimizer automatically adds the appropriate calls to the cache con guration api.
since the locations that require adding cache con guration apis may be scattered across the code cacheoptimizer helps developers reduce manual e orts by automatically adding these apis to the appropriate locations.
for example if the query that is executed by the request user ?query peter should becached cacheoptimizer would automatically call the caching framework s api to cache the executed query in the corresponding handler method searchuserbyname .
in our example shown in figure the miss ratio of caching objects in the user class is which is smaller than our threshold .
cacheoptimizer automatically adds the cachable annotation to the source code to enable cache for the user class.
.
ev aluation in this section we present the evaluation of cacheoptimizer .
we rst discuss the applications that we use for our evaluation.
then we focus on two research questions what is the performance improvement after using cacheoptimizer and what is the gain of cacheoptimizer when considering the cost of such caches.
experimental setup.
we evaluate cacheoptimizer on three open source web applications pet clinic cloud store and openmrs .
table shows the detailed information of these three applications.
all three applications use hibernate as the underlying framework to access database and use mysql as the dbms.
we use tomcat as our web server and use ehcache as our underlying caching framework.
pet clinic which is developed by spring aims to provide a simple yet realistic design of a web application.
cloud store is a web based e commerce application which is developed mainly for performance testing and benchmarking.
cloud store follows the tpc w performance benchmark standard .
finally openmrs is largescale open source medical record application that is used worldwide.
openmrs supports both web based interfaces and restful services.
we use one machine each for the dbms 8g ram xeon .67ghz cpu web server 16g ram intel i5 .3ghz and jmeter load driver 12g ram intel quad .67ghz .
the three machines are all connected on the same network.
we use performance test suites to exercise these applications when evaluating cacheoptimizer .
performance test suites aim to mimic the real life usage of the application and ensure that all of the common features are covered during the test .
thus for our evaluation performance test suites are a more appropriate and logical choice over using functional tests.
we use developer written tests for pet clinic and work with blackberry developers on creating the test cases for the other applications.
for cloud store we create test cases to cover searching browsing adding items to shopping carts and checking out.
for openmrs we use its restful apis to create test cases that are composed of searching by patient concept encounter and observation etc and editing adding retrieving patient information.
we also add randomness to our test cases to better simulate real world workloads.
for example we add randomness to ensure that some customers may checkout and some may not.
we use for our performance tests the mysql backup les that are provided by cloud store and openmrs developers.
the backup le for cloud store contains data for over 5k patients and 500k observations.
the backup le for cloud store contains about 300k customer data and 10k items.
rq1 what is the performance improvement after using cacheoptimizer?
motivation.
in this rq we want to examine how well the performance of the studied database centric web appli table statistics of the studied applications.
total lines number of of code java les pet clinic .8k cloud store 35k openmrs .8m table performance improvement throughput against nocache after applying di erent cache congurations.
throughput nocache cacheoptimizer cacheall defaultcache pet clinic .
.
.
cloud store .
.
.
.
openmrs .
.
.
.
cations can be improved when using cacheoptimizer to congure the caching framework.
approach.
we run the three studied applications using the performance test suites under four di erent sets of cache congurations without any cache con guration nocache with default cache con guration defaultcache cache con gurations that are already in the code which indicates what developers think should be cached with enabling all possible caches cacheall and with con gurations that are added by cacheoptimizer .
we compare the performance of the applications when con gured using these four di erent sets of cache con gurations.
we work with performance testing experts from blackberry to ensure that our evaluation steps are appropriate accurate and realistic.
we use throughput to measure the performance.
the throughput is measured by calculating the number of requests per second throughout the performance test.
a higher throughput shows the e ectiveness of the cache con guration as more requests can be processed within the same period of time.
there may exist many possible locations to place the calls to the cache con guration apis.
hence con guring the caching framework may require extensive and scattered code changes which can be a challenging and time consuming task for developers.
therefore to study the e ectiveness of cacheoptimizer and how it helps developers we also compare the number of cache con gurations that are added by cacheoptimizer relative to the total number of all possible caching con gurations that could be added and the number of cache con gurations that exist in defaultcache .
results.
cacheoptimizer outperforms defaultcache and cacheall in terms of application performance improvement.
table shows the performance improvement of the applications under four sets of con gurations.
we use nocache as a baseline and calculate the throughput improvement after applying cacheoptimizer cacheall and defaultcache .
the default cache con guration of pet clinic does not enable any cache.
therefore we only show the performance improvement of defaultcache for cloud store and openmrs.
using cacheoptimizer we see a throughput improvement of and for pet clinic cloud store and openmrs respectively.
the throughput improvement of applying cacheoptimizer is always higher than that of defaultcache and cacheall for all the studied applications.
figure further shows the cumulative throughput overtime.
we can see that for the three studied applications the throughput is about the same at the05010015005000100001500020000 time in secnumber of handled requests cumulative cacheoptimizercacheallno cache a pet clinic.
time in secnumber of handled requests cumulative cacheoptimizercachealldefault cacheno cache b cloud store.
time in secnumber of handled requests cumulative cacheoptimizercachealldefault cacheno cache c openmrs.
figure number of handled requests overtime cumulative .
table total number of possible places to add cache in the code and the number of location that are enabled by cacheoptimizer and that exist in the defaultcache .
object cache query cache total cacheoptimizer defaultcache total cacheoptimizer defaultcache pet clinic cloud store openmrs .
beginning regardless of us adding cache or not.
however as more requests are received the bene t of caching becomes more signi cant.
the reason may be that initially when the test starts the data is not present in the cache.
cacheoptimizer is able to discover the more temporal localities reuse of data in the workload and help developers congure the application level cache more optimally.
therefore as more requests are processed frequently accessed data is then cached which signi cantly reduces the overhead of future accesses.
we see a trend that the longer the test runs the more bene t we get from adding cache con guration code using cacheoptimizer .
we also observe that the performance of cloud store with defaultcache is close to the performance with no cache.
such an observation shows in some instances developers do not have a good knowledge of optimizing cache con guration in their own application.
cacheoptimizer enables a small number of caches to improve performance.
cacheoptimizer can help developers change cache con gurations quickly without manually investigating a large number of possible cache locations.
table shows the total number of possible locations to place calls to object and query cache apis in the studied applications.
we also show the number of cacheoptimizer enabled caches and the number of defaultcache enabled caches.
cacheoptimizer suggests adding object cache conguration apis to a fraction of the total number of possible cache locations.
in openmrs and cloud store where there are more hibernate queries cacheoptimizer is able to improve performance by enabling .
and of all the possible caches respectively.
for the object cache of cloud store cacheoptimizer even suggests enabling a smaller number of caches than defaultcache .
for large applications like openmrs with possible object caches and possible query caches manually identifying the optimized cache con guration is time consuming and may not even be possible.discussion.
in our evaluation of cacheoptimizer we observe a larger improvement in cloud store.
after a manual investigation we nd that cacheoptimizer caches the query results that contain large binary data e.g.
pictures.
since the sizes of pictures are often larger caching them signi cantly reduces the network transfer time and thus results in a large performance improvement.
we see less improvement when using defaultcache because most database access calls are done through queries like work ow in figure while the default cache con gurations of cloud store are mostly for object cache .
thus enabling only object caches does not help improve performance.
in openmrs both cacheoptimizer and defaultcache cache some database entity objects that are not often changed.
however cacheoptimizer is able to identify more object caches and queries that should be cached to further improve performance.
we also see that the overhead of cacheall causes openmrs to run slower when compared to defaultcache .
in pet clinic we nd that caching the owner information signi cantly improves the performance of searches.
moreover since the number of vets in the clinic is often unchanged caching the vet information also speeds up the application.
adding cache con guration code as suggested by cacheoptimizer improves throughput by which is higher than using the default cache con guration and enabling all possible caches.
the sub optimal performance of defaultcache shows that developers have limited knowledge of adding cache con guration.
rq2 what is the gain of cacheoptimizer when considering the cost of such caches?
motivation.
in the previous rq we see that cacheoptimizer helps improve application throughput signi cantly.
however caching may also bring some memory overhead to the application since we need to store cached objects in the memory.
as a result in this rq we want to evaluate cacheoptimizer suggested cache con guration when considering both the cost increase in memory usage and the bene t improvement in throughput .
approach.
in order to evaluate cacheoptimizer when considering both bene t and cost we de ne the gain of applying a con guration as gain c benefit c cost c where cis the cache con guration gain c is the gain of applying c while benefit c and cost c measure the bene t and the cost respectively of applying c. in our case study we measure the throughput improvement in order to quantify the bene t of caching and we measure the memory overhead in order to quantify the cost of caching.
we use the throughput and memory usage when no cache is added to the application as a baseline.
thus benefit c and cost c are de ned as follows benefit c tp c tp no cache cost c memusage c memusage no cache where tp c is the average number of processed requests per second with cache con guration c and memusage c is the average memory usage with cache con guration c. since the throughput improvement and the memory overhead are not in the same scale the calculated gain by equation may be biased.
therefore we linearly transform both benefit c and cost c into the same scale by applying minmax normalization which is de ned as follows x0 x xmin xmax xmin where xandx0are the values of the metric before and after normalization respectively while xmax andxminare the maximum and the minimum values of the metric respectively.
we note that if one wants to compare the gain of applying multiple con gurations the maximum and the minimum values of the metric are calculated by considering all the values of the metrics across the di erent con gurations including having no cache.
for example if one would like to compare the gain of applying cacheoptimizer and cacheall throughput max is the maximum throughput of applying cacheoptimizer cacheall and nocache .
to evaluate cacheoptimizer in this rq we compare the gain of applying cacheoptimizer cacheall and defaultcache against nocache .
the larger the gain the better the cache con guration.
if the gain is larger than the cache con guration is better than using nocache .
in order to understand the gain of leveraging cache con guration throughout the performance tests we split each performance test into di erent periods.
since a performance test with di erent cache con gurations runs for a di erent length of time see figure we split each test by each thousand of completed requests.
for each period we calculate the gain of applying cacheoptimizer cacheall and defaultcache .
we study whether there is a statistically signi cant di erence in gain between applying cacheoptimizer andcacheall and between applying cacheoptimizer and defaultcache .
to do this we use the mann whitney u test on the gains as the gains may be highly skewed.
since the mannwhitney u test is a non parametric test it does not have any assumptions on the distribution.
a p value smaller than .
indicates that the di erence is statistically signi cant.
we also calculate the e ect sizes in order to quantify the di erences in gain between applying cacheoptimizer and cacheall and between applying cacheoptimizer and defaultcache .
unlike the mann whitney u test which only tells us whether the di erence between the two distributions is statistically signi cant the e ect size quanti es the di erence between the two distributions.
since reporting only the statistical signi cance may lead to erroneous results i.e.
iftable comparing the gain of the application under three di erent con gurations cacheoptimizer cacheall and defaultcache gain cacheoptimizer gain cacheoptimizer gain cacheall ?
gain defaultcache ?
p value cli s d p value cli s d pet clinic .
.
large cloud store .
.
small .
.
large openmrs .
.
large .
.
large the sample size is very large the p value are likely to be small even if the di erence is trivial we use cli s d to quantify the e ect size .
cli s d is a non parametric e ect size measure which does not have any assumption of the underlying distribution.
cli s d is de ned as cli s d xi x j xi x j m n where is de ned the number of times and the two distributions are of the size mandnwith items xiandxj respectively.
we use the following thresholds for cli s d e ect size trivial if cli s d .
small if .
cli s d .
medium if .
cli s d .
large if .
cli s d results.
cacheoptimizer outperforms defaultcache and cacheall when considering the cost of cache.
table shows the result of our mann whitney u test and cli s d value when comparing the gainof applying cacheoptimizer with that of cacheall and defaultcache .
we nd that in all three studied applications the gain ofcacheoptimizer is better than the gain ofcacheall anddefaultcache statistically signi cant .
we also nd that the e ect sizes of comparing cacheoptimizer with cacheall ongain are large for pet clinic and openmrs .
the only exception is cloud store where the cli s d value indicates that the e ect of gain is small when comparing cacheoptimizer with cacheall .
on the other hand when compared todefaultcache cacheoptimizer has a large e ect size for both cloud store and openmrs.
discussion.
we investigate the memory overhead of applying cacheoptimizer cacheall and defaultcache .
we use the mann whitney u test and measure e ect sizes using cli s d to compare the memory usage between applying cacheoptimizer and the memory usage of having no cache cacheall and defaultcache respectively.
the memory usage of applying cacheoptimizer and having no cache is statistically indistinguishable for pet clinic and openmrs while for cloud store applying cacheoptimizer has statistically signi cantly more memory usage than having no cache with a large e ect size .
this may explain why we see larger throughput improvement in cloud store.
for openmrs the memory usage of applying cacheoptimizer and defaultcache is statistically indistinguishable.
finally when comparing cacheoptimizer with cacheall we nd that for pet clinic and cloud store the di erence in memory usage is statistically indistinguishable while for openmrs cacheoptimizer uses statistically signi cantly less memory than cacheall p value .
with an e ect size of .
large e ect .
nevertheless after considering both the improvement and cost cacheoptimizer out performs all other cache con gurations.
when considering both the bene t throughput improvement and cost memory overhead the gain of applying cacheoptimizer is statistically signi cantly higher than cacheall and defaultcache.
.
threats to v alidity external validity.
we only evaluated cacheoptimizer on three applications so our ndings may not generalize to other applications.
we choose the studied applications with various sizes across di erent domains to improve the generalizability.
however evaluating cacheoptimizer on other applications would further show the generalizability of our approach.
we implement cacheoptimizer speci cally for hibernate based web applications.
however the approach incacheoptimizer should be applicable to applications using di erent object relational mapping frameworks or other database abstraction technologies.
for example our approach for recovering the database accesses from logs may also be used by non hibernate based applications.
with minor modi cations e.g.
changes are needed to the de nitions of the tokens and transition functions in the coloured petri net cacheoptimizer can be leveraged to improve cache con gurations of other applications.
construct validity.
the performance bene ts of caching highly depends on the workloads.
thus we use performance tests to evaluate cacheoptimizer .
it is possible that the workload from the performance tests may not be representative enough for eld workload.
however cacheoptimizer does not depend on a particular workload nor do we have any assumption on the workload when conducting our experiments.
cacheoptimizer is able to analyze any given workload and nd the optimal cache con guration for different workloads.
if the workload changes greatly and the cache con guration is no longer optimal cacheoptimizer can save developers time and e ort by automatically nding a new optimal cache con guration.
for example developers can feed their eld workloads on a weekly or monthly basis and cacheoptimizer would help developers optimize the con guration of their caching frameworks.
to maximize the bene t of caching our approach aims to over t the cache con gurations to a particular workload.
thus similar to other caching algorithms or techniques our approach will not work if the workload does not contain any repetitive reads from the dbms.
our approach for recovering the database access.
prior research leverages control ow graphs to recover the executed code paths using logs .
we do not leverage control ow graphs to recover the database accesses from web access logs for two reasons.
first as a basic design principal of restful web services typically one web request handing method maps to one or very few database accesses .
second although leveraging control ows may give us richer information about each request it is impossible to know which branch would be executed based on web access logs.
heuristics may be used to calculate the possibility of taking di erent code paths.
however placing the cache incorrectly can even cause performance degradation.
thus to be conservative when enabling caching and to ensure that cacheoptimizer would always help improve performance we consider all possible database access calls.
our overestimation ensures that cacheoptimizer would not cache data that has a high likelihood of being frequently modi ed so the cacheoptimizer added cache con gurations should not negatively impact on the performance.
future research should consider the use of control ow information for optimizing the cache con gurations.
cache concurrency level.
there are di erent cache concurrency levels such as read only and read write.
in this paper we only consider the default level which is read write.
read write cache concurrency strategy is a safer choice if the application needs to update cached data.
however considering other cache concurrency levels may further improve performance.
for example read only caches may perform better than read write cache if the cached data is never changed.
future research should to add cache concurrency level information to cacheoptimizer when trying to optimize cache con guration.
distributed cache environment.
cache scheduling is a challenging problem in a distributed environment due to cache concurrency management.
most caching frameworks provide di erent algorithms or mechanisms to handle such issues.
since the goal of cacheoptimizer is to instruct these caching frameworks on what to cache we rely on the underlying caching frameworks for cache concurrency management.
however the bene t of using cacheoptimizer may not be as pronounced in a distributed environment.
.
conclusion modern large scale database centric web applications often leverage di erent application level caching frameworks such as ehcache and memcached to improve performance.
however these caching frameworks are di erent from traditional lower level caching frameworks because developers need to instruct these application level caching frameworks about what to cache.
otherwise these caching frameworks are not able to provide any bene t. in this paper we propose cacheoptimizer an automated lightweight approach that determines what should be cached in order to utilize such application level caching frameworks for hibernate based web applications.
cacheoptimizer combines static analysis of source code and logs to recover the database accesses and uses a coloured petri net to model the most e ective caching con guration for a workload.
finally cacheoptimizer automatically updates the code with the appropriate cache conguration code.
we evaluate cacheoptimizer on three open source applications pet clinic cloud store and openmrs .
we nd that cacheoptimizer improves the throughput of the entire application by compared to defaultcache andcacheall and the increased memory usage is smaller than the applications default cache con guration and turning on all caches.
the sub optimal performance of the default cache con gurations highlights the need for automated techniques to assist developers in optimizing the cache con guration of database centric applications.
.