a large scale study of test coverage evolution michael hilton carnegie mellon university pittsburgh pa usa mhilton cmu.edujonathan bell george mason university fairfax va usa bellj gmu.edudarko marinov university of illinois at urbana champaign urbana il usa marinov illinois.edu abstract statement coverage is commonly used as a measure of test suite quality.coverageisoftenusedasapartofacodereviewprocess if a patch decreases overall coverage or is itself not covered then the patch is scrutinized more closely.
traditional studies of how coveragechangeswithcodeevolutionhaveexaminedtheoverall coverage of the entire program and more recent work directly examinesthecoverageofpatches changedstatements .wepresent anevaluationmuchlargerthanpriorstudiesandmoreoverconsider anew importantkindofchange coveragechangesofunchanged statements.wepresentalarge scaleevaluationofcodecoverage evolution over builds of projects written in popular lan guages including java python and scala.
we find that in large mature projects simplymeasuring the change to statementcoverage does not capture the nuances of code evolution.
going beyond considering statement coverage as a simple ratio we examine how the set of statements covered evolves between project revisions.
we present and study new ways to assess the impact of a patch on a project s test suite quality that both separates coverage of the patch from coverage of the non patch and separates changes in coverage from changes in the set of statements covered.
ccs concepts software and its engineering software testing and debugging keywords software testing code coverage empirical study flaky tests acm reference format michael hilton jonathan bell and darko marinov.
.
a large scale studyoftestcoverageevolution.in proceedingsofthe201833rdacm ieee international conference on automated software engineering ase september3 montpellier france.
acm newyork ny usa 11pages.
introduction codecoveragemetricsareoftenusedbydeveloperstoidentifyhow well tested an application is.
there are a wide variety of coverage metrics including statement branch mc dc method file and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september montpellier france copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
coverage .
statement coverage the ratio of statements executed by tests divided by total number of statements is the simplest but most commonly used.
modern development workflows often use a continuous integration ci service to build every push and run their project s tests.
as ci services have become widely adopted ancillary services that track additional metrics from ci such as code coverage are also becoming more popular.
developers can configure ci services such as travis to post code coverage data to a service such as coveralls whichmakesthedataeasilyavailabletodevelopers forinstancewhenreviewingpullrequests.coverallsmaintainsa recordofaproject scoverageovertime.additionally coverallscan automatically update coverage badges that display the latest coverageresultonaproject shomepage .thisgrowingpublicrecord of coverage data organically collected by developers provides a great opportunity for researchers to study real code coverage data.
to the best of our knowledge we are the first researchers to study coverage data collected and so widely shared by developers.
while the overall statement coverage of a test suite provides some insight into its in completeness it reduces the quality measure to a single ratio making developers potentially miss valuable informationabouttheirtestsuiteanditslimitations.fordevelopers oflarge stableprojectsthathavealargenumberofstatements it isoftendifficulttorecognizeanynoticeablechangeinthismetric fromonecommit patch toanother e.g.
foraprojectwith1million lines1of code a change in coverage of even lines would onlyimpactcoveragebyonehundredthofonepercentagepoint.
nonetheless thesechangescanaddupovertime althoughasingle100 linepatchmaynotmakeanoticeablechangeincoverage many small patches can make such a change.
even more concerning coverage of some lines may change non deterministically due to inherent non determinism in the tests .
even in smallerprojects whereanincreaseintheoverallcoveragemight bemorenoticeable trackingonlythissimpleratiodoesnotcapture whichstatements are covered .
in an extreme case a project with50 codecoveragecouldmaintainthatoverallcoveragewhile completely flipping the set of statements covered.
coverage can alsoincrease seeminglyindicatingabettertestsuite evenwhen that is not necessarily the case e.g.
coverage might go up despite a drop in the number of executed statements if code is removed decreasing the total number of statements even more.
one approach for gaining better insights from statement coverage is to focus not on the coverage of the entiresystem under test sut but instead onlyonthe coverageofeachpatch changed statements performed on the sut .
collecting patch coverage can be useful because if a patch is not covered enough 1this paper uses line s and statement s interchangeably.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france michael hilton jonathan bell and darko marinov 2b.
extract data from coveralls api1b.
github coveralls mining1a.
literature search2a.
clone and build projects collect coverage3.
analyze diffs and coverage per build4.
visualize aggregate data projects29 projects builds5 builds47 projects buildsidentifying projects collecting coverage synthesizing resultsgenerating visualizations figure overview of methodology.
compared to prior studies green is new yellow is nearly an order of magnitude larger.
thendeveloperscaneasilyflagthispatchincodereviewandrequire more tests to be added with the patch.
however even if a patch is well covered the impact of the patch on the non patch unchanged statements part of the sut is not known a priori.
priorempiricalworkhasevaluatedhowcoveragechangesduring code evolution but for a relatively small number of projects and builds.
for example zaidman et al.
reported a high amount of manuallaborwhenstudyingjust3projects andcollectingcoverage for a total of builds while marinescu et al.
reported an immense amount of infrastructure needed to collect coverage data from just projects for a total of builds .
further while prior work has investigated thechange in overall coverage or the coverageofpatches nopriorempiricalstudyhasexaminedhowthe setoflinescovered changes.evenwhentheoverallsutcoverage appears stable and the coverage of new patches is high are the actual lines covered still changing?
or are there hidden changes to coveragethatdevelopersshouldbeawareof beyondcoverageof the entire sut and coverage of the patch?
to better understand codecoverage how it changes and how developerscanbetterreasonabouttheircodeandtheir tests we present a large scale longitudinal study of test coverage evolution.
ourstudybuildsonpriorempiricalworkstudyingcodecoverage but increases the number of projects written in different languages andrevisionsofthoseprojects byalmostanorder ofmagnitude.also oursisthe firststudyofcodecoveragetocorrelatethecoverageofindividuallinesofcoveragethroughoutproject evolution tracking how a line may become covered and then later becomenotcovered.further oursisthefirststudytoutilizecode coveragehistorythatdeveloperscollectoncoveralls gathered through the public api of the coveralls service in addition to coverage metrics that we collect ourselves.
in this paper we answer the following questions rq1 whatisthedistributionofpatchcoverageacrossrevisions?whatfractionofapatchiscoveredbytheregression testsuite?doprojectswithhighpatchcoveragealsohave high overall coverage?
rq2 whatimpactdopatcheshaveonthecoverageofnonpatch code?
do we see similar behavior across projects?
does high patch coverage imply that a patch increases coverage of non patch code?
rq3 areallchangesequallyvisible?
doprojectshavechanges that are occluded hidden ?
rq4 how does the set of covered lines change?
are there hot spots of coverage change i.e.
lines that flip between being covered and uncovered throughout evolution?
rq5 what kinds of changes to code drive changes to coverage?doescodecoveragechangemorebecauseoldcode becomestested orbecausenew testedcodeisadded?or do line deletions drive changes to code coverage?to answer these questions we prepare an extensive dataset of code coverage develop a toolset for automated analysis and perform various analyses making the following contributions dataset our dataset of code coverage information from revisions of projects is publicly available toolset ourtoolsetincludesthescriptsthatgatheredour dataset and that can analyze the dataset allowing otherresearcherstoperformsimilarexperimentsandbuildnew tools that analyze coverage change.
novelcoverage wehighlighttheimportanceofmeasuring change in coverage of unchanged code.
results weperformseveralanalysesandpresentnewfindingsthatanswerthelistedresearchquestions.onefinding is thatwhichlines arecovered can varywidely in aproject even when the overall coverage appears to remain the same.
implications weidentifyimportantimplicationsfordevelopers toolbuilders andsoftwareengineeringresearchers workingtomeasureandimprovetestquality.forinstance wefoundthatchangestonon codefilesoftenimpactcode coverage andhence regressiontestselectiontoolsshould track such dependencies to be safe.
methodology researchershavepreviouslystudiedtheevolutionofcodecoverage bydownloadingsomeopen sourceprojectsand for several revisions of these projects compiling the code and running testswhilecollectingcodecoverage.ourmethodologybuildson this approach but significantly expands the breadthof the study by including almost an order of magnitude more projects and revisions .
we leverage coveralls the increasingly popular service for tracking code coverage for open source projects.
we also significantly extend the depthof the study by tracking the change to coverage of individual statements across revisions.
figure1showsanoverviewofourmethodology.wefirstidentified candidate projects to include in our study selecting both projectsstudied inrecentregressiontesting researchandprojects thatusethecoverallsservice.wethencollectedcoverageforthese projects either running the test suites ourselves or collecting data fromcoveralls.wenextaggregatedthedatawithversion control historytotrackthecoverageofindividualcodelinesthroughout project evolution.
we finally summarize and visualize the results.
.
identifying projects automatically downloading compiling and executing tests for open sourceprojectsisoftennon trivial.someprojectsfailtocompile e.g.
due to missing dependencies and others require manual configuration or installation of external dependencies.
however authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a large scale study of test coverage evolution ase september montpellier france excludingprojectsthatrequiresomedegreeofmanualconfigurationcouldbiastheprojectsincludedinastudy.wereliedontwo complementary approaches to gather a diverse set of projects.
traditional evaluation we identified four recent research papers thathadexperimentswithsoftwareevolution specificallywithregressiontesting .wetabulatedtheopen source projects studied in these experiments and tried to clone build andtesteachofthem.intotal weselected29projects ofthe32 from of the from of the from and of the from .
we included all projects which used the maven build system and successfully compiled on its most recentcommit withthecommand mvn package .weallowedforup tothirtyminutesoftroubleshootingperprojecttopotentiallyinstall external dependencies required by the project as may have been specifiedinreadmefilesorerrormessagesduringthisprocess.
these projects make up our traditional evaluation set.
in vivoset we broadened the scope of our evaluation by includingprojectswhichwe didnotcompileortestourselves instead leveraging coverage data collected and shared by the project developers themselves.
coveralls is a free service that stores coveragedata allowingdeveloperstotrackthecoverageoftheirprojects overtime.manyopen sourceprojectsfromgithubusecoveralls as part of their continuous integration ci pipeline when developerspushtheirchangestogithub aciservice e.g.travisci automatically fetches these changes compiles the project runs the testsuite anduploadscoveragedatatocoveralls.whilesomeprior work hasusedtheoutputofci services liketravisci asadatasetforevaluation wearenotawareofanypriorworkthat used code coverage from services like coveralls as we do.
reusing coveragedatahasseveraladvantages wecanincludeprojects thataremorecomplextobuild forwhichdevelopershaveprovided automated configuration scripts to a ci service we can include projects writteninany languagesupported bycoveralls because it abstracts the actual collection of code coverage and we need notexpendresourcescompilingandrunningtheseprojects tests.
we refer to the data from coveralls as our in vivoevaluation set because the coverage results come directly from the field.
to identify projects for our in vivodataset we started by crawling github to find projects that use both travisci and coveralls services.
we searched github by project language including most popular and more recent languages in our criteria looking for projects with configuration files that refer to coveralls collecting the most starred projects per language meeting our criteria.
foreachprojectthatreferredtocoveralls wequeriedthecoveralls public api to detect if the project indeed has publicly available coverage data on coveralls.
for each project that had data we checked the number of builds for which the project shipped coverage data to coveralls and the number of linesof code in the most recentversionoftheproject.wethenpickedarbitrarythresholds tofilteroutprojectswithshorthistoriesoncoveralls lessthan250revisionsbuiltandtested ortriviallysmallprojects lessthan1 lines of code total leaving projects.
.
collecting coverage traditionalevaluation weconductedourtraditionalevaluation bycompiling testing andcollectingcoverageoneachofthe250 most recent commits of the projects that we had identified successfully completing a total of builds.
we ran these builds on a cluster of ubuntu .
virtual machines running apachemaven .
.
and java .
.0 131. we collected coverage using the maturejacocotool configuredtocollectcoverageofallproject codefiles.ifabuildfailed wedidnotseekoutmorebuilds hencewe may not have successful builds of each project.
we considered othercoveragetools cobertura andclover butneither fullysupportsjava8 andhence wouldhavelimitedourstudyto include only projects that do not use recent java features.
in vivoset coverallstermsofuserequestbroadlythatusersof theirapidonotimposeanundueloadontheservice andwedid not want to abuse the service.
unfortunately collecting our datarequired making many requests to the service one per file perrevision per project.hence aprojectwith1 000fileswouldrequire requests to collect detailed coverage of each file of a single revision.weself imposedaratelimitof5 000requestsperhourand restricted our data collection to only several days.
we contactedcoveralls to inquire if they could make the data easier to obtain e.g.
one request for all files in a revision and to check that our procedurewould notplacean undueloadontheir service butwe receivednoresponse.hence forthe19projects wedownloaded coverage data for only builds we skipped the remaining builds to not abuse the service.
.
determining code changes the next step in our study required unifying the coverage data with code change information.
for each commit of each project weneededtofindwhichlineswereadded modified orremoved fromthepreviouscommit.becauseourexperimentsincludeonly projectsthatusethegitversion controlsystem itwasrelatively straightforwardtocollectcodechangeinformationusing git diff .
using the diffalgorithm we simplified the potentially rather complexprocess oftracking linesthatmay havemoved orshifted throughoutthecodebase .wematchedeachcommitwithits parentcommitforwhichwehaddata usinggitinformationtotrack branchingandensuringthateachcommitwasproperlymatched with a prior commit from which it descended.
we compare each commit with its parent obtaining the list of added and removed lines.
for the remaining unchanged lines we built a mappingbetween the line numbers from the two commits which is nontrivial when new lines are added or old lines deleted from files making it difficult to identify where a line from a prior commit isinthenextcommit.weused difftogeneratethesemappings invokingitforeachchangedfiletodeterminethenewlinenumber of each line from the previous commit of that file.
.
aggregating results finally weaggregatedcodecoverageandcodechangeinformation and generated visualizations by creating and running a series of r scripts.table 1showsasummaryofbasicstatisticsforeachproject.
for the remainder of this paper we refer to projects by their id the far left column .
for each project we report the programming language the prior paper or coveralls abbreviated as c.io the numberofbuildsstudied averagelinesofcodeacrosscommits and thetotalcommittimewindowthatourcoveragedataspans.the coveralls projects are mostly smaller in loc than the rest but nonethelessaresimilarinoverallcoverage.beforeaddressingour authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france michael hilton jonathan bell and darko marinov table key statistics describing all of the projects included in this study.
coverage of commits changing avg.
patch size lines project lang source builds loctime range months start sparkline end test source both neither source test all p01apache commons collections java p02apache commons dbcp java p03apache commons exec java p04apache commons functor java p05apache commons io java p06apache commons jxpath java p07apache commons math java p08apache commons net java p09apache commons validator java p10apache empire db java p11apache httpcore java p12armmbed mbed ls python c.io p13bitwalker timex elixer c.io p14broadinstitute firecloud orchestration scala c.io p15containers virtcontainers go c.io p16coreos alb ingress controller go c.io p17damianszczepanik cucumber reporting java p18dask dask python c.io p19doanduyhai achilles java p20dropwizard dropwizard java p21ebay cors filter java p22f5networks k8s bigip ctlr go c.io p23fasseg exp4j java p24gillespie59 eslint plugin angular node c.io p25goldmansachs gs collections java p26google jimfs java p27hazyresearch deepdive scala c.io p28hector client hector java p29ikawaha kagome go c.io p30ilovepi compiler dotnet c.io p31jhy jsoup java p32jknack handlebars.java java p33jodaorg joda time java p34joel costigliola assertj core java p35mailgun kafka pixy go c.io p36mitlibraries topichub scala c.io p37platinumazure eslint plugin qunit node c.io p38pragtob benchee elixer c.io p39raml org raml java parser java p40shiftforward apso scala c.io p41spatialmodel inmap go c.io p42square okhttp java p43square retrofit java p44steamdatabase valveresourceformat dotnet c.io p45terasolunaorg terasoluna gfw java c.io p46undertow io undertow java p47zxing zxing java total projects loc average fiveresearchquestions wepresentthreedemographicquestions dqs that describe the overall composition of our dataset in terms of patches and overall coverage.
dq1 dopatchestouchbothcodeandtests?
thisquestionmirrorsaquestionoftenstudiedinthecontextofcodecoverageand miningsoftware repositories .weexaminedeach patchof eachproject lookingatthefileschangedbyeachpatch.wecategorizedeachfileasatestcodefile sourcecodefile oranon code file.
code files were defined as ending with the correct suffix given theprojectlanguage .java .scala .go .js .ts .cs .ex .exs .py.following prior work code files were then categorized as test or source code files if their path contained test or .spec .
table presentstheresults.similartopriorstudies wefoundthatfew commits modified only test files far more common were commits that modify both test and non test files or only non test files.dq2 whatare the sizesof eachpatch?
this question also mirrors one posed previously .
if a project has primarily small patches thenthesepatchesareperhapseasierforhumanstoreasonabout.however ifthosepatchesarelarger hundreds orthousandsoflines theymayrequiredifferentapproachestobereasonedabout.to answer this question again we categorize files as source code or testcode andcomputethenumberofchangedlinesineachfile.
this includes all changes to these files and counts each edited line asachange.table 1showstheresults avg.patchsize lines including allchanged lines in the all column in source code test code or non code files .
our results strikingly differ from those of marinescu et al.
their study of six c c programs reported a median number of patchlinesrangingbetween4and7.whilethedifferencemaybe partly due to us counting allchanged lines not only executable statements this alone is unlikely to lead to such a significant differenceintheextremecasesofprojectssuchas p11 p15 and p45.
we believe that this indicates that many of the projects that we studied were under substantially more active development thanthe mature projects in their study gnu binutils git lighttpd memcached redis and mq .
this finding also underscores the importanceofsamplingadiversesetofprojectsinempiricalstudies.
dq3 how does coverage change over time?
finally wecalculatedthecoverageforeachprojectacrossallcommitswestudied.
table1reportstheaveragelinesofcode loc andcoverageofeach authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a large scale study of test coverage evolution ase september montpellier france p1p2p3p4p5p6p7p8p9p10p11p12p13p14p15p16p17p18p19p20p21p22p23p24p25p26p27p28p29p30p31p32p33p34p35p36p37p38p39p40p41p42p43p44p45p46p47 percent of builds satisfying patch coverage at level indicated by color figure2 coverageofnewlinesineachpatch.eachbarrepresentstheproportionofbuildswiththatcoveragelevel asdenoted by the color .
the black bar indicates the average overallcoverage of each build per project against the percentage scale projectatthestartandendofourdataset alongwitha sparkline visualization showing how the coverage changed over time scaled to0 .weseethatourdatasetcontainsadiversesetofprojects some with low coverage others with high some with little change otherswithmore.formostprojects coverageremainsrelatively flat during the studied window similar to prior results .
however from the sparklines we see that some projects in particular p23 p27 and p44 havespikesintheircoverage where coverage drops significantly and then returns to its prior position.
after manual inspection it appears that this is often caused by broken tests if a test fails early in its execution then it does notcontinue to run and cover the statements that it would typically cover.unfortunately whilewehadtestresultinformationfromour own experiments we did not have easy access to test results from coveralls to filter out failed tests.
it is perhaps unsurprising that we do not observe significant changes to coverage given that our projects are non trivially large averaging lines of code.
the most visible spikes e.g.
p23 p27 are in projects with the fewest lines of code while the largest projects p07 p25 and p46 appear nearly flat.
given that statementcoverageissimplytheratioofexecutedstatementstototal statements increasing coverage by even one percentage point may requirecoveringthousandsoflinesofpreviouslyuncoveredcode.
hence onaday to daybasis developers especiallyoflarge mature projects are unlikely to see changes in total coverage.
results rq1 what is the distribution of patch coverage across revi sions?
since it is difficult to observe changes to overall project coverage on a day to day basis prior work as well ascurrenttools haveadvocatedthatdeveloperspayparticularattentiontocoverageofpatches.weusetheterm patch commit andchange setinterchangeably aunitthatrepresentsadeveloper s changestocode withoutattemptingtounderstandthenatureof the change as a bug fix or new feature .
given that patches are generally much smaller than the overall codebase patch coverage might be more meaningful to developers reviewing a patch.
to study the coverage of patches in thewild we calculated the coverage of all changed statements in each patch in our dataset.
to visualize these results we binned each patch by its coverage choosing bins of and coverage of the patch.
figure 2shows the distribution of patches in each bin by each project.
we also visualize the average coverageacrossallversionsofallofthecodeineachprojectwitha black bar.
for example for the project p01 almost of the code patcheshave100 coverage.however theoverallcoverageofall code across all builds for this project is .
thisvisualizationissimilartoonecreatedbymarinescuetal.in their study of patch coverage of six projects with two distinctions weaddtwomorebins and tosegregatepatchesthat are fully covered or not at all covered rather than simply and we superimpose the overall project coverage.
adding these two additional bins allows us to recognize that in fact patches are often either entirely covered or not at all covered it is far less frequent in the projects that we studied to observe patches that were partially covered.
whileitmightseemintuitivethathigherpatchcoverageimplies higher overall coverage when we look at our data we did not seeevidenceofthis.fromthischart wecanobservethathaving morepatcheswithhighercoveragedoesnotalwaysindicatehigher overall coverage.
for example when comparing p03and p02we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france michael hilton jonathan bell and darko marinov p01p02p03p04p05p06p07p08p09p10p11p12p13p14p15p16p17p18p19p20p21p22p23p24p25p26p27p28p29p30p31p32p33p34p35p36p37p38p39p40p41p42p43p44p45p46p47 patches with changes to code files patches with no changes to code files increase increaseno impactno impactdecreasedecrease figure for each patch we show whether it increases decreases or has no impact on coverage of existing non patch code.
the size of each bar represents the percent of patches in that bucket.
seethateventhough p02hasahigherpercentageofpatcheswith more coverage it has lower overall coverage than p03.
to better understand the relationship we tested the correlation between patchcoverageandoverallcoverage.wecomputedthekendalltau coefficientbetweenpatchcoverageandoverallcoverageforeach patch.
we found that there was no correlation between the two variables .
finding patchcoveragevarieswidelybetweenprojects.patch coverage does not correlate with overall coverage.
rq2 what impact do patches have on the coverage of existing non patch code?
while patch coverage considers how well tested a patch may not be it surely cannot be the only criteriausedtojudgetheimpactofthatpatch.forinstance apatch might have patch coverage but applying that patch might reduce the coverage of existing code.
hence to better understand the impact of each commit we also look at the effect that each commit has on the unchanged existing non patch codeintheproject.wecategorizedtheimpactofeach commitonexisting non patch codeasanet increasetothenumber of existing lines covered net decrease or having no impact.
upon apreliminaryinvestigation weobservedthatmanyofthecommits whichincreasedordecreasedcoverageinnon patchcodecontained no changes to code themselves.
hence we further separated each ofthesegroupsintopatcheswithandwithoutchangestocodefiles.
this statistic is complementary to patch coverage when reviewing a patch in addition to seeing that the patch is covered or not developers can also see if this patch increases or decreases thecoverageoftherestofthecodebase.ratherthanlookingattotalcoverage of both the new and the existing code by separating the coverage of a patch from the coverage of existing code we can observeinstanceswhereoverallcoveragemightgoup forinstance becauseapatchcontainedaverylargenumberofnewlycovered lines butcoverageofnon patchcodemightgodown becausethat patch removes calls to existing code .
figure3shows the impact of each commit on non patch code coverageforallcommitsforeachprojectinourcorpus.itisinterest ingtonotethatdifferentprojectshaveverydifferentprofiles.some projects suchas p02havemanycommitstonon codefileswhich nonetheless have an impact on coverage.
other projects such as p47havealmostnosuchcommits wheremostcommitsdotouch code files and do impact coverage of existing code.
uponmanualinspection wefoundthatmanyofthesenon code changes involvechanging configurations.
thesechanges could be causingchangestocoverageduetodifferencesbetweendifferent versionsofapisorothernon codechanges.
p46containedmany non code changes one example commit message describes thechange as fix build on latest jdk9 and the only changes are to theproject spom.xmlfile .thesechangestocoveragecouldalso be due to non determinism rather than intentional changes.
todetermineifthereisarelationshipbetweenpatchcoverage andnon patchcoverage weperformastatisticalanalysis.foreach commit in our data we look at all of the patches which have at least one statementin their diff.
we then computedthe change to non patchcoveragebycalculatingtheratiobetweenthenumber of non patch lines hit and the total number of non patch lines.
wecomputedthepearson scorrelationcoefficientbetweenpatch coverage and non patch coverage for each patch.
we found no authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a large scale study of test coverage evolution ase september montpellier france p01p02p03p04p05p06p07p08p09p10p11p12p13p14p15p16p17p18p19p20p21p22p23p24p25p26p27p28p29p30p31p32p33p34p35p36p37p38p39p40p41p42p43p44p45p46p47 figure percentage of commits size of bar with statements changing coverage color of bar even when the total project coverage did not appear to change.
correlationbetweenthetwovariables concluding thatcoverageofapatchisnotcorrelatedwiththepatch simpact to coverage of existing non patch code.
finding patches often impact coverage of existing nonpatch code and high patch coverage does not correlate with increasing non patch coverage.
rq3 areallchangesequallyvisible?
whendevelopersobserve thatoverallcoveragehasnotchangedbetweendifferentbuilds they might naturally assumethat there have not been changesto what their tests execute .
however it is possible that the linescovered might change perhaps drastically even if the total project coverageappearstobethesame.wecallthesechanges occluded and study their prevalence in our dataset.
to do so we filtered our data to examine only commits where there did not appear to beany change to coverage from the prior build specifically wherethe difference in coverage was less than .
percentage points and then calculated the number of statements changing coverage.
figure4showshowmanyoccludedchangesweobservedperproject per commit eachcoloredbarrepresentsarangeofoccluded statements and the size of the bar represents the percentage of that project s commits at that level.
we observe that the number of occludedchangesvarieswidelybyproject theyareveryprevalent in some projects and uncommon in others.
however every projecthadatleastonecommitwheretherewereoccludedchanges indicating that steady coverage does not imply no change to codecovered.welookedcloserat p25 p35and p46 whichalways hadoccludedchanges.wefoundthat p25containedalargeamount ofgeneratedcodethatoftenchangedbetweencommits butthesizeof that generated code remained stable making coverage appear to bestable.wefoundthatgeneralnon determinismin p35and p46 caused the lines covered to vary.
finding even when patches appearto leave coverage unchanged the set of lines covered can still vary widely.
developers should not trust a seemingly steady coverage metric to indicate that the samelines are continuously covered.
rq4 how does the set of existing lines covered change?
in ourpriorquestions weconsideredhowapatchiscoveredorhowitimpacts the coverage of existing code.
here we study the coverageof individual lines changing over time.
each time that tests are run the coverage of a line might flip from covered to uncovered or uncoveredtocovered.foreachcommit weexamineeachflipped lineinourdataset.todoso weusedthe difftooltoidentify alllinesthatexistin everyversionofeachprojectstudied and a mapping foreach ofthose linesin each revisionto theequivalent line number in the most recent revision.
we used this global id to track the position of each line over time and then computed the number of times that each of these lines flipped coverage.
figure showsthedistributionofthenumberofflipsforeachoftheselines.
we see lines which only flip once all the way up to a single line whichflipscoverage128times.alinewithahighflipcountislikely covered non deterministically and hence its coverage might be lessimportantfordeveloperstofollowonaday to daybasis.we notethat p25and p46 whichhadmanyoccludedchanges alsohave lines with many flips in coverage.
werandomlyselectsomeofthelineswithmanyflipstobetter understand why lines are flipping coverage.
in one a line in p20 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france michael hilton jonathan bell and darko marinov p01p02p03p04p05p06p07p08p09p10p11p12p13p14p15p17p18p19p20p21p22p24p25p26p27p28p29p30p31p32p33p34p35p36p38p39p40p41p42p43p44p45p46p47 figure distribution of how many times coverage flipped for unmodified lines throughout all revisions of each project showing only lines that changed coverage at least once.
the x axis displays the number of covered uncovered transitions.
dbihealthcheck.java the high flip statement checks to see if a database connection is still open at a regular time interval.in some cases tests might complete before this health check isscheduled causing it to not be executed.
in another case a line inp08 tftpserver.java wefoundahigh flipstatementthat was dependent on a network socket s state and would detect lost packets.
the coverage of this line is dependent on whether packetsarelostintransmissionduringthetestcase whichoccurs non deterministically.
finding lines may often flip between covered and uncovered suggestingnon determinisminthetestsuite.toolbuilders should consider how to best track and represent this nondeterministic coverage.
rq5 what kindsof changes to code drive changesto cover age?
weconcludeourstudybyreturningtoproject levelcoverage lookingat what kindsof changes causecoveragetochange.
atraditional viewpoint might be that coverage increases because existing linesofcodebecomecovered andcoveragedecreasesbecausethoselinesarenolongercove red.however ofcourse lookingataspecific revision of a project compared to the prior coverage can change foravarietyofreasons.forinstance addingnewlineswillcause coverage to increase or decrease depending on the coverage of the newlinesthatareaddedtothecode.likewise deletinglinescan alsoimpactcoverage.ifthedeletedlineswerecovered itcancause coveragetodecrease andifthedeletedlineswerenotcovered it cancausecoveragetoincrease.coveragecanalsochangedueto change in coverage of unchanged lines.
figure6showsalloftheseimpactfactorsforallrevisionsofeach project.byidentifyingthedifferentimpactfactorsandwhatroletheyplayineachprojects wecanmakeobservationsabouthow theprojects andtheir testcode are changingovertime ratherthansimplyobserving coverageincreased or coveragedecreased .for someprojects e.g.
p38 weobservethatover75 ofthechanges to coverage are because of new lines being added.
this suggests that most of the code changes are coming from new development.
however forotherprojects e.g.
p02 weobservethattheyexperience many changes when coverage is lost or added to existing lines.thisseemstopointtohigherlevelsofnon determinismin that project s tests especiallywhen there are a similar number of changes adding and losing coverage.
comparing to figure w e observethat projectswith lines thatflip coverageoften p02 p07 p25 p42 p46 also have significant numbers of coverage changes driven by changes to existing code.
finding many factors have an impact on coverage newly coveringexistingstatementsisnotalwaystheprimarydriver to coverage change.
discussion in this section we discuss our findings presenting implications fordevelopers tool builders andresearchers and discuss several limitations of our study.
.
implications developers betweensubsequentrevisions thereisoftenverylittle observable change in overall project coverage especially for large projects.
because of this developers use tools that examine patch coverage.
however while knowing if a patch is coveredornotclearlyhasvalue developersshould notusepatch coverage as a stand in metric to evaluate the impact of a patch authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a large scale study of test coverage evolution ase september montpellier france p01p02p03p04p05p06p07p08p09p10p11p12p13p14p15p16p17p18p19p20p21p22p23p24p25p26p27p28p29p30p31p32p33p34p35p36p37p38p39p40p41p42p43p44p45p46p47 deleted tested lines added new lines that are not coveredcoverage lost on existing linesadded coverage to existing linesadded new lines that are covereddeleted untested lines figure overview of key factors driving change to coverage of each project during the entire development period measured.
size of bar represents how much that factor contributed to the net change in coverage.
green factors increase coverage reddecrease coverage.
on the overall project coverage.
we found that patches often impactthecoverageofexistingnon patchcode andimportantly that having high patch coverage does notcorrelate with increasing the overallcoverageofaproject.developersshouldadjusttheircode review processes to consider not only the patch coverage but also itseffectonnon patchcodecoverage.moregenerally developersshould considerusing more detailedmetrics than justthe ratio of statements covered to measure their code s testedness.
developersshouldbeawarethatevenifthe numberoflinescoveredremainsrelativelystatic thesetofcoveredlinescangreatly vary betweenruns.
ifthere isconcernthat aparticular partof the project is covered developers need to specifically track that the relevant lines in that part are covered because we found that therecanbeasignificantchurninthesetoflinescovered.wefoundthat insome projects individual statementsmight changetheir coverageveryoften perhaps everysinglebuild.hence developerswho closely track coverage must be aware of inherent non determinism in coverage.
if developers have a better understanding of how and wheretheircoverageisnotdeterministic theymaybebetterprepared to address other impacts of this non determinism such as flaky test failures.
tool builders we observed that many commits do not involve changestocodefiles yetthesechangescanstillchangethecoverageofexistingcode afindingwithimplicationsfortoolbuilderswhose tools rely on code coverage for instance regression test selection tools .toolbuildersmustbeawareofallinputs e.g.configanddatafiles as well as non determinism that can impact test execution not only the code itself.
ourstudyalsohasimplicationsfortoolbuilderscreatingcode coveragetools.otherresearchers havealsoidentified non deterministicbehaviorwhenstudyingtests.webelievethat toolbuildersshouldbuildtoolstohelpdevelopersidentifywhich testsaredeterministic andwhicharenot.currently thereisnoway for developers to identify if their tests are covering code in a deterministic or non deterministic manner without rerunning tests and comparingverylowlevelcoveragedata orwaitingforatesttofailinanon deterministicmanner andhavingtotrackdownthesource oftheproblem.tohelpdevelopersidentifynon deterministicbehavior tools should show developers which lines have changesin coverage even unchanged lines and which of those changesare non deterministic or at least potentially non deterministic .
this information would be a valuable tool for developers when debugging flaky test failures .
we also found that there are various reasons why coverage can increaseordecrease.currenttoolsshowthechangeincoverage but they do not show whythere was a change.
if developers are awareofhowtheircoverageischanging andtheyarenotexpecting ittochangebasedontheirlatestcommit theycanthenevaluate whether their tests are flaky or if their latest change impacted thesystemcoverageinwaysthattheywerenotexpecting.either way havingthisinformationcanhelpdevelopersbetterunderstand their system and the state of their automated test coverage.
code authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france michael hilton jonathan bell and darko marinov coverage tools should fuse code change information with code coverage information as the operias tool also proposed.
researchers our study was the first to make use of the coveralls service to gather code coverage data.
code coverage dataisnotoriouslyhardtocollect becauseolderprojectversions are often hard to compile and run and collecting coverage takes machinetime.fortunately coverallsbothcollectsandmakesavailable real data from many different types of projects.
this data can be invaluable for researchers who wish to better understand code coverage.
by leveraging this data researchers can use coverage datawithouthavingtocollectandrunhistoricalversionsofsoftware whichmaybeverydifficultduetomissingdependenciesand other infrastucture related problems.
we expect that analysis of data from coveralls can lead to new insights similar to how analyses of github and travisci data enabled researchers to obtain new insights into software development.
.
threats to validity construct are we asking the right questions?
to ensure we are askingtherightquestions webaseanumberofourquestionson previous research.
for our new research questions we posed these questions before looking into our data based on our anecdotal experiencefromourowndevelopmentnoticingthatcodecoverage can vary greatly.
internal did we skew the accuracy of our results with how we collected and analyzed information?
we chose projects that were used in previous research enhanced with a set of large projects with diverse demographics collected from coveralls.
to provide confidencethatwehavenotskewedtheresultsandallowforgreaterscrutiny wehavemadeallofthescriptsthatwewroteandthedata we collected available with this paper.
external doourresultsgeneralize?
tohaveourresultsgeneralize as much as possible we selected a large and diverse set of projects invarious languages andfrom varioustypes ofapplications.our datasetisalmostanorderofmagnitudelarger bothintermsofthe number of projects and the number of builds than prior related studies whichisencouraging.alloftheprojectsareopensource so wecannotmakeanyclaimsabouthowourresultsmightgeneralize to proprietary projects.
replicability can others replicate our results?
to support others in replicating our results we have made our data and the r scriptsthatweusedtoprocessourdatapubliclyavailable.these canall befound ontheproject s companionwebsite code coverage.org related work previous coverage studies.
we are not the first researchers to studycodecoverageofsoftwareprograms.elbaumetal.
study two systems using different types of code coverage metrics.
the authorsfindthattheimpactofchangesoncoverageinformation canbedifficulttopredict butcallsforfurtherstudyoftheeffects of software evolution on coverage information is needed.
zaidman et al.
study three systems and observe changes to coverage.thepaperreportsthatthereareperiodswhenthetests and code evolve together but there also are periods of intense testing.
the paper also suggests future work should include analyzing more and larger cases to better understand test coverage evolution.the most related work to ours is from marinescu et al.
.
theauthorspresentbothatoolanddatasetofcodecoverage.to evaluate the tool the paper uses six c c systems.
the paper answersnineresearchquestions threeofwhicharerepeatedinthis paper.marinescuetal.
sworkwasalsothefirsttospecificallyfocus onpatch coverage although otherresearchers havedeveloped tools to help developers visualize patch coverage.
in this paper we examinebothpatchcoverageandalsonon patchcoverage anduse a significantly larger dataset.
other coverage work.
coverage has often been used as a metricwhenstudyingsomepropertyofasystem.kochharetal.
find that code coverage has an insignificant correlation with thenumber of bugs that are found after the release of software atthe project level.
mokus et al.
find that test effort increases exponentiallywithtestcoverage butthereductioninfieldproblemsincreaseslinearlywithtestcoverage.theysuggestthattheoptimal level of coverage for most projects is likely to be well short of .
ahmed et al.
study the relationship between statement coverage and mutation score.
they find that both metrics have only a weak negative correlation with bug fixes.
memon et al.
describe the challenge when dealing with a codebase the size of google s and how it is not possible for them to collect coverage at thatscale.pintoetal.
haveusedcoveragetostudyhowtests evolve over time.
one category of test evolution they identify is coverage augmentation tests.
gao et al.
investigate differences betweenunittesting systemtests andinvariantdetection.they find that when executing system tests there is often significantnon determinism in the lines that are executed by each test.
our results confirm this finding also.
conclusions statement coverage is often used by developers to evaluate thequality of their test suites.
however by reducing coverage to a singleratio muchvaluableinformationislost.whenworkingwith alargematureproject onlyverylargechangestothenumberof lines covered will be detectable as a change in the overall coverage so moderate changes to the test suite may not be observable.
even on smaller projects viewing coverage as a simple ratio hides potentialnon determinismthatexistsintestsandchangestowhich statementsare covered.of course manyof thesechangesto nonpatch code may be due to the genuine impact of code changes too interesting future work may try to identify changes due to non determinism versus those due to code changes perhaps using dynamictainttracking .inthispaper wefoundthatmeasuring the change in the setof statements covered and the impact of a patch on the coverage of those statements allows developers much more visibility into the impact of their changes.
we have released ourtoolsanddatasothatotherscanbenefitfromthemandbuildon our work to obtain new insights that eventually lead to improving quality of testing.