learning to sample exploiting similarities across environments to learn performance models for configurable systems pooyan jamshidi university of south carolina usamiguel velez christian k stner carnegie mellon university usanorbert siegmund bauhaus university weimar germany abstract most software systems provide options that allow users to tailor the system in terms of functionality and qualities.
the increased flexibility raises challenges for understanding the configuration space and the effects of options and their interactions on performance and other non functional properties.
to identify how options and interactions affect the performance of a system several sampling and learning strategies have been recently proposed.
however existing approaches usually assume a fixed environment hardware workload software release such that learning has to be repeated once the environment changes.
repeating learning and measurement for each environment is expensive and often practically infeasible.
instead we pursue a strategy that transfers knowledge across environments but sidesteps heavyweight and expensive transferlearning strategies.
based on empirical insights about common relationships regarding i influential options ii their interactions and iii their performance distributions our approach l2s learning to sample selects better samples in the target environment based on information from the source environment.
it progressively shrinks and adaptively concentrates on interesting regions of the configuration space.
with both synthetic benchmarks and several real systems we demonstrate that l2soutperforms state of the art performance learning and transfer learning approaches in terms of measurement effort and learning accuracy.
ccs concepts software and its engineering software performance keywords software performance configurable systems transfer learning acm reference format pooyan jamshidi miguel velez christian k stner and norbert siegmund.
.
learning to sample exploiting similarities across environments to learn performance models for configurable systems.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
sourcetarget samplemodel transferknowledge ii sampling i extractknowledgefigure l2s performs guided sampling employing the knowledge extracted from a source environment.
introduction software systems are increasingly becoming more configurable .
highly configurable software systems such as web servers robotics software and big data engines are used in dynamic and uncertain environments.
users of such systems are interested in knowing the consequences of changing the configuration options that are available to them e.g.
performance vs. accuracy .
similarly administrators are interested in identifying the most energy efficient configuration of the system under a specific workload.
to answer such questions performance influence models can characterize how options and their interactions affect performance in these systems .
in large configuration spaces such approaches can be expensive to build but they often yield accurate models.
however existing approaches usually focus on asingle environment fixed hardware fixed workload fixed software release and may need to relearn models from scratch when environments change which is expensive and slow.
we pursue a strategy to efficiently learn models in changed environments reusing information gained previously or in environments where measurements are cheaper to obtain e.g.
in simulations .
consider the following two scenarios in which we would like toreuse performance models across environments scenario hardware change the developers of a software system perform a thorough performance benchmarking of the system in its staging environment and built a performance model for instance in cloud .
therefore the model may not be able to provide accurate predictions for the performance of the system in the actual production environment e.g.
due to the instability of measurements in the cloud .
scenario workload change the developers of a database system build a performance model for the system using a read heavy workload however the model may not be able to provide accurate predictions once the workload changes to a write heavy one.
the reason is that if the workload changes different functionalities of the software might get activated more often and so the nonfunctional behavior changes too.
esec fse november lake buena vista fl usa pooyan jamshidi miguel velez christian k stner and norbert siegmund fortunately performance models typically exhibit similarities across environments even environments that differ substantially in terms of hardware workload or version .
the challenge is to i identify similarities and ii make use of them to ease learning of performance models.
we design l2s a sampling strategy that exploits common similarities across environments found in a recent empirical study of environment changes not all configuration options and their interactions are influential from the performance perspective but those which are influential are similar across environments.
for example the configuration option that determines the indexing mechanism of a database management system is influential for both read heavy and write heavy workloads though with different effects.
such common relationships allow us to design a sampling strategy that based on information from a source environment focuses performance exploration on a rather small subspace of the whole configuration space in the target environment here called interesting region whereby we are able to learn the performance model more efficiently.
that is l2sextracts transferable knowledge from the source to drive the selection of more informative samples in the target environment cf.
figure .
a standard learner e.g.
cart then uses the samples generated by l2sand produces a performance prediction model for the target environment.
more specifically l2sworks as follows based on identifying interesting regions from the performance model of the source environment it iteratively generates and selects configurations in the target environment.
the approach considers three characteristics from the source models i influential options ii option interactions and iii diversity of samples.
we determine influential options and their interactions from regression models of measurements in the source and establish diversity by taking into account the performance distribution fitted to the measurements in the source environment using kernel density estimation .
we evaluate l2s based on over synthetic models and real world configurable software systems with a various number of configuration options in multiple environmental changes.
we observed building custom models for every environment is unrealistically expensive and using a single model to predict the performance of all possible environments is not good enough often leading to a high prediction errors .
simple transfer learning approaches that assume changes in the environments are homogeneous e.g.
model shift work only for simple environmental changes but lead to a high prediction error when the assumption of a homogeneous change does not hold which is common in practice .
there exist more sophisticated transfer learning that reuse source data with the hope to capture correlation as a similarity measure between environments using learners such as gaussian processes gp .
this approach works for severe environment changes with a low number of options but due to the restriction of the underlying learning mechanism it is expensive to use and does not scale for larger numbers of configuration options.
moreover the quality of its prediction accuracy depends on the quality of the selected data points from the source and if the source is too dissimilar to the target it may even lead to more inaccurate performance models .
our approach l2s extracts knowledge from the source to inform the sampling of the target.
l2sexploits more variety of similarities e.g.
options and their interactions and performsbetter even for severe environmental changes.
combined with an off the shelf learner l2soutperforms state of the art transfer learning as well as traditional performance learning in prediction accuracy.
overall our contributions are the following l2s a sampling approach that selects samples in interesting regions by exploiting prior knowledge from cheap sources.
an approach for generating representative environmental changes on synthetically generated performance models to enable a largescale evaluation of our approach.
a thorough evaluation of l2sfor learning performance models of four real world configurable systems as well as over hundreds synthetically generated models including a comparison to stateof the art performance modeling approaches.
the big picture .
performance analysis for understanding the performance behavior of a software system performance models that predict the performance of the system in different configurations using a form of mathematical performance model can help.
historically white box models are developed manually using domain specific knowledge about the system s internal structure.
white box models are typically built early in the life cycle by studying the underlying design and architecture of the software system and are parameterized with the configuration options .
they are used for identifying performance bottlenecks so that developers can redesign the system.
queuing networks petri nets and stochastic process algebras are commonly used .
more recently mainly due to the abundance of data driven approaches the emphasis has been on black box models which focus on configuration options that are readily available to adjust the behavior of a system.
these black box models do not make any assumption on the design and architecture but are learned from observations running the system in different sampled configurations and extrapolating predictions for other configurations .
a common strategy to use machine learning techniques to generalize a model that characterizes system performance .
the learned performance model can be used for i performance debugging ii performance tuning by feeding the learned model to an optimizer to find good performing configurations iii detect configuration related bugs and iv capacity planning .
.
challenges our goal is to decrease the cost of learning a black box performance model by selecting a small but still representative set of samples from which we can derive an accurate model i.e.
accurately captures the regularities in the samples but also generalizes well to unseen data .
unfortunately for highly configurable systems we face the following problems that were acknowledged by previous research curse of dimensionality .
as the dimensionality of the configuration space increases the number of samples required to maintain the accuracy of the learned model can increase exponentially .
also most learners cannot scale to high dimensional spaces .
cost of sample acquisition .
acquiring a sample may have a nonnegligible cost.
for example to obtain a sample for a configurable database system we can spend half an hour for loading the gigabytes of data and run a benchmark query against the database 72learning to sample esec fse november lake buena vista fl usa sourcetarget samplemodel sourcetarget samplemodel modelshiftdatareuse sourcetarget samplemodel b model shift adaptation c data reuse d our approachtransferknowledgesamplingextractknowledgesourcetarget samplemodel a traditional learningsamplingsamplingsampling figure the big picture of performance model learning.
a traditional learning using only data sampled from the target environment b c d different variations of performance model learning employing a form of transfer learning.
.
this cost could be even worse e.g.
for long running batch systems limiting the rate at which samples can be acquired.
for instance acquiring samples corresponding to of a configuration space with binary options and average measurement time of minutes takes around years!
fixed environment and relearning from scratch .
a key assumption in most prior work is measuring performance for a fixed environment e.g.
fixed workload .
this means that the efforts we spent on learning the performance model cannot be reused when changing the environment but we must learn a new model from scratch.
.
transfer learning recently the use of transfer learning has been suggested to decrease the cost of learning by transferring performance predictive models measurement data or configuration constraints across environments.
similar to humans that learn from previous experience and transfer the learning to accomplish new tasks easier here knowledge about performance behavior gained in one environment can be reused effectively to learn models for changed environments with a lower cost.
figure summarizes prior work in performance model learning with a contrast to our approach figure a shows traditional performance model learning where the sampling takes place only for the target environment the source data and model if present do not contribute in the learning process in the target environment.
figure b shows transfer learning by shifting the model that has been learned in the source to predict the performance of the system in the target environment .
figure c shows transfer learning bydata reuse that exploits the source samples to learn a model in the target environment capturing the similarities between the source and target samples using a correlation measure .
.
the intuition behind our approach our approach figure d is based on extracting characteristics that likely remain stable across environments.
for example a configuration option cxthat does not affect any considerable change in the performance of the system in one environment is also less likely to have an influence in a different environment hence we do not prioritize generating samples with varying values of cx because such samples will likely not improve our understanding of the performance characteristics of the system.
but if an option has a significant influence in one environment there is a chance that it has a significant influence albeit with different strength in a different environment.
we extract information from the source model that guides the sampling in the target environment to gather fewer but more informative samples and therefore to gain a better understanding of the system performance with less cost.we propose the following list of information useful for sampling based on empirical observations across many environment changes in a prior study active subspace first we detect the interesting region of the target environment from which we will sample.
we perform this by determining statistically significant options that contribute to the performance of the system in the source environment.
option interactions second we prioritize our sampling on certain option interactions that are statistically influential in the source environment.
the main reason is that covering all possible combinations of the options in the reduced subspace may still be prohibitively large.
instead by concentrating on the more informative samples that cover only important interactions we would gain more information earlier.
diversity and coverage third it has been observed in several studies that performance distribution of configurable systems are multi modal .
for learning accurate performance models we require samples that proportionally cover the true empirical performance distribution.
we therefore use pseudo random sampling that iteratively selects the samples that make the distribution of the target more similar to the distribution of the source.
l2s learning to sample our approach consists of three phases see figure knowledge extraction section .
l2sgathers relevant characteristics i.e.
influential options influential interactions and performance distribution from the source environment.
active sampling section .
l2sperforms a guided sampling that takes advantage of the pieces of knowledge extracted from the source to take informative samples in the target.
performance model learning l2semploys a learner to iteratively build a performance model for the target environment using the generated samples.
this third step is standard and we reuse off the shelf learners e.g.
gp .
.
knowledge extraction we extract knowledge that is used in l2sfor space reduction and sample prioritization.
we first describe the concepts of influential options and interactions and then the machinery to extract them from the source environment.
.
.
influential options interactions and performance distribution.
influential options are configuration options that have a statistically significant influence on performance.
that is when comparing the pairs of configurations in which this option is enabled and disabled respectively an influential option has a consistent effect to speed up or slow down the program beyond random chance.
73esec fse november lake buena vista fl usa pooyan jamshidi miguel velez christian k stner and norbert siegmund performancedistributioninfluentialoptionlearninginfluentialinteraction ii sampling configurable systemperformancemodelexploitationexplorationtransferknowledgeperfconf conf perf source target i extractknowledgeperformanceinfluencemodel figure an overview of l2s.
option interactions are considered as nonlinear effects on performance where the influence of two options combined is different from the sum of their individual influences .
performance distributions defines a probability distribution over performance measures.
in other words it can be thought of as providing the probabilities of occurrence of different possible performance measures for a system in a specific environment.
previous research observed that performance distributions in configurable systems are multi modal .
this observation is a result of discrete decisions about options i.e.
some options are highlyinfluential in performance while others have little or no impact on the performance metric.
for instance in a database page size may interact with the hard drive of the host machine and result in a performance measure that is far away from most of the other measurements therefore a new peak may appear in the performance distribution of the system.
.
.
performance model representation.
for determining the influential options and their interactions we need a machinery that automatically performs the statistical analysis for determining whether an option or an interaction is influential.
for capturing the influence of options and interactions on performance we create a model using stepwise linear regression from the source data.
we used stepwise regression for two reasons.
first it is an efficient method to capture statistically significant options and interactions in an iterative manner that scales to a high dimensional space because of its approximated nature by ignoring insignificant terms and thus learning a model with only significant options .
second it also provides a self expressive model as an output that we can check whether the model can provide the information that we require for sampling in the target.
if the source data is not representative enough to learn a credible model we could measure more configurations to add to the source data.
note that we assume the measured samples in the source environment are readily available or cheap .
a regression model comprises several polynomial terms that determine the performance of the system under a configuration.
each term may refer to one or more options oi o describing the influence of that option or an interaction f o1 od 0 oi o ioi oi oj o i.
.j oi..oj where rrepresents the coefficients of the model ioirepresents the performance impact of individual options and i.
.j oi..oj represents the performance impact for interactions among multipleoptions comprising not only quadratic terms but also higher order terms up to the number of individual options .
in this work we assume configuration options are binary so if an option appears in the performance model the option is influential.
note that numeric and categorical options can be transformed into binary options by selecting two specific typically extreme values of the options corresponding to zero and one in this transformation though we sacrifice preciseness.
since the appearance of a term in the model is based on a statistical analysis as we will describe the structure of the model gives us a direct means to identify individual options and interactions with strong influences e.g.
based on p value .
as an example consider a configurable database system with options o1 encryption o2 compression o3 statistics o4 pagesize and o5 dbsize and a corresponding model f 3o1 o2 9o4 8o5 7o1o2 .5o1o4 option o3 statistics does not appear in the performance model because it does not have a significant influence on the performance of the system based on the model.
in contrast we observe an influential interaction between o1 encryption and o2 compression which enhances the performance of the system when combined.
.
.
extraction process of performance models.
we use both forward selection and backward elimination of the options or their interactions to learn the regression model.
more specifically we use the p value of an f statistic similar to t test determines if a group of variables are jointly significant to decide whether to add a term to the model or remove one.
if a term is not currently in the model the null hypothesis is that the term would have a zero coefficient if added to the model i.e.
i .
if there is sufficient evidence e.g.
p value .
to reject the null hypothesis the term will be added to the model.
conversely if a term is currently in the model the null hypothesis is that the term has a zero coefficient.
if there is insufficient evidence to reject the null hypothesis e.g.
p value .
the term is removed from the model.
more specifically initialization a model is fit to the data and then compare the explanatory power of incrementally larger by forward selection and smaller models by backward elimination .
forward selection if any terms options e.g.
o1 or their interactions e.g.
o1o2 not in the model have p values less than an entrance threshold we set the threshold to .
add the one with the smallest p value and repeat this step otherwise proceed to the next step.
backward elimination if any model terms have p values larger than an exit threshold we set the threshold to .
remove the one with the largest p value and go to the previous step.
termination extraction process stops when neither nor improve the model.
the output of the knowledge extraction phase is a performanceinfluence model specific to the source environment.
our code for learning and extraction process including some tutorials are made available at note that we also extract the performance distribution of the source.
to build a performance distribution for a system we fit a probability distribution to the measured performance of the source using kernel density estimation .
74learning to sample esec fse november lake buena vista fl usa .
active sampling l2sdoes iterative sampling that is guided by a predetermined exploitation exploration rate e.g.
eer .
.
the exploitationexploration rate determines the chance by which we take a sample from the list of configurations derived from the extracted performanceinfluence model exploitation or takes a sample from the whole configuration space exploration .
.
.
exploitation.
based on the regression model from the source we generate and prioritize a list of sample configurations as follows i l2ssorts the terms in the regression model from highest absolute coefficients to the lowest absolute coefficient and store the sorted index in a list.
ii l2sthen selects from the list the term with the highest priority.
iii l2stransforms each term into a configuration as follows the options that appear in the selected term are enabled and the options that do not appear are disabled.
for instance the term 7o1o2in will be transformed to a configuration in which o1 ando2are enabled and all other options are disabled.
the output of this step is a prioritized list of configurations that will be used in the iterative sampling phase.
.
.
exploration.
l2stakes samples according to the performance distribution of the source that was extracted prior to sampling.
the intuition behind this is to make the performance distribution of the target similar to the source.
the modes in the performance distributions are typically unproportioned.
therefore in the sampling process the configurations that are associated with the less populated modes may be missed and this results in learning a model that does not cover the response surface properly.
this may lead to a performance model that yields inaccurate predictions.
we use kullback leibler kl divergence to compare the similarity between the performance distributions.
l2srandomly samples the configuration space and selects a configuration from this randomly set that make the target distribution more similar to the source.
more specifically it measures the kl divergence of the distributions in the source and target after adding a selected configuration to the target data using the corresponding performance measurement from the source environment as an approximation.
l2sfinally selects a configuration that makes the largest decrease to the kl divergence between the posterior distributions.
experimental results we design and run experiments to compare our l2ssampling approach with respect to state of the art performance modeling and learning approaches in terms of effectiveness andefficiency of samples scalability to high dimensional spaces sensitivity to the degree of similarity among source and target environments and practical relevance to real configurable systems.
specifically we will explore the following research questions rq1 are the samples selected by l2seffective and efficient for learning accurate performance models in the target environment?
answering this question gives evidence whether the selected samples are informative enough to efficiently learn a reliable and accurate performance model for the target environment.
the rational behind this question is based on prior evidence that different samples have different information value for the learning process .
as part of rq1 we are especially interested in the robustness of our approach with respect to size and difficulty of the models and severity of the environment change.
rq2 how does l2sperform for sampling real world configurable software systems?
the answer to this question will shed some lightwhether our approach works in practice for real systems with moderately large number of configuration options and realistic scenarios for environmental changes that happen in practice.
to answer these questions we designed and ran experiments using both synthetic datasets to increase internal validity explore scalability and evaluate different environmental characteristics as well as real world configurable systems to ensure external validity with regard to practical systems .
this way we address both internal and external validity .
we use three specific metrics for our evaluations i prediction accuracy for measuring effectiveness ii number of measurements for measuring efficiency and iii learning time for measuring scalability .
.
experimental data we evaluate our approach both using synthetic models and realworld systems.
with synthetic models we can repeat evaluations on many different models while controlling the size and complexity of the models and the degree of similarity between a source and a target model.
that is instead of using only a small number of realworld systems which are expensive to measure to establish ground truth we can explore a large number of models and explore the sensitivity of our solution and other state of the art approaches to various characteristics of models and environment changes.
since we can cheaply lookup performance results in the models we can also explore models of large size that would be prohibitively expensive to sample in real world systems we could still build models with few samples but testing accuracy in an evaluation requires many additional measurements that are expensive to acquire .
in addition adding a small number of real world systems demonstrates that our results are applicable under realistic conditions.
.
.
synthetic models.
we carefully generated pairs of synthetic models source and target to simulate the performance behavior of configurable systems across environmental changes.
since performance modeling approaches and transfer strategies exploit common characteristics of real world systems and environmental changes we do not generate models entirely randomly but generate models pseudo randomly such that they still follow typical characteristics identified in empirical studies of performance models and environmental changes for example such that not all options interact in a system and such that influential options are often preserved across environments.
this way we can generate many models that differ in many aspects and in which we can explicitly control characteristics such as the relation between influential and non influential options but in which many other factors are randomized.
to generate pairs of synthetic models we proceed in two steps i generating a source model .
to generate source models that follow realistic distributions of performance values and interactions similar to real world software systems we use the thor generator .
thor uses measurement data from dozens of real systems e.g.
apache web server and combines them with kernel density estimation to rescale the data to the options interactions and configurations of a given configuration space.
although the output performance model is synthetically generated it follows interactions and option influences on the performance of real world software systems.
in our evaluation we will specifically vary the number of options and the rate of relevant options in the generated models and let thor generate all other aspects of the models in the described pseudo random way.
such a model acts as a ground truth from which we can derive samples by emulated benchmark runs.
75esec fse november lake buena vista fl usa pooyan jamshidi miguel velez christian k stner and norbert siegmund ii generating a target model .
to generate a target model we change the source model in a way that it maintains some controlled level of similarity to the source.
since no such generator exists we developed genperf1 that synthetically generates mutations of performance models we generate and apply genetic mutations to the source model while encoding the intended similarity characteristics as a fitness function.
again the similarity characteristics are based on insights from empirical observations .
our fitness function can optimize three similarity characteristics correlation of source and target response by correlating the performance of all configurations across source and target we can aim for very similar or very dissimilar models based on pearson metric.
for example simple linear shifts e.g.
the target model is always slower than the source have high correlations and are easy to exploit by some transfer learning strategies whereas in practice both high and low correlations are observed .
stable influential options and their interactions determining which percentage of influential options and interactions remain stable between the source and target allow us to further control similarity.
in practice it is common that a large percentage of options and interactions remain influential even across severe changes .
similarity of performance distributions finally we control to what degree performance distributions of source and target are similar measured with the kl divergence.
most environment changes maintain similar distributions despite larger changes .
a fitness function optimizes for all three similarity characteristics and pursues both easy environment changes with strong similarities across all characteristics as well as hard environment changes with low correlation but moderately high degrees of stability of influential options and performance distributions.
technically we use a weighted average of the three similarity characteristics in which weights control their relative importance.
to modify the model we define four mutation operators i add or remove option to vary the number of influential options ii add or remove interaction to vary the number of influential interactions iii change a coefficient of a model term to vary the influence of options or interactions on performance and iv switch sign of a model term s coefficient to drastically vary the influence of an option or interaction.
we apply these mutation operators in an iterative way according to their probability for generations.
in our experiments we systematically vary the difficulty of the environment change and distinguish three levels based on empirical observations easy correlation .
moderate .
correlation .
and hard correlation .
.
we kept the stable influence options proportional to the size of the model equal to of the number of options.
similarity we motivate decreasing of kl divergence across generations by formulating the fitness proportional to the inverse of divergence.
example.
using thor we generate a model with 11options based on the performance characteristics of the llvm compiler.
the resulting model has influential options and no interactions source model .69o1 .06o2 .88o3 .03o4 .82o5 using the fitness function for a moderate environment change we find the following model with fitness equals 1after genetic evolution with generations tar et model .95o2 .82o3 .44o4 .67o5 .01o6 .47o3o6 .95o1o2o4 12o2o3o5 real world subject systems system d c environment changes dnn easy azure tf aws micro tf hard azure tf aws micro theano xgboost easy nuc4 covtype azure covtype hard covtype cnae storm easy wordcount rollingcount hard sol rollingcount sac easy srad hotspot hard kmeans nw d number of configuration options c configurations note how source and target are different but still share certain characteristics such as similar influential options and similar coefficient but also some differences such as an extra option o6and extra interactions.
.
.
real world systems.
in addition to our synthetic models we extensively measured performance of four different configurable software systems described in table .
these systems come from different domains are written in different programming languages and have a different number of options.
all are from domains in which parameter tuning is important.
for each system we select two environment changes one that we expect to be rather easy and one that we expect to be more difficult.
dnn is a set of algorithms for deep neural networks.
we selected hyper parameters of the optimization algorithm and architecture related parameters that determine the depth of the network.
we used a time series dataset from the ucr archive as workload and measured the inference time as the response.
as environment change we varied hardware on which the network is deployed easy and varied hardware together with the specific deep learning framework hard see table for details.
the measurement code as well as experimental data can be found here h xgboost implements gradient boosting algorithms for supervised learning problems .
we selected configuration options including the booster parameters and learning task parameters.
we used two standard datasets as workload to train models and measure training time as the response variable.
we varied both hardware expected easy environment change and workload hard .
the measurement code as well as experimental data can be found here apache storm is a distributed real time stream processing system.
we selected configuration options and measured throughput as response on standard benchmarks sol wordcount and rollingcount .
as environment changes we varied between similar workloads easy and dissimilar workloads hard .
the measurement code as well as experimental data can be found here sac is a compiler for high performance applications .
we reused measurement data from a prior study in which options were selected that control optional optimizer passes.
the response is the execution time of a benchmark program.
again we varied between similar benchmark programs easy and dissimilar ones hard as environment changes.
the measurement data can be found here 76learning to sample esec fse november lake buena vista fl usa .
experimental setup in this section we define the independent iv and dependent variables dv for the evaluation.
iv learning approaches.
we compare l2s with gp as a learner against the following state of the art approaches which represent different strategies illustrated in figure model shift a model shift transfer learning approach .
datareusetl a data reuse transfer learning approach using random sampling in source and target environment .
l2s datareusetl an integration of datareusetl with the l2s sampler that is we use l2sinstead of random sampling in the target environment to investigate whether the l2ssampling can be beneficial to data reuse approaches.
random cart random sampling in the target environment and cart as a learner e.g.
without any transfer learning.
iv samples from target environment.
to identify how quickly each approach can learn with few samples in the target environment we systematically vary the number of samples from the target environment between and samples.
iv size of the configuration space using thor section .
.
we generate models with a different numbers of options that determine the dimensionality default .
iv model complexity using thor section .
.
we generate models with different number of possible interactions that determine the complexity of the generated model.
by default of all options will be influential and a model will have half low the same medium or twice high as many influential interactions as options default high.
iv severity of environmental changes.
using our genetic evolution approach section .
.
we simulate environment changes at three different levels of severity easy moderate and hard as discussed in section .
.
with a default of hard.
iv samples from source environment.
transfer learning approaches may perform differently depending on how many samples have been taken from the source environment.
by default we take random samples but consider also and .
iv exploitation exploration.
as final independent variable we varied the exploitation exploration rate of the learning process section .
between to default .
.
dv prediction accuracy.
to assess effectiveness we measure the prediction accuracy of the learned model as follows given the learned model and the ground truth from the synthetic model or systematic measurements of the target system we compare the predicted performance with the actual performance of the system on a large number of configurations evaluation set .
for small configuration spaces we use all configurations that have not been sampled for learning as evaluation set while for large configuration spaces we evaluate random configurations.
as specific metric we use the mean absolute percentage error mape between the response predicted from the learned model f c with the actual performance f c across all configurations in the evaluation set.
the absolute percentage error for a single configuration cis defined as ape f f f c f c f c .
although we could use other metrics such as root mean squared percentage error mape is scale independent and we can compare the errors across different data sets and response measures .to compare approaches across different sample sizes iv we report the cumulative mape mean average percentage error that averages the errors for different sample sizes reflecting the area under the accuracy curve c mape t i 1ape fi f t where tis the number of iterations and fiis the learned model at iteration i. dv performance.
to assess efficiency we additionally measure the time it takes to train a performance model for a given sample of measurement data.
the time is measured on a macbook pro with .
ghz intel core i7 cpu and 16gb of memory.
.
results rq1 effectiveness and efficiency first we plot accuracy dv and performance dv of different learning approaches iv for different sample sizes in the target environment iv in figure using a single synthetic model in which we used default values for all other independent parameters.
for this model after around iterations i.e.
taking samples from the target environment l2sachieves near perfect predictions while other approaches cannot reach the same accuracy even after exhausting the experimental budget at iteration .
without any transfer learning even iterations of pure random sampling random cart does not reach similar accuracy as targeted samples selected with l2s see diamond shape annotation with in figure .
model shift is generally ineffective and does not improve much with additional samples due to negative transfer with default hard difficulty i.e.
low correlation datareusetl and l2s datareusetl are reasonably effective but have higher learning costs that increase quickly with additional samples taken.
while a single synthetic model cannot answer the research question our experimental setting allows us to perform comparisons across many models.
in the following we will further systematically explore how the other independent variables influence the accuracy of the different learning approaches iv .
we therefore vary each independent variable iv to iv one at a time while keeping other variables at their defaults.
we report distributions of our cumulative accuracy measure using box plots from repeated observations with synthetic models for each setting.
model complexity.
with increased model complexity iv it becomes more difficult to capture the hidden structures in dimensional spaces.
our results in figure show that l2s s effectiveness decreases slightly with increased complexity but it still outperforms all other approaches.
sensitivity to change severity.
with increased change severity iv as expected our results in figure show that l2s s effectiveness decreases.
nonetheless it still outperforms all other approaches some of which struggle even more with severe changes the sharpest drop in effectiveness can be observed for model shift because it highly relies on the correlation across environments for hard changes with weak correlations the models that are learned by model shift on the source are not representative for the target environment.
also datareusetl performs better than the model shift approach which can be attributed to the fact that datareusetl relies on source data which are much richer than a model that has been learned on them.
more specifically datareusetl uses a distance metric to find the appropriate regions of the source for predicting the target response.
therefore even if part of the target response is related to the target datareusetl can find the appropriate regions in the configuration space that are correlated and learn a more accurate model for the target environment .
77esec fse november lake buena vista fl usa pooyan jamshidi miguel velez christian k stner and norbert siegmund sample size05101520learning time l2s gp l2s datareusetl datareusetl modelshift random cart3 70sample size051015202530mean absolute percentage error100200500l2s gpl2s datareusetldatareusetlmodelshiftrandom cart figure comparing accuracy and performance of learning approaches for different sample sizes for a synthetic model with default parameters.
low medium high model complexity interaction level 5101520253035mean average percentage errorl2s gp l2s datareusetl datareusetl modelshift random cart figure sensitivity analysis with model complexity.
sensitivity to source samples.
transfer learning approaches can usually benefit from more source samples because they can get more accurate source models from which they transfer knowledge.
when varying the number of source samples iv we find that l2saccuracy has slightly been improved but as expected the gain of accuracy for datareusetl was higher plot omitted due to space restrictions .
more importantly though we observe as shown in figure that learning times exponentially increase with more source samples whereas l2sandmodel shift scale well with a reasonable overhead.
the main reason is that l2sdistill the data to knowledge source sample size10 2100102104learning time l2s gp datareusetl modelshiftfigure runtime overhead of transfer learning approaches with respect to source sample size.
note that datareusetl did not finished within hours for source samples.
.
.
.
.
.
.
.
.
.
exploration exploitation parameter1015202530mean average percentage error figure sensitivity with the exploration exploitation rate.
a value of zero for the parameter means full exploitations while one means full explorations.
that will be used for sampling similarly model shift distill the data to a model for predicting the target whereas datareusetl reuses the data from the source in addition to the data that was taken from the target to learn a predictive model cf.
figure .
therefore the number of training samples that are used in the learning process of datareusetl are substantially higher than our approach and this contributes to the excessive runtime overhead of datareusetl .
exploration exploitation.
finally we explore sensitivity to l2s s parameter that controls the combination of exploiting the knowledge from the source and exploration via pseudo random sampling in the target iv .
our results in figure indicate that the pseudorandom sampling contributes to learning more accurate models but that for effective learning most samples should be drawn from exploiting source knowledge.
focusing mostly on exploitation with low values between .
and .
seems to be effective.
answer to rq1 .
the results confirm that l2soutperforms other approaches with respect to effectiveness and efficiency across many different models.
the results are robust with regard to model size model complexity and severity of changes.
78learning to sample esec fse november lake buena vista fl usa easy moderate hard difficulty051015202530mean average percentage errorl2s gp l2s datareusetl datareusetl modelshift random cart figure sensitivity with different change difficulty.
.
results rq2 real systems to demonstrate applicability we additionally evaluate effectiveness for learning performance models of real configurable systems.
as opposed to synthetically generated models and emulated environmental changes we perform large scale performance measurements of real systems in different environments.
we measured the performance of the systems in table in different environments over the course of two months of experimental time.
we plot the prediction error dv curves for different learning strategies iv and different sample sizes iv for of the analyzed environment changes in figure .
the remaining plots are available at our results are consistent with those from synthetic models l2soutperforms other approaches by sampling the space more efficiently.
however it is also comparable to l2s datareusetl for xgboost and sac both environment changes and to random cart anddatareusetl for storm hard change .
our results also scale to the large configuration space of sac where they even show a more pronounced benefit of l2s.
also as expected l2s typically outperforms other approaches by a larger margin on the environment changes we judged to be more severe.
moreover in large environmental changes where datareusetl result in negative transfer e.g.
sac l2scontributes in learning a more accurate model in l2s datareusetl by taking more informative samples.
answer to rq2 .
experiments with four real world systems confirm that l2sis practical and scales to real world scenarios for building performance models.
.
insights our experimental results show that transfer learning based on knowledge transfer for sampling in the target environment l2s outperforms transfer learning based onmodel shift anddata reuse as well as non transfer learning approaches especially for severe environment changes.
transfer learning based on model shift works well for easy environment changes but suffers from high prediction errors for more severe changes.
transfer learning based on data reuse may result in a negative transfer for severe environmental changes leading to inaccurate models.
it furthermore suffers from scalability issues because itcaries over raw data rather than abstracted knowledge that l2s andmodel shift employ.
transfer learning based on data reuse that was enhanced by l2s sampling i.e.
l2s datareusetl often performed better than the original approach.
the key intuition behind these observations is that both classes of transfer learning i.e.
model shift and data reuse assume that the performance measurements in the target environment are correlated with the ones in the source.
however this assumption only holds in small environmental changes .
on the contrary l2s is not based on the assumption of high correlation across environments but it is based on the pieces of knowledge that stay consistent across environments including large changes.
.
threats to validity internal and construct validity.
the use of synthetic models and a controlled environment allows us to rule out many alternative explanations for our results such as measurement noise or the influence of model complexity.
however our results hinge on how representative our synthetic models are for real systems and environmental changes.
as discussed we increase representativeness by carefully designing approaches to generate source and target models section .
.
using a published approach designed and validated for this purpose and combining it with a custom approach for generating environmental changes designed based on the insight from a large scale empirical study of real systems .
additional confidence can be gained from the fact that the results from synthetic models align with those from eight real environmental changes.
nonetheless our results must be interpreted within the constraints of how those models were generated.
for evaluations with real systems measurement noise cannot be excluded and may affect the results even though we carefully established ground truth by measuring the performance on dedicated systems and repeated the measurements several times.
we also compared the accuracy of a limited set of learning approaches using a specific metric i.e.
mape .
we implemented the approaches according to the description provided in the corresponding papers but cannot exclude smaller differences due to implementation and configuration differences as well as the use of a different comparison metric.
to control the randomness of sampling we repeated three times and averaged the results.
external validity.
we used a diverse set of real world configurable systems from different domains and a large number of purposefully selected environmental changes.
despite additional confidence from synthetic models the reader must be careful when generalizing results beyond the studied systems.
l2sonly supports sampling configurable systems with binary options and the results cannot be generalized to non binary configuration spaces.
related work learning and optimization .
several models e.g.
support vector machines decision trees fourier sparse functions sampling strategies e.g.
active learning and optimization strategies e.g.
search based optimization and evolutionary algorithms have been used for performance prediction and tuning of configurable systems .
several aspects from reducing measurement efforts increasing prediction accuracy and model reliability have been investigated.
79esec fse november lake buena vista fl usa pooyan jamshidi miguel velez christian k stner and norbert siegmund 70sample size020406080100mean absolute percentage error100200500l2s gpl2s datareusetldatareusetlmodelshiftrandom cart a dnn hard 70sample size020406080100mean absolute percentage error100200500l2s gpl2s datareusetldatareusetlmodelshiftrandom cart b xgboost hard 70sample size020406080100mean absolute percentage error100200500l2s gpl2s datareusetldatareusetlmodelshiftrandom cart c storm hard figure comparison of prediction accuracy of approaches on real systems.
optimization algorithms have been applied to find best configurations using only a limited sampling budget recursive random sampling hill climbing direct search optimization via guessing bayesian optimization and multi objective optimization .
recently the importance of configuration options for optimization has been explored .
our work is related to the performance model learning and optimization research mentioned above we learn performanceinfluence model on a source environment to extract information that can inform sampling for a target environment.
our aim is to learn a less costly but more accurate performance model for the target environment than learning from scratch.
all existing learning approaches still remain valid and important when there is no source environment i.e.
previous measurements be available.
sampling .
although a proper sampling strategy is a key factor for decreasing the cost of learning performance models this area is not entirely explored yet.
several experimental designs e.g.
full factorial designs fractional factorial designs and response surface designs have been developed to extract informative samples that guarantee certain statistical properties e.g.
option interactions .
they have also been applied in the domain of configurable systems .
several binary option sampling heuristics have been proposed with the goal of selecting configurations to learn the influence of each individual binary option and their two way interactions.
these sampling strategies especially have been used to identify interactions that are related to detect bugs in combinatorial testing .
adaptive sampling designs for statistical experiments also known as response adaptive designs are ones where the observations or the model learned on them are used to adjust the experiment as it is being run.
active sampling as an instance of adaptive sampling chooses subsequent samples based upon the models learned previously.
for instance bayesian optimization a sequential design strategy have been used for performance tuning of configurable systems .
since the performance model is unknown the bayesian strategy is to treat it as a random function and place a prior over it.
the prior captures our beliefs about the performance behavior of the system.
after observing the system performance for a configuration the prior is updated to form the posterior distribution.
the posterior distribution in turn is used to construct an acquisition function that determines what configuration should be measured next.
bayesian optimization has also been used withtransfer learning to select appropriate data from the source domain .
progressive and projective sampling are used for the performance prediction of configurable systems .
our work is essentially categorized as a sampling approach that has been informed using external knowledge pieces that have been extracted from similar environments with a lower cost.
transfer learning .
the idea of using measurement data or extracting some types of knowledge from other environments has been used in different application areas mapreduce applications anomaly detection micro benchmarking consistency analysis of parameter dependencies detection of performance regressions and performance predictions based on similarity search .
transfer learning has been used in the context of self adaptive software configuration dependency transfer across system version for optimization co design exploration for system software model transfer across hardware and configuration optimization .
transfer learning has also been applied in defect predictions and effort estimation .
our work can be viewed as a type of transfer learning but we do not shift a model or reuse source data instead we transfer knowledge across environments using the insights from to inform sampling figure see section .
conclusions the current approaches target a static scenario where one needs to learn an initial performance model for a specific environment.
here we target the use case when the environment changes.
we proposed l2s a guided sampling strategy which sits on top of any learning mechanisms and is able to exploit knowledge pieces from a similar environment and take informative samples.
we have performed extensive experiments using over synthetic models as well as configurable systems demonstrating that l2soutperforms state of the art transfer learning approaches as well as traditional sampling and learning mechanisms for performance analysis.
acknowledgment this work has been supported in part by the national science foundation awards and and afrl and darpa fa8750 .
siegmund s work is supported by the dfg under the contracts si and si .
80learning to sample esec fse november lake buena vista fl usa