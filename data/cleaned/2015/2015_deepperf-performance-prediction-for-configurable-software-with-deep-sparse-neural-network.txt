deepperf performance prediction for configurable software with deep sparse neural network huong ha the university of newcastle callaghan australia huong.ha uon.edu.auhongyu zhang the university of newcastle callaghan australia hongyu.zhang newcastle.edu.au abstract many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system.
as the combi nation of configurations could be exponential it is difficult toexhaustively deploy and measure system performance under allpossible configurations.
recently several learning methods havebeen proposed to build a performance prediction model based onperformance data collected from a small sample of configurations and then use the model to predict system performance under anew configuration.
in this paper we propose a novel approachto model highly configurable software system using a deepfeedforward neural network fnn combined with a sparsityregularization technique e.g.
the l 1regularization.
besides we also design a practical search strategy for automatically tuningthe network hyperparameters efficiently.
our method calleddeepperf can predict performance values of highly configurablesoftware systems with binary and or numeric configurationoptions at much higher prediction accuracy with less trainingdata than the state of the art approaches.
experimental resultson eleven public real world datasets confirm the effectiveness ofour approach.
index t erms software performance prediction deep sparse feedforward neural network highly configurable systems spar sity regularization i. i ntroduction many large and complex software systems are highly configurable i.e.
they provide a set of configuration options for users to select.
these options allow users to customize thesystem to meet their specific requirements hence improvingthe usability and reusability of the system.
different configu rations may lead to different quality attributes non functionalproperties .
among the quality attributes performance suchas response time or throughput is one of the most importantquality attributes as it directly affects user experience.
it isnecessary to understand the performance of a system under acertain configuration before the system is actually configuredand deployed.
this helps users make rational decisions inconfigurations and reduce performance testing cost.
however we cannot exhaustively deploy and measure system perfor mance under all possible configurations as even a small scaleconfigurable system already results in an exponential numberof configurations.
in recent years researchers have proposed to measure the performance of a system with only a limited set of configu rations sample build a performance prediction model andthen use the model to predict the performance of the systemunder new configurations population .
in this way performance can be predicted before avariant of the system is configured and deployed.
the difficultyhere is to predict system performance with high accuracywhile utilizing a small sample.
as it takes time and effortto configure the system and collect performance data it isdesirable that the sample size is kept minimum.
the challenge for building a performance prediction model is in the interactions between configuration options features i.e.
a particular combination of features causes an unexpectedbehaviour on the system performance while their individualpresences do not .
to address this challenge anapproach namely splconqueror aims to learn the influences of individual configuration options and their interactions fromthe differences among the measurements of the sample .
several sampling heuristics and experimental designsfor configuration options are combined with the suggestedlearning method to achieve good prediction accuracy.
a strongpoint of splconqueror is that it can derive performanceinfluence models from binary numeric configurable softwaresystems i.e.
using the prediction models users can understandhow individual features and their interactions influence thesystem performance.
a disadvantage of this method is relatedto its flexibility as it might not always be possible to measurethe performance values of configurations that meet certainpre defined coverage criteria.
besides splconqueror usually requires more sample than other approaches as their focusis to make the influences of configuration options and theirinteractions explicit .
another approach is to consider the performance prediction problem as a non linear regression problem and apply a statis tical learning method e.g.
the classification and regression trees cart technique to find this non linear model .
recently it was further extended by combining with variousresampling and machine learning hyperparameter tuning tech niques and became a more data efficient performance learningalgorithm decart .
that is compared to cart decart uses less measurement effort to learn and validate a performance prediction model .
however at present bothcart and decart can only predict configurable software systems with binary configuration options.
lately zhang et al.
addressed this challenge by for10952019 ieee acm 41st international conference on software engineering icse .
crown authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
mulating the software performance function as a boolean function.
using fourier transform of the boolean function the task of estimating the performance function becomes estimating its associated fourier coefficients from a small sample.
although the algorithm can derive a sample size that guarantees a theoretical boundary of the prediction accuracy the size of sample required to achieve a desired accuracy is still very large sometimes even more than the whole population of the system especially for a relatively small system .
similar to cart decart this approach works only on binary configurable software systems.
inspired by the strengths and weaknesses of all the state ofthe art methods in this paper we aim to derive an approach that can model all types of configurable software systems i.e.
binary and binary numeric systems and predict system performance at a high accuracy using a small sample.
similar to cart decart to address the problem of feature interaction we also consider the software performance prediction problem as a non linear regression problem i.e.
the performance is a non linear function of the configuration options.
however in our work we suggest to use a deep feedforward neural network fnn with non linear hidden layers to approximate this nonlinear performance function.
a deep fnn is a multilayer stack of computational units neurons with the first layer taking the input the last layer producing the output and the middle layers hidden layers connecting the input and the output layer .
the idea of approximating performance function by a deep fnn is possible as it has been shown that fnns with hidden layers provide a universal framework i.e.
an fnn with a linear output layer and at least one hidden layer with some specific activation functions and sufficient number of neurons can approximate various function classes from boolean functions to continuous real valued functions .
the first challenge when using deep fnns to model software performance is that a deep neural network usually requires a lot of training data to achieve high prediction accuracy however for the software performance prediction problem we have very limited number of measurements.
to address this problem we need to incorporate some prior knowledge about software performance into the network architecture.
a good way to do this is to tell the network how the parameters look like i.e.
whether they follow any particular distribution or have any special properties.
for configurable software systems it has been observed that the software performance functions are usually very sparse i.e.
only a small number of configurations and their interactions have significant impact on system performance .
based on this observation to model software performance we suggest to use a deep sparse fnn.
to construct a deep sparse fnn we will combine a normal deep fnn with a sparsity regularization technique e.g.
the l1regularization .
the second challenge with using deep sparse fnns is that they require many hyperparameters e.g.
the number of layers number of neurons regularization hyperparameters etc .
these hyperparameters need to be tuned optimally so that thenetwork achieves high prediction accuracy.
in practice these hyperparameters can be optimized either manually by human experts or automatically by some tuning methods.
as it is not always possible to find an expert to tune hyperparameters every time we need to predict software performance automatic tuning is needed.
however even with the use of an efficient automatic tuning method it usually takes hours or days to find an optimal hyperparameter set since the hyperparameter search space i.e.
the space contains all the possible combinations of the hyperparameters is very huge.
to overcome this challenge in this work we also propose a hyperparameter search strategy for our deep sparse fnn such that it takes much less computation time while still obtains a good hyperparameter set that leads to higher prediction accuracy.
we have implemented the proposed performance prediction approach as a tool deepperf and evaluated it on eleven realworld configurable software systems with up to 1031configurations and from different application domains e.g.
compiler web server database library video encoders etc.
the experimental results show that for most of the systems deepperf can achieve much higher prediction accuracy with smaller sample sizes compared to the state of the art methods.
for binary software systems i.e.
the systems with binary configuration options deepperf statistically outperforms decart a stateof the art method for predicting the performance of binary software systems on 3out of 6systems for all sample sizes and performs similarly on the other 3systems.
for binarynumeric software systems i.e.
the systems with both binary and numeric configuration options deepperf outperforms splconqueror a state of the art method for predicting the performance of binary numeric software systems on 4out of5systems for all sample sizes.
in summary our contributions are as follows we are the first to propose to use a deep sparse fnn to model highly configurable software systems with binary and or numeric configuration options.
we suggest a practical hyperparameter search strategy for the deep sparse fnn such that it can automatically find a good set of hyperparameters to achieve high prediction accuracy within a short time.
we implement our proposed method namely deepperf and extensively evaluate our method on eleven real world configurable software systems with various sample sizes.
the results show that for most of the systems deepperf outperforms other state of the art approaches i.e.
it achieves much higher prediction accuracy with less training data .
we empirically compare different types of regularized deep fnns and other common machine learning methods e.g.
svm to show that a deep sparse fnn is a better choice to predict the performance of configurable software systems.
ii.
b ackground a. the formulation of software performance prediction problem generally a performance function of a software system with nconfiguration options is a function from configuration space authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i example of a configurable softw are system and its performance v alues .
x1x2x3...x8x9x10x11f x ... .
... .
... .
... .
.
.
.... .
.
.
.
.
... .
... .
to a performance measurement f x f x1 x2 ... x n x r. where xi i n is the variable that stores the value of the configuration option ith.
it can either be a boolean value indicating if the configuration option is de selected or a real value in the value range for that configuration option.
xis the cartesian product of the domains of all the configuration options.
the objective is to predict the software performance value f x of any new configuration vector xgiven a small sample of sizem xi f xi i ... m .
table i shows an example of a software performance functionf x1 ... x n and its configuration space.
the software system has configuration options in which options take binary values and options take numeric values.
in total the system has valid configurations.
measuring the time performance of all these configurations is difficult since it requires a lot of time and effort.
to overcome this challenge researchers propose to measure only the performance values of a limited number of configurations sample then build a prediction model from these data to predict the performance values of all configurations population .
the challenge here is to use only a small sample while still be able to predict the performance of all other configurations in the population with a high accuracy.
b. thel1regularization thel1regularization technique was first introduced by tibshirani for linear regression with independent gaussian noise which is called least absolute shrinkage and selection operator lasso .
the idea is to add to the least squares loss function a penalty term that is constructed from the sum of the absolute values of the parameters i.e.
the l1norm of the parameters.
by adding this penalty term to the loss function the sum of the absolute values of the parameters is encouraged to be small.
in practice it has been frequently observed that l1regularization can cause many parameters to be equal to zero which makes the parameter vector sparse .
nowadays l1regularization is not only applied in linear regression but also in other models such as logistic regression or neural network .c.
hyperparameter tuning for any machine learning model there are variables that determine the model structure or decide how the network is trained which are called hyperparameters.
for example for neural networks the hyperparameters are the number of layers number of neurons layer regularization hyperparameter learning rate etc.
these hyperparameters need to be chosen accurately for the machine learning model to achieve a high prediction accuracy.
the problem of identifying a good set of hyperparameters is called hyperparameter optmization .
the general idea is to choose some trial points ... from the hyperparameter space evaluate the network performance on a validation dataset with these trial points and pick the i that has the smallest prediction error.
the critical step in hyperparameter optimization is to choose the set of trials ... effectively .
there are several widely used approaches to choose this set of trials from the hyperparameter space grid search random search bayesian optimization etc.
in reality the hyperparameter optimization tuning process is usually very time consuming as the hyperparameter space is huge and many trials are needed to find an optimal hyperparameter set.
iii.
a deep sparse feedforw ard neural network for softw are performance prediction in this section we describe in detail the design choices when using neural network to model performance values of configurable software systems and from there we propose a neural network architecture that can predict software performance values using a small random sample with high prediction accuracy.
besides we also suggest a practical hyperparameter search strategy to automatically find a good set of hyperparameters within a short time.
a. the design of the deep neural network design rationale the main consideration when selecting the architecture of an fnn for a specific problem is choosing the depth i.e.
the number of hidden layers and the width i.e.
the number of neurons per layer .
since our approach is to use an fnn to represent performance function of software system we are concerned with the question whether we should choose a shallow fnn network with one hidden layer and a large number of neurons per layer or a deep fnn network with a large number of hidden layers and a small number of neurons per layer .
as stated in the universal approximation theorem a shallow fnn having one linear output layer and one hidden layer with enough neurons and a suitable activation function e.g.
sigmoid relu can approximate any continuous function from one finite dimensional space to the other at any level of accuracy.
however to achieve a high level of accuracy in the worst case the number of neurons needed for a shallow fnn with one single hidden layer is an exponential number of the inputs .
for example representing boolean performance function using fourier learning can be considered similar to using a shallow fnn with one hidden layer and a large authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
the proposed l1regularized feedforward neural network for configurable software performance prediction.
the inputs of the network are the n configuration options of the software system and the output of the network is the performance value.
number of neurons per layer 2nneurons where nis the number of configuration options .
unfortunately in practice there is no guarantee we can train such a neural network that has a very large number of neurons .
the reasons are the optimization algorithm used might not be able to find the values of the parameters corresponding to the desired function the training algorithm might choose the wrong function .
besides training a huge number of parameters costs a lot of computation time and memory.
this is known as the curse of dimensionality problem.
deep fnns on the other hand can avoid the curse of dimensionality that shallow networks encounter .
when using fnns to approximate functions for a given upper bound of the approximation error shallow fnns require exponential more parameters than deep fnns .
therefore for our approach we propose to use a deep fnn to model performance values of configurable software systems.
besides to make the hyperparameter tuning process easier and faster we suggest to fix the number of neurons in each hidden layer to be the same.
by this the complexity of the fnn will be controlled by only two hyperparamters the number of hidden layers and the number of neurons in each hidden layer.
the network architecture the overall architecture of deep fnn for software performance prediction is as follows the input layer has nneurons where nis the number of configuration options of the software system needs to be predicted.
the output layer has 1neuron which outputs the performance value of the software system.
there are lhidden layers l and each hidden layer hasnlnumber of neurons.
all the hidden layers use a relu activation function while the output layer uses a linear activation function.
using a linear output layer is required since the performance prediction is a regression problem.
meanwhile the relu is chosen as the activation function of the hidden layers due to its ability to learn much faster in networks with many layers compared to other non linear activation functions .
for this fnn architecture there are nl n nll trainable parameters.
this number depends on n nlandl hence the network is able to represent more complex functions when the software system has more configuration options.
b. reducing network complexity through regularization selecting the regularization techniques a challenge in training the deep fnn architecture described in the previous subsection is that in reality we often have very limited amount of training data as the main purpose of the software performance prediction problem is to predict the performance values from a small sample.
with this condition the model is illposed meaning that there is an infinite number of parameters that can obtain a perfect fit to the training data.
however these parameters normally do not fit well to the new data.
one of the key ideas for solving ill posed or overfitting problems is to introduce additional information to the network by using a suitable regularization technique.
at present there are three most common regularization methods that are used in many deep learning applications thel1regularization.
as described in section ii b the idea ofl1regularization is to add to the loss function a penalty term which is constructed by applying an l1 norm on the parameters .
thel2regularization.
similar to l1regularization l2 regularization works by adding a penalty term to the loss function however in this case an l2norm is being used to generate the penalty term .
l2regularization is arguably the most popular technique in machine learning to combat overfitting.
the dropout technique.
dropout technique was specifically proposed for neural networks .
the key insight is to randomly drop neurons along with their connections from the network during training to prevent the network to adapt too much to the training data.
to choose a regularization technique that can improve the performance prediction accuracy we need to select the technique that is able to utilize the prior knowledge about configurable software systems.
l1 regularization of the network for configurable software systems it has been widely observed that even though the possible number of interactions among configuration options is exponential a very large portion of potential interactions has no influence on the performance of software systems .
this means that the parameters of the neural network could be very sparse i.e.
only a small number of parameters have significant impact on the model.
hence we suggest to use a regularization technique that enables the fnn to satisfy this condition which means that l1regularization technique is the best candidate due to its ability to make the model parameters sparse.
let denote as the weights of the fnn as the bias xas the input data yas the output data and j x y as the loss function of the network.
applying l1regularization to the network means changing the loss function from j x y to jreg x y j x y bardbl bardbl1 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
where denotes the regularization hyperparameters and bardbl.
bardbl1 is thel1norm.
a consideration when using the l1regularization in deep fnns is whether we need to apply regularization to all the hidden layers in the network.
intuitively it might increase the prediction accuracy if we apply l1regularization to all the layers and each layer has a different regularization hyperparameter.
unfortunately in practice this process is infeasible.
the reason is that the effectiveness of regularization depends mostly on the choice of the regularization hyperparameter and finding the right regularization hyperparameter for each layer is difficult especially when there are many layers in the network.
alternatively one can use one global regularization hyperparameter for all the layers in the network however in this case the choice of this hyperparameter becomes more sensitive.
a slight increase or decrease in this hyperparameter can affect the whole model accuracy greatly because this hyperparameter affects all the layers in the network.
when we have a very limited amount of data for validation such as in the case of software performance prediction this becomes a weakness since an abnormal segment of validation data can affect the choice of this hyperparameter which in turn affects adversely the prediction performance of the fnn.
based on these observations we suggest to only apply l1 regularization to the first hidden layer.
note that in fnns a layer is a function of the layer that preceded it so by shrinking some parameters in the first layer to zero we actually remove a lot of irrelevant connections in the latter layers within the network.
therefore with only one layer to be regularized the l1regularization still has good effect on the whole network.
the proposed l1regularized deep fnn for software performance prediction is shown in figure .
c. efficient hyperparameter tuning for the proposed deep fnn architecture there are three hyperparameters that control the complexity of the network number of layers number of neurons layer regularization hyperparameter and two key hyperparameters that control the model training process learning rate number of epochs.
the hyperparameter space constructed from these five hyperparemeters is the cartesian product of the domains of all hyperparameters which is huge.
searching for an optimal hyperparameter set from this hyperparameter space is computationally expensive as many trials may be required.
in this section we aim to reduce the hyperparameter searching effort by fixing some dependent hyperparameters deriving a search strategy to effectively reduce the hyperparameter space.
first we set some dependent hyperparameters to some fixed values.
specifically set the number of neurons layer nl to be a fixed value e.g.
.
as discussed in section iii a when using fnn to approximate functions a deep fnn is more beneficial.
previous studies also show that the network depth is what matters most .
therefore we choose to fix the number of neurons per layer nl instead of the number of layers l .
we can control thecomplexity of the fnn by the number of layers and the regularization hyperparameter.
set the number of epochs to be a fixed value e.g.
.
with this setting we can control the training process using the learning rate.
this setting is a way to constrain the training time budget i.e.
the tuning method needs to find the optimal learning rate given the training time budget.
second we split the sample into two parts training and validation and then use the proposed search strategy below to find the optimal hyperparameters.
our search strategy comprises of two steps step start from a non regularized fnn with 2hidden layers search for the optimal learning rate of this network architecture train the network with this learning rate and evaluate the prediction error on the validation dataset.
keep adding more hidden layers to the non regularized fnn until the validation error starts to go up which is the sign when the fnn starts to overfit.
this is the optimal number of hidden layers for a non regularized fnn to fit well with the performance values of the software system.
note that the optimal learning rate is chosen as the largest value that makes the training error closest to .
step use the number of hidden layers found in step add a few more layers and search for the optimal learning rate of this fnn architecture.
finally add l1regularization to this non regularized fnn and search for the optimal l1regularization hyperparameter.
the reason we need to add a few layers after finding the optimal non regularized fnn architecture is that when we apply regularization to a network the network needs to be deeper compared to a non regularized one in order to achieve high prediction accuracy .
we suggest to add more layers based on our empirical experiments with various configurable software systems.
with this hyperparameter search strategy the hyperparameter search space is much smaller than the original one.
specifically if we denote dn dl dras the search space of the number of layers l the learning rate and the regularization hyperparameter respectively the original hyperparameter search space is dndldr.
with our proposed search strategy the search space will be dndl dl dr which is much smaller than the original search space.
note that in each step of our search strategy any conventional hyperparameter tuning method e.g.
grid search random search bayesian optimization etc.
can be used to find the optimal hyperparameters.
d. training of the network in this section we describe some technical details we utilized in order to train our proposed model effectively.
these details are critical in achieving the high prediction accuracy of our approach.
loss function we choose the loss function to be the mean square errors between the real output and the predicted output as this is the most commonly used regression loss function in machine learning.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
input output weights normalization to make regularization and hyperparameter tuning method work effectively we normalize the input and the output of the training data.
during testing for predicting performance values of new configurations we normalize the new data in the same way we did during training.
that is we use exactly the same parameters for normalization during training for any new inputs to the network.
in addition we utilize xavier initialization to initialize weights of all the hidden layers in the network.
optimization algorithm we use the adam optimizer to train the neural network as it is a very computational efficient method .
besides to avoid exploding gradients gradient clipping technique i.e.
limits the magnitude of the gradients between 1and1 is utilized during the training process.
finally the batch size is set to the whole sample size i.e.
size of the training data since for performance prediction problem the size of the training data is small.
e. tool implementation we implement our proposed approach deepperf using python .
and tensorflow .
.
.
the input is normalized between 0and1since it is a standard way to do normalization.
the output is normalized between 0and instead of between 0and1 as we do not want the network parameters to be too small.
all the hyperparameters of the adam optimizer is set using the default values in tensorflow.
to search for the hyperparameters we use of sample for training and for validation.
for the learning rate we use grid search with points logarithmically equally spaced in the range .
we also use a learning rate schedule i.e.
the learning rate is dropped by factor of .
after every epoch.
for the l1regularization hyperparameter w eu s e grid search with points logarithmically spaced in the range .
note that here .01corresponds to a very little regularization and corresponds to a very large regularization.
since we normalize both input and output of the neural network all the software systems share the same range for searching for .
the optimal l1regularization hyperparameter is the value that achieves the smallest validation error.
iv .
e v alua tion a. experimental design in this section we aim at answering the following research questions rq rq1 how accurate is our proposed approach in predicting performance of configurable software systems with binary options?
to answer this rq we conduct an experiment to compare deepperf with other state of the art approaches on binary configurable software systems.
rq2 how accurate is our proposed approach in predicting performance of configurable software systems with binary and numeric options?
to answer this rq we conduct an experiment to compare deepperf with other state of the art approaches on binary numeric configurable software systems.rq3 is a complex model like the deep sparse fnn actually needed or can we utilize svm or other nn based regression methods to achieve the same level of prediction accuracy?
to answer this rq we conduct an experiment to compare deepperf with the support vector machine svm regression method and other alternative designs described in section iii b the l1regularized fnn where the l1regularization is applied to all layers the non regularized fnn the l2 regularized fnn and the dropout fnn.
rq4 what is the time cost of deepperf to predict performance of a configurable software system?
this rq is to evaluate the practicality and feasibility of our proposed approach.
to answer this rq we show the time consumed by the hyperparameter searching and training process of our approach on various highly configurable software systems including binary and binary numeric systems.
the detailed setup for each experiment will be described in the subsequent sections.
in general to compare the prediction accuracy between learning methods we use the training dataset sample to generate a performance model for each method and then use this model to predict performance values of configurations on the testing dataset.
to evaluate the prediction accuracy we use the mean relative error mre which is computed as mre c summationdisplay c v predicted c actual c actual c wherevis the testing dataset predicted cis the predicted performance value of configuration c actual cis the actual performance value of configuration c. we choose this metric as it is widely used to measure the accuracy of prediction models .
b. subject systems for the experiments we use eleven real world configurable software systems six of these systems have only binary configuration options and were used in the other five systems have both binary and numeric configuration options and were used in .
these systems have different characteristics and are from different application domains e.g.
multi grid solver web server video encoder database library database management system compiler etc.
they are also of different sizes thousands to more than thousand lines of code and written in different languages java c and c .
the number of configuration options ranges from to while the number of valid configurations ranges from to .
these software systems were measured and published online .
more information about these systems and how they were measured can be found in .
the overview of these eleven subject systems is given in table ii.
c. rq1 comparison on software systems with binary options as mentioned in introduction at present there are many learning methods for predicting performance values of software systems with binary options including splconqueror authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii the subject softw are systems system domain binary numeric configs apache web server x264 video encoder llvm compiler bdb c database system bdb j database system sqlite database system dune mgs multi grid solver hipaccimage processing hsmgp stencil grid solver javagc runtime env.
sac compiler binary is the number of binary configuration options.
numeric is the number of numeric configuration options.
configs is the number of valid configurations f ourierlearning and decart the improved version of cart .
among these approaches decart is recently proposed and can achieve higher prediction accuracy than others .
hence in this experiment we will only compare our proposed method with decart .
we will evaluate the two approaches on six binary subject systems in table ii apache x264 llvm bdb c bdb j and sqlite.
these six subject systems were also used in .
setup here we adopt the experiment setup in .
specifically for each subject system we randomly select a certain number of configurations and their corresponding performance values to construct the training dataset sample all the remaining measurements are then used as the testing dataset.
we use five different sizes for the training dataset of each subject system n 2n 3n 4n 5n wherenis the number of options of each system which is shown in the column binary of table ii .
to evaluate the consistency and stability of the approaches for each sample size of each subject system we repeat this random sampling training and testing process 30times.
we then report the mean and the confidence interval1of the mres obtained after experiments with deepperf and decart .
in addition we also use t test with the significant level .05to statistically compare the performance of the two methods for each sample size.
to replicate the decart results we utilize the code published on their project page .
we ran decart with the best hyperparameter tuning technique suggested in their paper grid search combined with fold cross validation.
other hyperparameter settings are the same as described in section of .
results table iii shows the prediction mres of deepperf and decart on six binary subject systems with multiple sample sizes.
it can be seen that deepperf statistically outperforms decart for apache x264 and llvm with all sample sizes.
for these software systems using deepperf both the mre means and their confidence intervals are much lower than those getting from decart .
specifically 1the95 confidence interval of a random variable xis computed as x .95 n x .95 n where xand are the mean and standard deviation of that random variable and nis the number of tests.
in our case the random variable is the mre and n .table iii comparison between deepperf and decart subject systemsample sizedecart deepperf better algorithm mean margin mean margin apache n na na .
.
na 2n .
.
.
.
deepperf 3n .
.
.
.
deepperf 4n .
.
.
.
deepperf 5n .
.
.
.
deepperf x264 n .
.
.
.
deepperf 2n .
.
.
.
deepperf 3n .
.
.
.
deepperf 4n .
.
.
.
deepperf 5n .
.
.
.
deepperf bdb j n .
.
.
.
same 2n .
.
.
.
same 3n .
.
.
.
deepperf 4n .
.
.
.
same 5n .
.
.
.
same llvm n .
.
.
.
same 2n .
.
.
.
deepperf 3n .
.
.
.
deepperf 4n .
.
.
.
deepperf 5n .
.
.
.
deepperf bdb c n .
.
.
.
same 2n .
.
.
.
same 3n .
.
.
.
same 4n .
.
.
.
same 5n .
.
.
.
same sqlite n .
.
.
.
same 2n .
.
.
.
same 3n .
.
.
.
same 4n .
.
.
.
same 5n .
.
.
.
same mean mean of the mres seen in 30experiments.
margin margin of the confidence interval of the mres in 30experiments.
better algorithm is chosen using t test on 30mre data points with significant level .
.
to get the same level of accuracy deepperf needs far less training data than decart .
for example with the system x264 deepperf only needs 2nsample to achieve a prediction mre of .
i.e.
accuracy of .
whilst decart needs 4n 5nsample to have the similar prediction mre.
for bdb c even though there is no statistically significant difference between deepperf and decart most of the time deepperf has much smaller mre means and margins which indicates that deepperf is more consistent than decart .f o r bdb j and sqlite deepperf and decart perform quite similarly.
d. rq2 comparison on software systems with binarynumeric options regarding the problem of predicting performance values of software systems with both binary and numeric options at present splconqueror is the only method that can do this task.
so in this section we will compare the effectiveness ofdeepperf with that of splconqueror .
we will use the five binary numeric subject systems in table ii dune mgs authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
hipacc hsmgp javagc and sac.
these subject systems are also used in for evaluating splconqueror .
setup splconqueror combines different sampling heuristics for binary options i.e.
option wise ow negative option wise now pair wise pw etc and several experimental design methodologies i.e.
plackett burman pbd random design rd etc for numeric options to achieve good prediction accuracy.
so in this experiment we cannot choose sample size as any random value.
thus to compare the two approaches for each subject system we evaluate the two methods using the same sample sizes that splconqueror suggested and for splconqueror the sample is chosen based on the sampling heuristics and experimental designs that are proposed by the authors of splconqueror .
for deepperf the sample is chosen using the random sampling heuristic.
since there are many combinations of sampling heuristics and experimental designs for each subject system we will only pick the best four combinations that enable splconqueror to achieve the highest prediction accuracy using the smallest sample.
besides since each combination yields a unique sample except combinations having random experimental design thus for splconqueror we only report the mean mre on the testing dataset.
for deepperf to reduce fluctuations caused by randomness with each sample size we repeat the random sampling training and testing process times.
we then report both the mre mean and the confidence interval margin on the testing dataset.
the testing dataset consists of all the remaining configurations after selecting the training sample.
for javagc and sac because they have too many configurations therefore we only select randomly selected configurations as their testing datasets.
results table iv shows the prediction mres of deepperf and splconqueror on five binary numeric subject systems with multiple sample sizes.
we can observe that deepperf outperforms splconqueror on subject systems dune mgs hipacc javagc and sac under all sample sizes.
to get the same level of accuracy splconqueror needs much more training data compared to deepperf .
for these software systems both the means and the margins of deepperf s prediction errors are small which indicates that deepperf can consistently predict performance values with high accuracy.
for hsmgp splconqueror performs better than deepperf but the difference is not too large only around .
e. rq3 comparison with svm and other nn based regression methods in this experiment we aim to evaluate whether we need to use a complex model like deep fnn or a much simpler regression method in order to achieve the same level of accuracy.
we also evaluate whether sparsity regularization is actually needed or we can use other regularization methods.
we compare deepperf with the svm regression method and other design alternatives that were discussed in section iii b table iv comparison between deepperf and splconqueror subject systemsample sizesplconqueror deepperf sampling heuristicmean sampling heuristicmean margin dune ow rd .
rd .
.
mgs pw rd .
rd .
.
ow pbd .
rd .
.
ow pbd .
rd .
.
hipacc261 ow rd .
rd .
.
ow pbd .
rd .
.
ow pbd .
rd .
.
pw rd .
rd .
.
hsmgp ow rd .
rd .
.
pw rd .
rd .
.
ow pbd .
rd .
.
ow pbd .
rd .
.
javagc ow pbd .
rd .
.
ow rd .
rd .
.
ow pbd .
rd .
.
pw pbd .
rd .
.
sac ow rd .
rd .
.
ow pbd .
rd .
.
ow pbd rd .
.
pw rd .
rd .
.
mean mean of the mres seen in 30experiments.
margin margin of the confidence interval of the mres seen in 30experiments.
deep fnn with the l1regularization applied to all layers l1 all fnn deep fnn without regularization plain fnn deep fnn with the l2regularization l2 fnn deep fnn with the dropout technique dropoutfnn five binary subject systems in table ii with three sample sizes per system will be used to evaluate the approaches.
setup for the svm regression method we use the function svr in scikit learn package to perform model training and prediction.
scikit learn svr function is implemented using libsvm an svm library proposed in .
to select the best hyperparameters for svr we utilize grid search and fold cross validation.
we construct the grid by varying four hyperparameters in svr values of cranging from .01to1000 values of gamma ranging from .
to values of epsilon ranging from .
to1and the kernel functions are linear poly orrbf.
with this setting the hyperparameter space contains different combinations of hyperparameter values hence we believe it is sufficient enough to find the optimal setting for svr .
for the plain fnn approach we use the step of our proposed hyperparameter search strategy described in section iii c to find the optimal network architecture and the learning rate.
for l1 all fnn l2 fnn and dropout fnn we also use our proposed hyperparameter search strategy to tune the hyperparameters.
all the settings of the hyperparameter searching process are the same as those for deepperf .
except that for dropout fnn the search range for the dropout hyper1102 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v comparison between deepperf svm and other nn based regression methods subject systemsample sizedeepperf l1 all fnn plain fnn l2 fnn dropout fnn svm mean margin mean margin mean margin mean margin mean margin mean margin apache n .
.
.
.
.
.
.
.
.
.
.
.
3n .
.
.
.
.
.
.
.
.
.
.
.
5n .
.
.
.
.
.
.
.
.
.
.
.
x264 n .
.
.
.
.
.
.
.
.
.
.
.
3n .
.
.
.
.
.
.
.
.
.
.
.
5n .
.
.
.
.
.
.
.
.
.
.
.
bdb j n .
.
.
.
.
.
.
.
.
.
.
.
3n .
.
.
.
.
.
.
.
.
.
.
.
5n .
.
.
.
.
.
.
.
.
.
.
.
llvm n .
.
.
.
.
.
.
.
.
.
.
.
3n .
.
.
.
.
.
.
.
.
.
.
.
5n .
.
.
.
.
.
.
.
.
.
.
.
bdb c n .
.
.
.
.
.
.
.
.
.
.
.
3n .
.
.
.
.
.
.
.
.
.
.
.
5n .
.
.
.
.
.
.
.
.
.
.
.
sqlite n .
.
.
.
.
.
.
.
.
.
.
.
3n .
.
.
.
.
.
.
.
.
.
.
.
5n .
.
.
.
.
.
.
.
.
.
.
.
mean mean of the mres seen in 30experiments.
margin margin of the confidence interval of the mres seen in 30experiments.
parameter is since the dropout hyperparameter needs to be smaller than .
and for l1 all fnn the search range for thel1regularization hyperparameter is whilst deepperf s search range is since in this case we apply regularization to all the layers so the regularization hyperparameter needs to be smaller.
results table v shows the prediction mres of the six approaches.
as expected for all subject systems plain fnn performs the worst compared to other nn based approaches.
the reason is that a deep neural network is prone to overfit on training data.
without a regularization technique it cannot produce good predictions for new data.
the l2 fnn and dropout fnn approaches can overcome the overfitting problem of the plain fnn .
however for software performance thel2regularization or dropout technique is not as effective as thel1regularization.
these approaches perform similarly todeepperf on systems apache bdb j llvm sqlite.
however they perform much worse than deepperf on systems bdb c and x264.
for l1 all fnn it performs reasonably well for most of the systems but performs badly on bdb c. lastly forsvm even though it has quite good performance on system sqlite it does not work well on other systems.
f .
rq4 time cost of deepperf the time cost of deepperf on searching for optimal hyperparameters and training a model is reasonable.
specifically for binary subject systems with the number of configuration options nless than apache x264 llvm bdb c when the sample size increases from nto 5n the time taken by hyperparameter searching and model training increases from 1to4minutes.
for binary subject systems with more than 20configuration options bdb j sqlite it takes deepperf 6minutes to do hyperparameters searching and network training.
for binary numeric subject systems with the number of configurations nless than dune mgs hsmgp and with the sample sizes as shown in table iv the time cost for the hyperparameter searching and model training process ranges from 2to5minutes.
for binary numeric subject systems with more than 20configuration options hipacc javagc sac and with the sample sizes as shown in table iv the hyperparameter tuning and model training process takes 4to30minutes.
meanwhile the time cost of decart and splconqueror is from a few seconds to 2minutes for the systems and sample sizes in tables iii and iv.
even though deepperf takes longer time to build the prediction model than decart and splconqueror the time cost of deepperf is still acceptable.
besides it is worth re emphasizing that deepperf can predict performance of binary and or numeric systems with very high accuracy while decart can only predict performance of binary systems and splconqueror needs to use some special sampling heuristics to achieve good prediction accuracy.
note that all the time cost here is measured when running all the methods on a windows computer with intel xeon cpu e51650 .2ghz 16gb ram g. discussions strengths and limitations of our proposed approach the first strength of deepperf is that it can predict performance of highly configurable software systems with both binary and numeric options at higher prediction accuracy than other state of the art approaches.
as shown in our experiments for software systems with binary options compared to decart most of the time deepperf can achieve much better prediction authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
accuracy while using less sample.
for software systems with binary and numeric options deepperf outperforms splconqueror in most of the subject systems for all sample sizes.
deepperf s second strength is that it uses random sampling heuristic to select sample for model training hence it is flexible when constructing the sample.
furthermore it can be incorporated with other sampling heuristics and experimental designs to further improve prediction accuracy and reduce measurement effort.
f ourierlearning and cart decart also use random sampling while splconqueror needs to use some specific sampling heuristics and experimental designs to achieve high prediction accuracy.
finally the third strength of deepperf is that it is both an automated algorithm i.e.
it does not require human effort to tune the hyperparameters and a progressive algorithm i.e.
users can always achieve a much higher model accuracy when having more training data .
this can be seen in tables iii and iv when the sample size increases the prediction accuracy ofdeepperf also increases.
a limitation of deepperf is that it takes longer time to train than decart and splconqueror .
for most of the subject systems using a windows computer with intel xeon cpu e5 .2ghz 16gb ram decart and splconqueror normally take a few seconds to a few minutes to train a model.
meanwhile deepperf takes a few minutes for systems with less than configuration options and can take up to minutes for systems with more than options.
threats to v alidity to increase the internal validity of our experiment results for each sample size we repeat the prediction process times with random training dataset.
for each process the prediction is evaluated on a test dataset which does not include any part of the training dataset.
we use mean relative error as a metric as it is a widely used metric in the literature for evaluating the effectiveness of performance prediction algorithm and it is also used to evaluate other approaches we compared.
note that in the paper to evaluate the algorithm stability we not only evaluate using the mean relative error but also using the confidence interval.
for external validity we evaluate the algorithms using eleven public datasets with different characteristics domains languages etc.
these subject systems have a large range of configuration options and has been used extensively in the literature to evaluate the effectiveness of performance prediction algorithm.
v. r ela ted work many large and complex software systems are highly configurable.
a configuration option can be treated as a feature and a configurable system can be treated as a software product line a family of similar software systems .
a large body of work has been devoted to modeling features and checking consistency of feature configurations e.g.
.
despite their importance quality attributes or non functional requirements such as performance have not been sufficiently addressed in product line practice.
to predict the quality attributes of a product line member zhang et al.
proposed a bayesian belief network bbn based approach.
by performing qualitative analysis over the bbn the quality of a product line member can be estimated.
their method is good for quality attributes such as security reusability etc that are hard to define impossible to measure easy to recognize .
however some quality attributes such as performance can be relatively easy to measure.
a quantitative analysis of these quality attributes can be complementary to the qualitative analysis .
recently researchers have measured performance of several large scale configurable systems and proposed various learning methods to build performance prediction models from these measurements.
in introduction we have described pros and cons of some state of the art methods including splconqueror cart decart and f ourierlearning .
our experimental results show that for most of the evaluated subject systems the proposed deepperf approach outperforms all the related methods i.e.
achieve higher prediction accuracy and use less sample data.
there are also some work on selecting an optimal sample of configurations.
for example sayyad et al.
utilized evolutionary algorithms to select optimal features regarding multiple objectives.
sarkar et al.
used projective sampling and feature frequency heuristic to determine sample that is small enough to decrease the measurement effort and large enough to increase the prediction accuracy.
nair et al.
proposed to use the what spectral learner to select a small number of configurations.
what computes distance matrix between the configurations and performs dimensionality reduction.
later they also proposed a rank based approach which can reduce the cost in terms of the number of configurations to be measured as well as the time required to build performance models.
our work aims to suggest a new learning method to construct software performance model from a sample which is a different goal compared to the above work.
in fact we can combine our learning approach with these work to further improve the accuracy of the performance prediction model and use less sample data.
vi.
c onclusion in this paper we have proposed deepperf a performance prediction model for highly configurable systems based on deep sparse fnn.
we also design a practical hyperparameter search strategy which can automatically find a good set of hyperparameters that can lead to high prediction accuracy within a short time.
the experimental results on public datasets show that deepperf can achieve better performance prediction accuracy with less data when compared to other state of theart approaches.
furthermore deepperf can work with both binary and numeric configuration options.
in the future we will explore the universal property of neural networks in approximations of different function classes to further improve the design of our model.
our experimental data and source code are publicly available at .
acknowledgment.
this work is supported by nsfc grant .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.