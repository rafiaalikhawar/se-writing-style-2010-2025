deeptest automated testing of deep neural network driven autonomous cars yuchi tian university of virginia yuchi virginia.edukexin pei columbia university kpei cs.columbia.edu suman jana columbia university suman cs.columbia.edubaishakhi ray university of virginia rayb virginia.edu abstract recent advances in deep neural networks dnns have led to the developmentofdnn drivenautonomouscarsthat usingsensors likecamera lidar etc.
candrivewithoutanyhumanintervention.
mostmajormanufacturersincludingtesla gm ford bmw and waymo googleareworkingonbuildingandtestingdifferenttypes ofautonomousvehicles.thelawmakersofseveralusstatesincludingcalifornia texas andnewyorkhavepassednewlegislation to fast track the process of testing and deployment of autonomous vehicles on their roads.
however despite their spectacular progress dnns just like traditional software often demonstrate incorrect or unexpected corner casebehaviorsthatcanleadtopotentiallyfatalcollisions.
several such real world accidents involving autonomous cars have alreadyhappenedincludingonewhichresultedinafatality.most existing testing techniques for dnn driven vehicles are heavily dependent on the manual collection of test data under differentdriving conditions which become prohibitively expensive as the number of test conditions increases.
in this paper we design implement and evaluate deeptest a systematic testing tool for automatically detecting erroneous be haviors of dnn driven vehicles that can potentially lead to fatal crashes.first ourtoolisdesignedtoautomaticallygeneratedtest cases leveraging real world changes in driving conditions like rain fog lighting conditions etc.
deeptest systematically explore differ ent parts of the dnn logic by generating test inputs that maximize the numbers of activated neurons.
deeptest found thousands oferroneous behaviors under different realistic driving conditions e.g.
blurring rain fog etc.
manyofwhichleadtopotentiallyfatal crashesinthreetop performingdnnsintheudacityself driving car challenge.
ccs concepts software and its engineering software testing and debugging securityand privacy softwareandapplicationsecurity computing methodologies neural networks permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
deeplearning testing self drivingcars deepneuralnetworks autonomous vehicle neuron coverage acm reference format yuchi tian kexin pei suman jana and baishakhi ray.
.
deeptest automated testing of deep neural network driven autonomous cars.
in icse icse 40th international conference on software engineering may june gothenburg sweden.
acm new york ny usa pages.
introduction significant progress in machine learning ml techniques like deep neural networks dnns over the last decade has enabledthe development of safety critical ml systems like autonomouscars.severalmajorcarmanufacturersincludingtesla gm ford bmw andwaymo googleare buildingand activelytesting these cars.
recent results show that autonomous cars have become very efficientinpracticeandalreadydrivenmillionsofmileswithoutanyhumanintervention .twentyusstatesincludingcalifornia texas and new york have recently passed legislation to enable testing and deployment of autonomous vehicles .
however despitethetremendous progress justlike traditional software dnn based software including the ones used for autonomous driving often demonstrate incorrect unexpected cornercasebehaviorsthatcanleadtodangerousconsequenceslikeafatal collision.
several such real world cases have already been reported see table .
as table1 clearly shows such crashes often happen underrarepreviouslyunseencornercases.forexample thefatal teslacrashresultedfromafailuretodetectawhitetruckagainst the bright sky.
the existing mechanisms for detecting such erro neous behaviors depend heavily on manual collection of labeled testdataoradhoc unguidedsimulation andthereforemiss numerouscornercases.sincethesecarsadaptbehaviorbasedon theirenvironmentasmeasuredbydifferentsensors e.g.
camera infraredobstacledetector etc.
thespaceofpossibleinputsisextremelylarge.thus unguidedsimulationsarehighlyunlikelyto find many erroneous behaviors.
at a conceptual level these erroneous corner case behaviors indnn basedsoftwareareanalogoustologicbugsintraditional software.
similar to the bug detection and patching cycle in traditionalsoftwaredevelopment theerroneousbehaviorsofdnns once detected can be fixed by adding the error inducing inputs to thetrainingdatasetandalsobypossiblychangingthemodelstructure parameters.however thisis achallengingproblem asnoted bylargesoftwarecompanieslikegoogleandteslathathavealready deployed machine learning techniques in several production scale acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden tian et al.
table examples of real world accidents involving autonomous cars reported date cause outcome comments hyundai competition december rain fall crashed while testing the sensors failed to pick up street signs lane markings and even pedestrians due to the angle of the car shifting in rain and the direction of the sun tesla autopilot mode july image contrast killed the driver the camera failed to recognize the white truck against a bright sky google self driving car february failed to estimate speed hita buswhileshifting lane the car assumed that the bus would yield when it attempted to merge back into traffic systemsincludingself drivingcar speechrecognition imagesearch etc.
.
our experience with traditional software has shown that it is hard to build robust safety critical systems only using manual test cases.moreover theinternalsoftraditionalsoftwareandnewdnnbased software are fundamentally different.
for example unlike traditionalsoftwarewheretheprogramlogicismanuallywritten by the software developers dnn based software automatically learnsitslogicfromalargeamountofdatawithminimalhuman guidance.inaddition thelogicofatraditionalprogramisexpressed in terms of control flow statements while dnns use weights for edgesbetweendifferentneuronsandnonlinearactivationfunctions for similar purposes.
thesedifferences make automated testing of dnn based software challenging by presenting several interesting and novel research problems.
first traditionalsoftwaretestingtechniquesforsystematically exploring different parts of the program logic by maximizing branch codecoverageisnotveryusefulfordnn basedsoftware as the logic is not encoded using control flow .
next dnns are fundamentallydifferentfromthemodels e.g.
finitestatemachines usedformodelingandtestingtraditionalprograms.unlikethetradi tionalmodels findinginputsthatwillresultinhighmodelcoverageinadnnissignificantlymorechallengingduetothenon linearity of the functions modeled by dnns.
moreover the satisfiabilitymodulo theory smt solvers that have been quite successful atgenerating high coverage test inputs for traditional software are knowntohavetroublewithformulasinvolvingfloating pointarithmetic and highly nonlinear constraints which are commonly used in dnns.
in fact several research projects have already attempted to build custom tools for formally verifying safety properties of dnns.unfortunately noneof themscale wellto real world sized dnns .finally manuallycreatingspecificationsforcomplex dnn systems like autonomous cars is infeasible as the logic is too complex to manually encode as it involves mimicking the logic of a human driver.
in this paper we address these issues and design a systematic testing methodologyfor automaticallydetecting erroneous behav iors of dnn based software of self driving cars.
first we leverage thenotionofneuroncoverage i.e.
thenumberofneuronsactivated byasetoftestinputs tosystematicallyexploredifferentpartsof thednnlogic.weempiricallydemonstratethatchangesinneuron coverage are statistically correlated with changes in the actions of self drivingcars e.g.
steering angle .therefore neuroncoverage canbeusedasaguidancemechanismforsystemicallyexploring differenttypesofcarbehaviorsandidentifyerroneousbehaviors.
next we demonstrate that different image transformations thatmimicreal worlddifferencesindrivingconditionslikechanging contrast brightness rotationofthecameraresultinactivationof differentsetsofneuronsintheself drivingcardnns.weshowthat bycombiningtheseimagetransformations theneuroncoverage can be increased by on average compared to the coverageachieved by manual test inputs.
finally we use transformationspecific metamorphic relations between multiple executions of the tested dnn e.g.
a car should behave similarly under different lighting conditions toautomatically detect erroneous corner case behaviors.
we found thousands of erroneous behaviors across the three top performing dnns in the udacity self driving car challenge .
the key contributions of this paper are wepresentasystematictechniquetoautomaticallysynthesize testcasesthatmaximizesneuroncoverageinsafety criticaldnn basedsystemslikeautonomouscars.weempiricallydemonstrate that changes in neuron coverage correlate with changes in an autonomous car s behavior.
we demonstrate that different realistic image transformations like changes in contrast presence of fog etc.
can be used to generate synthetic tests that increase neuron coverage.
we leverage transformation specificmetamorphicrelationstoautomatically detect erroneous behaviors.
our experiments also show that the syntheticimagescanbeusedforretrainingandmakingdnns more robust to different corner cases.
we implement the proposed techniques in deeptest to the best of our knowledge the first systematic and automated testing toolfordnn drivenautonomousvehicles.weusedeeptesttosystematicallytestthreetopperformingdnnmodelsfromthe udacity driving challenge.
deeptest found thousands of erro neous behaviors in these systems many of which can lead to potentially fatal collisions as shown in figure .
we have made the erroneous behaviors detected by deeptest available at we also plan to release the generated test images and the source of deeptest for public use.
.
original .
with added rain figure a sample dangerous erroneous behavior found by deeptest in the chauffeur dnn.
background .
deep learning for autonomous driving the key component of an autonomous vehicle is the perception module controlled by the underlying deep neural network dnn .
the dnn takes input from different sensors like camera light detection and ranging sensor lidar and ir infrared sensor that measure the environment and outputs the steering angle braking etc.
necessary to maneuver the car safely under authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deeptest automated testing of deep neural network driven autonomous cars icse may june gothenburg sweden figure a simple autonomous car dnn that takes inputs from camera light detection and ranging sensor lidar and ir infrared sensor and outputs steering angle braking decision andacceleration decision.
the dnn shown here essentially models the function x where s represent the weights of the edges and is the activation function.
the details of the computations performed inside a single neuron are shown on the right.
current conditions as shown in figure .
in this paper we focus on the camera input and the steering angle output.
atypicalfeed forwarddnniscomposedofmultipleprocessing layersstackedtogethertoextractdifferentrepresentationsofthe input .
each layer of the dnn increasingly abstracts the input e.g.
fromrawpixelstosemanticconcepts.forexample thefirstfew layersofanautonomouscardnnextractlow levelfeaturessuch asedgesanddirections whilethedeeperlayersidentifyobjectslike stopsignsandothercars andthefinallayeroutputsthesteering decision e.g.
turning left or right .
eachlayerofadnnconsistsofasequenceofindividualcomputing units called neurons.
the neurons in different layers are connectedwitheachotherthroughedges.eachedgehasacorresponding weight s in figure .
each neuron applies a nonlinear activation function on its inputs and sends the output to the subsequentneuronsasshowninfigure2.popularactivationfunctions include relu rectified linear unit sigmoid etc.
the edgeweightsofadnnisinferredduringthetrainingprocessof thednnbasedonlabeledtrainingdata.mostexistingdnnsare trained with gradient descent using backpropagation .
once trained a dnn can be used for prediction without any further changes to the weights.
for example an autonomous car dnn can predict the steering angle based on input images.
figure2illustratesabasicdnnintheperceptionmoduleofa self driving car.
essentially the dnn is a sequence of linear transformations e.g.
dotproductbetweentheweightparameters of each edge and the output value of the source neuron of that edge and nonlinear activations e.g.
relu in each neuron .
recent results have demonstrated that a well trained dnn fcan predict the steeringanglewithanaccuracyclosetothatofahumandriver .
.
different dnn architectures mostdnnsusedinautonomousvehiclescanbecategorizedinto two types feed forward convolutional neural network cnn and recurrent neural network rnn .
the dnns we tested see section include two cnns and one rnn.
we provide a brief description of each architecture below and refer the interested readers to for more detailed descriptions.
.
a simplified cnn architecture .
a simplified rnn architecture figure upper row a simplified cnn architecture with a convolution kernel shown on the top left part of the input image.
the same filter edges with same weights is then moved across the entire input space and the dot products are computed between theedgeweightsandtheoutputsoftheconnectedneurons.
lowerrow a simplified rnn architecture with loops in its hidden layers.
the unrolledversionontherightshowshowtheloopallowsasequenceofinputs i.e.
images tobefedtothernnandthesteeringangleis predicted based on all those images.
cnnarchitecture.
themostsignificantdifferencebetweenacnn andafullyconnecteddnnisthepresenceofa convolutionlayer.
the neurons in a convolution layer are connected only to some oftheneuronsinthenextlayerandmultipleconnectionsamong differentneuronssharethesameweight.thesetsofconnections sharing the same weights are essentially a convolution kernel that applies the same convolution operation on the outputs of a set ofneuronsinthepreviouslayer.figure3 upperrow illustratestheconvolutionoperationsforthreeconvolutionlayers.thissimplified architecture is similar to the ones used in practice .
convolutionlayers havetwo majorbenefits.first theygreatly reduce the number of trainable weights by allowing sharing of weights among multiple connections and thus significantly cut down the training time.
second the application of convolutionkernels is a natural fit for image recognition as it resembles the human visual system which extracts a layer wise representation of visual input .
rnn architecture.
rnns unlike cnns allow loopsin the network .specifically theoutputofeachlayerisnotonlyfedto thefollowinglayerbutalsoflowbacktothepreviouslayer.such arrangement allows the prediction output for previous inputs e.g.
previousframesinavideosequence tobealsoconsideredinpredictingcurrentinput.figure3 lowerrow illustratesasimplified version of the rnn architecture.
similar to other types of dnns rnns also leverage gradient descent with back propagation for training.
however it is well known that the gradient when propagated through multiple loops inanrnns mayvanishtozeroorexplodetoanextremelylarge value and therefore may lead to an inaccurate model.
long authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden tian et al.
short term memory lstm a popular subgroup of rnns is designedtosolvethisvanishing explodinggradientproblem.we encourage interested readers to refer to for more details.
methodology to develop an automated testing methodology for dnn driven autonomous cars we must answer the following questions.
i how do we systematically explore the input output spaces of an autonomous car dnn?
ii how can we synthesize realistic inputs to automatesuchexploration?
iii howcanweoptimizetheexplorationprocess?
iv howdoweautomaticallycreateatestoracle that can detect erroneous behaviors without detailed manual specifications?webrieflydescribehowdeeptestaddresseseachofthese questions below.
.
systematic testing with neuron coverage theinput outputspace i.e.
allpossiblecombinationsofinputsand outputs ofacomplexsystemlikeanautonomousvehicleistoolarge for exhaustive exploration.
therefore we must devise a systematic wayofpartitioningthespaceintodifferentequivalenceclassesand try to cover all equivalence classes by picking one sample from each of them.
in this paper we leverage neuron coverage a sa mechanismforpartitioningtheinputspacebasedontheassumption that all inputs that have similar neuron coverage are part of the sameequivalenceclass i.e.
thetargetdnnbehavessimilarlyfor these inputs .
neuroncoveragewasoriginallyproposedbypei et al.forguided differential testing of multiple similar dnns .
it is defined as theratioofuniqueneuronsthatgetactivatedforgiveninput s and the total number of neurons in a dnn neuron covera e activated neurons total neurons anindividualneuronisconsideredactivatediftheneuron soutput scaled by the overall layer s outputs is larger than a dnn wide threshold.
in this paper we use .
as the neuron activation threshold for all our experiments.
similar to the code coverage guided testing tools for traditional software deeptest tries to generate inputs that maximize neuron coverage of the test dnn.
as each neuron s output affects the finaloutputofadnn maximizingneuroncoveragealsoincreasesoutput diversity.
we empirically demonstrate this effect in section .
peiet al.definedneuroncoverageonlyforcnns .wefurther generalize the definition to include rnns.
neurons depending on thetypeofthecorrespondinglayer mayproducedifferenttypesof outputvalues i.e.singlevalueandmultiplevaluesorganizedina multidimensional array .
we describe how we handle such cases in detail below.
forallneuronsinfully connectedlayers wecandirectlycompare their outputs against the neuron activation threshold as these neuronsoutputasinglescalarvalue.bycontrast neuronsinconvolutional layers output multidimensional feature maps as each neuronoutputstheresultofapplyingaconvolutionalkernelacross the input space .
for example the first layer in figure .
illustratestheapplicationofoneconvolutionalkernel ofsize3 totheentireimage thatproducesafeaturemapofsize3 inthesucceedinglayer.insuchcases wecomputetheaverageof the output feature map to convert the multidimensional output ofaneuronintoascalarandcompareittotheneuronactivation threshold.for rnn lstm with loops the intermediate neurons are unrolled to produce a sequence of outputs figure .
.
we treat each neuronintheunrolledlayersasaseparateindividualneuronfor the purpose of neuron coverage computation.
.
increasing coverage with synthetic images generatingarbitraryinputsthatmaximizeneuroncoveragemay not be very useful if the inputs are not likely to appear in the realworldeveniftheseinputspotentiallydemonstratebuggybehaviors.
therefore deeptestfocusesongeneratingrealisticsyntheticimagesbyapplyingimagetransformationsonseedimagesandmimic different real world phenomena like camera lens distortions object movements different weatherconditions etc.to this end we investigateninedifferentrealisticimagetransformations changingbrightness changingcontrast translation scaling horizontal shearing rotation blurring fogeffect andraineffect .thesetransformations can be classified into three groups linear affine and convolutional.
our experimental results as described in section demonstrate that all of these transformations increase neuron coveragesignificantlyforallofthetesteddnns.below wedescribe the details of the transformations.
adjusting brightness and contrast are both linear transformations.thebrightnessofanimagedependsonhowlargethepixel valuesareforthatimage.animage sbrightnesscanbeadjustedby adding subtractingaconstantparameter toeachpixel scurrent value.
contrast represents the difference in brightness between different pixels in an image.
one can adjust an image s contrast by multiplying each pixel s value by a constant parameter .
table different affine transformation matrices affine transform example transformation matrix parameters translation bracketleftbigg10tx 01ty bracketrightbigg tx displacement along x axis ty displacement along y axis scale bracketleftbiggsx00 0sy0 bracketrightbigg sx scale factor along x axis sy scale factor along y axis shear bracketleftbigg1sx0 sy10 bracketrightbigg sx shear factor along x axis sy shear factor along y axis rotation bracketleftbiggcosq sinq0 sinqcosq0 bracketrightbigg q the angle of rotation translation scaling horizontal shearing and rotation are all differenttypesofaffinetransformations.anaffinetransformationisalinearmappingbetweentwoimagesthatpreservespoints straight lines and planes .
affine transforms are often used in image processingtofixdistortionsresultingfromcameraanglevariations.
in this paper we leverage affine transformations for the inversecase i.e.
to simulate different real world camera perspectives or movementsofobjectsandcheckhowrobusttheself drivingdnns are to those changes.
an affine transformation is usually represented by a transformation matrix m .
one can apply an affine transformation toa2dimagematrix ibysimplycomputingthedotproductof i andm the corresponding transformation matrix.
we list the transformation matrices for the four types of affine transformations translation scale shear androtation usedinthispaperintable2.
blurring andadding fog raineffects are allconvolutional transformations i.e.
they perform the convolution operation on the inputpixelswithdifferenttransform specifickernels.aconvolutionoperationadds weightedbythekernel eachpixeloftheinput authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deeptest automated testing of deep neural network driven autonomous cars icse may june gothenburg sweden imagetoitslocalneighbors.weusefourdifferenttypesofblurring filters averaging gaussian median and bilateral .
we compose multiple filters provided by adobe photoshop on the input images to simulate realistic fog and rain effects .
.
combining transformations to increase coverage as the individual image transformations increase neuron coverage oneobvious questionis whethertheycan becombinedto further increase the neuron coverage.
our results demonstrate that differentimagetransformationstendtoactivatedifferentneurons i.e.
theycanbe stacked togethertofurtherincreaseneuron coverage.
however thestatespaceofallpossiblecombinationsofdifferent transformations is too large to explore exhaustively.
we provide a neuron coverage guided greedy search technique for efficiently findingcombinationsofimagetransformationsthatresultinhigher coverage see algorithm .
algorithm greedy search for combining image tranformations to increase neuron coverage input transformations t seed images i output synthetically generated test images variable s stack for storing newly generated images tqueue transformation queue 2push all seed imgs i to stack s 3gentests 4whilesis not empty do 5img s.pop 6tqueue 7numfailedtries 8whilenumfailedtries maxfailedtries do iftqueue is not empty then t1 tqueue.dequeue else randomly pick transformation t1 from t end randomly pick parameter p1 for t1 randomly pick transformation t2 from t randomly pick parameter p2 for t2 newimage applytransforms image t1 p1 t2 p2 ifcovinc newimage then tqueue.enqueue t1 tqueue.enqueue t2 updatecoverage gentest gentests newimage s.push newimage else numfailedtries numfailedtries end end 27end 28return gentests the algorithm takes a set of seed images i a list of transformationstandtheircorrespondingparametersasinput.thekey ideabehindthealgorithmistokeeptrackofthetransformations thatsuccessfullyincreaseneuroncoverageforagivenimageand prioritizethemwhilegeneratingmoresyntheticimagesfromthe given image.
this process is repeated in a depth first manner to all images.
.
creating a test oracle with metamorphic relations oneofthemajorchallengesintestingacomplexdnn basedsystem likeanautonomousvehicleiscreatingthesystem sspecifications manually againstwhich the system sbehaviorcan bechecked.
it is challenging to create detailed specifications for such a system as it essentially involves recreating the logic of a human driver.
toavoidthisissue weleveragemetamorphicrelations between the car behaviors across different synthetic images.
the key insight is that even though it is hard to specify the correct behavior ofaself drivingcarforeverytransformedimage onecandefine relationshipsbetweenthecar sbehaviorsacrosscertaintypesoftransformations.
for example the autonomous car s steering angleshouldnotchange significantlyforthesameimageunderany lighting weatherconditions blurring oranyaffinetransformations withsmallparametervalues.thus ifadnnmodelinfersasteering angle ofor an input seed image ioand a steering angle tfor a new syntheticimage it whichis generatedby applyingthe transformation tonio onemaydefineasimplemetamorphicrelation where oand tare identical.
however thereisusuallynosinglecorrectsteeringanglefora givenimage i.e.
acarcansafelytoleratesmallvariations.therefore thereisatrade offbetweendefiningthemetamorphicrelationsvery tightly like the one described above may result in a large number offalse positives and makingthe relationsmore permissive may lead to many false negatives .
in this paper we strike a balance betweenthesetwoextremesbyusingthemetamorphicrelations defined below.
to minimize false positives we relax our metamorphic relations andallowvariationswithintheerrorrangesoftheoriginalinputim ages.weobservethatthesetofoutputspredictedbyadnnmodel for the original images say o1 o2 .... on in practice result in a small but non trivial number of errors w.r.t.their respective manual labels 1 2 .... n .
such errors are usually measured using mean squared error mse where mseori n summationtext.1n i i oi .
leveraging this property we redefine a new metamorphic relation as i ti mseori theaboveequationassumesthattheerrorsproducedbyamodel for the transformed images as input should be within a range of times the mse produced by the original image set.
here is a configurable parameter that allows us to strike a balance between the false positives and false negatives.
implementation autonomousdrivingdnns.
weevaluateourtechniquesonthree dnn models that won top positions in the udacity self driving challenge rambo 2ndrank chauffeur 3rdrank andepoch 6thrank .wechoosethesethreemodelsastheir implementationsarebasedonthekerasframework thatour current prototype of deeptest supports.
the details of the dnn models and dataset are summarized in table .
as shown in the right figure of table the steering angle is definedastherotationdegreebetweentheheadingdirectionofthe vehicle thevertical line and theheading directions of thesteering wheel axles i.e.
usually front wheels .
the negative steering angle indicatesturningleftwhilethepositivevaluesindicateturningleft.
themaximumsteeringangleofacarvariesbasedonthehardware of different cars.
the udacity self driving challenge dataset used in thispaperhasamaximumsteeringangleof 25degree .the steering angle is then scaled by so that the prediction should fall between and .
rambomodelconsistsofthreecnnswhoseoutputsaremerged using a final layer .
two of the cnns are inspired by nvidia s authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden tian et al.
no.
of reported our model sub model neurons mse mse chauffeurcnn .
.06lstm rambos1 cnn .
.
s2 cnn s3 cnn epoch cnn .
.
dataset hmb 3.bag table left details of dnns used to evaluate deeptest.
right the outputs of the dnns are the steering angles for a self driving car heading forward.
the udacity self driving car has a maximum steering angle of degree.
self driving car architecture and the third cnn is based on comma.ai s steering model .
as opposed to other models that takeindividualimagesasinput rambotakesthedifferencesamong three consecutive images as input.
the model uses keras and theano frameworks.
chauffeur modelincludesonecnnmodelforextractingfeatures from the image and one lstm model for predicting steering angle .
the input of the cnn model is an image while the input ofthelstmmodelistheconcatenationof100featuresextractedby thecnnmodelfromprevious100consecutiveimages.chauffeur uses keras and tensorflow frameworks.
epochmodelusesasinglecnn.asthepre trainedmodelfor epoch is not publicly available we train the model using the instructions provided by the authors .
we used the ch2 002 dataset from the udacity self driving challenge for training the epoch model.
epoch similar to chauffeur uses keras and tensorflow frameworks.
image transformations.
in the experiments for rq2 and rq3 we leverage seven different types of simple image transformations translation scaling horizontalshearing rotation contrastadjustment brightness adjustment and blurring.
we use opencv to implementthesetransformations .forrq2andrq3described insection5 weuse10parametersforeachtransformationasshown in table .
table4 transformationsandparametersusedbydeeptestforgenerating synthetic images.
transformations parameters parameter ranges translation tx ty to step scale sx sy .
.
to step .
.
shear sx sy .
to .
step .
rotation q degree 3to30with step3 contrast gain .2to3.0with step0.
brightness bias 10to100with step10 averaging kernel size gaussian kernel size blur median aperture linear size bilateral filter diameter sigmacolor sigmaspace results as dnn based models are fundamentally different than traditional software first wecheckwhetherneuroncoverageisagoodmetrictocapturefunctionaldiversityofdnns.inparticular weinvestigate whetherneuroncoveragechangeswithdifferentinput outputpairs of anautonomouscar.
anindividual neuron soutput goesthroughasequenceoflinearandnonlinearoperationsbeforecontributing tothefinaloutputsofadnn.therefore itisnotveryclearhow much if at all individual neuron s activation will change the final output.
we address this in our first research question.
table relation between neuron coverage and test output steering steering model sub model angle direction spearman wilcoxon effect size correlation test cohen s d chauffeur overall .
left ve right ve negligible cnn .
left ve right ve negligible lstm .
left ve right ve negligible rambo overall .
left ve right ve negligible s1 .
left ve right ve large s2 .
not significant negligible s3 .
not significant negligible epoch n a .
left ve right ve small indicates statistical significance with p value .
rq1.do different input output pairs result in different neuron coverage?
for each input image we measure the neuron coverage see equation in section .
of the underlying models and the corresponding output.
as discussed in section corresponding to aninput image each model outputs a steering direction left ve right ve and a steering angle as shown in table right .
we analyze the neuron coverage for both of these outputs separately.
steering angle .assteeringangleisacontinuousvariable we check spearman rank correlation between neuron coverage andsteeringangle.thisisanon parametricmeasuretocompute monotonic association between the two variables .
correlation withpositivestatisticalsignificancesuggeststhatthesteeringangle increases with increasing neuron coverage and vice versa.
table showsthatspearmancorrelationsforallthemodelsarestatistically significant whilechauffeurandrambomodelsshowanoverall negative association epoch model shows a strong positive correlation.thisresultindicatesthattheneuroncoveragechangeswith the changes in output steering angles i.e.different neurons get activated for different outputs.
thus in this setting neuron coverage can be a good approximation for estimating the diversity of input outputpairs.moreover ourfindingthatmonotoniccorrelationsbetweenneuroncoverageandsteeringanglealsocorroborate goodfellow et al.
s hypothesis that in practice dnns are often highly linear .
steering direction .
to measure the association between neuroncoverageandsteeringdirection wecheckwhetherthecoverage varies between right and left steering direction.
we use the wilcoxon nonparametric test as the steering direction can only havetwovalues leftandright .ourresultsconfirmthatneuron coverage varies with steering direction with statistical significance p .
forallthethreeoverallmodels.interestingly for rambo only the rambo s1 sub model shows statistically signif icantcorrelationbutnotrambo s2andrambo s3.theseresults suggestthat unlikesteeringangle somesub modelsaremoreresponsible than other for changing steering direction.
overall these results show that neuron coverage altogether variessignificantlyfordifferentinput outputpairs.thus aneuroncoverage directed ndg testingstrategycanhelpinfindingcorner cases.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deeptest automated testing of deep neural network driven autonomous cars icse may june gothenburg sweden .
difference in neuron coverage caused by different image transformations .
average cumulative neuron coverage per input image figure different image transformations activate significantly different neurons.
in the top figure the median jaccard distances for chauffeur cnn chauffeur lstm epoch rambo s1 rambo s2 and rambo s3 models are .
.
.
.
.
.
and .
.
result neuron coverage is correlated with input output diversity and can be used to systematic test generation.
next we investigate whether synthetic images generated by applying different realistic image transformations as described in table on existing input images can activate different neurons.
thus we check rq2.dodifferentrealisticimagetransformationsactivatedifferent neurons?
we randomly pick input images from the test set and transformeachofthembyusingsevendifferenttransformations blur brightness contrast rotation scale shear andtranslation.we also vary the parameters of each transformation and generate a total of new synthetic images.
we run all models with these syntheticimagesasinputandrecordtheneuronsactivatedbyeach input.
wethencomparetheneuronsactivatedbydifferentsynthetic images generated from the same image.
let us assume that two transformations t1andt2 when applied to an original image i activatetwosetsofneurons n1andn1respectively.wemeasure thedissimilaritiesbetween n1andn2bymeasuringtheirjaccard distance n1 n2 n1 n2 .
figure4.1showstheresultforallpossiblepairoftransformations e.g.
blurvs.rotation rotationvs.transformation etc.
fordifferent models.theseresultsindicatethatforallmodels exceptchauffeurlstm different transformations activate different neurons.
as discussedinsection2.
lstmisaparticulartypeofrnnarchitecturethatkeepsstatesfrompreviousinputsandhenceincreasing the neuron coverage of lstm models with single transformations is much harder than other models.
in this paper we do not explore this problem any further and leave it as an interesting future work.
wefurthercheckhowmuchasingletransformationcontributes inincreasingtheneuroncoverage w.r.t.allothertransformations foragivenseedimage.thus ifanoriginalimage iundergoesseven discretetransformations t1 t2 ...t7 wecomputethetotalnumber of neuronsactivated by t1 t1 t2 ... uniontext.
i 1ti.
figure4.
showsthe cumulative effect of all the transformations on average neuroncoverage per seed image.
we see that the cumulative coverageincreases with increasing number of transformations for all themodels.
in other words all the transformations are contributing towards the overall neuron coverage.
we also compute the percentage change in neuron coverage per image transformation nt w.r.t.to the corresponding seed image no as nt no no.
figure shows the result.
for all the studied models the transformed images increase the neuron coverage significantly wilcoxon nonparametric test confirms the statisticalsignificance.theseresultsalsoshowthatdifferentimage transformations increase neuron coverage at different rates.
result2 differentimagetransformationstendtoactivate different sets of neurons.
next we mutate the seed images with different combinations of transformations see section .
since different imagetransformationsactivatedifferentsetofneurons herewetrytoincreasethe neuroncoveragebythesetransformedimageinputs.tothisend we question rq3.canneuron coveragebefurtherincreased bycombining different image transformations?
weperformthisexperimentbymeasuringneuroncoveragein two different settings i applying a set of transformations and ii combining transformations using coverage guided search.
i cumulative transformations.
since different seed images activate a different set of neurons see rq1 multiple seed images collectively achieve higher neuron coverage than a single one.
hence we check whether the transformed images can still increase the neuroncoveragecollectively w.r.t.thecumulativebaselinecoverage ofa setofseed images.inparticular wegenerate atotalof 000images from seed images by applying transformations and varying10parameterson100seedimages.thisresultsinatotalof 000testimages.wethencomparethecumulativeneuroncoverage ofthesesyntheticimages w.r.t.thebaseline whichusethesame100 seedimagesforfaircomparison.table6showstheresult.acrossall the models except rambo s3 the cumulative coverage increased significantly.
since the rambo s3 baseline already achieved coverage the transformed images only increase the coverage by .
.
ii guidedtransformations.
finally wecheckwhetherwecanfurtherincreasethecumulativeneuroncoveragebyusingthecoverageguided search technique described in algorithm .
we generate and864imagesfrom100seedimagesforchauffeur cnn authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden tian et al.
median increase in neuron coverage transformation chauffeur epoch rambo cnn lstm s1 s2 s3 scale .
.
.
.
.
.
.
.
brightness .
.
.
.
.
.
.
contrast .
.
.
.
.
.
.
blur .
.
.
.
.
.
rotation .
.
.
.
.
.
.
translation .
.
.
.
.
.
.
shear .
.
.
.
.
.
.
all numbers are statistically significant numbers with and have small and large cohen s d effect.
figure neuron coverage per seed image for individual image transformations w.r.t.baseline.
table6 neuroncoverageachievedbycumulativeandguidedtransformations applied to seed images.
cumulative guided increase of guided w.r.t.
model baseline transformation generation baseline cumulative chauffeur cnn epoch rambo s1 rambo s2 rambo s3 .
.
epoch andrambomodelsrespectivelyandmeasuretheircollective neuron coverage.
as shown in table the guided transformations collectivelyachieve88 and98 oftotalneurons formodelschauffeur cnn epoch rambo s1 rambo s2 and rambo s3 respectively thus increasing the coverage up to and .
w.r.t.the unguided approach.
this method alsosignificantlyachieveshigherneuroncoverage w.r.t.baseline cumulative coverage.
result3 bysystematicallycombiningdifferentimagetransformations neuroncoveragecanbeimprovedbyaround100 w.r.t.
the coverage achieved by the original seed images.
next we check whether the synthetic images can trigger any erroneous behavior in the autonomous car dnns and if we can detectthosebehaviorsusingmetamorphicrelationsasdescribedin section .
.
this leads to the following research question rq4.canweautomaticallydetecterroneousbehaviorsusing metamorphic relations?
figure6 deviationsfromthehumanlabelsforimagesthatviolate themetamorphicrelation seeequation2 ishighercomparedtothe deviationsfororiginalimages.thus thesesyntheticimageshavea high chance to show erroneous behaviors.
herewefocusonthetransformedimageswhoseoutputsviolate the metamorphic relation defined in equation .
we call these imagesierrand their corresponding original images as ior .w e comparethedeviationbetweentheoutputsof ierrandior w.r.t.the corresponding human labels as shown in figure .
the deviations produced for ierrare much larger than ior also confirmed by wilcoxon test for statistical significance .
in fact mean squared error mse for ierris .
while the mse of the corresponding ior is .
.
such differences also exist for other synthetic images that are generated by composite transformations including rain fog and those generated during the coverage guided search.
thus overallierrhas a higher potential to show buggy behavior.
however for certain transformations e.g.
rotation not all violations of themetamorphic relations can be considered buggy as thecorrectsteeringanglecanvarywidelybasedonthecontentsof thetransformedimage.forexample whenanimageisrotatedbya largeamount say30degrees itisnontrivialtoautomaticallydefine itscorrectoutputbehaviorwithoutknowingitscontents.toreduce suchfalsepositives weonlyreportbugsforthetransformations e.g.
smallrotations rain fog etc.
wherethecorrectoutputshould notdeviatemuchfromthelabelsofthecorrespondingseedimages.
thus we further use a filtration criteria as defined in equation to identify such transformations by checking whether the mse of the synthetic images is close to that of the original images.
mse trans param mseor thus we only choose the transformations that obey equation for counting erroneous behaviors.
table shows the number of such erroneous cases by varying two thresholds and a higher value of and lower value of makes the system report fewer bugs and vice versa.
for example with a of and of .
we report330violationsforsimpletransformations.wedonotenforce thefiltrationcriteriaforcompositetransformations.rainandfog effects should produce same outputs as original images.
also in guidedsearchsincemultipletransformationsproducethesynthesized images it is not possible to filter out a single transformation.
thus for rain fog and guided search we report and erroneous behavior respectively for across all three models.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deeptest automated testing of deep neural network driven autonomous cars icse may june gothenburg sweden original fog original rain original translation original scale .5x original shear .
original rotation degree original contrast .
original brightness figure sample images showing erroneous behaviors detected by deeptest using synthetic images.
for original images the arrows are marked in blue while for the synthetic images they are marked in red.
more such samples can be viewed at io deeptest .
table number of erroneous behaviors reported by deeptest across all tested models at different thresholds simple tranformation composite transformation see eqn.
fog rain guided see eqn.
.
.
.
.
.
search table number of unique erroneous behaviors reported by deeptest for different models with see eqn.
transformation chauffeur epoch rambo simple transformation blur brightness contrast rotation scale shear translation composite transformation rain fog guided table8furtherelaboratestheresultfordifferentmodelsfor and .
as highlighted in table .
interestingly some models aremorepronetoerroneousbehaviorsforsometransformations thanothers.forexample rambo produces23erroneouscasesfor shear while the other two models do not show any such cases.
similarly deeptest finds instances of erroneous behavior in chauffeur for rain but only and for epoch and rambo respec tively.intotal deeptestdetects6339erroneousbehaviorsacrossall three models.
figure further shows some of the erroneous behaviors that are detected by deeptest under different transformations that can lead to potentially fatal situations.
wealsomanuallycheckedthebugsreportedintable8andreport thefalsepositivesinfigure8.italsoshowstwosyntheticimages the corresponding original images where deeptest incorrectly reportserroneousbehaviorswhilethemodel soutputisindeedsafe.
although such manual investigation is by definition subjective and approximate all the authors have reviewed the images and agreed on the false positives.simple model transformation guided rain fog total epoch chauffeur rambo total original translation epoch original shear .
rambo figure sample false positives produced by deeptest for .
result with neuron coverage guided synthesized images deeptestsuccessfullydetectsmorethan1 000erroneousbehavior as predicted by the three models with low false positives.
rq5.can retrainingdnns withsynthetic images improveaccuracy?
table9 improvementinmseafterretrainingofepochmodelwith synthetic tests generated by deeptest test set original mse retrained mse original images .
.
with fog .
.
with rain .
.
herewecheckwhetherretrainingthednnswithsomeofthe syntheticimagesgeneratedbydeeptesthelpsinmakingthednns more robust.we used the images from hmb 3.bag and created their synthetic versions by adding the rain and fog effects.
we retrained the epoch model with randomly sampled of thesesynthetic inputs along with the original training data.
we evaluatedboththeoriginalandtheretrainedmodelontherest34 of the synthetic images and their original versions.
in all cases theaccuracy of the retrained model improved significantly over the original model as shown in table .
result accuracy of a dnn can be improved up to by retrainingthednnwithsyntheticdatageneratedbydeeptest.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden tian et al.
threats to validity deeptest generates realistic synthetic images by applying different imagetransformationsontheseedimages.however thesetransformations are not designed to be exhaustive and therefore may not cover all realistic cases.
while our transformations like rain and fog effects are designed to be realistic the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors e.g.
the position of the sun the angle and size of the rain drops.
etc.
however as the image processing techniques become more sophisticated the generated pictures will get closer to reality.
acompletednnmodelfordrivinganautonomousvehiclemust alsohandlebrakingandaccelerationbesidesthesteeringangle.we restricted ourselves to only test the accuracy of the steering angle asourtestedmodelsdonotsupportbrakingandaccelerationyet.
however our techniques should be readily applicable to testing those outputs too assuming that the models support them.
related work testing of driver assistance systems.
abdessalem et al.
proposed a technique for testing advanced driver assistance systems adas in autonomous cars that show warnings to the drivers if it detectspedestriansinpositionswithlowdrivervisibility .they use multi objective meta heuristic search algorithms to generate tests that simultaneously focus on the most critical behaviors of the system and the environment as decided by the domain experts e.g.
moving pedestrians in the dark .
thekeydifferencesbetweenthisworkandoursarethreefold i we focus on testing the image recognition and steering logic intheautonomouscardnnswhiletheirtechniquetestedadas system s warning logic based on preprocessed sensor inputs ii their blackbox technique depends on manually selected critical scenarioswhileourgray boxtechniquelooksinsidethednnmodel andsystematicallymaximizeneuroncoverage.thetrade offisthat their technique can in theory work for arbitrary implementations while our technique is tailored for dnns and iii we leverage metamorphic relations for creating a test oracle while they depend on manual specifications for detecting faulty behavior.
testing and verification of machine learning.
traditional practices in evaluating machine learning systems primarilymeasure their accuracy on randomly drawn test inputs from manually labeleddatasetsandadhocsimulations .however without the knowledgeof the model s internals suchblackbox testing paradigmsarenotabletofinddifferentcorner casesthatmayinduce unexpected behaviors .
peiet al.
proposed deepxplore a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple dnns.
they introduced neuron coverage as a systematic metric for measuring how much of the internallogicofadnnshavebeentested.bycontrast ourgrayboxmethodsuseneuroncoverageforguidedtestgenerationinasingle dnn and leverage metamorphic relations to identify erroneous behaviors without requiring multiple dnns.
anotherrecentlineofworkhasexploredthepossibilityofverifying dnns against different safety properties .
however none of these techniques can verify a rich set of properties for realworld sized dnns.
by contrast our techniques can systematicallyteststate of the artdnnsforsafety criticalerroneousbehaviors but do not provide any theoretical guarantees.
adversarial machine learning.
a large number of projects successfully attacked machine learning models at test time by forcingittomakeunexpectedmistakes.morespecifically theseattacks focusonfindinginputsthat whenchangedminimallyfromtheir original versions get classified differently by the machine learningclassifiers.thesetypesofattacksareknowntoaffectabroad spectrum of tasks such as image recognition facedetection verification malwaredetection andtext analysis .
severalprior works haveexploreddefensesagainsttheseattackswithdifferenteffectiveness .
insummary thislineofworktriestofindaparticularclassof erroneousbehaviors i.e.
forcingincorrectpredictionbyaddinga minimumamountofnoisetoagiveninput.bycontrast wesystematically test a given dnn by maximizing neuron coverage and find a diverse set of corner case behaviors.
moreover we specifically focus on finding realistic conditions that can occur in practice.
test amplification.
there is a large body of work on test case generation and amplification techniques for traditional software thatautomaticallygeneratetestcasesfromsomeseedinputsand increase code coverage.
instead of summarizing them individually here we refer the interested readers to the surveys by anand et al.
mcminn et al.
and pasareanu et al.
.
unlike these approaches deeptest is designed to operate on dnns.
metamorphic testing.
metamorphic testing is a way of creating test oracles in settings where manual specifications arenotavailable.metamorphictestingidentifiesbuggybehavior bydetectingviolationsofdomain specificmetamorphicrelations that are defined across outputs from multiple executions of the test program with different inputs.
for example a sample metamorphic property for program padding two inputs aandbcan bep a b p a p b .
metamorphic testing has been used inthepastfortestingbothsupervisedandunsupervisedmachine learningclassifiers .bycontrast wedefinenewmetamorphic relations in the domain of autonomous cars which unlike the classifierstestedbefore produceacontinuoussteeringangle i.e.
it is a regression task.
conclusion inthispaper weproposedandevaluateddeeptest atoolforautomatedtestingofdnn drivenautonomouscars.deeptestmaximizestheneuroncoverageofadnnusingsynthetictestimages generatedbyapplyingdifferentrealistictransformationsonaset ofseedimages.weusedomain specificmetamorphicrelationsto finderroneousbehaviorsofthednnwithoutdetailedspecification.deeptestcanbeeasilyadaptedtotestotherdnn basedsystemsby customizingthetransformationsandmetamorphicrelations.we believe deeptest is an important first step towards building robust dnn based systems.