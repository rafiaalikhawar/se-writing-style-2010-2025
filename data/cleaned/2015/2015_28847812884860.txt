pac learning based verification and model synthesis yu fang chen academia sinicachiao hsieh academia sinica national taiwan universityond rej leng l academia sinica brno university of technology tsung ju lii academia sinica national taiwan universityming hsien tsai academia sinicabow y aw wang academia sinica farn wang national taiwan university abstract we introduce a novel technique for veri cation and model synthesis of sequential programs.
our technique is based on learning an approximate regular model of the set of feasible paths in a program and testing whether this model contains an incorrect behavior.
exact learning algorithms require checking equivalence between the model and the program which is a di cult problem in general undecidable.
our learning procedure is therefore based on the framework of probably approximately correct pac learning which uses sampling instead and provides correctness guarantees expressed using the terms error probability and con dence.
besides the veri cation result our procedure also outputs the model with the said correctness guarantees.
obtained preliminary experiments show encouraging results in some cases even outperforming mature software veri ers.
.
introduction formal veri cation of software aims to prove software properties through rigorous mathematical reasoning.
consider for example the c statement assert x specifying that the value of the variable xmust be positive.
if the assertion is formally veri ed it cannot be violated in any possible execution during runtime.
formal veri cation techniques are however often computationally expensive.
although sophisticated heuristics have been developed to improve scalability of the techniques formally verifying real world software is still considered to be impractical.
a common technique to ensure quality in industry is software testing.
errors in software can be detected by exploring di erent software behaviors via injecting various testing vectors.
testing cannot however guarantee software is free from errors.
consider again the assertion assert x .
unless all system behaviors are explored by testing vectors it is unsound to conclude that the value of xis always positive.
various techniques have been proposed to improve permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c acm.
isbn .
.
.
.
coverage of testing but it is its inherent feature that it cannot establish program properties conclusively.
in this paper we propose a novel learning based approach that aims to balance scalability and coverage of existing software engineering techniques.
in order to be scalable as for software testing our new technique explores only a subset of all program behaviors.
we however apply machine learning to generalize observed program behaviors for better semantic coverage.
our technique allows software engineers to combine scalable testing with high coverage formal analyses and improve the quality assurance process.
we hope that this work reduces the dichotomy between formal and practical software engineering techniques.
in our technical setting we assume programs are annotated with program assertions.
a program assertion is a boolean expression intended to be true every time it is encountered during program execution.
given a program with assertions our task is to check whether all assertions evaluate to true on all possible executions.
in principle the problem can be solved by examining all program executions.
it is however prohibitive to inspect all executions exhaustively since there may be in nitely many of them.
one way to simplify the analysis is to group the set of program executions to paths of a control ow graph.
acontrol ow graph cfg is derived from the syntactic structure of a program source code.
each execution of a program corresponds to a path in its control ow graph.
one can therefore measure the completeness of software testing using cfgs.
line coverage for instance gives the ratio of explored edges in the cfg of the tested program while branch coverage is the ratio of explored branches of this cfg.
note that such syntactic measures of code coverage approximate program executions only very roughly.
executions that di er in the number of iterations in a simple program loop have the same line and branch coverages although their computation may be drastically di erent.
a full syntactic code coverage does not necessarily mean all executions have been explored by software testing.
observe that program executions traversing the same path in a cfg perform the same sequence of operations although maybe with di erent values .
consider a path corresponding to a program execution in a cfg.
such a path can be characterized by the sequence of decisions that the execution took when traversing conditional statements in the cfg.
we call a sequence of such decisions a decision vector .
a decision vector is feasible if it represents one or more possibly in nitely many program executions and infeasible if it represents a sequence of branching choices than can never occur ieee acm 38th ieee international conference on software engineering in an execution of the program.
to check whether all assertions evaluate to trueon all executions it su ces to examine all feasible decision vectors and check they do not represent any assertion violating program execution.
although feasibility of a decision vector can be determined by using an o the shelf satis ability modulo theories smt solver the set of feasible decision vectors is in general di cult to compute exactly.
therefore we apply algorithmic learning in particular the framework of probably approximately correct learning to construct a regular approximation of this set.
within the framework of probably approximately correct pac learning with queries learning algorithms query about target concepts to construct hypotheses.
the constructed hypotheses are then validated by sampling.
if a hypothesis is invalidated by witnessing a counterexample learning algorithms re ne the invalidated hypothesis by the witness and more queries.
if on the other hand a hypothesis conforms to all samples pac learning algorithms return the inferred hypothesis with statistical guarantees.
in our approach we adopt a pac learning algorithm with queries to infer a regular language approximation to the set of feasible decision vectors of a program.
to grasp the statistical guarantees provided by pac learning consider the task of checking defects in a large shipment using uniform sampling.
because of the size of the shipment it is impractical to check every item.
we instead want to know with a given con dence if the defect probability is at most .
this can be done by selecting r to be determined later randomly chosen items.
if all chosen items are good the method reports that the defect probability is at most .
we argue the simple method can err with probability at most .
supposerrandomly chosen items are tested without any defect but the defect probability is in fact more than .
under this thesis the probability that r random items are all good is lower than r. that is the method is incorrect with probability lower than r. takersuch that r .
the simple method reports incorrect results with probability at most we equivalently say the result of the method is pac correct.
using a similar argument it can be shown that our pac learning algorithm returns a regular set approximating the set of feasible decision vectors of the program with the error probability and con dence of our choice.
if the inferred set contains no decision vector representing an assertionviolating program execution our technique concludes the veri cation with statistical guarantees about correctness.
our learning based approach nds a balance between formal analysis and testing.
rather than exploring program behaviors exhaustively our technique infers an approximation of the set of feasible decision vectors by queries and sampling.
although the set of feasible decision vectors is in general not computable pac learning with queries may still return a regular set approximation of it with a quanti ed guarantee.
such an approximate model with statistical guarantees can be useful for program veri cation.
with an approximate model that is pac correct and proved to be free from assertion violation one can conclude that the program is also pac correct.
the statistical guarantees are di erent from syntactic code coverages in software testing.
recall that our application of pac learning works over decision vectors.
decision vectors in turn represent program executions.
when our technique does not nd any assertion violation the statistical guarantees give software engineersa semantic coverage about program executions.
along with conventional syntactic coverages such information may help software engineers estimate the quality of software.
we implement a prototype named pac man pac learning based model synthesizer and analyzer of our procedure based on program veri ers cpachecker cbmc and the concolic tester crest.
we evaluate the prototype on the benchmarks from the recursive category of svcomp .
the results are encouraging we can nd all errors that can be found by crest.
we also provide quanti ed guarantee accompanied by a faithful approximate model for several examples that are challenging for program veri ers and concolic testers.
this approximate model can later be reused e.g.
for verifying the same program with a di erent set of program assertions.
our contributions are summarized in the following we show the pac learning algorithm can be applied to synthesize a faithful approximate model of the set of feasible decision vectors of a program.
such a model can be useful in many di erent aspect of program verication cf.
section for details .
we believe it is not hard to adopt our approach to handle di erent type of systems e.g.
black box systems and to obtain approximate models on a di erent level of abstraction e.g.
on a function call graph .
we develop a veri cation procedure based on the approximate model obtained from pac learning.
the procedure integrates the advantages of both testing and veri cation.
it uses testing techniques to collect samples and catch bugs.
the pac learning algorithm generalizes the samples to obtain an approximate model that can then be analyzed by veri cation techniques for statistical guarantees.
.
preliminaries letxbe the set of program variables and fthe set of function and predicate symbols.
we use x0for the setfx0j x2xg.
the sett of transition formulae consists of well formed rst order logic formulae over x x0 andf.
for a transition formula f2t andn2n we usefhni to denote the formula obtained from fby replacing all free variablesx2x withxhniandx02x0withxhn 1i.
we represent a program with a single procedure using a control ow graph.
section extends the notion to programs with multiple procedures and procedure calls.
a control ow graph cfg is a graphg v e v i vr ve xfp wherev vb vsis a nite set of nodes consisting of disjoint sets of branching nodesvband sequential nodesvs vi2v is the initial node vr2vsis the return node ve vis the set of error nodes xfp x is the set of formal parameters andeis a nite set of edges such thate v t v and the following conditions hold for any branching node vb2vb there are exactly two nodesv0 v0 12vwith vb f0 v0 vb f1 v0 2e wheref0 f12t are transition formulae for any non return sequential node vs2vsnfvrg there is exactly one node v02vwith v s f v0 2e and for the return node vr2vs there is no v02vsuch that vr f v0 2efor anyf2t .
we sayv0is a successor ofvif v f v0 2e.
assume moreover that the two successors v0 0andv0 1of the branching nodevare ordered.
intuitively the corresponds to 715theifbranch and the corresponds to the else branch.
we callv0 0andv0 1the successor and successor ofvrespectively.
similarly f0andf1are called the transition and transition formulae ofv.
note that the de nition of a cfg allows us to describe nondeterministic choice which is commonly used to model the environment.
to be more speci c a nondeterministic choice from a branching node v can be represented by de ning both the transition and transition formulae of vasv x2xx x0.
apath in the cfg gis de ned as a sequence hv0 f1 v1 f2 v2 f m vmisuch thatv0 viand vj fj vj 2efor every j m. the path is feasible if the path formulavm k 1fkhkiis satis able.
it is an error path ifvj2vefor some j m. the task of our analysis is to check whether gcontains a feasible error path.
a sequence w a1a2 anwithaj2f0 1gfor j n is called a word overf0 1g.
we de ne the length ofwas jwj n. the word of length is the empty word .
we also usew to denote the j th symbol aj.
ifu ware words overf0 1g u wdenotes the concatenation ofuandw.
alanguageloverf0 1gis a set of words over f0 1g.
we introduce the function decision that maps a path ofgto a sequence of decisions made in the branching nodes traversed by .
formally decision is a function from paths to words over f0 1gde ned recursively as follows decision hv f1 v1 f2 v2 f m vmi decision hv f1i decision hv f2 v2 f m vmi such that decision hvi decision hv fi ifv2vs ifv2vband fis the transition formula of v ifv2vband fis the transition formula of v for a path decision is the decision vector of .
we lift decision to a set of paths and de ne decision vectors of asdecision fdecision j g. a nite automaton with moves fa ais a tuple a q q i f consisting of a nite alphabet a nite set of statesq an initial stateqi2q a transition relation q f g q and a set of accepting statesf q. a transition q q0 is called a transition .
a wordwover is accepted bya if there are states q0 q m2qand symbols or s a1 a m2 f g such that w a1 am for every j m there is a transition q j aj qj and furtherq0 qiandqm2f.
the language ofais de ned as l a fwjwis accepted by ag.
a language risregular ifr l a for some fa a.aisdeterministic if its transition relation is a function from q toq.
for any fa a there exists a deterministic fa bsuch thatl a l b .
apushdown automaton pda is a tuple p q qi f where is a nite input alphabet qis a nite set of states is a nite stack alphabet qi2q is the initial state f qis the set of nal states and q f g f g q0 to denote the transition q a b c q0 and we sometimes simplify q q0 to q a q0 .
we de ne a con guration ofpas a pair q q .
a wordwover is accepted bypif there exists a sequence of con gurations q qm m 2q and a sequence of symbols or s a1 a m2 f g such thatw a1 am q0 qi qm2f and for everyp ac automata learning algorithm section m echanical teacher r esolving membership queries section re solving equivalence queries by sampling section f ound a feasible error decision vectorth e system is pac correct mem w y es no equ c c ounterexample figure components of our veri cation procedure j m it holds that there are some bj bj f g and j j such that j bj j j bj j and there is q j qj .
the language of pis de ned asl p fwjwis accepted by pg.
.
overview in this section we give an overview of our veri cation procedure.
let gbe a cfg of a program.
our goal is to check whether there is a feasible error path in g. more concretely consider the set of feasible paths in gand the setbof error bad paths in g. we call the languages decision and decision b over the alphabet f0 1gasfeasible decision vectorsanderror decision vectors respectively.
the program is correct if the intersection decision decision b is empty i.e.
if there are feasible error paths in g. representing the language decision of all feasible decision vectors in gis not so easy.
in general this language may not be regular or even computable.
in our procedure we construct a candidate fa cthat approximates decision the set of feasible decision vectors of g. we infer cusing a probably approximately correct pac online automata learning algorithm .
the use of pac learning provides us with statistical guarantees about the correctness of c we can claim that cispac correct i.e.
with con dence the deviation of l c from decision is less than we give a proper explanation of the terms in section .
on the other hand it is straightforward to convert g to an fabaccepting the set of all error decision vectors decision b .
intuitively states of bcorrespond to nodes ofg the initial state of bcorresponds to the initial node ofg and accepting states of bcorrespond to g s error nodes.
an edge from a sequential node is translated to a transition.
for a branching node the edges to its 0and successors are translated to transitions over symbols and respectively cf.
section .
a high level overview or our learning procedure is given in figure the procedure is similar in structure to the one of .
it consists of two main components the learning algorithm asks the teacher two kinds of questions membership is a given decision vector feasible?
and equivalence is a candidate fa pac correct?
queries.
the teacher resolves the queries while observing whether some of the tested decision vectors corresponds to a feasible error path.
by posing these queries either the learning algorithm iteratively constructs a pac correct approximation of decision or our procedure nds an error path.
as with other online learning based techniques we need to devise a mechanical teacher that answers queries from 716the learning algorithm.
checking membership queries i.e.
membership in the set decision of feasible decision vectors is relatively easy for example given a decision vectord we obtain its corresponding path by unfolding the cfggaccording to d and use an o the shelf solver to decide whether is feasible or not cf.
section .
when the automata learning algorithm infers a candidate fac we need to check whether l c approximates decision i.e.
whether cispac correct.
since we cannot compare decision with l c directly we employ the sampling based approximate equivalence technique of pac learning.
while generally unsound the technique still provides statistical guarantee about the correctness of the inferred model details are given in section .
.
pac automata learning here we explain the pac automata learning algorithm that we use to nd an approximation to decision .
classical pac automata learning algorithm cannot be used directly for the purpose of program veri cation.
it has to be modi ed to handle the case when the program contains an error.
the classical pac automata learning algorithm was obtained from modifying the requirement of the exact automata learning algorithm .
our modi cation follows the same route.
in this section we rst describe the classical exact automata learning algorithm of regular languages and then describe how to modify it for veri cation.
then we explain how to relax the requirement of an exact automata learning algorithm to infer an approximation to decision .
.
exact learning of regular languages supposeris a target regular language such that its description is not directly accessible.
automaton learning algorithms automatically infer an fa arrecognizingr.
the setting of an online learning algorithm assumes ateacher who has access to rand can answer the following two types of queries membership query mem w is the word wa member ofr i.e.
w2r?
equivalence query equ c is the language of fa c equal tor i.e.
l c r?
if not what is a counterexample to this equality a word in the symmetric di erence of l c andr ?
the learning algorithm will then construct an fa arsuch thatl ar rby interacting with the teacher.
such an algorithm works iteratively in each iteration it performs membership queries to get from the teacher information aboutr.
using the results of the queries it proceeds by constructing a candidate automaton cand nally makes an equivalence query equ c .
ifl c r the algorithm terminates with cas the resulting fa ar.
otherwise the teacher returns a word wdistinguishing l c fromr.
the learning algorithm uses wto modify the conjecture for the next iteration.
the mentioned learning algorithms are guaranteed to nd an fa arrecognizing rusing a number of queries polynomial to the number of states of the minimal deterministic fa for r. in the rest of the paper we denote online automata learning simply as automata learning .
.
learning for program verification under the context of program veri cation it may be the case that decision l b in such a case our procedure should return a feasible error path in the program.this is very similar to the setting of learning based veri cation where the learning algorithm is modi ed to return a counterexample in case the system contains an error.
we modi ed the used learning algorithm in a similar way.
to be more speci c when the classical learning algorithm poses an equivalence query equ c we rst check whether there exists a decision vector csuch thatc2l c l b and then test if c2decision .
.
in case that the two tests identi ed a decision vector c such thatc2l c l b andc62decision then c is in the di erence of l c and decision and hence a valid counterexample for the classical learning algorithm to re ne the next conjecture automaton c. .
in case that the tests identi ed a decision vector c2 l c l b such that c2decision then cis a feasible error decision vector and we report it to the user.
.
in the case l c l b the modi ed learning algorithm asks the teacher an equivalence query equ c .
given a teacher answering membership and equivalence queries about decision the modi ed automata learning algorithm has the following properties.
lemma .let decision be regular.
the modi ed automata learning algorithm eventually nds a counterexample c2l b decision whenl b decision and an fa recognizing decision whenl b decision .
note that when the program does not contain any error the behavior of the modi ed learning algorithm is identical to the classical one and hence is still an exact automata learning algorithm.
next we relax the requirements of the exact automata learning algorithm to obtain a pac automata learning algorithm suitable for program veri cation.
.
probably approximately correct learning the techniques for learning automata we discussed in the previous section assume a teacher who has the ability to answer equivalence queries.
this assumption is however invalid in our procedure.
testing decision l c can be undecidable.
angluin showed in that even if we substitute equivalence test with sampling we can still make statistical claims about the di erence of the inferred and target sets.
assume that we are given a probability distribution d over elements of a universe u and a hypothesis in the form prob w2ujd in the hypothesis the term prob w2ujd denotes the probability that the formula w is invalid for wchosen randomly fromuaccording to d. we call theerror parameter and use the term con dence to denote the least probability that the hypothesis is correct.
we say that w is pac valid ifprob w2ujd with con dence .
in the setting of automata learning the considered universe is and the target regular language is r .
the task of an equivalence query equ c is changed from checking exact equivalence which we can express as checking that8w2 w 2r l c we use to denote the symmetric di erence operator to checking approximate equivalence i.e.
checking whether the formula w w 2r l c is pac valid.
in other words 717we check whether prob w2 jd with condence .
for a xed rand a candidate c we say that cis pac correct ifw 2r l c ispac valid.
the teacher checks the pac correctness of cby pickingrsamples according to dand testing if all of them are not inr l c .
for thei th equivalence query of the learning algorithm the number of samples qineeded to establish thatcispac correct is given by angluin in as qi ln1 iln since the inferred set cis guaranteed to be pac correct this approach is termed probably approximately correct pac learning .
.
resolving equiv alence queries by sampling the current section discusses how to design a mechanism that the teacher can use for equivalence queries to provide thepac correctness guarantee as de ned in section .
given a probability distribution dover the set of feasible decision vectors decision we can use dto give a formal de nition of the quality of a candidate automaton c. in particular we use as a measure the probability with which a decision vector chosen randomly from decision according to the distribution d is contained in c. a sampling mechanism o ering such a distribution must satisfy the following conditions .
only decision vectors in decision are sampled.
.
the samples are independent and identically distributed iid i.e.
the distribution is xed and the probability of sampling a particular element does not depend on the previously picked samples.
in this paragraph we introduce the random input sampling mechanism.
we treat all nondeterministic choices and formal parameters of the program as input variables and assume that all input variables are over nite domains.
each set of initial values of input variables yields a path in the cfg of the program.
based on this observation random input sampling works by picking uniformly at random a set of initial values for input variables of the program and then obtaining the corresponding decision vector by traversing the cfg of the program using the picked values.
the sampling mechanism forms a distribution such that the probability of a decision vector dbeing chosen is proportional to the number of program paths corresponding to d. the issue of random input sampling is that it su ers from the well known fact that coverage of input values is a bad approximation of program path coverage.
depending on the sizes of input domains of program variables some paths might have only a negligible probability of being selected e.g.
given two random bit integers xandy the probability of taking the true branch in the test x y is equal to .
the situation gets even worse for input variables over unbounded domains.
even with an extremely high coverage rate of input variables values many paths may still not be explored while other are explored repeatedly.
in order to get a sampling mechanism with a better distribution over program paths we developed a technique that randomly explores program s paths using a concolictester which is an e cient means for exploring decision vectors corresponding to rare paths.
in the following we describe the technique and prove its properties.
.
concolic testing concolic testing is a testing approach that explores paths in the cfg of a program while searching for bugs.
the algorithm begins with a decision vector generated by randomly picked input values.
then it nds the next decision vector by ipping some decision made in the path and obtains new input values leading the program execution according to the new path.
this mechanism gives rare paths greater chance to be explored.
the selection of which decision should be ipped depends on the used search strategy of the tester.
in our procedure we use the concept of a batched sample.
abatched sample is de ned as a set of decision vectors of the sizek wherekis a given parameter obtained from a concolic tester by exploring kpaths using its search strategy.
we denote dkthe distribution over elements of kobtained in this way.
our procedure restarts the concolic tester after taking every batched sample.
the previous point gives us the guarantee that the probability of taking each batched sample remains the same during the execution of our procedure we assume that the concolic tester is state less and that the distribution is iid and therefore meets condition de ned above.
the principal functioning of concolic testers guarantees that condition is also met.
.
generalized stochastic equivalence in this section we show that our sampling mechanism using batched samples has the property required for the pac correctness guarantee of the learning algorithm given in section .
.
recall that for the set of feasible decision vectors of a program decision and a candidate automaton cinferred by the learning algorithm using some distribution dover if the teacher gives the answer yesfor the equivalence query equ c it guarantees with con dence that prob w2 jd since our sampling technique uses batched samples from the universeuk kw.r.t.
the distribution dkinstead of elements of and distribution d we need to change the provided guarantee in our modi cation of the learning algorithm.
if our algorithm answers yes it guarantees that prob s2ukjdk with con dence we hereafter use the term pac correct to denote this form of guarantee .
when a teacher receives an equivalence query equ c it uses a concolic tester to obtain qi see batched samples.
for each batched sample s the teacher checks if there exists a decision vector w2ssuch thatw 2l c by de nition it holds that w2decision .
the teacher answers yesif there is no such w. otherwise the teacher checks if wis an error decision vector and either reports wis feasible or returnswto the learning algorithm to re ne the conjecture.
the following lemma shows that if we use the number qi of batched samples for testing the equivalence we obtain the modi ed pac correctness guarantee from .
lemma .let and be the error and con dence parameters.
if no decision vector w 2l c is found in qibatched samples then it holds that cis pac correct.
718based on the fact that l c l b the property of the modi ed learning algorithm in section .
and the lemma above we obtain the following corollary.
corollary .let and be the error and con dence parameters.
if no decision vector w 2l c is found in qi batched samples then the program is pac correct.
.
resolving membership queries in this section we describe how a membership query mem d in the algorithm in figure is discharged by the teacher.
let be the set of feasible paths of a cfg g. when the learning algorithm asks a membership query mem d the teacher needs to check whether the decision vectordis in the set of feasible decision vectors decision .
to answer the query the teacher rst constructs a path hv0 f1 v1 f2 v2 v m fm vmiingsuch that there are exactly jdjoccurrences of branching nodes in the pre xhv0 f1 v1 f2 v2 v m 1iof ifvkis thej th branching node in it holds that decision v k fk d and vm is a branching node.
recall that is a feasible path if and only if vm j 1fjhji is satis able.
therefore the teacher can simply construct the formula from the path and check its satis ability using an o the shelf constraint solver.
alternatively the teacher can check feasibility by translating the path into a sequence of program statements with conditions substituted by assumptions on the values of the conditions and asking a symbolic executor or software model checker whether the nal line of the constructed program is reachable.
the alternative option is easier to implement but usually su ers from some performance penalty.
.
error decision vectors letbbe the set of error paths in a cfg.
in this section we show how we construct an fa accepting the set of all error decision vectors decision b of the given cfg.
this automaton will later be intersected with the automaton representing the set of feasible paths to determine whether the cfg contains a feasible error path.
definition .letg v b vs e v i vr ve xfp be a cfg.
we de ne the error trace automaton forgas the fab f0 1g v b vs vi e ve where eis de ned as follows v v0 eifv2vbnve v f v0 2e andv0 0is the0 successor of v v v0 eifv2vbnve v f v0 2e andv0 1is the1 successor of v v v0 eifv2vsnveand v f v0 2e and v v v v eifv2ve.
informally bcontains a state for every node and a transition for every edge of g. in each state it reads a symbol corresponding to a branching node and performs transitions for states corresponding to sequential nodes.
for every error node breads all remaining symbols and accepts the input word.
it is straightforward to see that baccepts exactly the set of decision vectors corresponding to error paths in g.lemma .letg v e v i vr ve xfp be a cfg and bbe the set of error paths in g. letbbe the error trace automaton for g. it holds that l b decision b .
in section we describe an extension of our procedure to programs with procedure calls.
because representing the set of error decision vectors using an fa is in this setting imprecise the section also discusses an extension that represents the set of error paths in a program with procedure calls using pushdown automata.
.
the main procedure we summarize our procedure in this section.
let gbe the cfg of the veri ed program kbe the size of a batched sample be the error parameter and be the con dence.
the goal of our procedure is to either nd a feasible error path ingor show that gispac correct.
in the latter case we also accompany our answer with a pac correct regular representation of the set of feasible decision vectors of g. let be the set of feasible paths of g dkthe distribution de ned by our sampling mechanism cf.
section and l b the set of error decision vectors of g cf.
section .
a detailed ow chart of our procedure can be found in figure .
first the bottom part of the gure describes our learning algorithm.
we extend the online automata learning algorithm with two additional tests for veri cation as described in section .
.
in particular when the automata learning algorithm outputs a candidate c before sending teacher the equivalence query equ c we rst test whether l c contains a feasible error decision vector c. in case it does we report cas an error.
otherwise in the case cis both inl c andl b but is not feasible we return cto the learning algorithm to further re ne the conjecture.
the top part of the gure describes our design of a mechanical teacher.
the task of the teacher is to answer queries from the learning algorithm.
membership queries of the form mem w can be answered by constructing the path corresponding to the decision vector wand the associated path formula which is then solved using a constraint solver cf.
section .
equivalence queries on the other hand are discharged using a concolic tester by checking whether there is a decision vector sin the set of batched samples ssuch that it does not belong to the language of c cf.
section .
if no such decision vectors exist we conclude that the program ispac correct.
otherwise we test whether s2l b if this holds we report that we have found a feasible error decision vector.
in the case s 2l b it holds that sis a feasible decision vector in decision but not in the language of the current conjecture l c .
if this happens we return s to the automata learning algorithm to re ne the conjecture and continue with the next iteration of the learning loop.
in general our procedure is not guaranteed to terminate.
when the procedure terminates and reports an error either by the teacher or the learning algorithm a feasible error decision vector is found and the program is reported to be incorrect.
if the teacher approves an approximate fac our procedure reports that cis an approximate model of decision w.r.t.
the pac correctness guarantee which in turn implies that the program is pac correct.
from lemma we have the following theorem.
theorem .let and be the error and con dence parameters respectively.
if our procedure terminates with an approximate fa c the program is pac correct.
719p ac automata learning algorithm section b uild and check the path formula ofw section m echanical teacher b uild and check the path formula ofw section s ample a set s of decision vectors section 9s2s s62l c ?
s2l b ?
b uild re ne model learning algorithm ci s a counterexamplesi s a counterexampleth e system is pac correct sis a feasible error decision vector 9c c2l c l b ?
c2decision ?cis a feasible error decision vectormem w y es no equ c y es n oy esn oequ c sn o st arty es y es n o f igure a detailed ow chart of our veri cation procedure moreover we obtain the following corollary.
corollary .suppose our procedure reports a program pis pac correct.
if we run the concolic tester with the same search strategy and batch size used in our procedure onp with con dence the concolic tester will nd an error with a probability less than .
in case our procedure is used during the development of a product corollary can be used to determine when to stop the testing phase.
in particular testing can be stopped at the point when the guaranteed probability that the next testing batch would nd a bug decreases below a given threshold and it is e.g.
not economical to continue the testing phase .
thanks to the properties of the modi ed automata learning algorithm when decision is a regular set our algorithm is guaranteed to terminate and either return a counterexample c2l b decision or nd an approximate model of decision that is disjoint with l b .
.
handling procedure calls in this section we extend our formalism of cfgs to handle programs with multiple procedures.
we use a pda to represent error decision vectors in this setting.
the issue of using fas to represent error decision vectors is that when returning from a procedure call an fa cannot remember an unbounded number of return points in the case of recursive procedures .
therefore an overapproximation such as a nondeterministic jump to any return point needs to be used.
the said overapproximation is however too imprecise and yields numerous spurious errors.
in contrast pdas can represent the set of error decision vectors precisely.
on the other hand we still use an fa to represent the approximation of the set of feasible decision vectors decision .
as a consequence except that we need to use pda operations instead of fa operations and handle procedure calls in the membership queries all other components remain unchanged for the setting of multiple procedures.
.
extending cfgs with procedure calls assume the set of procedure names p. acfg with calls cfgc is de ned as a graph g v e v i vr ve xfp wherev vi vr ve andxfpare de ned in the same way as for a cfg and e v t v v p t t v is an extended set of edges that apart from local cfg edges v f v0 forf2t also contains procedure call edges e v p g in gout v0 for p g in gout 2p t t and sequential nodes v. theginandgoutcomponents of ecorrespond to formulae for passing values to formal parameters of p formulagin and passing the return value of pback to the caller formula gout .
in this extension we de ne a program as a set of cfgcs prog fg1 g ngtogether with a bijective mapping cfgcprog p!
prog that assigns procedure names to cfgcs.
we abuse notation and use prog to denote cfgcprog i.e.
prog p denotes the cfgc of a procedure pin a program prog.
we assume that all cfgcs in prog have pairwise disjoint sets of nodes and an entry point main2p.
this paragraph gives an informal description of how we extend the de nition of a path from a program consisting of single cfg to a program consisting of a set of cfgcs and a dedicated entry point see appendix a for a formal description .
given a procedure call edge ein a cfgc g we call theinlining ofeingthe cfgcg0obtained from gby substitutingewith the cfgc of the called procedure.
we use jprog kto denote the set of cfgs obtained from prog main by performing all possible even recursively called sequences of inlinings and removing any left procedure call edges from the output cfgcs.
a path inprog is then a sequence hv0 f1 v1 f2 v2 f m vmisuch that there exists a cfg g02jprog kfor which it holds that 2g0.
.
encoding error decision vectors with pushdown automata this section describes how we construct the pda encoding the set of error traces in the considered extension.
the general idea is the same as the one for the use of fas described in section .
the main di erence is that we add jumps between cfgcs corresponding to procedure call edges which use the stack to remember which state the pda should return to after the procedure call terminates.
in the following given a cfgc g v e v i vr ve xfp we usev g e g xfp g to denote the corresponding components of g and moreover we use vs g and vb g to denote the set of sequential and branching nodes ofgrespectively.
consider a program prog fg1 g ng.
we construct the error path automaton as the pda bp f0 1g q q q i f in the following way q v g1 v gn qi vi gk such that prog main gk f ve g1 ve gn nwhere every jis de ned as follows v v0 jifv2vb gj nve gj v f v0 e gj andv0 0is the successor of v v v0 jifv2vb gj nve gj v f v0 e gj andv0 1is the successor of v v v0 jifv2vs gj nve gj and v f v0 2e gj v v v v jifv2ve gj and v vi gk vr gk v0 jif v2vs gj nve gj v p g in gout v0 2e gj andgk prog p .
lemma .let prog be a program bthe set of error paths of prog and bpbe the error path pda for prog.
then it holds thatl b p decision b .
.
implementation we created a prototype tool pac man that implements the veri cation procedure described in this paper.
the tool uses several third party libraries and tools.
first it uses cil c intermediate language to convert the veri ed c program to a set of cfgcs from which we construct the error trace pushdown automaton bp.
further we use thelibamore library to perform operations of automata such as testing their membership and emptiness or computing their intersection.
for learning automata we use the implementation of various learning algorithms within the libalf library .
membership queries are discharged using a concolic tester mentioned as an alternative option in section .
given a decision vector our tool uses the cfg of the program to generate a path corresponding to the decision vector.
the path is passed in the form of a sequence of program statements to the software model checker cpachecker which checks its feasibility.
it is possible to switch the model checker with other checkers such as cbmc .
to deal with equivalence queries we modi ed the concolic tester crest to generate a batch of kdecision vectors as described in section .
as crest may fail to generate the decision vector of a program execution when the execution terminates abnormally we modi ed crest to take a nite pre x of the execution in this case.
one issue of crest that we encountered is that when it processes a condition with boolean connectives it expands the condition into a cascade ofifstatements corresponding to the boolean expression making the program longer and harder to learn.
we addressed this by modifying crest to process conditions with boolean connectives without expanding them and in this way we increased the performance and precision of the analysis.
we also implemented the following three optimizations.
intersection with bad automaton.
recall that our modi ed learning algorithm described in section .
rst checks whether the intersection of the language of the conjecture l c and the bad language l b p is empty.
checking emptiness of a pda is however more di cult than that of an fa.
to speed up the procedure we build an fa bothat over approximates the error language and always rst checks whether l c l b o which is an emptiness test for fas.
we check l c l b p only for the cases that the previous test fails.table comparison of learning algorithms a lgorithms kvl l col. r sn l v eri ed .
.
.
b ug found .
.
b y bad .
.
b ycrest .
.
.
f alse positives .
f alse negatives .
.
.
ti meouts .
.
.
.
ofmem queries ofeququeries t otal time mem qu eries time counterexample from the learning algorithm.
when an equivalence query returns a counterexample c automata learning algorithms usually do not guarantee that cis not a valid counterexample in the next conjecture automaton.
in our preliminary experiments we found out that it happens very often that the mechanical teacher returns the same counterexample in several consecutive iterations.
therefore we decided to check whether cis still a valid counterexample by a membership query for the learning algorithm before proceeding to the emptiness test.
in the casecis valid it will be immediately returned to the learning algorithm to re ne the conjecture.
handling membership queries.
the main bottleneck of our approach is the time spent for membership queries.
in our implementation the software model checker cpachecker is used to check whether a path is feasible.
for each membership query if we invoke cpachecker with a system call a java virtual machine will be created and the components of cpachecker need to be loaded which is time consuming.
to make membership queries more e cient we modi ed cpachecker to run in a server mode so that it can check more than a single path without being re invoked.
.
experiments this section presents our experimental results to justify the claims made in this paper.
we evaluated the performance of our prototype using the recursive category of svcomp as the benchmark.
the recursive category consists of non trivial examples such as ackermann mccarthy and euclidean algorithms.
among eight tools participating in only two can solve or more examples correctly.
among the examples of them contain an error.
we performed our experiments with the error parameter con dence and the size of batched samplesk .
we ran our prototype on each example three times in all experiments.
the provided statistical data were calculated based on the average of the three runs unless explicitly stated otherwise.
we set the timeout to s to match the rules of sv comp .
.
comparison of learning algorithms we evaluated our approach with di erent automata learning algorithms implemented within the libalf library.
there are ve active online automata learning algorithms implemented in libalf angluin s original l l columns kearns vazirani kv rivest schapire rs and nl .
among the search strategies pro721table evaluation of pac man w.r.t.
the used search strategy of crest search strategy rbs cds veri ed bug found false positives false negatives timeouts of mem queries of eququeries total time time for one sample .
.
vided by crest we chose the random branch strategy.
the experimental results are in table .
the results show that kv is the algorithm with the best performance it solved out of the examples.
our technique solves more than any participant in the recursive category of sv comp but the winner.
the main reason for the performance di erence is that kv uses a tree based data structure to store query results.
compared to other learning algorithms that use table based structures kv requires much less number of membership queries to maintain the consistency of the tree based structure.
for all learning algorithms except rs the number of error paths found by the emptiness test of the intersection of the conjecture and the bad automaton is more than that found by crest.
in our experiments the time spent for membership queries is usually the performance bottleneck.
table shows that membership queries took of the total execution time for kv and at least for other algorithms.
.
comparison of search strategies we also evaluated how the used crest search strategy a ects the performance of our algorithm.. according to the most e cient strategies are random branch strategy rbs and controlow directed strategy cds .
therefore we tested the performance of our prototype using these two strategies.
we selected kv as the learning algorithm in this experiment.
the results are shown in table .
the table shows that although the average time for taking one sample with cds is more than with rbs the total time is less.
the main reason is that cds explores untouched branching points more aggressively than rbs but requires more overhead.
our experiments conform the results in .
.
evaluation of crest with restarts to justify our modi cation to the pac correctness guarantee given in section .
we show in the experiment below that running crest in batches does not decrease its bug hunting capabilities.
we compared the performance of crest with two di erent scenarios restart after each decision vectors and never restart.
we performed the experiment on the buggy examples in the recursive category and calculated the number of examples where crest found a bug within the timeout period.
in table we chose rbs as the search strategy.
we also tried the experiments with the cfg strategy and got a similar result.
we list the worst result for scenario and the best result for scenario that we received in our three runs.
we found out that the worst runs in scenario can still nd with a little overhead all bugs found by the best runs in scenario .table evaluation of crest with and without restart.
s ettings ex amples ba tch size n ever restart a ckermann02 b atch i teration a ddition02 b atch i teration a ddition03 ti meout ti meout b allrajamani spin2000 b atch i teration ev enodd03 b atch i teration f ibonacci04 b atch i teration f ibonacci05 ti meout ti meout m ccarthy91 b atch i teration .
evaluating quality of learned automata besides the performance in terms of the running time we also compared the quality of the learned automata produced by our prototype using the two strategies for the successfully veri ed bug free examples.
to evaluate the quality of the learned automata for each example we ran crest with the given search strategy to get batched samples and tested how many of them are accepted by the automaton.
the average values of the runs are shown in table where evaluation strategies are strategies used to generate the testing batched samples.
the table shows that the quality of the automata learned with the two strategies is almost the same.
also observe that the guarantee of our procedure is that the sample coverage is higher than .
our experimental results show that the quality of the automata produced by our procedure matches the theoretical expectations.
finally we tested how many words generated by crest are not covered in the automata learned with the kv algorithm and rbs.
again we ran crest with rbs in two scenarios restart after each decision vectors and never restart.
for each from learned automata and each scenario we generated decision vectors and checked how many of them are accepted by the automaton.
in total for scenario we observed accepted batches of size for the total of tested vectors yielding the correctness .
.
for scenario we observed accepted vectors for the correctness .
.
we notice that whichever strategy we use the learned automaton accepts over of the decision vectors produced by crest.
.
discussion there are several advantages of having a program model with statistical guarantees.
for instance the model can be reused for verifying a di erent set of properties of the program.
assume that the new property to be veri ed is described as an error path automaton b0andcis the learned automaton.
if l b0 l c we veri ed the program with the new property and the same pac correctness guarantee.
for the case that there exists a decision vector w2l b0 l c we test whether wis feasible and either report that wis a feasible error decision vector w.r.t.
b0or continue the learning algorithm with was a counterexample for re ning the next conjecture.
moreover during a product development the probabilistic guarantee can be used to determine at which point to stop the testing phase of the product.
in particular suppose that we are at the i th iteration of a veri cation run in which no bug has been found so far.
the guarantee gives the maximum probability w.r.t.
the given con dence pa722table evaluation of pac man with di erent crest search strategies w.r.t.
the quality of the learned automata.
the total number of tested batches is .
eququery strategy rbs cds evaluation accepted rbs ratio .
.
evaluation accepted cds ratio .
rameter that the i th iteration nds an error.
if the probability decreases below a given threshold the testing phase can be stopped.
in this paper we focus on checking validity of program assertions.
the veri cation step is handled by making an intersection of the conjecture automaton cand the error path automaton band testing its emptiness.
this procedure can be generalized to more sophisticated safety properties by replacing the tests l c l b ands2l b with other tests.
for example we can check the property the program contains at most consecutive decisions on any path with a statistical guarantee of the correctness of the received answer.
by extending the alphabet f0 1gwith program labels one can also check temporal properties related to those labels e.g.
label ashould be reached within decisions after label bis reached.
one possible extension of our work is to learn sequences of feasible function calls instead of decision vectors.
this might lead to a more compact model in contrast to the current approach.
in this case the alphabet of the model to be learned will however be all function names in the program which is usually signi cantly larger than the size of the alphabet in our work.
moreover in this setting it is harder to answer membership queries a program path composed of function calls might perform a complex traversal through loops and branches in between the calls making the problem of checking feasibility of a program path already undecidable.
one bene t of our approach is that in principle it can be extended to black box system veri cation and model synthesis.
by observing the behavior of the environment we may nd some pattern e.g.
some statistical distribution of the inputs and then based on that design a sampling mechanism.
under the assumption that the behavior of the environment remains unchanged we can verify or synthesize the model of the system w.r.t.
the given sample distribution.
.
related works exact automata learning algorithm was rst proposed by angluin and later improved in many works .
the concept of probably approximately correct pac learning was rst proposed by valiant in his seminal work .
the idea of turning an exact learning algorithm to a pac learning algorithm can be found in section .
of .
applying pac learning to testing has been considered before .
the work in considers a program that manipulates graphs and check if the output graph of the program has properties such as being bipartite k colorable etc.
our work considers assertion checking which is more general than the specialized properties.
the work considers more theoretical aspects of the problem.
the author estimates the maximal number of queries required to infera model of a black box machine.
the context is quite di erent e.g.
the work does not discuss how to sample according to some distribution e ciently to produce the desired guarantee bounded path coverage as we do in this paper.
thel algorithm has been used to infer the model of error traces of a program.
in instead of decision vectors the authors try to learn the sequences of function calls leading to an error.
their teacher is implemented using a bounded model checker and hence can only guarantee correctness up to a given bound.
the authors do not make use of the pac learning technique as we did in this work.
both our approach and statistical model checking provide statistical guarantees.
as mentioned in the introduction statistical model checking assumes a given model while our technique generates models of programs with statistical guarantee.
those models can be analyzed using various techniques and reused for verifying di erent properties.
.