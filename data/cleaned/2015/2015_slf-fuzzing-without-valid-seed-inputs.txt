slf fuzzing without valid seed inputs wei you1 xuwei liu2 shiqing ma1 david perry1 xiangyu zhang1 bin liang3 1department of computer science purdue university indiana usa 2school of computer science and technology zhejiang university zhejiang china 3school of information renmin university of china beijing china email you58 ma229 perry74 xyzhang purdue.edu xuweiliu zju.edu.cn liangb ruc.edu.cn abstract fuzzing is an important technique to detect software bugs and vulnerabilities.
it works by mutating a small set of seed inputs to generate a large number of new inputs.
fuzzers performance often substantially degrades when valid seed inputs are not available.
although existing techniques such as symbolic execution can generate seed inputs from scratch they have various limitations hindering their applications in real world complex software.
in this paper we propose a novel fuzzing technique that features the capability of generating valid seed inputs.
it piggy backs on afl to identify input validity checks and the input fields that have impact on such checks.
it further classifies these checks according to their relations to the input.
such classes include arithmetic relation object offset data structure length and so on.
a multi goal search algorithm is developed to apply class specific mutations in order to satisfy inter dependent checks all together.
we evaluate our technique on popular benchmark programs collected from other fuzzing projects and the google fuzzer test suite and compare it with existing fuzzers afl and aflfast symbolic execution engines klee and s2e and a hybrid tool driller that combines fuzzing with symbolic execution.
the results show that our technique is highly effective and efficient out performing the other tools.
i. i ntroduction fuzzing is a commonly used technique to discover software bugs and vulnerabilities.
it derives a large number of new inputs from some initial inputs called seed inputs using mutations guided by various heuristics with a certain degree of randomness.
it usually does not rely on complex program analysis or even source code and hence features applicability.
many existing cve reports were generated by fuzzing techniques .
valid seed inputs play a critical role in fuzzing.
however there are situations that seed inputs are not available or the available seed inputs may not cover important input formats.
the former situation occurs for software that does not come with inputs or input specification e.g.
those downloaded from the internet third party libraries or even malicious code .
the latter situation arises for software that supports many formats including even non standard undocumented formats .
some software implementations may not fully respect documented specification leading to implementation specific input formats.
they are unlikely to be represented in seed inputs.
we will show in section iv b that a mutated tiff format is uniquely supported by exiv2 but completely undocumented.
in fact some cves e.g.
cve and cve2016 were discovered with inputs of undocumented format.hence there is a strong need to fuzz software without requiring valid seed inputs.
while most fuzzing techniques can operate without a valid seed input they have to compose valid inputs from scratch through a more or less random search procedure such as random growth of the input length and random bit flipping which have limited capabilities in generating valid inputs due to the large search space.
there are learning based techniques that aim to derive input specification from a large number of training inputs .
they require the training set to have comprehensive coverage of input specification.
such an assumption is difficult to meet in practice.
moreover as we discussed earlier many inputs even have non standard undocumented implementation specific formats.
there are reverse engineering techniques such as tie or reward that can derive the grammatical structure of an input by monitoring program execution on the input.
they hardly generate the entire input specification but rather the syntactical structure of individual concrete inputs.
symbolic execution is a highly effective testing technique that can generate inputs from scratch .
despite its effectiveness in unit testing applying symbolic execution to whole system testing has a number of practical hindrances.
for example a lot of libraries need to be manually modeled inputs with nontrivial formats often involve operations that are difficult for symbolic execution engines such as those in checksum computation and many input validity checks require reasoning about the offsets counts and length of input fields which are difficult for most symbolic execution engines.
there are proposals to use gradients to guide fuzzing to provide the abilities of resolving path conditions in a nonrandom fashion and to combine symbolic execution and fuzzing .
however they do not focus on solving the problem of generating valid seed inputs and hence still have the aforementioned limitations in our target setting.
in this paper we develop a novel fuzzing technique slf stands for seedless fuzzer that features valid seed input generation.
it is fuzzing based and hence has the benefits of applicability.
it does not require source code nor does it rely on complex program analysis.
its operation is largely piggy backing on regular fuzzing.
specifically it starts with a very short random input e.g.
bytes which quickly fails some validity check.
slf then performs sophisticated input mutation to get through these validity checks until a valid seed input is generated which is further subject to regular fuzzing.
to get through validity checks it first groups inputs into ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a b c d e f 00000000h 4b fc 00000010h 00000020h 0a 4b 00000030h fc 00000040h 00000050h 4b 00000060h 31local file header lfh central directory header cdh end of central directory eocd 4b 4b 4b fc 0a crc checksumdata size compressed data size uncompressed magic eocd cd size cd offfname len lfh off nentry cur nentry allfile name cmt lencommentdata off eocd off cmt off fig.
a valid zip archive file.
fields by observing how consecutive bytes influence individual checks.
such information can be extracted from the underlying fuzzing infrastructure which is afl in our case.
it then classifies the checks based on their relations with the input by observing the state differences at the checks caused by a predefined set of mutations.
for example some checks concern the values of individual input fields while others test the offset count of specific fields.
such category information can be used as guidance in applying the corresponding mutations.
in practice multiple validity checks are often inter dependent by predicating on a same variable or multiple variables derived from the same input fields.
as part of slf we develop a search algorithm that can mutate input in a way to satisfy the interdependent checks.
we evaluate slf on real world programs collected from the existing fuzzing projects and the standard google fuzzer test suite and compare it with existing fuzzers afl and aflfast symbolic execution engines klee and s2e and a combination of fuzzing and symbolic execution driller .
our results show that slf is highly effective and efficient.
it can handle many more cases than the other techniques and generate .
times more valid seed inputs and cover .
times more paths on average.
we make the following contributions.
we propose a novel fuzzing technique that can effectively generate valid seed inputs.
the technique is piggybacking on afl and does not require any complex program analysis or even source code.
we develop various supporting techniques such as grouping input bytes to fields detecting inter dependent input validity checks and a search algorithm to satisfy these checks together.
we propose a classification of input validity checks based on their relations with input elements.
we also develop the corresponding mutation strategies.
ii.
m otiv ation we use libzip to explain the limitations of existing techniques e.g.
mutation based fuzzing and symbolic execution and motivate the idea of slf.
libzip is an open source library that reads creates and modifies zip archives.
it is widely integrated in commodity software such as pdf readers and sql databases for data compression and decompression.
zip file format .
zip is one of the most popular compressed file formats .
a zip archive must contain an end of central directory eocd for short structure located at the end of the file e.g.
bytes in black at the index 0x52 in figure .each file stored in a zip archive is described by a local file header lfh structure which records the basic information about the file such as data size and crc checksum e.g.
the bytes in gray at index 0x00 1f .
the file data is placed after the local file header index 0x20 .
for ease of indexing each file is accompanied with a record of central directory header cdh which indicates the offset of the corresponding lfh and is located in a central place of the archive together with the cdh records of other files e.g.
plain bytes at 0x2251 pointing to the aforementioned lfh .
the eocd holds a pointer pointing to the cdh section.
note that the valid zip file in figure 1is generated by our tool from scratch.
assume that we are using libzip to read the content of a specified file in a given zip archive.
libzip first identifies the eocd structure from which it can locate the cdh records.
it then iterates through the individual records until the one for the specified file is found.
finally the corresponding lfh structure is accessed and the file data is extracted.
during the process libzip performs multiple checks on different fields of the input file to ensure validity of the archive.
if any check fails the process is terminated.
figure 2presents some critical checks.
check a circlecopyrtensures the input file has enough length to hold an eocd structure.
check b circlecopyrtsearches for the magic number of eocd structure in the input file.
if found its offset is recorded in the eocd offvariable.
after that libzip examines whether the number of cdh records on the current disk equals to the the total number of cdh records check c circlecopyrt and whether it is larger than the index of the specified file check d circlecopyrt .
subsequently the size cd size and offset cd off of the cdh section are extracted from the input file at the offset eocd off and eocd off respectively e.g.
indices 0x5e and 0x62 in figure .
then libzip checks whether the cdh section overlaps with the eocd structure check e circlecopyrt and whether its size is larger than the default minimal size of a cdh record check f circlecopyrt .
next libzip gets the length of the comment string cmt len and checks whether it equals to the length of the remaining data check g circlecopyrt .
finally the integrity of the archive is inspected by comparing the computed crc checksum with the one stored in the archive check h circlecopyrt .
mutation based fuzzing .
mutation based fuzzing or fuzzing in short generates inputs by modifying valid seed inputs.
although many fuzzing techniques can operate without a valid seed input their effectiveness often substan1note that libzip does not support cross disk zip archive and check c circlecopyrt ensures the condition is satisfied.
specified index in check d circlecopyrtis a command line parameter that indicates the index of the file to be accessed.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
01int file size size of file 02char buf read from file 03int min size is zip64 buf ?size eocd64 size eocd 04if file size min size error a circlecopyrt 05int eocd off 06while memcmp buf eocd off magic eocd !
eocd off b circlecopyrt 08if eocd off file size error 09int nentry cur read buffer16 buff eocd off 10int nentry all read buffer16 buff eocd off 11if nentry cur !
nentry all error c circlecopyrt 12if nentry cur specified index error d circlecopyrt 13int cd size read buffer32 buf eocd off 14int cd off read buffer32 buf eocd off 15if cd size cd off eocd off error e circlecopyrt 16if cd size size cdh error f circlecopyrt 17int cmt len read buffer16 buf eocd off 18int tail len buffer left buf 19if cmt len !
tail len error g circlecopyrt 20int crc cmp compute crc buf data off data size 21int crc str read buffer32 buf lfh off 22if crc cmp !
crc str error h circlecopyrt fig.
critical validation checks of libzip.
tially degrades.
while some checks in figure 2can be satisfied by random mutation with dictionary e.g.
checks a circlecopyrtand b circlecopyrt other checks are very difficult to satisfy.
for example check g circlecopyrtconstrains the file size with a value extracted from the input file.
randomly mutating the input bytes or extending the file size are unlikely to satisfy the check.
in fact both afl and aflfast would get stuck at similar checks.
to overcome some of these difficulties angora enhances fuzzing with gradient guided mutation which considers a path condition as a black box function on inputs and searches for input valuations that satisfy the condition.
it uses taint analysis to identify the input bytes that affect a predicate observes the changes on the predicate after mutating the identified bytes to derive the direction for changes i.e.
if howsignificant individual input changes impact the comparison at the predicate and performs further mutations accordingly.
although achieving substantial improvement over random fuzzing it has inherent limitations in satisfying multiple path conditions involving the same variable.
for example when computing the gradient for satisfying check d circlecopyrt angora only tries to change the value of nentry curwithout keeping it equal to nentry all which is required by check c circlecopyrt.
in such cases angora could hardly find the direction of input changes.
symbolic execution .
symbolic execution uses symbolic variables to denote individual input elements and then constructs symbolic constraints i.e.
formulas involving symbolic variables to represent the conditions that need to satisfy in order to explore various program paths.
concrete inputs are derived by solving these constraints.
while symbolic execution can generate seed inputs from scratch and handle non trivial arithmetic logic relations across multiple input elements it has limitations in dealing with other relations commonly present incomplex real world programs.
particularly arithmetic relations are often induced by data dependencies and resolving constraints on such relations requires reasoning about input value changes which symbolic execution is very good at.
however in practice there are other more subtle correlations pertain to field offset and file field length which require reasoning about fields relocation length variation and fields removal and duplication.
consider check e circlecopyrt whose left hand side lhs is affected by the values cd size and cd off extracted from the input file and the right hand side rhs affected by the offset eocd off which is not derived from input bytes through arithmetic operations but rather from the branch outcomes of loop predicate at line .
such a check can be satisfied by either decreasing the lhs value or increasing the rhs value.
in general it is difficult for symbolic execution to directly reasonabout mutate the value of eocd off.
hence it is more likely that the lhs variables are set to a small value to satisfy check e circlecopyrt.
this would result in the failure of check f circlecopyrt.
in fact we used klee to symbolically execute libzip for hours and found that check f circlecopyrtis always unsatisfiable preventing klee from further path exploration.
there has been proposal to use symbolic variables to model loop counts and then leverage program analysis to identify linear relations between loop count variables and input features such as length .
while modeling linear relations between loop count and input length is particularly effective in reasoning about length changes to overflow buffers such relations are only one kind of the non arithmetic non logical relations.
for example it cannot handle the relation involving eocd off in check e circlecopyrt which denotes the offset of a particular input field instead of simple length.
symbolic execution also has difficulty in modeling constraints derived from complex computation such as those involved in checksum e.g.
check h circlecopyrt .
in addition many symbolic execution tools require generating models for library calls entailing substantial manual efforts in practice.
our technique .
we develop a novel fuzzing technique slf that can effectively generate valid seed inputs from scratch.
it is mutation fuzzing based so that it inherits the benefits of fuzzing.
in addition it overcomes the limitation of existing fuzzing techniques regarding seed input reliance and is capable of effectively composing valid seed inputs.
slf is inspired by two important observations.
first input validation checks can be classified into a number of categories each corresponding to specific input relations and entailing unique mutation strategies.
in our classification gradient guided fuzzing and symbolic execution are good at certain categories but not sufficiently general to deal with all the categories.
second the categories can be effectively detected during fuzzing by mutating input bytes in pre defined fashions and observing the changes at predicates.
such runtime detection can piggyback on a vanilla fuzzing infrastructure such as afl.
our technique does not require any heavy weight analysis such as taint analysis.
for example by monitoring check e circlecopyrt w ec a n authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
gbjmjoh dheck?
mutation execution oew doverage?select an input ninsert mutated input to input queue afl fuzzing mutated input path coverage yinput queue input field fufdujpovalidation check classificationinter dependence identificationmulti goal 4earch fieldinfoslf typeinfofield0 field1check check check depinfo seedsinsert seeds to input queue check category info type i field4...... fig.
overview of slf.
observe that the value of the lhs variable changes if we mutate certain input bytes indicating likely arithmetic logic relations and the value of rhs changes if we add extra bytes to the head of input file indicating index of relations.
based on the observations we develop slf.
it starts from a random input of a few bytes.
while the input most likely fails some validation check very shortly after the execution starts our technique mutates the input and observes how the predicate corresponding to the check changes.
this is to determine the category of the check and the correlated input byte s .
based on the type of check it applies the corresponding mutation search strategy.
it is very common that the input bytes that are identified as influencing the check may involve in some preceding checks that the execution has succeeded.
as such mutating them may lead to undesirable failures at those preceding checks.
slf features the capabilities of detecting inter dependent checks that may be affected by mutating the same input bytes and a multi goal gradient guided search algorithm that aims to satisfy all these checks.
for our motivation example our tool starts with a byte random input within minutes it generates a valid seed input as shown in figure 1that passes all the input validation checks.
in hours it generates valid seed inputs which are further leveraged by regular fuzzing to cover paths.
in contrast neither klee nor afl generates any valid input.
none of the test cases generated by klee can pass through check f circlecopyrt.
afl does not pass through check f circlecopyrteither.
most of the paths explored by afl are exception handling code.
the comparison with other tools such as s2e aflfast and driller shows similar improvement see section iv b .
iii.
d esign a. overview figure 3presents the overview of slf which is built on top of afl.
it consists of the following components.
the first one is an input field detection component that groups input bytes to fields.
this allows mutation to be performed at the unit of fields instead of bytes.
the second one is a validation check classification component which detects the types of validation checks in the current execution.
the third one is an interdependence identification component which identifies all the preceding checks that have inter dependencies with the checkthat fails the current execution.
based on the results from the second and third components the forth component a multigoal gradient based search algorithm performs corresponding mutations to satisfy all the inter dependent checks.
the overall procedure of slf is presented in algorithm .
the input queue is initialized with a randomly generated 4bytes input lines .
in the main loop slf executes the target program with the current input line analyzing the encountered checks and error information to identify the check that fails the execution line .
if no validation failure is found which indicates the current input is well formed slf switches to regular afl fuzzing line .
otherwise slf tries to get through the failed check as follows.
first it groups input bytes into fields line .
this is achieved by observing how the mutation of individual bytes affects the encountered checks.
consecutive bytes whose mutation affects the same set of checks are grouped to an input field as detailed in section iii b .
given the field information slf then classifies the encountered checks line by mutating input fields in predefined manners e.g.
flipping input field value and duplicating input field as detailed in section iii d .
after that slf identifies all the checks that are correlated to the failed check that is they are influenced by the same input field s section iii e .
a multi goal search algorithm is then applied to satisfy these checks section iii f .
finally if sfl succeeds in generating valid seeds it adds them to the input queue for further processing.
slf does not require source code.
it runs the program executable on a modified qemu to monitor the lhs and rhs values of cmp andtest instructions that correspond to the conditional jump instructions.
b. input field detection the goal of input field detection is to group consecutive bytes that affect the same set of checks to a field.
algorithm 2describes the process.
we first execute the target program over the given input and collect the information of the encountered checks including their lhs and rhs values denoted as checkinfo line .
then we flip each input byte line that is flipping individual bits in the byte and execute the target program over the flipped input to collect new check information denoted as checkinfo prime line .
by authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm slf s fuzzing loop.
1ifinputqueue then 2inputqueue randombytes 3forinput inputqueue do 4checkinfo runprogram input 5fcheck getfailingcheck checkinfo iffcheck negationslash null then fieldinfo groupbytes input typeinfo classifychecks input fieldinfo depinfo getdependents fcheck typeinfo seeds multigoalsearch fcheck typeinfo depinfo ifseeds negationslash null then addtoqueue inputqueue seeds 13aflfuzzing program input 0x5c 0x5e e1 00flip byte 0x5c flip byte 0x5e fe lhs 0x6a 0x52 0x01 0x01 0x52 0x30rhs 0x16 0x6a 0x01 0x01 0x52 0x check lhs 0x6a 0x52 0x01rhs 0x16 0x6a 0xfecheck lhs 0x6a 0x52 0x01 0x01 0x103rhs 0x16 0x6a 0x01 0x00 0x52check qsxw khfn qir s s s fig.
example of input bytes grouping.
comparing the difference of checkinfo andcheckinfo prime we can identify those checks that appear in both executions and have different lhs or rhs values line .
the two pieces of information are aligned based on the program counters.
if the difference caused by flipping the current byte is the same as that of the previous byte they are grouped together line .
otherwise a new field is created line .
note that in the case where bytes affect one check and bytes affect another slf treats each byte as a separate group since they have different affected check sets.
figure 4illustrates how the bytes at index 0x5c are grouped.
figure 4a shows the checkinfo of the execution on the input which records the lhs and rhs values of the encountered checks.
particularly the rhs value of check c circlecopyrt is 0x01 and the lhs value of check e circlecopyrtis 0x52.
after flipping the byte at 0x5c the checkinfo primeof the mutated execution is shown in figure 4b.
the difference between checkinfo andcheckinfo primeis that the rhs value of check c circlecopyrtchanges.
similarly after flipping the byte at 0x5d we get the same difference.
in contrast flipping the byte at 0x5e results in the difference at the lhs value of check e circlecopyrt.
hence we group the bytes at 0x5c 5d as a field and byte 0x5e starts a new field.
an important feature of the algorithm is that it does not require the input to be a valid one to begin with.
c. input v alidation check classification to facilitate discussion we introduce a language to model input validation checks which allows us to classify such checks based on the underlying relations with input elements.
figure 5shows the syntax of the language.
for simplicity the language operates at individual input bytes while ouralgorithm groupbytes input input field grouping.
1fieldinfo 2checkinfo runprogram input 3checkdiffprv 4forbyte 0tolen input do 5input prime flipbyte input byte 6checkinfo prime runprogram input prime 7checkdiffcur diff checkinfo checkinfo prime ifcheckdiffcur checkdiffprv then fieldinfo updatelastgroup fieldinfo byte else fieldinfo newgroup fieldinfo checkdiffprv checkdiffcur angbracketleftexpression angbracketright e input c x e1binope2 e1relope2 f e0 ite ep et ef count input ep indexof input ep angbracketleftcheck angbracketright c assert e angbracketleftinput angbracketright input byte angbracketleftconstant angbracketright c true false ... angbracketleftbinoperator angbracketrightbinop ... angbracketleftreloperator angbracketrightrelop !
... fig.
language implementation deals with fields .
we use input with lbthe lower bound and ubthe upper bound to denote an input segment.
note that input denotes the entire input.
observe that an expression could be an input segment a constant a variable xwhose use will be explained later in this paragraph binary arithmetic operation and relational operation of expressions a function f on a parameter e0to denote an uninterpreted function whose internals are not visible understandable an if then else ite expression whose value may be etoref depending on if epis true a count primitive that counts the number of cases in an input segment that satisfy a condition ep and an indexof primitive that identifies the offset of the first case that satisfies ep.
variable xis used in epto denote some byte in the given input segment.
for example count input x counts all the input bytes whose value is larger than .
a validation check asserts an expression to be true.
note that we do not model program variables in our language and expressions are essentially formulas on input segments and constants.
the language is expressive to describe most of the validation checks we have observed in our subject programs.
for example check h circlecopyrtis an instance of f where fstands for the checksum function that a symbolic execution tool has difficulty modeling.
other examples of f include libraries without source code.
expression count input t r u e models the predicate variable tail lenin check g circlecopyrt which counts the bytes from the comment offset cmt off to the end of file.
expression indexof input x magic eocd models the eocd offpredicate variable in check b circlecopyrt which means to find the offset of the eocd magic number.
the formalization allows us to classify validation checks.
in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm classifychecks input f ieldinfo detecting check types.
1typeinfo 2checkinfo runprogram input 3forfield fieldinfo do fortype typelist do policy getdetectpolicy type input prime enforcepolicy input policy checkinfo prime runprogram input prime checkdiff diff checkinfo checkinfo prime forcheck checkdiff do ifmatchpolicy check policy then info classifytype check type typeinfo info the following we discuss four most popular categories each corresponding to some form of expression.
the classification is designed in a way to facilitate later fuzzing.
type i arithmetic check.
if the variables involved in a check are derived from input bytes through only the arithmetic operations e.g.
the binop in our language the check is a type i arithmetic check.
symbolic execution is particularly good at satisfying type i checks.
gradient based fuzzing is also good when there are no other inter dependent checks.
checks c circlecopyrtand d circlecopyrtin figure 2are type i checks.
type ii index offset check.
index checks have the form of assert indexof input ep relop e that compares the offset of the first case satisfying epin the given input segment with another expression e0.
they are also quite common but difficult to satisfy by existing techniques.
check e circlecopyrtin figure 2is a type ii check.
type iii count check.
count checks have the form of assert count input ep relop e that compares the number of cases satisfying epin the given input segment with an expression e0.
all the length checks checks that count the number of data structures objects fall into this category.
they are commonly present but difficult to satisfy by existing techniques.
check g circlecopyrtin figure 2is a type iii check.
type iv ite check.
ite checks have the form of assert ite ep et ef relop e in which epis an expression that are based on input bytes through only arithmetic operations.
check a circlecopyrtin figure 2is a type iv check.
observe that the result of the check has control dependence on the input bytes instead of data dependencies.
our technique resorts to the default random mutation scheme of afl to reason about such checks.
note that a more complex solution may entail heavy weight program analysis that affects the applicability of our technique.
we leave it to our future work to develop a more sophisticated solution.
while these categories cover the checks in the programs we consider we do not claim their comprehensiveness.
moreover they do not represent a strict partitioning and hence a check may fall into more than one categories.
for example the lhs of check e circlecopyrtin figure 2suggests type i and the rhs type ii.
4b 00field field field field 2field 4b 06field 00detect type ii check field 0detect type i check change check change checkinput mutated input changediff changediff inter dependenc e fig.
type checking and inter dependence detection.
d. detecting check types due to the different features of each type of check we define different policies to detect each of them.
algorithm 3describes the process of type detection.
with the field information got from the previous step we iterate over individual input fields and try the predefined detection policy for each type.
we classify a check to a type by examining whether the change of its predicate variable values match the feature of the type.
the order of type detection attempts does not matter.
currently we support the detection policies for types i ii iii.
detecting type i .
for each field under consideration we add a random value to the field.
we then examine whether the mutation results in the value of a certain predicate variable changed.
if so we mark the corresponding check as an arithmetic check.
intuitively it leverages the observation that most arithmetic operations in programs denote one to one mapping from input to output.
in other words any change of the input leads to change of output.
there are corner cases such as mod operation that does not denote a one to one mapping.
slf currently does not handle them.
we also record the field that affects the check.
the subsequent multi goal search algorithm relies on such information.
detecting type ii .
to detect type ii checks we insert an extra byte before the current field under consideration.
we examine whether the extra byte results in the value of a certain predicate variable off by one.
if so we mark the corresponding check as index offset check.
we also record the field before which we insert the padding byte.
detecting type iii .
to detect type iii checks the idea is to duplicate the object data structure that is being counted and observe if any variable at the check changes.
the challenge lies in that we do not know the boundaries of objects structures.
note that an object structure may consist of multiple fields.
we resort to a search algorithm.
assume the input has nfields in total.
for each field iunder consideration in algorithm w e start a loop jfrom iton.
in the loop we duplicate the input segment including fields from itojand insert the duplicated segment after field j. essentially we are considering fields i j form an object.
we then examine whether the insertion results in variable value changes at the check which indicates a type iii check.
we also record iandj.
example .
figure 6presents an example to illustrate check type detection.
assume we are given an invalid input as shown authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in the top of the figure.
when we iterate on field and try the detection policy for type ii an extra byte is inserted before field .
such a mutation results in the rhs of check e circlecopyrtincreasing by one which matches the feature of type ii.
for field when we increase its value by the lhs of check e circlecopyrtchanges.
it matches the feature of type i check.
we mark check e circlecopyrtas type i due to lhs and record that it is affected by field and also type ii due to rhs.
e. identifying inter dependent checks given the type information of each check we correlate those checks whose predicates are affected by the same field.
for example the input in figure 6will fail check f circlecopyrt.
the information provided by the check type detection phase shows that checks e circlecopyrtand f circlecopyrtare affected by the same field i.e.
field .
we hence correlate them for further processing.
f .
gradient based multi goal search algorithm this step aims to identify mutation that is likely to pass the failing check and does not fail any preceding checks.
it starts from the current failing check and the corresponding input and mutates the input based on the type and inter dependence information of the failing check.
algorithm 4presents part of the procedure.
for simplicity we assume a singular interdependent check rcand a singular field fdthat affects the failing check fc.
our implementation handles multiple interdependent checks and multiple fields affecting fc.
in the first case lines the lhs of fcindicates that it is a type i check while the rhs is a constant value.
at lines the program is re executed with the mutated field fd prime.
the mutation is bounded random mutation.
the new runtime information checkinfo primecontains the updated information of the failing check denoted as fc prime.
at lines and the gradients of the failing check and the relevant check are computed.
observe that they denote how the mutation on fdaffects the difference of the lhs and rhs at the check.
line tries to select a field value that can negate the failing check but still respect the relevant check by choosing a value that is greater than fd fc.lhs fc.rhs gradient which would change the branch outcome at fcif it denotes a linear function and smaller than fd rc.lhs rc.rhs relgrdt so that rclikely retains its branch outcome.
if the range at line is empty and fd prime primecannot be selected indicating potential infeasibility of satisfying both checks.
lines choose to first satisfy the check that has less flexibility i.e.
influenced by smaller number of other input fields and hence smaller room to manipulate while getting closer to satisfy the other check.
specifically line sets the mutated value to one that is slightly smaller than a value that would likely negate the branch outcome of rc line negates fcwith the smallest possible value so that the violation of rcmay likely be minimized.
the resulted seed input is added to the set.
lines denote another case in which the lhs indicates type i whereas the rhs indicates type iii.
since both sides can be manipulated by input mutations the strategy of slf is to fix one side and mutate the other side.
it generates two seedalgorithm multigoalsearch fc t ypeinfo depinfo gradient based multi goal search.
variables input fc rc fd seeds denote the current input the failing check the inter dependent check the field that impacts fc s variables and inputs generated respectively.
1iffc.lhs.cat i fc.rhs.cat const then 2fd getdepfield typeinfo fc.lhs 3rc getdepcheck depinfo fd 4fd prime mutatefield fd 5checkinfo prime runprogram replacefield input fd fd prime 6gradient fc prime.lhs fc prime.rhs fc.lhs fc.rhs fd prime fd 7relgrdt rc prime.lhs rc prime.rhs rc.lhs rc.rhs fd prime fd 8fd prime prime a value in fd fc.lhs fc.rhs gradient fd rc.lhs rc.rhs relgrdt iffd prime prime null then iffchas more flexibility than rcthen fd prime prime fd rc.lhs rc.rhs gradient else fd prime prime fd fc.lhs fc.rhs gradient 14seeds addseed seeds replacefield input fd fd prime prime 15iffc.lhs.cat i fc.rhs.cat iii then fix rhs and mutate lhs like lines fix lhs and mutate rhs angbracketleftlb ub angbracketright getdepfield typeinfo fc.rhs 19checkinfo prime runprogram insertfields input up fields 20gradient fc prime.lhs fc prime.rhs fc.lhs fc.rhs 21seeds addseed seeds insertnfields input ub fiels gradient ... inputs one by fixing rhs and mutating lhs and vice versa.
the search space of mutating both sides is explored by further mutating the two new seeds in later rounds .
this strategy is particularly effective for passing checksum validity checks.
since lhs is type i mutating lhs is similar to that in lines and elided.
lines are to fix lhs and mutate rhs.
in particular since rhs indicates type iii line extracts the starting and ending field indexes of the object structure that is being counted.
it then mutates the input by duplicating the object structure once and adding it right after the ending index ub and re executes the program line .
a gradient value is computed to measure the influence at the failing check.
note that the denominator is as we only add one object.
line tries to insert n gradient objects to negate the failing check.
note that when a failing type ii iii check is inter dependent to other type i checks slf always tries to negate the type ii iii check first and lets the following rounds to fix the possible violations of the inter dependent checks.
there are other cases.
due to the space limitations we omit them from discussion.
they are similarly handled as the aforementioned two cases.
example.
assume check f circlecopyrtin figure 1fails.
it belongs to the first case in algorithm .
further assume that cdsize cdoffset size cdh and eocd off are 0x2f 0x22 0x33 and 0x51 respectively.
slf identifies checks e circlecopyrtand f circlecopyrtinter dependent.
slf determines the gradients for both e circlecopyrt and f circlecopyrtare by mutating cdsize and comparing lhs rhs authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
before and after mutation .
however it fails to choose a value that satisfies f circlecopyrtwithout failing e circlecopyrt a scdsize cdoff eocd off while cdsize size cdh 0x4.
in this case slf chooses to satisfy the more restricted check f circlecopyrtby adding to cdsize.
in a later round the input is further mutated by changing cdoff to pass all checks.
g. limitations slf cannot handle checks that have complex control dependencies with input fields and hence relies on regular fuzzing to deal with these fields.
in addition slf identifies input fields at the byte level and hence cannot reason about checks that involve bit level fields.
iv .
e v aluation a. experiment setup our evaluation is performed on two datasets.
one contains real world programs that are commonly used in other fuzzing projects .
the other is benchmark programs selected from the google fuzzer test suite that do not have valid seed inputs provided.
these programs cover a wide range of categories including image audio compression font etc.
the first three columns in table iand table iipresent detailed information of the programs.
we compare our tool slf with other popular state of theart testing tools.
for symbolic execution tools we compare with klee and s2e .
due to the input size needed in our programs we provide a bytes symbolic file as input for both klee and s2e.
we use the coverage optimized search strategy for klee and the class uniform analysis search strategy for s2e which are the recommended settings.
for mutation based fuzzers we compare with the original afl and its improved version aflfast that performs optimized fuzzing energy allocation.
for hybrid fuzzers we compare with driller which combines fuzzing with symbolic execution for exploring paths difficult to fuzz .
all of our experiments are performed on a machine with cores intel coretmi7 cpu .20ghz and gb memory running the ubuntu .
operating system.
b. results for the programs commonly used in fuzzing to evaluate the effectiveness of slf we perform a set of experiments by running slf and other tools on the real world programs commonly used in fuzzing with a 4byte randomly generated invalid input and observing their path coverage changes over time.
we run each experiment one testing tool on one program for hours.
results are presented in table iand figure in which x axis represents time y axis represents the number of covered paths for each experiment and the points on the curves denote the moments that a valid seed input is generated.
from these results we can make the following observations.
firstly traditional mutation based fuzzers i.e.
afl and aflfast do not work well without valid inputs in most cases.
at the beginning they can cover some paths by satisfyingenvironment related path conditions but very few or no inputrelated conditions.
after that the executions got stuck at finding the first valid input and cannot proceed.
in many cases it cannot make any progress within hours leading to low path discovery.
for example in the evaluation of libtiff figure 7c afl and aflfast got stuck within hours yielding only path coverage compared with slf.
as shown in table i on average sfl covers .
and .
times more paths than afl and aflfast and generates much more valid seeds.
secondly the symbolic execution tools work well on small programs.
for example s2e achieves the best result than all the fuzzing based tools on otfcc figure 7j .
however they do not handle complex real world programs well.
we have spent substantial engineering efforts to make klee and s2e to work on these target programs even under the guidance from some of their original developers.
we were able to make klee run on target programs and s2e on target programs.
those programs that cannot run for klee s2e are marked as n a in table i. also note that s2e is very memory consuming.
if the memory is about to reach the limit it starts to randomly kill some of the existing states until there is enough memory for continuation.
due to the random state killing it often terminates early as there are no more states to explore.
it runs for hours on average in our experiments.
we mark the termination point of s2e with an x symbol in figure .
for programs that can be run by klee slf generates .
times more seeds than klee in total for those programs that can be run by s2e slf generates .
times more seeds.
sfl outperforms symbolic execution except for the three smallest programs i.e.
giflib otfcc ttf2woff .
thirdly the hybrid fuzzer driller alternates between fuzzing in most of the time and symbolic execution when fuzzing is stuck .
it scales to large programs and achieves reasonable performance.
however it still does not work as well as our tool.
from our manual inspection the reason seems to be that the optimal timing of switching from fuzzing to symbolic execution is difficult to identify for complex programs.
switching at a wrong time does not really exploit the benefits of the combination.
furthermore there are still cases that the symbolic execution engine cannot handle well.
lastly our tool slf achieves the best performance i.e.
the largest number of explored paths in all cases except otfcc.
as we can see from the graph newly generated seed inputs usually lead to exploration of a large number of new paths.
these results show that slf is very effective in generating new valid seeds which lead to better path coverage than other tools in most cases.
case study.
we use exiv2 as a case study.
within hours slf generates unique seeds that lead to the coverage of different program paths while other tools do not generate any valid seed.
slf has more than three times path coverage than other tools.
more interestingly we found that one of the generated seeds has undocumented format.
figure 8shows the snippets of a documented valid seed and the undocumented valid seed.
in the documented format the bytes at offset authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i evaluation result on real world programs.
category program slocslf afl aflfast driller klee s2e paths seeds paths seeds paths seeds paths seeds paths seeds paths seeds imageexiv2 n a n a giflib libtiff openjpeg n a n a audiolame n a n a libsndfile compressionlibzip n a n a lrzip fontotfcc ttf2woff a exiv2.
b giflib.
c libtiff.
d openjpeg.
e lame.
f libsndfile.
g libzip.
h lrzip.
i ttf2woff.
j otfcc.legend fig.
path coverage.
x axis time over hours y axis the number of unique paths.
0x04 is an offset field that points to the location where the subsequent data e.g.
count and tag are stored.
usually the offset points to the following bytes.
however in the undocumented format the offset points to itself.
as a result the subsequent data overlaps with the offset field.
hence the count is 0x04 and the tag is 0x00.
such seed is uniquely supported by exiv2 and not supported by other tiff image processing tools e.g.
libtiff .
this seed input yields new path coverage in the fuzzing process.
we illustrate how slf generates such undocumented seed.
figure 9shows two critical checks in exiv2.
during fuzzing process an byte invalid input that fails the two checks is given to slf.
check circlecopyrtis of type i lhs and type ii rhs .
our multi goal search algorithm will try to fix rhs i.e.
file size and mutate lhs i.e.
input to satisfy the check assertion line of algorithm which results in assigning input with value 0x04.
check circlecopyrtis of type i lhs and type ii rhs and inter dependent with check1 circlecopyrton input .
to satisfy both checks our search algorithm will try to extend the file size.
after satisfying other subsequent checks the undocumented seed is generated.
c. google fuzzer test suite another set of experiments is on the google fuzzer test suite.
as a standard fuzzing benchmark it tags some locations to see if a testing tool can reach those locations.
table ii shows the programs the target location in the program and time used to reach this location measured by hours .
we only use the programs that do not come with a valid seed.
in total we evaluate on locations.
in the table t o means time out after hours and n a means the tool does not work on this program.
s2e does not work on this benchmark.
we have confirmed that with the authors of s2e.
the probable reason is that the target programs or the underlying third party libraries contain instructions operating on sse mmx and fp registers which are currently not supported by s2e.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
2a 00offset version count tag magic number 2a 00offset version tag countmagic number 00grfxphqwhg irupdw xqgrfxphqwhg irupdw fig.
example of undocumented tiff format.
01offset input 02if offset file size error circlecopyrt 03count input 04if count offset file size error circlecopyrt fig.
two critical checks in exiv2.
in summary our tool ran on all the programs and can reach locations with reasonable execution time.
afl aflfast and driller also work on all programs and each can reach locations within hours.
klee does not work on harfbuzz as some of its instructions are not supported by klee.
in total klee is able to reach locations.
slf is able to reach all the locations reached by any other tools.
in terms of time used to reach the location it outperforms afl aflfast and driller in all cases.
in some cases klee takes less time.
manual inspection discloses that these locations are on shallow paths.
for complex cases e.g.
lcms where klee cannot reach within hours slf is still able to reach the location.
d. threats to v alidity the initial input may impact the effectiveness of fuzzing.
to be fair we used the same byte invalid input for afl aflfast driller slf.
for symbolic execution tools the symbolic file size and path exploration algorithm impact results.
we used the recommended settings for s2e and klee.
v. r elated work mutation based fuzzing.
mutation based fuzzing generates new inputs by mutating seed inputs.
the random nature of mutations often leads to lengthy fuzzing time.
a number of existing works were proposed to boost fuzzing by improving seed selection optimizing fuzzing guidance using statistical metrics and leveraging vulnerability specific patterns .
these techniques mainly aim to improve the quality of generated selected seeds for fuzzing.
this demonstrates the importance of seed inputs.
compared to these techniques slf does not require a valid seed to begin with.
it has the unique feature of satisfying input validity checks.
generative fuzzing.
another popular fuzzing method is to generate valid seed inputs from input format specifications.
some approaches e.g.
spike and peach take human written input specifications as templates.
other approaches e.g.
skyfire and learn fuzz learn specification from a large corpus.
they are effective when a high quality training set is available.table ii evaluation result on google fuzzer test suite.
program sloc locationreaching time hours slf afl aflfast driller klee freetype2 ttgload.c t o t o t o t o t o guetzli output image.cc .
t o t o t o t o harfbuzz hb buffer.cc t o t o t o t o n a lcms cmsintrp.c .
t o t o t o t o libarchive archive...warc.c t o t o t o t o t o libjpeg jdmarker.c .
.
.
.
.
libpng 820png.c .
t o t o t o .
pngread.c .
t o t o t o .
pngrutil.c t o t o t o t o t o pngread.c .
t o t o t o .
pngrutil.c t o t o t o t o t o pngrutil.c .
.
.
.
.
proj4 pjurm5.c .
.
.
.
t o vorbis 990codebook.c t o t o t o t o t o codebook.c t o t o t o t o t o res0.c t o t o t o t o t o woff2 594woff2 dec.cc t o t o t o t o t o woff dec.cc .
.
.
.
.
symbolic execution.
symbolic execution techniques are very popular and effective software testing .
like slf it does not require seed inputs to begin with.
fuzzing and symbolic execution both have their pros and cons.
for example fuzzing is usually more applicable and easily works on large and complex software.
slf belongs to fuzzing and hence has similar benefits.
while traditional fuzzers are inferior to symbolic execution in precise resolution of path conditions slf mitigates such limitations by having a gradient guided multi goal search algorithm.
additionally slf features the capabilities of directly resolving the constraints in various kinds of validity checks such as offset and count checks.
search based testing.
search based testing techniques view testing problems including automatic generation of test cases test case minimization and prioritization and regression testing as search problems and try to use meta heuristic search techniques e.g.
genetic algorithms simulated annealing and tabu search to solve them .
in particular evosuite optimizes a test suite towards satisfying a coverage criterion.
while fuzzing can be viewed as a search based testing technique slf uniquely focuses on validity checks and works by classifying such checks and performing class specific mutations.
on the other hand sfl may be complementary to existing search based techniques by providing field information and validity check type information.
hybrid fuzzing.
hybrid fuzzing performs fuzzing for most of the time and performs symbolic execution when needed.
driller and angora are both such fuzzers.
they can handle large real world programs using fuzzing as well as leveraging the power of symbolic execution to solve difficult constraints.
however when to start the symbolic execution remains a hard problem for such tools as shown in section iv .
vi.
c onclusion we develop a novel fuzzing technique that can generate valid seed inputs from scratch.
it works by classifying input validity checks into a number of types and conduct type specific mutations.
it features a multi goal search algorithm that can satisfy multiple inter dependent validity checks all together.
our evaluation shows that our technique is highly effective and efficient outperforming existing tools.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
acknowledgment the authors would like to thank the anonymous reviewers for their constructive comments.
also the authors would like to express their thanks to yang xiao and hui peng for their help in experiment settings and xinjie wang for her help in illustration.
the authors were supported in part by darpa fa8650 c nsf and onr n000141410468 and n000141712947 sandia national lab under award and nsfc u1836209.