automated test input generation for android are we there yet?
shauvik roy choudhary georgia institute of technology usa shauvik cc.gatech.edualessandra gorla imdea software institute spain alessandra.gorla imdea.orgalessandro orso georgia institute of technology usa orso cc.gatech.edu abstract mobile applications often simply called apps are increasingly widespread and we use them daily to perform a number of activities.
like all software apps must be adequately tested to gain confidence that they behave correctly.
therefore in recent years researchers and practitioners alike have begun to investigate ways to automate apps testing.
in particular because of android s open source nature and its large share of the market a great deal of research has been performed on input generation techniques for apps that run on the android operating systems.
at this point in time there are in fact a number of such techniques in the literature which differ in the way they generate inputs the strategy they use to explore the behavior of the app under test and the specific heuristics they use.
to better understand the strengths and weaknesses of these existing approaches and get general insight on ways they could be made more effective in this paper we perform a thorough comparison of the main existing test input generation tools for android.
in our comparison we evaluate the effectiveness of these tools and their corresponding techniques according to four metrics code coverage ability to detect faults ability to work on multiple platforms and ease of use.
our results provide a clear picture of the state of the art in input generation for android apps and identify future research directions that if suitably investigated could lead to more effective and efficient testing tools for android.
.
introduction in the past few years we have witnessed an incredible boom of the mobile applications or simply apps business.
according to recent reports google play the most popular app market for the android platform currently offers over one million applications.
similarly the app market for apple s ios the itunes store provides a comparable number of applications .
the prevalence of smartphones tablets and their applications is also witnessed by the recent overtake of mobile apps over traditional desktop applications in terms of internet usage in the us .
apps like all software must be adequately tested to gain confidence that they behave correctly.
it is therefore not surprising that with such a growth the demand for tools for automatically testingmobile applications has also grown and with it the amount of research in this area.
most of the researchers and practitioners efforts in this area target the android platform for multiple reasons.
first and foremost android has the largest share of the mobile market at the moment which makes this platform extremely appealing for industry practitioners.
second due to the fragmentation of devices and os releases android apps often suffer from cross platform and cross version incompatibilities which makes manual testing of these apps particularly expensive and thus particularly worth automating.
third the open source nature of the android platform and its related technologies makes it a more suitable target for academic researchers who can get complete access to both the apps and the underlying operating system.
in addition android applications are developed in java.
even if they are compiled into dalvik bytecode which significantly differs from java bytecode there exist multiple frameworks that can transform dalvik bytecode into formats that are more familiar and more amenable to analysis and instrumentation e.g.
java bytecode jimple and smali .
for all these reasons there has been a great deal of research in static analysis and testing of android apps.
in the area of testing in particular researchers have developed techniques and tools to target one of the most expensive software testing activities test input generation.
there are in fact a number of these techniques in the literature nowadays which differ in the way they generate inputs the strategy they use to explore the behavior of the app under test and the specific heuristics they use.
it is however still unclear what are the strengths and weaknesses of these different approaches how effective they are in general and with respect to one another and if and how they could be improved.
to answer these questions in this paper we present a comparative study of the main existing test input generation techniques for android.1the goal of the study is twofold.
the first goal is to assess these techniques and corresponding tools to understand how they compare to one another and which ones may be more suitable in which context e.g.
type of apps .
our second goal is to get a better understanding of the general tradeoffs involved in test input generation for android and identify ways in which existing techniques can be improved or new techniques be defined.
in our comparison we ran the tools considered on over realworld apps while evaluating their effectiveness along several dimensions code coverage fault detection ability to work on multiple platforms and ease of use .
we considered coverage because test input generation tools should be able to explore as much behavior of the application under test as possible and code coverage is typically used as a proxy for that.
we therefore used the tools to generate 1we had to exclude some techniques and tools from our study because either they were not available or we were not able to install them despite seeking their authors help.arxiv .07217v2 mar 2015test inputs for each of the apps considered and measured the coverage achieved by the different tools on each app.
although code coverage is a well understood and commonly used measure it is normally a gross approximation of behavior.
ultimately test input generation tools should generate inputs that are effective at revealing faults in the code under test.
for this reason in our study we also measured how many of the inputs generated by a tool resulted in one or more failures identified as uncaught exceptions in the apps considered.
we also performed additional manual and automated checks to make sure that the thrown exceptions represented actual failures.
because of the fragmentation of the android ecosystem another important characteristic for the tools considered is their ability to work on different hardware and software configurations.
we therefore considered and assessed also this aspect of the tools by running them on different versions of the android environment.
finally we also evaluated the ease of use of the tools by assessing how difficult it was to install and run them and the amount of manual work involved in their use.
although this is a very practical aspect and one that normally receives only limited attention in research prototypes reasonable ease of use can enable replication studies and allow other researchers to build on the existing technique and tool.
our results show that although the existing techniques and tools we studied are effective they also have weaknesses and limitations and there is room for improvement.
in our analysis of the results we discuss such limitations and identify future research directions that if suitably investigated could lead to more effective and efficient testing tools for android.
to allow other researchers to replicate our studies and build on our work we made all of our experimental infrastructure and data publicly available at .
the main contributions of this paper are a survey of the main existing test input generation techniques for apps that run on the android operating system.
an extensive comparative study of such techniques and tools performed on over real world android apps.
an analysis of the results that discusses strengths and weaknesses of the different techniques considered and highlights possible future directions in the area.
a set of artifacts consisting of experimental infrastructure as well as data that are freely available and allow for replicating our work and building on it.
the remainder of the paper is structured as follows.
section provides background information on the android environment and apps.
section discusses the test input generation techniques and tools that we consider in our study.
section describes our study setup and presents our results.
section analyzes and discusses our findings.
finally section concludes the paper.
.
the android platform android applications are mainly written in java although some high performance demanding applications delegate critical parts of the implementation to native code written in c or c .
during the build process java source code gets first compiled into java bytecode then translated into dalvik bytecode and finally stored into a machine executable file in .dex format.
apps are distributed in the form of apk files which are compressed folders containing dex files native code whenever present and other application resources.
android applications run on top of a stack of three other main software layers as represented in figure .
the android framework which lays below the android apps provides an api to access facilities without dealing with the low level details of the operating linux kernel librariesjniandroid runtimedalvik vmartzygoteandroid frameworkandroid apipre installed appsandroid apps nativefigure the android architecture system.
so far there have been different framework releases and consequent changes in the api.
framework versioning is the first element that causes the fragmentation problem in android.
since it takes several months for a new framework release to become predominant on android devices most of the devices in the field run older versions of the framework.
as a consequence android developers should make an effort to make their apps compatible with older versions of the framework and it is therefore particularly desirable to test apps on different hardware and software configurations before releasing them.
in the android runtime layer the zygote daemon manages the applications execution by creating a separate dalvik virtual machine for each running app.
dalvik virtual machines are register based vms that can interpret the dalvik bytecode.
the most recent version of android includes radical changes in the runtime layer as it introduces art i.e.
android run time a new runtime environment that dramatically improves app performance and will eventually replace the dalvik vm.
at the bottom of the android software stack stands a customized linux kernel which is responsible of the main functionality of the system.
a set of native code libraries such as webkit libc and ssl communicate directly with the kernel and provide a basic hardware abstraction to the runtime layer.
android applications android applications declare their main components in the androidmanifest.xml file.
components can be of four different types activities are the components in charge of an app s user interface.
each activity is a window containing various ui elements such as buttons and text areas.
developers can control the behavior of each activity by implementing appropriate callbacks for each life cycle phase i.e.
created paused resumed and destroyed .
activities react to user input events such as clicks and consequently are the primary target of testing tools for android.
services are application components that can perform longrunning operations in the background.
unlike activities they do not provide a user interface and consequently they are usually not a direct target of android testing tools although they might be indirectly tested through some activities.
broadcast receivers and intents allow inter process communication.
applications can register broadcast receivers to be notified by means of intents about specific system events.
apps can thus for instance react whenever a new sms is received a new connection is available or a new call is being made.
broad cast receivers can either be declared in the manifest file or at runtime in the application code.
in order to properly explore the behavior of an app testing tools should be aware of what are the relevant broadcast receivers so that they could trigger the right intents.
content providers act as a structured interface to shared data stores such as contacts and calendar databases.
applications may have their own content providers and may make them available to other apps.
like all software the behavior of an app may highly depend on the state of such content providers e.g.
on whether the list of contacts is empty or it contains duplicates .
as a consequence testing tools should mock content providers in an attempt to make tests deterministic and achieve higher coverage of an app s behavior.
despite being gui based and mainly written in java android apps significantly differ from java standalone gui applications and manifest somehow different kinds of bugs .
existing test input generation tools for java cannot therefore be straightforwardly used to test android apps and custom tools must be adopted instead.
for this reason a great deal of research has been performed in this area and several test input generation techniques and tools for android applications have been proposed.
the next section provides an overview of the main existing tools in this arena.
.
existing android testing tools an overview as we mentioned in the introduction there are several existing test input generation tools for android.
the primary goal of these tools is to detect existing faults in android apps and app developers are thus typically the main stakeholders as they can automatically test their application and fix discovered issues before deploying it.
the dynamic traces generated by these tools however can also be the starting point of more specific analyses which can be of primary interest of android market maintainers and final users.
in fact android apps heavily use features such as native code reflection and code obfuscation that hit the limitations of almost every static analysis tool .
thus to explore the behavior of android apps and overcome such limitations it is common practice to resort to dynamic analysis and use test input generation tools to explore enough behaviors for the analysis.
google for instance is known to run every app on its cloud infrastructure to simulate how it might work on user devices and look for malicious behavior .
only apps that pass this test are listed in the google play market.
finally users can analyze apps focusing on specific aspects such as observing possible leaks of sensitive information and profiling battery memory or networking usage .
test input generation tools can either analyze the app in isolation or focus on the interaction of the app with other apps or the underlying framework.
whatever is the final usage of these tools the challenge is to generate relevant inputs to exercise as much behavior of application under test as possible.
as android apps are event driven inputs are normally in the form of events which can either mimic user interactions ui events such as clicks scrolls and text inputs or system events such as the notification of a newly received sms.
testing tools can generate such inputs following different strategies.
they can generate them randomly or follow a systematic exploration strategy.
in this latter case exploration can either be guided by a model of the app which constructed statically or dynamically or exploit techniques that aim to achieve as much code coverage as possible.
along a different dimension testing tools can generate events by considering android apps as either a black box or awhite box .
in this latter case they would consider the code structure.
grey box approaches are alsopossible which typically extract high level properties of the app such as the list of activities and the list of ui elements contained in each activity in order to generate events that will likely expose unexplored behavior.
table provides an overview of the test input generation tools for android that have been presented in different venues.
to the best of our knowledge this list is complete.
the table reports all these tools and classifies them according to the metrics reported above.
moreover the tables reports other relevant features of each tool such as whether it is publicly available or rather it is only described in a paper or its distribution is under restricted policies of a company whether the tool requires the source code of the application under test and whether it requires instrumentation either at the application level or of the underlying android framework.
the following sections report more details of each of these tools.
.
random exploration strategy the first class of test input generation tools we consider employs a random strategy to generate inputs for android apps.
in the most simple form the random strategy generates only ui events.
randomly generating system events would be highly inefficient as there are too many such events and applications usually react to only few of them and only under specific conditions.
many tools that fall in this category aim to test inter application communications by randomly generating values for intents .
intent fuzzers despite being test input generators have quite a different purpose.
by randomly generating inputs they mainly generate invalid ones thus testing the robustness of an app.
these tools are also quite effective at revealing security vulnerabilities such as denial of service vulnerabilities.
we now provide a brief description of the tools that fall in this category and their salient characteristics.
monkey is the most frequently used tool to test android apps since it is part of the android developers toolkit and thus does not require any additional installation effort.
monkey implements the most basic random strategy as it considers the app under test a black box and can only generate ui events.
users have to specify the number of events they want monkey to generate.
once this upper bound has been reached monkey stops.
dynodroid is also based on random exploration but it has several features that make its exploration more efficient compared to monkey.
first of all it can generate system events and it does so by checking which ones are relevant for the application.
dynodroid gets this information by monitoring when an application registers a listener within the android framework.
for this reason it requires to instrument the framework.
the random event generation strategy of dynodroid is smarter than the one that monkey implements.
it can either select the events that have been least frequently selected frequency strategy and can keep into account the context biasedrandom strategy that is events that are relevant in more contexts will be selected more often.
for our study we used the biasedrandom strategy which is the default one.
an additional improvement of dynodroid is the ability to let users manually provide inputs e.g.
for authentication when the exploration is stalling.
null intent fuzzer is an open source basic intent fuzzer that aims to reveal crashes of activities that do not properly check input intents.
while quite effective at revealing this type of problems it is fairly specialized and not effective at detecting other issues.
intent fuzzer mainly tests how an app can interact with other apps installed on the same device.
it includes a static analysis component which is built on top of flowdroid for identifyingtable overview of existing test input generation tools for android.
name available instrumentation eventsexploration strategyneeds source codetesting strategy platform app ui system monkey x x random black box dynodroid x x x x random black box droidfuzzer x random black box intentfuzzer x random white box null intentfuzzer x random black box guiripper xa x x model based black box orbit x model based x grey box a3e depth first x x x model based black box swifthand x x x model based black box puma x x x model based black box a3e targeted x x systematic grey box evodroid x x systematic white box acteve x x x x x systematic x white box jpf android x x systematic x white box a not open source.
the expected structure of intents so that the fuzzer can generate them accordingly.
this tool has shown to be effective at revealing security issues.
maji et al.
worked on a similar intent fuzzer but their tool has more limitations than intent fuzzer.
droidfuzzer is different from other tools that mainly generate ui events or intents.
it solely generates inputs for activities that accept mime data types such as a vi mp3 and html files.
the authors of the paper show how this tool could make some video player apps crash.
droidfuzzer is supposed to be implemented as an android application.
however it is not available and the authors did not reply to our request for the tool.
in general the advantage of random test input generators is that they can efficiently generate events and this makes them particularly suitable for stress testing.
however a random strategy would hardly be able to generate highly specific inputs.
moreover these tools are not aware of how much behavior of the application has been already covered and thus are likely to generate redundant events that do not help the exploration.
finally they do not have a stopping criterion that indicates the success of the exploration but rather resort to a manually specified timeout.
.
model based exploration strategy following the example of several web crawlers and gui testing tools for stand alone applications some android testing tools build and use a gui model of the application to generate events and systematically explore the behavior of the application.
these models are usually finite state machines that have activities as states and events as transitions.
some tools build more precise models by differentiating the state of activity elements when representing states e.g.
the same activity with a button enabled and disabled would be represented as two separate states .
most tools build such model dynamically and terminate when all the events that can be triggered from all the discovered states lead to already explored states.
guiripper which later became mobiguitar dynamically builds a model of the app under test by crawling it from a starting state.
when visiting a new state it keeps a list of events that can be generated on the current state of the activity and systematically triggers them.
guiripper implements a dfs strategy and it restarts the exploration from the starting state when it cannot detect new states during the exploration.
it generates only ui events thus it cannot expose behavior of the app that dependon system events.
guiripper has two characteristics that make it unique among model based tools.
first it allows for exploring an application from different starting states.
this however has to be done manually.
moreover it allows testers to provide a set of input values that can be used during the exploration.
guiripper is publicly available but unfortunately it is not open source and its binary version is compiled for windows machines.
orbit implements the same exploration strategy of guiripper but statically analyzes the application s source code to understand which ui events are relevant for a specific activity.
it is thus supposed to be more efficient than guiripper as it should generate only relevant inputs.
however the tool is unfortunately not available as it is propriety of fujitsu labs.
it is unclear whether orbit requires any instrumentation of the platform or of the application to run but we believe that this is not the case.
a3e depth first is an open source tool that implements two totally distinct and complementary strategies.
the first one implements a depth first search on the dynamic model of the application.
in essence it implements the exact same exploration strategy of the previous tools.
its model representation is more abstract than the one used by other tools as it represents each activity as a single state without considering different states of the elements of the activity.
this abstraction does not allow the tool to distinguish some states that are different and may lead to missing some behavior that would be easy to exercise if a more accurate model where to be used.
swifthand has as its main goal that to maximize the coverage of the application under test.
similarly to the previously mentioned tools swifthand uses a dynamic finite state machine model of the app and one of its main characteristics is that it optimizes the exploration strategy to minimize the restarts of the app while crawling.
swifthand generates only touching and scrolling ui events and cannot generate system events.
puma is a novel tool that includes a generic ui automator that provides the random exploration also implemented by monkey.
the novelty of this tool is not in the exploration strategy but rather in its design.
puma is a framework that can be easily extended to implement any dynamic analysis on android apps using the basic monkey exploration strategy.
moreover it allows for easily implementing different exploration strategies as the framework provides a finite state machine representation of the app.
it also allows for easily redefining the state representation and the logic to generate events.
puma is publicly available andopen source.
it is however only compatible with the most recent releases of the android framework.
using a model of the application should intuitively lead to more effective results in terms of code coverage.
using a model would in fact limit the number of redundant inputs that a random approach generates.
the main limitation of these tools stands in the state representation they use as they all represent new states only when some event triggers changes in the gui.
some events however may change the internal state of the app without affecting the gui.
in such situations these algorithm would miss the change consider the event irrelevant and continue the exploration in a different direction.
a common scenario in which this problem occurs is in the presence of services as services do not have any user interface see section .
.
systematic exploration strategy some application behavior can only be revealed upon providing specific inputs.
this is the reason why some android testing tools use more sophisticated techniques such as symbolic execution and evolutionary algorithms to guide the exploration towards previously uncovered code.
a3e targeted provides an alternative exploration strategy that complements the one described in section .
.
the targeted approach relies on a component that by means of taint analysis can build the static activity transition graph of the app.
such graph is an alternative to the dynamic finite state machine model of the depth first search exploration and allows the tool to cover activities more efficiently by generating intents.
while the tool is available on a public repository this strategy does not seems to be.
evodroid relies on evolutionary algorithms to generate relevant inputs.
in the evolutionary algorithms framework evodroid represents individuals as sequences of test inputs and implements the fitness function so as to maximize coverage.
evodroid is no longer publicly available.
in section .
we provide more details about this.
acteve is a concolic testing tool that symbolically tracks events from the point in the framework where they are generated up to the point where they are handled in the app.
for this reasons acteve needs to instrument both the framework and the app under test.
acteve handles both system and ui events.
jpf android extends java pathfinder jpf a popular model checking tool for java to support android apps.
this would allow to verify apps against specific properties.
liu et al.
were the first who investigated the possibility of extending jpf to work with android apps .
what they present however is mainly a feasibility study.
they themselves admit that developing the necessary components would require a lot of additional engineering efforts.
van der merwe et al.
went beyond that and properly implemented and open sourced the necessary extensions to use jpf with android.
jpf android aim to explore all paths in an android app and can identify deadlocks and runtime exceptions.
the current limitations of the tool however seriously limit its practical applicability.
implementing a systematic strategy leads to clear benefits in exploring behavior that would be hard to reach with random techniques.
compared to random techniques however these tools are considerably less scalable.
.
empirical study to evaluate the test input generation tools that we considered see section we deployed them along with a group of androidapplications on a common virtualized infrastructure.
such infrastructure aims to ease the comparison of testing tools for android and we make it available such that researchers and practitioners can easily evaluate new android testing tools against existing ones in the future.
our evaluation does not include all the tools listed in table .
we explain the reasons why we had to exclude some of the tools in section .
.
the following sections provide more details on the virtualized infrastructure section .
and on the set of mobile apps that we considered as benchmarks for our study section .
.
our study evaluated these tools according to four main criteria c1 effectiveness of the exploration strategy.
the inputs that these tools generate should ideally cover as much behavior as possible of the app under test.
since code coverage is a common proxy to estimate behavior coverage we evaluate the statement coverage that each tool achieves on each benchmark.
we also report a comparison study in terms of code coverage among different tools.
c2 fault detection ability.
the primary goal of test input generators is to expose existing faults.
we therefore evaluate for each tool how many failures it triggers for each app and we then compare the effectiveness of different tools in terms of the failures they trigger.
c3 ease of use.
usability should be a primary concern for all tool developers even when tools are just research prototypes.
we evaluate the usability of each tool by considering how much effort it took us to install and use it.
c4 android framework compatibility.
one of the major problems in the android ecosystem is fragmentation.
test input generation tools for android should therefore ideally run on multiple versions of the android framework so that developers could assess how their app behaves in different environments.
each of these research questions is addressed separately in section .
c1 section .
c2 section .
c2 and section .
c4 .
.
selected tools our evaluation could not consider all the tools that we list in table .
first we decided to ignore intent fuzzers as these tools do not aim to test the whole behavior of an app but rather to expose possible security vulnerabilities.
therefore it would have been hard to compare the results provided by these tools with other test input generators.
we initially considered the possibility of evaluating droidfuzzer intentfuzzer and null intent fuzzer separately.
however null intent fuzzer requires to manually select each activity that the fuzzer should target and it is therefore not feasible to evaluate it on large scale experiments.
moreover we had to exclude droidfuzzer because the tool is not publicly available and we were not successful in reaching the authors by email.
among the rest of the tools we had to exclude also evodroid and orbit.
evodroid used to be publicly available on its project website and we tried to install and run it.
we also contacted the authors after we ran into some problems with missing dependencies but despite their willingness to help we never managed to get all the files we needed and the tool to work.
obtaining the source code and fixing the issues ourselves was unfortunately not an option due to the contractual agreements with their funding agencies.
moreover at the time of writing the tool is no longer available even as a closedsource package.
the problem with orbit is that it is a proprietary software of fujitsu labs and therefore the authors could not share the tool with us.
we also excluded jpf android even if the tool is publicly available.
this tool in fact expects users to manually specify the inputsequence that jpf should consider for verification and this would have been time consuming to do for all the benchmarks we considered.
finally we could not evaluate the a3e targeted since this strategy was not available in the public distribution of the tool at the time of our experiments.
.
mobile app benchmarks to evaluate the selected tools we needed a common set of benchmarks.
obtaining large sets of android apps is not an issue as binaries can be directly downloaded from app markets and there are many platforms such as f droid2that distribute open source android applications.
however since many of these tools are not maintained and therefore could easily crash on apps that utilize recent features.
thus for our experiments we combined all the open source mobile application benchmarks that were considered in the evaluation of at least one tool.
we retrieved the same version of the benchmarks as they were reported in each paper.
puma and a3e were originally evaluated on a set of apps downloaded from the google play market.
we excluded these apps from our dataset because some tools need the application source code and therefore it would have been impossible to run them on these benchmarks.
we collected applications in total.
of them come from the dynodroid paper from guiripper from acteve and from swifthand .
table reports the whole list of apps that we collected together with the corresponding version and category.
for each app we report whether it was part of the evaluation benchmarks of a specific tool and we also report whether during our evaluation the tools failed in analyzing it.
.
experimental setup we ran our experiments on ubuntu .
virtual machines running on a linux server.
we used oracle virtualbox3as our virtualization software and vagrant4to manage these virtual machines.
each virtual machine was configured with cores and 6gb ram.
inside the virtual machine we installed the test input generation tools the android apps and three versions of the android sdk versions gingerbread ice cream sandwich and kitkat .
we chose these versions based on their popularity to satisfy tool dependencies and the most recent at the time of the experiments.
the emulator was configured to use 4gb ram and each tool was allowed to run for hour on each benchmark application.
for every run our infrastructure creates a new emulator with necessary tool configuration and later destroys this emulator to avoid side effects between tools and applications.
given that many testing tools and applications are non deterministic we repeated each experiment times and we report the mean values of all the run.
coverage and system log collection.
for each run we collected the code coverage for the app under test.
we selected emma5as our code coverage tool because it is officially supported and available with the android platform.
however since emma does not allow exporting raw statement coverage results we parsed the html coverage reports to extract line coverage information for comparison between tools.
in particular we used this information to compute for each tool pair the number of 3oracle vm virtualbox 4vagrant 5emma a free java code coverage tool sourceforge.net table list of subject apps.
xindicates that the app was used originally in the tool s evaluation and indicates that the app crashed while being exercised by the tool in our experiments subject monkey acteve dynodroid a3e guiripper puma swifthand name ver.
category amazed .
.
casual x anycut .
productiv.
x divide conquer .
casual x lolcatbuilder 2entertain.
x munchlife .
.
entertain.
x passwordmakerpro .
.
tools x photostream .
media x quicksettings .
.
.
tools x randommusicplay 1music xx spritetext sample x syncmypix .
media x triangle sample x a2dp v olume .
.
transport x alogcat .
.
tools x aarddictionary .
.
reference x baterrydog .
.
tools x ftp server .
tools x bites .
lifestyle x battery circle .
tools x addi .
tools x manpages .
tools x alarm clock .
productiv.
x auto answer .
tools x hndroid .
.
news x multi sms .
comm.
x world clock .
tools x nectroid .
.
media x acal .
productiv.
x jamendo .
.
music x androidomatick.
.
comm.
x yahtzee 1casual x aagtl .
.
tools x mirrored .
.
news x dialer2 .
productiv.
x fileexplorer 1productiv.
x gestures 1sample x hotdeath .
.
card x adsdroid .
reference x mylock 42tools x lockpatterngen.
2tools x agrep .
.
tools x k 9mail .
comm.
x netcounter .
.
tools x bomber 1casual x frozenbubble .
puzzle x anymemo .
.
education x x blokish 2puzzle x zooborns .
.
entertain.
x importcontacts .
tools x wikipedia .
.
reference x keepassdroid .
.
tools x soundboard 1sample x countdowntimer .
.
tools x ringdroid .
media x spritemethodtest .
sample x bookcatalogue .
tools x translate .
productiv.
x tomdroidnotes .0a social x wordpress .
.
productiv.
x mileage .
.
finance x sanity .
comm.
x dalvikexplorer .
tools x mininoteviewer .
productiv.
x myexpenses .
.
finance x learnmusicnotes .
puzzle x tippytipper .
.
finance x weightchart .
.
health x whohasmystuff .
.
productiv.
x statements covered by both tools and the number of statements covered by each of them separately.monkey acteve dynodroid a3e guiripper puma android test input generation tools020406080100coveragefigure variance of the coverage across the applications over runs.
to each benchmark we added a broadcast receiver to save intermediate coverage results to the disk.
dynodroid used a similar strategy and this was necessary to collect coverage from the applications before they were restarted by the test input generation tools and also to track the progress of the tools at regular intervals.
swifthand is an exception to this protocol.
this tool in fact internally instruments the app under test for collecting branch coverage and to keep track of the app s lifecycle.
the instrumentation is critical to the tool s functioning but conflicts with android s emma instrumentation which could not be resolved by us within weeks.
thus we decided not to collect and compare the statement coverage information of this tool with others.
however the branch coverage information collected by swifthand on the benchmark applications is available with our dataset.
to gather the different failures in the app we also collected the entire system log also called logcat from the emulator running the app under test.
from these logs we extracted failures that occurred while the app was being tested in a semi automated fashion.
specifically we wrote a script to find patterns of exceptions or errors in the log file and extract them along with their available stack traces in the log.
we manually analyzed them to ignore any failures not related to the app s execution e.g.
failures of the tool themselves or initialization errors of other apps in the android emulator .
all unique instances of remaining failures were considered for our results.
.
c1 exploration strategy effectiveness test input generation tools for android implement different strategies to explore as much behavior as possible of the application under test.
section presented an overview of the three main strategies that is random model based and systematic.
although some of these tools have been already evaluated according to how much code coverage they can achieve it is still unclear whether there is any strategy that is better than others in practice.
previous evaluations were either incomplete because they did not include comparison with other tools or they were according to our opinion biased.
since we believe that the most critical resource is time tools should evaluate how much coverage they can achieve within a certain time limit.
tools such as dynodroid and evodroid instead have been compared to monkey by comparing the coverage achieved given the same number of generated events.
we thus set up the experiment by running each tool times on each application of table and we collected the achieved coverage as described in section .
.
time in minutes020406080100coveragemonkey acteve dynodroid a3e guiripper pumafigure progressive coverage figure reports the variance of the mean coverage of runs that each tool achieved on the considered benchmarks.
we can see that on average dynodroid and monkey perform better than other tools followed by acteve .
the other three tools i.e.
a3e guiripper and puma achieve quite a low coverage.
despite this even those tools that on average achieve low coverage can reach very high coverage approximately for few apps.
we manually investigated these apps and we saw that these are as expected the most simple ones.
the two outliers for which every tool achieved very high coverage are divideandconquer and randommusicplayer.
the first one is a game that accepts only touches and swipes as events and they can be provided without much logic in order to proceed with the game.
randommusicplayer is a music player that randomly plays music.
the possible user actions are quite limited as there is only one activity with buttons.
similarly there are some applications for which every tool even the ones that performed best achieved very low coverage i.e.
lower than .
two of these apps k9mail a mail client and passwordmakerpro an app to generate and store authentication data highly depend on external factors such as the availability of a valid account.
such inputs are nearly impossible to generate automatically and therefore every tool stalls at the beginning of the exploration.
few tools provide an option to manually interact with application at first and then use the tool to perform subsequence test input generation.
however we did not use this feature for scalability reasons and concerns of giving such tools an unfair advantage of the manual intelligence.
figure reports the progressive coverage of each tool over the maximum time bound we gave i.e.
minutes.
the plot reports the mean coverage across all apps over the runs.
this plot evidences an interesting finding that is all tools can hit their maximum coverage within few minutes between and minutes .
the only exception to this is guiripper.
the likely reason why this happens in that guiripper frequently restarts the exploration from the starting state and this operation takes time.
this is the main problem that swifthand addresses by implementing an exploration strategy that limits the number of restarts.
.
c2 fault detection ability the final goal of test input generation tools is to expose faults in the app under test.
therefore beside code coverage we checked how many failures each tool can reveal with a time budget of one hour per app.
none of the android tools can identify failures other than runtime exceptions although there is some promising work that goes in that direction .
figure reports the results of this study.
numbers on the y axis represent the cumulative unique failures across the runs acrossmonkey acteve dynodroid a3e guiripper puma swifthand android test input generation tools0102030405060708090failuresjava.io java.net java.lang library custom android.database android.contentfigure distribution of the failures triggered by the tools.
all apps.
we consider a failure unique when its stack trace differs from other ones.
the plot also reports some high level statistics about which are the most frequent failures we report the package name of the runtime exception .
only few of them are custom exceptions i.e.
exceptions that are declared in the app under test .
the vast majority of them generate standard java exceptions and among them the most frequent ones are null pointer exceptions.
because swifthand is the tool with the worst performance in this part of the evaluation we looked into its results in more detail to understand the reasons behind that.
we found that swifthand crashes on many of our benchmarks which prevents it from producing useful results in these cases.
further analysis revealed that the crashes are most likely due to swifthand s use of asmdex to instrument the apps it is testing.
the asmdex framework is in fact known to be buggy not well maintained and crash prone.
figure reports a pairwise comparison of each tool according to coverage upper part of the figure i.e.
boxes with white background and fault detection ability lower part of the figure i.e.
boxes with yellow background .
in this figure we compare the coverage of the best run out of runs for each tool whereas the number of failures reported are the cumulative failures across runs with the same stack trace.
the comparison reports which lines are covered and respectively which failures are reported by both tools reported in grey and which lines are covered by only one of the two tools.
results show that tools do not complement each other in terms of code coverage but they do in terms of fault detection.
in fact while for coverage the common parts are significant it is almost the opposite in terms of failures.
.
c3 ease of use tools should ideally work out of the box and should not require extra effort of the user in terms of configuration.
table reports whether the tool worked out of the box no effort whether it required some effort little effort either to properly configure it or to fix minor issues or whether it required major efforts major effort to make it work.
we now briefly report our experience with installing each tool and we describe the required fixes to make each of them work.
some of the changes were required to make the tools run on our infrastructure.
monkey we used the vanilla monkey from the android distribution for our experimentation.
the monkey tool was configuredtable ease of use and compatibility of each tool with the most common android framework versions.
name ease use compatibility monkey no effort any dynodroid no effort v. .
guiripper major effort any a3e depth first little effort any swifthand major effort v. .
puma little effort v. .
acteve major effort v. .
to ignore any crash system timeout or security exceptions during the experiment and to continue till the experiment timeout was reached.
in addition we configured it to wait milliseconds between actions as this same delay value was also used in other tools.
configuring monkey for our infrastructure required no extra effort.
acteve we consulted the authors to apply minor fixes to the instrumentation component of acteve which instruments both the android sdk and the app under test.
while generating tests acteve often restarts the application.
to ensure that we do not lose the coverage information we modified acteve to save the intermediate coverage before application restarts.
guiripper guiripper is available as a binary distribution with batch scripts to run it in the windows os.
we reversed engineered guiripper and wrote shell scripts to port it to our linux based infrastructure.
we configured the tool to use its systematic ripping strategy instead of the default random strategy.
during the experiments guiripper often restarts the android emulator from a snapshot to return to the initial state.
we modified the tool to save intermediate coverage before such restarts.
dynodroid we obtained a running version of dynodroid from the virtual machine provided on the tool s page.
dynodroid is tightly coupled with the android emulator for which the tool includes an instrumented system image.
in addition the tool performs an extensive setup of the device after boot before starting the exploration.
we configured our timer to start counting at the start of the actual exploration to account for this one time setup time.
a3e we updated a3e s dependencies to make it work with the latest version of the android sdk.
the public release only supports the depth first exploration strategy for systematic testing which was used in our experiments.
in addition we modified the tool to report verbose results for android commands invoked and to not shutdown the emulator after input generation to allow us to collect reports from it.
swifthand the swifthand tool consists of two components front end which performs bytecode instrumentation of the apps and back end which performs test input generation.
we fixed the dependencies of the front end tool by obtaining an older version of the asmdex library and wrote a wrapper shell script to connect the two components in our infrastructure.
puma to get puma running we applied a minor patch to use an alternate api for taking device screenshots.
we also altered the different timeouts in the tool to match our experimental settings.
.
c4 android framework compatibility android developers have to constantly deal with the fragmentation problem that is their application has to run on devices that have different hardware characteristics and use different versions of the android framework.
it is therefore desirable of a test input generator tool to be compatible with multiple releases of the an monkey acteve0255075100acteve .
.
.
monkey dynodroid0255075100dynodroid .
.
.
monkey a3e0255075100a3e .
.
.
monkey guiripper0255075100guiripper .
.
.
monkey puma0255075100puma .
.
.
acteve dynodroid0255075100 .
.
.
acteve a3e0255075100 .
.
.
acteve guiripper0255075100 .
.
.
acteve puma0255075100 .
.
.
dynodroid a3e0255075100 .
.
.
dynodroid guiripper0255075100 .
.
.
dynodroid puma0255075100 .
.
.
a3e guiripper0255075100 .
.
.
a3e puma0255075100 .
.
.
guiripper puma0255075100 .
.
.
acteve monkey0255075100acteve1 dynodroid monkey0255075100dynodroid1 dynodroid acteve0255075100 a3e monkey0255075100a3e3 a3e acteve0255075100 a3e dynodroid0255075100 guiripper monkey0255075100guiripper4 guiripper acteve0255075100 guiripper dynodroid0255075100 guiripper a3e0255075100 puma monkey0255075100puma0 puma acteve0255075100 puma dynodroid0255075100 puma a3e0255075100 puma guiripper0255075100 swifthand monkey0255075100swifthand0 swifthand acteve0255075100 swifthand dynodroid0255075100 swifthand a3e0255075100 swifthand guiripper0255075100 swifthand puma0255075100 32monkeymonkeyfigure pairwise comparison of tools in terms of coverage and failures triggered.
the plots on top right show percent statement coverage of the tools and the ones in the bottom left section show absolute number of failures triggered.
the gray bars in all plots show commonalities between the pairs of tools .
droid framework in order to let developers assess the quality of their app in different environments.
therefore we ran each tool on three popular android framework releases as described in section .
and assessed whether it could work correctly.table reports the results of this study.
the table shows that out of tools do not offer this feature.
some tools puma and swifthand are compatible only with the most recent releases of the android framework while others acteve and dynodroid arebound to a very old one.
acteve and dynodroid could be compatible with other frameworks but this would require to instrument them first.
swifthand and puma instead are not compatible with older releases of the android framework because they use features of the underlying framework that are not available in previous releases.
.
discussion and future research directions the experiments presented in section report unexpected results the random exploration strategies implemented by monkey and dynodroid can obtain higher coverage than more sophisticated strategies implemented by other tools.
it thus seems that android applications are different from java stand alone application where random strategies have been shown to be highly inefficient compared to systematic strategies .
our results show that most of the behavior can be exercised by generating only ui events and to expose this behavior the random approach is effective enough.
considering the four criterion of the study monkey would clearly be the winner among the existing test input generation tools since it achieves on average the best coverage it can report the largest number of failures it is ease to use and works for every platform.
this does not mean that the other tools should not be considered.
actually it is quite the opposite since every other tool has strong points that if properly implemented and properly combined can lead to significant improvements.
we now list some of the features that some tools already implement and should be considered by other tools system events.
dynodroid and acteve can generate system events beside standard ui events.
even if the behavior of an app may depend only partially on system events generating them can reveal failures that would be hard to uncover otherwise.
minimize restarts.
progressive coverage shows that tools that frequently restart their exploration from the starting point need more time to achieve their maximum coverage.
the search algorithm that swifthand implements aims to minimize such restarts and thus allows tools to achieve high coverage in less time.
manually provided inputs.
specific behaviors can sometimes only be explored by providing specific inputs which may be hard to generate randomly or by means of systematic techniques.
some tools such as dynodroid and guiripper let users manually provide values that the tool can later use during the analysis.
this feature is highly desirable since it allows tools to explore the app in presence of login forms and similar complex inputs.
multiple starting states.
the behavior of many applications depend on the underlying content providers.
an email client for instance would show an empty inbox unless the content provider contains some messages.
guiripper starts exploring the application from different starting states e.g.
when the content provider is empty and when it has some entries .
even if this has to be done manually by the user by properly creating snapshots of the app it allows to potentially explore behavior that would be hard to explore otherwise.
avoid side effects among different runs.
using the tool on multiple apps requires to reset the environment to avoid side effects across multiple runs.
although in our experiments we used a fresh emulator instance between runs we realized that some tools such as dynodroid and a3e had capabilities to partially clean up the environment by uninstalling the application and deleting its data.
we believe that every tool should reuse the environment for efficiency reasons but should do it without affecting its accuracy.during our study we also identified limitations that significantly affect the effectiveness of all tools.
we report these limitations together with desirable and missing features such that they could be the focus of future research in this area reproducible test cases.
none of the tools allows to easily reproduce failures.
they report uncaught runtime exceptions on the logfile but they do not generate test cases that can be later rerun.
we believe that this is an essential feature that every tool of this type should have.
debugging support.
the lack of reproducible test cases makes it hard to identify the root cause of the failures.
the stack trace of the runtime exception is the only information that a developer can use and this information is lost in the execution logs.
testing tools for android should make failures more visible and should provide more information to ease debugging.
in our evaluation we had a hard time understanding if failures were caused by real faults or rather were caused by limitations of the emulator.
more information about each failure would have helped in this task.
mocking.
most apps for which tools had low coverage highly depend on environment elements such as content providers.
it is impossible to explore most of k9mail functionalities unless there is a valid email account already set up and unless there are existing emails in the account.
guiripper alleviates this problem by letting users prepare different snapshots of the app.
we believe that working on a proper mocking infrastructure for android apps would be a significant contribution as it would lead to drastic code coverage improvements.
sandboxing.
testing tools should also provide proper sandboxing that is they should block operations that may potentially have disruptive effects for instance sending emails using a valid account or allow critical changes using a real social networking account .
none of the tools keeps this problem into account.
focus of fragmentation problem.
while c4 of our evaluation showed that some tools can run on multiple versions of the android framework none of them is specifically designed for cross device testing.
while this is a different testing problem we believe that testing tools for android should also move towards this direction as fragmentation is the major problem that android developers have to face.
.
conclusion in this paper we presented a comparative study of the main existing test input generation techniques and corresponding tools for android.
we evaluated these tools according to four criteria code coverage fault detection capabilities ease of use and compatibility with multiple android framework versions.
after presenting the results of this comparison we discuss strengths and weaknesses of the different techniques and highlight potential venues for future research in this area.
all of our experimental infrastructure and data are publicly available at software androtest so that other researchers can replicate our studies and build on our work.