symbolic execution driven extraction of the parallel execution plans of spark applications luciano baresi dipartimento di elettronica informazione e bioingegneria politecnico di milano italy luciano.baresi polimi.itgiovanni denaro dipartimento di informatica sistemistica e comunicazione universit di milano bicocca italy denaro disco.unimib.itgiovanni quattrocchi dipartimento di elettronica informazione e bioingegneria politecnico di milano italy giovanni.quattrocchi polimi.it abstract the execution of spark applications is based on the execution order and parallelism of the different jobs given data and available resources.
spark reifies these dependencies in a graph that we refer to as the parallel execution plan of the application.
all the approaches that have studied the estimation of the execution times and the dynamic provisioning of resources for this kind of applications have always assumed that the execution plan is unique given the computing resources at hand.
this assumption is at least simplistic for applications that include conditional branches or loops and limits the precision of the prediction techniques.
this paper introduces seepep a novel technique based on symbolic execution and search based test generation that i automatically extracts the possible execution plans of a spark application along with dedicated launchers with properly synthesized data that can be used for profiling and ii tunes the allocation of resources at runtime based on the knowledge of the execution plans for which the path conditions hold.
the assessment we carried out shows that seepep can effectively complement dynaspark an extension of spark with dynamic resource provisioning capabilities to help predict the execution duration and the allocation of resources.
ccs concepts computer systems organization cloud computing software and its engineering massively parallel systems searchbased software engineering .
keywords apache spark symbolic execution search based test generation elastic resource provisioning acm reference format luciano baresi giovanni denaro and giovanni quattrocchi.
.
symbolic execution driven extraction of the parallel execution plans of spark applications.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
introduction big data applications executed onto large clusters of physical virtual machines are increasingly becoming central in industry and academia .
special purpose frameworks divide the computation into different phases and split the input dataset into partitions that are stored in a distributed way and processed in parallel.
apache spark hereafter spark probably the de facto standard for batch computations exploits in memory processing and storage as means to reuse partial results.
spark applications exploit two types of dedicated operations action s and transformation s. actions trigger distributed computations that return results to applications.
transformations carry out data transformations in parallel.
spark groups operations into stage s and then into jobs.
a stage comprises a sequence of transformations that do not require data shuffling while a job identifies a sequence of transformations between two actions.
for each job spark computes a parallel execution plan pep to maximize the parallelism while executing an application.
in fact a stage is by definition executed in parallel but different stages can also be executed concurrently.
for this reason spark materializes peps as direct acyclic graphs of stages1while the complete pepof an application is simply the sequence of the peps of its jobs.
the growing importance of big data applications has motivated diverse analysis techniques to estimate the execution time of spark applications and to allocate resources properly.
for example islam et al.
propose a solution to statically allocate resources to deadline constrained spark applications while minimizing execution costs.
sidhanta et al.
estimate the duration of spark applications using a closed form model based on the size of the input dataset and the size of available cluster.
alipourfard et.
al use bayesian optimization to generate performance models of spark applications and compute the best configuration for their execution.
baresi et al.
propose dynaspark an extension of spark that exploits control theory and containers2to scale allocated resources elastically given the execution times of interest and the other applications running on the same cluster.
all these approaches implicitly assume that the pepof an application is unique for any possible input dataset and input parameters but this is at least simplistic for all but the most trivial applications.
in general application code embeds branches and loops and different input data and parameter values may make the application traverse different program paths and thus materialize a different 1this is not the control flow graph of the application but the specific execution flow of the tasks that can be executed in parallel.
it is a graph and not a sequence because of the parallelism.
esec fse august tallinn estonia l. baresi g. denaro g. quattrocchi pep.
for example a spark application can evaluate partial results through actions and then use these results in conditional expressions of program branches.
this paper proposes the use of an original combination of lightweight symbolic execution and search based test generation to properly infer the actual pep given used data and parameters.
our approach is called seepep symbolic execution driven extraction of parallel execution plans and relies on symbolic execution of the application s code to derive a representative set of execution path conditions of the peps in the application.
it then uses these conditions with a search based test generation algorithm to compute sample input datasets to execute each pep.
these inputs allow us to profile the application with respect to when it uses each specific pep.
this way seepep identifies what we call the pep of applications that is the set of all possible peps along with their respective path conditions and profiling data.
the computed pep feeds dynaspark seepep a custom version of dynaspark to evaluate all feasible peps against the concrete values of symbolic variables and identify those that remain feasible i.e.
their path conditions continue to hold.
dynaspark seepep then tunes allocated resources on the worst feasible pep.
our preliminary evaluation shows that i seepep efficiently extracts all the possible peps that is all the possible different execution of an application and ii seepep helps dynaspark seepep improve dynaspark and allocates resources more efficiently.
the rest of the paper is organized as follows.
section recalls how spark works briefly introduces dynaspark and exemplifies the problem addressed in the paper.
section explains our symbolic execution of spark applications and section how dynaspark uses it.
section presents the experiments we carried out.
section surveys related approaches and section concludes the paper.
2dynaspark spark deploys a master worker architecture on a cluster of machines.
a spark application driver program starts by creating a context that interacts with the master node that is the manager of the actual computing resources called worker node s. each worker node is installed on a dedicated machine and contains one or more executor s that run for the entire lifetime of the application.
each executor performs multiple tasks in parallel using a pool of threads and the number of parallelized tasks depends on the number of cpu cores it has been provisioned with.
tasks can persist the results of their computations on a distributed storage layer e.g.
hdfs hosted on the same cluster or on dedicated machines.
when multiple applications are executed on the same cluster each application has its own context master and worker nodes are shared while executors must be assigned to the different applications before starting their execution.
spark operations manipulate rdd s resilient distributed datasets .
an rdd is an immutable and fault tolerant collection of record s that is split into multiple redundant partition s to facilitate parallel computations.
transformations create new rdds while actions generate values.
spark starts executing a program by identifying its jobs that is sequences of transformations delimited by the presence of actions in the code and stage s within jobs delimited by transformations that require data to be shuffled i.e.
movedamong executors .
these transformations are called wide while the others are called narrow .
for each job spark identifies the operations that are to be executed given the specific input data and parameter values and creates a pep parallel execution plan .
peps do not contain branches and loops they have already been unrolled and each peponly considers a specific execution.
a pepdefines the execution order among stages and the extent to which they can be executed in parallel.
note that a task performs all the operations of a particular stage on a partition of the input rdd of that stage.
these tasks are executed in parallel depending on the number of available executors and the number of cpu cores provisioned to them.
the different jobs of an applications are then executed following a lifo approach.
dynaspark3extends spark and uses containers i.e.
docker to support a more flexible and advanced management of cpu cores and meet set execution times deadlines .
spark requires that resources be provisioned statically if the execution completes faster than expected fewer resources could have been provisioned but if it takes longer more resources are needed.
dynaspark moves the allocation problem to run time and deals with these cases.
dynaspark augements the master node with a stage scheduler a task scheduler and a heuristic based application level controller for each running application.
stages are key for dynaspark the stage scheduler intercepts the beginning and end of each stage and uses the heuristic embedded in application level controller to compute an execution deadline for each stage.
the heuristic considers the remaining amount of time with respect to the global deadline set for the whole application and some performance data collected through a profiling phase.
this profiling is also in charge of providing the pep as said before dynaspark assumes that the pepdoes not change with respect to input data and this is the key reason that has motivated this work.
dynaspark constrains executors to specific stages while spark would allow executors to execute the tasks of different stages.
this way the resources dynamically provisioned to a given executor can only impact the performance of the stage associated with it anddynaspark can obtain a finer grained control of the execution of the different stages and thus of the whole application.
dynaspark controls stages individually and distributes computation and data over the whole set of nodes equally.
executors are wrapped in containers and individually controlled by a control theoretical planner container level controller .
these planners exploit a feedback loop that monitors the progress of the executors and allocates computing resources to match estimated deadlines.
resource contention on worker node s could occur because executors bound to different applications stages are deployed onto it.
this is why dynaspark uses a worker level controller to gather all the requests for cores from the control theoretical planners and if their sum is greater than those available scales them down according to different configurable strategies.
.
example applications most of the approaches in the literature e.g.
uses the execution graph i.e.
the pep to reason on the work to do the 247symbolic execution driven extraction of the parallel execution plans of spark applications esec fse august tallinn estonia from pyspark import sparkcontext def run numiterations threshold sc sparkcontext local example x sc.textfile ... .map ... .groupby ... .map ... .aggregate ... y sc.textfile ... .map ... .groupby ... .map ... .aggregate ... if x threshold and y threshold for i in range numiterations z sc.parallelize ... .map ... .sort ... .take if x y w sc.parallelize ... .map ... .filter ... .count a source code job aggregate 1textfilemapaggregategroupbystage 1maptextfilemapstage 0x threshold y threshold endjob take 1textfilesortstage 5parallelizemapstage 4noyesjob aggregate 2textfilemapaggregategroupbystage 3maptextfilemapstage x y job take numiterationstextfilesortstage m 1parallelizemapstage mnojob count 1textfilemapfilterparallelizestage stage m 2mapyes b the four peps of the application figure example spark application with conditional branches and loops.
degree of parallelism the duration of tasks and other applicationspecific characteristics.
they also assume that the graph does not change since many of the conditional branches and loops are mimicked in the code e.g.
filter map .
as said this is wrong when the code contains explicit loop and conditional statements.
for example one can think of a simple two job application.
the first job retrieves some records from a data source e.g.
a file and filters then according to a given criterion the second job sorts them and returns the first xrecords.
to avoid problems one may constrain the execution of the second job to the fact that the cardinality cof filtered records that is the result produced by the first job is greater than zero.
the pepwould then comprise two jobs i f c it would only comprise the first job otherwise.
this example shows how spark can return partial results c through actions to the driver program and use them to evaluate conditional loop expressions and thus produce different peps .
to overcome this problem dynaspark and many solutions exploit an initial profiling phase to retrieve the pepand collect some performance metrics.
back to the simple example above the initial profiling would return the pep implied by the data used to run the application.
this single choice impacts the quality of obtained results and there is currently no means to adjust the graph with respect to the different data.
even if one adopts a conservative approach and retrieves the pepthat corresponds to the worst case i.e.
two jobs in the previous example this would result in overallocating resources and or over estimating execution times.
if one adopted the best case one job too few resources and too short execution times would be foreseen.
the pepcan also depend on user parameters or local variables and they must be considered in a sound analysis.
for example figure 1a shows the code of an example application that takes two input parameters numiterations andthreshold itspepdepends on both these user parameters and input dataset.
the first two aggregate jobs are always executed line 4and6 and the results are assigned to variables xandy respectively.
line 8checks if both variables are greater than threshold .
if it is the case a take job line is repeated numiterations times forloop .
finally if x y line count line is executed.
this code corresponds to four possible peps figure 1b i the sequence of the two aggregate s if both conditional statementsare false ii the sequence of the two aggregate s and take repeated numiterations times if the first conditional statement is true and the second is not iii the sequence of the two aggregate s and count if the second conditional statement is true but not the first and iv the concatenation of the two aggregate s take repeated numiterations times and count if both conditional statements are true .
3seepep seepep symbolic execution driven extraction of parallel execution plans is an original combination of lightweight symbolic execution and search based test generation that allows us to extract thepep of spark applications.
a pep associates each controlflow path of the target application with the pepgenerated by its execution the relative profiling data and the path condition that activates the path.
seepep consists of four main phases i it relies on a lightweight symbolic execution of the driver program of the spark application to derive a representative set of execution conditions of the controlflow paths in the program ii it exploits those execution conditions with a search based test generation algorithm to compute sample input datasets that make each path execute iii it executes the target application with those datasets as input to profile the pep generated by each path and synthesize the pep accordingly iv it generates an artifact called guardevaluator that returns the feasible peps given a partial set of concrete values of the symbolic variables.
we exploit the information in the pep computed with seepep to extend dynaspark see section with the ability of tuning its adaptation strategy according to the worst case behaviour of the application.
at runtime our extended version of dynaspark exploits theguardevaluator to refine the control policy by recomputing the worst case estimation every time the current worst case refers to a path for which the execution condition stored in the pep becomes unsatisfiable.
below we describe each phase of seepep in detail.
.
lightweight symbolic execution seepep relies on a lightweight symbolic execution of the driver program of the spark application to identify the execution conditions of the feasible program paths of the driver program.
to this end seepep models with unconstrained symbolic values the results of 248esec fse august tallinn estonia l. baresi g. denaro g. quattrocchi se s l vv pc t l op l l vv pc ifop x e l vv pc c vv ifop assume c l vv pc ifop x parallel op .
.
.
e.g.
sparkctx.textfile .
.
.
data.map data.filter .
.
.
l vv pc ifop x aggregate op .
.
.
e.g.
data.count data.collect data.take n .
.
.
figure symbolic execution algorithm of seepep .
the parallel computation jobs issued in the driver program thus abstracting from the details of those computations and symbolically analyzes the dependencies of the program paths on these symbolically modeled results.
seepep leaves to the subsequent test generation phase the burden of identifying concrete input datasets that make the parallel computation jobs encompassed in the driver program yield results that satisfy the path constraints identified during symbolic execution.
this section formalizes the symbolic execution algorithm of seepep for a simple imperative programming language in which all operations are either assignments of program variables or assume operations.
the assignments are in the form x e where xis a program variable and eis an expression of values of program variables.
the assume operations are in the form assume c where cis a condition on the values of program variables with the semantics that the program continues to execute only if the condition cevaluates to true.
a program in this language defines a transition system with a finite set of program locations l l1 l2 .
.
.
ln a specified initial location linit l and a transition relation t t l op l that states the semantics of the program that can move from l l tol lby executing a valid assignment or assume operation op.
two special classes of assignments that is assignments of the form x parallel op .
.
.
andx aggregation op .
.
.
respectively define parallel computations.
the assignments x parallel op .
.
.
assign the variable xof the special type dataset to the result of the expression parallel op .
.
.
which in our context can refer to evaluating any of the parallel computation operations allowed in spark e.g.
map filter reducebykey .
the assignments x aggregation op .
.
.
assign the variable xto the result of a data aggregation operation e.g.
count collect etc evaluated against a dataset computed in parallel.
figure defines the symbolic execution algorithm of seepep .
we denote the symbolic states computed during the analysis with s l vv pc being lthe program location to which this symbolic state refers vvthe set of program variables assigned so far and pc the path condition the path constraint due to the assume operations traversed so far.
the algorithm starts from the initial state s init linit true no variable assigned unconstrained path and unfolds the transitions of each program path by recursively executing the atomic step s se s t of figure where s is the state reached from swhen executing transition t. figure defines the algorithm as a list of four cases.
the first two cases describe the classic symbolic execution algorithm that handles i the assignment operations x e by setting variable x to the value of expression ein the current state x e vv and ii the assume operations assume c by conjoining the current path condition with the value of condition cin the current state pc c vv .
the last two cases in figure define the abstract modeling of the assignments that involve parallel operations iii the assignments x parallel op .
.
.
result in setting the variable xto the unique symbolic value which we use to symbolically model every dataset accessed and computed in the program iv the assignments x aggregation op .
.
.
result in setting the variable xto a new unconstrained symbolic value lthat models the result of the aggregation operator called at program location l. for the sake of simplicity the figure omits the additional incremental index that we use to symbolically model the results of subsequent assignments at a location that is traversed multiple times in the same program path.
the right part of figure exemplifies a set of both parallel op andaggregation op operations.
these examples include the spark operations that appear used in this paper.
beyond these examples with reference to the rdd programming guide the parallel op operations of figure encompass the complete list of transformationandshuffle operations while the aggregation op operations encompass all action operations.
an important remark about the algorithm is that the conditions of the assume operations defined in the driver program cannot explicitly predicate on the internal state of variables of type dataset .
in fact although the variables of type dataset undergo parallel computations the data produced with these computations may propagate in the driver program only indirectly as the result of invoking some x aggregation op ... .
thus the assume operations in the driver program may predicate only on variables assigned as x e andx aggregation op ... .
this guarantees that the symbolic value that models the assignments x 249symbolic execution driven extraction of the parallel execution plans of spark applications esec fse august tallinn estonia parallel op ... never appears in a path condition.
this is the reason why we can adopt the simplification of using this single symbolic value to abstractly model all the datasets that the target driver program may manipulate.
seepep uses the algorithm of figure to symbolically analyze the paths of the target driver program and returns the path condition computed for each path.
as usual in symbolic execution we use a constraint solver to check if any path condition becomes unsatisfiable at some point of the analysis and dismiss the analysis of the program paths with unsatisfiable path conditions.
our current seepep prototype implements the algorithm described in this section on top of the symbolic executor jbse that relies on z3 as the constraint solver.
for example for the spark application in figure 1a when analyzing the paths of the driver program that do not enter the loop at line let 5and 7be the symbols that represent the results of theaggregate actions at line and line respectively and thresh anditers the symbols that represent the input values of parameters threshold andnumiterations respectively seepep computes the path conditions 5 thresh 5 7 5 thresh 5 7 5 thresh 7 thresh 5 7 5 thresh 7 thresh iters 5 7 5 thresh 7 thresh iters 5 7 while it identifies the unsatisfiable path condition 5 thresh 7 thresh 5 7. for programs with loops like the one in figure 1a seepep bounds the iterations of the loops to an user defined maximum value thus guaranteeing the analysis of a finite amount of paths.
.
search based test generation seepep exploits the path conditions identified with symbolic execution to generate test cases one test case for each path condition that comprise the input values and input datasets that make the target spark application concretely execute the paths of the driver program that correspond to the path conditions.
the goal is to use these test cases to profile the behavior of the pepgenerated by the execution of each path of the driver program.
note that we cannot rely on the classic approach of solving the path conditions to satisfy assignments by means of an smt solver.
our path conditions capture partial symbolic representations of the datasets since they do not characterize the functions passed to spark transformations.
this means that smt solvers would miss the proper constraints to produce valid datasets.
to generate a test case for a given path condition seepep incrementally explores the space of the possible test cases in a searchbased fashion steering the search with a fitness function that quantifies the extent to which each incrementally considered test case is close to or far from satisfying the path condition at hand.
below we first describe the seepep search algorithm in detail and then explain the test execution sandbox that the algorithm uses to speed up the execution of the test cases.
.
.
search algorithm.
the seepep search algorithm generates test cases that call the target application with the inputs both the input parameters and the input datasets assigned to concretevalues both concrete values of the parameters and datasets .
the algorithm samples the possible values of the inputs as genetic algorithms do.
it starts generating a population of test cases that comprise randomly selected inputs and then evolves from the initial population by incrementally generating a series of next generation populations each obtained by manipulating the test cases in the previous population by means of mutation andcrossover operators.
mutation operators generate new test cases by randomly modifying some inputs of a test case of the previous population.
crossover operators generate new test cases as the children of some pairs of the test cases of the previous population by conjoining inputs taken from either test case of the pair.
theseepep fitness function quantifies the goodness of each generated test case with respect to the goal of satisfying a previously identified path condition yielding a value that we interpret as the distance of the current test case from a satisfying test case.
if the fitness function yields a distance of the current test case is indeed a satisfying test case and the search algorithm returns it as result.
if the fitness function yields a value greater than zero the search algorithm exploits it to comparatively order the test cases of the current population.
the search algorithm proceeds by probabilistically favouring the application of mutation and crossover operators to test cases with lower distance from the goal thus increasing the chances of eventually converging to a satisfying test case.
in detail seepep computes the fitness of a test case with respect to a path condition as follows.
first it executes the test case and collects the results of the spark aggregation actions that the driver program executes thereby.
next it evaluates the path condition for the valuation of the symbolic values induced by the execution of the test case that is by assigning the symbolic values that model input parameters to the concrete values set in the test case and the symbolic values that model the results of aggregation actions the lsymbols of figure to the corresponding results collected while executing the test case.
if the test case does not execute an aggregation action referred to in the path condition we assign the corresponding symbol to the special value unde f .
then if there are no unde f symbols and if the path condition evaluates to true for the concrete assignment induced by the test case the fitness is zero indeed the test case satisfies the path condition.
otherwise the fitness becomes the positive value computed by the formula reported in the following where tis the test case andpcthe path condition f itness t pc c1 c2 cn n i 1distance t ci where ciare the atomic conditionals in the path condition pc and the function distance that appears in the summation recursively computes the distance of each atomic conditional from being satisfied.
in turn the function distance is defined as follows let c o1 o2be a conditional where is a comparison operator ando1 o2are operands either literals or symbolic expressions distance t c ift o1 t o2 true ift o1 undef t o2 undef t o1 t o2 otherwise 250esec fse august tallinn estonia l. baresi g. denaro g. quattrocchi where t o1 andt o2 are the values of operands o1ando2 respectively for the concrete assignments set in the test case t while is an arbitrary small number.
note that t o1 andt o2 are set tounde f if they depend on any symbol assigned to unde f after executing the test case.
since function distance always yields a value in the interval the overall f itness ranges in the interval for a path condition with natomic conditionals.
function distance yields zero first case in the formula for satisfied conditionals and thus the overall f itness is zero only for a test case that satisfies all conjuncts that is a satisfying test case as expected.
function distance yields the maximum value second case in the formula for conjuncts that refer to any symbol assigned to unde f and thus the overall f itness is never zero if it depends on any symbol assigned to unde f as expected.
function distance yields values increasingly closer to zero third case in the formula if the operands of the referred conditional evaluate to increasingly mutually closer values meaning that the corresponding test cases do not satisfy the conditionals for increasingly smaller amounts.
thus the closer to zero the overall f itness is the higher the number of satisfied or close to be satisfied conditionals is as expected.
.
.
test execution sandbox.
each fitness evaluation issued in the above search algorithm requires at least in principle the execution of the parallel application under test which can quickly become computationally infeasible given the many test cases that the algorithm generates during the search.
to address this issue the seepep search algorithm executes the test cases in a test execution sandbox that specializes the rdd typed datasets of the target spark application as a custom type of datasets that we call sparse diversity data sdd datasets .
an sdd dataset synthetically represents an rdd object with many data points that hold the same value.
an sdd dataset is modelled as a list of data blocks each with two attributes namely size andvalue a data block with size equal to sandvalue equal tovstands for a set of sdata points all with the same value v. seepep uses this format to model datasets in which the amount of distinct values is significantly much smaller than the overall amount of values in the dataset.
for example a dataset with data points in which half of the data points have value and the other half can be very concisely represented with an sdd dataset with two data blocks both with size equal to and value equal to and respectively.
the test execution sandbox recasts the computation of the parallel operations allowed on rdd objects e.g.
operations like map filter etc.
to sequential operations executed on the data blocks in the sdd objects.
for example a map transformation executed on an sdd dataset dwith data blocks yields a new sdd dataset d with data blocks b b ... b n such that for all i ..n b i.size bi.sizeandb i.value bi.value .
similarly a filter transformation on dyields d with the subset of the data blocks of dthat satisfy condition bi .
yet a count action on dyields value ibi.size as result.
our sdd objects handle all transformations and actions defined in the rdd programming guide .
the crossover and mutation operators of the seepep search algorithm manipulate the input sdd datasets of the target application bymodifying adding and removing data blocks mutation operators or combining the data blocks from the sdd datasets in the parent test cases crossover operator .
our current seepep prototype exploits the sushi test generation framework to implement the search algorithm described in this section.
sushi converts the path conditions generated with jbse in fitness functions as the ones described in this section and adapts the test genetic search procedure of evosuite to use these fitness functions.
for example given one of the aforementioned path conditions computed for the spark application of figure 1a seepep would compute a test case similar to the following one.
test threshold numiterations d1 new sdd size value d2 new sdd size value setinputtextfile ... d1 setinputtextfile ... d2 run numiterations threshold this test sets the input parameters threshold andnumiterations to concrete values lines builds two sdd datasets both with a single data block lines sets these datasets as the input files the application reads as input lines and executes the application with these inputs.
.
synthesis of the pep seepep uses the test cases generated with the search algorithm to execute the target spark application and profile the pepgenerated by the execution of each path of the driver program.
in this phase seepep replaces the sdd datasets that appear in the test cases yielded by the search algorithm with proper rdd datasets that mimic the blocks in the sdd but comprise millions of replicas of each block.
as a result these rdds satisfy the same path conditions as the corresponding sdds and thus exercise the corresponding peps while producing execution times that are representative approximations of real datasets.
seepep executes the test cases against the target application in parallel.
while executing each test case it stores the pep that the spark engine produces before starting the execution of each job and monitors the parallel execution of the jobs to collects the timing data that are relevant for the control policy.
seepep builds the pep model as a set of triples pc peps times where each triple represents the sequence of peps and the timing data times associated with the execution of the test case that corresponds to the path condition pc.
together with the pep seepep produces a guardevaluator that takes as input a partial set of concrete values of the symbolic variables evaluates the path conditions of the triples in the pep against these values identifies which path conditions evaluate to f alse for these values and returns the subset of the pep with only the triples with non falsified path conditions.
the control policy described in the next section invokes the guardevaluator at runtime and feeds it with the concrete values of the input parameters and incrementally with the concrete values of the executed actions to keep 251symbolic execution driven extraction of the parallel execution plans of spark applications esec fse august tallinn estonia master noderdd apicountcollecttakesparksubmit appapplication level controllerstage schedulertask schedulerappdriver programstart with args symbolstore store valuesave argspep storeguardevaluatorstoreseepeplauncher1launcher4guardevaluatoruse runtime123 pepselectorworstcaseselectornotifyresultretrieve set pepbecretrieve valid peps memorycontrolleradstored stored pep figure dynaspark seepep the still reachable program paths updated at every intermediate execution state.
seepep also addresses the possible incompleteness of either its symbolic execution or search phase.
as we already commented above in the symbolic execution phase seepep analyzes the loops in the program up to a finite user configured amount of iterations and the analysis may thus produce incomplete results if we dismiss some program path if any that iterates a loop more times than that amount.
in this case seepep tracks the path conditions pcthat corresponds to the interrupted prefix of the non analyzed paths and stores these path conditions in the pep as special triples with missing data pc .
similarly if the search algorithm fails to converge to the optimal solution for some path condition pc seepep stores the corresponding triples with missing data pc .
these special triples in the pep allow the control policy described in the next section to anticipate when an un profiled path is going to be executed at runtime and act to mitigate the impact of these unforeseen situations.
dynaspark seepep this section describes how seepep integrates with dynaspark the resulting tool chain is called dynaspark seepep .
figure shows the main elements of the tool chain and exemplifies it on profiling and controlling the example application of figure 1a app hereafter.
as first step seepep generates the n in general launchers which activate the four n peps of the program and a guardevaluator for these peps .
the toolchain associates each pepwith its path condition uses the generated launchers to obtain the profiling data of each pep and synthesizes the pep for the application.
it stores the pep and the guardevaluator in the master node in components pep store andguardevaluator store respectively.
the guardevaluator implements a common interface to be dynamically instantiated and used without importing it statically.after this phase dynaspark seepep can execute app.
during the execution dynaspark seepep traces the concrete values that correspond to the symbols referred to in the path conditions that is the concrete values of the application input parameters and the results of the actions executed at runtime.
specifically component sparksubmit stores the values of the application parameters in component symbolstore and our modified version of the rdd api notifies the result of any action is executed to component pepselector which stores these results in symbolstore .
pepselector is in charge of selecting the pep and its profiling data then used by application level controller to compute the local deadlines for the next stages and thus oversee the provisioning of resources.
upon storing new values in the symbolstore the pepselectorretrieves the guardevaluator and feeds it with the symbolstore to retrieve the list of peps whose path conditions still hold.
this operation has negligible overhead since it requires only to evaluate the path conditions with symbols assigned to the corresponding concrete values in symbolstore without checking any rdd.
for instance at the beginning of the execution of function run ofapp four peps are valid since neither xnoryhave been resolved to a value.
the job at line 4produces the value of xand if the value is less than or equal to threshold the if statement of line is not evaluated.
therefore even if the value of yis still unknown guardevaluator only returns two peps that is the two whose path conditions still hold it excludes all the path conditions that depends on the expression x threshold .
this way since the selected pep is constantly updated dynaspark seepep becomes aware of of the amount of work still to be completed and can use this information to refine resource provisioning.
note that pepselector receives all the feasible peps and computes the next pepto use.
the selection mechanism can be customized.
currently we always select the worst case pep that is the pep with the greatest number of remaining stages to be conservative and minimize deadline violations.
if one wanted to optimize different performance indicators e.g.
deadlines are not strict and used resources must be minimized the selection might privilege a pep that corresponds to an average case instead of the worst one.
evaluation we evaluated seepep dynaspark seepep to control the parallel execution of two example spark applications.
our evaluation addressed two main research questions rq 1candynaspark seepep effectively control the execution times of spark applications?
rq 2to what extend can dynaspark seepep improve the resource allocation capabilities of dynaspark given it used a single constant pep?
below we introduce the applications we used as experimental subjects describe the setting of our experiments and discuss the obtained results.
.
subjects the subjects of our experiments are the applications called promocalls andlouvain .
252esec fse august tallinn estonia l. baresi g. denaro g. quattrocchi table promocalls paths.
path f j s y y y y y y y y 8table louvainpaths.
path f j s y y y y n n y y promocalls is a paradigmatic example that was developed at our lab4.
it mimics a batch application of a telecommunication company that wants to offer promotional discounts based on the amount of long longer than a parametric threshold local and long distance calls that customers make per day.
if a customer makes more than minllong local lcalls or more than minalong long distance calls or both in a day in the last mmonths or in the current month s he may receive discounts.
s he may get some or all the discounts according to the possible combinations of the two triggering conditions.
promocalls exploits spark to efficiently analyze the data of all calls and compute applicable discounts.
we used promocalls to guide the development of seepep and to initially evaluate the precision of the technique.
to evaluate our approach on a real world application we selected louvain a spark implementation of the louvain algorithm that we downloaded from a highly rated github repository5.
louvain exploits graphx a spark module dedicated to graph processing to represent large networks of users and analyze communities in these networks.
.
experimental setting for each subject application our experiments include three steps.
first we ran seepep to retrieve the path conditions and generate the launchers for each corresponding path.
next we profiled the subject application with the generated launchers and retrieved the peps of each path.
finally we used dynaspark seepep to control the execution of the application when it is fed with input datasets of an order of magnitude larger than the ones used in the launchers.
we generated at least one large dataset for each profiled path.
we also set the goal of guaranteeing a reasonable deadline which we defined as longer than the minimum feasible deadline measured on the datasets used in these experiments and spark configured to use all the resources available in the cluster.
to compare dynaspark seepep against the original version of dynaspark for each considered application we identified the best and the worst cases and the paths with the lowest and the highest number of stages6.
this way we quantify the error that dynaspark can make being unaware of which pepwas used in the profiling phase.
we executed the applications on microsoft azure using machines of type standard d14 v2 vms with cpus gb 6in case of two paths with the same number of stages we used the one with the shortest longest execution time.of memory and gb of local ssd storage.
this kind of vm is optimized for memory usage with a high memory to core ratio.
each machine ran canonical ubuntu server .
.
lts oracle java apache hadoop .
.
apache spark .
.
and dynaspark .
we dedicated 5vms to hdfs to store the input datasets and 5to dynaspark 5fordynaspark and5fordynaspark seepep of which 1for the master and 4for the workers.
the datasets were generated randomly by using seepep .
.
results tables and show the results produced by dynaspark seepep for promocalls and louvain respectively up to the profiling phase.
column path enumerates the paths that seepep identified in both cases it identified 8unique paths.
column f found indicates whether or not yorn respectively seepep succeeded in generating a test case and thus a corresponding profiling launcher for the identified paths it successfully generated a test case for each path of promocalls and for out of paths of louvain.7the test case generation took 30minutes for louvain and less than 10minutes for promocalls.
it incurred low storage overhead since input datasets are concisely represented as sdds that is small linked lists of data blocks.
then for profiling we rendered each sdd with an rdd with million replicas of each data block thus we used rdds whose size ranges between millions and millions of data records.
column j jobs and column s sta es report the number of jobs and stages we collected when profiling the launchers with dynaspark .
we got between and jobs and and stages for promocalls and and jobs and and stages for louvain respectively.
these data are not available for the two paths of louvain for which seepep did not generate a launcher.
in both tables we marked with and the paths that correspond to the best and worst case respectively of each application.
tables and summarize the results of our experiments about the effectiveness of dynaspark seepep to control the execution of the subject applications when these applications execute with large datasets as inputs.
furthermore the tables include the results produced by dynaspark tuned on the worst and best case datasets above to compare dynaspark seepep against dynaspark .
the different columns are explained below.
column experiment indicates for each profiled path pi the data computed with dynaspark seepep dynaspark s and dynaspark tuned on the worst case dataset dynaspark w and on the best case dataset dynaspark b respectively.
column dlshows the imposed deadline in seconds.
column etreports the actual execution time of the application in seconds this is the average of 5repetitions of the experiments for a total of 120executions of promocalls paths repetitions techniques and 90executions of louvain.
column v violation indicates whether the deadline was violated i.e.
et dl .
column quantifies the error defined as dl et dl 7we manually inspected the two paths of louvain for which seepep did not identify a corresponding test case.
however due to the complexity of the code we were not able to either prove that these paths are infeasible or devise input datasets that exercise these paths.
thus we currently have no evidence of whether louvain might indeed execute these program paths.
253symbolic execution driven extraction of the parallel execution plans of spark applications esec fse august tallinn estonia table results for promocalls experiment dl et v ca c s pen dynaspark s .
.
n .
.
p0dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s .
.
n .
.
p1dynaspark w .
.
n .
.
.
dynaspark b .
.
n .
.
.
dynaspark s107.
.
n .
.
p2dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s .
.
n .
.
p3dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s147.
.
n .
.
p4dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s .
.
n .
.
p5dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s122.
.
n .
.
p6dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s112.
.
n .
.
p7dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
that is the percentage ratio between the distance between the actual execution time and the deadline and the deadline itself.
in general in case of non violations the smaller is the more efficient the resource allocation is since fewer resources can be used to accomplish the goal.
on the other hand in case of violations the smaller the error is the shorter the delay is.
note that if the deadline were considered strict the penalty for a violation would be considered to be infinite more on that later .
column ca reports the average core allocation during execution defined as ca et s 0coresallocatedatsecond s et note that the maximum value of cais64core second since 64were the cores available in the cluster used for these experiments.
finally column penquantifies how dynaspark sperformed compared to the experiments with dynaspark pen penalty is defined as pen caw orst best caseepep caseepep ifv n ifv y since dynaspark snever violated the deadlines penmeasures how many resources were used by dynaspark w bwith respect to dynaspark s. for example if penis equal to it means that dynaspark w bused a quantity of resources that was greater than the ones used by dynaspark s. in contrast a negative penimplies that dynaspark w bused fewer resources than dynaspark s. finally if the deadline is violated by dynaspark w bwe considered an infinite penalty .table results for louvain experiment dl et v ca c s pen dynaspark s184.
.
n .
.
p0dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s228.
.
n .
.
p1dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s292.
.
n .
.
p2dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s228.
.
n .
.
p3dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s163.
.
n .
.
p6dynaspark w .
.
n .
.
.
dynaspark b .
.
y .
.
dynaspark s156.
.
n .
.
p7dynaspark w .
.
n .
.
.
dynaspark b .
.
n .
.
.
the data in tables and indicate that dynaspark bviolates the deadline in 7cases out of the 8paths in the experiments with promocalls and out of paths in the experiments with louvain.
this is due to the mistakenly optimistic estimations made in the profiling phase.
for example if we consider promocalls dynaspark b computes the local deadlines and the resource allocation as if the pep always consisted of 3stages.
this means that in all the experiments but p1 that truly corresponds to the best case path dynaspark under allocated the resources and the resulting execution time eventually exceeded the deadline by .
.
the maximum error .
was measured when executing p6 the worst case path where dynaspark experiences the largest mismatch between the estimations computed during profiling and the actual work to do at runtime.
dynaspark wdoes not violate any deadline conversely it causes the earlier termination of the applications in most of the cases with an error between .
and .
in the case of promocalls and between .
and in the case of louvain.
the earlier terminations are due to the profiling based pessimistic estimations that mistakenly assume the worst case path of the applications as representative of all the possible paths.
in particular when dealing with paths p1 p4 p5 and p7of promocalls and p0 p3 and p7 of louvain the error is greater than leading to significantly sub optimal resource allocations.
in contrast dynaspark sdoes not violate any deadline and successfully provides an efficient resource allocation.
the error measured in our experiments is on average equal to .
for promocalls where dynaspark banddynaspark wmake an average error of .
and8.
respectively and equal to .
for louvain where dynaspark banddynaspark wmake an average error of .
and8.
respectively.
the data in columns cafurther witness thatdynaspark soutperforms the performance of dynaspark band dynaspark w.dynaspark bunderestimates allocated resources so as 254esec fse august tallinn estonia l. baresi g. denaro g. quattrocchi to make dynaspark violate the deadlines in all experiments but path p1in promocalls and path p7in louvain the best cases .
in this two cases profiled data match what happens at runtime and therefore dynaspark boutperforms both dynaspark wanddynaspark sand minimizes the error and used resources.
compared to dynaspark w dynaspark sallocates on average .
fewer resources with promocalls and .
with louvain.
to summarize our experiments suggest a positive answer to both research questions rq1andrq2since dynaspark seepep effectively and precisely controls the allocation of resources to execute promocalls and louvain and keeps the execution within considered deadlines with significantly smaller errors and fewer resources than the original version of dynaspark .
.
threats to validity we conducted a total of 226experiments using two different applications a paradigmatic example and a real world application taken from github.
we showed that seepep was able to find a test case for out of the application paths statically identified with symbolic execution and how a tool such as dynaspark could benefit from the integration with seepep .
in this section we highlight the threats that may constrain the validity of our current results .
.
internal threats.
to run the experiments we slightly modified the test cases generated by seepep to increase the size of the datasets without breaking the path conditions .
this way we are sure to test the desired paths and reliably obtain different repetitions of the experiments.
we also ran some initial experiments with datasets created randomly and we obtained similar results to those presented a more complete evaluation with random data will be conducted in the future.
.
.
external threats.
the main limit to the generalization of our results refers to considered subjects.
we used two spark applications one uses core transformations of spark and one uses graphx to analyze graphs.
in the future we plan to increase the number and the types of spark applications including those that implement machine learning solutions and use sql.
another limitation of the current approach lies in the profiling phase.
currently we need a profiling run for each test case launcher found by seepep but this could be an issue if the number of possible paths becomes high.
in the future we plan to improve this part of the tool chain by adopting a branch based criterion for selecting profiling executions instead of a path based one since executing all program branches guarantees that any possible pep of the program be profiled.
this optimization might also mitigate the issues with paths that our test generator cannot cover as the two uncovered paths we experienced with louvain.
related work our work touches both the problem of managing big data applications at design run time and the use of symbolic execution for analyzing parallel algorithms.
spark natively provides means for monitoring application and analyze the response time at each stage.
spark is however not aware of the program control flow graph and thus only provides a rudimentary facility to adjust allocated resources at runtime .some research efforts focus on monitoring big data applications with the assumption that their pep is constant over different executions.
others concentrate on predicting the response time by using optimization techniques formal approaches and machine learning solutions .
in general they compute a static resource allocation plan to avoid deadline violations based on the execution graph the size of the dataset and the nominal performance measured during profiling.
these techniques do not aim to address any dynamic elastic provisioning of resources as our approach does.
rather they compute and use a fixed resource allocation thus failing to account for possible runtime deviations.
address the self adaptive resource allocation for mapreduce applications to meet deadlines by modifying the scheduler of hadoop.
compared to our work all these approaches are unaware of the control flow graph of the application and thus can achieve precise predictions only when the execution graph is truly unique that is only for simple applications.
some approaches use formal analysis including symbolic execution to verify and improve parallel applications .
in particular siegel et al.
use symbolic execution to analyze the equivalence between a parallel program and a corresponding sequential program assumed to be available as specification of the parallel version.
siegel and zirkel verify assertions in messagepassing parallel programs with unbounded loops by means of symbolic execution with novel loop invariants for multi thread programs.
raychev et al.
rely on symbolic execution to parallelize the computation of user defined aggregations in mapreduce platforms.
none of these solutions deals with spark applications.
to the best of our knowledge this paper is the first that combines symbolic execution and search based testing to automatically generate test cases that exercise the control flow paths of a parallel program.
conclusions and future work this paper presents seepep and dynaspark seepep to use symbolic execution and search based test generation to compute all the parallel execution paths that are actually embedded in complex spark applications.
dynaspark seepep misses fewer deadlines and allocate resources more efficiently than dynaspark .
as for future work we plan to optimize the profiling phase by using a branchbased selection criterion instead of the simple path based solution adopted in this paper.
acknowledgment we thank davide bertolotti for his important contribution to the implementation of this project.
this work has been partially supported by the gauss national research project miur prin contract 2015kwremx .