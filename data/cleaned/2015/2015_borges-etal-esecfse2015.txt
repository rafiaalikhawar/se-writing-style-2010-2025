iterative distribution aware sampling for probabilistic symbolic execution mateus borges antonio filieri marcelo d amorim corina s. p as areanu univ.
of stuttgart germany fed.
univ.
of pernambuco brazil cmu sv nasa ames research center usa abstract probabilistic symbolic execution aims at quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions.
the technique collects constraints on the inputs that lead to the target events and analyzes them to quantify how likely it is for an input to satisfy the constraints.
current techniques either handle only linear constraints or only support continuous distributions using a discretization of the input domain leading to imprecise and costly results.
we propose an iterative distribution aware sampling approach to support probabilistic symbolic execution for arbitrarily complex mathematical constraints and continuous input distributions.
we follow a compositional approach where the symbolic constraints are decomposed into sub problems whose solution can be solved independently.
at each iteration the convergence rate of the computation is increased by automatically refocusing the analysis on estimating the sub problems that mostly affect the accuracy of the results as guided by three different ranking strategies.
experiments on publicly available benchmarks show that the proposed technique improves on previous approaches in terms of scalability and accuracy of the results.
categories and subject descriptors d. .
software program verification keywords symbolic execution monte carlo sampling probabilistic analysis .
introduction probabilistic symbolic execution is a promising technique for quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions .
the input distributions allow data from real world observations to be incorporated in the analysis of programs that interact with their environment as well as to encode uncertainty in design assumptions about the usage profile of a program.
the technique has many potential applications e.g.
it can be used in debugging for ranking program errors in analyzing the controlsoftware of autonomous vehicles that interact with uncertain environments for computing software reliability and for quantitative information flow analysis to name a few.
the technique uses a symbolic execution of the program to collect symbolic constraints on the inputs that lead to the occurrence of target program events.
the constraints are then analyzed to estimate their solution spaces and to quantify how likely it is for an input distributed according to a given probabilistic distribution to satisfy the constraints.
however the current techniques are quite limited as they can either handle only linear constraints or if they handle complex mathematical constraints they only use uniform input distributions while more complex distributions are handled using discretization techniques which may be imprecise and costly in practice.
to achieve higher accuracy and scalability for the analysis of non linear constraints under continuous input distributions we propose an iterative distribution aware statistical analysis method.
our method is compositional and builds upon qcoral to decompose the analysis of program constraints into sub problems that can be solved separately.
we improve upon qcoral by providing an iterative technique that re focuses the sampling effort on the constraints that have higher impact on the accuracy of the probabilistic analysis results.
further the method samples the inputs according to the distributions defined in the input profiles avoiding the cost of discretizing continuous distributions as was needed with qcoral .
statistical methods have proved to be effective in solving integration problems such as those arising from probabilistic analysis.
especially when the number of variables grows statistical methods outperform symbolic and numerical ones .
nonetheless general statistical integration methods available off the shelf are not capable of exploiting all the information about a program behavior obtained through symbolic execution e.g.
certain constraints have higher impact in driving the program execution toward the occurrence of a target event.
similarly certain constraints have higher impact on the convergence rate of statistical estimation since they provide more information about the possibility for the target event to occur.
based on this insight we analyze the path conditions leading to the occurrence of the target event to rank the constraints according to their impact on the convergence of the statistical analysis.
exploiting this ranking our method iteratively re focuses the sampling to gather more information about the satisfaction of the most important constraints first achieving higher estimation accuracy in a shorter time.
we propose three ranking strategies two of them are based on gradient descent optimization while the third one uses a simple but efficient heuristic which gives more importance to the constraints whose probability estimates are farther from convergence.
we provide a formal assessment of our technique study its convergence and evaluate it experimentally on publicly available benchpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
esec fse august september bergamo italy acm.
... .
866goal new uniform flapposition new uniform windeffect new normal .
weak wind windeffect new normal .
strong wind actuatoreffect max position min position if goal actuator flapposition flapposition actuatoreffect windeffect else flapposition flapposition actuatoreffect windeffect if flapposition max position safety check flapposition min position throw new overrunexception figure flap controller.
marks from including real world software taken from the medicine and the aero space domains.
our experimental results show significant improvement over previous discretization based approaches and built in routines of general purpose mathematical tools both in terms of accuracy of results and analysis time.
.
example to illustrate our analysis we introduce a small code snippet modeling a safety check for a simplified flap controller of an aircraft modified from see figure .
the controller is composed of a flap actuator and a safety check to avoid overrun of the flap.
the variables influencing the behavior of the flap are the goal position the current position of the flap and the wind effect.
the actuator performs a move towards the goal but the actuation can be hindered by the effect of the wind that can lead to an overrun of the flap.
the goal position can vary in the range while the current position of the flap when the next control step is activated is assumed to be within .
for both these variables any value in the domain is considered equally likely i.e.
the concrete inputs are assumed as realizations of a uniform distribution over each variable domain.
the wind effect is instead assumed to behave as a normal distribution with mean and a standard deviation which depends on the strength of the wind.
we will consider two different application scenarios in case of weak wind the standard deviation is assumed to be meaning that the effect is most of the time quite close to in case of strong wind the standard deviation is assumed to be so larger values of the the wind effect are more likely.
in either case the effect of the wind is bounded by the interval .
this simplified model exemplifies how the uncertainty about this physical phenomena can be taken into account for the analysis.
in figure the random distributions for the input variables are characterized by the value of their parameters and the lower and upper bound of the domain which are always the last two arguments .
the uniform distribution does not need additional parameters besides the domain.
the normal distribution is characterized by its mean and its standard deviation respectively besides the domain.
the probabilistic distribution of the wind effect can be obtained systematically from telemetry mission data.
it is possible for example to measure the frequency over time of values occurring within certain ranges during a mission to obtain realistic usage profiles.
techniques for the automatic inference of probabilistic profiles are described elsewhere see e.g.
.
analyzing the example program.
probabilistic symbolic execution computes the probability of a certain event to occur or not during the execution of the program given a usage profile.
the re sult of this kind of analysis is not a boolean indicating presence or absence of an error but a quantitative figure whose value depends on both the program and its usage.
in this example the probability of throwing an overrunexception is only in the presence of weak wind but it grows up to when the wind is strong.
this paper describes a technique for the automatic estimation of the probability for a target event to occur given input profiles described by continuous probability distributions over floating point input domains enhanced by a set of strategies to speed up the convergence rate of the estimation.
.
background .
probabilistic symbolic execution probabilistic symbolic execution quantifies the probability of the software satisfying a given safety property .
this approach is comprised of two stages symbolic execution and solution space quantification.
tools to support the first stage are presented in e.g.
this paper focuses on the second stage solution space quantification.
we provide here a quantification procedure that can be easily incorporated in the above tools.
symbolic execution is a program analysis technique that executes programs on symbolic rather than concrete inputs and computes the values of program variables as symbolic expressions in terms of the inputs.
symbolic execution abstracts all possible program executions into a symbolic execution tree where the nodes are the symbolic program states and the edges are the program instructions.
each symbolic execution path is uniquely identified by a path condition pc i.e.
a constraint that the program inputs have to satisfy for driving the execution along the corresponding path.
to avoid the problem of non terminating executions the approach bounds the symbolic execution tree.
the paths hitting the execution bound are specifically marked and provide an index of the dependability of the analysis .
let pctbe the set of pcs identifying executions satisfying the target property similarly pcfdenotes the set of pcs for the paths that lead to property violation.
for example for the code in figure the set of path conditions pcfleading to the occurrence of an overrunexception are goal flapposition actuatoreffect windeffect goal flapposition actuatoreffect windeffect goal flapposition actuatoreffect windeffect goal flapposition actuatoreffect windeffect the goal of the second stage solution space quantification is to compute the expected probability that the program inputs satisfy any of the pcs in pct given a probabilistic distribution over the input domain.
formally z d1pct x p x where dis the input domain defined as the cartesian product of the domains of the input variables p x is the probability of an input xto occur according to the probability distribution over the inputs and 1pct x is the indicator function on pct that returns when xsatisfies any of the pcs in pct and 0otherwise .
the probability of failure can be computed similarly.
.
monte carlo estimation exact solutions to equation are not always possible because the integral might be ill conditioned by the presence of complex non linear constraints or because of scalability issues to deal with high dimension problems .
monte carlo methods provide a general approximate solution to the integration problem.
the basic solution called hit or miss hm mc consists in generating 867nrandom inputs over the input domain drawn according to the specified distributions and to count the number of hits i.e.
the inputs satisfying any of the constraints in pct and conversely the number of misses.
the ratio xbetween the number of hits and nis an efficient unbiased and consistent estimator of the probability of satisfying the constraints .
the mean and the variance of xare e x x var x x x n where x n1pct xi nis the sample mean.
the accuracy of the estimate xcan be assessed through the variance of x the closer the variance is to 0the more accurate is the estimation.
as evident from equation variance decreases with the number of samples going asymptotically to when n!
.
stratified sampling and interval constraint propagation icp .
the slow convergence rate of monte carlo hit or miss methods can be improved by using stratified sampling which partitions the input domain into regions strata which can be analyzed separately using for instance hit or miss monte carlo.
the local result xi associated to each region riis combined to obtain an estimator x over the global input domain with the following properties e x iwi e xi var x iw2 i var xi where wiis defined as wi size ri size d i.e.
the ratio between the size of the strata riand the entire domain d. the use of stratified sampling never increases the variance of the global estimator and in the worst case it is equivalent to non stratified sampling.
in previous work we proposed the use of an interval constraint propagation solver realpaver to prune out regions of the solution space that do not contain any solution of pct consequently improving convergence even further.
realpaver produces as output a set of boxes whose union reliably contains all the solutions of a given complex non linear constraint.
stratified sampling is then used for composing the results of analyzing the boxes.
.
compositional quantification in probabilistic symbolic execution the number of constraints to analyze can be very large leading to potentially high analysis cost.
to address this issue we follow the compositional approach from that splits the analysis into smaller sub problems based on the structure of the constraints composing pct orpcf allowing also the reuse of partial results.
pctcontains all the pcs leading to property satisfaction.
the goal is thus to quantify the probability of satisfying the disjunctionw pc2pctpc.
note that all pcs are disjoint by construction.
.
.
handling disjunction assume that we estimated the probability of satisfying two path conditions pci pcj2pct through the estimators xiand xj respectively and for these estimators we know mean and variance.
let us denote by xthe estimator of the disjunction pci pcjthat we want to compute.
we can use the following composition rule for disjunction to compute the mean and an upper bound for the variance of x. e x e xi e xj var x var xi var xj .
.
handling conjunction each path condition pciis a conjunction of simpler constraints ci0 ci1 cim.
this set of conjoined constraints can be partitioned in independent subsets each subset including the constraintswhich are related through variable dependency.
intuitively two variables iandjdepend on one another if there exists a constraint predicating on both of them e.g.
vi vj or if they both depend on a third variable.
this dependency relation is symmetric and transitive by construction and is extended with reflexivity i.e.
vi always depends on itself becoming an equivalence relation which induces a partition fv0 v1 vngon the set of variables v. for each constraint cikofpci a cik v indicates that cikpredicates on variable v. let ci jdenote the set of constraints occurring in pci and predicating on any of the variables in vj i.e.
ci j fcikj a cik v v2vjg .
let xi jand xikbe the estimators of ci jandcik respectively.
then we can use the following composition rule for conjunction to compute the estimator xofci j cik e x e xi j e xik var x e xi j var xik e xik var xi j var xi j var xik the rule extends naturally to many constraints.
note that some constraints may occur in multiple pcs we can thus cache and re use the analysis results for efficiency.
for example analyzing the pc extracted in section .
for the example from figure the following six independent constraints can be identified notice that being actuatoreffect a constant symbolic execution simplifies it c 0 goal c 1 goal c 2 flapposition windeffect c 3 flapposition windeffect c 4 flapposition windeffect c 5 flapposition windeffect each of these constraints can be analyzed independently using the monte carlo techniques described in section .
and their estimators can be combined according to the structure of the the pcs using the composition rules of equations and .
.
distribution a ware sampling in this section we extend the compositional sampling approach from with an efficient way to handle inputs that follow continuous distributions.
specifically we enhance the monte carlo estimation with a distribution aware sampling strategy where the random samples can be generated according to a continuous distribution instead of a uniform one.
however the use of interval constraint propagation i.e.
realpaver and stratified sampling requires some care since we need to restrict the sampling to specific sub regions of the input domains.
we assume that each input variable iis defined over a bounded continuous interval and its values are distributed according to a known distribution distribution i qi restricted or truncated to this interval.
qiis a possibly empty constant vector of known parameters characterizing the distribution e.g.
a normal distribution is characterized by the values of its mean and its variance while an exponential distribution only by its mean.
we will use the notation distribution i qi ja bto represent the distribution truncated to the interval .
the mapping between input variables and their corresponding truncated probability distributions constitutes the usage profile see the variable declaration in figure .
as mentioned in section the output of icp is a set of boxes containing all the inputs satisfying pct.
each box is defined by the conjunction of constraints of the form vi2 where each variable is restricted to a particular interval within its domain.
since in general the range of distribution i qi may fall outside we need to restrict the sampling to .
a simple approach would be to generate for each variable via set of samples x1 i x2 i xn i drawn according to the distribution and then prune out all the xj i .
however this may be inefficient especially if the intersection between the range of distribution i qi and is small.
we propose an efficient solution obtained by exploiting the results of probability theory for truncated distributions .
consider a random variable viwith distribution i qi .
each distribution is associated to a unique known cumulative distribution function cdf i t defined as cdf i t pr vi t which we will use for the sampling.
consider also a non empty interval .
let the random variable rvibe the restriction of vito the interval then the following result holds cdf rvi t cdf i max min t bi ai cdf i ai cdf i bi cdf i ai furthermore the cumulative distribution has inverse cdf rvi u cdf i cdf i ai u cdf i bi cdf i ai where u .
for the most common continuous distributions both cdf and its inverse cdf can be computed efficiently using off the shelf tools or libraries e.g.
.
in the following we show how to use these functions to obtain samples of any truncated distributions from samples drawn from uniform distributions which can be easily obtained from many existing off the shelf libraries.
example.
recall from figure that variable goal follows an uniform distribution in the interval .
according to the definition above cdf goal t t for t fort and 1fort .
the reader can refer to for the definition of cdf for the most popular continuous distributions.
the ability to compute cdf rvi and its inverse cdf 1rvi allows us to implement a general sampling strategy for continuous distributions restricted to intervals of interest.
indeed to take a sample from rvi i.e.
variable xirestricted to it is sufficient to generate a sample ufrom a uniform distribution over by using any robust pseudo random generator and use this sample u to generate the sample rvi cdf 1rvi u from the restricted random variable rvi.
this approach allows to bring distribution awareness to icp enabled stratified sampling thus allowing to achieve both the precision and scalability of distribution aware sampling and the improved convergence rate due to stratified sampling.
.
distribution aware sampling versus discretization the analysis from assume that the probability distributions over the input domain are specified by a usage profile up defined as up c1 p1 c2 p2 where the ciare a partition of the input variables i.e.
ici dand ci cj6 i j and ipi we abuse the notation here by referring with cito both a set of inputs and the constraints uniquely characterizing such set .
each pair ci piis called usage scenario .
this formalism for ups allows to arbitrarily partition the input domain into a finite set of regions each with an assigned probability.
the constraints cican be arbitrarily complex making the formalism expressive enough to predicate about non trivial relations among input variables.
however if the values of an input are distributed according to a continuous probability distribution casting this case into a finite up requires a discretization procedure partitioning the domain of each variable into a finite number of intervals andassigning to each interval a probability computed from the original continuous distribution.
depending on the number and the size of the intervals discretization may be an arbitrarily precise approximation of the continuous distribution.
nonetheless the unavoidable loss of precision due to discretization may introduce a bias in the analysis results when the approximation is not fine enough.
on the other hand a finer discretization requires to partition variables domain into a larger number of intervals.
assuming vinput variables are partitioned into mintervals each one the total number of constraints to obtain a discretized version of the original up would be vm.
though the complexity of the analysis is linear in the number of usage scenarios the latter grows exponentially with the required precision of discretization limiting the scalability of the analysis.
section .
compares the accuracy and performance of our distribution aware sampling procedure with the same analysis based on the discretization of the usage profiles showing its advantages.
.
iterative optimal sampling the divide and conquer procedure reported in section .
solves the problem of quantifying the solution space of pctin terms of simpler independent sub problems.
it further caches and reuses the results for the sub problems to speed up the quantification.
the estimates for the sub problems are composed according to the disjunction and conjunction rules from equations and respectively.
the variance of such estimators is composed as well providing an index of the accuracy of the final results.
although the asymptotic convergence of the compositional estimators is guaranteed i.e.
when the number of samples used to obtain the local estimators for each sub problem grows to the convergence rateof the procedure is hard to quantify and may be slow in practice.
this section introduces an iterative sampling approach to speedup the convergence rate of the quantification procedure.
at each iteration the sampling is focused on the parts of the input space that are likely to have the largest influence on the variance of the composed estimator obtained from the previous iteration with the goal of minimizing the variance and thus increasing the overall accuracy of the estimation.
we explore three iterative approaches.
the first approach is based on gradient descent optimization section .
which provides a natural solution to the sampling allocation problem.
it uses the composition rules from equations to compute the gradient of the global variance with respect to the number of samples allocated to each local estimator.
the impact of each local estimator as quantified by the gradient is then used to decide how many new samples to allocate for each sub problem aiming at minimizing the global variance.
the second approach overcomes the computational overhead related to bootstrapping new sampling procedures for multiple subproblems.
it uses a relaxed form of gradient descent optimization based on a sensitivity analysis section .
where at each iteration new samples are allocated only for the single most influent subproblem as identified by the gradient.
the third approach introduces a simple heuristic sampling allocation that at each iteration allocates new samples for the subproblem whose estimator has the largest variance section .
.
this heuristic is computationally cheaper than the other optimization methods because it does not require the computation of the gradient of the global variance with respect to the number of samples allocated for each sub problem.
however our experiments show that this simple heuristic works well in practice.
the three different strategies discussed in the next sections will be evaluated on several case studies in section .
.
gradient descent variance minimization gradient descent is a common optimization method for finding theminimum of functions for which derivatives can be defined .
the gradient descent method starts with an initial random candidate solution and iteratively discovers new candidates closer to the optimum.
at each step a new candidate solution is produced following the direction of the negative gradient of the function.
as a local search method it can get stuck in local optima.
however in our context there is a unique minimum as the variance is guaranteed to decrease with each new sample.
the dependency of the global variance on the number of samples allocated for each sub problem can be computed in analytical form combining equations to .
for the sake of simplicity we will for now ignore icp based stratified sampling thus assuming each sub problem to be quantified by simple hit or miss monte carlo equation .
we will bring icp based stratified sampling later.
as an example of this computation consider from section .
the pcgoal flapposition actuatoreffect windeffect .
the quantification problem for this pc can be reduced to the quantification of the independent constraints c0 goal 0andc4 flapposition windeffect where the constant actuatoreffect has been already evaluated whose solution space is quantified by the estimators x0and x1 respectively.
if n0samples are allocated for the estimation of x0andn1for the estimation of x1 their respective variances would be equation var x0 x0 n0var x1 x1 n1 the variance of the estimator for the pc as a function of n0andn1 can then be computed applying the conjunction composition rule equation as follows.
var n0 n1 x2 x1 x1 n1 x2 x0 x0 n0 x0 x0 n0 x1 x1 n1 an analogous procedure can be applied to deal with disjunctive forms.
our goal is now to minimize the function var n0 n1 nm which denotes the variance of the global estimate as a function of the number of samples allocated for the estimation of each of the m 1independent constraints the quantification problem has been split in.
we want to find a sequence of sample allocations that brings global variance near quickly.
note that is the unique global minimum for the variance and it is reachable when the number of samples grows to infinity.
the initial solution n0 n0 n0 n0m can be computed in a bootstrap stage where an arbitrary number of samples is allocated uniformly to each estimation sub problem.
this initial round of sampling provides also an initial estimate of the expected value and the variance of each independent constraint which will be used to estimate the value of var n0 at the initial point.
given the value of nkat step k the value of nk 1is computed according to the following formula nk nk g var nk where gis the step size we will get back to this later and var n is the gradient i.e.
the vector of the partial derivatives of var with respect to the arguments ni.
intuitively the gradient indicates at each step how much each of the independent constraints can contribute to minimize var .recall that the quantification problem consists in estimating the probability that an input satisfies any of the path conditions in pct given a probability distribution on the input space.
in other words we aim to estimate the probability of satisfying the disjunction of the pcs in pct and each pc is the conjunction of independent constraints.
exploiting the compositional rules of section .
the gradient of the global variance with respect to the number of samples to be allocated to each independent sub problem can be computed compositionally too.
theorem .derivative of disjunction compositions.
the derivative of the variance of the estimator xcfor the disjunction c c0 c1 ckwith respect to the number of samples niallocated to the estimators xifor the constraints ci i2 can be computed as var ni i2 var ni proof .the composition rule in equation overestimates the variance of a disjunction as the sum of the variances of the disjuncts.
the proof follows from the linearity of the derivative operator.
theorem .derivative of conjunction compositions.
the derivative of the variance of the estimator xcfor the conjunction c c0 c1 ckwith respect to the number of samples niallocated to the estimators xifor the constraints ci i2 can be computed where the constraints ciare independent according to the definition introduced in section .
and the estimators xiuse hm mc as var ni var ni j2 j6 i e var proof .the proof is omitted for space reasons the interested reader can refer to for it.
the gradient descent method terminates when either the target variance is achieved or when the gradient gets close enough to .
the latter indicates the optimum has been reached usually within a finite accuracy.
notably the convergence speed of gradient descent methods is proportional to the value of the gradient.
this implies that the closer the gradient gets to 0the slower the convergence is .
nonetheless for the problem at hand the improvement on the convergence rate of the global estimator is still significant as will be shown in section .
notice that the expected values of the estimates obtained by the composition rules of section are not affected by the gradient descent procedure.
the estimators keep their unbiasedness and their consistency while only the rate of convergence of the variance to is possibly optimized.
back to our example assume that during the initial bootstrap phase after allocating samples for estimating each of the constraints c0 goal andc4 flapposition windeffect we obtained the estimates x0 489and x1 considering the weak wind profile.
after the first iteration we obtain the following gradient var n0 n1 .
thus the sampling budget available for the next iteration should be assigned to x0and x1proportionally to their corresponding derivatives roughly of the samples for c0and for c1.
choosing the step size.
the partial derivatives composing the gradient are used to decide how the sampling budget for the next iteration will be distributed among the independent constraints.
this budget is quantified by the step size g. in general if gis too small then the algorithm will converge very slowly.
on the other hand if gis not chosen small enough then the algorithm may 870use sampling time inefficiently.
in our case we have to take into account the overhead of starting a new sampling procedure for each independent constraint at each iteration.
for a too small budget the bootstrapping time for the sampling procedures might overcome the time for sampling increasing the computational overhead.
an optimal value for gdepends in general on the specific problem at hand.
furthermore instead of fixing the value of g we fix the total number of samples to be allocated for each iteration and compute g so to use it all i.e.
g var nihas to be equal to the total number of samples allowed for each iteration.
since only an integer number of samples can be allocated for each sub problem possible decimal results are rounded up to the smallest larger integer.
for this work we empirically evaluated different values for the total number of samples to be allocated during each iteration and in turng section .
adaptive decisions for ghave been proposed too e.g.
and we plan to evaluate them in the future.
icp based stratified sampling.
icp based stratified sampling see section .
can be used to reduce the variance of the estimates for the single independent sub problems.
icp is used to partition the sampling space into a set of disjoint boxes containing all the solutions for the constraint pruning out the domain regions containing no solutions.
with the same number of samples this approach cannot perform worse than hm mc and usually performs better .
therefore the variance obtainable by hm mc can be seen as an upper bound of the one of stratified sampling for the same problem.
the gradient descent procedure we defined is based on hm mc and decides how many samples to allocate on each sub problem during each iteration.
enhancing the local estimators with stratified sampling can only produce better results which will be also reflected by the more accurate values for the local estimates used to evaluate v on the next iteration but requires an additional decision about how to distribute the samples allocated on an independent constraint among the boxes containing its solutions.
our decision strategy is to distribute of the samples proportionally to the product between the variance of the local estimate within the box and the size of the box and uniformly among all the boxes.
recalling equation both the variance and the size of the box directly affect the variance of the stratified sampling estimates.
with this heuristic we take into account this dependency.
the remaining third of samples is distributed uniformly to speedup the convergence of the estimators for all the boxes and thus a better assessment of their variance for the next iterations.
this is especially relevant when the probability of satisfying the target property within the box is close to if no samples satisfy the constraint restricted within the box the variance is incorrectly estimated as and the box would receive samples on the next iteration preventing the local estimator to assess the actual variance.
.
sensitivity analysis and computational overhead each sampling round requires to start a sampling procedure for each box of each independent constraint for which we allocate new samples.
this operation introduces a significant overhead especially when the number of sub problems is large and the partial derivatives in the gradient are mostly in the same order of magnitude.
this implies that the budget will be distributed almost uniformly requiring to add only a few samples for each independent subproblem at each iteration.
a sub optimal strategy to trade convergence rate for lower computational overhead consists in a relaxation of the gradient descent method based on sensitivity analysis.
the sensitivity of the global variance var with respect to the number of samples niallocatedtable characterization of the benchmarks.
subject asrt.
paths ands ar.
ops.
var.
parts.
volcomp subjects artrial1 cart4 coronary6 egfr epi8 egfr efi simple invpend pack13 qcoral subjects apollo conflict turn logic to the constraint ciis defined by the partial derivative var ni.
the single constraint mostly affecting the global variance is the one having the larger sensitivity in absolute value note that all of the derivatives are negative since adding more samples can only decrease the global variance .
the strategy thus consists in allocating the entire sampling budget for the next iteration to the single most important constraint.
recalling our example if var n0 n1 the whole sampling budget for the next iteration will increase n1because the corresponding sub problem has the highest expected impact on the global variance.
from a mathematical viewpoint this means that instead of following the gradient we follow its projection on the single dimension providing the best improvement.
this is in general less effective then using all the information in the gradient but may produce valuable results at a lower computational cost as shown in section .
.
local heuristic for sampling allocation we also considered a low overhead sampling heuristic that does not require to compute the gradient of var .
this heuristic prescribes to allocate at each iteration all the samples to the single estimator having the highest variance.
back again to our example where after allocating samples for each of the sub problems c0 goal andc4 flapposition windeffect we obtained the estimates x0 489and x1 088with variance 4and8 respectively this heuristics would require to allocate the entire sampling budget for the next iteration to increase n0since the estimator x0is the one showing the highest variance.
due to the nature of the problem this heuristic intuitively guarantees the convergence of the estimator since the allocation of more samples to an estimator with positive variance always strictly decreases its variance equation .
this guarantees that all the estimators with non zero variance will eventually receive additional samples following the convergence of the global estimator as well.
.
ev aluation we described distribution aware sampling and iterative sampling allocation for improving the convergence rate of a compositional simulation based probabilistic symbolic execution.
we implemented 871these techniques in the java based tool qcoral and evaluated their effectiveness in section .
and .
respectively.
.
experimental settings for our evaluation we used the publicly available benchmarks ofvolcomp and qcoral .
the subjects from the volcomp benchmark only contain linear constraints that we translated in the input format for our tool .
these subjects are a heart fibrillation risk calculator artrial a steering controller to deal with wind disturbances cart a coronary disease risk calculator coronary an estimator of chronic kidney s disease egfr epi andegfr epi simple an inverted pendulum invpend and a model of a robot deciding how to pack goods of different weights into envelopes with limited capacity pack .
the subjects from the qcoral benchmark contain non linear constraints and more complex mathematical functions e.g.
sinus they are a model of the apollo lunar vehicle autopilot apollo and two core modules of an aircraft collision detection monitor conflict andturn logic .
these subjects were implemented in java and analyzed using symbolic pathfinder to compute the pcs of paths leading to assert violations .
table reports a characterization of these subjects.
column asrt denotes a unique id we used to identify the checked assertions during the discussion.
column paths indicates the number of symbolic paths leading to the violation of the assertion.
columns ands and ar.
ops denote respectively the number of conjuncts and the number of arithmetic operations.
for column ar.
ops in parenthesis the number of different operators e.g.
orsin .
a value of in this column means that all the constraints involve only comparisons e.g.
x yorx .
finally parts.
indicates the number of independent sub problems identified for compositional analysis and var indicates the number of sub problems with exact solutions after applying icp.
column var.
parts.
indicates how many of the sub problems we obtained variance on an execution of qcoral with 100k samples without iterative sampling allocation.
a sub problem can obtain variance after sampling when either icp returned an exact solution or if all the samples returned the same truth value cf.
equation .
baseline.
to compare the accuracy of the different approaches we solved the quantification problem with the commercial tool mathematica version .
we used the off the shelf function nprobability with default arguments.
this procedure is designed to provide solutions to a broad range of problems.
in contrast our method is tailored to probabilistic symbolic execution.
this results in some cases in both a slower performance and exceptions reported by mathematica where nprobability fails to achieve precise results.
different configurations of nprobability may avoid these exceptions however they would require human expertise beyond the off the shelf use of the tool.
execution environment.
we run qcoral on an intel core i7 .67ghz 8m cache machine with 8gb of ram.
considering the higher computational demand for numerical integration with mathematica we performed this operation on an r3.large amazon ec2 machine running on an intel xeon e5 v2 .
ghz 25m cache with 16gb ram.
both machines run ubuntu bits.
.
distribution aware sampling distribution aware sampling aims at providing direct support for input variables characterized by continuous probability distributions.
this section reports on two different experiments we conducted to evaluate our proposed technique.
the first experiment compares the results of qcoral and nprobability with respect to precisionand efficiency.
the second experiment compares the precision lack of bias and scalability of distribution aware sampling against the analysis with discretized usage profiles.
.
.
comparison with nprobability table reports the results of analyzing each subject with qcoral and nprobability.
qcoral performs a single sampling round with 100k samples thus no iterative sampling allocation is used for this experiment.
the cells shadowed in grey highlight the cases for which mathematica reported an exception and the results might thus not meet the default prescribed accuracy of at least five decimal digits.
a is used to mark the cases where the analysis failed to return results within hours of processing.
to evaluate distribution aware sampling we experimented with different continuous input distributions.
in the first set of experiments we assigned to each variable a truncated normal distribution centered in the middle of the variable domain and with standard deviation equal to of the domain length and truncated by the bounds of the domain.
we set the standard deviation to so that already the non truncated distribution has probability to generate a sample within the domain.
truncation introduces a small correction to guarantee that all samples fall within the bounds.
in the second set of experiments we assigned to each variable a truncated exponential distribution.
the rate parameter of the distribution has been tuned again to make of the samples fall within the original variable domain.
since exponential distributions are defined over positive domains we excluded the subjects whose values have negative domains.
for all the cases where nprobability terminated without exceptions the results of qcoral are consistent at the reported accuracy s .
in several cases the results of qcoral are exact s up to java double accuracy and match those of nprobability.
the execution time of qcoral is significantly shorter than the one required by nprobability for the same subjects especially on the more complex ones where the difference is by several orders of magnitude.
qcoral also produces more robust results compared to nprobability completing the analysis for all the subjects without exceptions.
this is due to the intrinsic robustness of simulation based approaches.
a final note concerns the result of qcoral for assertion with normal usage profile.
this result is indeed larger than which is clearly not a valid probability.
this is due to the accumulation of inaccuracies of the sub problems estimates when the result is close to .
however the corresponding smakes the result compatible with nprobability.
cutting it to would alter the confidence intervals computable with the estimate and s thus we report the estimate as is including accumulated errors.
.
.
comparison with discretization this section evaluates the tradeoff between accuracy and scalability of distribution aware sampling with the same analyses performed on discretized usage profiles.
for the experiments reported in table we assigned for each subject a truncated normal distribution to two of the input variables following the same parameterization procedure described for the previous experiments.
all the other variables have the original uniform distribution defined in .
only two variables have been assigned a non uniform profile for scalability reasons since the size of discretized usage profiles grows exponentially in the number of non uniform variables cf.
section .
this results in a complexity already sufficient for comparing the approaches.
there are several possibilities for discretizing a continuous distribution.
a simple solution requiring no prior knowledge on the problem consists in dividing the domain into a certain number of 872table comparison of different discretization methods.
discretization invokes qcoral once for every region within each constraint partition.
for every subject two input variables are normally distributed the others are uniformly distributed.
subject asrt.nprobabilitydiscretizationqcoral intervals intervals solution time s avg.
est.
time s avg.
est.
time s est.
time s artrial1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cart4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
coronary6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
egfr epi8 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
egfr efi simple .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
invpend .
.
.
.
.
.
.
.
pack13 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2h0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
apollo .
2h0.
.
.
.
.
.
conflict .
.
.
.
.
.
.
.
turnlogic .
.
.
.
.
.
.
.
equally large intervals and to assign each interval the probability it would have according to the original distribution i.e.
the probability for an interval would be cdf b cdf a where cdf is the cumulative distribution function of the original continuous distribution.
a deeper knowledge of the constraints to be quantified may allow more effective discretization where smaller intervals are used to increase the resolution of the approximate distributions around the points mostly affecting the satisfaction of the constraints to be quantified.
however this would require in general human expertise on the specific problem.
table reports our experimental results.
we keep the result of nprobability as reference value.
notice that the results in this table differ from those on table due to the different usage profiles.
furthermore with the simplified usage profile we use for these experiments nprobability always terminates correctly.
we discretized the domain of the two non uniform variables in and intervals.
the total number of usage scenarios composing the discretized profile is thus and respectively cf.
section .
.
a too coarse grained discretization may introduce a bias in the result due to the loss of information.
finer discretization improves the precision of the result but does not scale due to the exponential blowup in the number of usage scenarios .
this is visible in table where the results for a intervals discretization deviate from the reference value more than those for the intervals.
distributionaware sampling prevents the risk of introducing such biases.
finally finer discretization requires a higher computation cost.
though this cost might be reduced leveraging the caching of partial results for independent sub problems shared by the different usage scenarios the worst case complexity remains exponential.
even with the relatively coarse grained discretization we applied on only two non uniform variables the analysis time for the discretized profiles takes longer than with distribution aware sampling.
the latter grows instead only linearly with the number of variables thus scaling to significantly larger problems.
.
iterative optimal sampling allocation iterative sampling allocation aims to improve the convergence rate of simulation based quantification by allocating samples to sub problems according to their predicted importance.
section described three strategies to decide how to allocate samples to non initial iterations of the quantification procedure we evaluate these strategies here.
the first strategy gradient decides how many samples to allocate to each sub problem pursuing a gradient descent minimization of the global variance.
the second strategy sensitivity performs a gradient based sensitivity analysis to identify the single partial sub problem with highest impact on the global variance and allocate new samples to improve the estimate.
the third strategy local uses a low overhead heuristic selecting the sub problem to improve only based on the variance of its local estimator thus not requiring a global impact analysis.
the goal of the three strategies is to increase the convergence rate of the compositional simulation based quantification described previously.
we take as baseline the distribution aware sampling described and evaluated in the previous sections.
we evaluate the iterative techniques with a sampling budget per iteration of 1k and 10k samples which determines the step size for the gradient based methods .
we report the results of our experiments on selected subjects in figure and table .
each subject is identified by the name of the program and the id of the assertion.
for the experiments in this section we assigned each variable a truncated normal distribution with mean in the center of the variable domain defined in the original papers of the benchmarks and standard deviation equal to of the domain length as we did in the previous section.
.
.
convergence rate figure shows the convergence rate of the three approaches and the baseline through plots having the wall clock time on the x axis and the average standard deviation of the global estimator i.e.
the square root of its variance on the y axis in logarithmic scale.
all the experiments have been ran for minutes.
the initial bootstrapping has been performed by taking 50k samples uniformly among all the sub problems.
gradient and sensitivity outperformed the baseline for all of the subjects and both 1k and 10k sampling budget per iteration.
when more samples are allowed these two approaches performs almost equally.
the best improvement is achieved for apollo while the worst is for artrial .
these two subject are the most complex in terms of number of pcs and number of conjuncts per pc.
however while in the case of apollo only a few sub problems have a high impact on the global variance for artrial the sub873table distribution aware sampling comparison of nprobability andqcoral .
subject asrt.nprobability qcoral solution time s estimate avg.s time s gaussian distributions artrial1 .
.
.
.00e .
.
.
.
.
.
.
26m .
.
.
cart4 .
12m .
.
.
.
13m .
.
.
coronary6 .
.
.
.26e .
.
.
.
.91e .
egfr epi8 .
.
.
.00e .
.
.
.
.00e .
egfr epi .
.
.
.
.
simple .
.
.
.00e .
invpend .
23m .
.
.
pack13 .
.
.
.00e .
.
.
.
.00e .
.
.
.
.00e .
.
.
.
.00e .
.
31h .
.
.
.
32h .
.
.
.
36h .
.00e .
apollo .
4h .
.
.
conflict .
.
.
.
.
turn logic .
10m .
.
.
exponential distributions artrial1 .
.
.
.00e .
.00e .
.00e .00e .
.
15m .
.
.
coronary6 .
.
.
.
.
.00e .
.00e .00e .
egfr epi8 .
.
.
.00e .
.
.
.
.00e .
egfr epi .
.
.
.00e .
simple .
.
.
.00e .
pack13 .
.
.
.00e .
.
.
.
.00e .
.
.
.
.00e .
.
.
.
.00e .
.
2h .
.
.
.
.
.
.
.
.
.
.
.00e .
apollo 40h .
.
.
conflict .00e .
.
.
.
turnlogic .
2h .
.
.
problems to be analyzed have similar impact on it.
in particular for apollo only a few sub problems have a large local variance and a high impact on the global result.
this is an optimal condition for all three allocation strategies which perform similarly.
the baseline approach is instead unable to exploit this information and allocates samples on sub problems already close to convergence or with low impact on the global result.
for artrial there is not a big difference in the impact of the different sub problems though a few of them have slightly larger effect on the global result.
in this case the benefit of gradient based techniques is limited and also local does not provide a significant improvement.
the local strategy fails to improve the convergence rate and actually slows it down when the sub problems with highest variance have a low impact on the global result artrial and coronary .
indeed the impact of a sub problem depends not only on its local variance but also on the estimates and variance of the other sub problems that are in conjunction within the pcs under analysis.
in coronary it is possible to observe a small spike for the local strategy.
the initial sampling provides indeed only approximate estimates for the various sub problems.
these estimates are improved through the subsequent iterations.
the greediness of local makes the method to keep sampling from a single sub problem until its variance is reduced enough to move to another one.
when more samples are allocated to a sub problem not only the variance of the corresponding estimator is decreased but also the approximation of the global result is improved through the composition rules possiblyartrial 1k 1000artrial 10k artrial 1k artrial 10k cart 1k cart 10k cart 1k cart 10k coronary 1k coronary 10k apollo 1k apollo 10k baseline gradient sensitivity local10 figure convergence rate of iterative sampling and the baseline approach on selected subjects y axis in logarithmic scale .
correcting a wrong initial assessment.
for this reason it is possible to obtain small spikes when moving from one sub problem to another.
.
.
time to converge table reports in tabular form the results of some of the experiments for a deeper investigation.
all the runs have been interrupted after minutes.
target srepresents different target accuracies in terms of standard deviation of the global result.
we reported the computation time required by the three iterative allocation strategies and the baseline approach to achieve the target accuracy.
for lower accuracy larger s the gradient based methods performs better with 1k sampling budget per iteration while for higher accuracy 10k performs slightly better.
in both cases the convergence rate gets slower while moving toward higher accuracy.
this observation is consistent with the theory on gradient descent optimization since this optimization approach gets less efficient when the result approaches its optimum.
nonetheless the two methods outperform baseline and local even for higher accuracies since they are capable of exploiting more information about the problem.
indeed though all the derivatives tend to converge to when approaching the optimum the small differences among them are still enough to improve on the uninformed uniform allocation performed by baseline while local may waste time sampling from low impact sub problems having high variance.
for higher accuracy when the differences among the derivatives get smaller the expected impact of the different subproblems tends to become similar.
in this case since there is no 874table time to reach a target accuracy for incremental sampling techniques plus baseline for 1k and 10k sampling budget per iteration.
initial uniform sampling bootstrap 50k.
subject asrt.prc.time s xbaseline local sensitivity gradient 1k 10k 1k 10k 1k 10k 1k 10k apollo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30m 30m .
.
.
.
.
.
cart41 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30m 30m .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30m 30m .
.
.
.
.
.
artrial24 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30m 30m .
.
.
.
30m 30m 30m 30m .
.
30m 30m .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30m 30m 30m .
.
.
30m .
coro .
.
.
.
.
.
.
.
nary .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30m 30m 30m 30m .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30m 30m 30m 30m .
.
.
.
significantly better choice it is more efficient to take more samples per iteration instead of consuming analysis time for updating the gradient and taking new decisions frequently.
we plan to investigate in the future dynamic techniques to handle this situation.
according to our experiments sensitivity overall provides the best strategy for iterative sampling allocation combining the effectiveness of gradient based impact analysis with a reduced overhead for decision making eventually leading to more accurate quantification results in a shorter time.
.
related work our work is related to probabilistic program analysis probabilistic abstract interpretation probabilistic model checking and volume computations .
we discuss here some of the most closely related work.
probabilistic analysis based on symbolic execution has been described in e.g.
.
geldenhuys et al.
considered uniform distributions for the inputs linear integer arithmetic constraints and used latte machiato to count solutions of path conditions produced during symbolic execution.
sankaranarayanan et al.
and filieri et al.
proposed similar techniques to compute probabilities of violating program assertions.
both techniques remove the restriction of uniform distributions although in the latter case it is by discretizing the domain into small uniform regions.
as with both approaches only consider linear constraints.
sankaranarayanan et al.
developed algorithms for under and overapproximations of probabilities.
they use linear programming lp solvers to compute over approximations and heuristics based ray shooting algorithms to compute under approximations which is applicable for convex polyhedra.
filieri et al.
used the latte tool to compute probabilities.
furthermore the approach in provides treatment of multi threading and input data structures it uses the korat tool for counting the input structures .
follow on work provides thorough treatment of nondeterminism and describes alternative statistical exploration of symbolic paths .
another simulation based approach has been proposed in for the analysis of probabilistic programs.
in that work markovchain monte carlo estimation is enhanced with a preliminary program analysis aiming at generating verification conditions for all the operations involving probabilistic variables and operators.
a violation of any of these conditions implies the violation of the program assertions and since these conditions may be processed before reaching the assertions the simulations can terminate earlier reducing the overall analysis time.
the work targets small probabilistic programs which describe complex probability distributions and the probabilistic analysis is not compositional itself.
in turn our work provides compositional sampling techniques for the analysis of constraints generated from arbitrary programs enhanced with iterative techniques that focus the sampling on the constraints deemed the most important.
the technique in proposed a compositional quantification of the solution space based on monte carlo estimation briefly recalled in section .
this approach can deal with arbitrarily complex numeric constraints over floating point domains and scales better than previous approaches however it is limited to discretized profiles and as a simulation based approach may suffer from slow convergence rate.
we extend that work in two directions direct handling of non uniform distributions and focused iterative sampling.
bouissou et al.
and adje et al.
handle non linear constraints with a combination of abstraction based on affine and p box arithmetic.
the approach relies on the use of noise variables to represent the uncertainty of non linear computations.
they use an abstraction based approach whereas we use a statistical approach.
we can thus handle a wider set of non linear constraints such as complex mathematical functions sine cosine etc.
.
in future work we would like to do an empirical comparison between these two approaches on the examples that both can handle.
our work is also related to weighted model counting and distribution aware sampling which has many applications beyond probabilistic symbolic execution e.g.
in machine learning and constrained random verification.
chakraborty et al.
address the problem in the context of cnf boolean formulas while our work is concerned with arbitrarily complex mathematical constraints.
.
conclusions we presented an iterative distribution aware sampling technique for probabilistic symbolic execution.
given a set of complex mathematical constraints representing the path conditions collected with symbolic execution over a program we use a statistical technique to estimate the probability of satisfying them assuming the values of the constrained inputs follow given continuous probability distributions.
to speed up the convergence rate of the analysis we proposed three strategies for iterative sampling allocation which focus the sampling on the constraints that have the largest influence on the estimated results.
experimental evaluation of the three iterative allocation strategies show their respective effectiveness.
in the future we plan extend our tool with distribution aware sampling from multivariate input distributions that relate multiple input variables .
in theory our proposed approach works for such cases however efficient sampling from multivariate distributions is an open problem in statistics often involving more sophisticated techniques such as gibbs sampling .
we also plan to explore the automatic tuning of the sampling budget to allocate for each iteration for reducing the decision overhead by taking into account the relative gain of each possible decision over the others.
.