a neural model for generating natural language summaries of program subroutines alexander leclair siyuan jiang collin mcmillan dept.
of computer science and engineering university of notre dame notre dame in usa email aleclair cmc nd.edu dept.
of computer science eastern michigan university ypsilanti mi usa email sjiang1 emich.edu abstract source code summarization creating natural language descriptions of source code behavior is a rapidly growing research topic with applications to automatic documentation generation program comprehension and software maintenance.
traditional techniques relied on heuristics and templates built manually by human experts.
recently data driven approaches based on neural machine translation have largely overtaken template based systems.
but nearly all of these techniques rely almost entirely on programs having good internal documentation without clear identifier names the models fail to create good summaries.
in this paper we present a neural model that combines words from code with code structure from an ast.
unlike previous approaches our model processes each data source as a separate input which allows the model to learn code structure independent of the text in code.
this process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided.
we evaluate our technique with a dataset we created from .1m java methods.
we find improvement over two baseline techniques from se literature and one from nlp literature.
index t erms automatic documentation generation source code summarization code comment generation i. i ntroduction a summary of source code is a brief natural language description of that section of source code .
one of the most common targets for summarization are the subroutines in a program for example the one sentence descriptions of java methods widely used in automatically formatted documentation e.g.
javadocs .
these descriptions are useful because they help programmers understand the role that the subroutine plays in a program empirical studies have repeatedly shown that understanding the role of the subroutines in a program is a crucial step to understanding the program s behavior overall .
even a short summary of a subroutine e.g.
returns the player s hitpoint count can tell a programmer a lot about that subroutine and the program as a whole.
a holy grail of software engineering research has long been to generate these summaries automatically.
forward et al.
pointed out in that software professionals value this work is supported in part by nsf ccf ccf and cns grants .technologies that improve automation of the documentation process and that documentation tools should seek to better extract knowledge from core resources such as source code .
however the state of the practice has barely changed since that time for tool support for automated documentation generation.
tools such as javadoc and doxygen automate the format and presentation of documentation but still leave programmers with the most labor intensive effort of writing the text and examples.
research into generation of natural language descriptions of code has come to be known as source code summarization with significant effort focused on generation of summaries of subroutines for several years significant progress was made based on content selection and sentence templates or even somewhat idiosyncratic solutions such as mimicking human eye movements .
however as in many research areas and as chronicled in a recent survey by allamanis et al.
these techniques have largely given way to ai based on big data input.
the inspiration for a vast majority of efforts into ai based code summarization originates in neural machine translation nmt from the natural language processing research community.
an nmt system converts one natural language into another.
it is typically thought of in terms of sequence to sequence seq2seq learning in which an e.g.
english sentence is one sequence and is converted into a equivalent target sequence representing a e.g.
french sentence.
in software engineering research machine translation can be considered as a metaphor for source code summarization the words and tokens in the body of a subroutine are one sequence while the desired natural language summary is the target sequence.
this application of nmt to code summarization has shown strong benefits in a variety of applications .
however an achilles heel to nearly all source code summarization techniques is a reliance on programmers having written high quality internal documentation in the form of identifier names or comments.
in order to generate a meaningful summary meaningful words must be observed in the body of the subroutine.
in traditional nmt this is accepted ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
because a natural language input sentence will definitely have words related to the output target sentence.
but in software the words in code are not actually related to the behavior of that code.
a subroutine s behavior is dictated by the structure of programming language keywords and tokens that define control flow data flow etc.
these differences between code and natural language are a barrier to improving performance in several ai applications to software engineering as hellendoorn et al.
pointed out to some controversy at fse .
in this paper we present a neural model for summarizing subroutines.
our model combines two types of information about source code a word representation treating code as text and an abstract syntax tree ast representation.
a distinguishing factor of our model compared to earlier approaches is that we treat both representations separately.
previous techniques have shown promise by annotating a word representation with ast information but ultimately the annotated representation is sent as a single sequence through a standard seq2seq model.
in contrast our model accepts two inputs one for the word representation and one for the ast.
the advantage is that we are able to treat each input with differently which increases the flexibility of our approach as we will show in this paper.
in essense the neural model we propose involves two unidirectional gated recurrent unit gru layers one to process the words from source code and one to process the ast.
we modify the sbt ast flattening procedure by hu et al.
to represent the ast.
we then use an attention mechanism to attend words in the output summary sentence to words in the code word representation and a separate attention mechanism to attend the summary words to parts of the ast.
we concatenate the vectors from each attention mechanism to create a context vector.
finally we predict the summary one word at a time from the context vector following what is typical in seq2seq models.
we evaluate our technique in two stages.
first we collect over 51m java methods from the sourcerer repository and preprocess them to form a dataset of around .1m methods with suitable javadoc summary comments.
we divide the dataset into training validation testing sets and perform a set of tests comparing results from our model to three competitive baselines.
we call this the standard experiment because it conforms to common practice in both se and nlp venues.
second to evaluate the limits of our model in a scenario without words from source code we repeat the standard experiment using only the ast for each java method in this study we assume no code words are available as in obfuscated code poorly written code or situations in which there is only bytecode from which an ast can be extracted but code words are likely to have been removed during compilation .
this no code words experiment simulates a situation unique to the se domain and as we will show is far more difficult than the standard application of nmt in which a programmer provides useful keywords.
we call this the challenge experiment .
our results in a nutshell are in the standard experiment our model and the competitive nlp baseline provide compara ble performance but with orthogonal predictions implying that they are good candidates for ensemble decoding.
an ensemble provides state of the art performance of .
bleu an improvement over the nearest baseline .
in the challenge experiment our model achieves .
bleu versus for any baseline.
this is a significant step forward in source code summarization since it requires zero meaningful code words.
we release all data code and implementation via our online appendix see section x .
ii.
p roblem and overview we target the problem of source code summarization of subroutines automatic generation of natural language descriptions of subroutines.
specifically we target summarization of java methods with the objective of creating method summaries like those used in javadocs.
while we limit the scope of the experiments in this paper to a large java dataset in principle the techniques described in this paper are applicable to any programming language that has subroutines from which an ast can be computed and from which text e.g.
identifier names can be extracted.
our scoping of our target problem is consistent with the problem definition in many papers on code summarization .
a solution to this problem would have many practical applications.
the primary practical application would be in automatic documentation generation to help programmers write documentation more quickly as well as understand code that has not been documented.
of the 51m java methods we found in the sourcerer dataset only about have any sort of method summary and only about contain summaries that met basic quality filters we define in section v. in our view it seems likely that more than should be documented well and an automatic summary generator would help improve the amount of code that could be documented.
but more generally our goal for this paper is to also contribute to an ongoing academic debate about how to represent source code to solve software engineering problems using ai.
as mentioned there is reasonable doubt that neural based techniques are even appropriate for software engineering data a recent workshop at aaai focused heavily on this debate.
given the long history of ai use to solve se problems our sincere hope for this paper is to provide insight into ways to build neural models of se data even for researchers outside of the specific task of code summarization.
we have made significant efforts to keep our data and techniques public and reproducible see section x to help these other researchers as much as possible.
an overview of this paper is below.
in the next section we cover background and related technologies.
then we introduce our proposed neural model.
we then describe how we obtained and processed the java datasets we use.
we conduct the standard and challenge experiments on the same set of java methods.
finally we spend significant space on examples and discussion.
we feel an in depth look at examples where the model worked and did not will provide key insights for improving or adapting the model in the future.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iii.
b ackground and rela ted work this section covers the supporting technologies behind our work plus related work in source code summarization.
a. source code summarization related work in source code summarization can be broadly classified as either ai data driven or heuristic template driven.
data driven among data driven techniques recent work by hu et al.
is the most closely related to this paper.
that work proposes to use the ast to annotate the words in the source code and then use the annotated representation as input to a seq2seq neural model.
the model itself is an off the shelf encoder decoder the main advancement is the ast annotated representation called structurebased traversal sbt .
sbt is essentially a technique for flattening the ast and ensuring that words in the code are associated with their ast node type.
for example the code request.remove id becomes methodinvocation simplename request simplename request simplename remove simplename remove simplename id simplename id methodinvocation the intent is that the words request remove and id be associated with the context in which they appear.
in this case a methodinvocation node.
the sbt representation forms an important baseline for comparison in our experiments in later sections.
a casual reader will note that sbt was shown in that paper to obtain remarkable performance of bleu but we caution that this is not directly comparable to the results in our experiments.
the reason is that in the dataset was split by function so the training validation and test sets contain random selections of functions in the entire dataset.
in contrast we split by project .
in functions from the same project can be in both the training and test sets.
in our experiments all methods from a project are either training validation or test.
in addition we performed other preprocessing such as autogenerated code removal see section v to avoid situations where identical methods appear in both training and test sets.
taken together we expect that the nominal performance scores for all approaches will be far lower in our experiments.
other related ai data approaches in generating summaries of subroutines includes work by hu et al.
focusing on creating summaries from sequences of api calls and code nn by iyer et al.
which similar to creates a custom word representation of code which it then feeds to an off the shelf seq2seq model.
there is also related work outside the task of subroutine summaries.
jiang et al.
and loyola et al.
generate descriptions of code changes i.e.
commit messages .
allamanis et al.
predict a name for a subroutine from the body of a subroutine.
oda et al.
create pseudocode from source code by adapting statistical machine translation.
yin et al.
movshovitz et al.
and allamanis et al.
target comments of short snippets of code a task facilitated by public datasets .
gu et al.
have demonstrated using a neural model for source code search another task growing in popularity and facilitated by public datasets .
of note is that the attentional encoder decoder seq2seq model originally described by bahdanau et al.
is at the core of many of these papers as it provides strong baseline performance even for many software engineering tasks.
heuristic template based haiduc et al.
is often cited as the first attempt to create text summaries of code and indeed is the first to introduce the term source code summarization.
these early approaches create extractive summaries by calculating the top nkeywords with metrics such as tf idf.
shortly thereafter work by sridhara et al.
adapted swum a technique for finding parts of speech of words in code to create short summary phrases for source code using templates.
another template based solution by mcburney et al.
also used swum but summarized a subroutine s context defined as the functions that call or are called by a method in addition to the method context.
rodeghero et al.
made further improvements to content extraction for heuristic and template solutions by modifying the heuristics to mimic how human programmers read code with their eyes.
as in other research areas related to natural language generation data driven techniques have largely supplanted template based techniques due to a much higher degree of flexibility and reduced human effort in template creation.
we direct readers to a comprehensive survey by nazar et al.
.
b. neural machine translation the workhorse of most neural machine translation nmt systems is the attentional encoder decoder architecture .
this architecture originated in work by bahdanau et al.
and is explained in great detail by a plethora of very highlyregarded sources .
in this section we cover only the concepts necessary to understand our approach at a high level.
in an encoder decoder architecture there are a minimum of two recurrent neural networks rnns .
the first called the encoder converts an arbitrary length sequence into a single vector representation of a specified length.
the second called the decoder converts the vector representation given by the encoder into another arbitrary length sequence.
the sequence inputted to the encoder is one language e.g.
english and the sequence from the decoder is another language e.g.
french.
encoder decoder architectures learn to predict sentences one word at a time the decoder generally does not try to predict a whole sentence at once.
the way this usually works is that during training instead of sending the network authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the network receives the whole input sequence a sequence of output words so far plus the correct next word during inference the trained model is given an input sequence which is used to predict the first word in the output sentence.
then the input sentence is sent to the model again along with the first prediction.
the decoder outputs a prediction for the second word in the sentence and so on until the decoder predicts an end of sentence token.
the problem with this strategy is that the encoder is burdened with creating a vector representation suitable for prediction at every output step.
in reality some words in the input sentence will be more important than others for a particular output.
e.g.
on for sur .
this is the motivation for attentional encoder decoder networks .
essentially what happens is that instead of a single vector representation of the input sentence an attention mechanism is placed between the encoder and decoder.
that attention mechanism receives the encoder s state at every time step in the example above four vectors for each of the four positions in the sentence.
the attention mechanism in essence selects which vector from the encoder to use so that different decoder predictions receive input from different positions in the input sequence.
our work builds on the attentional encoder decoder strategy in key ways that we describe in the next section.
iv .
o ur proposed model this section describes our proposed neural model.
the model assumes a typical nmt architecture in which the model is asked to predict one word at a time as described in the previous section.
a. model overview our model is essentially an attentional encoder decoder system except with two encoders one for code text data and one for ast data.
in the spirit of maintaining simplicity where possible we used embedding and recurrent layers of equal size for the encoders.
we concatenate the output of attention mechanisms for each encoder as depicted here precedent for combining different data sources comes heavily from image captioning e.g.
merging convolution image output with a list of tags .
one aim in this paper isto demonstrate how a similar concept is beneficial for code summarization in contrast to the usual seq2seq application to se data in which all information is put into one sequence.
we also hope to sow fertile ground for several areas of future work in creating unique processing techniques for each data type treating software s text and structure differently has a long tradition .
b. model details to encourage reproducibility and for clarity we explain our model as a walkthrough of our actual keras implementation.
the following starts at line inmodels ast attendgru xtra.py all code is available for download from our online appendix section x .
txt input input shape self.txtlen com input input shape self.comlen ast input input shape self.astlen first above are three input layers corresponding to the code text sequence the comment sequence and the flattened ast sequence.
we chose the sequence lengths as a balance between model size and coverage of the dataset.
the sequence sizes of for code text and ast and words for comment each cover at least of the training set.
shorter sequences are padded with zeros and longer sequences are truncated.
ee embedding output dim self.embdims input dim self.txtvocabsize txt input se embedding output dim self.embdims input dim self.astvocabsize ast input we start with a fairly common encoding structure including embedding layers for each of our encoded input types code text and ast .
the embedding will output a shape of batch size txtvocabsize embdims .
what this means is that for every batch each word in the sequence has one vector of length embdims .
for example means that for each of examples in a batch there are words and each word is represented by a length embedding vector.
we found two separate embeddings to have better performance than a unified embedding space.
ast enc cudnngru self.rnndims return state true return sequences false astout sa ast enc se next is a gru layer with rnndims units we found to provide good results without oversizing the model to serve as the ast encoding.
we used a cudnngru to increase training speed not for prediction performance.
the return state flag is necessary so that we get the final hidden state of the ast encoder.
the return sequences flag is necessary because we want the state at every cell instead just the final state.
we need the state at every cell for the attention mechanism later.
txt enc cudnngru self.rnndims return state true return sequences true txtout st enc ee initial state sa the code text encoder operates in nearly the same way as the ast encoder except that we start the code text gru with the final state of the ast gru.
the effect is similar to if we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
had simply concatenated the inputs except that we keep separate embedding spaces we allow for attention to focus on each input differently rather than across input types we ensure that one input is not truncated by an excessively long sequence of the other input type and we keep the door open for further processing e.g.
via convolution layers that would benefit one input type but not the other.
as we show in our evaluation this is an important point for future work.
tensor txtout would normally have shape batch size rnndims a n rnndims length vector representation of every input in the batch.
however since we have return sequences enabled encout has the shape batch size datvocabsize rnndims which is thernndims length vector at every time step.
that is the rnndims length vector at every word in the sequence.
so we see the status of the output vector as it changes with each word in the sequence.
we also have return state enabled which just means that we get st the rnndims vector from the last cell.
this is a gru so this st is the same as the output vector but we get it here anyway for convenience to use as the initial state in the decoder.
de embedding output dim self.embdims input dim self.comvocabsize com input dec cudnngru self.rnndims return sequences true decout dec de initial state st the decoder is as described in many papers on nmt a dedicated embedding space followed by a recurrent layer.
we start the decoder with the final state of the code text rnn.
txt attn dot axes txt attn activation softmax txt attn the next step is the code text attention mechanism with a design similar to that described by luong et al.
.
first we take the dot product of the decoder and code text encoder output.
the output shape of decout is e.g.
batch size andtxtout is batch size .
the axis of decout is long.
the axis of txtout is also long.
so by computing the dot product along axis in both we get a tensor of shape batch size .
for one example in the batch we get decout of andtxtout .
decout axis txtout axis txt attn .
.
1v1 2v2 .. .
.
1v3v4 .. .
.
1ab 2cd .. where ais the dot product of vectors v1 andv3 and bis the dot product of v1 andv4 etc.
the result is that each of the positions in the decoder sequence is now represented by a length vector.
each value in the length vector reflects the similarity between the element in the decoder sequence and the element in the encoder sequence.
i.e.
babove reflects how similar element in the decoder sequence is similar to element in the code text encoder sequence.
the length vector for each of the input positions reflects how much that a given input position is similar should pay attention to a position in the output.then we apply a softmax to each of the length vectors.
the effect is to exaggerate the most similar things so that more attention will be paid to the more similar input vectors the network learns during training to make them more similar.
note that the dot product here is not normalized so it is not necessarily equivalent to cosine similarity.
txt context dot axes next we make use of the attention vectors by using them to create the context vectors for the code text input.
to do that we scale the encoder vectors by the attention vectors.
this is how we pay attention to particular areas of input for specific outputs.
the above line of code takes txt attn with shape batch size and computes the dot product with txtout batch size .
recall that the encoder has txtvocabsize elements since it takes a sequence of words.
axis of this tensor means for each element of the input sequence.
the multiplication for each sample in the batch is txt attn axis txtout axis txt context .
.
1v1 2v2 .. .
.
1v3v4 .. .
.
1ab 2cd .. the result is a context matrix that has one context vector for each element in the output sequence.
this is different than the vanilla sequence to sequence approach which has only one context vector used for every output.
each output sequence location has its own context vector.
this vector is created from the most attended to part of the encoder sequence.
ast attn dot axes ast attn activation softmax ast attn ast context dot axes we perform the same attention operations to the ast encoding as we do for the code text encoding.
context concatenate but we still need to combine the code text and ast context with the decoder sequence information.
this is important because we send each word one at a time as noted in the previous section.
the model gets to look at the previous words in the sentence in addition to the words in the encoder sequences.
it does not have the burden of predicting the entire output sequence all at once.
technically what we have here are two context matrices with shape batch size and a decout with shape batch size .
the default axis is which means the last part of the shape the one in this case .
this creates a tensor of shape batch size one length vector for each of the input elements instead of three length vectors.
out timedistributed dense self.rnndims activation relu context we are nearing the point of predicting a next word.
a timedistributed layer provides one dense layer per vector in the context matrix.
the result is one rnndims length vector for every element in the decoder sequence.
for example one authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
length vector for each of the positions in the decoder sequence.
essentially this creates one predictor for each of the decoder positions.
out flatten out out dense self.comvocabsize activation softmax out however we are trying to output a single word the next word in the sequence.
ultimately we need a single output vector of length comsvocabsize .
so we first flatten the matrix into a single vector then we use a dense output layer of length comsvocabsize and apply softmax.
model model inputs txt input com input ast input outputs out the result is a model with code text ast and comment sequence inputs and a predicted next word in the comment sequence as output.
c. hardware details the hardware on which we implemented trained and tested our model included one xeon e5 1650v4 cpu 64gb ram and two quadro p5000 gpus.
it was necessary to train on gpus with 16gb vram due to the large size of our model.
v. c orpus prepara tion we prepared a large corpus of java methods from the sourcerer repository provided by lopes et al.
.
the repository contains over million java methods from over projects.
we considered updating the repository with new downloads from github but we found that the sourcerer dataset was quite thorough leading to a large amount of overlap with newer projects that could not be eliminated due to name changes code cloning etc.
.
this overlap could lead to major validity problems for our experiments e.g.
if testing samples were inadvertently placed in the training set .
we decided to use the sourcerer projects exclusively.
significant preparation was necessary to make the repository a suitable dataset for applications of nmt and we view this preparation as an important contribution of this paper to the research field unlike in the nmt research area there are relatively few curated datasets for code summarization .
after downloading the archives we used a toolkit provided by mcmillan et al.
to extract the java methods and organize them into a sql database.
then we filtered for methods that were preceded by javadoc comments indicated by .
we used only comments intended as javadocs because there is an assumption that the first sentence in the comment will be a summary of the method s behavior .
then we extracted the first sentence by looking for the first period or the first newline if no period was present.
next we used the langdetect library to remove comments not in english.
about 4m methods remained after these steps.
a potential problem was auto generated code.
autogenerated code is a problem because both the code and comments created by auto generators tend to be very similar.
if nearly identical code is in the training and testing sets the model will learn these cases easily which could simultaneously reduce performance on the real examples whilefalsely inflating performance metrics such as bleu since the metrics would reward the model for correctly identifying the duplicate cases.
happily the solution is fairly simple we remove any methods from files that include phrases such as generated by suggested by shimonaka et al.
.
this filter is quite aggressive as it reduced the dataset size to around 2m methods and on manual inspection we found no cases of autogenerated code.
in fact a majority of the filtered methods were exact duplicates around 100k unique examples out of 2m removed methods .
but because comments to auto generated code are often still meaningful we added one copy of each of the 100k unique examples back into the dataset and ensured that they were in the training set only so we did not attempt to test against auto generated comments .
the result is a dataset of around .1m methods.
our other preprocessing steps followed the practice of many software engineering papers.
we split the code and comments on camel case and underscore removed non alpha characters and set to lower case.
we did not perform stemming.
we then split the dataset by project into training validation and test sets.
by by project we mean that we randomly divided the projects into the three groups of projects into training into validation and into testing.
then all the methods from a project went into the group assigned to its project.
a side effect is that since projects have different numbers of methods of methods are in training .
in validation and .
in testing.
but this slight variation is necessary to maintain a realistic situation.
as mentioned in section iii we respectfully believe that not splitting by project and not removing auto generated code are mistakes made by a vast majority of previous nmt applications to code summarization and artificially inflates the reported scores for example sbt is reported to have bleu versus bleu with the same technique in our evaluation .
to obtain the asts we first used srcml to extract an xml representation of each method.
then we built a tool to convert the xml representation into the flattened sbt representation to generate sbt formatted output described by hu et al.
.
finally we created our own modification of sbt in which all the code structure remained intact but in which we replaced all words except official java api class names in the code to a special other token.
we call this sbt ao for sbt ast only.
we use this modification to simulate the case when only an ast can be extracted.
from this corpus of java methods we create two datasets the standard dataset contains three elements for each java method the pre processed java source code for the method the pre processed comment and the sbt ao representation of the java code.
the challenge dataset contains two elements for each method the pre processed comment and the sbtao representation of the java code.
technically we also have a third dataset containing the default sbt representation with code words and the preprocessed comment which we use for experiments to compare authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
our approach to the baselines.
however the standard and challenge datasets are our focus in this paper intended to compare the case when internal documentation is available and the much more difficult case with only an ast.
vi.
e v alua tion this section covers our evaluation comparing our approach to baselines over the standard and challenge datasets.
a. research questions our research objective is to determine the performance difference between our approach and competitive baseline approaches in two situations that we explore through these research questions rqs rq1 what is the difference in performance between our approach and competitive approaches in the standard situation assuming internal documentation?
rq2 what is performance of our approach in the challenge situation assuming an ast only?
the rationale for these rqs was largely covered in the introduction and background sections.
essentially existing applications of nmt for the problem of code summarization almost entirely rely on the programmer writing meaningful internal documentation such as identifier names.
as we will show this assumption makes the problem easy for seq2seq nmt models since many methods have internal documentation that is very similar to the summary comment a phenomenon also observed by tan et al.
and louis et al.
.
we ask rq 1in order to study the performance of our approach under this assumption.
in contrast we ask rq 2because the assumption of internal documentation is often not valid.
v ery often only the bytecode is available or programmers neglect to write good internal documentation or code has even been obfuscated deliberately.
in these cases it is usually still possible to extract an ast for a method even if it contains no meaningful words.
in principle the structure of a program is all that is necessary to understand it since ultimately that is what defines the behavior of the program.
in practice it is very difficult to connect structure directly to high level concepts described in summaries.
we seek to quantify a baseline performance level with our approach since to our knowledge no published approach functions in this situation .
b. baselines to answer rq the standard experiment we compare our approach to three baselines.
one baseline which we call attendgru is a generic attentional encoder decoder model to represent an application of a strong off the shelf approach from the nlp research area.
note that there are a huge variety of nmt systems described in the nlp literature but that a vast majority have an attentional encoder decoder model at their heart see section iii .
to maintain an apples to apples comparison the baseline is identical to the code text encoder in our approach the decoder is identical as well .
in essence the baseline is the same as our proposed approach exceptwithout the ast encoder and associated concatenation.
while we could have chosen any number of approaches from nlp literature it is very difficult to say up front which will perform best for code summarization and we needed to ensure minimal differences to maximize validity of our results.
if for example we had used an architecture with an lstm instead of a gru in the encoder we would have no way of knowing if the difference between our approach and the baseline were due to the ast information we added or due to using an lstm instead of a gru.
a second baseline is the sbt approach presented by hu et al.
.
this approach was presented at icpc and at the time of writing represents the latest publication about source code summarization in a software engineering venue.
that paper used an lstm based encoder decoder architecture based on a popular guide for building seq2seq nmt systems but used their sbt representation of code instead of the source code only.
for our baseline we use their sbt representation but use the same gru based encoder decoder from our nlp baseline also to ensure an apples to apples comparison.
since the model architecture is the same we can safely attribute performance differences to the input format e.g.
sbt vs. code only .
a third baseline is codenn presented by iyer et al.
.
given the complexity of the approach we used their publiclyavailable implementation.
the original paper describes only applications to sql and c but we noticed that their c parser extracted common code features that are also available in java.
we made small modifications to the c parser so that it would function equivalently for java.
we call our approach ast attendgru in our experiments.
we used a greedy search algorithm for inference for all approaches rather than beam search to minimize the number of experimental variables and computation cost.
c. methodology our methodology to answer both rqs is identical and follows best practice established throughout the literature on nmt see section iii for rq we train our approach and each baseline with the training set from the standard dataset for a total of epochs.
then for each approach we computed performance metrics for the model after each epoch against the validation set.
in all cases validation performance began to degrade after five or six epochs.
next we chose the model after the epoch with the highest validation performance and computed performance metrics for this model against the testing set.
these testing results are the results we report in this paper.
our methodology to answer rq 2differs only in that we trained and tested using the challenge dataset.
we report the performance metric bleu also in keeping with standard practice in nmt.
bleu is a measure of the text similarity between predicted summaries and reference summaries.
we report a composite bleu score in addition to bleu 1through bleu bleu nis a measure of the similarity of n length subsequences versus authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
entire summary sentences .
technically speaking we used nltk.translate.bleu score in our implementation.
d. threats to v alidity the primary threats to validity to this evaluation include our dataset.
we use a very large dataset with millions of java methods in order to maximize the generalizability of our results but the possibility remains that we would obtain different results with a different dataset.
and we did not perform cross validation.
we attempt to mitigate this risk by using random samples to split the training validation testing sets a different split could result in different performance.
this risk is common among nmt experiments due to very high training computation costs hours per epoch .
vii.
e v alua tion results this section discusses our evaluation results and observations.
after answering our research questions we explore examples to give an insight into how the network functions and why it works.
note that we use these observations to build an ensemble method at the end of this paper.
a. rq standard experiment we found in the standard experiment that ast attendgru andattendgru obtain roughly equal performance in terms of bleu score but provide orthogonal results as we will explain in this section and the example in subsection c. in terms of bleu score ast attendgru andattendgru are roughly equal in performance .
bleu vs .
bleu.
sbt is lower at about bleu and codenn is about bleu.
figure includes a table with the full bleu results for each result and additional data in our online appendix .
forsbt the results conflicted with our expectations based on the presenting paper in which sbt outperformed a standard seq2seq model like attendgru .
we see two possible explanations first even though our seq2seq baseline implementation represents a standard approach there are a few architectural differences from the paper by hu et al.
such as different embedding vector sizes.
while we did not model b b 1b 2b 3b dataset ast attendgru .
.
.
.
.
standardattendgru .
.
.
.
.
sbt .
.
.
.
.
codenn .
.
.
.
.
ast attendgru .
.
.
.
.
challenge fig.
below are bleu1 scores and the composite bleu score for each approach and dataset.
above the chart depicts the composite scores only.
we observe that attendgru and ast attendgru perform equally in terms of bleu score on the standard set though we improve it with an ensemble decoder in section viii.observe major changes in the results from these architectural differences in our own pilot studies it is possible that one s mileage may vary depending on the dataset.
second as we note in sections iii and v the previous study did not split by project so methods in the same project will be in the training and test set.
the very high reported bleu scores in could be explained by overloaded methods with very similar structure sbt would detect a function in the test set with a very similar ast to an overloaded method in the same project in the training set.
the improvement by all approaches over codenn matches expectations from previous experiments.
the codenn approach was intended as a versatile technique for both code search and summarization and was a relatively early attempt at applying nmt to the code summarization problem.
in addition it was designed for c and sql datasets we adapted it to java as described in the previous section.
a key observation of the standard experiment is that ast attendgru andattendgru provide orthogonal predictions there is a set of methods in which one performs better and a different set in which the other has higher performance.
while ast attendgru is slightly ahead of attendgru w e do not view a .
bleu difference a major improvement in and of itself.
normally we would expect an approach to outperform a different approach by some margin across a majority of the examples i.e.
non orthogonal performance and this is indeed what we observe when comparing ast attendgru tosbt as shown on the left below around 60k methods in which ast attendgru performed better vs. 20k for sbt but what we observe for ast attendgru andattendgru is that there are two sets of roughly 33k methods in the 91k test set in which one or another approach has higher performance above right .
in other words among the predictions in which there was a difference between the approaches ast attendgru and gives better predictions in terms of bleu score for about half while attendgru performs better on about half.
orthogonal performance makes these two approaches a good candidate for ensemble prediction which we further explain in subsection cand section viii.
b. rq challenge experiment we obtain a bleu score of about .
for ast attendgru in the challenge experiment.
note that the only difference between the standard and challenge experiments is that we trained and tested using the ast only in the form of the sbt ao representation fed to ast attendgru .
technically there are other configurations that would produce the same result such as using sbt ao as input to attendgru instead of the source code.
any of these configurations would meet our objective with this experiment of establishing performance for the scenario when only an ast is available.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
c. explanation and example merely reporting bleu scores leaves an open question as to what the scores mean in practice.
consider these two examples from the standard and challenge experiments method ids align with our downloadable dataset for reproducibility .
we chose the following examples for illustrative purposes and as an aid for explanation.
while relatively short we feel that these methods provide a useful insight into how the models operate.
for a more in depth analysis a human evaluation would be required which is beyond the scope of this paper.
example is one of the cases where ast attendgru succeeds when attendgru fails.
to understand why recall that in our model as with a majority of nmt systems the system predicts a sentence one word at a time.
for each word the model receives information about the method the code text plus the ast for models that use it along with each word that has been predicted so far.
so to predict token example method id public config tokenurl string tokenurl this.tokenurl tokenurl return this reference sets the token url ast attendgru sets the token url attendgru stan.
returns the url of the token sbt sets the unk ast attendgru chal.
sets the value of the unk property tokenized code text input s public config token url string token url this token url token url return this s sbt ao input unit function specifier specifier other type name name other type name name other parameter list parameter decl type name name string type name name other decl parameter parameter list block expr stmt expr name name name other operator operator other name name other name operator operator other name name other expr expr stmt return expr name name other expr return block function unit a attendgru b ast attendgru fig.
heatmaps of the attention layer in a attendgru and b ast attendgru for the code text input for example .
the xaxis is the positions in the summary input.
the y axis is the positions in the code input.
images are truncated to code input length.ast attendgru would receive the code text the ast and the phrase sets the .in contrast attendgru only receives the code text and sets the .
to predict the first word sets attendgru only knows that it is the start of the sentence indicated by a start of sentence s token and the code text input.
to help make the prediction attendgru is equipped with an attention layer learned during training to attend to certain parts of the input.
that layer is depicted in figure a .
note that there is high activation bright yellow in position indicating significant attention paid to location in the code text input this is the word return.
what has happened is that during training the model saw many examples of getter methods that were only a few lines and ended with a return.
in many cases the model could rely on very explicit method names such as getplayerscore method id .
attendgru performed remarkably well in these cases as the situation is quite like natural language it learns to align words in the input vocabulary to words in the target vocabulary and where they belong in a sentence.
however in cases such as example where the method name does not clearly state what the method should do the name tokenurl is not obviously a setter attendgru struggles to choose the right words even if as in example it correctly identifies the subject of the action url of the token .
these situations are where the ast is beneficial.
the code text activation layer for ast attendgru attends heavily to the start of sentence token note column in figure b which since s is the start of every sentence probably acts like a not sure signal.
but the model also has the ast input.
figure shows the ast attention layer of ast attendgru when trying to predict the first word.
there are four areas of interest that help elucidate how the model processes the structure of the model denoted a through d in the figure and color coded to the corresponding areas in the ast input.
first area a is the portion of the method signature prior to the parameter.
recall that our ast representation is structure only so almost all methods will start the same way.
so as expected the attention in area a is largely formless.
the heatmap shows much more definition in area b. it is the parameter list and the model has likely learned that short methods with parameter lists tend to be setters.
the model activates very heavily at locations c and d which are the start and end of the expr stmt ast node.
a very common situation in the training set is that a short method with a parameter and an assignment is a setter.
the model has learned this and chose sets as the first word.
all of the models with ast input correctly chose sets .
sbt found that the method is a setter but could not determine fig.
heatmap of the attention layer in ast attendgru for the ast input for example .
the x axis is the summary input and the y axis is the ast sbt ao input.
high activation more yellow indicates more attention paid to e.g.
position of the ast input.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
what was being set we attribute this behavior to the fact that the sbt representation blends the code text and structural information into a single input which creates a challenge for the model to learn orthogonal types of information in the same vector space which work in other areas e.g.
image captioning implies is not advisable .
while there is not space in this paper to explore fully we note that even ast attendgru during the challenge experiment correctly characterized the method as setting the value of a property generating an unknown token when it could not determine which property.
in fact ast attendgru correctly predicted the first word of the summary which is usually a verb of the time during the challenge experiment compared to of the time in the standard experiment.
briefly consider example example method id public void disconnect try socket.flush socket.close connected false notifydisconnect catch ioexception ex ex.printstacktrace reference closes the socket for reconnection ast attendgru disconnect from the server attendgru stan.
disconnects from the server sbt disconnect from the server ast attendgru chal.
closes the connection all approaches performed well for this method but for different reasons.
attendgru linked the method name to the verb disconnects .
sbt relied more on later features such as the call to notifydisconnect .
most interestingly ast attendgru performed best in the challenge experiment.
in exploring this result we found a few methods with a similar ast ids .
all of these had a few lines in a try block followed by a short catch block and method calls and assignments to null or false in the try.
these methods had summaries like close the communication with the gps device stops the timer and disconnect from the current client all these methods deal with close and cleanup behavior.
the model probably learned this during training and chose similar words for the summary.
in answering rq we found that attendgru and ast attendgru performed better on different sets of methods.
while we are hesitant to overinterpret single examples the examples in this section are consistent with numerous others in the dataset we provide a script for randomly sampling examples called rand samples preds.py in our online appendix for interested readers .
the examples are also consistent with the interpretation that the off the shelf nmt system attendgru performs quite well in cases where the summaries are clear from the method signature and in these cases the ast may be superfluous.
but the model benefits from the ast in cases when words in the code text input are not sufficient or clear.
viii.
e nsemble decoding and future work as a hint toward future work we test a combination of theattendgru andast attendgru models using ensem ble decoding.
the combination itself is straightforward we compute an element wise mean of the output vector of each model the same trained models used in our evaluation .
the training and test procedure does not change except that during prediction we use the maximum value of the combined output vector rather than just one output vector from one model.
this is the same ensemble decoding procedure implemented by opennmt and is one of the most common of several options described by literature on multi source nmt .
since we are combining output vectors the models work together during prediction of every word it is not just choosing one model or another for the whole sentence.
the idea is that one model may assign similar weights in the output vector to two or more words in cases where it performs less well.
and another model that performs better in that situation may assign more weight to a single word.
in our system the hope is that attendgru will contribute more when code text words are clear but ast attendgru will contribute more when they are unclear.
the ensemble decoding procedure improves performance to .
bleu from .
for ast attendgru and .
forattendgru .
this is more than a full bleu point improvement which is quite significant for a relatively simple procedure.
this result points us to future work including more advanced ensemble decoding e.g.
predicting when to use one model or another optimizations to the network e.g.
dropout parameter tuning and critically using different data processing techniques on each type of input.
ix.
c onclusion we have presented a neural model for generating natural language descriptions of subroutines.
we implement our model and evaluate it over a large dataset of java methods.
we demonstrate that our model ast attendgru in terms of bleu score outperforms baselines from se literature and is slightly ahead of a strong off the shelf approach from nlp literature.
we also demonstrate that and ensemble of our approach and the off the shelf nlp approach outperforms all other tested configurations.
we provide a walkthrough example to provide insight into how the models work we conclude that the default nmt system works well in situations where good internal documentation is provided but less well when it is not provided and that ast attendgru assists in these cases.
we demonstrate how ast attendgru can produce coherent predictions even with zero internal documentation.
x. r eproducibility our dataset code models and results are available via acknowledgment this work is supported in part by the nsf ccf ccf1717607 and cns grants.
any opinions findings and conclusions expressed herein are the authors and do not necessarily reflect those of the sponsors authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.