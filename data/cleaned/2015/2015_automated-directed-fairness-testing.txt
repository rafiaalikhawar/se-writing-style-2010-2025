automated directed fairness testing sakshi udeshi singapore univ.
of tech.
and design singapore sakshi udeshi mymail.sutd.edu.sgpryanshu arora bits pilani india pryanshu23 gmail.comsudipta chattopadhyay singapore univ.
of tech.
and design singapore sudipta chattopadhyay sutd.edu.sg abstract fairness is a critical trait in decision making.
as machine learning modelsareincreasinglybeingusedinsensitiveapplicationdomains e.g.educationandemployment fordecisionmaking itiscrucial thatthedecisions computedbysuchmodels are freeofunintended bias.buthowcanweautomaticallyvalidatethefairnessofarbitrarymachine learningmodels?foragivenmachine learningmodeland asetofsensitiveinputparameters ouraeqitasapproachautomatically discovers discriminatory inputs that highlight fairness violation.
at the core of aeqitas are three novel strategies to employ probabilistic search over the input space with the objectiveofuncoveringfairnessviolation.ouraeqitasapproachleveragesinherentrobustnesspropertyincommonmachine learningmodels to design and implement scalable test generation methodologies.
anappealingfeatureofourgeneratedtestinputsisthattheycan besystematicallyaddedtothetrainingsetoftheunderlyingmodel and improve its fairness.
to this end we design a fully automatedmodule that guarantees to improve the fairness of the model.
weimplementedaeqitasandwehaveevaluateditonsixstateof the art classifiers.
our subjects also include a classifier that was designedwithfairnessinmind.weshowthataeqitaseffectively generates inputs to uncover fairness violation in all the subjectclassifiers and systematically improves the fairness of respective modelsusingthegeneratedtestinputs.inourevaluation aeqitas generatesupto70 discriminatoryinputs w.r.t.thetotalnumber of inputs generated and leverages these inputs to improve the fairness up to .
ccs concepts software and its engineering software testing and debugging keywords software fairness directed testing machine learning acm reference format sakshi udeshi pryanshu arora and sudipta chattopadhyay.
.
automated directed fairness testing.
in proceedings of the 33rd acm ieee international conference on automated software engineering ase september3 montpellier france.
acm newyork ny usa 11pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
b a1 b1a2 b2decision boundary figure classifier fairness introduction nondiscrimination is one of the most critical factors for social protection and equal human rights.
the basic idea behind non discrimination is to eliminate any societal bias based on sensi tive attributes such as race gender or religion.
for example itis not uncommon to discover the declaration of following nondiscrimination policy in universities theuniversityiscommittedtoapolicyofequalopportunity for all personsand does not discriminate on the basis of race color national origin age marital status sex sexual orientation gender identity gender expression disability religion height weight orveteranstatusinemployment educationalprograms and activities and admissions due to the massive progress in machine learning in the last few decades itsapplicationhasnowescalatedoveravarietyofsensitivedomains includingeducationandemployment.thekeyinsightisto primarily automate decision making via machine learning models.
on the flip side such models may introduce unintended societalbias due to the presence of bias in their training dataset.
this inturn violates the non discrimination policy that the respective organizationorthenationisintendedtofightfor.thevalidation ofmachine learningmodels tocheckforpossiblediscrimination is therefore critically important.
in this paper we are concerned about the case that any two individuals who are similar with respect to a job at hand should alsobetreatedinasimilarfashionduringdecisionmaking.thus we focus towards individual fairness as it is critical for eliminating societal bias and aim to check for discrimination that might violate individual fairness .
the precise nature of such discrimination dependsonthemachine learningmodelanditsinputfeatures.consequently given amachine learningmodeland theinputfeatures ofthemodel itispossibletosystematicallyexploretheinputspace and discover inputs that induce discrimination.
we call such inputsdiscriminatoryinputs.theprimaryobjectiveofthispaperis authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france sakshi udeshi pryanshu arora and sudipta chattopadhyay to design scalable techniques that facilitate rapid discovery of discriminatory inputs.
in particular given a machine learning model and aset ofdiscriminatory input features e.g.
race religion etc.
our aeqitas approach automatically discovers inputs to clearly highlight the discriminatory nature of the model under test.
as an example consider the decision boundary of a classifier shown in figure .
assume the two points aandbthat differ only in beinggenderaorgenderb.
despite being vastly similar except in the gender aspect the model classifies the points aandbdifferently.ifweconsiderthatsuchaclassifierisusedtopredictthelevel ofsalary thenitcertainlyintroducesunintendedsocietalbiasbasedon gender.
such unfair social biases not only affect the decisions of today but also might amplify it for future generations.
the reason behindthediscrimination i.e.unfairness asshownbetweenpoints aandb can be due to outdated training data that unintentionally introduces bias in certain attributes of the classifier model e.g.
gender in figure .
using our aeqitas approach we automatically discover the existence of inputs similar to aandbwith high probabilities.
these inputs then are used to systematically retrain the model and reduce its unfairness.
thereasonaeqitasworksisduetoitsdirectedstrategyfortest generation.inparticular aeqitasexploitstheinherentrobustness propertyofcommonmachinelearningmodelsforsystematically directingtestgeneration.asaresultofthisrobustnessproperty the modelsshouldexhibitlowvariationintheiroutput s withsmall perturbations in their input s .
for example consider the points a1 anda2whichareintheneighbourhoodofthepoint a.sincethe pointaexhibits discriminatory nature it is likely that both points a1anda2will be discriminatory as reflected via the presence of points b1andb2 respectively.
in our aeqitas approach we firstrandomlysampletheinputspacetodiscoverthepresenceof discriminatoryinputs e.g.point ainfigure1 .then wesearchthe neighbourhoodoftheseinputs asdiscoveredduringtherandom sampling tofindthepresenceofmoreinputs e.g.points a1and a2infigure of the same nature.
an appealing feature of aeqitas is that it leverages the generatedtestinputsandsystematicallyretrainsthemachine learning modelundertesttoreduceitsunfairness.theretrainingmodule is completely automatic and it therefore acts as a significant aidto the software engineers to improve the individual fairness of machine learningmodels.thedirectedtestgenerationandautomated retraining set aeqitas apart from the state of the art in fairnesstesting .whileexistingwork alsoconsiderstestgeneration such tests were generated randomly.
if the discriminatory inputsarelocatedonlyinspecializedlocationsoftheinputspace then random test generators are unlikely to be effective in finding individuals discriminated by the corresponding model.
to this end aeqitasempiricallyvalidatesthatadirectedtestgeneration to uncover the discriminatory input regions is indeed more desir able than random test generation.
moreover aeqitas providesstatistical evidence that if it fails to discover any discriminatory input thenthemachine learningmodelundertestisfairwithhigh probability.
theremainderofthepaperisorganizedasfollows.afterprovidinganoverviewof aeqitas section2 wemakethefollowing contributions we present aeqitas a novel approach to systematicallygenerate discriminatory test inputs and uncover the fairness violation in machine learning models.
to this end we proposethreedifferentstrategieswithvaryinglevelsofcomplexity section .
we present a fully automated technique to leverage the generateddiscriminatoryinputsandsystematicallyretrainthe machine learning models to improve its fairness section .
weprovideanimplementationof aeqitasbasedonpython.
ourimplementationandallexperimentaldataarepublicly available section .
we evaluate our aeqitas approach with six state of theartclassifiersincludingaclassifierthatwasdesignedwith fairness in mind.
our evaluation reveals that aeqitas is effective in generating discriminatory inputs and improving the fairness of the classifiers under test.
in particular aeqitas generated up to discriminatory inputs w.r.t.
the totalnumberofinputsgenerated andimprovedthefairness up to section .
afterdiscussingtherelatedwork section6 weoutlinedifferent threats to validity section before conclusion and consequences section .
background inthissection wewilldiscussthecriticalimportanceoffairness testing and outline the key insight behind our approach.
importanceoffairness theusageofmachinelearningisincreasingly being observed in areas that are under the purview of antidiscrimination laws.
in particular application domains such as law enforcement credit educationandemploymentcanallbenefitfrom machine learning.
hence it is crucial that decisions influenced by any machine learning model are free of any unnecessary bias.
as an example consider a machine learning model that predicts the income levels of a person.
it is possible that such a model was trainedonadataset which inturnwasunfairlybiasedtoacertain genderoracertainrace.asaresult forallequivalentcharacteristics barringthegenderorrace thecreditworthinessofapersonwill be predicted differently by this model.
if financial institutions used suchamodeltodeterminethecreditworthinessofanindividual then individuals might be disqualified only on the basis of their gender or race.
such a discrimination is certainly undesirable as it reinforcesandamplifiestheunfairbiasesthatwe asasocietyare continuously fighting against.
fairness in aeqitas aeqitas aims to discover the violation ofindividual fairness inmachine learning models.
thismeans aeqitasaimstofindinstancesofpairofinputs iandi primethatare classified differently despite being vastly similar.
the similarity between inputs iandi primeis based on a set of potentially discriminatory input parameters see definition .
detecting the violation of individual fairness is challenging.
this is because inputs that arepronetotheviolationofindividualfairnessmightbelocated onlyinspecificregionsoftheinputspaceofamodel.consequently specializedanddirectedtechniquesarerequiredtorapidlylocate these input regions.
this is theprimary motivation behind the developmentof aeqitas.fortherestofthepaper wewillsimply authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated directed fairness testing ase september montpellier france use the term fairness instead of individual fairness in the light of our aeqitas approach see definition .
towards fair machine learning models a naive approach to designfairmachine learning models is to ignore certain sensitive attributes suchas race color religion gender disability orfamily status.itisnaturaltoassumethatifsuchattributesareheldback from decision making then the respective model will not discriminate.
unfortunately such an approach of accomplishing fairness throughblindness fails.thisisbecauseofthepresenceofredundant encoding in the training dataset .
due to the redundant encoding it is frequently possible to predict the unknown sensitive attributesfromotherseeminglyinnocuousfeatures.forexample consider certain ethnic groups in a city that are geographically boundtocertainareas.insuchcases evenifamachine learning model in a financial institute does not use ethnicity as a parameter todecidecreditworthiness itispossibletoguessethnicityfromgeo graphiclocations whichindeedmightbeaparameterforthemodel.
therefore it is critical to systematically test a machine learning model to validate its fairness property.
why fairness testing is different in contrast to classic software testing testingmachine learningmodelsfaceadditionalchallenges.
typically these models are deployed in contexts where the formal specification of the software functionality is difficult to develop.in fact such models are designed to learn from existing data because ofthe challengesin creating amathematical definitionof the desiredsoftwareproperties.moreover anerroneoussoftwarebehaviourcanberectifiedbyretrainingthemachine learningmodels.
however for classic software a software bug is typically fixed via modifying the responsible code.
state of the art in fairness testing the state of the art in systematictestingofsoftwarefairnessisstillatitsinfancy.incontrast to existing work aeqitas focuses on directed test generation strategy.
as evidenced by our evaluation this is crucial to locate specific input regions that violate individual fairness.
to illustrate ourobjective consideramachine learningmodel fanditsinputs i andi prime.idiffersfrom i primeonlyinbeingassignedadifferentvalueina potentially discriminatory input parameter.
for example if gender is the potentially discriminatory input parameter then iwill be differentfrom i primeonlyinbeing genderaorgenderb.weareinterestedtodiscoverinputs iori prime wherethedifferenceinoutputsof the model captured via f i f i prime is beyond a pre determined threshold.
we call such inputs iori primeto bediscriminatory inputs for the model f. it is important to note that the discrimination thresholdand thepotentially discriminatoryinputparameters are supplied by the users of our tool.
in the preceding example the potentially discriminatory input parameter i.e.
gendercan be specified by the user.
similarly users can also fine tune the value at which f i f i prime is considered to be discriminatory.
robustness in machine learning robustness is a notion that saysthattheoutputofamachine learningmodelisnotdramatically affected by small changes to its input .
assume a model f let ibe the input to fand be a small value.
if fis robust then f i f i .
nevertheless existing techniques provide evidence to find inputs that violate this robustness property.
such inputs are called adversarial inputs .
however adversarial inputs generallycoveronlyasmallfractionoftheentireinputspace.thisisevidentbythefactthatadversarialinputsneedtobecraftedusing very specialized techniques.
additionally aeqitas is designed to avoidtheseadversarialinputregionsbysystematicallydirectingthe test generators.intuitively aeqitasachievesthis by reducingthe probability to explore an input region when tested inputs from the regiondidnotexhibitdiscriminatorynature seealgorithm 2for details .consequently ifadversarialornon robustinputregions do not exhibit discriminatory nature such regionswill eventually be explored only with very low probability.
approach at a glance we propose design and evaluate three schemes with varying levels of complexities to systematically uncover software fairness problems.
the crucial components of our approach are outlined below.
globalsearch inthefirststepofallourproposedschemes weuniformlysampletheinputsandrecordthediscriminatoryinputsthat we find.
in the light of uniformly sampling the input space we can guarantee with very high probability to discover a discriminatory input if such an input exists.
for instance figure a highlights the probability of finding a discriminatory input in an input space with only discriminatory inputs.
therefore if discriminatory inputs exist the first step of our proposed schemes guarantee to find at least one such input with high probabilities.local search the second step of our proposed schemes share the following hypothesis if there exists a discriminatory input i i where icaptures the input domain then there exist more discriminatoryinputs inthe inputspacecloser to i.the inputdomain ican be considered as the cartesian product of the domain of ninput parameters say p1 p2 ... pn.
we assume ikcaptures the domain ofinputparameter pk.therefore i i1 i2 ... in.aninput parameter p uniontext.1n i 1pican be potentially discriminatory if the outputofthemachine learningmodelshouldnotbebiasedtowards specificvaluesin ip.withoutlossofgenerality weassumeasubset of parameters pdisc uniontext.1ni 1pito be potentially discriminatory.
for an input i i w eu s e ikto capture the value of parameter pkwithininput i.basedonthisnotion weexplorethefollowing methods to realize our hypothesis.our methods differ on how we systematicallyexploretheneighbourhoodofadiscriminatoryinput i d .i d in turn was discovered in the first step of aeqitas.
first a parameter p uniontext.1ni 1pi pdiscis randomly chosen.
thenasmallperturbation i.e.change isaddedto i d p.typically as we consider integer and real valued input parameters in our evaluation.
inthesecondmethod weassignprobabilitiesonhowtoperturbachosenparameter.aspecificparameter p uniontext.1n i 1pi pdiscis still chosen uniformly at random.
however if a givenperturbation ofi d pconsistentlyyieldsdiscriminatoryinputs thentheperturbation isemployedwithhigher probability.since typicallybelongstoasmallsetofvalues such a strategy works efficiently in practice.
thethird methodaugments thesecondmethod byrefining probabilities to perturb an input parameter.
concretely if authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france sakshi udeshi pryanshu arora and sudipta chattopadhyay .
.
.
.
iterationsprobability of finding a discriminatory input a b figure a probability of finding discriminatory inputs b estimation of the percentage of discriminatory inputs x i1 ik im in i and i only differ herei i neighbourhood of i is obtained by adding small change to an input parameterpdisc x i1 ik im in i and i only differ herepdisc x i1 ik im in... x in ik im i1 figure our aeqitas approach at a glance perturbingthevalueofparameter p uniontext.1n i 1pi pdiscconsistentlyyieldsdiscriminatoryinputs thentheparameter pwill be significantly more likely to be chosen for perturbation.
ourproposedmethodologiesarefullyautomated theydonotrequirethesourcecodeofthemodelsandworkefficientlyinpractice for state of the art classifiers.
figure 3illustrates aeqitas approach when iandi primewere discovered in the first step.
then the second step explored the neighbourhood of iby adding small changes to an input parameter.
estimation of discriminatory inputs an appealing feature of aeqitas is that we can estimate the percentage of discriminatory inputsin i.tothisend weleveragethelawoflargenumbers lln inprobabilitytheory.inparticular wegenerate kinputsuniformly atrandomandcheckwhethertheycanleadtodiscriminatoryinputs.
assume that k prime kinputs turn out to be discriminatory.
we compute theratiok prime kover a largenumber of trials.accordingto lln the average of these ratios closely approximates the actual percentageofdiscriminatoryinputsin i.figure2 b highlightssuch convergence after only trials when kwas chosen to be .
why aeqitas works?
thereasonaeqitasworksisbecause oftherobustnesspropertyofcommonmachine learningmodels.in particular ifweperturbtheinputtoamodelbysomesmall then the output is not expected to change dramatically.
as we expectthe machine learning models under test to be relatively robust we can leverage their inherent robustness property to systematically generate test inputs that exhibit similar characteristics.
in our aeqitas approach we focus on the discriminatory nature of a given input.
we aim to discover more discriminatory inputs in thetable notations used in aeqitas approach nthe number of input parameters to the machinelearning model under test ithe input domain of the model pithei th input parameter of the model pset of all input parameters i.e.
p uniontext.1n i 1pi pdiscsetofsensitiveorpotentiallydiscriminatoryinputparameters e.g.
gender .
clearly pdisc uniontext.1n i 1pi ipthe value of input parameter pin inputi i a pre determined discrimination threshold proximityofanalreadydiscovereddiscriminatoryinputleveraging the robustness property.
how aeqitas can be used to improve software fairness?
we have designed a fully automated module that leverages on the discriminatory inputs generated by aeqitas and retrains the machine learningmodelundertest.weempiricallyshowthatsuch astrategyprovidesusefulcapabilitiestoadeveloper.specifically our aeqitas approach automatically improves the fairness of machine learningmodelsviaretraining.forinstance incertaindecision tree classifiers our aeqitas approach reduced the fraction of discriminatory inputs up to .
detailed approach inthissection wediscussouraeqitasapproachindetail.tothis end we will use the notations captured in table .
our approach revolves around discovering discriminatory inputs via systematic perturbation.
we introduce the notion of discriminatory inputs and perturbation formally before delving into the algorithmic details of our approach.
definition .
discriminatory input and fairness letf be a classifier under test be the pre determined discrimination threshold e.g.
chosen by the user and i i. assumei prime isuch that there exists a non empty set q pdiscand for all q q iq nequali primeq and for all p p q ip i primep.i f f i f i prime theniis called a discriminatory input of the classifier fand is an instance that manifests the violation of individual fairness in f. authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated directed fairness testing ase september montpellier france model local search local exp disc inpsglobal search global exp testfairness improvement moduletraining dataimproved model threshold pdisc figure an overview of our aeqitas approach definition .
perturbation we define perturbation as a function i p pdisc iwhere captures the set of directions to perturb an input parameter.
if i prime i p wherei i p p pdiscand theni primep ip and for all q p p we have i primeq iq.
it is worthwhile to mention that the set of directions to perturb aninputparameter i.e.
caneasilybeextendedwithmorepossibilitiestoperturb.besides itcanalsobecustomizedwithrespect to different input parameters.
however for the sake of brevity we will stick with the simplified version stated in definition .
an overview of our overall approach appears in figure .
the maincontributionofthispaperisanautomatedtestgeneratorto discoverfairnessviolation.thisinvolvestwostages globalsearch global exp and local search local exp over the input domain i. optionally the generated test inputs can be leveraged to retrain the model under test and improve fairness.
in the following we will describe the crucial components of our aeqitas approach as shown in figure .
.
global search themotivationbehindourglobalsearch cf.procedureglobal exp in algorithm is to discover some points in ithat can be used todriveourlocalsearchalgorithm.tothisend wefirstselectan inputirandomlyfromtheinputdomain.input i then isusedto generateasetofinputsthatcoverallpossiblevaluesofsensitive parameters pdisc p. this leads to a set of inputs i d .
we note thatthesetofsensitiveparameters e.g.race religion gender pdisc typically has a small size.
therefore despite the exhaustive nature ofgenerating i d thisispracticallyfeasible.finally wediscover the discriminatory inputs cf.
definition within i d and use the resulting discriminatory input set for further exploration during our local search over i. .
local search inthistestgenerationphase wetaketheinputsgeneratedbyour global search i.e.
disc inputs and then search in the neighbourhood ofdisc inputsto discover other inputs with similar characteristics cf.
procedure local exp in algorithm .
our search strategy is motivated from the robustness property inherent inalgorithm global search procedure global exp p pdisc disc inps nis the number of trials in global search foriin n do select an input i iat random i d extendsiwith all possible values of pdisc i d i prime p p pdisc.ip i primep if i i prime i d f i f i prime then disc inps disc inps i end if end for returndisc inps end procedure algorithm local search procedure local exp disc inps p pdisc v pr test letp prime p pdisc let pr p prime for allp p prime let v .
for allp p prime fori disc inpsdo nis the number of trials in local search foriin n do selectp p primewith probability pr select with probability v note that iis modified as a side effect of modifying ip ip ip i d extendsiwith all values of pdisc i d i prime p p pdisc.ip i primep if i i prime i d f i f i prime then add the perturbed input i test test i end if update prob i p test v pr end for end for returntest end procedure common machine learning models.according to the notionof robustness the neighbourhood of an input should produce similar output.
therefore it becomes logical to search the neighbourhood ofdisc inputs as these are the discriminatory inputs and their neighbourhood are likely to be discriminatory for robust models.
tosearchtheneighbourhoodof disc inputs aeqitasperturbs an inputi disc inputsby changing the value of some parameter p p pdisc i.e.ip .
the value of the parameter pis perturbed by .
we note that as a side effect of changing ip input iis automatically modified.
this modified version of iis further perturbedinsubsequentiterationsoftheinnerloopinalgorithm .
our aeqitas approach chooses a parameter p p pdiscwith probability pr cf.algorithm .forall p p pdisc initially pr was assigned to1 p pdisc .
oncepis chosen its value is perturbed by with probability v and by with probability1 v .
v isinitializedto0 .5forallparameters inp p pdisc.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france sakshi udeshi pryanshu arora and sudipta chattopadhyay algorithm aeqitas semi directed update probability procedure update prob i p test v pr if i test i nelementtest then v min v v end if if i nelementtest i test then v max v v end if end procedure algorithm aeqitas fully directed update probability procedure update prob i p test v pr if i test i nelementtest then v min v v end if if i nelementtest i test then v max v v end if ifi testthen pr pr pr pr pr summationtext.
x p pdisc pr for allp p pdisc end if end procedure aeqitas employs three different strategies namely aeqitas random aeqitassemi directedandaeqitasfully directed to update the probabilities in prand v. this is to direct the test generation process with a focus on discovering discriminatory inputs.
in the following we will outline the different strategies implemented within aeqitas.
aequitas random .aeqitas random does not update the initial probabilitiesassignedto prand v.thisresultsin i.e.perturbation value and p i.e.
the parameter to perturb both being chosen randomly.
intuitively aeqitas random explores inputs around the neighbourhood of disc inputs i.e.
set of discriminatory inputs discoveredviaglobalsearch uniformlyatrandom.nevertheless aeqitasrandomempiricallyoutperformsapurelyrandomsearch over the input space.
this is because it still performs a random searchinaconstrainedinputregion specifically theinputregion that already contains discriminatory inputs.aequitas semi directed .aeqitas semi directed drives the test generation bysystematically updating v i.e.
theprobabilities to perturbthevalueofaninputparameterby cf.algorithm .
theparameter p toperturb isstillchosenrandomly.initially we choose where the probability that 1i s v and the probability that 1i s1 v .
if the perturbed input isdiscriminatory cf.definition thenweincreasetheprobability associated with v by a pre determined offset v. otherwise v isreducedbythesameoffset v.intuitively theupdatesto probabilities in vprioritise a direction when the respective direction results in discriminatory inputs.
aequitas fully directed .aeqitas fully directed extends aeqitas semi directed by systematically updating the probabilities to choose a parameter for perturbation.
to this end we updateprobabilities in prduring the test generation process cf.
algorithm4 .
assume we pick a parameter p p pdiscto perturb.
initially we have pr p pdisc .
if the perturbation of the givenparameter pby resultsinadiscriminatoryinput thenwe add a pre determined offset prto pr .
to reflect this change inprobability we normalize pr to pr summationtext.
x p pdisc pr forevery p prime p pdisc.
intuitively the updates to probabilities in prprioritize a parameter when perturbing the respective parameter results in discriminatory inputs.
.
estimation using lln anattractivefeatureof aeqitasisthatwecanestimatethepercentage of discriminatory inputs in ifor any given model.
we leverage the law of large numbers lln from probability theory to accomplish this.
let be an experiment.
in this experiment we generateminputsuniformlyatrandom.theseareindependentand identically distributed iid samples i1 i2...im.
we execute these inputsandcountthenumberofinputsthatarediscriminatoryin nature.let m primebethenumberofinputsthatarediscriminatory.
then outputs the percentage m m prime m. is conducted ktimes.in eachinstance ofthe experiment we collecttheoutcome m1 m2...mk.letm k summationtext.1k i 1mi.according to lln the average of the results i.e.
m obtained from a large number of trials should be close to the expected value and it will tendtobecomecloserasmoretrialsareperformed.thisimpliesas k m m wherem isthetruepercentageofthediscriminatoryinputspresent inifor the machine learning model under test.
this phenomenon was observed in our experiments.
figure b shows that the m converges only after trials i.e.
k .
.
improving model fairness it has been observed that generated test inputs showing the violation of desired properties in machine learning models can be leveraged for improving the respective properties.
this was accom plishedviaaugmentingthetrainingdatasetwiththegeneratedtest inputs and retraining the model .
hence we intend to evaluate the usefulness of our generated testinputstoimprovethemodelfairnessviaretraining.tothisend aeqitas has a completely automated module that guarantees reductionofthe percentageofdiscriminatoryinputs in i.weachieve this by systematically adding portions of generated discriminatory inputs to the training dataset.
assumetestbe the set of discriminatory inputs generated by aeqitas.aeqitasiseffectiveingeneratingdiscriminatoryinputs and the size of the set testis usually large.
a naive approach to retrainthemodelwillbetoaddallgenerateddiscriminatoryinputs tothetrainingdataset.suchanapproachislikelytofailtoimprove the fairness of the model.
this is because the generated test inputs are targeted towards finding discrimination and are unlikely to follow the true distribution of the training data.
therefore blindly addingallthetestinputstothetrainingsetwillbiasitsdistribution towards the distribution of our generated test inputs.
to solve this authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated directed fairness testing ase september montpellier france algorithm retraining procedure retraining f test training data n fcur f foriin n do pi a random real number between 2i 2i ifpi 100then exit the loop end if k len training data naddn pi k tdaddn randomly selected naddninputs from test tdnew training data tdaddn fnew model trained using tdnew estimate the number of discriminatory inputs section .
faircur lln fairness estimation fcur fairnew lln fairness estimation fnew if faircur fairnew then fcur fnew else exit the loop end if end for returnfcur end procedure challenge itisimportantthatonlyportionsofdiscriminatoryinputs fromtestare added to the training dataset.
letpibe the percentage with respect to the size of the training data that we choose at any given iteration i. if size of training dataism thenweselectpi m 100discriminatoryinputsfrom testat randomandaddthesediscriminatoryinputstothetrainingdataset.
fori wesetpirandomlyinarangebetween .
theintuitionbehindthisistofindanefficientmechanismtosystematically add inputs from testto the training dataset and to approximate the optimal reduction in discriminatory inputs.
we terminatetheprocesswhenaddinginputsfrom testtothetraining datasetdoesnot decreasetheestimatedfraction ofdiscriminatory inputsin i.thecurrentlytrainedmodel i.e.
fcurinalgorithm is then taken as the improved model with better individual fairness score.
in this way we can guarantee that our retraining process always terminates with a reduction in discriminatory inputs.
ourretraining strategyis designedto befast withoutsacrificing the fairness significantly.
our main objective is to demonstrate that aeqitas generated test inputs can indeed be used by the developers to improve the individual fairness of their models.
the amount of added test inputs generated by aeqitas is chosen fromexponentiallyincreasingintervals i.e.theinterval inalgorithm .suchastrategyistakentoquicklyscopethesensitivityofthemodelwithrespecttothegeneratedtestdata.moreover bychoosingarandomnumber piintheinterval wetrynottoovershoot the value of piby a large margin that causes the optimal reductionofdiscriminatoryinputsin i.asaresult ourproposed retraining strategy maintains a balance between improving model fairness and the efficiency of retraining.
it is well known that adding more data to a machine learning algorithm is likely to lead to increased accuracy .
a relevantchallengehereisattributedtothelabelingofthegeneratedtestdata.
there exists a number of effective strategies to tackle this problem.
one such strategy is finding the label via a simple majority of anumberofclassifiers .majorityvotinghasbeenshowntobe veryeffectiveforawiderangeofproblems andwebelieveit shouldbereadilyapplicableinourcontextofimprovingfairness aswell.nevertheless testdatalabelingisanorthogonalproblem inthedomainofmachinelearningandweconsiderittobebeyond the scope of the problem targeted by aeqitas.
.
termination aeqitascanbeconfiguredtohavevariousterminationconditions dependingontheparticularusecaseofthedeveloper.inparticular aeqitascanbeterminatedwiththefollowingpossibleconditions aeqitascanterminateafterithasgeneratedauserspecified number of discriminatory inputs from i. this feature canbeusedwhenacertainnumberofdiscriminatoryinputs need to be generated for testing evaluation or retraining of the model.
aeqitascanalsoterminatewithinagiventimebound.thisisusefultoquicklycheckifthemodelexhibitsdiscrimination for a particular set of sensitive parameters.
inourevaluation weusedboththeterminationcriteriatoevaluate the effectiveness and efficiency of aeqitas.
results experimentalsetup .weevaluateaeqitasacrossawidevariety ofclassifiers includingaclassifierwhichwasdesignedtobefair.some salient features of these classifiers are outlined in table .
in particular fair svm cf.
table was specifically designed with fairnessinmind .therestoftheclassifiersundertestarethe standard implementations found in python s scikit learn machine learning library.
these classifiers are used in a wide variety of applications by machine learning engineers across the world.
otherthanfairsvm wehaveusedscikit learn ssupport vectormachines svm multi layerperceptron mlpc random forestanddecisiontreeimplementationsforourexperiments.we alsoevaluateanensemblevotingclassifier ensemble inwhich wetakethecombinationoftwoclassifierpredictions.theclassifiersweusearerandomforestanddecisiontreeestimators cf.
table2 .
table subject classifiers used to evaluate aeqitas classifier name lines of python code input domain fair svm 106svm mlpc random forest decision tree ensemble allclassifierslistedin table2areusedforpredictingtheincome.
these classifiers are trained with the data obtained from the us census .thesizeofthistrainingdatasetisaround32 .we train all the six classifiers on this training data.
the objective is to classify whether the income of an individual is above capturedviaclassifieroutput orbelow capturedviaclassifier output .foralltheclassifiers setofdiscriminatoryparameters authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france sakshi udeshi pryanshu arora and sudipta chattopadhyay figure the effectiveness of aeqitas i.e.pdiscis thegenderof an individual.
the threshold value for identifying a discriminatory input is set to zero.
this means if i differsfrom i primeonlyinbeing genderaorgenderb theniori primeare discriminatoryinputsofaclassifier fwhen f i f i prime .in our experiments we set the perturbation and both vand pras .
.
these are user defined variables that guide ouraeqitasapproach.inparticular thesevariablesareusedto systematicallyrefinetheprobabilitiestochooseaninputparameter to perturb and to choose a perturbation value cf.section .
we implement aeqitas in python as it is a popular choice oflanguageforthedevelopmentofmachine learningmodelsand related applications.
the implementation is around lines of pythoncode.allourexperimentswereperformedonanintel i7 processor having 64gb of ram and running ubuntu .
.
keyresults .weusethreedifferenttestgenerationmethodologies namely aeqitas random aeqitas semi directed and aeqitas fully directed.
these methodologies differ with respect to the increasing levels of sophistication in systematically searching the input space cf.
section .
.
in particular aeqitas fully directed involves the highest level of sophistication in searching the inputspace.asexpected aeqitasfully directedconsistentlyoutperforms the aeqitas random and aeqitas semi directed as observed from figure .
however aeqitas fully directed and aeqitassemi directeddemandmorecomputationalresourcesper unit time than aeqitas random.
as a result aeqitas random is moreappropriatetouse ascomparedtotherestofourapproaches fortesting withlimited computationalresourcesperunit time.the test subject used in figure 5was the fair svm cf.
table .
toillustratethepowerofouraeqitasapproachoverthestateof the art fairness testing we also compare our approaches withthestate of the art which inturniscapturedvia random infigure .
it is evident that even the least powerful technique implementedwithinouraeqitasapproach i.e.aeqitasrandom significantlyoutperformsthestate of the art.inourevaluation we discoveredthataeqitasismoreeffectivethanthestate of the art randomtestingbyafactorof9.6onaverageanduptoafactorof20.
.
we measured the effectiveness via the number of discriminatory inputs generated by a test generation technique.
aeqitas also providescapabilitiestoautomaticallyretrainamachine learning modelwiththeobjectivetoreducethenumberofdiscriminatory inputs.tothisend aeqitasreducedthenumberofdiscriminatory inputs by .
on average with a maximum reduction of .
.rq1 how effective is aeqitas in finding discrimi natory inputs?
weevaluatethecapabilityof aeqitasineffectivelygenerating discriminatory inputs.
for all the subject classifiers we measure the effectiveness of our test algorithms via the number of discriminatory inputs generated with respect to the number of total inputs generated.
apurelyrandomapproachisnoteffectiveingeneratingdiscriminatory inputs.as observedfrom figure5 thenumber ofdiscriminatory inputs generated by such an approach does not increase rapidlyoverthenumberofinputsgenerated.thisisexpected as a purely random approach does not incorporate any systematic strategytodiscoverinputsviolatingfairness.theineffectivenessofrandomtestingpersistsacrossallthesubjectclassifiers asobserved intable .
as observed from table all test generation approaches implementedwithinaeqitasoutperformapurelyrandomapproach.
inparticular therateatwhichouraeqitasapproachgenerates discriminatory inputs is significantly higher than a purely random approach.
as a result aeqitas provides scalable and effective techniquefor machinelearningengineers whoaimto rapidlydiscover fairness issues in their models.
aeqitas random aeqitas semi directed and aeqitas fully directed involve increasing level of sophistication in directing the test input generation.
as a result aeqitas fully directed approach performs the best among all our testgenerators.inparticular aeqitassemi directedisonanaverage .
and up to .
better than aeqitas random.
finally aeqitas full directed is on an average .
and up to .
better than aeqitas semi directed.
by design aeqitas does not generate any false positives.
this meansthatanydiscriminatoryinputgeneratedbyaeqitasare indeeddiscriminatorytothemodelundertest subjecttothechosen threshold of discrimination.
finding aeqitas fully directed approach outperform apurelyrandomapproachuptoafactorof20.4interms ofthenumberofdiscriminatoryinputsgenerated.italso performs up to .
better than aeqitas semi directed which in turn performs up to .
better than aeqitas random our least sophisticated approach.
rq2 how efficient is aeqitas in finding discriminatory inputs?
table 4summarizes how muchtime eachof themethods takes to generate discriminatory inputs.
on an average aeqitas random performs .
faster than the state of the art.
the improvement in aeqitas fully directed is even more profound.
on an average aeqitas fully directed is .
faster than the state oftheart withamaximumimprovementof96.
inthecaseof multi layer perceptron.
it is important to note that the reported time in table 4includes both the time needed for test generation and for test execution.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated directed fairness testing ase september montpellier france table effectiveness of aeqitas approach classifier random aeqitas randomaeqitas semi directedaeqitas fully directed discriminatory input discriminatory input inputs generated discriminatory input inputs generated discriminatory input inputs generated fair svm .
.
.
.
svm .
.
.
.
mlpc .
.
.
.
random forest .
.
.
.
decision tree .
.
.
.
ensemble .
.
.
.
table test generation efficiency classifier randomaeqitas randomaeqitas semi directedaeqitas fully directed fair svm .87s .47s .65s .14s svm .54s .9s .8s .21s mlpc .23s .63s .76s .87s random forest .12s .98s .67s .34s decision tree .32s .13s .89s .25s ensemble .79s .45s .75s .43s hence thereportedtimeishighlydependentontheexecutiontime of the model under test.
finding aeqitasfully directedis83.
fasterthanthe stateoftheart withamaximumimprovementof96.
in the case of multi layer perceptron.
rq3 howusefularethegeneratedtestinputstoimprove the fairness of the model?
table retraining effectiveness classifierestimated of disc input ini confidence interval impr inps added before retrainingafter retraining fair svm .
.
.
.
.
.
.
.
svm .
.
.
.
.
.
.
.
mlpc .
.
.
.
.
.
.
.
random forest .
.
.
.
.
.
.
.
decision tree .
.
.
.
.
.
.
.
ensemble .
.
.
.
.
.
.
.
aeqitas has a completely automated module which guarantees a decrease in the percentage of discriminatory inputs in i. the discriminatory inputs as discovered by aeqitas were systematicallyaddedtothetrainingdataset cf.
section4.
.theresultsof retraining the classifiers appear in table .
in general retraining the classifiers is not significantly time consuming.
in particular eachclassifierwasretrainedwithinanhour.forsomeclassifiers such as the svm our retraining scheme only took a few minutes.
we leverage the law of large numbers lln from statistical theory to estimatethe percentage ofdiscriminatory inputs in i cf.
section .
.
in particular we randomly sample a large numberof inputs from iand compute the ratio of discriminatory inputs to the total inputs sampled.
this experiment is repeated a large numberoftimesandtheaverageofthecomputedratioisusedas theestimateforthepercentage ofdiscriminatoryinputs in i.w e note from statistical theory that as the number of experiment is repeatedalargenumberoftimes theaverageofthecomputedratio shouldbeclosetotheexpectedfractionofdiscriminatoryinputs ini.
we also compute the confidence interval estimate for thepercentage of discriminatory inputs in i. it is useful to note that theseintervalsarefairlytightandthataddstotheconfidencewe have in our point estimates as well.
as observed from table aeqitas is effective in reducing the percentageofdiscriminatoryinputsin iforalltheclassifiersunder test.
specifically we observe an average improvement of .
in terms of reducing the discriminatory inputs.using our retraining module we added an average of only datapoints .
of the original training data to achieve the result obtained in table .
finding retrainingusingaeqitaslowersthediscriminationpercentagein ibyanaverageof43.
andupto .
.
related work in this section we review the related literature and position our work on fairness testing.
fair machinelearning models themachinelearningresearch communityhaveturnedtheirattentionondesigningclassifiersthat avoid discrimination .
these works primarily focus onthetheoreticalaspectsofclassifiermodelstoachievefairness in theclassification process.
sucha goal iseither achieved bypreprocessing training data or by modifying existing classifiers to limitdiscrimination.ourworkiscomplementarytotheapproaches that aim to design fair machine learning models.
we introduce an efficientwaytosearchtheinputdomainofclassifierswhosegoal is to achieve fairness in decision making.
we wish to provide a mechanismfortheseclassifierstoquicklyevaluatetheirfairness propertiesandhelpimprovetheirfairnessindecisionmakingvia retraining if necessary.
fairness testing from the software engineering point of view the research on validating the fairness of machine learning models isstillatitsinfancy.arecentwork alongthislineofresearch defines software fairness and discrimination including a causalitybased approach to algorithmic fairness.
however in contrast to our aeqitas approach the focus of this work is more on defining fairness and tests were generated in random .
in particular aeqitascanbeusedasadirectedtestgenerationmoduletouncover discriminatoryinputs anddiscoveryof theseinputsis essentialto understand individual fairness of a machine learning model.
in additiontothisandunlikeexistingapproach aeqitasprovides amoduletoautomaticallyretrainthemachine learningmodelsand reduce discrimination in the decisions made by these models.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france sakshi udeshi pryanshu arora and sudipta chattopadhyay testing and verification of machine learning models deepxplore is a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple deep neural networks dnns .
the neuron coverage was usedasasystematicmetricformeasuringhowmuchoftheinternal logic of a dnns have been tested.
more recently deeptest leveragesmetamorphicrelationstoidentifyerroneousbehaviorsin adnn.theusageofmetamorphicrelationssomewhatsolvesthe limitationof differential testing especiallyto liftthe requirement of having multiple dnns implementing the same functionality.
finally a feature guided black box approach is proposed recently to validate the safety of deep neural networks .
this work uses their method to evaluate the robustness of neural networks in safety critical applications such as traffic sign recognition.
theobjectiveoftheseworks asexplainedintheprecedingparagraph is largely to evaluate the robustness property of a given machine learning model.
in contrast we are interested in the fairness property which is fundamentally different from robustness.
therefore validatingfairnessrequiresspecialattentionalongthe line of systematic test generation.
searchbased testing search basedtestinghasalongandvaried history.
the most common techniques are hill climbing simulated annealing and genetic algorithms .
these have been applied extensivelytotestapplicationsthatlargelyfallintheclassofdeterministic software systems.
aeqitas is the first instance in our knowledgethatemploysanovelsearchalgorithmtotestthefairness of machine learning systems.
we believe that we can port aeqitas for the usage in a much wider machine learning context.
threats to validity the effectiveness and efficiency of aeqitas critically depends on the following factors robustness ouraeqitasapproachisbasedonthehypothesis that the machine learning models under test exhibit robustness.
thisisareasonableassumption asweexpectthemodelsundertest to be deployed in production settings.
as evidenced by our evaluation aeqitas approach which is based on the aforementioned hypothesis was effective to localize the search in the vicinity of discriminatory input regions for state of the art models.
trainingdataandaccesstomodel aeqitasneedsaccesstothe training data and the training mechanism of the machine learning modeltobeabletoevaluateandretrainthemodel.withoutaccesstothetrainingdata aeqitaswillnotbeabletosuccessfullyimprove the fairness of the model.
this is because aeqitas is used to generate test inputs that violate fairness and augment the original training set to improve the model under test.
the generated test inputs however isnotsufficienttotrainamachine learningmodel from scratch.
inputstructure aeqitasworksonreal valuedinputs.aeqitas initscurrentform doesnothandleimage soundorvideoinputs.
this however does not diminish the applicability of aeqitas.
numerousreal worldapplicationsstilluseonlyreal valueddatafor prediction.
these include applications in finance security social welfare education healthcareandhuman resources.examples of applicationsincludeincomeprediction crimeprediction disease prediction jobshort listingandcollegeshort listing amongothers.for models that take inputs such as images and videos we need toincorporateadditionaltechniquesforautomaticallygenerating valid input data.
however we believe that the core idea behind our aeqitas approach namely the global and the local search employed over the input space will still remain valid.
probabilitychangeparameter theusersof aeqitaswillhave to experiment and carefully choose vand prvalues which change the probabilities of choosing p i.e.
the input parameter toperturb and i.e.theperturbationvalue .if v respectively pr istoohigh thenanovershootmightoccurandacertaindiscriminatoryinputregionmayneverbeexplored.if v respectively pr istoolow thentheeffectivenessof aeqitassemi directed and aeqitas fully directed would be very similar to aeqitas random.
in our experiments we evaluated with a few vand pr values before our results stabilized.
limited discriminatory input features we evaluate aeqitas with discriminatory input feature gender.
hence we cannot conclude the effectiveness of aeqitas for other potentially discriminatory input features.
however the mechanism behind aeqitas is generic and allows extensive evaluation for other discriminatory input features in a future extension of the tool.
conclusion inthispaper weproposeaeqitas afullyautomatedanddirected test generation strategyto rapidlygenerate discriminatoryinputs in machine learning models.
the key insight behind aeqitas is to exploit the robustness property of common machine learningmodelsanduseittosystematicallydirectthetestgeneration process.aeqitasprovidesstatisticalevidenceonthenumberof discriminatoryinputsinamodelundertest.moreover aeqitas incorporatesstrategiestosystematicallyleveragethegeneratedtestinputs to improve the fairness of the model.
we evaluate aeqitas withstate of the artclassifiersanddemonstratethataeqitasis effective in generating discriminatory test inputs as well as im proving the fairness of machine learning models.
at its current state however aeqitasdoesnothavethecapabilitytolocalize the cause of discrimination in a model.
further work is required to isolate the cause of discrimination in the model.
aeqitas provides capabilities to lift the state of the art in testingmachine learningmodels.weenvisiontoextendouraeqitasapproach beyond fairness testing and for machine learning models taking complex inputs including images and videos.
we hope that thecentralideabehindouraeqitasapproachwouldinfluencetherigoroussoftwareengineeringprinciplesandhelpvalidatemachine learningapplicationsusedinsensitivedomains.forreproducibility and advancing the state of research we have made our tool and all experimental data publicly available acknowledgment theauthorswouldliketothankchundongwangandtheanonymousreviewersfortheirinsightfulcomments.thefirstauthoris supported by the president s graduate fellowship funded by the ministry of education singapore.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated directed fairness testing ase september montpellier france