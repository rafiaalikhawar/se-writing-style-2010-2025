programming bots by synthesizing natural language expressions into api invocations shayan zamanirad boualem benatallah moshe chai barukh fabio casati and carlos rodriguez school of computer science and engineering university of new south wales unsw sydney nsw fshayanz boualem mosheb crodriguezg cse.unsw.edu.au university of trento italy tomsk polytechnic university russia fabio.casati unitn.it abstract at present bots are still in their preliminary stages of development.
many are relatively simple or developed ad hoc for a very specific use case.
for this reason they are typically programmed manually or utilize machine learning classifiers to interpret a fixed set of user utterances.
in reality real world conversations with humans require support for dynamically capturing users expressions.
moreover bots will derive immeasurable value by programming them to invoke apis for their results.
today within the web and mobile development community complex applications are being stringed together with a few lines of code all made possible by apis.
yet developers today are not as empowered to program bots in much the same way.
to overcome this we introduce botbase a bot programming platform that dynamically synthesizes natural language user expressions into api invocations.
our solution is two faceted firstly we construct an api knowledge graph to encode and evolve apis secondly leveraging the above we apply techniques in nlp ml and entity recognition to perform the required synthesis from natural language user expressions into api calls.
i. i ntroduction the api economy is in full swing.
applications are rapidly developed by composing apis.
companies increasingly structure their development and internal systems in terms of apis and business strategies for many companies revolve around apis.
this novel economy facilitates the creation of new business models and opens many revenue opportunities .
technological advances the cloud awareness at the cxo level starting from the famous memo by jef bezos and the increased need for business agility are making applications based on composing apis mainstream.
the internet of things is pushing this trend even more by providing api access to all sort of devices so that we can use apis not only to place an order but also to make coffee.
this paper leverages the api economy in combination with advances in bots technology to facilitate the development of intuitive computing solutions that connect user needs expressed in natural language with invocations of the apis that can fulfill these needs.
intuitive computing envision a collaboration between users and an intelligent digital assistant that models user intent and that suggests or carries out actions likely to satisfy that intent .
developing applications that enable such kind of collaboration today is facilitated by the many bot builders frameworks and systems provided by nearly everybig it player such as facebook google microsoft amazon ibm watson and more.
messaging bots could in fact be well set as the viable alternative or even successor of mobile apps .
text messaging itself are already embedded in many apps and used by over billion people with this number is increasing rapidly .
messaging bots look and feel not much different to chatting with a friend via instant messaging.
there is no need to learn understand or navigate disparate interfaces or languages yet they carry the potential of being programmed to automate conversations transactions or workflows .
however at present bot building systems while useful in detecting the user s intent still require significant development and configuration work for each usage scenario and they hardcode the dependency with the apis to be invoked.
in other words today for each class of functionality we want to enable through a bot and for each api we need to hardcode the logic that interacts with the user to identify their intent and obtain the desired values for the specific api parameters.
bot building framework have no knowledge of these apis they support the identification of specific intents and parameters and then they can be configured to make a rest call.
apis are therefore external to the bot builder and is completely outside of the process only appearing at the end in an hardcoded fashion.
our vision consists of making apis first class citizens of bot builders.
we aim at synthesizing natural language expression and at dynamically determining which api to invoke based on our understanding of the users intent and on the knowledge over an api knowledge graph that describes what the methods do and how they can be invoked.
the vision we set forth in this paper is that of users being able to talk with assistants as some of us do every day with siri alexa or cortana and with the help of a knowledge of apis modeled via a knowledge graph and built incrementally dynamically identify intents apis fulfilling the identified intent and collect from the user the value of the required parameters for invocation.
if successful this can enable a new approach to the development of cognitive services where the program is built on the fly based on users requests and available services exposed through apis.
more specifically we devise a technique for synthesizing natural language user expressions into concrete api calls by .
c ieeease urbana champaign il usa technical research new ideas832 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
leveraging an api knowledge graph kg to achieve this.
the api kg contains information about apis their declarations expressions parameters and possible values.
ii.
p reliminaries the following are accepted terminology as related to bottechnology expression.
this refers to a natural language user utterance whilst conversing with a bot e.g.
what is the weather forecast for tomorrow?
.
based on the users expression the bot recognizes a category such as weather condition find restaurant .
this refers to the users intention.
intent.
the notion of intent is widely used in conversation systems and refers to the users purpose in an expression.
for the example above the underlying intent may be summarized as weather condition .
in conversation systems a fundamental step to answer a users query is to recognize intention.
entity.
a common notion amongst nlp systems an entity represents a concept specific to domain or context.
in the previous example an expression such as what is the weather like tomorrow?
identified a keyword such as tomorrow .
when reshaped as an entity we link this to a type such as time date.
formulating entities often requires analyzing the keyword in conjunction with the overall intent.
conversation.
in order to execute an action by the bot e.g.
calling an api all required parameters would need to filled in prior to execution.
however in the event that something is missing the bot will typically prompt users with more questions to obtain any missing information.
this back and forth question answering between user and bot and vice versa is referred to as conversation.
action.
this ties a high level user expression into a concrete execution plan in order to obtain the required results to satisfy the user s query.
for example we may have an action such as findrestaurant.
in many ways actions are akin to functions in traditional programming languages.
whereby just like functions actions also often requires input parameters.
for example more specifically the action above could likely be described as findrestaurant type place location such as findrestaurant italian cafe sydney .
our approach.
botbase builds upon all the above concepts plus in our work we propose the notion of declaration which refers to the expressivity of an api.
moreover the notion of api and its constituent declarations in our context are synonymous with the traditional notion of intent.
figure illustrates the relationship between expression apis declarations entities and api calls.
iii.
api k nowledge graph a. knowledge model we propose a lightweight model to capture useful information about apis and their related information .
such knowledge which may be created incrementally and collectively will significantly assist bot programming both during development and execution.
during development it is the onus fig.
.
botbase approach an api oriented methodology of the developer to properly specify set of expressions or patterns thereof possible keywords and the required actions for each.
clearly specifying this each time is not only tedious but likely to lack full inclusion e.g.
there may be some valid expressions that the developer did not think about .
thus the value of a knowledge graph in this context would alleviate these challenges.
more so in our proposed work the synthesis technique performs in real time with the help of the knowledge graph thus also acting during execution time to assist in the bot s processing of the result.
the following are the main entities of our model api is the root and contains a name description a set of descriptive tags and a base uri e.g.api.yelp.com .
an api may have one or more declarations that are supported for this particular api.
it contains a type e.g.
get orpost and a path e.g.
search?term location .
declarations are linked to a set of textual sample expressions.
declarations may also have one or more parameters e.g.
term location which may be linked to possible values para values e.g.
food san francisco .
parameters also has arequired field to flag if the parameter is optional or not.
finally we define an entity called value wevalue that links concrete values with semantically similar words that are derived from our deep learning model.
note we is an acronym for word embedding.
b. knowledge acquisition we employ a combination of two techniques for acquisition and enrichment of new knowledge firstly we harness crowdsourcing and this paradigm enables an incremental and collective approach.
crowdsourced workers such as i developers enrich the kg by entering sample expressions at bot training time ii end users enrich the kg whilst talking with bots.
secondly we apply deep learning and this paradigm introduces a layer of intelligence and automation.
word embedding deep learning model.
we use a deep learning model called word embedding to generate semantically similar keywords that relate to existing parameter values.
to more the dense the easier it makes it to more successfully link arbitrary user expressions with api declaration.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
this is because we would be able to better recognize a match to the api s declaration and parameters and find an api that would useful to process the expression s intent.
more specifically the model we use is word2vec .
this model takes as input a corpus and produces a vector space where each word in the corpus is represented by a vector.
words that are semantically similar are close to each other in the vector space and such distance is typically measured with the help of the cosine similarity metric .
while our model can be trained on any text corpus in this paper we use a dataset of wikipedia1because it is general and it covers a variety of topics.
this corpus contains more than billion words.
the neural network uses hidden layers sliding window size of five skip gram with negative sampling sets the minimum occurrence of a words to fifty i.e.
the word must appear at least fifty times to be inside the training set and ignores stopwords by using the nltk library for preprocessing the corpus.
our trained model contains both unigrams e.g.
car restaurant computer and biagrams2 e.g.
opera house new york ice cream .
we use our trained model i in api kg enrichment process for a given parameter value e.g.
paris we can add its neighbours from the word embedding model such as sydney new york london as another possible values for that parameter e.g.
location ii in the synthesis process for an extracted value e.g.
melbourne from user expression we can link it to the most relevant parameter e.g.
location by computing similarity ratio between the extracted value and values already linked to that parameter inside the api kg.
iv.
s ynthesis of expression to api botbase is actualized as a set of components that when placed in orchestration serves to fulfill the fundamental goal of mapping a natural language user expression into a concrete api call.
we refer to this overall process as synthesis .
underlying this process the components critically relies on the intelligence of the knowledge graph and word embedding model that we proposed in the previous section.
this process or even individual components could potentially be used at every stage of a bots lifecycle.
for example during bot development a bot developer usually have to select the api and declaration from a pool of apis.
especially when there are a large number of apis this could be much simplified using the kg to help guide the developer select the matching api s. the developer could input a set of sample user expressions and then feed them into the process to discover the api that the synthesis process deems most relevant.
similarly during bot training developers may enter sample expressions to allow the bot to learn possible values for the various parameters of the selected api declaration.
finally during execution of course it serves the fundamental purpose of processing user expressions into api calls.
fig.
.
entity extractor using stanford pos tagger and word embedding figure illustrates the functional architecture of these components in orchestration.
we now describe in more detail the role and individual functionality of each of these components entity extractor.
this component takes a user expression and decomposes into a set of entities which are then classified into nouns adjectives and proper nouns with the help of stanford pos tagger .
this filters all words that are irrelevant e.g.
stop words like theandat .
secondly this component derives bigrams e.g.
ice cream andnew york with the help of our word embedding model figure .
api selector.
upon performing entity extraction from the inputted user expression this provides the first step towards understanding the basic intent.
accordingly we can use this information to select apis that could potentially be a match in satisfying the user s request.
we can match with apis based on semantic similarity this means using the extracted entity information i.e.
nouns verbs as well as complex analysis derived from bigrams see figure we can match with apis in the kg by comparing with their associated tags parameter names and values.
it is important to note finding the best api match will require additional layers of analysis namely examining the declarations of apis and checking if their parameter signatures are covered.
this will therefore be dealt with at subsequent steps as we will describe in more detail below .
however the purpose of this stage is to perform an early filtration and produce a set of viable apis.
api declaration selector.
now that we have a viable set of apis the next step is identifying the best matched declarations.
we may recall declaration curated in the kg are linked with sample expressions .
accordingly we can perform semantic similarity analysis between the inputted user expression and the set of sample expressions stored for each authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
synthesis process natural language user expressions into api invocations declaration.
to implement this we again leverage our word embedding model to compute semantic similarity.
we compute the vector representation of both the inputted user expression and each of the sample expressions as follows !s nx i !swiwhere !swi2we !
tk nx i !
tk wiwhere !
tk wi2we where !sand !
tkare the vector representation of the user expression and a sample expression from the api knowledge graph respectively !swiand !
tk wiare the vector representation of a single word within each expression.
we is the vector space defined by the word embedding model.
the actual semantic similarity of !sand !
tkis computed using the cosine similarity metric .
using this metric the api declaration selector chooses the declaration that has the highest score for similarity which is equated as follows expr arg max !
tk i similarity !s !
tk i dec declaration expr where expr is the sample expression from the api knowledge graph that has the highest similarity with the user expression !s and dec is the declaration that is associated to the sample expression.
the declaration is obtained through the function declaration above that takes as input an expression and returns an api declaration.
entity parameter mapper epm .
we are now ready to map extracted entities to the parameters of the chosen declaration.
let s return to our earlier example for illustration.
consider we have a user expression that states is there any chinese restaurant near sydneyopera house and the selected api declaration is api.yelp.com search?term location .
the job of the entity parameter mapper is to recognize that chinese restaurant should be mapped to the parameter term and that sydney opera house tolocation.
in order to determine which entity should be mapped to which parameter we once again apply the word embedding model.
we compute for each parameter of the selected api declaration the semantic similarity with stored parameter values for that declaration.
we may recall these stored values are acquired based on the methods discussed at section iii b .
therefore more specifically in this example if we compare the input entity chinese with all stored parameter values of the selected api declaration we could conclude the keyword chinese is probably a value of the term parameter because of its semantic similarity with e.g.
french which is also a value of the term parameter as stored in the knowledge graph.
the output of the epm component is paravalue matrix.
it contains parameter value pairs i.e.
mapped entities to parameters together with confidence ratio for each.
we also use a component named kg updater to store new values into the knowledge graph as users are conversing with the bot.
this component only store new values provided the confidence ratio above a certain threshold.
at this stage we set it to .
however this may considered an initial threshold for experimentation purposes.
we believe as increasingly more knowledge is gained it will become clearer to understand the right balance and this figure will be more precise.
coverage checker.
this component is responsible for performing two main tasks i firstly to choose the best declaration that has the maximum number of fulfilled required parameters amongst candidate api declarations ii secondly to make sure that all the required parameters of the selected declaration are fulfilled before calling the api.
accordingly thecoverage checker component takes two inputs the candidate api declarations and secondly the paravalue matrix that was produced by the previous component.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we determine coverage for each viable declaration by computing the following coverage dec k pn i 1mapping pi n mapping pi 1piis mapped to a value 0piis not mapped to a value where coverage dec k represents the ratio of parameters of the declaration deckwith a value assigned to it mapping pi 2f0 1gindicates whether or not a parameter has been mapped to a value and nis the total count of required parameters for the api declaration.
when this component is performed during execution we require that thecoverage dec k should be equal to .
this implies all required parameters of the api declaration are fulfilled.
when this happens a call to the api can be performed and the results returned as a json object for processing of the final results and display to end user.
if coverage dec k is not equal to then coverage checker notifies the end user and tries to obtain the necessary feedback about parameters for which no suitable value could be found.
in some cases we may want to choose one declaration amongst a set e.g.
many declarations appear too similar or if used during development mode the bot developer may simply wants help to compare various declarations .
accordingly the best declaration is picked based on equation dec arg max deck coverage dec k where dec represents the declaration that scored the highest coverage metric as defined in equation .
as a result of this the coverage checker outputs the best api declaration.
v. r elated work and discussions there is a considerable body of research conducted on spoken dialog systems.
some works used crowds in areas like writing and editing image description and interpretation .
some others worked on taking advantage of the crowd to empower the dialog system abilities answer to questions in several domains with various styles by creating api calls .
botbase takes inspiration from the latter work for building api calls dynamically albeit by using machine learning and knowledge graph.
besides there are some works focused on api recommendation techniques which are relevant to our work.
rack translates a natural language query into a list of relevant apis by mapping keywords to apis portfolio recommends relevant api methods for a given natural language query by employing indexing and graph based algorithms proposed technique in recommends a graph of api methods from precise textual phrase matching.
furthermore improvements in natural language processing techniques as well as bot development approaches have led to a renewal in bots.
most recent efforts in natural languageprocessing have focused on translating user expressions into program codes .
for bot development side rulebased approaches such as aiml rivescript chatscript pandorabots and superscript provide scripting based languages for describing service user conversations as a rule set e.g.
ifthen statements .
flow based approaches such as motion.ai chatfuel manychat flowxo and sequel provide platforms to describe end user conversations as a flowchart e.g.
sequence of options and actions .
hybrid approaches like wit.ai api.ai microsoft luis meya pullstring gupshup microsoft botframework ibm conversation service and amazon lex provide platforms to describe service user conversations as intents and natural language expressions.
however these approaches are forcing developers to handle almost all the parts e.g.
training bot writing pieces of codes for each user intents interacting with underlying backend services such as apis code invocation commands and programming libraries to generate user responses.
botbase benefits from techniques in hybrid platforms to learn possible values for api parameters and to enrich its api knowledge graph.
the idea of building an api knowledge graph for botbase comes from augur which is a knowledge graph of human actions activities and their relation with objects knowbot a question answer system that builds a knowledge graph of concepts in science and their relations through conversational dialogs between user and system commandspace a knowledge graph of user expressions and adobe photoshop application commands and qf graph a mapping between user s vocabulary and system features.
we use the same idea to construct api knowledge graph contains api api declarations parameters and values.
furthermore botbase leverages concepts in previous effort on synthesizing java language codes for a given input text by using pcfg model and mapping words and api declarations .
botbase constructs mappings between api declarations to sample expressions and parameters to values in its knowledge graph.
vi.
l imitations f uture work in this paper we have unleashed the first generation of our vision towards an era of intelligent bots that learns from complex data sets and mimics the way of the human brain.
at this stage we make certain assumptions we elaborate upon these limitations below together with possible solutions for each.
conversational bots.
asbotbase is only within its early stages it does not yet support conversational bots.
at this stage we assume a stateless environment where each question from the user has an answer from the bot and the bot does not remember anything from the previous chat messages.
future works therefore requires a stateful conversation mechanism in botbase.
process workflows.
one of the most significant limitations of our current work is the assumption that an action can be performed by one api declaration alone.
while it is true there are currently a plethora of apis and large amounts of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
user requests could in fact be fulfilled by a single declaration.
nevertheless in future work we intend to support executing dynamic process workflows i.e.
calling upon a series or combination of apis to fulfill a user request.
advanced knowledge mining.
the success of this proposed approach depends greatly on the quantity and accuracy of the knowledge graph.
at present we have acquired a sizable quantity of data using bootstrapping crowdsourcing and general purpose wikipedia corpus to train our word embedding model.
this truly does supply an enormous amount of data however in future we would benefit from gathering from other sources of data no only formal datasets or corpuses.
vii.
c onclusion in this paper we provide a solution for dynamically synthesizing natural language user expressions into api invocations.
to achieve this we have proposed a lightweight api knowledge graph that contain the required information about apis.
we enrich the kg with new apis as well as appending supplementary knowledge about existing apis.
enrichment is achieved with the intelligence of deep learning word embedding model trained by wikipedia corpus in conjunction with crowdsourcing approaches.
in the synthesis process botbase benefits from the api kg and the word embedding model to determine which api to invoke based on user expressions.
acknowledgment we acknowledge data to decisions crc for funding scholarship on query answering and predictive techniques for analyst tasks in end user big data analytic.