statistical errors in software engineering experiments a preliminary literature review rolando p. reyes ch.
universidad polit cnica de madrid madrid spain universidad de las fuerzas armadas espe sangolqu ecuador rpreyes1 espe.edu.ecoscar dieste universidad polit cnica de madrid madrid spain odieste fi.upm.es efra n r. fonseca c. universidad de las fuerzas armadas espe sangolqu ecuador erfonseca espe.edu.ecnatalia juristo universidad polit cnica de madrid madrid spain natalia fi.upm.es abstract background statisticalconceptsandtechniquesareoftenapplied incorrectly eveninmaturedisciplinessuchasmedicineorpsychology.
surprisingly there are very few works that study statistical problems in software engineering se .
aim assess the existence of statistical errors in seexperiments.
method compile the most commonstatisticalerrorsinexperimentaldisciplines.surveyexperimentspublishedinicsetoassesswhethererrorsoccurinhigh qualitysepublications.
results thesameerrorsasidentifiedin othersdisciplineswerefoundinicseexperiments where30 of thereviewedpapersincludedseveralerrortypessuchas a missing statistical hypotheses b missing sample size calculation c failure toassessstatisticaltestassumptions andd uncorrectedmultiple testing.thisratherlargeerrorrateisgreaterforresearchpapers where experiments are confined to the validation section.
the originoftheerrorscanbetracedbackto a researchersnothaving sufficient statistical training and b a profusion of exploratory research.
conclusions this paper provides preliminary evidence that se research suffers from the same statistical problems as other experimentaldisciplines.however thesecommunityappearsto be unaware of any shortcomings in its experiments whereas other disciplinesworkhardtoavoidthesethreats.furtherresearchisnecessarytofindtheunderlyingcausesandsetupcorrectivemeasures buttherearesomepotentiallyeffectiveactionsandareapriorieasy to implement a improve the statistical training of se researchers andb enforcequalityassessmentandreportingguidelinesinse publications.
ccs concepts general and reference surveys and overviews permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
literature review survey prevalence statistical errors introduction experimentationmakesextensiveuseofstatistics.severalstudies warn about the existence of scientific articles using inappropriatestatisticalprocedures .thishappenseveninmature disciplines such as the health sciences .
in turn there are very few papers studying statistical errors insoftwareengineering se articles.therearepapersdiscussing statistical power heterogeneity in meta analysis and the relative strengths and weaknesses of cross over designs in se.
this stands in contrast to other disciplines where papers warningaboutproblemsinsimplestatisticalconceptssuchashypothesis statement interpretation of p values sample size calculation significance levels etc.
are quite common.
we aim to assess the prevalence of these problems in the se literature.
we have compiled the most common statistical errors in experimental disciplines and surveyed empirical papers published in icse between and to check whether or not these papers are subject to the compiled errors.
our results suggest that se experiments have the same weaknesses as in other sciences.
se researchersdonotuserelativelysimpleconceptslikehypothesis statement sample size estimation inference and post hoc testing correctly.
these problems seem to be related to poor statistical training and the use of exploratory research.
our contributions confirm the shortcomings in experimental seresearchandidentifytheirorigin.inouropinion thesecommunity should improve researchers statistical training and more importantly establishmechanisms e.g.
qualityassessmenttools reporting guidelines to identify and correct statistical problems in se experiments before they are published.
the structure of this paper is as follows.
section provides background on the topic of statistical errors in science and se.
section presents a short literature review identifying severalstatistical errors.
we screen articles reporting experiments for a subset of the above errors in section .
the origin of the identified acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden reyes et al.
errors is evaluated in section .
section offers a critical appraisal of this.
finally the conclusions are reported in section .
background .
statistical errors in experimental disciplines scientificandengineeringresearchersapplystatisticaltechniques to analyze and interpret many of their research results.
hence statisticaltechniqueshaveexperiencedanincreaseinuse particularly inmedicine psychology education andsocial science .
there is a relatively large collection of publications that provide information about the existence of statistical problems in virtually alldisciplines.
notall publicationsare recent theyhave beenavailablesincethewidespreadadoptionofexperimentalresearchintheir respectiveareas.thereportedproblemshaveabroadscope including the definition of statistical hypotheses interpretation of p values sample size calculation significance levels and confidence intervals and so on.
papersaboutstatisticalshortcomingsinother disciplineshave derivedtheirresultsfromsometypeofliteraturereviewofprimary studiesfromoneormorespecializedjournals.theirconclusions are surprising and worrying since they report high error rates welch studied145articlesfromoneofthemostrenowned medical journals the american journal of obstetrics and gynecology andfoundthat52.
ofthearticlescontainedinadequate or incomplete statistical descriptions.
bakker evaluated articles from high and low impact psychologyjournals.theauthorreportedthatlowimpact journalsexhibitstatisticalinconsistenciesmorefrequently than high impact journals.
bakker determined that about ofallthepapersfrombothhighandlowimpactjournals have at least one incorrect statistical conclusion.
ercanetal.
evaluated164and145articulesinpsychiatry andobstetrics respectively.ofthepsychiatricandobstetrics publications and respectively contained mistakes regarding sampling sample sizecalculation and contradictory interpretations of inferential tests.
kilkenny et al.
assessed the experimental design of papers published in medlineandembase from to .
more than of the papers are subject to biases during theassemblyofthestudycohort weakstatisticalanalysis missing information etc.
the origin ofthe statistical errors can betraced back to several causes accordingtocastroetal.
theanalysisandinterpretationofempiricalresultsinanyscientificdisciplinedepend primarily on how well researchers understand inferential statistics.
theauthors suggestedthat researchersin theeducation community especially phd students are prone to misconceptions particularlywhentheyareusingabstract statisticalconcepts suchasconfidenceintervals sampling distributionswithsmallnumbers samplingvariability different types of distributions and hypothesis tests.
cohen et al.
conducted an empirical study with degree students.theyfoundthatstudentslackstatisticalknowledge whichleadstothemisinterpretationofstatisticalconcepts and biased judgements.
brewer evaluated18statisticalhandbooksbyrenowned publishers e.g.
academic press addison wesley mcgrawhill prentice hall john wiley etc.
these books contained inaccurate statements in topics such as sampling distributions hypothesis testing and confidence levels.
.
statistical errors in se the se community apparently has limited awareness of the existence and impact of statistical shortcomings in its publications.
whenwesearchedforsepapersrelatedtostatisticalproblems the only results were dyb et al.
s paper regarding statistical power miller spaperonmeta analysis andtwopapersbykitchenham and vegas et al.
that focused on within subject designs.
several other papers discuss specific statistical issues.
for instance kitchenham introduced robust statistical methods while arcuri and briand discussed statistical tests for the assessment of randomized algorithms .
these papers do not assess theweaknessesincurrentresearch.theysuggestopportunitiesfor improvement in the toolset that se researchers currently use.
there is a manifest difference between se and other experimental disciplines regarding statistical errors.
in medicine and other sciences statisticalproblemsareroutinelyidentifiedinpublications this issue is almost completely overlooked in se.
otherdisciplineshavenotaddressedstatisticaldefectsandmethodological problems until relatively late on.
for instance while thefirst formal randomized clinical trial in medicine was conductedin the 1940s the first publication about statistical defects in this field that we are aware of was published in the 1970s .
given thatse isonly justadopting experimental methodsand the associated statistical techniques its failure to pay attention to the assessment of statistical issues should come as no surprise.
thispaperreportsanexploratorystudyaimingtoanswerthe following research questions rq1 what are the most common problems associated with the use of experimental procedures in exper imental disciplines?
rq2 what is the rate of statistical errors in se research?
statistical errors in experimental disciplines .
review strategy toanswerrq1 wereviewedseveralspecializedbookspublished onthetopic suchasgoodetal.
vickers andhuck .
thesebooksprovideagoodstartingpointforourexploratorystudy because they are not related to any specific discipline althoughthere is some bias toward the health sciences and they focus on serious errors often inspired by real research.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
statistical errors in software engineering experiments icse may june gothenburg sweden .
collected data tworesearchers o.diesteandr.p.reyes reviewedtheabovethree books.theyfound93textsectionsclearlypointingtosometypeof error that canbe frequently found inthe literature.
discrepancies were solved by consensus.
the complete listing of paragraphs is available at including links to the reference books and related literature.
.
analysis method we applied thematic synthesis to classify the statistical errors.
we applied the guidelines by creswell and cruzes et al.
t o avoidbiasandachievemethodologicalrigorinthesynthesisand interpretation of results .
the analysis consisted of two stages coding and theme definition.
it was conducted by the same two researchers than collected the data.
during the coding stage both researchers independently assigned low level codes to each text section which were later reviewed and harmonized.
we created different codes.
during the themedefinitionstage codesweregroupedtogetherbymeansof higher level codes.
this procedure was aligned with our purposes since the high level themesrepresenterror prone areas.
bothresearchersworkedcollaboratively.theyorganizedthelow levelconcepts into high level themes according to a directed graph shown infig.
.themesandconnectionsbetweenthemesandconcepts are available at nodes represent categories of statistical errors.
categories becomeprogressivelymoreabstractasthetreeistraversedfromright to left.
for instance the node study design fig.
bottom left is connected to the nodes assignment andsampling.
this means that thehigh levelcategory studydesign containstwotypesoferrors assignment andsampling errors.
likewise assignment splits into furtherlower levelerrortypes suchas matching randomization etc.
notice that fig.
shows only one subset of the error types that we have identified to keep the graph within page limits.
the full graph is available at theremaybemultipleconnectionsbetweencodesandhigh level themes and between one high level theme and another because theyarementionedinseveralbooks ormorethanonceinthesame book in different contexts.
for instance randomization is discussed twice in terms of the representativeness of the random samples item misconception ifatrulyrandomprocessis used to select a sample from a population the resulting sample willturn outto bejust likethe population but smaller.
item misconception asampleofindividualsdrawn fromalarger finitegroupofpeopledeservestobecalled arandomsamplesolongas everyoneinthelarger group has an equal chance of receiving an invitation to participateinthestudyand randomreplacements arefoundforanyoftheinitialinviteeswhodeclineto be involved.
andonceagainwithregardtotheequivalenceofexperimental groups formed by random assignment item theideabehindrandomizationistomake thegroupsassimilaraspossible .baselinedifferencesat the beginning of the trial such as in age o gender are due to chance.
giving a p value for baseline differencebetween groupscreated byrandomizationis testinganullhypothesisthatweknowtobetrue.
pp.
these repeated associations are an indication of relevance and thus the arcs connecting the corresponding nodes have been made proportionally wider.
the number next to the arc indicates the numberoftimesthattheconnectionappearsintherawdata.dotted lines represent connections that appear just once.
.
review results statistical errors can be classified in to three groups a experimentation b meta analysis andc prediction.mosterrorsarerelatedto experimentation.nevertheless itisnoticeablethatmeta analysis appears three times in connection with subgroup analysis and the combinationofstudieswithdifferentdesigns.predictionappears just once with respect to with linear modeling.
in what follows we focus on problems associated exclusively with experiments.
analysisistheexperimentationfacetmostoftenmentionedin connection with statistical errors.
in the three reviewed books analysis errors appear times.
there are two main sources ofproblems with analysis the application of inferential techniques and the interpretation of results the inferential techniques most often used during experimentaldataanalysisareclassicaltests suchast tests and theirrelatedconcepts suchasp valuesandtails.researchers often make wrong assumptions about the tests e.g.
robustnessoft test andtheyselecttestsincircumstancesinwhichthey cannot be applied e.g.
ordered alternative hypotheses or are sub optimal e.g.
low power tests .
all common tests includingt tests correlations andanova arementioned in this context.
anotherfrequentlymentionedinferentialtechniqueislinearmodeling multiplelinearregressionisthebestknown exampleoflinearmodeling.themostfrequentlymentioned problemistherationalebehindthedefinitionofthelinear model.
other issues such as the violation of assumptions and usage beyond limits e.g.
outside the linear phase are also reported.
manysupposedlybasicconcepts suchasconfidenceintervals statistical significance or p valuesare frequently misinterpreted.
studydesignissecondtoanalysis.thisconceptincludesmethodologicalissuesconnectedtothemanagementofexperimentalunits such as sampling and assignment.
in both cases the sources of the problemsareinappropriateormissingrandomizationandsample size calculation.
reporting is another troublesome issue which is mentioned the samenumberoftimes ten asstudydesign.therearemanysources of reporting defects e.g.
overlooking experimental incidents or multipletesting althoughtheabsenceofdescriptivestatistics e.g.
means is emphasized tree times in the reviewed books.
the last prominent issue is goal definition.
researchers often do notstate statisticalhypotheses.
failureto explicitlydefine null hypotheses appears three different times in fig.
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden reyes et al.g s s s n a p e r s 9a m t l d a d s s d b i d i t s a p p p c c p r u m g c d r m d r b 3c m e c figure classification of statistical errors in experimental research papers authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
statistical errors in software engineering experiments icse may june gothenburg sweden statistical errors in icse experiments the aim of rq2 is to find out the rate of statistical errors in se research.
to answer rq2 we evaluated the experiments published in icse over years .
icse is se s flagship conference.
ourevaluationshouldidentifytherateofcommonstatisticalerrors inthebestseresearch thesituationoflowerqualityseresearch is likely to be worse.
.
evaluation instrument thecomplete listof statisticalerrors thatwe have compiled contains almost items.
since statistical errors are ubiquitous in the generalresearchliterature itishighlylikelythatseveralofthe problem typeswould appear in virtually any sepaper as well.
therefore anexhaustivereviewofseexperimentswoulddrawa too pessimistic picture of our field.
wehavefocusedonrecurrenttypesoferrors denotedbywidearrowsinfig.
.forinstance nullhypothesis relatedproblemsarereferenced multiple times in fig.
as well as test assumptions or centralmeasures.wehaveselectedthemosterror pronestatisticalconcepts developedappropriatequestions andcreatedthe10 questionchecklist shown in table .
all these questions can be easily traced back to fig.
or the online version at further clarification is required at this point q1.
and q1.
may look outdated due to the increasingcriticisms of the null hypothesis and significance testing nhst and the recommendations to adopt other statistical approaches such as confidence intervals and effect size in dices .
however se research has not yet taken up these recommendations.
for instance only four out of 21experimentspublishedinicsefrom2006to2015report anymeasureofeffectsize andtwooutof21refertoconfidence intervals.
nowadays nhst is still the main statistical approach used in se.
q4 have subjects been randomly assigned to treatments?
may not be applicable to some types of experiments e.g.
when two defect prediction algorithms are applied to the same code that is matched pairs or similar designs.
in such cases the question is answered as n a. a similar strategy is appliedforanyquestionthatdoesnotmakesenseforagivenexperiment e.g.
q5 havethetestassumptions i.e.
normality andheteroskedasticity beencheckedor atleast discussed?
when an experiment does not use statistical tests.
testassumptionsvaryfromtesttotest.inmanycases reference books state incomplete or even questionable assumptions.thus inq5 havethe testassumptions i.e.
normality andheteroskedasticity beencheckedor atleast discussed?
wewillpayattentiononlytothemostusualconditions normality and heteroskedasticity that have to be examined before applying virtually any parametric test.
q7 havetheanalysisresultsbeeninterpretedwithreferenceto applicable statistical concepts such as p values confidence intervals andpower?
wouldappeartobearathercrucialquestion.fig.1showsthatthenode interpretation isconnected bywidearcswithnodesrepresentingrelativelysimplestatisticalconcepts suchaspower confidenceinterval p value and so on.
however we doubt that we can answer this ques tionobjectively.whileauthorstypicallydiscusstheirresults atlength theymaysimplify oromit somestatisticalissues required to clearly transmit their message to readers.
thus we face the risk of making mistakes e.g.
evaluating q7 negatively due to incomplete reporting.
we decided to skip this question and there is a line through it in table .
multipletestingdoesnotappeartobeakeyissueinfig.
.
however it was cited three times as a source of problems duringbothanalysisandreporting notethattherearethreeincomingarcsforthisnodeinfig.
.thisjustifiesq9 ismul tipletesting e.g.bonferronicorrection reportedandaccounted for?
.
q10 aredescriptivestatistics suchasmeansandcounts reported?
is important for both analysis and reporting.
we considerthisissueinthecontextofreportingonly soasnot to inflate the number of defects found.
table evaluation checklist question q1.
are null hypotheses explicitly defined?
q1.
are alternative hypotheses explicitly defined?
q2 has the required sample size been calculated?
q3 have subjects been randomly selected?
q4 have subjects been randomly assigned to treatments?q5 havethetestassumptions i.e.
normalityandheteroskedasticity been checked or at least discussed?
q6 has the definition of linear models been discussed?
q7havetheanalysis resultsbeeninterpreted bymaking reference torelevantstatistical concepts suchasp values confidence intervals andpower?
q8do researchers avoid calculating and discussing post hoc power?
q9is multiple testing e.g.
bonferroni correction reported and accounted for ?
q10aredescriptivestatistics suchasmeansandcounts reported?
.
target studies ouroriginalaimwastosurveyonlyicseexperimentalpapersfrom 2006to2015.however thedecisionsoonprovedtobequestionable.
we conducted a pilot study on icse edition to check the feasibilityofourstudy.weimmediatelyrealizedthatthenumber of fully fledged experiments was quite low we found only four experiments.inturn wefoundmanysmall scaleexperimentsaimedatevaluatingthepropertiesofnewtechniquesormethods typically reportedinresearchpaperevaluationsections.tobeprecise we identified experiments as evaluations .
of the total number of papers in icse .
the question was whether the survey should be extended to experimentsasevaluations orrestrictedtostandaloneexperiments.
experiments as evaluations often apply an experimental methodology butaretypicallyonlyonetothreepageslong.thecompressed reporting format may lead to writing practices that may be mis conceived as statistical errors by reviewers.
on the other hand experimentsasevaluationsaccountforalargeshareofempirical research andtheresultsofthissurveywouldbeincompleteifthey were overlooked.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden reyes et al.
we decided to evaluate both types of studies separately.
in a firststage wesearchedforallstandaloneexperimentspublished in icse from to .
we found a total of papers.
we then collected a similar number1of experiments as evaluations to avoid over representation.
.
study selection two researchers o. dieste e. r. fonseca and r. p. reyes worked separately to screen the tables of contents of the icse technical tracks.
they reviewed the title and abstract for any indication that the paper reported an experiment.
if in doubt they examined the full text in search of further evidence of at least two treatmentsbeing compared thatis the minimumconditionto be met by any experiment.
the total number of papers and the papers pre selected after screeningareshownintable2.thepre selectionagreementwas calculatedusingfleiss asrecommendedbyk.l.gwet pp .
.
.
typically considered as moderate .
this implies that we may have failed to identify some experiments.
note that it isnotstraightforwardtoidentifyexperimentsusingmetadata such as titles and abstracts due to missing methodological descriptors.
three researchers o. dieste e. r. fonseca and r. p. reyes individually reviewedthe pre selectedpapers andclassified them into theexperiment andnon experiment categories.
disagreement was resolved by consensus.
a total of21 papers were classified as standaloneexperiments.thisrepresents2.
ofthetotalnumber of papers published in icse.
previous research has already pointed outthelownumberofcontrolledexperimentspublishedinicse .
the agreement level for this step of the selection process wasfleiss .
typicallyconsideredas moderate .as reportedbelow thislowagreementisduetotheexistenceofmissing information e.g.
hypothesesor randomization procedures in the manuscripts.furtherdetailsareavailableat table summary of the selection process.
experiments as evaluations between parentheses yeartotal papers tp after screeningselected .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
total .
.
finally three icse experiments as evaluations per year were selectedatrandomfromthetablesofcontentsoftheicsetechnical track.
the three researchers independently reviewed these papers and discrepancies were resolved by consensus.
the process was 1we rounded up from to i.e.
three papers per edition 10icse years papers.repeateduntilthreeexperimentsasevaluationshadbeenidentified for each icse conference from to .
.
execution the three researchers individually evaluated all papers and gave a yes no notapplicable answertoeachchecklistquestion seetable7 .
thelevelofagreementwas substantial toalmostperfect inmany cases whichincreasesthereliabilityofourresults.detailsofthe evaluation are available at standalone experiments and experiments as evaluations .
table agreement levels per question standalone exp.exp.
as eval.
sec.
stage agree agree goal definitionq1.
.
almost perfect .643substantial q1.
.
substantial .788substantial study designq2 perfect .000perfect q3 .
slight .389fair q4 .
moderate .585moderate analysisq5 .
substantial .662substantial q6 .
perfect .558moderate q8 .
almost perfect .803almost perfect reporting q9 .
moderate .659substantial q10 .
perfect .480moderate .
survey results table summarizes the survey results.
percentages are calculated as yes no n a .
the no column represents the percentage of papers in the sample that are affected by the error indicated by the corresponding question i.e.
the prevalence of the statistical error rate.
q1 was split into two parts to differentiate the problems related to the null q1.
and the alternative q1.
hypotheses.
table defect rates standalone experimentsexperiments asevaluation sections stage yes no n a yes no n a goal definitionq1.
.
.
.
.
.
.
q1.
.
.
.
.
.
.
study designq20.
.
.
.
.
.
q328.
.
.
.
.
.
q466.
.
.
.
.
.
analysisq561.
.
.
.
.
.
q64.
.
.
.
.
.
q885.
.
.
.
.
.
reporting q99.
.
.
.
.
.
q1095.
.
.
.
.
.
wefoundclearevidenceoftheexistenceofstatisticalerrorsin icse papers.
the rate of the different errors varies but it is rather large in many cases e.g.
q1 q2 q3 and q5 hypothesis definition samplesizecalculation randomselectionandassumptionchecking respectively .
although the current situation is rather serious it has improved as compared to previous reports .
the results are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
statistical errors in software engineering experiments icse may june gothenburg sweden somewhatdifferentforstandaloneexperimentsandexperiments as evaluations.
for experiments as evaluations the number of n a responses is much higher.
there appear to be reasons for this mostoftheexperimentsasevaluationsapplyamatchedpairs design.random assignment q4 istypically notapplicable in this case e.g.
two different bug prediction algorithms are applied to the same code .
alargenumberofstudies e.g.
conducttheanalysis using descriptive statistics only.
descriptive statistics do not have assumptions to check q5 .
when inferential statistics are not used q6 linear modelling power and post hoc testing are not applicable either.
we ran a chaid tree2classification to confirm the above observations.an avalueinq4generatesasubsetcontaining80 of all the experiments as evaluations studies 2 .
df p value .
.
the classification tree confirms that nonrandom assignment due to matching is a differential characteristic of the experiments as evaluations.
focusingontheerrorrate wefoundthatbothstandaloneexperiments and experiments as evaluations yield similar values3when examinedusing question studytype contingencytables withthe exceptionofq1.
2 .
df p value .
andq1.
2 .
df p value .
.
in both cases standalone experiments define null hypotheses q1.
often and alternative hypotheses q1.
five .
vs. .
andeighttimes .
vs. .
respectively more often than experiments as evaluations.
both types of studies do not show statistically significant differences for theremaining questions although some may befalse negatives.
there are a large number of n as for several questions which reduces the amount of usable data thus lowering the power of the tests.
however the low p values for both the 2and the fisher sexact testsuggest thatq3 q4 q5 q10could achievestatistical significance with larger samples.
in all cases standalone experimentsperform randomselection q34 random assignment q4 assumption checking q5 and reporting of descriptive statistics q10 more frequently than experiments as evaluations.
albeit not as large as in the case of q1.
and q1.
differences are still substantial e.g.
.
vs. .
for q5.
we also find from table that the required sample size q2 has been calculated in just one study.
the definition of the linear model q6 has been considered in just two cases.
multiple testing q9 is a pervasive problem in seresearch.
most studies fail to report or correct for multiple testing using adequate e.g.
bonferroni or false discovery rate methods.
2the response variable was the studytype standalone experiments and experiments asevaluations andthepredictorswerethequestionsq1 .weusedthespssdefault chaid parameters with the exception of the parent and child nodes which were set to and cases respectively due to the small number of cases.
3notice that n a values may suggest misleading relations.
for instance q9 yields yes novaluesof9.
and71.
forstandaloneexperiments andof3.
and26.
for experimentsasevaluations.valuesdiffergreatly buttheodds71.
.
.
.
.
.
are rather similar.
4notice that q3 yields .09and .39for standalone experiments and experiments as evaluations respectively.
random sampling is a controversial issue in se.
results for q3 should be viewed with caution.
there isa high rate ofrandom selection q3 .nevertheless thisproblemisnoteasytosolveinhumanexperimentsbecause it is troublesome to assemble cohorts.
in turn randomselectioncouldbeeffectivelyappliedinnon humanresearch e.g.
when data is extracted from code repositories.
this survey shows that common statistical errors that occur in other sciences happen in se as well.
we have been able to survey a very limited number of experimental papers in one se conference.
however both the type and number of problems found suggest that se is facing the same challenges as in other sciences.
discussion the most likely explanation for the occurrence of the statistical errorsassociatedwithq1 10istherecentadoptionofexperimental methods in se.
many researchers have not taken formal courses onexperimentalmethodologyandinferentialstatisticsaspartof theirpostgraduatetraining.self educationtendstoleadtomajor differences among individuals.
if these assumptions were true two scenarios would be logical consequences the studies conducted by skilled researchers would be ofhigher quality where quality means error freeness e.g.
yes answers all answers than experiments conducted by less skilled researchers.
we could thus expect the quality values spread torange0 to100 .aserrorsareindependent qualityfollows a normal distribution5.
any statistical concepts closely related to practice e.g.
random assignment q4 assumption checking q5 and re porting q10 would have a lower error probability than theoretical notions e.g.
hypotheses definition q1 sample sizecalculation q2 randomselection q3 linearmodeling q6 post hocpower calculation q8 andpost hoc testing q9 .
in order to check scenario above fig.
shows the histograms for both types of studies.
in the case of standalone experiments the histogram matches the assumption the quality scores rangeacrossthe0 to100 interval andthedistributionisnormal shapiro wilk .
df p value .
.experimentsas evaluations paintaratherdifferentpicture.thedistributionisskewedtotheleft skewness .
indicatingthatpaperquality isconcentratedaroundthelowscores.thisisaclearlynon normal distribution shapiro wilk .
df p value .
.
theaboveanalysissuggeststhatthecausesbehindthestatistical errorsdifferdependingonthestudytype.inthecaseofstandalone experiments poor statistical training may explain the observed errors.
in the case of experiments as evaluations training alone cannot explain the data.
in our opinion the low scores point to the sec ondary role of statistics and experimental methodology in these papers.notonlydoexperimentsasevaluationstakeuparelatively smallspaceofpapers whichprovidesanexcuseforsummarizing unnecessarystuff butstatisticalrigoralsoprobablytakessecond 5statistical errors are probably dependent.
when a researcher learns a statistical topic e.g.
sample size calculation this knowledge is likely to lead to the avoidance of other errors e.g.
post hoc power calculation.
ho wever the errors underlying q1 are too wide ranging to appear strongly clustered in papers.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden reyes et al.
place in the authors objectives they are probably more interested in providing a convincing case for their proposals .
to check the scenario above table contains the odds of makinganerror.theoddsarethesameconceptasintroducedin footnote they represent the probability of an event occurring providing a negative response to the qiquestion i.e.
the paper includesastatisticalerror ratherthananother providingapositive response to qi i.e.
there is no such error .
table odds of making the error indicated in q1 odds no yes concept type qstandalone experimentsexperiments as evaluations practical conceptsq4 .
.
q5 .
.
q10 .
.
theoretical conceptsq1.
.
.
q1.
.
.
q2 inf inf q3 .
.
q6 .
.
q8 .
.
q9 .
.
inthecaseofexperimentsasevaluations thedata exactlymatches ourassumption6.alltheoreticalconceptshavelargeodds .
asopposedtopracticalnotionswhoseoddsaresmall .
.for standaloneexperiments thesituation is moreor less thesame.
for the theoretical concepts odds are smaller than for experiments as evaluations with the only exception of q9.
this is consistent withthehighererrorrateoftheexperimentsasevaluationsstudies.
however q1.
and q1.
odds are much smaller .
and .
respectively and comparable to the odds that appear in the group of practical concepts.
theaboveanalysisconfirmsthatpoortrainingisthemostlikely explanation for the presence of statistical errors in experiments.
in thecaseofexperimentsasevaluationsstudies amorecasualusage of statistics increases the error rate but the final outcome is the same.
one anomaly in table is the large odd that q9 exhibits for standalone experiments.
it has the same value as in experimentsas evaluations.
this value is even less plausible given the small odds for q1.
and q1.
any researcher with a good knowledge of statistical hypotheses should be aware of the impact of multipletestingon levels.themostlikelyreasonisthat inaddition totestingthestatisticalhypotheses standaloneexperimentsalso performexploratoryresearch whichshowsupasalargenumber of uncorrectedpost hoc tests .
exploratoryresearch is acommon feature of many se experiments e.g.
.
post hoc testing is associated to p hacking that is the acceptance of outcomes that fit expectations .
p hacking leads to publication bias.
j rgensen et al.
evaluated the existence of publication bias in se publications following ioannidis critical perspectiveformedicine .bothpaperscametoasimilarconclusion 6wearecrossingoutq6andq8because a q6wasapplicableonlyintwooutof51 studies and b post hoc power analysis q8 is a commission not omission error authors may perform correctly simply by not conducting a power analysis.
their inclusion would not have challenged our conclusions.the likelihood of publication bias is rather high.
more importantly forourpurposes bothpapersreportthattheunderlyingreasons for publication bias are statistical for example multiple inference testsandapreferenceforstatisticallysignificancetesting.ourdata supports j rgensen et al.
s observations post hoc testing increases thenumberoftests andthefailuretousecorrectionmethodsfor multipletestingprobablyinflatesthenumberoffalsepositives thus leading to publication bias.
threats to validity thisstudy appliedtworesearch protocols aliterature reviewand apapersurvey.thetwoprotocolsareverysimilar.theyhaveto meet a number of criteria concerning the relevance of the primary studies with respect to the research questions and the consistency across studies.
table shows an assessment according to the appraisalcriteriasuggestedbythompsonetal.
.onthewhole the results of the evaluation were positive.
we can be relatively confidentthattheliteraturereviewandthesurveyresultsaretrustworthy.however theyare incompleteduetothelimitednumberof theprimarysourcesused threewell knownbooksaboutstatistical errors and experimental papers from one se conference were used inthestudy.theexternalvalidityofthisresearchisthuslimited.
additionally theliteraturereviewfollowedasimplified butwelldefinedprotocol.wetooknoteofthepagenumbersofthebooks fromwhichweextractedinformationaboutstatisticalerrors.we disclosed the entire thematic analysis including codes and highlevelthemes.alldecisionsweremadebyatleasttworesearchers.
these precautions increased the validity of the literature review.
with regard to the paper survey we have taken reasonable precautionstoavoidbiases.threeresearchersparticipatedinthepaper selection and evaluation.
all decisions were recorded and made publicforreview.agreementlevels usingfleiss werecalculated and disclosed.
however theprecautionstakendidnotmeanthatweperformed a correct assessment in all cases.
the selection process yielded a low fleiss value which suggests that we may have skipped some experimentsand thus potentiallybiasedtheresults.evenso there isnoquestionabouttherebeingstatisticalerrorsinse although their occurrence rates or percentages may vary.
we do not claim that the reported rates are representative of alltypesofseresearch.actually theratesreportedinthispaper probablyrepresentthebestpracticeinseresearch withthepossible exceptionsoftheesemandeaseconferences andmaybesome journals likeempiricalsoftwareengineering.aswemoveaway from outlets of repute the number and severity of statistical errors is likely to increase.
finally we should point out that the results addressed in the discussionsectionaresomewhatspeculative.wecannotruleout alternativeexplanationsforthedistributionofqualityscores.as usual further research will be required to confirm our deductions.
7there are many appraisal procedures we have chosen because it is quite simple and domain independent.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
statistical errors in software engineering experiments icse may june gothenburg sweden a standalone experiments b experiments as evaluations figure histograms table appraisal criteria used for review appraisal criteria rq1 assessment rq2 assessment wastheliteraturesearchcomprehensive?
no no were appropriate criteria used to select articles for inclusion?partially theselectedbookswereappropriate butwere notspecialized.thebooksmayhaveomittedthediscussion ofspecificstatisticalerrorsthatprobablyappearinother sources such as research papers.partially icse is the flagship conference in se.
other conferences may publish lower quality experiments.
werestudiesthatweresufficientlyvalid for the type of question asked included?yes the three books specifically addressed the topic of statistical errors.yes experiments published in icse represent the best practice in ese.
were the results similar from study to study?yes they were very consistent.
several errors were identified by two or three books simultaneously.
the same highlevel themes were synthetized from the different books.yes statistical problems repeated across experiments.
conclusions the results of this preliminary review suggest that se is subject to thesametypeofstatisticalerrorsasarefoundinotherscientificdisciplines.theseproblemsarenotcomplicatedorsophisticated.they are surprisingly simple and include undefined hypotheses missingsamplesizecalculations randomization andmultipletesting amongothers.itisrathersurprisingthatthereisnoinformation abouttheexistenceofsuchproblemsinse.thesemethodological literature has not widely addressed this topic only some papers have scratched the surface.
researchers may not be awareoftheexistenceofstatisticalerrors andmuchlesssooftheir prevalence and potential impact.
there are two reasons that appear to explain the presence of statistical errors in se research a the recent widespread adoption ofexperimentationinse andb thefrequentuseofexploratoryre search.inouropinion therapidadoptionofexperimentalmethods in se research has forced researchers into statistical self education.
additionally it is rather unlikely that se research teams include or have access to statistical consultants.
as a result errors tend slip into designs and ultimately published papers.
this situation matchesothersciencesthathavealongexperimentaltradition such as medicine and ecology which have only recently paid attention to statistical errors.as empirical research in se approaches a mature stage there will be a greater awareness about statistical errors and the need to avoid them.
however it would be unwise for the se community sit back and wait for the day to come.
besides setting up formal training courses at universities and professional societies which is nowafoot thesecommunityshallenforcegoodpractices such as reporting guidelines and quality standards that have proved tobe usefulin othersciences e.g.
medicine psychology andeducation .furthermore thesegoodpracticescanbeeasily enforcedbyjournaleditorsandconferencepcchairsatrelatively little cost and effort.
exploratory research is another source of problems.
from the viewpoint of this research exploratory research takes the form of missing statistical hypotheses and the execution of multiple uncorrectedtests.however theseerrorsleadtopublicationbias asalreadydetectedinse .experimentpre registrationisprobably thebestwaytofightagainstpublicationbias butitisnoteasy tosetupandenforce.tothebestofourknowledge pre registration hasnotbeendiscussedsofarinse.furtherresearchisneededto find out effective ways to combat publication bias in se.
in themeantime the establishment of reporting guidelines and quality standards may improve the situation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden reyes et al.