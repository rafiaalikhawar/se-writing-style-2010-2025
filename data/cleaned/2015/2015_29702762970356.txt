how good are the specs?
a study of the bug finding effectiveness of existing java api specifications owolabi legunsen wajih ul hassan xinyue xu grigore ro su and darko marinov department of computer science university of illinois at urbana champaign usa legunse2 whassan3 xxu52 rosu marinov illinois.edu abstract runtime verification can be used to find bugs early during software development bymonitoring test executions again st formal specifications specs .
the quality of runtime verification depends on the quality of the specs.
while previous research has produced many specs for the java api manually or through automatic mining there has been no large scale study of their bug finding effectiveness .
we present the first in depth study of the bug finding effectiveness of previously proposed specs.
we used javamop to monitor manually written and automatically mined specs against more than 18k manually written and .1m automatically generated tests in open source projects.
the average runtime overhead was under .
.
we inspected violations of manually written specs and randomly sampled violations of automatically mined specs.
we reported bugs out of which developers already fixed .
however most violations .
of and .
of were false alarms.
our empirical results show that runtime verification technology has matured enough to incur tolerable runtime overhead during testing and the existing api specifications can find many bugs that developers are willing to fix however the false alarm rates are worrisome and suggest that substantial effort needs to be spent on engineering better specs and properly evaluating their effectiveness.
ccs concepts software and its engineering software testing and debugging keywords runtime verification specification quality empirical stu dy .
introduction in runtime verification the execution of a software system is dynamically checked against formal specifications spec sfor short .
at a high level the program being monitored is instrumented to capture as events method calls and field updates that are related to the specs being checked.
then at runtime the instrumented program creates listener objects commonly referred to as monitors which check that the events conform to the specs and reportviolations when some spec is violated.
in this paper a spec refers to a behavioral specification defined by robil lard et al.
as a way to use an api as asserted by the developer or analyst and which encodes information about the behavior of a program when an api is used .
a spec violation indicates that some api is used in a way that is not consistent with its usage guideline but such violation may or may not be a real bug in the code.
the potential for using runtime verification during software testing was previously recognized but combining testing with runtime verification of multi objec t parametric specs required by object oriented api specs only recently became practically feasible thanks to resea rch and development progress on i making parametric spec runtimeverification more efficient ii being able to monitor many specs simultaneously and iii better engineered runtime verification tools .
we recently proposed to combine runtime verification with regression testing wheretest executionsaremonitored agai nst formal specs to find bugs during software evolution .
the quality of specs has generally been taken for granted in the runtime verification research community where the major research direction over the last decade has been to improve the efficiency and scalability of algorithms techniques and tools.
the specs used in previous research were manually written or automatically mined .
these specs were monitored to measure their runtime overhead.
however for findingbugsbycombiningruntimeverificationwithsoftware testing the effectiveness of these specs becomes critical.
in this paper we present the first in depthinvestigation of the effectiveness of existing specs for finding bugs.
we consider a spec effective for bug finding if it can catch true bugs but does not generate too many false alarms.
we consider specs of the standard java api because such specs can potentially find bugs in many projects across various domains require no domain knowledge and the runtime verification tool that we evaluate javamop works for java.
we evaluate existing manually written and automatically mined specs.
specifically we use manually written specs that were formalized directly from the java api documentation and used in previous studies on the effipermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
ciency and scalability of runtime verification .
we also use specs that were mined automatically from large traces and were used in spec mining studies .
our work differs from previous evaluations of specs in the runtime verification and spec mining literature in three major ways.
first previous runtime verification studies most ly focused on the efficiency of monitoring but we focus on theeffectiveness question how good are the specs?
second most previous evaluations were conducted on the dacapo benchmarks with at most projects or with a smaller number of open source projects in contrast we use open source projects.
our results thus provide fresh in sights to researchers in both runtime verification and spec mining communities because our evaluation is based on a substantially larger set of more diverse projects.
we belie ve that evaluating specs on current open source projects in stead of old benchmarks can be more representative for assessing the effectiveness of specs from developers point of view and should be strongly considered in future evaluation s of specs.
third in many previous studies researchers assumed that any spec violations were bugs or decided themselves what was a bugor not butwe submit bugreports and fixes i.e.
pull requests to let the developers be the judges of the bugs we discovered by inspecting spec violations.
in our experiments we monitored the manually written and automatically mined specs while running manually written and automatically generated test s in open source projects manually inspected a subset of the spec violations and sent pull requests for the violatio ns that we believed to be bugs.
on the positive the average runtime overhead of monitoring was under .
and developers already fixed of bugs that we reported.
on the negative we found a large rate of false alarms among the inspected violations.
we inspected of violations of manually written and of violations of automatically mined specs and observed overall false alarm rates of .
and .
respectively.
further only a small fraction of the specs led to the discovery of bugs of manually written and of automatically mined specs and even among these the average false alarm rates were high .
and .
respectively.
we reported several issues about the existing specs and javamop maintainers already corrected some but in many cases the specs appear completely ineffective and should not be used at all.
inspecting spec violations and submitting pull requests to developers took an estimated hours and was challenging for three reasons.
first understanding the root cause of a violation is non trivial.
although javamop reports the line number for each spec violation reasoning about a change that could correct the violation often requires deep er understanding of the code and we were not developers on any of the open source projects moreover some of the violations were in third partylibraries so we neededto co mprehend parts of those libraries as well.
second it is challenging to decide what constitutes an actual bug that should be submitted to the developers.
at one extreme we could only submit violations that can lead to program crashes.
at the other extreme we could simply submit every violation to the developers and see what they say but this could unnecessarily burden the developers who may then blacklist us or start to desk reject our pull requests if t hey feel those are mostly useless to them .
even between these two extremes it is debatable how to classify so called cod esmells which mayindicate apimisunderstanding by developers but are harmless in the current version of the code e.g.
calling close on anoutputstream instance for whichclose is a no op.
third preparing a pull request in a way that developers would find useful requires substantial effort another reason to not even attempt to submit every violation and sometimes involved multiple internal iter ations before submission.
for all these reasons we chose to report to the developers those cases where at least one of the authors believed that a violation indicated some problem in the current version of the code.
the results from our study show that the effort spent by the runtime verification community over the last decade on improving the performance of simultaneous monitoring of parametric specs has paid off.
indeed the technology has matured enough to incur acceptable runtime overhead when monitoring test executions in open source projects agains t dozens of specs.
also the existing api specs from prior runtime verification and spec mining research can find many bugs that developers are willing to fix.
however the false alarm rates are worrisome and suggest that there is a need for the research community to fundamentally re think spec finding and spec engineering approaches towards making runtime verification a more effective early stage bug findi ng aid that developers can use.
this paper makes the following contributions large scale evaluation.
wepresentthefirstlarge scale evaluation of runtime verification during software testing with specs and open source projects.
the results show that monitoring has acceptable overhead duringtesting and can find important bugs but the specs are largely ineffective and generate way too many false alarms.
analysis of effectiveness.
we analyze reasons for bugfinding effectiveness of existing specs in particular the high rates of false alarms and discuss developers feedbac k on our pull requests.
recommendations and data.
we provide a set of recommendations that can help the research community engineer more effective specs and better evaluate these specs.
we also make data from our study publicly available .
.
background we briefly describe runtime verification of specs in javamop .collection synchronizedcollection csc shown in figure is one of the specs in our study.
csc was proposed by bodden et al.
called asynciteration to check for cases where a synchronized collection siterator is accessed from non synchronized code.
figure 1shows the three parts of a javamop spec lines 10defineeventsrelevant at runtime line 11is the formal property to monitor over the events and line 12shows user defined handlercode that javamop invokeswhen the monitored program reaches a certain state i.e.
when the spec is violated.
each spec is parameterized by the types of objects whose instances may generate the events.
specifically cscis parameterized line bycollection c anditerator i which means that one monitor object will be created at runtime for everypairofrelated candi.
thecreation keywordindicates that a monitor will be created after the syncevent occurs i.e.
when one of the synchronized methods on line 4is invoked on a collection .
the monitor subsequently listens for the events syncmk line5 asyncmk line7 andaccess 6031collections synchronizedcollection collection c iterator i 2collection c 3creation event syncafter returning collection c 4call collections .
synchronizedcollection collection .
.
.
more calls this.
c c 5event syncmk after collection c returning iterator i 6call collection .iterator target c condition thread.
holdslock c 7event asyncmk after collection c returning iterator i 8call collection .iterator target c condition !
thread.
holdslock c 9event access before iterator i 10call iterator .
.. target i condition !
thread .
holdslock this.
c 11ere sync asyncmk sync syncmk access match rvmlogging.
out .
println level .critical default message .
.
.
more printing figure example spec collections synchronizedcollection csc with its events and propert y 1im collections .
synchronizedlist .. .
synchronized im 3for iinvokedmethod iim im 4itestngmethod tm iim .
gettestmethod .
.
.
figure buggy code in testng 1specification collections synchronizedcollection has been violated on line org.testng.reporters.suitehtmlreporter.generatemeth odschronologically suitehtmlreporter.java .
documentation for this prop erty can be found at annotated java properties html java util collections synchronizedcollection.html 2a synchronized collection was accessed in a thread unsafe manner.
figure a sample violation line9 .
thesyncmkevents occur after iterator is invoked on acollection instance c to create an iterator i and the thread did synchronize on c lines5 .
theasyncmkevents occur after iterator is invoked on c but the thread did not synchronize on c lines7 .
finally the accessevents occur before any invocation of iterator methods on ifrom any thread that did not synchronize on c lines9 .
if the monitored program reaches a state where the extendedregularexpression ere propertyonline 11ismatched thenthe handlercode on line 12is invoked.
the erematches when non synchronizedcode creates an iterator from a synchronized collection sync asyncmk orwhenaccessingasynchronized collection siterator from non synchronized code sync syncmk access .
in our experiments we used the default handler in javamop print a violation containing the spec name the program line number where the spec violation occurred a url for the spec and an explanation.
as an example consider the buggy code in figure simplified from one of the six bugs we found in testng.
the lines not starting with 1and3 represent part of the original code that iterates over the synchronized collection im.
note that the forloop is not synchronized leading to a violation of the cscspec.
the violation that javamop reports is shown in figure our inspection starting from this reported line of code led us to find the bug.
the developers accepted our pull request that added the synchronization code in the lines starting with 2and6 .
.
experimental setup we describe the open source projects used in our study the specs that were monitored while running tests in thetable statistics of projects used in our study pid project sha locmantests autotests p1 altoros.ycsb bfcfe23a p2 logblock.logblock 40548aad p3 edanuff.cassandracompositetype 6d09cceb p4 jriecken.gae java mini profiler 80f3a59e p5 mqtt f4384253 p6 plista.kornakapi 178061c3 p7 threerings.playn c969160c p8 tbuktu.ntru 8126929e p9 opengamma.elsql db6c6d07 p10 sematext.actiongenerator 10f4a3e6 p11 vivin.generictree 15c59c99 p12 hoverruan.weiboclient4j 6ca0c73f p13 joda time cc35fb2e p14 ivantrendafilov.confucius 2c302878 p15 mikebrock.jboss websockets fd03a4ef p16 b3log.b3log latke afb48c40 p17 thomas s b.visualee 410a80f0 p18 asterisk java b07617fe p19 cue.lucene interval fields 8f8bff6d p20 jsqlparser 001d665d p21 ovea.jetty session redis afb2b25b p22 bcel 24014e5e p23 zookeeper utils a2b80474 p24 bucchi.oauth2.0providerforjava db5e1d06 p25 htrace c32ec0b1 p26 ptgoetz.storm jms d152d72f p27 urbancode.terraform d67ac40c p28 pignlproc 1a609980 p29 jmxtrans.embedded jmxtrans 4f1ce2cc p30 apache.gora bb09d891 f69 projects with far various n101 projects without violations various total avg .
.
.
min max projects and how we automatically generated tests using randoop .
we also explain our procedure for running javamop and for inspecting resulting violations.
.
experimental subjects we selected the projects for our studyfrom github starting from a list of the most popular java projects.
from these we selected projects that i used maven for ease of automation ii had at least one test so we can monitor test runs iii had all tests pass without monitoring and iv had all tests pass when monitoring with javamop.
requirements iii and iv are important to havea fair measurement of runtime overhead of javamop if tests were to fail between the two runs with and without monitoring they may fail at different points in the execution leading to rather different time measurements.
furthermore tests could fail due to problems in the project or due to integration of javamop.
for example we observed some failures of time sensitive tests that have some timeouts result ing from the overhead of javamop.
we also observed test failures that happened because javamop instrumentation interacted unexpectedly with some other instrumentation frameworks e.g.
test mocking frameworks.
we already reported some of these issues to the javamop project .
table1lists some basic statistics about the projects used in our study.
pid either starts with p to provide the 604short id of a project in which we found some real bug or summarizes multiple projects with similar characteristic s f69 summarizes projects in which all inspected violations were false alarms and n101 summarizes projects in which no violations were generated for the specs that we inspected.
project is the project name sha is the project revision we used loc is the number of java lines in the project mantests is the number of manually written tests and autotests is the number of automatically generated tests.
marks that we did not have randoop tests which happened for projects with multiple maven modules projects where generated tests did not compile and projects where randoop did not generate any test within the time limit.
for f69 and n101 mantests and autotests show the sums for all respective projects.
the rows total avg min and max are the sum average minimum and maximum across all projects in each column.
.
specs used in this study all java api specs that we used in our study were obtained from the literature manually written specs and automatically mined specs .
we describe our rationale and procedure for selecting each set of specs.
.
.
manually written specs we used manually written javamop specs which are publicly available .
the specs were written by lee et al.
who read javadoc comments in four widelyused packages java.lang java.net java.io andjava.util and formalized sentences describing must should or i s better to conditions.
the specs are formalized using finite state machines extended regular expressions linear temp oral logic andcontext freegrammars.
javamop canmonitor specs in any formalism for which a suitable plugin exists.
to illustrate manual formalization of specs consider agai n thecscspec from section .
it was formalized from text incollections.synchronizedcollection method s javadoc it is imperative that the user manually synchronize on the returned collection when iterating over it ... failure to fo llow this advice may result in non deterministic behavior .
section2explained cscin detail.
as mentioned this spec had been also used earlier by analyzing javadoc comments lee et al.
ended up with some of the same specs that others had formalized before.
monitoring cscin our experiments revealed bugs in several widely used projects including testng activemq andxstream.
however our experiments also revealed issues and opportunities for impro ving the manually written specs discussed in section .
.
.
.
automatically mined specs to compare the effectiveness of manually written specs and automatically mined specs we monitored of the specs automatically mined by pradel et al.
.
before settlingonthese specs weperformed amini surveyo f the spec mining literature to search for specs and to identif y how spec mining was evaluated.
paper search we searched for spec mining papers on dblp using this query specification propert contrac t invariant precondition mining monitor enforce infe r mi ne venue icse venue ase venue rv venue pldi venue po pl v enue issta venue ieee trans software eng tse venue sigs oft fse venue autom softw eng ase venue esec sigso ft fs e venue tacas venue icsm venue icsme venue sas venu e sactable mini survey.
ref