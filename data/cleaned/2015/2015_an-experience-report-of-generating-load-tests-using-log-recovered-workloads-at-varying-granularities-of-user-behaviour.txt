an experience report of generating load tests using log recovered workloads at v arying granularities of user behaviour jinfu chen weiyi shang ahmed e. hassan y ong wang diamondmath jiangbin lin diamondmath department of computer science and software engineering concordia university montre al canada school of computing queen s university kingston canada alibaba group hangzhou china diamondmath fuchen shang encs.concordia.ca ahmed cs.queensu.ca wangyong.wy jiangbin.lin alibaba inc.com diamondmath abstract designing field representative load tests is an essential step for the quality assurance of large scale systems.
practitioners may capture user behaviour at different levels of granularity.
a coarse grained load test may miss detailed user behaviour leading to a non representative load test while an extremely fine grained load test would simply replay user actions step by step leading to load tests that are costly to develop execute and maintain.
workload recovery is at core of these load tests.
prior research often captures the workload as the frequency of user actions.
however there exists much valuable information in the context and sequences of user actions.
such richer information would ensure that the load tests that leverage such workloads are more field representative.
in this experience paper we study the use of different granularities of user behaviour i.e.
basic user actions basic user actions with contextual information and user action sequences with contextual information when recovering workloads for use in the load testing of large scale systems.
we propose three approaches that are based on the three granularities of user behaviour and evaluate our approaches on four subject systems namely apache james openmrs google borg and an ultra large scale industrial system sa from alibaba.
our results show that our approach that is based on user action sequences with contextual information outperforms the other two approaches and can generate more representative load tests with similar throughput and cpu usage to the original field workload i.e.
mostly statistically insignificant or with small trivial effect sizes .
such representative load tests are generated only based on a small number of clusters of users leading to a low cost of conducting maintaining such tests.
finally we demonstrate that our approaches can detect injected users in the original field workloads with high precision and recall.
our paper demonstrates the importance of user action sequences with contextual information in the workload recovery of large scale systems.
index t erms workload recovery load tests software log analysis software performance i. i ntroduction large scale software systems e.g.
amazon aws and googles gmail have brought a significant influence on the daily lives of billions of users worldwide.
for example netflix services million subscribers across the globe .
as a result the quality of such systems is extremely important.
failures of such planet scale systems can result in negative reputational and monetary consequences .
quite oftenfailures in such systems are load and performance related rather than due to functional bugs .
hence load tests are widely used in practice to ensure the quality of such systems under load.
the goal of a load test is to ensure that a system performs well in production under a realistic field workload.
therefore one must first recover a workload then design a load test based on the recovered workload .
the recovery of a field representative workload is a challenging task.
in particular one must achieve a balance between the level of granularity of the workload and the cost to conduct a load test with such a workload.
all too often in practice the recovered workloads are too coarse i.e.
over simplified workloads.
for example the specweb96 benchmark defines a workload that only specifies the probability of accessing files such as files less than 1kb account for of all requests .
such coarse grained workloads fail to capture the variance of user behaviour leading to non representative load tests.
on the other extreme a workload can simply replay the exact field workload step by step.
although such a workload replicates the exact user behaviour conducting a load test using such a workload and maintaining such a workload are extremely costly.
first of all due to the large amount of users of these systems replaying the exact workload requires the load tests to simulate every user with a great amount of contextual information and complexity.
one would also need to develop simulation code for each specific sequence of events.
in addition since it is almost impossible to observe the exact same workload twice one would constantly need to update such a detailed workload.
to reach a desirable level of granularity for a workload prior work often clusters user bahaviour based on important aspects in the workload .
with the clusters of users instead of maintaining millions or billions of user profiles a workload is designed based on representative user behaviours from a considerably smaller number of clusters.
for example a recent workload clustering approach clusters users by the frequency of different user actions .
however due to the high variability of users in ultra large scale software 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
systems we argue that solely considering the frequency of actions is too coarse instead the sequence and the context of user actions can make workloads much more representative to the actual field.
consider the following example one user repetitively reads small pieces of data from a file then writes each of the small pieces back to the file while another user interactively reads and writes a large number of small pieces of data to a file.
a workload should capture both users differently.
however only considering the frequency of actions read and write would not differentiate the workloads of these two users.
adding more detailed information about these user actions would lead to a finer granularity of workload which in turn might be too costly to recover execute and maintain.
therefore in this paper we report on our experience in understanding the impact of adding different levels of details in the recovered workloads for load tests.
we first replicate a prior approach that captures the frequency of basic actions we refer to this approach as action .
afterwards we design an approach that enriches the basic user actions with their context values we refer to this approach as actioncontext .
finally we design an approach that augments actioncontext with the frequently appearing sequences of actions we refer to this approach as actionsequence .
the three approaches use the frequency of actions the frequency of enriched actions and the frequency of sequences of enriched actions respectively as signatures for each user then group the users into clusters.
afterwards we automatically generate load tests based the signature of the representative center user in each cluster.
our study is performed on two open source systems apache james and openmrs and two commercial systems google borg and an ultra large scale software system from alibaba we refer to it as sa in the rest of the paper .
we compare our three approaches by recovering workloads based on the execution logs from the subject systems generating load tests and running the automatically generated load tests on the subject systems.
in particular we answer these two research questions rq1 how field representative are our generated workloads?
actionsequence generates the most fieldrepresentative workloads.
when conducting load tests using actionsequence the throughput of out of user actions as well as the cpu usage are either statistically insignificant to the original workload or differ with a small trivial effect size.
rq2 how many clusters of users are captured by each of our recovery workload approaches?
the number of clusters of users is not overwhelming.
the most field representative workload actionsequence is based on eight to clusters of users which is only three to six clusters more than a less field representative workload actioncontext .
the least field representative workload action is based on as few as two clusters of users.
the rest of the paper is organized as follows section iidiscusses the background and the related work to this paper.
section iii describes our approaches in detail.
section iv presents our case study setup.
section v presents our case study results by answering our two abovementioned research questions.
section vi discusses other usage scenarios for our approaches.
section viii discusses section vii discusses the challenges that lessons learned from the industrial evaluation.
finally section ix concludes the paper.
ii.
b ackground and rela ted work workload recovery is an essential part in the performance assurance of large scale systems.
prior research proposes approaches for recovering workloads to assist in the design of load tests validating whether load tests are fieldrepresentative as production optimizing system performance and detecting system performance issues .
all above prior work illustrates the value and importance of recovering representative workloads.
prior approaches for recovering and replaying workloads can be categorized along the granularity of the captured user actions.
one may choose a coarse granularity by recovering only the type of workload from a system or to the other extreme considering each individual user and replaying their individual workload one by one.
one may anonymize all high level user behaviours and only consider the physical metrics such as cpu i o and other system resources .
one may choose a finer granularity by building complex models such as hidden markov models to capture the details for each user.
a pilot study by cohen et al.
demonstrates that grouping workloads into a smaller number of clusters outperforms having one unified workload.
intuitively recovering a workload at a too fine or too coarse grained detail is neither desired.
a too coarse grained approach may miss the important characteristics of user behaviour leading to a non representative workload while a too fine grained approach may lead to a workload that is costly to replay and maintain.
to achieve an optimal granularity of user behaviour prior research often chooses event or action driven approaches for workload recovery .
however there exists extensive research on execution log analysis that demonstrates the value of considering contextual information and sequence of actions for various software engineering tasks .
such extensive usage of contextual information and user action sequences in log analysis motivates our approach to leverage the similar information to recover richer workloads from execution logs for generating load tests.
as an experience report our focus is primarily on exploring whether research approaches work in practice.
based on our industrial experience this was not the case.
hence we had to propose two novel approaches.
in particular compared to prior research our work uses more valuable contextual and action sequence information from execution logs to recover workloads.
the next section presents our three approaches to cluster user actions contextual information and user action authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i our running example of execution log lines with extracted user actions and context v alues .
timestamp user log line action byte .
alice get .
.
.
openmrs ws rest v1 person search .
dan get .
.
.
openmrs ws rest v1 person search .
bob get .
.
.
openmrs ws rest v1 person search .
alice delete .
.
.
openmrs ws rest v1 person delete .
alice get .
.
.
openmrs ws rest v1 person search .
bob delete .
.
.
openmrs ws rest v1 person delete .
bob post .
.
.
openmrs ws rest v1 person addperson add .
dan post .
.
.
openmrs ws rest v1 person addperson add .
bob get .
.
.
openmrs ws rest v1 person search .
alice delete .
.
.
openmrs ws rest v1 person delete .
bob delete .
.
.
openmrs ws rest v1 person delete .
bob post .
.
.
openmrs ws rest v1 person addperson add .
dan get .
.
.
openmrs ws rest v1 person search .
alice get .
.
.
openmrs ws rest v1 person search .
dan delete .
.
.
openmrs ws rest v1 person delete .
dan get .
.
.
openmrs ws rest v1 person search .
alice post .
.
.
openmrs ws rest v1 person addperson add .
dan delete .
.
.
openmrs ws rest v1 person delete .
bob get .
.
.
openmrs ws rest v1 person search .
alice post .
.
.
openmrs ws rest v1 person addperson add .
dan post .
.
.
openmrs ws rest v1 person addperson add .
bob post .
.
.
openmrs ws rest v1 person editperson edit .
dan post .
.
.
openmrs ws rest v1 person editperson edit .
alice post .
.
.
openmrs ws rest v1 person editperson edit sequences from execution logs for recovering a workload for load testing.
iii.
o ur approaches of recovering a workload for load testing in this section we present our approaches to automatically recover workloads using system execution logs.
an overview of our recovery process is shown in figure .
in total our recovery process consists of six steps a extracting user actions b enriching user actions with context c identifying frequent action sequences d grouping similar frequent action sequences e grouping users into clusters and f generating load tests.
our action workloads are generated by steps a e and f of our recovering process.
our actioncontext workloads are generated by steps a b e and f. our actionsequence workloads are generated by all the steps.
a. extracting user actions user actions are a typical source of information to recover workload .
therefore in this step we extract user actions from execution logs that are generated during the execution of a software system.
execution logs record system events at runtime to enable monitoring remote issue resolution and system understanding .
each line of an execution log contains a corresponding action and its contextual information e.g.
user names and data sizes .
in this step we first parse the execution logs by identifying the actions and their contextual information.
for example in table i we extract four types of actions from the logs that are generated by openmrs.
we also extract the timestamp the user and the byte values as the contextual values of each action.
after extracting user actions the workload signature of each user can be represented by one point in an n dimensional space where nis the total number of extracted actions i.e.
the n dimension vector for each user records the number of occurrence of each action with each action type mapped to one dimension.
such vectors are directly fed into step e to recover our action workload.
table ii shows the vectors of the frequency of user actions from our running example.table ii frequency of actions for users in our running example for the action workloads .
user actions search add edit delete alice bob dan table iii frequency of enriched actions with context for users in our running example for the actioncontext workloads .
enriched actions search1 search2 add1 add2 edit1 delete1 alice bob dan b. enriching user actions with context each instance of a user action is associated with a context which may contain useful information to represent workload.
for example a disk read event is often associated with the size of the read.
however a disk read event with a large size versus one with a small size of data may correspond to different user behaviours.
a small disk read may correspond to a user reading a data index while a large disk read may correspond to the actual data reading.
therefore in this step we enrich the recovered user actions by considering their context values.
in particular we split each action into multiple ones by categorizing the context values.
for example a disk read action may become two different actions i.e.
a large read disk and a small read disk.
in particular we use jenks natural breaks on the context values of each action.
jenks natural breaks is a one dimension number clustering algorithm which minimizes each class s average deviation from the class mean while maximizing each class s deviation from the means of the other clusters.
in our running example we consider the byte value of each action as its context and for the search action we split it into two actions search1 bytes to bytes and search2 bytes to bytes .
after this step the workload signature of each user becomes a vector where each dimension is the number of occurrence of each enriched user action.
such vectors are directly fed into step e to recover our actioncontext workload.
table iii show the vectors of enriched user actions from our running example.
c. identifying frequent action sequences users often perform multiple actions frequently together.
in order to capture these actions we identify action sequences that frequently appear during system execution.
splitting user action sequences we first group all user actions by each user and sort the actions by their timestamp in order to generate a sequence of actions for each user.
such sequences are often very long consisting of thousands of actions while each user may not perform all the action at once i.e.
a user may perform two series of actions with a long period of idling time in between.
therefore we wish to split the long user action sequences into smaller ones.
one naive approach to split long user action sequences is to consider the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
process data execution logsa.
extracting user actionsb.
enriching user actions with context d. grouping similar frequent action sequences d. computing distance matrixe.
grouping users into clusters e. hierarchical clusteringf.
generating load tests f. generating a workload for load testsf.
executing the load testc.
identifying frequent action sequences c. splitting user action sequencesc.
extracting frequent subsequencessoftware system user actionssequence of actions by each user frequent action sequences list of users distance matrixd.
transforming vectors of frequent action sequences with the distance matrixfrequent action sequences after transformation e. dendrogram cuttingenriched user actionsgroup enriched actions by users fig.
.
an overview of our workload recovery process.
time interval between two actions i.e.
if there is a long time interval between two actions the sequence is split into two actions.
however some actions may actually require a long time to finish leading to the long wait between two actions.
in such cases the naive approach may incorrectly split the sequences.
therefore we wish to identify the actions where the long running time is due to the idling between two septate actions.
we leverage a heuristic that considers the relationship between context values and the time interval of each user action as an indicator of the type of wait.
for example a data read action with a large data size may take longer time than a small size.
based on this intuition we build a linear regression model using the associated context values of each action as independent variables and its time interval to the next action as the dependent variable.
with the linear regression model if an action has a time interval higher than the predicted value with a residual that is greater than our process considers that the user has idled after the action.
in our running example we split user dan s events into three sequences search2 delete1 search2 delete1 search2 add2 and add1 edit1 .
extracting frequent sub sequences of actions we aim to mine frequent sub sequences in order to represent the series of actions that a user may often perform.
an example frequent sub sequence can be a user first reading small data to locate the data using an index before repetitively reading large data reading the actual data i.e.
small read large read large read large read .
we apply a suffix array structure and the longest common prefix lcp algorithm to mine frequent sub sequences for each sequence of user actions.
such an approach has been used in prior research to uncover usage patterns from execution logs .
we re implemented the same algorithm as prior research .
due to the limited space our detailed implementation can be found in our replication package.
.i n our running example one of the frequent sub sequences that we extract is search2 delete1 which is identified originally from the sequence search2 delete1 search2 delete1 .
since some sub sequences may be trivial too short or do not frequently appear too rare we rank the extracted subsequences based on the frequency of their occurrence and the iv frequency of frequent action sequences in our running example for the actionsequence workloads .
frequent action sequences add1 add2 search1 search1 search2 search2 search2 edit1 edit1 delete1 edit1 add1 add2 delete1 add1 alice bob dan frequency of events in the actual sub sequence as follows rank occurrence events where occurrence is the frequency of a sub sequence s occurrence and events is the number of events in the sub sequence.
is a weight factor for the number of subsequence s occurrence.
we determine as .
since we consider the occurrence and events to be equally important.
we use the rank value to keep the top sub sequences such that the kept sub sequences cover more than of all actions.
we call these kept sub sequences as frequent action sequences.
after extracting the frequent action sequences the workload of each user is represented by one point in an n dimensional space where nis the total number of identified frequent action sequences i.e.
a vector for each user where each dimension is the frequency of each frequent action sequence.
table iv shows the result of frequent action sequences in our example.
d. grouping similar frequent action sequences the extracted frequent action sequences are not independent from each other.
intuitively for example two frequent action sequences read1 read2 read2 read1 and read1 read2 read1 are similar.
one user may only have read1 read2 read2 read1 and another user may only have read1 read2 read1 .
the two users may be considered completely different if we do not consider the similarities between the two frequent action sequences.
computing distance matrix for all the frequent action sequences we calculate the distance between each pair of them using the normalized levenshtein distance .
for example the normalized levenshtein distance between read1 read2 read2 read1 and read1 read2 read1 is .
.
transforming vectors of frequent action sequences with the distance matrix in order to address the similarities be672 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v result of frequent actions sequences after transforma tion based on distance ma trix for actionsequence workload .
frequent action sequences add1 add2 search1 search1 search2 search2 search2 edit1 edit1 delete1 edit1 add1 add2 delete1 add1 alice .
.
.
.
bob .
.
.
.
.
.
dan .
.
.
.
.
.
tween frequent action sequences we apply a vector transformation based on the distance matrix as follows vectoru angbracketleftbigsu .
.
.
su n angbracketrightbig d1 d2 ... dn d1 d2 ... dn ... d1 n d2 n... dn n wherevectoruis the final vector for user u su nis the frequency of frequent action sequence nfor useruandd1 nis the normalized levenshtein distance between frequent action sequence 1and frequent action sequence n. we perform a vector transformation in our running example and the result is shown in table v. e. grouping users into clusters in order to reach a desirable level of granularity for a workload in this step we apply a clustering algorithm to group users into clusters.
hierarchical clustering we apply a hierarchical clustering to cluster users based on the pearson distance.
we choose hierarchical clustering since it is suitable for data with arbitrary shape and there is no need to determine a specific number of clusters beforehand .
in addition hierarchical clustering performs well in prior research of workload recovery .
in our approach hierarchical clustering first considers each user as an individual cluster.
afterwards we merge the most neighbouring clusters into a new cluster and recalculate the pearson distance matrix between each two clusters based on average linkage.
dendrogram cutting hierarchical clustering can be visualized using a dendrogram.
such a dendrogram must be cut with a horizontal line at a particular height to create our clusters.
in practice one may choose a desired level of granularity in a recovered workload by cutting the dendrogram at different heights in order to retrieve a different number of clusters.
to avoid any subjective bias we use the calinski harabasz stopping rule to perform our dendrogram cutting.
prior research notes that the calinski harabasz stopping rule outperforms other rules when clustering workload signatures .
the calinski harabasz index measures the dissimilarity of the intra cluster variance and the similarity of inter cluster variance.
in our running example alice bob and dan are all grouped into one cluster for the action workload.
users alice and dan are grouped into one cluster while bob is in another cluster for the actionsequence and actioncontext workloads.
f .
generating load tests in the final step of our process we generate the load tests as the final outcome of our approach.
generating a workload for load tests we identify a representative user in each cluster of users to generate a workload for load testing.
we apply the partitioning around me pam algorithm to identify the representative point of each cluster.
pam is based on the krepresentative me among the instances of the clustering data.
pam is an iterative process of replacing representative instances by other instances until the quality of the resulting clustering cannot be improved.
the quality is measured by the me with the smallest average dissimilarity to all other points.
in our case we set the k as since we only choose one user to represent each cluster.
we then iterate each user inside the cluster to find the best representative user based on pam.
after obtaining a representative user for each cluster we obtain a vector angbracketleftbig su .
.
.
su n angbracketrightbig for that user where su nis the number of occurrences of frequent action sequences n from user u for the actionsequence workload.
we use the frequency of occurrences of each frequent action sequences from the representative user to calculate a probability of occurrence of that frequent action sequence.
then we generate the synthesized workload based on such probability of frequent action sequences.
in our running example the center of the cluster that consists of users alice and dan is user dan.
then to generate a workload for user alice in the load test we replace the corresponding actions into search2 delete1 with a probability of search2 add1 with a probability of and add2 edit1 with a probability of because each of them has a frequency of two one and one respectively see table iv .
for the action and the actioncontext workload this step is similar to above but instead a probability of having an action or enriched action is calculated.
for the action workload since all the users are in one cluster the load test is generated based on the probability of having each action shown in table ii i.e.
the search add edit and delete actions have a probability of .
.
and respectively.
for the actioncontext workload users alice and dan are in one group with exactly the same distribution frequency of actions with context values.
therefore the generated load test where the search2 action has a probability of .
the add1 add2 and edit1 actions each have a probability of .
and the delete1 action has a probability of .
executing load tests finally our approach executes the load tests based on fio and jmeter .
for software systems that cannot be directly driven by fio and jmeter our approach outputs simulated execution logs.
such systems can generate the load test by directly replaying the workload based on our simulated execution logs line by line.
iv .
c ase study setup in this section we present the setup of our case study.
a. subject systems we choose two open source systems including apache james and openmrs as well as two industrial systems authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi overview of our subject systems .
subjects v ersion sloc k users lines of logs k apache james .
.
.
.
openmrs .
.
.
google borg may n a sa n a greatermuch5 greatermuch1 including google borg and one large software system sa as our subject systems.
apache james is a java based mail server developed by the apache foundation.
openmrs is an opensource health care system to develop to support customized medical records.
openmrs is a web system developed using the model view controller mvc architecture.
google borg is a large scale cluster management system that are used to manage internal workloads and schedule machines jobs and tasks.
sa is an ultra large scale cloud computing service application that is deployed to support business worldwide and used by a hundred of millions of users.
due to a nondisclosure agreement nda we cannot reveal additional details about the system.
we do note that the sa system is one of the largest in the world in its domain with a long development history.
all our subject systems cover different domains and are studied in prior research .
the overview of the four subject systems is shown is table vi.
b. data collection in this subsection we present how we collect execution logs in order to study the use of our different workload approaches.
in particular for the two open source systems apache james and openmrs we deployed the systems in our experimental environment and conducted load tests to exercise the systems.
we then collected execution logs that are generated during the load tests.
we also collected the cpu usage of both systems by monitoring the particular process of the system with a performance monitoring tool named pidstat for every five seconds.
the data from the two industrial systems are from real end users.
the production deployment of sa provides the cpu usage of the system while google borg does not provide the cpu usage hence we do not evaluate that aspect for this system .
we discuss the details of our data collection for each of our subject systems.
the details of our data collection can be found in our replication package.
a apache james we use jmeter to create load tests that exercise apache james.
we replicate a similar workload to prior research .
in particular we simulated email users who send receive and read different sizes of emails with and without different sizes of attachment.
users may only read the email header or load the entire email.
we deploy apache james on a server machine with an intel core i7 8700k cpu .70ghz gb memory on a 3tb sa ta hard drive.
we run jmeter on a machine with intel core i5 cpu .10ghz gb memory and 320gb sa ta hard drive to generate a two hours workload.
b openmrs we used the default openmrs demo database in our load tests.
the demo database contains data for over 5k patients and 476k observations.
openmrs containsfour typical scenarios adding deleting searching and editing operations.
we designed the load tests that are composed of various searches of patients concepts encounters and observations and addition deletion and editing of patient information.
to simulate a more realistic workload we added random controllers in jmeter to vary the workload.
we deployed the openmrs on two machines each with intel core i5 cpu .10ghz gb memory 512gb sa ta hard drive.
one machine is deployed as application server and the other machine as a mysql database server.
openmrs provides a web based interface and restful services.
we used the restful api from openmrs and ran jmeter on another machine with the same specification to simulate users in the client side with an eight hours workload.
c google borg we used a publicly available open dataset from the google borg system .
the data is published by google with a goal of workload related research.
due to the large size of google borg data we only picked the first part of data to analyze which consists of minutes of data from the entire google borg cluster.
due to the inaccessibility of the google borg system we could not run load tests directly on the system.
in the data from google borg there exists no information about users.
however the workload is described asjobs .
therefore we considered each job as a user when applying our approach on the google borg data.
d sa we retrieved the execution logs and the corresponding cpu usage from sa that is deployed in production and is used by real users.
the sa is deployed on a cluster with more than a thousand machines.
due to the nda we cannot mention the detailed information of the hardware environment and the usage scenarios of sa.
c. preliminary analysis clustering tendency before we answer the research questions for our case study we first conduct a preliminary analysis on the clustering tendency of the data from our subject systems.
if the users from our subject systems have random behaviour and do not appear to have inherent groups of similar behaviours the data from our subject systems would be unsuitable for our study.
therefore we calculated hopkins statistic to assess the cluster tendency of our data.
hopkins statistic is a statistical hypothesis test that can be used to accept or reject the random position hypothesis .
the value of the hopkins statistic ranges from to .
a value of means that the data has a high cluster tendentious a value of indicates the data is uniformly distributed not cluster tendentious .
similar to previous research we used .
as the threshold to reject the alternative hypothesis.
if the value of the hopkins statistic is higher than .
we consider that the data has a high clustertendentious.
we used the function hopkins of the clustertend package in r to calculate the hopkins statistic.
we observe that our data has a high cluster tendentious with hopkins statistic values that range between .
to .
with an average of .
across all of our subject systems.
since the hopkins statistic values are higher than .
we reject the alternative hypothesis and confirm that our data is suitable for our study.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
v. c ase study results in this section we present our case study results by answering two research questions.
rq1 how field representative are our generated workloads?
motivation.
in order to illustrate a practical impact we wish to first examine whether the generated workloads can lead to similar system behaviour and performance as the original system workload.
if the system behaviour and performance that are produced by the generated workloads are drastically different from the original workload such automatically generated workloads are not field representative and hence would not be useful in practice.
approach.
we use all three workload approaches action actioncontext and actionsequence to generate load tests based on the execution logs of the subject systems.
the execution logs from google borg do not contain any context values hence we only use the action and actionsequence approaches to generate load tests for google borg.
when running the generated load tests we monitor the behaviour and the performance of the systems.
for the behaviour of the system we measure the throughput of each type of action for every minute during the execution of the load tests.
for the system performance we measure the cpu usage of the particular process of the system.
in the load tests we use a performance monitoring tool named pidstat to collect the physical performance for every five seconds.
for our subjects apache james openmrs and sa we are able to run the generated load tests directly on the system to measure system behaviour and performance.
however for the subject google borg we cannot directly run the load tests since we do not have access to the system.
therefore we cannot measure the system performance.
since we can generate simulated execution logs for load tests we use the simulated execution logs to compute the throughput of each type of user action.
we perform statistical analysis to examine the existence of significant differences between the generated workloads and the original workload in terms of the throughput of each action and the cpu usage.
we use the mann whitney u test to determine if there exists a statistically significant difference i.e.
p value .
.
we choose the mann whitney u test because it does not enforce any assumptions on the distribution of the data.
reporting only the statistical significance may lead to erroneous results i.e.
if the sample size is very large pvalue can be small even if the difference is trivial .
hence we use cliff s delta to quantify the effect size .
the smaller the effect sizes the more similar the workload is to the original workload.
since statistical tests do not consider the trend of the actions we visualize the differences between the number of each type of actions during execution from the load tests and the original workload using cumulative density graphs.
results.
the load tests from our recovered workloads have similar system behaviour to the original workload.
the results of throughput of actions between the original workload and the generated load tests are shown in table vii.
in out of user actions in all the subject systems at least onetable vii comparing throughput between the original workload and the genera ted workloads .
openmrs action add delete throughput comparing with original throughput comparing with original per minute p value effect size per minute p value effect size original n a n a n a n a action lessmuch0.
.
large lessmuch0.
.
small actioncontext lessmuch0.
.
large lessmuch0.
.
large actionsequence lessmuch0.
.
small lessmuch0.
.
small action exit search throughput comparing with original throughput comparing with original per second p value effect size per second p value effect size original n a n a n a n a action lessmuch0.
.
small lessmuch0.
.
large actioncontext lessmuch0.
.
large lessmuch0.
.
large actionsequence lessmuch0.
.
medium lessmuch0.
.
medium apache james action send receive throughput comparing with original throughput comparing with original per minute p value effect size per minute p value effect size original n a n a n a n a action lessmuch0.
.
medium lessmuch0.
.
large actioncontext lessmuch0.
.
small lessmuch0.
.
medium actionsequence .
.
small lessmuch0.
.
small sa action action a action b throughput comparing with original throughput comparing with original per minute p value effect size per minute p value effect size original n a n a n a n a action .
.
large lessmuch0.
.
large actioncontext .
.
trivial lessmuch0.
.
large actionsequence .
.
trivial .
.
small google borg action submit schedule throughput comparing with original throughput comparing with original per minute p value effect size per minute p value effect size original n a n a n a n a action .
.
trivial .
.
trivial actionsequence .
.
trivial .
.
trivial action fail finish throughput comparing with original throughput comparing with original per minute p value effect size per minute p value effect size original n a n a n a n a action .
.
small .
.
medium actionsequence .
.
trivial .
.
medium action evict kill throughput comparing with original throughput comparing with original per minute p value effect size per minute p value effect size original n a n a n a n a action .
.
medium lessmuch0.
.
medium actionsequence .
.
medium .
.
trivial note bold font indicates that the load tests from the corresponding approaches are representative to the original workload p value .
or effect sizes trivial or small .
elapsed time minute cumulative actions original action actioncontext actionsequence fig.
.
cumulative density plot of the number of user actions from the original workload and the generated load tests for the receive action in apache james.
workload is field representative bold in table vii .
even the most coarse grained workload action has a similar system behaviour in five out of user actions while the most fine grained workload actionsequence has a similar system behaviour in out of user actions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table viii comparing cpu usage between original workload and the load tests genera ted by our workload approaches .
comparing with original action actioncontext actionsequence subjects p value effect size p value effect size p value effect size apache james lessmuch0.
.
large lessmuch0.
.
large lessmuch0.
.
small openmrs lessmuch0.
.
large lessmuch0.
.
large lessmuch0.
.
small sa lessmuch0.
.
large lessmuch0.
.
medium lessmuch0.
.
small note the bold fond indicates the most field representative workload.
the load tests generated by actionsequence outperform the ones from action and actioncontext .we observe that in out of user actions actionsequence outperforms action and actioncontext when comparing the throughput of the user actions.
for example for the send action in apache james the load test from actionsequence generates actions per minute which is much closer to the original workload actions per second than action actions per minute and actioncontext actions per minute .
we also use cumulative density graph to evaluate the trend of each action between the original and the load tests that are generated from our different workloads.
the x axis of the graphs is the elapsed time of the load tests and the y axis of the graph is the total number of actions.
due to space limitation we only present the cumulative density graph for one action of apache james see figure .
the detailed cumulative density graph for each user action can be found in our replication package.
the graphs show that the trend of the receive action from the actionsequence workload is much closer to original workload than the action and actioncontext workloads.
in particular figure shows that the total of number of user actions from the action workload is far from the original.
the actionsequence workloads produce system performance closer to the original workload than the action and actioncontext workloads .
shown in table viii the effect sizes are always trivial or small between the cpu usage during the load tests that are generated from the actionsequence workloads and the original workload.
on the other hand the effect sizes of cpu usage differences are medium to large when comparing the original workload with the action and actioncontext workloads.
rq2 how many clusters of users are captured by each of our recovery workload approaches?
motivation.
our workloads may contain a large number of clusters of users leading to a too fine of a granularity for load tests.
if our workloads consist of an overwhelming amount of clusters they would not be useful for practitioners due to the large overhead of developing the infrastructure to execute such workloads as well as executing and maintaining them.
approach.
we use all the workloads of each of our approaches to generate load tests for the four subject systems.
we compare the number of clusters of users that are recovered by each workload.
afterwards we manually examine each cluster of users to understand the differences between our different workload recovery approaches.
results.
our approaches do not generate an overwhelming number of clusters.
the numbers of generated user clusterstable ix number of user clusters for each of our workload approaches .
subjects action actioncontext actionsequence apache james openmrs google borg sa are shown in table ix.
although including richer user information in our workloads we do not generate an overwhelming number of clusters.
in particular google borg and sa are both large scale systems with a large amount of end users while our process only generates a maximum of and clusters for the actionsequence approach for google borg and sa respectively.
however the granularity of the action workloads is very coarse.
in particular for the three subject systems i.e.
apache jame openmrs and sa the action workload only consists of two to three clusters of users.
such a small number of clusters helps explain the results from rq1 where the action workload is not able to generate field representative load tests.
on the other hand the most field representative workload from rq1 i.e.
the actionsequence workloads consists of only three to six more clusters than the actioncontext workloads.
such a small number of clusters of users make it possible to manually examine each cluster to qualitatively understand the field workload.
we manually examine each cluster of users and aim to understand the difference between the recovered clusters for the actionsequence and actioncontext workload recovery approaches.
we identify three typical scenarios that cause the differences.
orders of actions.
some users have the same distribution of actions but they are ordered differently.
such differences are not captured by actioncontext workloads.
similar sequences of actions but different distribution.
user may have the same action sequences but the general distribution of each sequence of actions is different.
actioncontext workloads would consider the users into different clusters while actionsequence workloads group the users together.
v arying frequency of actions.
some users have the same distribution of action sequences although their frequencies are varying differently.
actioncontext workloads would consider the users in the different clusters while actionsequence workloads consider them the same.
such scenarios show that the actionsequence considers a different levels of granularities of user behaviours which may explain the differences in clusters.
vi.
d iscussion in this section we discuss the use of our three workload recovery approaches to detect unseen workloads and a sensitivity analysis.
a. detecting unseen workload one of the challenges of designing load tests is keeping the load tests field representative as the user workloads evolve .
when there exist users with unseen workload practitioners should be informed in order to act accordingly.
for example if the unseen workload is due to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
new user behaviours one may wish to update the load tests.
on one hand if the recovered workloads from our approaches are too fine grained our approaches would report false positively unseen workloads leading to additional wasted costs to practitioners.
on the other hand if our recovered workloads are too coarse grained we may miss the reporting of unseen workloads leading to load tests that are not fieldrepresentative.
therefore we study the use of our workload recovery process to detect unseen workloads by injecting users with unseen workloads into our existing data.
we injected four types of unseen workloads that are typically used in prior research .
extra actions.
we randomly pick one action that is not the most frequent action.
then we replace all occurrences of the picked action by the most frequent action.
removing actions.
we randomly pick an action type and remove all occurrences of the action from a user.
double context values.
for every action with a context value we change the context value by doubling it.
reordering actions.
for all the actions that are performed by a user we randomly reorder the actions.
for every type of unseen workload we randomly select one user and alter the data from the user to inject the unseen workload.
we apply each of our approaches to recover workloads from the data with only one user injected with an unseen workload.
in particular if one user is located in one cluster without any other users we consider that the user has an unseen workload the user is not similar to any other one .
if the user is indeed injected with an unseen workload we consider it as a true positive detection.
any normal user located in a one user cluster will be considered as a falsepositive detection.
we repeat this process times with every time injecting a random user with an unseen workload for every type of unseen workload.
in total for our four subject systems we have sets of data each of them having one user injected with an unseen workload.
we generate workloads to detect those users.
we define precision as the number of the true positive detection divided by the total number of users that are in a one user cluster recall as the number of the true positive detection divided by the total number of users with injected unseen workloads.
actioncontext and actionsequence workloads can accurately detect the injected users with an unseen workload.
the results of detecting injected users with an unseen workload is shown in table x. the results show that actioncontext workloads have a precision between .
to with an average of .
and a recall between .
to .
with an average of .
.
the actionsequence workloads achieve a precision between .
to with an average of .
and a recall between .
to with an average of .
.
however the action workloads have both low average precision .
and recall .
.
the precision and recall of the actionsequence workloads are generally consistently high across all subject systems for all types of injected unseen workloads.
the only exception is apache jame with a lower recall in detecting extra actionstable x results of precision and recall in detecting injected unseen workloads .
apache james unseen workload typeaction actioncontext actionsequence precision recall precision recall precision recall extra actions .
.
.
.
.
.
removing actions .
.
.
.
.
.
double context values .
.
.
.
reordering actions .
.
.
.
average .
.
.
.
.
.
openmrs unseen workload typeaction actioncontext actionsequence precision recall precision recall precision recall extra actions .
.
.
.
.
.
removing actions .
.
.
.
.
.
double context values .
.
.
.
reordering actions .
.
.
.
average .
.
.
.
.
.
google borg unseen workload typeaction actionsequence precision recall precision recall extra actions .
.
.
.
removing actions .
.
.
.
double context values .
.
.
.
reordering actions .
.
.
.
average .
.
.
.
sa unseen workload typeaction actioncontext actionsequence precision recall precision recall precision recall extra actions .
.
.
.
.
.
removing actions .
.
.
.
.
.
double context values .
.
.
.
reordering actions .
.
.
.
average .
.
.
.
.
.
action actioncontext actionsequence precision recall precision recall precision recall all average .
.
.
.
.
.
note v alues in bold font indicate the best performing approach for each setting.
and removing actions workloads.
we find that the apache james mail server has a small number of action types cf.
section iv while adding extra actions and removing actions may still generate a user with a high similarity to a previously recovered cluster of users.
actionsequence workloads have a similar precision but much higher recall than actioncontext workloads.
table x shows that on average actionsequence achieves a similar precision .
as actioncontext .
while the recall of actionsequence .
is much higher than that of actioncontext .
.
actionsequence considers finer grained information than actioncontext hence intuitively providing a higher ability to uncover more unseen workloads.
in particular in all the cases actionsequence has a higher or similar recall as actioncontext .
on the other hand even though in some cases when actioncontext has a higher precision the difference is rather small.
b. sensitivity analysis our workload recovery process leverages several techniques such as hierarchical clustering which may be replaced by other similar techniques.
our process also leverages threshold values.
for example the residual value for the linear regression prediction that is used to split user action sequences is set to .
in our process.
the value to rank frequent action sequences is also set to .
.
in order to better understand the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table xi comparing the results of choosing different threshold v alues and clustering algorithm for apache james .
send action changed residual .
residual .
.
.
with mean shift default p value effect size p value effect size p value effect size p value effect size p value effect size residual .
.
.
.
.
.
.
.
.
.
.
.
hierarchical clustering trivial trivial small trivial small receive action changed residual .
residual .
.
.
with mean shift default p value effect size p value effect size p value effect size p value effect size p value effect size residual .
.
.
.
.
.
.
.
.
.
.
.
hierarchical clustering trivial trivial small trivial trivial sensitivity of our workloads to these thresholds we individually increased each threshold value to .
and decreased the threshold value to .
.
we also change the clustering algorithm to mean shift .
we choose mean shift since similar to hierarchical clustering mean shift does not require us to pre specify the number of clusters.
we re generated the actionsequence workload and calculated the throughput of each action for each subject system.
we used the actionsequence workload in this analysis because actionsequence is the best performing workload shown in our evaluation results c.f.
section v and the actionsequence covers all the steps of all our three workload recovery approaches.
we compared the throughput of each action with the default threshold and the clustering algorithm using the mannwhitney u test and cliff s delta.
we observed that our approaches are insensitive to both the residual and thresholds.
in addition the effect size between the hierarchical clustering and mean shift is trivial or small.
table xi only shows the results of such a comparison for apache james which is the most peculiar workload in our subject systems.
other comparison results are in our replication package.
vii.
c hallenges and lessons learned from the industrial ev alua tion of our approaches .
in this section we discuss the learned lessons and faced challenges during the implementation and evaluation of our approaches in industry.
in particular the first author of this paper was embedded on site with the industrial team for over half a year to enable a faster feedback loop from practitioner ensure the smooth adoption of our approaches in a largescale complex industrial setting.
our documented challenges and lessons can assist researchers and practitioners who would like to integrate their research in complex industrial settings.
a. domain knowledge is crucial for the successful transfer of research to practice.
our approaches depend on the availability and quality of the important knowledge that resides in system execution logs.
due to the large scale and complexity of the logs in system a we often faced the challenge that we may not fully understand the information that is communicated in the logs making it challenging for us to determine the important contextual information to include and analyze by our approaches.
how to address at a first attempt we na vely included all log information for our analysis.
we ended up observing that such an attempt introduced noise which negatively impacted our results.
hence the first author flew down to spend six monthson site at alibaba where he held several in person scrum meetings with developers and operators of system a in order to better understand the information that is communicated in system as logs.
these meetings helped the academic team and the industrial team get a better understanding of the problem at hand as well as the strengths and limitations of the research solutions.
being on site helped the academic team create a strong relation with the industrial team as well.
such a relation enabled a much faster and more open feedback loop.
lessons learned good domain knowledge is crucial in log analysis and workload recovery.
blindly applying log analysis techniques on large scale complex logs may not achieve the expected goal.
one should work closely with practitioners to leverage their valuable knowledge about their logs.
b. team support is crucial for the successful transfer of research to practice.
customization of tooling we started our research working on open source systems.
such open source systems commonly use standard load driver tools such as jmeter.
however industrial systems are commonly tested using various in house custom tools.
such tools hinder us from demonstrating and evaluating our work.
it is often costly time consuming and sometimes impossible to design and implement a specific load replay tool for each system.
how to address working closely with the practitioners of alibaba we gained a deeper understanding of their in house load replay tools.
we observed that many of them support the direct reading of logs and the replaying of the exact workload based on such logs.
therefore we provided an option in our toolset for the direct driving of a load test using widely used tools like jmeter or the transformation of our generated load test into logs which can be fed to the customized load replay tools from alibaba.
lessons learned there exists a strong need for future research in load replay using more flexible frameworks in order to avoid practitioners having to implement customized toolsets.
addition of needed log lines when not all information is available not all the needed information was available in the logs when we started our research on alibabas system.
in particular the log data was not perfectly designed for our approach and important information was missing.
how to address working closely with the practitioners we requested the addition of new logging probes.
in order to demonstrate the values of our requests we conducted several additional analyses.
lessons learned the technical support and quick turn around from the industrial team were extremely crucial in addressing this challenge.
however even with all the logging probes in place we had to wait till the builds with such probes were deployed long enough in the field for us to have sufficient data for our approaches.
setting up a realistic industrial environment the context of the load testing environment has an important influence on the performed load tests.
for example there should be a large amount of realistic data in a database before one can authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
test a database driven application.
using open source systems it is relatively easy to create a load testing environment that is similar to widely adopted benchmarks.
however setting up such an environment in an ultra large scale industrial environment is extremely challenging.
how to address to make our automatically generated load tests run successfully we had the luxury of being strongly supported by the infrastructure team of alibaba.
in particular we were provided with a testing environment that is a replica of the field environment from which we collected the analyzed logs.
however such a solution is not optimal and may not be cost effective especially for practitioners that do not have direct access to infrastructures that are similar to their deployed systems.
lessons learned preparing the load testing environment is an important challenging and yet open problem for load testing practices and research.
technical and infrastructural support is crucial in overcoming this challenge.
c. coping with the large scale industrial data our experiments on open source systems are conducted with a limited scale of data.
when adopting our approach using the industry scale data of alibaba our approach did not scale well.
the statistical analyses and clustering techniques often suffered from poor scalability.
how to address in order to ease the adoption of our approach on industry scale data we optimized our solution by learning some threshold values by pre processing the logs and caching the thresholds across runs.
for example learning the best threshold values to categorize contextual values is time consuming.
we can save the learned thresholds and use them directly to generate the categorized contextual values in logs since such thresholds rarely change within a specific context.
lessons learned one should not assume that a successful research tool can directly scale to industrial data.
optimization for the particular industrial setting is important.
viii.
t hrea ts to v alidity external validity.
our evaluation is conducted on data from two open source and two industrial systems.
although our subject systems cover different domains and sizes our evaluation results may still not generalize to other systems.
given the automated nature of our process others can study its effectiveness on their own data sets using our replication script.
internal validity.
our approaches depend on the availability and quality of system execution logs.
if a system records limited information about user actions in the execution logs our approaches may not perform as expected.
the evaluation of approaches uses the system cpu usage that is recorded by pidstat .
the quality and the frequency of recorded cpu usage can impact the internal validity of our study.
currently our approach only categorizes numerical contextual values due to the characteristics of the logs in our subject systems.
future work can complement our approach by consider categorizing string literals.
our approach depends on various statistical analyses.
therefore for small systems with a small amountof data our approach may not perform well due to the nature of statistical analyses.
construct validity.
there exists other aspect of system bahaviour and performance.
we focus on the throughput and cpu usage due to special need of sa from our industrial collaborator.
future study may investigate the impact on other system aspects to complement our findings.
due to inaccessibility of the subject system the workloads on google borg is based on simulated execution logs instead of actually running the load tests on google borg.
therefore the evaluation results may be different if we were able to run the load tests on the actual google borg cluster.
in the evaluation of our approaches to detect users with unseen workloads we only injected four types of unseen workloads.
similar evaluation approaches based on mutation techniques have been often used in prior research .
however these unseen workloads may not be the same as in real life.
in addition there may exist other ways to inject unseen workload to complement our results.
ix.
c onclusions workload recovery from end users is an essential task for the load testing of large scale systems.
in this paper we conduct a study on recovering workloads at different levels of granularity of user behaviours to automatically generate load tests.
we design three approaches that are based on user actions user actions with contextual information and user action sequences with contextual information to generate load tests on two open source systems and two industrial systems.
we find that our richest approach which uses user action sequences with contextual information outperforms the other two approaches.
in most cases the throughput and cpu usage from the load tests that are generated from the user action sequence based workload outperform the other two and are statistically insignificant relative to the original workload or with small or trivial effect sizes.
such a field representative workload is generated only using a small number of clusters of users.
in addition we find that the recovered workloads from our approaches can also be used to detect injected users with unseen workloads with a high precision and recall.
our paper has the following contributions to the best of our knowledge our approach is the first large scale study on the use of different granularity of recovered user details for load testing.
to the best of our knowledge our approaches are the first ones in the field to leverage user contextual information and frequent action sequences in workload recovery.
our approaches have been adopted in practice to assist in the testing and operation of an ultra large scale industrial software system.
acknowledgement we would like to thank alibaba for providing access to their system used in our study.
the findings and opinions expressed in this paper are those of the authors and do not necessarily represent or reflect those of alibaba and or its subsidiaries and affiliates.
moreover our results do not reflect the quality of alibaba s products.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.