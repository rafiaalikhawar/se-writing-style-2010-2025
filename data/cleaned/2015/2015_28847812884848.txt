on the naturalness of buggy code baishakhi ray vincent hellendoorn saheel godhane zhaopeng tu alberto bacchelli premkumar devanbu university of virginia university of california davis huawei technologies co. ltd. rayb virginia.edu vjhellendoorn srgodhane ptdevanbu ucdavis.edu tuzhaopeng gmail.com delft university of technology a.bacchelli tudelft.nl abstract real software the kind working programmers produce by the kloc to solve real world problems tends to be natural like speech ornatural language it tends to be highly repetitive and predictable.
researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines porting tools coding standards checkers and idiom miners.
this suggests that code that appears improbable or surprising to a goodstatistical language model is unnatural in some sense and thus possibly suspicious.
in this paper we investigate this hypothesis.
we consider a large corpus of bug fix commits ca.
from different java projects and focus on its language statistics evaluat ing the naturalness of buggy code and the corresponding fixes.
we find that code with bugs tends to be more entropic i.e.
unnatural becoming less so as bugs are fixed.
ordering files for inspection by their average entropy yields cost effectiveness scores comparable to popular defect prediction methods.
at a finer granularity focusing on highly entropic lines is similar in cost effectiveness to some well known static bug finders pmd findbugs and ordering warnings from these bug finders using an entropy measureimproves the cost effectiveness of inspecting code implicated inwarnings.
this suggests that entropy may be a valid simple way to complement the effectiveness of pmd or findbugs and that search based bug fixing methods may benefit from using entropyboth for fault localization and searching for fixes.
.
introduction our work begins with the observation by hindle et al that natural code in repositories is highly repetitive and that this rep etition can be usefully captured by language models originally developed in the field of statistical natural language processing nlp .following this work language models have been used to good effect in code suggestion cross language porting coding standards idiom mining and code deobfuscation .
since language models are useful in these tasks baishakhi ray and vincent hellendoorn are both first authors and contributed equally to the work.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c copyright held by the owner author s .
publication rights licensed to acm.
isbn .
.
.
.
are capturing some property of how code is supposed to be.
this raises an interesting question what does it mean when a code fragment is considered improbable by these models?
language models assign higher naturalness to code tokens syntactic forms etc.
frequently encountered during training and lower naturalness to code rarely or never seen.
in fact prior work showed that syntactically incorrect code is flagged as improbableby language models.
however by restricting ourselves to code thatoccurs in repositories we still encounter unnatural yet syntactically correct code why?
we hypothesize that unnatural code is more likely to be wrong thus language models actually help zeroin on potentially defective code.
this notion appears plausible highly experienced programmers can often intuitively zero in on funny looking code when trying to diagnose a failure.
if statistical language models could capture this capability then they could be a useful adjunct in a variety ofsettings they could improve defect prediction help provide an im proved priority ordering for static analysis warnings improve the performance of fault localization algorithms or even recommend more natural code to replace buggy code.
to investigate this phenomenon we consider a large corpus of bug fix commits from different projects and focus on itslanguage statistics evaluating the naturalness of defective code and whether fixes increase naturalness.
language models can rate probabilities of linguistic events at any granularity even at the level of characters.
we focus on line level defect analysis giving far finergranularity of prediction than typical statistical defect prediction methods which most often operate at the granularity of files or modules.
in fact this approach is more commensurate with staticanalysis or static bug finding tools which also indicate potentialbugs at line level.
for this reason we also investigate our lan guage model approach in contrast and in conjunction with two wellknown static bug finders namely pmd and findbugs .
overall our results corroborate our initial hypothesis that code with bugs tends to be more unnatural.
in particular the main findings of this paper are .
buggy code is rated as significantly more unnatural improbable by language models.
.
this unnaturalness drops significantly when buggy code is replaced by fix code.
.
furthermore we find that above effects are substantially strongerwhen the buggy code fragment is shorter fewer lines and the bug is short lived viz.more quickly fixed.
.
using cost sensitive measures inspecting unnatural code indicated by language models works quite well performanceis comparable to that of static bug finders findbugs and pmd.
ieee acm 38th ieee international conference on software engineering .
ordering warnings produced by the findbugs and pmd tools using the unnaturalness of associated code significantly improves the performance of these tools.
our experiments are mostly done with java projects but we have strong empirical evidence indicating that the first two find ings above generalize to c as well we hope to confirm the rest infuture work.
.
background our main goal is evaluating the degree to which defective code appears unnatural to language models and the extent to whichthis can enable programmers to zero in on bugs during inspections.furthermore if language models can help pinpoint buggy lines wewant to identify how their performance and applicability relate to commonly used fault detection methods.
to this end we explore the application of language models first to file level defect prediction comparing with statistical defect prediction methods andthen to line level defect prediction comparing their performancewith popular static bug finders sbf .
in this section we present the relevant technical background and the main research questions.
.
language modeling language models assign a probability to every sequence of words.
given a code token sequence s t1t2...t n a language model estimates the probability of this sequence occurring as a product ofa series of conditional probabilities for each token s occurrence p s p t ny i 2p ti t1 ... t i p ti t1 ... t i denotes the chance that the token tifollows the previous tokens the prefix h t1 ... t i .
the probabilities are impractical to estimate due to the huge number of possibleprefixes.
a common fix is the ngram language model using the markov assumption to condition just on the preceding n 1tokens.
p ngram ti h p ti ti n ... t i this we estimate from the training corpus as the fraction of timesthatt ifollows the prefix ti n ... t i .
this is reversible we can also compute each token given its suffix the subsequent to kens .
we compute entropies based on both prefix and suffix token sequences to better identify the buggy lines section .
.
thengram language models can effectively capture the regularities in source code and have been applied to code suggestion tasks .
tu et al.
improved such language models by considering that software tends to be repetitive in a local context.they introduced a cache language model gram that deploys anadditional cache list of ngrams extracted from the local context to capture the local regularities.
the ngrams extracted from each file under test form its local context in the cache model.
we use the state of the art gram to judge the improbability measured as cross entropy of lines of code.
.
line level defect detection sbf static bug finders sbf use syntactic and semantic properties of source code to locate common errors such as null pointer dereferencing and buffer overflows.
they rely on methods rang ing from informal heuristic pattern matching to formal algorithmswith proven properties they typically report warnings at build time.
most of the pattern matching tools require users to specify the buggy templates.
others can automaticallyinfer rules by mining existing software they raise warnings if violations of the rules occur.
most e.g.
pmd and findbugs areunsound yet fast and widely used compared to more formal approaches.
generally sbf produce false positives and false negatives which reduce their cost effectiveness .
bothsbf and our model fairly imperfectly indicate potential defect locations our goal is to compare these approaches and seewhether they can be combined.
.
evaluating defect predictions we take the simplified view that sbf and gram are comparable in that they both select suspicious lines of code for manual re view.
we therefore refer to language model based bug prediction as nbf naturalness bugfinder .
with either sbf ornbf human code review effort spent on lines identified as bug prone will hopefully find some defects.
comparing the two approaches requires a performance measure.
we adopt a cost based measure that has become standard aucec area under the cost effectiveness curve .
like roc aucec is a non parametric measure thatdoes not depend on the defects distribution.
aucec assumes thatcost is inspection effort and payoff is the number of bugs found.
given a model sbf ornbf that predicts buggy lines we rank all the lines in decreasing order of their defect proneness score.thus the best possible model would place the buggiest lines at the top of the list.
this approach helps reviewers to inspect a smaller portion of code i.e.
cost while finding a disproportionately larger fraction of defects i.e.
payoff .
we normalize both cost and payoff to and visualize the improvement that a predic tion model provides when compared against a random guess using a lift chart .
in this chart cost on the x axis refers to the per centage of the code base inspected at prediction time and payoff on the y axis indicates the portion of the known bugs discovered by data gathering already in the code that are covered by warnedlines.
aucec is the area under this curve.
under uniform bug distribution across sloc inspecting x of lines of code at random will in expectation also yield x of the bugs i.e.
random selection produces a diagonal line on the lift chart.
the corresponding aucec when inspecting of lines at random is .
.
inspecting of sloc in a project is probably unrealistic.
prior research has assumed that sometimes of the code could realistically be inspected under deadline .
additionally rahman et al.
compare sbf withdp a file level statistical defect predictor by allowing the number warnings from sbf to set the inspection budget denoted aucecl .
they assign the dp the same budget and compare the resulting aucec scores.
weextend this approach to our comparison of sbf andnbf .t o understand how nbf s payoff varies with cost we first measure its performance for both and inspection budget.
we thencompare aucecs of nbf andsbf at both the budget and under aucecl budget.
finally we investigate defect prediction performance under severalcredit criteria .
a prediction model is awarded credit ranging from to for each ipso facto eventually buggy line flagged as suspicious.
previous work by rahman et al.
has compared sbf anddpmodels using two types of credit full or optimistic and partial or scaled credit which we adapt to line level defectprediction.
the former metric awards a model one credit point foreach bug iff at least one line of the bug was marked buggy by the model.
thus it assumes that a programmer will spot a bug as soon as one of its lines is identified as such.
partial credit is more con servative for each bug the credit is awarded in proportion to the fraction of the bug s defective lines that the model marked.
hence 1calculated as .
.
.
.
this could be normalized differently but we consistently use this measurement so our comparisons work.
429table summary data per project used in phase i ecosystem project description study period files ncsl unique bug fixes githubatmosphere web socket framework oct to oct elasticsearch distributed search engine jul to jul facebook and android sdk for dec to dec roid sdk facebook facebook application netty network application framework aug to aug presto sql query engine jul to jul apachederby relational database jul to jul lucene text search engine library jan to jan openjpa java persistence api jul to jul qpid messaging system apr to apr wicket web framework jul to jul overall sep to jul partial credit assumes that the probability of a developer finding a bug is proportional to the portion of the bug that is marked by the model.
it should be noted the aucec is non parametric underpartial credit but not under full credit as it depends on the de fect distribution however we get the same overall result under both regimes.
.
research questions our central question is whether unnaturalness measured as entropy or improbability indicates poor code quality.
the abun dant history of changes including bug fixes in oss projects allows the use of standard methods to find code that was implicatedin bug fixes buggy code .
rq1.
are buggy lines less natural than non buggy lines?
project histories contain many bug fixes where buggy code is modified to correct defects.
do language models rate bug fix codeas more natural than the buggy code that was replaced i.e.
was bugfix code rated more probable than the buggy code ?
such a finding would also have implications for automatic search based bug repair if fixes tend to be more probable then a good language model might provide an effective organizing principle for the search or ifthe model is generative even generate possible candidate repairs.
rq2.
are buggy lines less natural than bug fix lines?
even if defective lines are indeed more often rated improbable by language models this may be an unreliable indicator there maymany be false positives correct lines marked improbable and falsenegatives buggy lines indicated as natural .
therefore we investigate whether naturalness i.e.
entropy can provide a good ordering principle for directing inspection.
rq3.
is naturalness a good way to direct inspection effort?
one can view ordering lines of code for inspection by naturalness as a sort of defect prediction technique we are inspecting lines in a certain order because prior experience suggests that cer tain code is very improbable and thus possibly defective.
tradi tional defect prediction techniques typically rely on historical process data e.g.
number of authors previous changes or bugs however defectiveness is predicted at the granularity of files or methods .
thus we may reasonably compare naturalness as an orderingprinciple with sbf which provide warnings at the line level.
rq4.
how do sbf andnbf compare in terms of ability to direct inspection effort?finally if sbf provides a warning on a line andit appears unnatural to a language model we may expect that this line is evenmore likely a mistake.
we therefore investigate whether naturalnessis a good ordering for warnings provided by static bug finders.
rq5.
is naturalness a useful way to focus the inspection effort on warnings produced by sbf ?
.
methodology we now describe the projects we studied and how we gathered and analyzed the data.
.
study subject we studied oss java projects as shown in table among these are five projects from github while the others are from theapache software foundation.
we chose the projects from differ ent domains to measure nbf s performance in various types of systems.
all projects are under active development.
we analyzed nbf s performance in two settings phase i considers nbf s ability to find bugs based on continuous usage during active development see section .
.
we chose to analyze eachproject for the period of one year which contained the most bugfixes in that project s history here we considered both developmenttime and post release bugs.
then for the chosen one year dura tion we extracted snapshots at month intervals.
a snapshot cap tures the state of the project at a given point in time.
thus for each project we studied snapshots in total analyzing snapshots across projects including distinct file versions and .
million total non commented source code lines ncsl .overall we studied distinct bug fix commits comprising of2.
million total buggy lines.
subsequently we confirmed our results across the entire history of each studied project using snapshots at month intervals snapshots commits bug fixes .
due to page limitations we are presenting results fromour study of month snapshots only.
next for phase ii see section .
we focused only on postrelease bugs to evaluate nbf s performance as a release time bug prediction tool.
we used the data set from rahman et al.
in which snapshots of the five apache projects were taken at setable summary data per project used in phase ii.
the dataset is taken from rahman et al.
projectncsl warnings k findbug pmd issues derby 192k lucene 31k openjpa 171k qpid 80k wicket 30k 430lected project releases.
the project snapshot sizes vary between and 630k ncsl.
the bugs were extracted from apache s jiraissue tracking system the bug count per release across all the projects varies from see table .
we further used warnings produced by two static bug finding tools f indbugs and pmd as collected by rahman et al.. pmd operates on source code and produces line level warnings f indbugs operates on java bytecode and reports warnings at line method and class level.
.
data collection phase i here we describe how we identified the buggy lines in a snapshotcorresponding to the bugs that developers fixed in an ongoing development process.
estimating bug fixing commits.
development time bug fixes are often not recorded in an issue database.
thus to estimate bug fixing activities during an ongoing development process we analyzed commit messages associated with each commit for the entire project evolution and looked for error related keywords.
first we converted each commit message to a bag of words and thenstemmed the bag of words using standard natural language pro cessing nlp techniques.
then similar to mockus et al.
we marked a commit as a bug fix if the corresponding stemmed bagof words contains at least one of the error related keywords error bug fix issue mistake incorrect fault defect flaw and type .
this method was adapted from our previous work .
for the apache projects as well as atmosphere and netty we further improved the classification with information available fromthe jira issue database.
to evaluate the accuracy of the above classification we manually verified the result for commits from each project chosen randomly .
here we only evaluated whether the author of a presumed bug fix commit really marked their commit as a bug fix.
out of these commits were classified correctly commits were described as a potential issue thus may have developed into a bug later and commits were classified incorrectly false negatives and false positives.
thus our approach achieves accuracy conf.int.
.
to .
.selecting snapshots.
to evaluate nbf in continuous active development ideally we need to study all the commits made to a project for its full history.
but using git blame to get line level bug data at this scale is not feasible.
thus we chose year evaluation periods for each project.
since our focus is studying bugs we con sidered the one year period of each project that contained the most bug fixes.
within these periods we looked at monthly snapshots thereby simulating near continuous usage of our tool.
for instance snapshots were taken at month intervals between and2007 for project derby see table .
identifying buggy lines in a snapshot.
this part consists of three steps identifying lines related to a bug identifying commitsthat introduced the bug and mapping the buggy lines to thesnapshots of interest.
in step we assumed that all the lines deleted or changed in a bug fix commit were buggy lines.
to find these lines we looked at the versions of a file before and after a bug fix.
we used git diff to identify the deleted changed lines in the old version and marked them as buggy the added changed lines in the new version are marked as fixed .
next in step we used git blame to locate the commits that had introduced these buggy lines in the system.
the first two steps are analogous to the szz algorithm .
once we know where the buggy lines originated we used git blame with reverse option to locate these lines in the snapshots of interest.
this step maps the buggy lines to specific snap project time linec5 c4 buggy linesfixed lines s3 s2c3 c2git blame s1c1 bug fix bug bugmap onto s2 map onto s1 old new figure collecting bug data vertical dashed lines are snapshots s1... s3 and triangles are commits c1... c5 that occurred between these snapshots.
for every bug fix commit e.g.
c4 we first git blame the buggy lines blue arrowed lines and then map them to the corresponding snapshots red arrowed lines .
shots.
figure explains the procedure where commit c4 is a bug fix commit.
the corresponding buggy lines marked red in the old ver sion are found to originate from two earlier commits c1 and c2.
wethen map the buggy lines from c1 to both s1 and s2 whereas the buggy lines from c2 are mapped only to s2.
note that we considered all the bugs that appeared at any time in the entire evolution and map them back to the snapshots of interest.
however we lose the buggy lines that were fixed before or arose after our study period as we cannot map these to any snapshots of interest.
we also miss some transient bugs that appeared and werefixed within a snapshot interval thus lasting less than a month .
atthe end of this step we know exactly which lines in each of oursnapshots are buggy and were fixed in some future commit and which ones are benign modulo time window censoring effects.
phase ii in phase ii we studied post release bugs for the apache projects using rahman et al.
s dataset.
rahman et al.
selected a number of release versions of each apache project and for each release identified post release bugfix commits from the jira issue trackingsystem.
they then identified buggy and non buggy lines for each release version similar to steps and of the previous section.
.
entropy measurement choice of language model.
we measured entropy using tu et al.
s cache based language model gram tool as described in section .
.
we computed the entropy over each lexical token in allthe java files of all the snapshots.
for a given file in a snapshot the tool estimates a language model on all the other files of the same snapshot.
it then builds a cache by running the language model on the given file computing the entropy of each token based on bothprolog preceding tokens and epilog succeeding tokens .
finally based on the training set and locally built cache the tool computes the entropy of each token of the file the line and file entropies arecomputed by averaging over all the tokens belong to a line and alllines corresponding to a file respectively.
to generate entropies of the fixed lines we leveraged the data set gathered for the entire evolution period with months interval as mentioned in section .
.
this was necessary because a bug may get fixed after our studied period.
for each bug fix commit wetrained the dataset on its immediate preceding snapshot and testedit on the new file version corresponding to the bug fix.
determining parameters for cache language model.
several factors of locality can affect the performance of the cache language model cache context cache scope cache size and cache order.
in this work we built the cache on the entire file under investigation.
431in this light we only needed to tune cache order i.e.
maximum andminimum order of ngrams stored in the cache .
in general longer ngrams are more reliable but quite rare thus we backed off to shorter matching prefixes or suffixes when needed.
we followed tu et al.
to set the maximum order of cache ngrams to10.
to determine the minimum back off order we performed experiments on the elasticsearch and netty projects looking for opti mal performance measured in terms of entropy difference between buggy and non buggy lines.
the maximum difference was found at a minimum backoff order of with no change in the backoffweight.
thus we set the minimum backoff order to and the backoff weight to .
.
adjusting entropy scores.
language models could work for defect prediction at line granularity if bug prone lines are more entropic.
for instance a non buggy but high entropy line would be a false positive and worsen the language model s performance at theprediction task.
for example lines with previously unseen identifiers such as package class and method declarations have substantially higher entropy scores on average.
vice versa for loopstatements and catch clauses being often repetitive have muchlower entropy scores.
such inter type entropy differences do not necessarily reflect their true bug proneness.
in fact for statements though less entropic are often more bug prone than the more en tropic import declarations.
this observation led us to using abstract syntax based line types and computing a syntax sensitive entropy score.
first we used eclipse s jdt 2to parse an abstract syntax tree ast of all files under consideration.
any line in a java file is always contained by either a single ast node e.g.
compilation unit root node or several ast nodes in hierarchical order e.g.
a nested line with if statement method declaration and class declaration.
for each line its syntax type is the grammatic entity associated with the lowest ast node encompassing the full line.
examples include statements e.g.
if for while return declarations e.g.
variable structure method import or other ast nodes that tend to span one line such as switch cases and annotations.
we then computed how much a line s entropy deviated from the mean entropy of its line type using normalized z score zline type entropy line type sd type where typedenotes mean gram entropy of all the lines of a given type and sd typedenotes standard deviation.
this gave us a syntaxsensitive entropy model gram type.
the above normalization essentially uses the extent to which a line is unnatural w.r.t.
other lines of the same type.
in addition based on the fact that all line types are not equally buggy we computed relative bug proneness of a type based on the fraction of bugs and total lines loc it had in all previous snapshots.
here we used the previous snapshots as training set and computed bug weight of a line type as wtype bug type loc typep t2typesbug t loc t where the bugs and locs per type were counted over all previous snapshots.
we then scaledthe z score of each line by its weight wto achieve our final model which we name gram wtype.
phase i and phase ii data set and the entropy generation tool are available at .
evaluation this section discusses the answers to research questions introduced in section .
.
the first two rqs are primarily based on the phase i data set.
rq3 uses phase ii data to evaluate nbf s capability as a file level defect predictor.
line level defect prediction is evaluated using both data sets rq3 rq4 and rq5 .
we begin with the question that is at the core of this paper 2java development tools entropy difference and effect size between buggy vs. nonbuggy and buggy vs. fixed lines buggy vs. non buggy rq1 buggy vs. fixed rq2 unique bug fix entropy diff.
cohen s d entropy diff.
cohen s d bugs threshold bug non bug effect bug fix effect detected .
to .
.
.
to .
.
.
.
to .
.
.
to .
.
.
.
to .
.
.
to .
.
.
.
to .
.
.
to .
.
.
.
to .
.
.
to .
.
.
.
to .
.
.
to .
.
.
.
to .
.
.
to .
.
.
.
to .
.
.
to .
.
.
overall .
to .
.
.
to .
.
.
buggy lines have higher entropy than non buggy lines.
buggy lines also have higher entropy than fixed lines.
entropy difference decreases as bug fix threshold increases.
entropy differences are measured with t test for confidence interval and shows statistical significance pvalue .
.
cohen s d effect size .
small .
medium and .
large .
rq1.
are buggy lines less natural than non buggy lines?
to evaluate this question we compare entropies of buggy and non buggy lines for all the studied projects.
a wilcoxon nonparametric test confirms that buggy lines are indeed more entropic than non buggy lines with statistical significance p .
.
average entropy of buggy lines is .21while that of non buggy lines is .
.
however cohen s d effect size between the two is .
see last row in table which is considered small.
one explanation for the small effect size across bugs of all sizes is an impact of tangled bug fixes lines that are changed in a bugfix commit but are not directly related to the bug.
herzig et al.
showed that around of all source files are incorrectly associ ated with bugs due to such tangled changes.
the impact of tangled changes on bug entropy is more visible for larger bug fix commits.
some of the lines changed in a larger bug fix may not be directlyassociated with the erroneous lines and thus may not be unnatu ral .
in contrast for smaller bug fix commits say for or lines of fix the fixed lines i.e.
the lines deleted from the older version are most likely to be buggy.
non buggy buggy fixedline entropy a entropy difference between non buggy buggy and fixedlines at bug fix threshold .
low durationmedium durationhigh durationline entropy b entropy difference between buggy lines of different bug du ration.
figure to understand the effect of tangled changes on the naturalness of buggy code we compute entropy difference between buggy and non buggy lines at various bug fix size thresholds.
we define bugfix threshold as the number of lines in a file that are deleted from the older version of a bug fix commit.
table shows the result.
both entropy difference and effect size decrease as the bug fix threshold increases.
for example for a threshold size of entropies of buggy lines are on average .95to2.00bits higher than non buggy lines confidence interval .
cohen s d effect size between the two lies between medium and large .
.
.
of the bug fixes lie below this threshold.
for a threshold size of buggy lines have .15to1.17bits higher entropy than their non buggy counterpart.
in this case we see a small to medium effect effect size .
.
of all changes both bug fixes and feature implementations in our data set contain no more than lines of deletion 432this is also shown in figure a .
at bug fix threshold we see only .55to0.57entropy difference with small effect .
.
these results indicate that the lines that are indirectly associated with real buggy lines in tangled bug fix commits may have lower entropy and thus would diminish the overall entropy of buggy lines.
we further observe that bugs that stay longer in a repository tend to have lower entropy than the short lived bugs.
bug duration of a buggy line is measured as the number of months until a buggy line is fixed starting from the day of its introduction bug duration bug fix date minus bug introduction date .
the following table shows the summary of bug duration in our data set in months min.
1st qu.
median mean 3rd qu.
max.
.
.
.
.
.
.
based on bug duration we divide all the buggy lines into three groups low medium and high.
the bugs in lowgroup survived for less than months 1stquartile bugs in medium group survived from to months 1stquartile to median and the remaining bugs are in the high group.
figure b shows their entropy variation.
the low group has significantly higher entropy than themedium and high group with cohen s d effect size of .
and .
respectively medium to large effect size .
the difference is also confirmed with wilcoxon non parametric test with statistical significance.
the medium group is also slightly more entropic thanthe high group with statistical significance although the effect size is very small .
.
these results indicate that the bugs that are fixed more quickly are more unnatural than the longer lived bugs.we hope to explore the reasons in future work perhaps the highlyentropic bugs are easier to locate diagnose and fix and thus getspeedily resolved or perhaps more intriguingly highly entropic code is more strongly associated with failures that are more likely to be quickly encountered by users.
in summary we have the overall result result buggy lines on average have higher entropies i.e.are less natural than non buggy lines.
a natural question is whether the entropy of the lines in a bug drops once the bug is fixed.
this leads us to the following question rq2.
are buggy lines less natural than bug fix lines?
to answer rq2 we collected bug fix commit patches of all the bugs that exist in any snapshot under study.
in a bug fix commit the lines deleted from the original version are considered buggy lines and lines added in the fixed versions are considered fixed lines.
we collected all such buggy and fixed lines for all the projects asdescribed in section .
.
establishing a one to one correspondencebetween a buggy and fixed line is hard because buggy lines areoften fixed by a different number of new lines.
hence we compare the mean entropies between buggy and fixed lines across all the patches.
wilcoxon non parametric test confirms that entropy of buggy lines in general drops after the bug fixes with statisticalsignificance see figure a .
similar to rq1 tangled changes may also impact the entropies of bugs and their fixes.
to measure the impact we further com pare the entropy differences between buggy and fixed lines at vari ous bug fix thresholds.
table shows the result.
both the entropydifference and effect size decreases as bug fix threshold increases.
for example at a bug fix threshold of one line average entropy drops upon fixing between .58to1.67bits confidence interval and with statistical significance .
the cohen d s effect size is medium to large .
.
however with threshold size at meantable examples of bug fix commits that nbf detected successfully.
these bugs evinced a large entropy drop after the fix.
bugs with only one defective line are shown for simplicity purpose.
the errors are marked in red and the fixes are highlighted in green.
example wrong initialization value facebook android sdk file session.java entropy dropped after bugfix .
if newstate.isclosed before entropy .
this.tokeninfo null after entropy .
this.tokeninfo accesstoken.createemptytoken collections.
string emptylist ... example wrong method call netty file threadperchanneleventloopgroup.java entropy dropped after bugfix .
if isterminated before entropy .
terminationfuture.setsuccess null after entropy .
terminationfuture.trysuccess null example unhandled exception lucene file fsdirectory.java entropy dropped after bugfix .
if !directory.exists before entropy .
directory.mkdir after entropy .
if !directory.mkdir throw new ioexception cannot create directory directory ... entropy difference between the two are slightly more than half a bit with a small effect size of .
.
such behavior suggests that tangled changes may be diluting the entropy of buggy code and theirfixes.
bug duration also impacts the drop of entropy after bug fix.
for bugs with low duration the entropy drop is significant .68to2.
bits on average effect size .
.
for medium duration bugs entropy drops from .09to0.18bits effect size .
while entropy does not necessarily drop for high duration bugs.
table shows three examples of code where entropy of buggy lines dropped significantly after bug fixes.
in the first example a bug was introduced in facebook android sdk code due to a wrong initialization value tokeninfo was incorrectly reset to null.
this specific initialization rarely occurred elsewhere so the buggy line had a rather high entropy of .
.
once the bug was fixed the fixed line followed a repetitive pattern indeed with two prior instances in the same file .
hence entropy of thefixed line dropped to .
an overall .12bit reduction.
the second example shows an example of incorrect method call in thenetty project.
instead of calling the method trysuccess used three times earlier in the same file the code incorrectly called themethod setsuccess which was never called in a similar context.
after the fix entropy drops by .63bits.
finally example shows an instance of missing conditional check in lucene.
the developer should check whether directory creation is successful bychecking return value of directory.mkdir call following the usual code pattern.
the absence of this check raised the entropy of the buggy line to .
.
the entropy value drops to .34after the fix.
in certain cases these observations do not hold.
for instance in example of table entropy increased after the bug fix by .
433table examples of bug fix commits where nbf did not perform well.
in example nbf could not detect the bug successfully marked inred and after bugfix the entropy has increased.
in example nbf incorrectly detected the line as buggy due to its high entropy value.
example wrong argument nbf could not detect netty file httpmessagedecoder.java entropy increased after bugfix .
if maxheadersize throw new illegalargumentexception before entropy .
maxheadersize must be a positive integer maxchunksize after entropy .
maxheadersize must be a positive integer maxheadersize example nbf detected incorrectly facebook android sdk multiple snapshots file request.java entropy .
logger logger new logger loggingbehaviors.
requests request ... bits.
in this case developer copied maxchunksize from a different context but forgot to update the variable name.
this is a classic example of copy paste error .
since the statement related tomaxchunksize was already present in the existing corpus the line was not surprising.
hence its entropy was low although it was a bug.
when the new corrected statement with maxheadersize was introduced it increased the entropy.
similarly in example the statement related to logger was newly introduced in the corpus.
hence its entropy was higher despite not being a bug.
however for all bug fix thresholds wilcoxon non parametric test confirms with statistical significance that the entropy of buggylines is higher than the entropy of fixed lines.
overall we see .
to0.74bit entropy drops after bug fixes with a small effect size of .
.
thus in summary result entropy of the buggy lines drops after bug fixes with statistical significance.
having established that buggy lines are significantly less natural than non buggy lines we investigate whether entropy can be usedto direct inspection effort towards buggy code.
we start with thefollowing research question rq3.
is naturalness a good way to direct inspection effort?
baseline detecting buggy files.
we first consider file level defect prediction dp the de facto standard in the literature.
specifically we evaluate whether ordering files by entropy will better guide us to identifying buggy files than traditional logistic regression andrandom forest based dp.
dpis typically used at release time to predict post release bugs so for this comparison we use the post release bug data collected in phase ii.
dpis implemented using two classifiers logistic regression lr and random forest rf where the response is a binary variable indicating whether a fileis buggy or not.
the predictor variables are the process metrics from such as developers file commit code churn andprevious bug history prior research shows that process metrics arebetter predictors of file level defects .
for each project we trainour model on one release and evaluate on the next release a defectproneness score is assigned to every file under test.
we repeat this procedure for all releases for all the projects under study.
a file s entropy is measured as the average entropy of all the lines in that file.
we rank each file in each release based on entropy andaucec 20 .
.
.
.
percentage inspected linesaucec normalized method avg file entropylogistic regressionrandomrandom forest figure performance evaluation of nbf w.r.t.dpfor identifying buggy files logistic regression based prediction score.
figure shows the normalized aucec performance of all the classifiers similar to .here the y axis shows the aucec scores as a fraction of the perfect score files ranked using an oracle for each model.
at the higher inspection budget of sloc the logistic regression and random forest dpmodels perform .
and .
better than the entropy based model respectively.
however at the stricter inspection budget of of sloc the entropy based predictor performs better than lr and only .
worse than rf all are measured w.r.t.
entropy based predictor.
detecting buggy lines.
having shown that entropy can help detect bug prone files we now focus on a finer granularity can the entropy of a line of code be used to direct inspection effort towards buggy lines?
specifically will ordering lines by entropy will guide inspection effort better than ordering lines at random?
in all our experiments the random baseline choosess lines at random fromnon commented source lines ncsl picking just as many as nbf andsbf in rqs .
for the reasons outlined in section .
we evaluate the performance of entropy ordering with the aucec scores at and of inspected lines aucec in short according to two types of credit partial and full in decreasing order of strictness .
since this is a line level experiment comparing aucec values here with file level optimum as we did earlier in figure risks confusion arising from ecological infer ence so we just present the raw aucec scores without normalization.
we further calculate aucec for different bug fix thresholds as entropy is better at predicting smaller bug fixes.
when measuring aucec at a threshold of say nlines we ignore bug fixes spanning nlines.
performance is evaluated in terms of percentage gain of aucec over random aucec gain p project aucec random aucec p projectrandom aucec figure a shows aucec 20scores for partial credit averaged over all projects for bug fix threshold .
under partial credit the default gram model without the syntax weighting described in .
performs better than random particularly at of inspected lines.
at of inspected line gram performs .
better than random.
figure b focuses on the performance on studied projects up to of the inspected lines.
at this level gram s performance varies.
for projects facebook netty and qpid gram performs significantly better than random but in other projects gram either performs similar or worse than random.
on closer examination we found that some program constructs are intrinsically more entropic than others.
for example method declarations are often more entropic because they are less frequent.
this observation led us to consider syntactic line type in bug prediction as discussed in section .
.
scaling the entropy scores by line type improves aucec 5performance in all but facebook .
.
.
.
percentage inspected linesaucec 20method gram gram type gram wtype random a overall aucec upto inspecting lines for all the projectsatmosphere derby elasticsearch facebook lucene netty openjpa presto qpid wicket0.
.
.
.
.
.
.
.
percentage inspected linesaucec 5method gram gram type gram wtype random b closer look at low order aucec upto inspecting lines for individual project figure performance evaluation of nbf with partial credit.
andatmosphere and significantly improves performance in all cases where gram performed no better than random.
including the bugginess history of line types gram wtype furthermore out performs random and gram in all but lucene andatmosphere and achieves an overall aucec 5scores .
higher than random at bug fix threshold .
these results are similar under full credit see table .
since gram wtype is the best performing naturalness approach so far we hereafter refer to it as nbf .
table performance evaluation of nbf with random for different bug fix threshold full credit partial credit bugfix random gain random gain threshold aucec aucec aucec aucec aucec 5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
aucec 20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
table further shows that nbf performance worsens with larger bug fix thresholds for both aucec 5and aucec .
for example for aucec with bug fix threshold we see .
and .
performance gain over random aucec for full and partial credit respectively.
these gains drop to .
and .
at a threshold of .
notice that in table under partial credit the random selection approach yields constant aucec 5and aucec 20scores independent of the bug fix threshold.
partial credit scoring assigns credit toeach line based on the size of the bug fix that it is part of if any thus selecting of lines at random should in expectation yield of the overall credit that is available see also section .
.
full credit on the other hand assigns the credit for detecting a bug as soon as a single line of the bug is found.
therefore the aucec scores of a random selection method under full credit will dependon the underlying distribution of bugs large bugs are detected witha high likelihood even when inspecting only a few lines at random whereas small bugs are unlikely to be detected when inspecting of lines without a good selection function.
this is reflected in table as the bug fix threshold increases the random aucec scores increase as well.
the nbf approach on the other hand ex cels at detecting small bugs under both full and partial credit which we also found to be the most entropic see figure .
thus although its performance increases slightly with an increasing bugfixthreshold its gain over random decreases.
overall we summarizethat result entropy can be used to guide bug finding efforts at both the file level and the line level.
rq4.
how do sbf andnbf compare in terms of ability to direct inspection effort?
to compare nbf withsbf we use gram wtype model on phase ii data set.
to investigate the impact of tangled changes we choose the overall data set and a bug fix threshold of roughly corresponds to the fourth quartile of bug fix sizes on this dataset .
further we select pmd and f indbugs from a pool of available sbf tools because they are popular and have been studied in previous research .
as discussed in section .
rahman et al.
developed a measure named aucecl to compare sbf anddpmethods on an equal footing .
in this method the sbf under investigation sets the line budget based on the number of warnings it returns and the dp method may choose a roughly equal number of lines.
the models performance can then be compared by computing the aucec scores both approaches achieve on the same budget.
we follow thisapproach to compare sbf withnbf .
furthermore we also compare the aucec 5scores of the algorithms.
for the gram wtype model this is analogous to the re sults in rq3.
to acquire aucec 5scores for the sbf we simulate them as follows first assign each line the value zero if it was notmarked by the sbf and the value of the sbf priority otherwise for f indbugs for pmd then add a small random amount tie breaker from u to all line values and order the lines by descending value.
this last step simulates the developerrandomly choosing to investigate the lines returned by sbf first from those marked by the sbf in descending native sbf toolbased priority and within each priority level at random.
we repeat the simulation multiple times and average the performance.
figure a and b show the aucec 5and aucecl scores for pmd using partial credit and at bug fix threshold .
the results for findbugs were comparable as were the results using full credit.
as can be seen performance varied substantially between projects and between releases of the same project.
across all releases andunder both aucec 5and aucecl scoring all models performed significantly better than random paired t test p with 435derby lucene openjpa qpid wicket .
.
.
.
.
releaseaucecmethod nbf pmdpmd mixrandompartial credit a aucec 5performance of gram wtype vs. pmd and the combination modelderby lucene openjpa qpid wicket .
.
.
.
.
releaseauceclmethod nbf pmdpmd mixrandompartial credit b aucecl performance of gram wtype vs. pmd and the combination model figure partial credit performance evaluation of gram wtype w.r.t.sbf and a mix model which ranks the sbf lines by entropy.
large effect cohen s d .
sbf andnbf performed comparably nbf performed slightly better when using both partial credit and the specified bug fix threshold but when dropping the threshold and or with full credit no significant difference remains between nbf andsbf .
no significant difference in performance was found between f indbugs and pmd either.
in all comparisons all approaches retrieved relatively bug prone lines by performing substantially better than random.
in fact at inspection budget both the line level nbf and the two sbf performed substantially better than the earlier presented dpmethod andfile level nbf compare figure a and figure .
result entropy achieves comparable performance to commonly used static bug finders in defect prediction.
notably nbf had both the highest mean and standard deviation of the tested models whereas pmd s performance was most robust.
this suggests a combination of the models we can order the warnings of the sbf using the gram wtype model.
in particular we found that the standard priority ordering of the sbf is already powerful so we propose to re order the lines within each priority category.
rq5.
is naturalness a useful way to focus the inspection effort on warnings produced by sbf ?
to answer this question we again assigned values to each line based on the sbf priority as in rq4.
however rather than add random tie breakers we rank the lines within each priority bin bythe deterministic gram wtype score.
the results for pmd are shown in figure first using the aucec 5measure a and then using the aucecl measure b .
pmd mix refers to the com bination model as proposed.
overall the combined model produced the highest mean performance in both categories.
it significantly outperformed the two sbf s in all cases p .
and performed similarly to the nbf model significantly better on lucene andqpid significantly worse on derby p .
all with small effect .
these results extended to the other evaluation methods using full credit and or removing the threshold for max bug fix size.
in all cases the mix model was either significantly better or no worse than any of theother approaches when averaged over all the studied releases.
we further evaluated ranking all warnings produced by the sbf by entropy ignoring thesbf priorities and found comparable but slightly weaker results.
these results suggest that both nbf and sbf contribute valuable information to the ordering of bug prone lines and that their combination yields superior results.result ordering sbf warnings by priority and entropy significantly improves sbf performance.
.
threats to validity internal validity.
a number of threats to the internal validity arise from the experimental setup.
first our identification of buggy linescould be wrong as we used a simple key word based search toidentify buggy commits see section .
.
to minimize this threat we manually evaluated our bug classification tool and reported an overall accuracy of .
another source of false negatives is the presence of yet undetected bugs that linger in the code base.
also developers may tangle unrelated changes into one commit .
however given the high significance of entropy difference between buggy and nonbuggy lines we consider it unlikely that these threats could invali date our overall results.
furthermore nbf s positive performance on higher quality jira based phase ii dataset confirms our expectations regarding the validity of these results.
a threat regarding rq2 is the identification of fixed lines which replaced buggy lines during a bugfix commit.
the comparisons between these categories could be skewed especially when thebugfix commits replace buggy lines with a larger number of fixed lines.
in fact small bug fixes do indeed add more lines than they delete on average the reverse holds for fixes spanning over 50lines .
however a majority of bugs of any size were fixed withat most as many lines.
in particular more than two third of oneand two line bugs which demonstrated the greatest decrease in entropy were fixed with one and two lines respectively.
thus these findings minimize the above threat.
other non bugfixing changes e.g.
introduction of clone may also show drop in entropy w.r.t.
its previous version.
our comparison of sbf andnbf assumes that indicated lines are equally informative to the inspector which is not entirely fair nbf just marks a line as surprising whereas sbf provides specific warnings.
on the other hand we award credit to sbf whether or not the bug has anything to do with the warning on the same lines indeed earlier work suggests that warnings are not often related to the buggy lines which they overlap.
so this may not be a major threat to our rq4 results.
finally the use of aucec to evaluate defect prediction has been criticized for ignoring the cost of false negatives the development of better widely accepted measures remains a topic of future research.external validity.
external validity concerns generalizability of our result.
to minimize this threat we use systems from different 436domain from github and apache having a substantial variation in age size and ratio of bugs to overall lines see table .
we alsoconfirmed our overall result by studying entire evolutionary history of all the projects analyzed with months snapshot interval.
next does this approach generalize to other languages?
there is nothing language specific about the implementation of n gram and gram models gram wtype model however does require parsing which depends on language grammar .
prior research showedthat these models work well to capture regularities in java c and python .
to investigate nbf s performance for other languages we performed a quick sanity check on c c projects libuv bitcoin andlibgit studying evolution from november january .
the results are consistent with those pre sented in table and figure buggy lines are between .
and .
bits more entropic than non buggy lines at bug fix threshold slightly larger than in table .
also the entropy of buggy linesat this threshold drops by nearly one bit.
these findings strongly suggest that our results generalize to c c we are investigating the applicability to other languages.
finally even though we showed good empirical results with our approach this does not assure that it actually helps developers in their bug finding efforts as was shown in a similar scenario withautomated debugging techniques by parnin et al.
.
to tackle this the natural next step would be a controlled experiment with developers using our approach.
.
related work statistical defect prediction.
dpaims to predict defects yet to be detected by learning from historical data of reported bugs in issue databases e.g.
jira .
this is a very active area see for a survey with even a dedicated series of conferences i.e.
promise .
d ambros et al.
survey and provide a direct comparison of a number of representative dpapproaches including those using process metrics such as previous changes and defects and product metrics such as code complexity metrics .
while earlier work evaluated models using ir measures such as precision recall and f score more recently non parametric methods such as auc and aucec have gained in pop ularity.
d ambros et al.
follow this trend and conduct an evaluation similar to ours.
adpmay work beyond file granularity giger et al.
presented adpat the level of individual methods .
we are the first to predict defects at a line level using only statistical models.static bug finders.
sbf can work at line granularity as opposed todp hence the comparison with our approach.
the work closely related to ours is by rahman et al.
by comparing dpperformance with sbf they reported that popular sbf tools like findbugs and pmd do not necessarily perform better than dp.w e also find that nbf can be used to rank sbf warnings but we are not the first to tackle this challenge.
kremenek et al.
usezranking and a cluster based approach to prioritizing warnings based on the warnings previous success rate .
kim and ernstmine information from code change history to estimate the impor tance of warnings .
our approach ranks warnings based on properties of the source code rather than the output of the sbf or whether and how warnings have been fixed in history.
future research could evaluate how our approaches can complement thework above.
ruthruff et al.
propose a filtering approach to detecting accurate and actionable sbf warnings .
they use priority of warnings defined by the sbf type of error detected and features of the affected file e.g.
size and warning depth to do the filtering.
our approach ranks warnings on a different aspect ofsource code than those they consider and could be used to com plement their model.
finally heckman et al.
proposed faultbench a benchmark for comparison and evaluation of static analysis alert prioritization and classification techniques and used it to validate the aware tool to prioritize static analysis tool warnings.
since results of our approach are promising further research could investigate our approach against this additional benchmark.
the field of sbf has advanced rapidly with many developments researchers identify new categories of defects and seek toinvent methods to find these defects efficiently either heuristicallyor though well defined algorithms and abstractions.
since neithermethod is perfect the actual effectiveness in practice is an empirical question.
a comprehensive discussion of related work regardingsbf and their evaluation can be found in rahman et al.
.
inferring rules and specifications.
statistical language models are employed to capture the repetitive properties of languages in our case of programming languages thus inferring the style or even some latent specification about how the language is supposed to beused in a specific project and context.
as such our work is re lated to previous research that tries to automatically infer specifica tions and use it to identify outliers as probable defects.
kermenek et al.
present a framework based on a probabilistic model namely factor graph for automatically inferring specifications from programs and use it to find missing and incorrect properties in a specification used by a commercial static bug finding tool .
in our case we allow errors to be localized without language specific tuning without defining the set of annotations to infer and withoutmodeling domain specific knowledge.
wasylkowski et al.
mine usage models from code to detect anomalies and violations in meth ods invoked on objects and demonstrated that these can be used to detect software defects .
similarly thummalapenta and xie develop alattin an approach to mine patterns from apis and detect violations .
in contrast our model is less specialized and tries to highlight unexpected patterns at token level without focusing onthe specific case of method invocations.
.
conclusion the predictable nature naturalness of code suggests that code that is improbable unnatural might be wrong.
we investigate this intuition by using entropy as measured by statistical language models as a way of measuring unnaturalness.
we find that unnatural code is more likely to be implicated in a bug fix commit.
we also find that buggy code tends to becomemore natural when repaired.
we then turned to applying entropy scores to defect prediction and find that when adjusted for syntactic variances in entropy and defect occurrence our model is about as cost effective as the commonly used static bug finders pmd andfindbugs.
applying the deterministic ordering of entropy scores to the warnings produced by these static bug finders produces the most cost effective method.
these findings suggest that entropyscores are a useful adjunct to defect prediction methods.
the findings also suggest that certain kinds of automated search based bugrepair methods might do well to have the search in some way influenced by language models.
in the near future we plan to build extensions into pmd findbugs and other static bug finders that order warnings based on our gram wtype regime.
further ahead we plan to study other applications of these approaches including dynamic fault isolation methods and automated bug patching tools.
acknowledgment.
this material is based upon work supported by the national science foundation under grant no.
.
.