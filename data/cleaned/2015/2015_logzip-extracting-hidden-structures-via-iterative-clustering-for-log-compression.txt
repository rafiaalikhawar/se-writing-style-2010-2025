logzip extracting hidden structures via iterative clustering for log compression jinyang liu bardbl jieming zhu shilin he pinjia he zibin zheng bardbl michael r. lyu bardblsun yat sen university guangzhou china huawei noah s ark lab shenzhen china the chinese university of hong kong hong kong china eth zurich switzerland liujy57 mail2.sysu.edu.cn jmzhu ieee.org slhe lyu cse.cuhk.edu.hk pinjia.he inf.ethz.ch zhzibin mail.sysu.edu.cn abstract system logs record detailed runtime information of software systems and are used as the main data source for many tasks around software engineering.
as modern software systems are evolving into large scale and complex structures logs have become one type of fast growing big data in industry.
in particular such logs often need to be stored for a long time in practice e.g.
a year in order to analyze recurrent problems or track security issues.
however archiving logs consumes a large amount of storage space and computing resources which in turn incurs high operational cost.
data compression is essential to reduce the cost of log storage.
traditional compression tools e.g.
gzip work well for general texts but are not tailed for system logs.
in this paper we propose a novel and effective log compression method namely logzip .
logzip is capable of extracting hidden structures from raw logs via fast iterative clustering and further generating coherent intermediate representations that allow for more effective compression.
we evaluate logzip on five large log datasets of different system types with a total of .
gb in size.
the results show that logzip can save about half of the storage space on average over traditional compression tools.
meanwhile the design of logzip is highly parallel and only incurs negligible overhead.
in addition we share our industrial experience of applying logzip to huawei s real products.
index t erms logs structure extraction log compression log management iterative clustering i. i ntroduction system logs typically comprise a series of log messages each recording a specific event or state during the execution of both user applications and components of a large system.
these logs have widespread use in many software engineering tasks.
they are not only critical for system operators to diagnose runtime failures to identify performance bottlenecks and to detect security issues but also potentially valuable for service providers to track usage statistics and to predict market trends .
nowadays logs have become one type of fast growing big data in industry .
as systems grow in scale and complexity logs are being generated at an ever increasing rate.
for example either in the cloud side e.g.
a data center hosts thousands of machines or in the client side e.g.
a smartphone vendor with millions of smart devices worldwide it is common for these systems to generate tens of tbs of logs in a single day .
the massive logs could easily lead to several pbs of pinjia he is the corresponding author.data growth a year.
in addition each log is usually replicated into several copies such as in hdfs for storage resilience.
some important parts of log data are even synchronized across at least two separate data centers for disaster recovery.
this imposes severe pressure on the capacity of storage systems.
what s more many logs require long term storage usually a year or more according to the development lifecycle of software products.
historical logs are amenable to discovering fault patterns and identifying recurrent problems .
for example many users often rediscover old problems because they have not installed fix packs .
meanwhile auditing logs which record sensitive operations performed by users and administrators are often required to be kept for at least two years for possible tracking of system misuse in future.
although storage has become much cheaper than before archiving logs in such a huge volume is still quite costly.
it not only takes up a great amount of storage space and electrical power but also consumes network bandwidth for transmission and replication.
to reduce the heavy storage cost of log data our engineering team seeks two directions of data reduction reducing logs from the source and log compression.
logs can be largely reduced by requesting developers to print less logging statements and setting appropriate verbosity levels e.g.
info and error .
yet logging too little might miss some key information and result in unintended consequences .
how to set up an optimal logging standard is still an open problem .
instead we focus on log compression in this paper.
it is a common practice to apply compression before storing the data on disks.
mainstream compression schemes e.g.
gzip and bzip can usually reduce the size of logs by a factor of .
these general purpose compression algorithms allow for encoding arbitrary binary sequences but can only exploit redundant information within a short sliding window e.g.
32kb in gzip s deflate algorithm .
as such they cannot take advantage of the inherent structure of log messages that might enable more effective compression.
to address this problem in this paper we present logzip a novel log compression method.
in contrast to traditional compression methods logzip can compress large log files with a much higher compression ratio by harnessing the inherent structures of system logs.
log messages are printed 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
by specific logging statements thus each has a fixed message template.
the core idea of logzip is to automatically extract such message templates from raw logs and then structurize them into coherent intermediate representations that are better suitable for general purpose compression algorithms.
to achieve this we propose the iterative clustering algorithm for structure extraction following an iterative process of sampling clustering and matching.
logzip further generates three level intermediate representations with field extraction template extraction and parameter mapping.
these transformed representations are further fed to a traditional compression method for final compression.
the whole logzip process is designed to be efficient and highly parallel.
as a side effect the structured intermediate representations of logzip can be directly utilized in many downstream tasks such as log searching and anomaly detection without further processing.
we evaluate logzip on five real world log datasets i.e.
hdfs spark andriod windows and thunderbird from the loghub repository .
they are chosen to span multiple types of systems including distributed systems mobile systems operating systems and supercomputing systems and also have different sizes ranging from .
gb to .
gb.
the experimental results confirm the effectiveness of logzip which achieves high compression ratios .
.
.
compared to traditional compression schemes i.e.
gzip bzip2 lzma logzip achieves additional .3x .1x compression ratios.
this leads to a reduction of .
storage cost on average.
additionally logzip is highly efficient since the proposed iterative clustering algorithm can be embarrassingly parallelized.
we have successfully applied logzip to a real product of huawei and also share some of our experiences.
we emphasize that logzip is generally applicable to all system generated textual logs and we leave its use for binary logs for future research.
in summary our paper makes the following contributions we propose an effective compression method logzip which leverages the hidden structures of logs extracted by iterative clustering.
extensive experiments are conducted on a range of log datasets to validate the effectiveness and the general applicability of logzip.
we not only share our success story of deploying logzip in industry but also open the source code of logzip1to allow for future research and practice.
the remainder of this paper is organized as follows.
section ii introduces the structure of system logs.
we present our iterative structure extraction approach in section iii and then describe its use in log compression in section iv.
the experimental results are reported in section v. the industrial case study is described in section vi.
we review the related work in section vii and finally conclude the paper in section viii.
ii.
l ogstructure in this section we introduce the structures of execution logs which will be utilized to facilitate log compression.
fig.
a logging statement code snippet extracted from spark storage blockmanager.scala loginfo s found block blockid remotely d d d d found block rdd 2 3 locallyd 17 info storage.blockmanager found block rdd 2 0 locally info storage.blockmanager found block rdd 2 3 locally found block locally d w rdd 2 0 rdd 2 317 info storage.blockmanager d fig.
.
an example of extracted log structure shows the outputs of the logging statement loginfo s found block blockid locally in the source code of spark.
loginfo is a logging framework in scala and the free text within the brackets are written by developers.
this logging framework automatically records information like logging date time verbosity level component etc.
when the logging statement is executed it outputs a log line like info storage.blockmanager found block rdd locally.
.
the cluster computing framework spark uses logs like this to monitor its execution.
in practice a large scale software system such as spark records a great deal of information.
to reduce the storage cost we focus on optimizing compression of logs by exploring the structure of logs.
specifically hidden structures can be observed in the example in fig.
.
the log message contains two parts the message header automatically generated by the logging framework and the message content recorded by developers.
there are several fields in the message header such as date time level component .
the format is generally fixed for a system since developers barely change the logging framework they use.
therefore it is possible to easily extract these fields from each log of a system by using manually defined regular expressions.
if more than one logging frameworks are involved users could define different regular expressions according to different log formats which only takes minutes for a developer.
unlike the message header the message content is unstructured because developers are allowed to write free form texts to describe system operations.
however it is possible to find hidden structures in the message content.
for example in fig blockid in the logging statement is a variable that may change in every execution i.e.
variable part whereas other parts remain unchanged i.e.
constant part .
we propose to automatically extract the constant part from raw logs as hidden structure via iterative structure extraction ise .
in the process the constant part and variable part of a given raw log message can be distinguished.
in this paper we denote the constant part as event template or template in short and the variable part as parameters.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iii.
i tera tive structure extraction our proposed approach logzip mainly leverages the hidden structures of logs to facilitate log compression.
in this section we introduce the iterative clustering algorithm for hidden structure extraction.
a. overview there are three major ways to extract templates2from logs manual construction from logs extraction from logging statements in source code and extraction from raw logs.
in practice software logs have complex hidden structures and are large scale.
thus manual construction of templates is labor intensive and error prone.
additionally the source code of specific components of the system is often inaccessible e.g.
third party libraries .
therefore template extraction from software logs is the most widely applicable approach and thus logzip proposes an iterative clustering algorithm to extract templates from logs automatically.
according to the benchmark by zhu et al existing template extraction approaches could perform accurately on software logs.
however these methods require all the historical logs as input leading to severe inefficiency and hindering them from adoption in practice.
inspired by the cascading clustering by he et al.
we propose iterative structure extraction ise which effectively extracts templates from only a fraction of the historical logs.
figure illustrates the overview of ise.
ise is an iterative algorithm containing steps in each iteration sampling clustering and matching.
the input of ise is a log file consisting of raw log messages and the output is extracted templates and structured logs.
specifically in an iteration we first sample a portion of the input logs.
a hierarchical division method is then applied to the sample logs to generate multiple clusters from which templates can be extracted automatically.
in the matching step we try to match all the unsampled raw logs with these templates collect unmatched logs and feed them into the next iteration as input.
by iterating these steps all log messages could be matched accurately and efficiently with a proper template assignment.
the reason behind this is that the sampled logs can often cover the templates hidden in most of the input logs in each iteration.
in particular a fraction of the logging statements could be executed much more frequently than the others.
therefore templates generated from a small portion of logs can generally match most raw logs at the first several iterations.
in the following we introduce each step of ise in detail.
b. sampling we first randomly sample a portion of logs from the given raw log file with a ratio p. thus each log line has an equal probability p e.g.
.
to be selected.
if the input contains llog lines the sampling step results in s p lsampled log lines.
this step is inspired by the insight that dominant templates in the original input logs are still dominant in the sampled logs.
2we use hidden structure and template interchangeably in this paper.
5dz rjv 6dpsolqj oxvwhulqj 7hpsodwh wudfwlrq0dwfklqj whudwlrq 0lvpdwfkhg 0dwfkhg 0dwfklqj fig.
.
overview of iterative structure extraction c. clustering these sampled log lines are then grouped into clusters.
ise extracts a template from each cluster by hierarchical divisive clustering in a top down manner where we start with a single cluster that consists of all sampled log lines.
we observed that execution logs have multiple features i.e.
verbosity level component name and most frequent tokens that can be utilized to distinguish the clusters they belong to.
thus we hierarchically divide logs into coarse grained clusters by using one feature at each division.
after that an efficient clustering algorithm is applied to each of these clusters to further divide logs into fine grained clusters.
to facilitate efficient log compression the fine grained clustering algorithm is designed to be highly parallel.
we detail the coarse grained clustering i.e.
divisions by level component name most frequent tokens and the fine grained clustering algorithms as follows divide by level intuitively logs in the same cluster should share the same level e.g.
info logs are generally quite different from debug logs because they are recorded for different purposes.
therefore we first divide logs into clusters according to their levels.
divide by component name similar to the reason for using the level feature logs generated by different components in a system are barely in the same cluster.
so we further separate logs with the same level by their components.
divide by frequent tokens intuitively the constant parts of a log generally have higher occurrences than its parameter parts because the parameter parts may vary in executions of a logging statement while the constant parts do not.
therefore it is reasonable to group logs that share the same frequent tokens into the same cluster.
to achieve this we first tokenize each log message to a list of tokens by using system defined or as user input delimiters e.g.
comma and space .
then we count the frequency of each token in the sampled logs.
after that we find the top frequent token for each log line according to which we further divide the clusters obtained from the last division using component names.
thus in this step logs grouped to the same cluster share the same top frequent tokens.
moreover top top ..top nfrequent tokens can be applied in the same way to further divide the clusters where nis a tunable parameter that is normally set to .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lvwlqj oxvwhuv1hz rj 0hvvdjhv6lplodulw rpsxwlqj8sgdwh 7hpsodwhv fig.
.
workflow of sequential clustering divide by fine grained clustering the hierarchical division by log features results in coarse grained clusters.
logs in the same cluster share the same features as described above.
however these features are not sufficient to determine fine grained clusters from which we could extract accurate templates.
therefore we further conduct fine grained clustering on each of the clusters.
inspired by spell we use longest common subsequence lcs to compute the similarity between log messages.
importantly we also improve the original lcs for speedup.
we define the improved similarity as a b a b whereaandbare tokenized log messages denotes the number of tokens in a sequence.
in other words a b is the number of common tokens of a and b. we perform the fine grained clustering in a streaming manner.
fig.
describes the workflow of our method.
given a log message m we first tokenize it then assign it to an existing cluster.
to be more specific we compute the similarity between the input log message with the representative template of each existing cluster while we keep the largest similarity and the corresponding cluster.
if the kept similarity is greater than a threshold of we assign the input log message to the cluster.
note that m 2by default where m denotes the number of tokens contained in the input log.
after the assignment we update the template of the cluster aslcs m t wheretis the old template representing the cluster.
note when computing lcs we mark at the places where the two sequences disagree.
for example the lcs of the two logs delete block blk blk and delete block blk is delete block .
if the largest similarity could not reach we create a cluster for m withmitself as the representative template.
the time consuming step is the computation of similarity between the given log and each template of existing clusters.
we propose to use the number of common tokens instead of lcs to measure similarity which is much more efficient yet effective for two reasons logs with same tokens but different orderings rarely occur in logs.
we have utilized obvious log features to divide logs into coarse grained clusters.
in each of the clusters logs are expected to share only a few templates i.e.
there are few opportunities for conflicts to occur.as described above the sampled logs are divided into clusters hierarchically each of which has a template to represent logs within the cluster.
we emphasize that the clustering algorithm is highly parallel.
in particular the clusters after each division are independent so they could be dispatched to different nodes for parallel computation on the subsequent steps.
d. matching after collecting all templates from clusters we use the templates to match each unsampled log message as described in fig.
.
by matching each log message is assigned a template thus the hidden structure is extracted.
we use the hidden structure to facilitate log compression.
a traditional matching strategy is to transform templates into regular expressions by replacing with .
?
then apply regular expressions matching between every combination of log messages and templates which may explode because of the large number of templates.
to mitigate this efficiency issue we propose to build all candidate templates as a prefix tree and perform matching by searching through it.
we build all templates as a prefix tree before searching.
the prefix tree starts with a start node.
when a template arrives we tokenize it as is done to a log message.
the first token of the log is inserted as a child node of the start node.
then we pass through the token sequence while the previous token is the parent node of the current one.
at the end of the last node we add an end node that contains the whole template for convenience.
intuitively each template sequence is mapped as a path in the prefix tree.
we put all templates into the same tree and since different templates can have several prefix tokens in common their paths may overlap.
because of sharing the prefix tokens a log message is compared with all templates at the same time while searching in the compact tree.
specifically given a log message we tokenize it and read from the first token to the last while comparing with nodes in the tree.
for the first token we search if it exists in the second layer of the tree the first layer is a start node .
if the first token matches a node we continue to check the second token and the children of the node.
we stop when all tokens are read.
if an end node is reached we return the template or we return none to denote mismatching.
note that in a template denotes parameters with variable length thus we allow in the tree to hold more than one tokens if no child node of matches the next log token.
for example delete block can successfully match delete block blk blk .
in addition parameters of a log could be extracted while matching by keeping tokens that match .
in the above example blk blk is the parameter.
as a result of the matching step the template and parameters of a log message are extracted if it matches successfully.
the intuition of the tree matching scheme is to compress all templates into a prefix tree by overlapping paths.
therefore the comparison between log message and all templates becomes one pass searching.
moreover checking whether a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
6wuxfwxuh wudfwlrq0hvvdjh hdghu dwh 7lph rpsrqhqw 6solw rpsuhvv7hpsodwhv 3dudphwhuv6xe lhogv yhqw 6solw 0hvvdjh rqwhqw5dz orjv 6xe lhogv rpsuhvvhg loh qwhuphgldwh 5hsuhvhqwdwlrq j ls e ls o pd fig.
.
overall framework of logzip compression token matches a node can be done in o by hashing which makes the tree based strategy a lot more efficient in comparison with regular expression matching.
more importantly the matching step is highly parallel because the search for different log lines on the tree is independent.
e. iteration at the end of each iteration ise obtains several templates that could cover all sampled logs.
these templates are sufficient to match the majority of all the input log messages in this iteration while some log messages may remain unmatched.
therefore we repeat the above procedures i.e.
sampling clustering and matching for the unmatched log messages as shown in fig.
.
to this end new templates are extracted from these log messages and new unmatched data is generated in each iteration.
we keep iterating until the percentage of matched log messages reaches a user defined threshold empirically .
in practice logging statements of a system evolve slowly.
therefore ise could be considered as a one off procedure for a specific system.
to be more specific we can perform ise on a portion of logs of the system and collect templates for future use.
after having templates we could extract structures of new logs from the system through matching instead of running the ise.
iv .
l ogcompression in this section we present our log compression method logzip.
we first summarize the workflow of logzip.
then the detail of the compression approach is introduced.
a. overview of logzip the main idea behind logzip is to reduce the redundant information contained in the original log file.
fig.
depicts the overall framework of logzip.
for each raw log message in the log data it is firstly structurized into message header and message content via manually defined regular expressions.
then the message header is split into multiple objects according to their fields and further sub fields.
regarding message content hidden structures are extracted by applying ise.
after that we represent each log message as a template an eventid as well as the corresponding parameter.
in addition each item in the parameter list is split into sub fields.
then those logs that share the same event id are stored into the same object in a compact manner.
at last all generated objects are compressed to a compact file by existing compression tools.
details are described as follows.
b. approach logzip can perform compression in levels and achieve different effectiveness and efficiency.
fig.
is an example of logzip workflow.
level field extraction .
we first extract fields of given raw logs by applying a user defined regular expression.
for the example in fig.
da ta time and level could be extracted by identifying the white space delimiter.
component and message content are separated by a colon.
we emphasize that it is easy to manually construct the regular expression which generally remains unchanged for a specific system since the message header is automatically recorded by the logging framework e.g.
log4j in java logging in python .
then each field is further split into sub field according to special characters e.g.
non alphabetic characters to increase the coherence in a sub field.
these sub fields are then stored into separate objects.
for level we do not process the message content and store it into an object.
level template extraction .
the message content is further processed by extracting the hidden structures i.e.
message templates.
we directly apply ise as described in section iii.
after that the message content could be represented as its template and parameters and we assign autoincremental eventid initialized to for unique templates which forms a template mapping dictionary eventid is the key and the corresponding template is the value .
in fact a template may be shared by many log messages.
for example the hdfs logs that we studied contain around .
million log messages but they share only templates.
therefore we use the corresponding eventid to denote the template of each log message.
in this log messages are transformed into a compact form containing short eventid and parameters.
at last the template mapping dictionary is stored into an object authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
vwrudjh orfn0dqdjhu rxqg eorfn uggb b orfdoo vwrudjh orfn0dqdjhu rxqg eorfn uggb b orfdoo vsdun dfkh0dqdjhu 3duwlwlrq uggb b qr w irxqg frpsxwlqj lw vsdun dfkh0dqdjhu 3duwlwlrq uggb b qr w irxqg frpsxwlqj lw vwrudjh orfn0dqdjhu vwrudjh orfn0dqdjhu vsdun dfkh0dqdjhu vsdun dfkh0dqdjhu rj 6wuxfwxul dwlrq lhog wudfwlrq 6wufwxuh wudfwlrq 0dsslqj vwrudjh orfn0dqdjhu vwrudjh orfn0dqdjhu vsdun dfkh0dqdjhu vsdun dfkh0dqdjhu 7hpsodwh 0dsslqj 2emhfw yhqw 2emhfw6xe lhog 2emhfw rxqg eorfn orfdoo 3duwlwlrq qrw irxqg frpsxwlqj lw ugg b 7hpsodwhv 3dudphwhuv yhqw 3dud 3dudphwhu 0dsslqj 2emhfw6xe lhog 2emhfw3dud 2emhfw6xe lhog 2emhfw0 rxqg eorfn uggb b orfdoo rxqg eorfn uggb b orfdoo 3duwlwlrq uggb b qrw irxqg frpsxwlqj lw 3duwlwlrq uggb b qrw irxqg frpsxwlqj lw 5dz rjv .huqho rpsuhvvlrq fig.
.
an example of logzip workflow in three levels alone while the eventids are stored in an eventid object.
for the parameters extracted from the message content we split each item of parameters in a similar manner as in level .
clearly each parameter is split with non alphabetic characters as delimiters.
then each generated sub field within a group constitutes one object separately.
here a group represents all logs that share the same template.
the intuition behind is that parameters within a group may be duplicated or similar and putting similar items into a file could make the best of existing tools e.g.
gzip.
level parameter mapping .
we further optimize the representation of parameters in level .
based on our observation some inseparable and very long parameters i.e.
no delimiter inside waste too much space.
for example the block id e.g.
blk is space consuming and may have high occurrence.
to sidestep the problem as shown in fig.
we encode unique sub field values to sequential 64base numbers paraid which forms a parameter mapping dictionary paraid is the key and the corresponding parameter is the value .
for the sake of saving more space parameters from all groups share the same parameter mapping dictionary.
to conclude in level parameters are encoded into paraids.
at last one parameter mapping dictionary object and paraid objects for each group are generated separately.
compression .
after three levels of splitting encoding and mapping several objects are generated.
the last step is to pack all these objects to be a compressed file without losing any information.
since our main interest lies in the aforementioned three levels of processing we directly utilize those off theshelf compression algorithms and tools in this step e.g.
gzip bzip2 and lzma.
in this way our logzip is compatible with existing compression tools and algorithms.
it is worth noting that most log analysis algorithms e.g.
anomaly detection take templates as input without parameters.
therefore logzip could perform lossy compression in this case by discarding all parameter objects before compression with existing tools which could be more effective.
decompression .
as the reverse process of log compression decompression should be able to recover the original dataset without losing any information.
at first multiple objects are generated after unzipping the compressed file.
then we recover the message header by simply merging all the subfields values extracted in level in order.
as for recovering the message content we first get the templates by indexing the event encoding dictionary with the eventid.
similar the parameter list is retrieved by indexing the parameter encoding dictionary using the paraid.
by replacing the in the template using parameters in order the message content can be completely recovered.
v. e v alua tion in this section we conduct comprehensive experiments by applying logzip to a variety of log datasets and report the results.
we aim to answer the following research questions.
rq1 what is the effectiveness of logzip?
rq2 how effective is logzip in different levels?
rq3 what is the efficiency of logzip?
a. experimental setup log datasets we use five representative log datasets to evaluate logzip as presented in table i. these log datasets are generated by various systems spanning distributed systems hdfs spark operating system windows mobile system android and supercomputer thunderbird .
some datasets hdfs thunderbird are released by previous log research while the other spark windows android are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i summary of logda tasets dwdvhw hvfulswlrq 7lph 6sdq 0hvvdjhv 6l h v vwhp orj krxuv 6sdun 6sdun mre orj qgurlg qgulrg v vwhp orj lqgrzv lqgrzv hyhqw orj gd v 7kxqghuelug 6xshufrpsxwhu orj gd v collected from real systems in our lab environment.
moreover the total size of all datasets is around .
gb which contains a total amount of more than million log messages.
all the datasets that we use are available on our github.
evaluation metrics to measure the effectiveness of logzip we use compression ratio cr which is widely utilized in the evaluation of compression methods.
the definition is given below cr original file size compressed file size note that the original size of a given log dataset is always fixed while the compressed file size may vary.
when reducing the compressed file size a higher compression can be achieved which indicates more effective compression.
compression kernels as introduced in section iv logzip utilizes existing compression utilities as the compression kernel in the last step.
in the experiments three prevalent and effective compression algorithms i.e.
gzip bzip2 lzma are selected.
note that these compression algorithms can also be employed to compress log files solely which will serve as baselines in our experiments.
experimental environment we run all experiments on a linux server with intel xeon e7 .20ghz cpu and 1tb ram running red hat .
.
with linux kernel .
.
.
b. rq1 effectiveness of logzip to study the effectiveness of logzip we use logzip it to compress all five collected log datasets.
as introduced before we use these existing popular compression tools i.e.
gzip bzip2 lzma as well as two log compression algorithms i.e.
cowic logarchive as baselines for a fair comparison.
since logzip can be equipped with different compression kernels it also has three variants i.e.
logzip gzip logzip bzip2 logzip lzma .
we report both the compressed file size and the compression ratio cr in table ii.
note that all results of logzip are obtained in level .
we first make brief comparisons among gzip bzip2 and lzma.
lzma is generally the most effective one on most datasets while gizp performs the worst.
as for the two algorithms specifically designed for log data cowic and logarchive logarchive could achieve higher cr than gzip but is generally less effective than bzip2 and lzma.
cowic is even worse than gzip since cowic is designed for a quick query on compressed data instead of pursuing high cr.
logzip variants with different compression kernels result in different compressed size which is determined by theeffectiveness of the kernels.
compared with the three baseline methods logzip equipped with the corresponding compression kernel achieves higher cr on all five datasets.
in particular logzip can achieve a cr of .56x on average and .1x at best over the gzip traditional mature compression algorithm.
for example the compressed file is around 149mb by gzip while 72mb by logzip gzip and our method can save around half of the storage which is crucial in practice.
logzip equipped with other compression kernels achieves similar results.
c. rq2 effectiveness of logzip in different levels as introduced in section iv logzip is designed in three levels splitting the original log message into multiple objects with different fineness.
in this section we evaluate the effectiveness of logzip at each level.
in practice logs are generated and collected in a streaming manner and stored as a file when they grow to a proper size e.g.
1gb.
for the simplicity of comparison our experiments are conducted on the first 1gb logs of all five datasets.
besides since our focus lies in varying different levels and compression kernel is not a major concern we conduct experiments by taking gzip as the baseline method and logzip gzip as our compression approach.
based on this we vary the level of logzip gzip to evaluate the effectiveness of an individual level and the experimental results should also apply to other compression kernels.
fig.
shows compressed file sizes on five datasets in different levels.
we can find that level field extraction works well on all datasets compressing the original 1gb log files to files of less than 100mb.
it is because generally most fields of a log file have limited unique values and gzip is able to compress such text data into a file of small size.
moreover compared to the baseline gzip compression logzip with only level already achieves much better compression results.
considering the structure extraction of message content in level it sharply reduces the compressed size on almost every dataset.
in particular after applying logzip level the compressed log file of android takes up only1 3of the baseline gzip compressed file and similarly the fraction is even less than1 10on windows.
the results confirm the effectiveness of level in log compression.
the reason is also straightforward.
in level ise extracts the invariant templates out of log message content and replace it with an event id.
hence only keeping event ids instead of original template strings saves a large amount of storage space.
besides parameters are also split in a similar way as level field extraction which contributes to reducing the compressed file size.
however we also observe that the improvement is not obvious on the hdfs log file.
after close analysis on the hdfs logs we find that the major part of hdfs log message content is parameters instead of the template.
parameters such as block id e.g.
blk are too long and cannot be separated in level .
therefore the compressed file size of the hdfs data is not as small as other log files.
in level we map parameters into base numbers.
as shown in fig.
logzip gets at least half the compressed size compared with gzip on all five datasets.
in particular authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii compression results w .r.t.size inmb and compression ra tio cr ofdifferent compression methods cowic and logarchive are baseline algorithms for log compression .
6l h 6l h 6l h 6l h 6l h 5dz rzlf rj ufklyh j ls orj ls j ls lpsuryhphqw e ls orj ls e ls lpsuryhphqw o pd orj ls o pd lpsuryhphqw 7kxqghuelug rpsuhvvlrq 6sdun qgurlg lqgrzv fig.
.
compressed file size mb in different levels comparing to logzip level the compressed file size is greatly reduced on hdfs dataset which confirms the importance of encoding the long and duplicate parameters as aforementioned.
comparable or slightly worse results show on other datasets.
this is caused by introducing extra paraids for these logs without many space consuming parameters.
in fact these extra paraids cost little space which is tolerable.
that is users could directly apply logzip level to their dataset to achieve the best performance without considering whether the log contains such parameters.
to conclude logzip is effective in every level for the log dataset that we study.
more importantly logzip theoretically generalizes well for the text log data of other types for reasons only a little prior knowledge is required when formatting raw logs.
once set up no more manual effort is required unless a system updates greatly.
logs generated by logging statements naturally contain hidden structures.
the key step of logzip ise is able to automatically extract the hidden information used for log compression.
note that logzip is designed for logs stored in text form which is the most common case.
those in a binary format are beyond our consideration and we will explore the case in future work.d.
rq3 efficiency of logzip in this section we evaluate the efficiency of logzip gzip in short logzip .
logzip is designed to be highly parallel thus we would like to know the efficiency achieved by utilizing different numbers of workers.
gzip is known as an efficient compression tool thus used as a baseline.
we apply both algorithms on the first 1gb log data of hdfs spark thunderbird and windows for the same reason mentioned in section v c. we exclude android dataset for saving space.
in addition we vary the worker number of logzip in range .
fig.
depicts the execution time and compressed size achieved by logzip and gzip on four datasets.
note that the execution time consists of all steps including ise sec iii and compression sec iv .
as for logzip the time cost halved after doubling the number of workers.
more importantly it is worth noting that the time cost by using workers is comparable with that of gzip even better in hdfs and spark.
the result shows the parallelizability and efficiency of logzip which can be explained by the design of logzip we extract highquality templates from only a small portion of logs in ise e.g.
log messages are sufficient to generate templates that authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a hdfs b spark c thunderbird d windows fig.
.
compression time size vs worker match logs messages.
the top down manner clustering are higly parallel since fine grained clustering could be performed simultaneously and independently.
we buildall templates into a compact prefix tree the one pass matchingscheme is efficient.
a log file could be split into severalchunks and performing logzip on each chunk at the same timeis possible to reduce time consumption.
in addition the compressed size slightly increases with more workers involved.
this is the result of chunking of logs.
awhole log file is split into chunks before feeding to a worker as a result each worker only sees a part of the data whichslightly hinders logzip to utilize the global information forcompression.
to conclude logzip is highly parallel and could achieve comparable or better efficiency than gzip.
note that afterchucking the input log file the compressed size may increasea little bit but it is tolerable in practice.
vi.
i ndustrial case study at huawei logs are continuously collected during the whole product lifecycle.
with the rapid growth of scale andcomplexity of industrial systems logs become a representativetype of big data for software engineering teams at huawei.for example system x anonymized for confidential issues which is one of the popular products at huawei generatesabout tb of log data daily.
storage of logs at such a scalehas become a challenging task.
most of the logs need to bestored for a long time usually years considering the product lifecycle of system x is about two years.
in particular historical logs are kept for the following practical tasks root cause analysis identification of similar failures or faultsthat happened before failure categorization categorization of similar failures for the development planning of nextsoftware version and automated log analysis tools acting as experimental data for the research and development ofautomated log analysis tools.
log storage currently takes up over pbs of disk space in a cluster which indicates a huge cost of power consumption.meanwhile log data is regularly replicated to one or two datacenters according to different importance levels at differentlocations for disaster tolerance.
this results in another typeof expensive cost i.e.
bandwidth consumption.
reducing thestorage cost of log data has become a main objective ofthe product team because their storage budget is limited butthe number of products is growing.
with close collaborationwith the product team we have recently transferred logzipinto system x. logzip is deployed on a core linuxserver with ubuntu .
installed to compress raw log files.when logzip is parallelized with processors it achievescomparable compression time with the traditional gzip method.the product team accepts the performance of logzip sincemost of the old logs are rarely accessed and can be archived at one time.
yet the use of logzip successfully reduces the size of logs saving about of space compared to the gzip algorithm that is previously used.
it not only reduces the cost of log storage but also cuts the cost on network consumption during replication.
this has become a successful use case oflogzip.
vii.
r ela ted work log management for se.
logs are critical runtime information recorded by developers which are widely analyzedfor all sorts of tasks.
log analysis is conducted for various targets such as code testing problem identification user behavior analysis security monitoring etc.
most of these tasks use data mining models to extractcritical features or patterns from a large volume of softwarelogs.
therefore we believe our work on log compression couldbenefit log data storage and save the cost of dumping thelarge volume of logs.
log parsing.
is generally utilized as the first step of downstream tasks.
in recent years variouslog parsers have been proposed.
slct is the first workon automated log parsing based on token frequency to thebest of our knowledge.
then data mining based methods lke iplom spell drain areproposed.
lke and iplom are offline parsers and shisoand drain could parse online in a streaming manner.
theseparsers are evaluated and compared in the benchmark by zhuet al.
.
the parsers could extract hidden structures but theytake all logs as input thus are not efficient compared with theproposed ise.
text compression.
file compression algorithms have been developed for years and some of them are utilizedin compressing tools gzip lzma bzip2 .
these general toolsare commonly used and achieve satisfactory cr.
to furtherimprove cr specific for text files lempel ziv welch lzw based methods are widely studied .
oswald et al.
explore text compression in the perspective of data mining.
they enhance huffman encoding by frequent pattern mining.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
since log data as a kind of text data is more structured we propose logzip to use the information to facilitate compression.
log compression.
due to the inherent structure of log data it s possible to compress log files with higher cr thus some log specific algorithms are proposed.
clc and dslc utilize prior knowledge and manual pre treatment to compress log files.
logarchive adaptively distributes log messages to different buckets and track most recent log messages in each bucket with a sliding window.
finally buckets are compressed separately in parallel.
cowic introduced by lin et al.
divides log messages into fields and manually build a model for each field but cowic targets query efficiency instead of high cr.
mlc explores data redundancy of log file and divides logs into buckets based on similarities then apply existing compression tools as we do in logzip.
these algorithms explore hidden structures of logs to compress logs which could outperform general compression tools.
but they are limited by the trade off between high cr and efficiency.
compared with them we extract hidden structures via ise which is efficient and highly parallel.
as a result logzip achieves high cr without loss of efficiency.
log analytics powered by ai logpai .
logpai is a research project originating from cuhk.
the ultimate goal of logpai is to build an open source ai platform for automated log analysis.
towards this goal we have built open benchmarks over a set of research work as well as release open datasets and tools for log analysis research.
in particular loghub hosts a large collection of system log datasets.
logparser provides a toolkit and benchmarks for automated log parsing .
loglizer implements a number of machine learning based log analysis techniques for automated anomaly detection .
logadvisor is a framework for determining optimal logging points in source code .
in this work logzip provides an tool for effective log compression.
with both datasets and source code available we hope that our logpai project could benefit both researchers and practitioners in the community.
viii.
c onclusion in this paper we propose logzip a log compression approach that largely reduces the operational cost for log storage.
logzip extracts and utilizes the inherent structures of logs via a novel iterative clustering technique.
logzip is designed to be seamlessly integrated with the existing data compression utilities e.g.
gzip .
furthermore the semi structure intermediate representations generated by logzip can be directly used for a variety of downstream log mining tasks e.g.
anomaly detection .
extensive experiments on five real world system log datasets have been conducted to evaluate the effectiveness of logzip.
the experimental results show that logzip significantly enhances the compression ratios over three widely used data compression tools and also outperforms the state of the art log compression approaches.
moreover logzip is highly parallel and achieves comparable efficiency as gzip on a core machine.
we believe that our work together withthe open source logzip tool could benefit engineering teams facing the same problem.
ix.
a cknowledgement the work described in this paper was supported by the national key research and development program 2016yfb1000101 the national natural science foundation of china the research grants council of the hong kong special administrative region china no.
cuhk of the general research fund and microsoft research asia microsoft research asia collaborative research award .