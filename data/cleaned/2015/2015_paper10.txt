can testedness be effectively measured?
iftekhar ahmed oregon state university ahmedi oregonstate.edurahul gopinath oregon state university gopinatr oregonstate.educaius brindescu oregon state university brindesc oregonstate.edu alex groce oregon state university agroce gmail.comcarlos jensen oregon state university cjensen oregonstate.edu abstract among the major questions that a practicing tester faces are deciding where to focus additional testing e ort and deciding when to stop testing.
test the least tested code and stop when all code is well tested is a reasonable answer.
many measures of testedness have been proposed unfortunately we do not know whether these are truly e ective.
in this paper we propose a novel evaluation of two of the most important and widely used measures of test suite quality.
the rst measure is statement coverage the simplest and best known code coverage measure.
the second measure is mutation score a supposedly more powerful though expensive measure.
we evaluate these measures using the actual criteria of interest if a program element is by these measures well tested at a given point in time it should require fewer future bug xes than a poorly tested element.
if not then it seems likely that we are not e ectively measuring testedness.
using a large number of open source java programs from github and apache we show that both statement coverage and mutation score have only a weak negative correlation with bug xes.
despite the lack of strong correlation there arestatistically and practically signi cant di erences between program elements for various binary criteria.
program elements other than classes covered by any test case see about half as many bug xes as those not covered and a similar line can be drawn for mutation score thresholds.
our results have important implications for both software engineering practice and research evaluation.
ccs concepts software and its engineering !empirical software validation keywords test suite evaluation coverage criteria mutation testing statistical analysis permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
acm.
isbn .
.
.
introduction the quality of software artifacts is one of the key concerns for software practitioners and is typically measured through e ective testing.
while it is widely held that you cannot test quality into a product you canuse testing to detect that the software under test sut has faults and to estimate its likely overall quality.
moreover while testing itself does not produce quality it leads to the discovery of faults.
when these faults are corrected software quality improves.
testing software poses questions.
first how much testing is needed?
has enough testing been done?
second where should future test e orts be applied in a partially tested program?
the typical approach to answering these questions is to measure the quality of the test suite not the sut.
numerous measures primarily focused on code coverage have been proposed and numerous organizations set testing requirements in terms of coverage levels .
both code coverage and mutation score measure the testedness of an sut using the dynamic results of executing a test suite1.
however it is not established that using such testing requirements or suite quality measures in general translates into e ective an practice for producing better software.
while test driven development in particular has pushed testing to new prominence practicing software programmers often balk at having to satisfy what they see as arbitrary coverage requirements.
some go so far as to suggest that code coverage is a useless target measure .
some studies seem to support this conclusion at least in part .
the utility of testing itself has even come under attack3.
our aim in this paper is to place the evaluation of test suites and thus decision making in testing on a rmer footing using a measure that translates directly into practical terms.
there is of course no end to studies measuring the e ectiveness of test suite evaluation techniques.
however these studies tend to either cover only a few subject programs or faults not use real faults not measure what 1most studies consider coverage as measuring the testedness of the entire sut for a given suite but it is also obviously reasonable to project this concept onto individual program elements and measure how well tested each is and most practical applications of coverage assume this usage as well.
2for examples see testcoverage.html and code coverage is a useless target measure .
3for examples see magazines its not about the unit tests and unit testing is waste.pdfdevelopers directly care about or assume the validity of mutation testing which is itself a relatively unproven method for evaluating a test suite.
the methodology of such studies also often involves using tool generated or randomly chosen subset based test suites to measure correlation.
such suites may not resemble real world test suites and thus be of little relevance to most developer practice.
we propose a simpler and more direct method of evaluation that eliminates some of the concerns mentioned above.
it can be argued that a correlation between measure of suite quality and fault detection is meaningless if the faults detected are unimportant trivial or arti cial.
as a recent paper evaluating the ability of automated unit test generators to detect real faults phrased it just because a test suite... does not necessarily mean that it will nd the faults that matter .
it is very hard to argue that a bug that has been discovered and xed did not matter.
however xing bugs is di cult and resource intensive requiring developers and possibly testers to devote time to implementing a correction and hopefully validating it .
in many cases the bug was detected because it caused problems for users.
there is evidence that problems in code such as those identi ed by code smells that do not have signi cant immediate consequences are often never corrected even at the price of design degradation .
bug xes usually indicate important defects as developers thought these problems worth addressing.
the most practical goal of testing therefore is to prevent future bug xes by detecting faults before release avoiding impact on users and usually lowering development cost.
we should then evaluate test suite quality measures by a simple process does a higher measure of testedness for a program element in our work a statement block method or class correlate with a smaller number of future bug xes?
by avoiding whole project measures and focusing on individual program elements we avoid the confounding e ects of test suite size .
while a large test suite can produce higher coverage and detect more faults even if coverage and fault detection are not themselves directly related it cannot at least in any way we can imagine cause statements that are covered to have fewer faults than those that are not covered unless coverage itself is meaningful .
similarly using individual program entities as the basis of analysis mitigates some of the possible e ects of e.g.
test suites with good assertions also having better coverage.
this can cause a test suite with high coverage to perform better than a test suite without high coverage on average but it cannot so far as we can see plausibly result in covered entities having fewer bug xes than entities not covered unless coverage itselfmatters.
the core argument for our analysis is as follows if a particular program element is fully tested to conform to its speci cation then that program element should have no bugxes applied until the element ceases to exist as a result of a non bug x modi cation e.g.
adding new functionality potentially invalidating the old speci cation .
similarly a program element that is not tested at all should on average have a higher chance of seeing future bug xes applied for the simple reason that a fault had no chance of being caught through testing.
this on its own should result in a strong negative correlation between testedness and future bug xes if our fundamental assumption about the utility of testing is correct and our measure of testedness is e ective.in general a well tested program element should require fewer bug xes than a less well tested program element.
in this paper we assume and later based on empirical data argue that testing itself is bene cial.
we therefore primarily aim to evaluate the measurement of test suite quality testedness.
we focus on two important widely studied measures of suite quality.
first statement coverage is the simplest most widely used and easiest to understand coverage measure and has some support as an e ective measure of test suite quality in recent work .
second mutation score is not only commonly advocated as the best method for evaluating suite quality but is essential to most other studies of coverage method e ectiveness .
we evaluate these test suite evaluation methods empirically using a large representative set of real world programs real world test suites and bug xes and nd that while there is a small but statistically signi cant negative correlation between our testedness measures and future bug xes for program elements the e ect is so small as to be practically insigni cant.
there is very little useful continuous relationship between measures of testedness and actual tendency to not have bugs detected and xed and while it is reasonable to bet that a more tested element will have fewer faults the size of the e ect is very small.
however we do nd that there isa consistent and practically as well as statistically signi cant di erence in the mean number of bug xes for code if we apply selected binary measures of well testedness based on coverage or mutation score.
for example program elements with at least a mutation score see on average only about half as many future big xes normalized4 as program elements with a lower mutation score .
one intuitively appealing explanation for the low correlation of testedness to bug xes is that even if poorly tested unimportant pieces of code are likely to see few future bug xes.
if very few users execute a program element or if its e ects have very limited impact then the bug will likely not be xed even if reported .
however the problem of varying program element importance is unlikely to be the root cause for the lack of a useful continuous correlation for suite evaluation measures.
if it were we would expect the e ects of importance to also prevent binary testedness criteria based on mere coverage from predicting future bug xes since no one will bother to test program elements that are unlikely to ever exhibit important bugs .
however like program elements with mutation score program elements that are not covered are also likely to see nearly twice as many future bug xes5.
nonetheless perhaps a suite quality measure should reect the importance of program elements.
however forcing developers to annotate code by its importance is impractical we need a static measure of importance.
one approach is to say that complex elements are more likely to be important since developing complex code with many operators 4by normalized bug xes we mean bug xes per statement line for elements larger than a single statement or line unless we indicate otherwise we always normalize bug xes when required.
5we only demonstrate this result for statements and methods there were too few classes that were not covered by any tests in our data to show a signi cant relationship.and conditionals but low importance is an unwise use of development resources.
in this case in addition to its other advantages mutation testing may help take importance of code into account in that complex program elements produce more mutants than simple elements e.g.
a simple logging statement will seldom perform any calculations and so often only produce a single statement deletion mutant .
we therefore also measure whether the number of mutants as a measure of code complexity predicts the number of bug xes applied to a program element and whether the number of mutants predicts the mutation score for an element.
both e ects are signi cant but small.
surprisingly more complex code sees slightly fewer bug xes than simple code.
as might be expected if complexity is associated with importance more complex code is also slightly more tested according to mutation score.
both e ects are too weak to be of much practical value however.
our ndings with respect to correlation of testedness measures and bug xes are potentially troubling for the research community.
software testing researchers often use a di erence of a few percentage points in mutation score or a coverage measure as a means to assert that one test generation or selection technique is superior to another.
however our data shows that relying on a few percentage points is dangerous as such small di erences may not indicate real impact in terms of defects that are worth xing.
on the other hand our data seems to support the types of arbitrary adequacy criteria often imposed by managers or governments if not the precise values used .
indeed our data suggests that while a continuous ranking of testedness for program elements is not currently possible using various empiricallyvalidated strata of testedness not covered covered but with poor mutation score covered with high mutation score may provide a simple practical way to direct testing e orts.
the contributions of this paper are a novel approach to examining the utility of test suite quality measures that is based on direct practical consequences of testing.
analysis of relationships between bug xes test suite quality testedness measures and code complexity and importance metrics for sampled projects from github and apache.
evidence that there is small negative correlation between the number of mutants normalized and the number of future bug xes to a program element indicating that complexity alone does not predict importance well in fact more complex program elements seem to be changed less often than simple ones.
however this may partly be due to the fact that more complex elements are also somewhat more well tested in terms of mutation score .
evidence that the negative correlation between testedness by our measures and number of future normalized bug xes is statistically signi cant but far too small to have much practical impact.
evidence that well chosen adequacy criteria e.g.
is the mutation score above can be used to predict future normalized bug xes in a practical way leading to di erences of a factor of two in expected future bugs and can serve to distinguish untested poorly tested and well tested elements of an sut.
.
related work ours is not the rst study to attempt to evaluate measures of testedness .
researchers have often attempted to prove that mutation score is well correlated with real world faults.
demillo et al.
empirically investigated the representativeness of mutations as proxies for real faults.
they examined the errors in t ex and found that were simple faults while the rest were complex errors.
daran et al.
investigated the representativeness of mutations empirically using error traces.
they studied the real faults found in a program developed by a student and rstorder mutants.
they found that of the mutants were similar to real faults.
another important study by andrews et al.
investigated the ease of detecting a fault both real faults and hand seeded faults and compared it to the ease of detecting faults introduced by mutation operators.
the ease was calculated as the percentage of test cases that killed each mutant.
their conclusion was that the ease of detection of mutants was similar to that of real faults.
however they based this conclusion on the result from a single program which makes it unconvincing.
further their entire test set was eight c programs which makes the statistical inference drawn liable to type i errors.
we also note that the programs and seeded faults were originally from hutchins et al.
who chose programs that were subject to certain speci cations of understandability and the seeded faults were selected such that they were neither too easy nor too di cult to detect.
in fact the study eliminated faults for being either too easy or too hard to detect ending up with just faults.
this is clearly not an unbiased selection and cannot really tell us anything about the ease of detection of hand seeded faults in general because the criteria of selection itself is confounding .
a follow up study using a large number of test suites from a single program space.c found that the mutation detection ratio and fault detection ratio are related linearly with similar results for other coverage criteria .
to .
.
linear regression on the mutation kill ratio and fault detection ratio showed a high correlation .
.
the problems with some of these studies were highlighted in the work of namin et al.
who used the same set of c programs as andrews et al.
but combined them with analysis of four more java classes from the jdk.
this study used a di erent set of mutation operators and fault seeding by student programmers for the java programs.
their analysis concluded that we have to be careful when using mutation analysis as a stand in for real faults.
the programming language the kind of mutation operators used and even the test suite size all have an impact on the relation between mutations introduced by mutation analysis and real faults.
in fact using a di erent mutation operator set they found that there is only a weak correlation between real faults and mutations.
however their study was constrained by the paucity of real faults which were only available for a single c program also used in andrews et al.
.
thus they were unable to judge the ease of detection of real faults in java programs.
moreover the students who seeded the faults had knowledge of mutation analysis which may have biased the seeded faults thus resulting in high correlation between seeded faults and mutants .
finally the manually seeded faults in c programs originally introduced by hutchins et al.
were again confounded bya selection criteria which eliminated the majority of faults as being either too easy or too hard to detect.
just et al.
using real faults from projects showed that adding more fault detecting tests to a test suite led to the mutation score increasing more often than either branch or statement coverage and mutation score was more positively correlated with fault detection than either of the other measures.
multiple studies provide evidence that mutation analysis subsumes di erent coverage measures and it is on this basis that mutation score is often regarded as the gold standard for test suite quality measures.
one metric that is commonly used to measure the adequacy of testing is code coverage that is a measure of the set of program elements or code paths that are executed by a set of tests.
a large body of work considers the relationship between coverage criteria and fault detection.
mockus et al.
found that increased coverage leads to a reduction in the number of post release defects but increases the amount of test e ort.
gligoric et al.
used the same statistical approach as our paper measuring both kendall andr2 to examine correlations for realistically non adequate suites.
gligoric et al.
found that branch coverage does the best job overall of predicting the best suite for a given sut but that acyclic intra procedural path coverage is highly competitive and may better address the issue of ties which is important in their research method comparison context.
inozemtseva et al.
investigated the relationship of various coverage measures and mutation score for di erent random subsets of test suites.
they found that when test suite size is controlled for only low to moderate correlation is present between coverage and e ectiveness.
this conclusion held for all the coverage measures used.
frankl and weiss performed a comparison of branch coverage and def use coverage showing that def use is more e ective than branch coverage for fault detection and there is stronger correlation to fault detection for def use than branch coverage.
namin and andrews showed that fault detection ratio non linearly correlated well with block coverage decision coverage and two di erent dataow criteria.
their research suggested that test suite size was a signi cant factor in the model.
wei et al.
examined branch coverage as a quality measure for suites for ei el classes showing that for randomly generated suites branch coverage behavior was consistent across many runs while fault detection varied widely.
in their experiments early in random testing when branch coverage rose rapidly current branch coverage had high correlation to fault detection but branch coverage eventually saturated while fault detection continued to increase the correlation at this point became very weak.
o ut et al.
showed that mutation coverage subsumes many other coverage criteria including the basic six proposed by myers .
gupta et al.
compared the e ectiveness and e ciency of block coverage branch coverage and condition coverage with mutation kill of adequate test suites as their evaluation metric.
they found that branch coverage adequacy was more e ective killed more mutants than block coverage in all cases and condition coverage was better than branch coverage for methods having composite conditional statements.
the reverse however was true when considering the e ciency of suites average number of test cases required to detect a fault .
li et al.
compared four di erent criteria mutation edge pair all uses and prime path and showed that mutation adequate testing was able to detect the most hand seeded faults while other criteria performed similarly to each other in the range of detection .
similarly mutation coverage required the fewest test cases to satisfy the adequacy criteria while prime path coverage required the most.
therefore while there are no compellingly large scale studies of many suts selected in a non biased way to support the e ectiveness of mutation testing it is at least highly plausible as a better standard than other criteria.
cai et al.
investigated correlations between coverage criteria under di erent testing pro les whole test set functional test random test normal test and exceptional test.
they investigated block coverage decision coverage c use and p use criteria.
curiously they found that the relationship between block coverage and mutant kills was not always positive.
block coverage and mutant kills had a correlation ofr2 when considering the whole test suite but as low as .
for normal testing and as high as .
for exceptional testing.
the correlation between decision coverage and mutation kills was higher than statement coverage for the whole test suite .
ranging from normal test .
to exceptional test .
.
frankl et al.
compared the e ectiveness of mutation testing with all uses coverage and found that at the highest coverage levels mutation testing was more e ective.
kakarla and inozemtseva demonstrated a linear relationship between mutation detection ratio and coverage for individual programs.
inozemtseva s study used machine learning techniques to come up with a regression relation and found that e ectiveness is dependent on the number of methods in a test suite with a correlation coe cient in the range r .
the study also found a moderate to high correlation in the range between e ectiveness and block coverage when test suite size was ignored which reduced when test suite size was accounted for.
kakarla found that statement coverage was correlated to mutation coverage in the range of r and gopinath et al.
found that statement out of branch and path coverages best correlated with mutation score and hence may best predict defect density in a study that compared suites and mutation scores across projects rather than using multiple suites for the same project.
the study by tengeri et al.
provided a simple essentially non statistical assessment of how statement coverage mutation score and reducibility predicted project defect densities for four open source projects using a limited set of mutation operators.
none of these studies to our knowledge adopted the method used in this paper where rather than investigate faults and their detection we look at whether being well tested has predictive power with respect to future defects6.
most also consider a smaller less representative at least of open source projects set of programs and the majority are based on programs chosen opportunistically rather than by our more principled sampling approach.
the programs used are often small but well studied benchmarks such as the siemens sir suite partly for purposes of comparison to earlier papers and partly due to the lack of easily available 6it is remotely possible that tengeri et al.
are using a similar method but this is not clear from their description and the reasoning behind our approach is not elaborated in their work.
cloctestsmuscore .
.
.
.
110100commitsfigure cloc vs. tests for our projects.
realistic projects with test suites and defects before the advent of very large open source repositories.
unfortunately as noted by arcuri and briand not at least attempting to randomize selection of programs to study can greatly reduce the generalizability of results .
.
methodology our goal was to evaluate various approaches to assessing the testedness of a program or program element using test suite quality measures.
.
collecting the subjects for our empirical evaluation we tried to ensure that the programs chosen o ered a reasonably unbiased representation of modern software.
we also tried to reduce the number of variables that can contribute to random noise during evaluation.
with these goals in mind we chose a sample7of java projects from github and the apache software foundation .
all projects selected used the popular maven build system.
we randomly selected projects.
from these we eliminated aggregate projects that were di cult to analyze leaving projects of which only had test suites.
out of these remained after eliminating projects that did not compile for reasons such as unavailable dependencies or compilation errors due to syntax or bad con gurations .
next the projects that did not pass their own test suites were eliminated as mutation analysis requires a passing test suite.
finally we eliminated projects our ast walker could not handle.
this resulted in projects selected.
the distribution of project size vs. test suite size and the corresponding mutation score is given in figure .
.
mutant generation in the next phase of our analysis we used pit for our mutation analysis.
pit has been used in multiple previous studies .
we extended pit to provide the full matrix of test failures over mutants and tests.
mutants can basically be divided into three groups based on their runtime behavior not covered killed and live mutants.
we used this basic categorization in our analysis.
7github allows us to access only a subset of projects using their search api.
we believe that the results returned by github search would not be dependent on their test suites and hence should not confound our results.
.
tracking program elements we started our investigation from an arbitrarily determined recent but not too recent point in time deemed the epoch december .
this was done to provide a point from which testedness mutation score and statement coverage could be calculated and with respect to which bug xes could be considered to be in the future .
for the source code and test suite at epoch we computed mutation score and statement coverage for each statement block method and class in each project.
in order to determine when a program element statement block method or class was changed and track its history we used the gumtree di erencing algorithm .
for each element of interest we considered it changed if the corresponding ast node was changed or had any children that were added deleted or modi ed.
the algorithm maps the correspondence between nodes in two di erent trees which allowed us to accurately track the history of the program elements.
using ast di erencing gives us three advantages over simple line based di erencing.
the rst is that the algorithm ignores any whitespace changes.
second we are able to track a node even if its position in the le changes e.g.
because lines have been added or deleted before our node of interest .
third we are able to track nodes across refactorings as long as the node stays in the same le.
for example we can track a node that has been moved because of an extract method refactoring.
when considering which statements to track we used the version of the source code at epoch to determine which ast node resided at that particular line.
we ltered only the commits that touch the le of interest.
we then tracked that ast node forward in time taking note of the commits that changed that particular node.
for java it is possible for multiple statements to be in the same line for example a local variable declaration statement inside an if statement .
in this case we considered the innermost statement as this gives the most precise results.
.
classifying commits in order to answer our research questions we needed to categorize the code commits.
for each program element we computed the number of commits that touched that element starting from the epoch.
for our purpose code commits can be broadly grouped into one of two categories bug xes and improvements modifying existing code and other commits that introduced new features or functionality adding new code or commits that were related to documentation test code or other concerns.
two key problems are that it is not always trivial to determine which category a commit falls under and that larger projects see a huge amount of activity.
manual classi cation of all commits was therefore not an option and we decided to use machine learning techniques for this purpose rather than limit the statistical power of our study especially as arbitrarily dropping the most active subjects would clearly potentially introduce a large bias into our results .
.
.
manual classification of fix inducing changes in order to build a classi er for bug xing commits we randomly sampled commits and manually labeled x inducing commits.
some keywords indicating bug xes were fix bug and resolves along with their derivatives.
we should men .
.
.
.
.
mutation scorebug fix commitsfigure mutation score vs. bug x commits for covered lines.
.
.
.
.
.
.
.
.
.
.
.
mutation scorebug fix commitsfigure mutation score vs. bug x commits for covered blocks.
precision recall f1.score support bug x .
.
.
.
other .
.
.
.
table naive bayes classi er details tion that not all bug xing commit messages include the words bugor x indeed commit messages are written by the initial contributor of a patch and there are few guidelines as to their contents.
a similar observation was made by bird et al.
who performed an empirical study showing that bias could be introduced due to missing linkages between commits and bugs.
improvements were manually identi ed based on the following keywords cleanup optimize and simplify or their derivatives.
commits were placed into the other category if they had the keywords add orintroduce .
the number of lines modi ed was also compared with the lines added.
those commits with more lines added than modi ed were considered more likely to be associated with new features and were placed in the other category.
anything that did not t into this pattern was also marked as other .
we manually classi ed a set of commits.
.
.
training the commit classifier we used the set of manually classi ed commits as the training data for the machine learning classi ers.
two evaluators worked independently to classify the commits.
their datasets had a overlap which we used to calculate the inter rater reliability.
this gave us a cohen s kappa of .
.
in our training dataset the portion of bug xes was .
with .
of the commits assigned to the other category.
we trained a naive bayes nb classi er and a support vector machine svm for automatically classifying the commits using the scikit platform.
we applied the classiers to the training data with fold cross validation.
our goal was to achieve high precision and recall so we used the f1 score to measure and compare the performance of the models.
the f1 score considers precision and recall by taking their harmonic mean.
the nb classi er outperformed the svm.
tian et al.
suggested that for keyword based classi cation the f1score is usually around .
which also happened in our case.
we used the classi cation identied by the nb classi er to classify commits.
table has the quality indicator characteristics of the nb classi er.
while our classi er is far from perfect it is comparable to good classi ers for this purpose in the literature over a larger set of projects and we believe it is likely that any biases do not have confounding interactions with the goals ofour project.
that is while we may only analyze about of bug xes it would be surprising if the missed bug xes relate in some systematic way to the dynamic testedness measures of program elements given that the classi er only sees code commits.
since our analysis only relies on relative counts of bug xes for elements so long as we do not systematically undercount bug xes for only some elements our results should be valid.
the bug xes associated with each program element in our analysis are based on the classi er results in a simple way.
for each element we count commits that a ect that element that are classi ed as bug x up to the rst commit that is classi ed as other.
this is because once an element has had a change that is not a bug x it is often no longer valid to assume tests at the epoch apply to that element or that it even still exists with the same functionality.
however so long as only bug xes are applied we assume the tests still apply to the program element so all bug xes count as missed by the tests at epoch.
note that our classi er for other commits is highly e ective.
.
analysis we analyze the impact of testedness on program element bug xes using two measurements mutation score and statement coverage.
for mutation score we analyze the score of each statement block method and class in increasing lexical scope.
since statement coverage is already at the statement level we investigate the statement coverage of each block method and class in increasing lexical scope.
.
correlation results we answer this question in increasing scope from statement smallest block method and then class.
in each scope we evaluate how the degree of adequacy in both mutation score and statement coverage a ects the total number of bug x commits.
.
.
mutation score the correlation between number of bug xes per statement and mutation score is given in table .
for statements and methods there is a statistically signi cant small negative linear correlation between number of bug xes per statement and mutation score.
a similar e ect is observed with kendall bcorrelation where a small but statistically signi cant negative correlation is observed for statements blocks and methods but not for classes where the correlation was surprisingly a very weak but signi .
.
.
.
.
.
.
.
.
.
.
mutation scorebug fix commitsfigure mutation score vs. bug x commits for covered methods.
.
.
.
.
.
.
.
.
.
.
.
mutation scorebug fix commitsfigure mutation score vs. bug x commits for covered classes.
a r2 mean low high p statements .
.
.
.
blocks .
.
.
.
methods .
.
.
.
classes .
.
.
.
b kendall b mean p .
.
.
.
.
.
.
.
table correlation between total number of bug xes per line and mutation score a r2 mean low high p statements .
.
.
.
blocks .
.
.
.
methods .
.
.
.
classes .
.
.
.
b kendall b mean p .
.
.
.
.
.
.
.
table correlation between total number of bug xes per line and statement coverage cant positive correlation.
the plot of mutation score vs. normalized bug xes for statements is given in figure for blocks in figure for methods in figure and for classes in figure .
.
.
statement coverage the correlation between number of bug xes per statement and statement coverage is given in table .
for statements blocks methods and classes there was a small but statistically signi cant negative linear correlation between coverage and bug xing commits.
a similar small but statistically signi cant negative correlation is also observed using kendall b. these correlations for mutant score and for statement coverage are much lower than those seen in recent studies showing good correlation between coverage metrics and mutation scores these studies are measuring a di erent property but in some sense aiming for similarly strong correlations .
these correlations are not so small as to be completely devoid of value but they do make the use of these measures dubious when comparing program elements or test suites with only small testedness di erences.
unfortunately this is a common practice in the evaluation of software testing experiments.
worse still these results might be thought to suggest that testedness cannot be e ectively measured leaving the practicing tester without useful guidance.covered uncovered p statement .
.
.
block .
.
.
method .
.
.
class .
.
.
table di erence in bug xes between covered and uncovered program elements .
binary testedness is it covered?
however using testedness as a continuous valuation where we expect slightly more tested program elements to have fewer bug xes is not the only way to make use of testedness.
instead of trying to separate very similarly tested elements we could simply draw a line between tested and not tested program elements.
for example common sense suggests that if testing is useful at all then code that is not covered should probably have more bug xes that code that has at least some test covering it.
this rationale is the intuition behind ideas like getting to code coverage though it does not justify any particular target value.
code that isn t executed in tests is surely less tested than code that executes in even very poor tests since even very badly designed tests with a weak oracle may catch crashes uncaught exceptions and in nite loops for example .
we compared the mean number of bug xes for covered vs. uncovered program elements using a t test.
the results are shown in table .
by covered element we mean a program element which has at least a single statement exercised by some test case.
while this is a reasonable binary distinction up the method level a class with only a single statement covered may not be much more tested than a class that does not have any statements covered.
this may account for the di erence seen for classes in table .
we also note that there is insu cient data for statistical signi cance in classes most classes are covered by at least some test .
.
binary testedness mutation score and coverage thresholds while measuring testedness based on mutation score or statement coverage as a continuous value of limited value we can do much better than just drawing a meaningful dividing line between covered and not covered program elements.
we can instead evaluate whether the mean number of bugxes di ers signi cantly when the tests reach a given adequacy threshold.
table and table tabulate the mean number of normalized bug x commits per line for both a .
p statements .
.
.
blocks .
.
.
methods .
.
.
classes .
.
.
b .
p .
.
.
.
.
.
.
.
.
.
.
.
c .
p .
.
.
.
.
.
.
.
.
.
.
.
d .
p .
.
.
.
.
.
.
.
.
.
.
.
table mutation score thresholds a .
p statements .
.
.
blocks .
.
.
methods .
.
.
classes .
.
.
b .
p .
.
.
.
.
.
.
.
.
.
.
.
c .
p .
.
.
.
.
.
.
.
.
.
.
.
d .
p .
.
.
.
.
.
.
.
.
.
.
.
table statement coverage score thresholds above and below the thresholds f0 0gand f0 0g.
we nd that there is a statistically and practically signi cant di erence between the mean number of bug xes for both measures at all thresholds selected though with classes perfect statement coverage strangely becomes a predictor of more faults .
note that for individual statements all thresholds based on statement coverage are equivalent coverage is always or .
table shows mutant threshold results if we rst remove all program entities that are not covered.
this has little impact on the ability of thresholds to predict bug xes.
.
complexity and change we also compare the number of mutants normalized by the size of the program element dividing by the number of lines to the number of post epoch bug xes for that element.
statements comparing the number of bug xes to the number of mutants per statement we nd that the con dence interval is given by f .013204gatp .
methods comparing the number of bug xes to the number of mutants per method we nd that the con dence interval is given by f 048715gatp .
classes comparing the number of bug xes to the number of mutants per class we nd that the con dence interval is given by f 000863gatp .
summary most of the results are statistically signi cant.
we also observe that there is a weak correlation between the number of mutants normalized and the number of bug xes.
more complex code as measured by number of mutations has slightly fewer bug xes but the correlation is even weaker than between testedness measures and bugxes.
however the di erence in correlation is not very large so another way to interpret this is that as a continuous measure simple number of mutants normalized is only slightly worse as a predictor of bug xes than testedness.
however unlike testedness measures the number of mutants does not provide a useful binary predictor for bug xes.
binary splits based on a threshold using the mean number of normalized mutants .
do not produce signi cantly di erent populations.
setting a threshold of or more normalized mutants does produce signi cant di erences p value .
but the means are very similar e.g.
.
bug xes for less com plex statements vs. .
bug xes for statements with more mutants.
.
complexity and testedness statements comparing the normalized number of mutants to the mutation score per statement we nd that the con dence interval is given by f0.
.
gat p .
methods comparing the normalized number of mutants to the mutation score per method we nd that the con dence interval is given by f0.
.
gatp .
classes comparing the normalized number of mutants to themutation score per class we nd that the con dence interval is given by f .046223gatp .
summary we found that at the statement level only there is a statistically signi cant but very weak correlation between the number of mutants normalized and the mutation score.
more complex statements are very slightly more well tested.
.
discussion this paper presents a novel approach to determining if what we call testedness measures actually help predict how many defects that escaped testing will be found and xed in parts of a program.
our empirical results have some potentially important consequences for testing research and practice.
.
the danger of relying on small testedness differences first there is only a weak correlation between either statement coverage or mutation score and future bug xes.
this indirectly suggests that research e orts using coverage or mutants to evaluate test suite selection generation or reduction algorithms may draw unwarranted conclusions from small signi cant di erences in these measures.
in particular it may suggest that using mutation to evaluate testing experiments can potentially fail to re ect the ability of systems to detect the types of faults that are detected by practitioners and worth correcting in real life.
given that the literature supporting the value of code coverage as a predictor of fault detection mostly relies on the ability of muta a .
p statements .
.
.
blocks .
.
.
methods .
.
.
classes .
.
.
b .
p .
.
.
.
.
.
.
.
.
.
.
.
c .
p .
.
.
.
.
.
.
.
.
.
.
.
d .
p .
.
.
.
.
.
.
.
.
.
.
.
table mutation score thresholds with uncovered program elements ltered out tion testing to re ect real fault detection and that mutation testing s e ectiveness is validated by only a small number of studies none of which present overwhelming evidence over a large number of programs we strongly suggest that testing experiments whenever possible should rely on the use of some real faults in addition to coverage or mutation score based evaluations.
in some contexts where detecting all possible faults is the goal e.g.
safety critical systems and the oracle for correctness is known to be extremely good mutation based analyses may be justi ed but even in those cases data based on real faults would be ideal.
.
practical application of thresholds on the other hand our results show that numerous simple percentage thresholds for statement coverage and mutation score can in a statistically signi cant way predict the number of bug xes with mean di erences between populations of about 2x .
this suggests a simple method for prioritizing testing targets in a program.
the entities with the highest bug x counts were unsurprisingly those not even covered by any tests.
as a rst priority covering uncovered program elements is likely to be the most rewarding way to improve testedness since these elements can be expected to have the most potential undetected bugs that will be revealed in the near future.
surviving mutants of entities with low mutation scores can then be used to guide further testing.
one obvious question is which threshold should be used since many thresholds seem e ective?
our data shows that it really does not matter much the signi cance and even average bug xes are not radically di erent for di erent thresholds.
the simplest answer is to start with low thresholds keep improving testing until there are no remaining interesting elements below the current threshold then move on to a higher threshold.
setting a particular threshold for projectlevel testing is not supported by our data however as there is no clearly best dividing line only a number of ways to de ne less tested and more tested elements most of which equate to more bug xes for less tested elements.
.
complexity bug fixes and testedness there does not seem to be any very strong or interesting relationship between complexity as measured by number of mutants and bug xes or between complexity and testedness.
more complex code is very slightly less xed perhaps because it is very slightly more tested.
the main take away from the complexity analysis is that the number of mutants is almost as good a predictor of lack of bug xes as testedness if used as a simple correlation but it does not support useful binary distinctions in likely bug xes.
.
testing is likely effective one nal point to note is that our data provides fairly strong support for the idea that testing is e ective in forcingquality improvements in code .
our measures of testedness are essentially based purely on the dynamic properties of a test suite not on static properties of program elements the number of mutants for an entity depends on static properties but all statements with any mutants can achieve or fail to achieve a score of any particular threshold .
this means that without using the static properties of code the degree to which code is exercised in a test suite can often be used to predict which of two entities will turn out to require more bug xes.
as far as we can determine there are only a few potential causes for this ability to use the dynamics of a test suite to predict bug xes .
some unknown property not related to code quality results in both a tendency to write tests that cover code and in fewer bug xes for that code.
.
a known property results in both a tendency to write tests that cover code and in fewer bug xes for that code namely good developers write tests for their already more correct code.
testing itself is more a sign of good code than a cause of good code.
.
tests covering code often detect bugs and developers x the bugs so the code has fewer bugs to x. the rst possibility is in our opinion unlikely it is di cult to imagine such an unknown factor.
some obvious candidate factors do not really bear up on examination.
for example perhaps code with many bug xes is new code and so has not yet had tests written for it.
if the act of writing tests for the new code makes it less buggy however then testing is in fact e ective.
moreover the predictive power of mutation score being over a threshold is present even if we restrict our domain to entities that have at least one covering test.
new code might be expected to be completely untested removing most truly new no tests code from this population.
the second possibility is more plausible and may well be true to some extent.
the third possibility seems most plausible and we believe is likely to be the main cause of the observed e ects.
however even if we assume that the second explanation is the primary cause for the relationships we observed notice the peculiar consequences of this claim developers who believe testing is worthwhile and devote more time to it are wrong in that testing itself is useless but on the whole produce statistically better code than those who do not value testing.
this may not be an appealing argument to those dubious about testing s value.
.
threats to validity while we have taken care to ensure that our results are unbiased and have tried to eliminate the e ects of randomnoise we cannot guarantee that our results are valid.
in particular our results are subject to the following threats.
threats due to sampling bias to ensure representativeness of our samples we opted to use search results from the github repository of java projects that use the maven build system.
we picked allprojects that we could retrieve given the github api and selected from these only based on necessary constraints e.g.
the project must build and tests at epoch must pass .
however our sample of programs could be biased by skew in the projects returned by github.
github s selection mechanisms favoring projects based on some unknown criteria may be another source of error.
we also handpicked some projects from apache such as commons lang.
as our samples only come from github and apache this may be a source of bias and our ndings may be limited to open source programs.
however we believe that the large number of projects more than adequately addresses this concern.
bias due to tool used for our study we relied on pit.
we have done our best to extend pit to provide a reasonably su cient set of mutation operators ensuring also that the mutation operators were non redundant and have checked for redundancy in past work using pit .
secondly we used the gumtree algorithm discussed earlier for tracking program elements across commits.
however the algorithm used is unable to track program elements across renames or movement to another folder.
further refactoring that involves modi cation of scope such as moving the code out of the current compilation unit also causes the algorithm to lose track of the program element after refactoring.
bias due to mutant distribution there is still a possibility that the kind of mutants produced may be skewed which may impact our analysis.
bias due to equivalent mutants in this study we did not apply a systematic method for the detection of equivalent mutants and also did not remove equivalent mutants.
this might have impacted the mutation score of some projects where a large portion of the mutants were equivalent and were not killed.
bias due to commit classi cation our determination of commits as bug xes or not and of commits that end the history of a program element both depend on a learned classi er.
while our results do not require those results to be anywhere near perfect it may be that some unknown bias in the failures unduly in uences our results or gives rise to the weakness of observed correlations.
bias due to lack of high coverage some researchers have found that a strong relationship between coverage and e ectiveness does not show up until very high coverage levels are achieved .
since the coverage for most projects rarely reached very high values it is possible that we missed the existence of such a dependent strong relationship.
.
conclusion this paper uses a novel method to evaluate the e ectiveness of test suite quality measurements which we suggest essentially aim to capture the testedness of a program or program elements.
much of previous research attempting to evaluate such measures operates by a procedure that at a suitably high level of abstraction can be described as rst collecting a large set of tuples of the form testedness measure for suite faults found by suite then applying some kind of statistical analysis.
details vary in thatsuites may all be for one sut or for multiple suts though seldom for more than suts and in most cases actual faults are either hand seeded or faults produced by mutation testing which is assumed to measure real fault detection on a largely recently established and still limited empirical basis .
these studies have produced a variety of results sometimes almost contradictory .
is coverage useful?
is mutation score more useful?
we propose a di erent approach.
measuring fault detection for a suite can be extremely labor intensive worse depending on the de nition of faults we may give too much credit for detecting faults that are of little interest to most developers.
instead our evaluation chooses a point in time collects testedness measures statement coverage and mutation score for a passing test suite from that date and then examines whether these measures predict actual future bugxes for program elements.
if well tested elements of a program require no less e ort to correct then either we are not measuring testedness e ectively or testing itself is not e ective.
we assume that testing is e ective.
under this assumption we show that there isthe expected negative correlation between testedness and number of future bug xes.
however this correlation is so weak that it makes using it to compare testedness values in the continuous fashion where slightly more tested code is assumed to be slightly better or slightly higher scoring test suites are assumed to be better than slightly lower scoring test suite a dubious enterprise.
this suggests that the evaluation method in many software testing publications may be of questionable value.
on the other hand when we use testedness measures to split program elements into simple more tested and less tested groups the population di erences are typically signi cant and the mean bug xes are su ciently di erent usually about a factor of 2x to provide practical guidance in testing.
so is statement coverage useful?
is mutation score relevant?
is mutation score more useful?
the answers we believe may be that it depends on what you expect to achieve using these methods.
testing is an inherently noisy and idiosyncratic process and whether a suite detects a fault depends on a large number of complex variables.
it would given this complexity of process be very surprising if any simple dynamic measure computable without human e ort for any test suite produced strong correlations like those often shown between code coverage and mutation score .
.
.
the correlations between these measures are often high because both result from regular even handed automated analysis of the dynamics of a test suite.
actual faults are apparently unsurprisingly produced and detected by a much more complex and irregular process.
however when used to draw the line between less tested and more tested program elements testedness measures can provide a simple automated way to prioritize testing e ort and recognize when all the elements of an sut have passed beyond a high threshold of testedness and are thus likely to have fewer future faults.
in short while we cannot at present measure testedness as precisely as we software engineering researchers would like we can measure testedness in such a way as to provide some practical assistance to the humble working tester.
our data is available for inspection and further analysis at