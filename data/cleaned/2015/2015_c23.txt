what would users change in my app?
summarizing app reviews for recommending software changes andrea di sorbo1 sebastiano panichella2 carol v. alexandru2 junji shimagaki3 corrado a. visaggio1 gerardo canfora1 harald gall2 1university of sannio department of engineering italy 2university of zurich department of informatics switzerland 3sony mobile communications japan disorbo unisannio.it fpanichella alexandru g i .uzh.ch junji.shimagaki sonymobile.com fvisaggio canforag unisannio.it gall i .uzh.ch abstract mobile app developers constantly monitor feedback in user reviews with the goal of improving their mobile apps and better meeting user expectations.
thus automated approaches have been proposed in literature with the aim of reducing the e ort required for analyzing feedback contained in user reviews via automatic classi cation prioritization according to speci c topics.
in this paper we introduce surf summarizer of userreviews feedback a novel approach to condense the enormous amount of information that developers of popular apps have to manage due to user feedback received on a daily basis.
surf relies on a conceptual model for capturing user needs useful for developers performing maintenance and evolution tasks.
then it uses sophisticated summarisation techniques for summarizing thousands of reviews and generating an interactive structured and condensed agenda of recommended software changes.
we performed an end to end evaluation of surf on user reviews of mobile apps of them developed by sony mobile involving developers and researchers in total.
results demonstrate high accuracy of surf in summarizing reviews and the usefulness of the recommended changes.
in evaluating our approach we found that surf helps developers in better understanding user needs substantially reducing the time required by developers compared to manually analyzing user change requests and planning future software changes.
ccs concepts information systems !summarization software and its engineering !software maintenance tools keywords mobile application user feedback text summarization .
introduction user feedback plays a paramount role in the development and maintenance of mobile applications.
the experience an end user has with the app is a key concern when creating and maintaining a successful product.
consequently developer permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse seattle wa usa c acm.
isbn .
.
.
.
4teams are interested in exploiting opinions and feedback of end users during the evolution of their software .
on distribution platforms such as the apple store and google play users can leave plain text reviews for the apps they install and use.
these reviews may not only contain simple sentiments e.g.
great app!
but they can provide valuable information regarding several topics that are highly relevant to the development and maintenance of the app .
in particular user reviews may include i bugs or issues that need to be xed ii summaries of the user experience with certain features iii requests for enhancements and iv ideas for new features .
however existing app distribution platforms provide limited support for developers to systematically lter aggregate and classify user feedback to derive requirements .
moreover manually reading each user review to gather useful feedback is not feasible considering that popular apps receive hundreds of reviews every day .
for this reason automated approaches have been proposed in literature with the aim of reducing the e ort required to analyze feedback contained in user reviews .
however most of them only perform a classi cation or prioritization of user reviews according to speci c topics e.g.
bugs enhancements etc.
without reducing the amount of reviews and information developers have to deal with which is very large for popular apps.
to the best of our knowledge no approach is able to at the same time i determine for a large number of reviews the speci c topic discussed in the review e.g.
ui improvements security licensing issues etc.
ii identify the maintenance task to perform for addressing the request stated in the review e.g.
bug xing feature enhancement etc.
and iii present such information in the form of a condensed interactive and structured agenda of recommended software changes which is actionable for developers.
we argue that combining topic extraction intention classi cation and the ability to synthesize well de ned maintenance tasks regarding speci c aspects of an app will concretely help developers in planning further software changes and meeting market requirements.
paper contribution.
the contributions of our paper are summarized as follows we rst de ne urm userreviews model a twolevel classi cation model which takes into account both theusers intentions when giving feedback to developers and the review topic which is the speci c aspect of the app covered by the review we introduce surf summarizer of userreviewfeedback a novel approach built on top of urm to automatically generate summaries of users feedback with the aim of helping developers better understanding user needs.
surf exploits summarization techniques to summarize thousands of user reviews and generate an interactive structured andcondensed agenda of recommended software changes .
we conduct an empirical study involving developers engineers from companies in switzerland italy and the netherlands and developers engineers from the development team of sony mobile in japan to investigate the practical usefulness of summaries generated bysurf in the developers working context we make publicly available in a replication package12 with i material and working data sets of our study ii complete results of the survey iii raw data for replication purposes and to support future studies and iv the prototypical implementation of surf .
.
approach in this section we detail urm andsurf the model and approach we use for synthetizing app reviews.
.
the user reviews model urm aims to model informative paragraphs contained in app reviews from a software maintenance and evolution perspective along two orthogonal dimensions .user intention modeling the user s goals when writing a review e.g.
is the reviewer requesting a new feature or reporting a bug?
.
.review topic capturing the speci c topic covered by the review e.g.
is the writer discussing the user interface or a pricing issue?
.
previous app review content classi cation attempts have seen limited adoption by researchers because they consider just one of these twocomplimentary dimensions.
in recent work we introduced a taxonomy that classi es app review content into categories relevant for software maintenance and evolution namely feature request problem discovery information seeking information giving and other .
however this one dimensional classi cation results in an insu cient leverage of the available review information because for example having a huge amount of reviews classi ed as feature requests is of limited use to developers trying to distill actionable tasks.
for this reason urm aims to enrich and complement the preliminarily classi cation described in table by identifying the speci c topic of each review.
in the rest of the paper we refer to the aspects of an app targeted by a review as thereview topics .
to determine a complete set of review topics we analyzed reviews present in the dataset of our previous work .
thus we split the dataset in two parts i a training set t training containing randomly selected reviews and ii a test set t testcontaining the remaining reviews.
then starting from an empty list of topics ltopics two authors of the paper performed a manual labeling of each sentence contained in t training we work at sentence level since di erent sentences in the same review can contain di erent kinds of feedback .
during the labeling process when a sentence did not match any of the de ned topics a new topic was added to the topic list ltopics and .uzh.ch seal people panichella tools surf.html intention categories de nition information giving sentences that inform other users or developers about some aspect of the app.
information seeking sentences describing attempts to obtain information or help from other users or developers.
feature request sentences expressing ideas suggestions or needs for enhancing the app.
problem discovery sentences reporting unexpected behavior or issues.
other sentences not belonging to any of the previous categories.
the two validators went through all the previously labeled sentences to see whether it is more appropriate to tag some of them with the newly de ned topic.
vice versa if a review sentence matched an already de ned topic in ltopics the sentence was simply tagged with that topic.
since a review sentence can refer to more than one topic a user can ask to improve the ui and in the same sentence request a bug x related to stability of the app some sentences were tagged with multiple labels.
at the end of this process a preliminary set of topics was de ned3.
then we merged or clustered together review topics that are semantically related and that can generate redundancies and overlaps in the recommended software changes.
after this process we obtained a set of topic clusters detailed in table .
however a more accurate process for investigating removing possible overlaps in the resulting clusters is part of our future agenda.
table topic clusters cluster description app sentences related to the entire app e.g.
generic crash reports ratings or general feedback gui sentences related to the graphical user interface or the look and feel of the app contents sentences related to the content of the app pricing sentences related to app pricing feature or functionalitysentences related to speci c features or functionality of the app improvement sentences related to explicit enhancement requests updates versionssentences related to speci c versions or the update process of the app resources sentences dealing with device resources such as battery consumption storage etc.
security sentences related to the security of the app or to personal data privacy download sentences containing feedback about the app download model sentences reporting feedback about speci c devices or os versions company sentences containing feedback related to the company team which develops the app urm assigns to each review sentence one of the intention categories detailed in table and one or more app topic clusters de ned in table .
for instance the example sentence i love this app but it crashes my whole ipad and it has to restart itself.
will be classi ed as a problem discovery involving the app andmodel topics since it reveals that the user is experiencing an app crash while she is using a speci c device i.e.
ipad .
.
the surf approach surf automatically i extracts the topics treated in reviews ii classi es the intention of the writers to suggest the speci c kinds of maintenance tasks developers have to accomplish and iii groups together sentences covering the same topic.
figure depicts an overview of the surf process activities which we detail in the following sections.
.uzh.ch seal people panichella tools surf appendix.
pdffigure the surf process .
.
data collection the most common app distribution platforms are heterogeneous with regard to how reviews are posted and they collect data in di erent kinds of data structures.
for this reason we designed a data exchange format for collecting information that is potentially useful to developers and software analysts for maintaining and evolving apps.
speci cally since xml is particularly well suited for data where eld lengths are unknown and unpredictable and where eld contents are mainly text we developed an xml data exchange schema for representing review data from multiple sources.
for each user review it stores i the date on which the review has been posted ii the star rating given by the reviewer iii the handle of the user who posted the review iv the version of the app v the title of the review and vi the review text itself.
the output of the data collection phase of surf is a homogenized collection of reviews for each app in a well de ned xml format.
.
.
intention classification surf employs an approach we previously de ned to automatically mine reviewer intentions .
such an intent classifier combines natural language parsing nlp sentiment analysis sa and text analysis ta techniques through a machine learning ml algorithm for detecting sentences in user reviews that are important from a maintenance perspective see table .
to help other researchers replicate the analysis performed in this step of surf we make available online4the java version of our intent classifier already used by some developers in germany5.
.
.
topics classification as discussed in section .
one of the aims of surf is to group sentences involving similar review topics see table .
in this step surf automatically associates one or more concepts illustrated in table to each review sentence.
de nition of concept dictionaries.
we argue that discovering relevant keywords associated with the concepts reported in table represents a crucial information for better understanding relationships between concepts and informative user reviews .
thus two authors of the paper separately labeled each of the sentences in t training see section .
with keywords that in their opinion led to the assignment of a review topic to the sentence.
e.g.
the keywords orientation and button can indicate that the sentence deals with the application s gui.
we conjecture that on top of such a labeled dataset it is possible to de ne annlp classi er able to determine the a liation of a user .uzh.ch seal people panichella tools ardoc.html app reviews zu powernapp dienen als.htmlreview with speci c topics .
afterwards the two raters discussed their keyword assignments as well as possible discrepancies and nally created for each concept a nal collection of keywords and n grams .
to make the dictionaries more exhaustive we used wordnet to generate synonyms for all the keywords.
however to obtain dictionaries having a good balance between exhaustiveness and coherency we added the synonyms that are according to the wu palmer semantic relatedness wup at least equal to the original set of keywords.
among all the semantic relatedness metrics we selected this particular measure because it presents an upper limit i.e.
it is de ned between and .
more formally given an ontology o i.e.
wordnet made by a set of nodes c and a root node r establishing a threshold of .
to the wu palmer metric means that the least common subsumer i.e.
the nearest common node between c 1and c2 i.e.
the two nodes of the ontology for which we would compute the semantic relatedness must have a distance n from the root node at least equal to a quarter of the sum of the distances n 1and n i.e.
the distances between the root node r and the nodes c 1and c respectively wup n n1 n2 n n1 n2 oracle de nition.
once the dictionaries were enriched with wordnet synonyms one of the authors and an external validator a software engineer with more than years of industrial experience separately assigned each sentence inttest described in section .
to one or more of the de ned concepts.
for of these sentences the two raters assigned the same labels i.e.
inter rater agreement .
.
for the remaining sentences the two raters assigned di erent labels of them were sentences belonging to multiple concepts and the raters decided to consider both their respective suggested labels as correct for of these sentences the raters were in disagreement.
however after a separate discussion they reached an agreement on the nal labeling.
the manual labelling of each of the sentences in t testconstitutes our oracle for evaluating the nlp classi er described in the next paragraph.
automatic concept identi cation.
we built an nlp classi er to automatically assign a sentence in a review to one or more concepts see table .
as rst step we stemmed each sentence using the snowball stemmer algorithm to reduce words to their root form.
thus we built the classi er on top of the concept related dictionaries i.e.
the concepts dictionaries de ned in the previous sub steps to assign to each sentence in a review the probability to belong to one or more concepts.
more formally let s and c be a sentence and a concept respectively let wcbe the number of tokens insthat also appear in the list of c related keywords i.e.
the concepts dictionary ofcde ned in one of the previous sub steps let wsbe the total number of token in s the nlp classi er computes the probability that s falls in the concept c as p s c wc ws to avoid cases in which sentences are erroneously assigned to a concept c because they contain just one word also appearing in the c related keyword list the topic classi er assigns to s all the concepts for which p s c is greater than .
i.e.
.
we evaluated the e ectiveness of the nlp classi er comparing the labels assigned by the classi er withthe labels assigned in the human oracle de ned in the previous sub step i.e.
oracle de nition .
the e ectiveness of the classi er was evaluated relying on widely adopted metrics in the information retrieval eld precision recall and f measure.
the nlp classi er performed very well the sentences assignment to topics achieving a global recall of .
a global precision of .
and a global f measure of .
seetable .
hence we used such an nlp classi er in this topic classi cation step of surf.
.
.
sentence scoring and extraction surf uses a sentence selection and scoring mechanism to generate the summaries which is based on the following observations obs1 .
maintenance feedback rst user feedback discussing bug reports and feature requests are more important for developers than all others review types.
obs2 .
review topics developers need reasonably useful sentences discussing a speci c aspect of an app with respect to other review sentences.
obs3 .
review length longer sentences are usually more informative than shorter ones.
obs4 .
popular features reviews treating frequently discussed features may attract more attention of developers than reviews dealing with features or functionalities rarely used or discussed by users.
the heuristics de ned in this section are calibrated by using reviews of the dataset described in section .
and validated by two authors of the paper and an external validator a software engineer with more than years of industrial experience .
initial clustering and preprocessing.
first we needed to lter out useless sentences and to cluster the remaining ones according to their review topics.
in particular we discarded all sentences with less than tokens e.g.
good app great feature etc.
which rarely provide useful information for developers.
then we cluster sentences belonging to the same review topic and we remove duplicated in the summary by applying the following process a. all the sentences are preprocessed applying the snowball stemming algorithm and stop words removal.
b. for each pair of sentences s i sj we compute the duplicate tokens rate dtr i j through the formula dtr i j dti j twj where dt i jis the number of duplicate tokens appearing both in s iand s j tw iand tw jare the total number of tokens in s iand s jrespectively table concepts classi cation results table intention relevance scores problem discoveryfeature requestinformation seekinginformation givingother .
.
.
.
.
c. in each pair of sentences s i sj that satis es one of the following condition a if twj and dtr i j i.siis removed if twj twi ii.sjis removed otherwise b if twj and dtr i j isiis removed if twj twi iisjis removed otherwise scoring phase.
we assign a global score gs s c to each sentence s with respect to a topic c using the equation gs s c irs s p s c where the partial scores irs s p s c ls andmfwr s c corresponds to our four observations i.e obs1 obs4.
the rst partial score irs stakes into account our obs1 it assigns to each sentence san initial score on the basis of its intention category assigned during the classi cation step.
speci cally we statically provide di erent relevance scores to each of the intention categories de ned in table which are reported for completeness in table .
while other relevance scores could be used a systematic study on the impact of di erent scores is part of our future agenda.
however we did not observed any variation of the quality of the generated summaries using di erent scores.
the second partial score p s c is the probability of a sentence s to be relevant for a given topic c previously de ned in section section .
.
.
thus it is used to deal with obs2.
indeed if a sentence s share many words with the list of c related keywords then sis highly related to cresulting in a higher score.
according to obs3 longer sentences are more useful thus the length has to be incorporated in gs s c .
to this aim we use the third partial score ls which denotes the total number of characters constituting the sentence s. finally for each sentence s belonging to the topic c we compute the most frequent words rate i.e.
mfwr s c to take into account our obs4.
in particular mfwr s c measures the ratio of frequent words appearing in the sentence mfr s c with respect of the number of total words in the sentence tw s mfwr s c mfr s c tws sentences belonging to each of the topics reported in table are ranked through the scoring function gs s c in equation .
however only sentences in the top positions of the ranked list are selected.
more formally let nscbe the number of total scored sentences for the topic c the sentences occupying the rst nscpositions in the ranked list i.e.
about of total sentences are extracted for further processing while the remaining sentences occupying the last nscpositions are discarded.
.
.
summary generation we needed to nd a proper way to present the mined review sentences to developers such that they can i easily retrieve the necessary information ii properly understand the maintenance tasks to accomplish and iii identify which parts of the app to change.
to handle this we decided to generate summaries as structured html since it represents the right compromise between compactness informativeness and usability.
indeed using this format the summaries can be easily reported in a hierarchical way giving to developersfigure an extract of summary for stone flood control over paths of browsing the information that can be progressively expanded at multiple levels of detail.
in this step surf summarizes the sentences collected from user reviews pre processed and scored in the previous steps via clusterization i.e.
grouping them according to their review topics and intention categories.
in particular it performs a two level clustering rst it groups the sentences according to their topics e.g.
app gui etc.
leveraging on the gs s c scores computed in the previous surf step then sentences in each topic are grouped by intention categories which were assigned during the intention classi cation step.
finally groups in the second level are ordered as follows we rst show all the bugs i.e.
the sentences classi ed as problem discovery then all the requests i.e.
feature request then all the questions i.e.
information seeking and nally all the info i.e.
all the sentences classi ed as information giving and other .
moreover when clicking on each of the sentences reported in the summary a popup presents to developers the original review from which the speci c sentence has been extracted including app version user handle date star rating title and the full text.
surf also uses explicitly labels with the belonging semantic category i.e.
bug request question orinfo with the aim at helping developers in planning and accomplishing speci c maintenance task e.g.
implementation of new functionality .
finally in order to further increase the readability of the summaries surf shows a maximum of ten sentences for the info category i.e.
only the ten sentences most frequently reported by users in the reviews highest gs s c score .
figure depicts an extract of the summary for the app stone flood the complete summary can be found online6 .
in particular figure reports two feedbacks extracted from app reviews that could be bene cial for developers interested in xing bugs experienced by users points and in figure in the ui point in figure of the app.
.
study design the goal of our study is to investigate to what extent the summaries generated by surf help developers in better understanding user needs and planning future software changes.
speci cally we measure usefulness in the context of a working scenario in which developers and researchers analyzed user feedback contained in user reviews relying on surf with the goal of identifying feedback useful from a software maintenance perspective see section .
.
the quality focus concerns the capability of developers to collect useful user feedback when supported by summarization tools.
the perspective is that of researchers interested in evaluating the e ectiveness of automatic approaches for user feedback summarization when applied in a real working context.
we therefore designed our study to answer the following research questions rqs rq1 isurm a robust and suitable model for repre6 .uzh.ch seal people panichella tools surf stoneflood summary.ziptable dataset senting user needs in meaningful maintenance tasks for developers?
our rst goal is to verify whether developers consider urm an enough robust and suitable framework to model users needs in terms of software maintenance tasks.
rq2 to what extent does a summarization technique developed on top of urm help mobile developers better understand the users needs?
we want to assess whether surf facilitates the analysis of user feedback by developers.
thus starting from the general rq2 we derive two further sub questions that need to be answered to qualitatively and quantitatively measure the practical usefulness of surf and the impact of its generated summaries in the developers working context rq2 a how do app review summaries generated bysurf impact the time required by developers to analyze user reviews?
rq2 b how do developers perceive or consider the app review summaries generated by surf in terms of correctness content adequacy conciseness and expressiveness?
.
context as shown in table the context of our study consists of reviews from di erent apps belonging to di erent app categories and mined from di erent online platforms.
the reviews are extracted for each app in a period of between june 1st and january 1st .
in order to evaluate surf in a real working context sony mobile provided us with access to the google beta test pages of apps i.e.
lifelog beta trackid beta sketch beta movie creator beta video beta used by developers to collect feedback by beta testers about new functionalities.
table reports the main information of all the apps considered in our study i the app name ii the app category iii the platforms from which reviews have been gathered and iv the number of reviews.
for each user review involved in our study we collected the title comment and posting date theauthor s nickname the version of the app to which the review is referring and the star rating.
it is important to highlight that both surf and urm have been de ned relying on app reviews contained in a di erent dataset from the one used for answer our research questions.
speci cally urm and surf was de ned and calibrated on reviews of the dataset described in section .
while the evaluation of both urm and surf was performed considering reviews of the dataset described in table .
.
analysis method to answer rq1 andrq2 we performed two experiments involving developers testers managers and researchers from the netherlands switzerland italy and japan in total participants asking them to complete a survey7to evaluate i the suitability and robustness ofurm rq1 ii the practical usefulness ofsurf in a real working environment and how it can speed up the process of collecting useful review feedback rq2 a and iii the quality of the summaries generated by surf according to widely known dimensions rq2 b .correctness which measures the accuracy of the automated classi cation made by surf .content adequacy which assesses whether surf generates summaries containing all important information to understand user needs .conciseness which assesses whether surf generates summaries not containing any super uous and unneeded information .expressiveness which assesses whether surf produces summaries that are easy to read and whether the way they are presented facilitates the understanding of the user needs table summarizes the questions in our survey.
we designed two experiments the rst mainly aims at assessing the quality of extracted feedback and the meaningfulness thereof for developers while the second aims to investigate the practical usefulness of surf s summaries in a working environment when compared to the analysis of app reviews without the support of the summaries.
experiment i .
to evaluate surf we rst generated the summaries of reviews on apps picturex powernapp cstp blinq doodle pairs karaoke singme freelite karaoke singme minesweeper reloaded sheep o block stone flood weight track wi file transfer movie creator beta video beta and trackid beta.
then we contacted all the original developers of each of the selected apps and asked them to complete the survey and evaluate the summaries generated by surf.
initially all of the original developers of such apps con rmed their availability to participate in our study.
however in the end only six of them were actually able to participate.
consequently for the remaining apps we asked other developers or experts to evaluate the usefulness of surf s summaries.
thus for this rst experiment we involved participants in total.
of them are the original developers of ve applications considered in our dataset i.e.
picturex powernapp cstp movie creator beta and video beta while of the others are researchers in the eld of software engineering and are software developers employed in italian swiss dutch and japanese companies.
we assigned to each participant .uzh.ch seal people panichella tools surf survey.pdftable survey questions an app except for the six original developers involved in our study and provided the corresponding summaries generated by surf.
after that we explained to the participants the tasks to be performed during the experiment i.e.
how to browse the summaries and validate them and asked them to answer the questions of our survey.
experiment ii .
the second is a controlled experiment involving participants all employed at sony mobile.
the survey participants had the following pro les i test engineers who do not program but deal with bug db e.g.
bugzilla ii device driver engineers who have never worked with applications iii product project managers who care about software requirements deadlines and people workload and iv team managers who evaluate technical performance.
the summaries enrolled in this experiment are regarding the user reviews of two sony mobile apps lifelog beta and sketch beta .
the data of the reviews was collected from speci c google pages of sony mobile.
we rst separated the participants in two groups group subjects and group subjects .
then we performed two sub experiments experiment ii a and experiment iib.
during the experiment ii a group analyzed the original reviews contained in the google page of lifelog beta trying to manually extract useful feedback and to classify it according to tables and .
meanwhile group analyzed the original reviews contained in the google page of sketch beta in the same fashion.
likewise during experiment ii b group analyzed the summary of sketch beta s reviews generated by surf with the purpose of validating its content while group validated the lifelog beta s summary.
the time for the entire experiment ii was of minutes depending on the availability of sony mobile participants i minutes for explaining the purpose of the study sharing the materials and grouping participants in group and group ii minutes for experiment iia and experiment ii b and iii minutes for answering the questionnaire.
the short time was necessary to remove the in uence of confounding factors e.g.
collaboration with colleagues access to the web and have high con dence in the exclusive impact of our method on the task s outcome.table classi cation accuracy we plan to replicate our experiments involving developers of further apps performing longer evaluation tasks.
for each task the participants reported the number of reviews they analyzed and classi ed during experiments iia and ii b and answered the questions of our survey.
.
research method in order to assess the suitability and the robustness of urm and answer rq1 we asked the survey participants to rank the categories of reviews reported in the summaries q7 in table and to also suggest possibly missing categories q7.
in table .
to answer rq2 a we asked the survey participants to report the time required for performing the validation of summaries we provided q2 in table and to express their opinion on the speed up introduced by the summary in analyzing the user feedback q6 in table .
to qualitatively complement this quantitative information we also asked respondents to judge the usefulness and the comprehensibility of the provided summaries q3 in table and to provide their opinion on any di culties when analyzing the user feedback in the reviews with or without the provided summary q4 and q5 in table .
finally to answer rq2 b we asked survey participants to manually validate the classi cation correctness of data contained in the summaries q1 in table and provide their opinion on i the content adequacy q9 of table ii the conciseness q10 of table and iii the expressiveness q12 of table .
we complement this data by asking survey respondents for their opinion on the statement in q8 of table and for a general judgment on the usefulness of the summarization approach in a real working context q13 in table .
.
results .
rq1 results to answer rq1 we analyzed replies collected from survey participants of both experiments.
.
out of of participants declared that urm is not missing any relevant information and that the topics considered in urm are exhaustive or maybe even too detailed i.e.
just one participant complained about the number of topics in the model .
.
.
don t need many categories categories are enough.
.
.
.
the remaining feedback comprised proposals to i discriminate change requests between adaptive and corrective maintenance ii add a raging customers category iii introduce an advertisement category iv add a topic dealing with the connection to external devices and v insert a topic treating the comparison with other apps .
since the aim of our work is to facilitate the extraction of useful feedback from a maintenance perspective we be lieve that the raging customers andadvertisement categories would not add valuable information for developers while the other suggestions can be considered for future improvements of the model.
from a software development perspective participants considered the most important topics highlighted by urm to be i the app .
of participants ranked it in the rst three positions ii the gui .
of participants ranked it in the rst three positions and iii the feature or functionality .
of participants ranked it in the rst three positions .
vice versa the topics considered least important are i the company .
of participants ranked it in the last three positions ii the model .
of participants ranked it in the last three positions and iii download .
of participants ranked it in the last three positions .
a possible explanation for these ranking results may be related to the fact that app developers are more concerned with collecting functional requirements rather than non functional ones.
survey respondents also considered the intention classi cation of sentences that urm provides to be very interesting.
for instance some participants say that .
.
.
i found the classi cation gui bug app bug etc very useful.
.
.
.
.
.
in case i m searching for bugs i can just look for the category instead of reading everything over and over again.
.
.
.
.
.
categorization of reviews with a summary is very helpful to me.
especially the categories topics related to bugs crashes and security issues.
.
.
.
in summary we can conclude that rq1 according to the developers judgment urm has shown to be robust and suitable enough for representing user needs in meaningful maintenance tasks and to be usable in practical contexts.
the most important topics modeled in urm are the app gui and feature or functionality categories.
.
rq2 results .
.
rq2 a results experiment i results.
survey participants spent on average minutes and seconds for browsing and validating the summaries.
.
out of of participants considered our approach time saving out of of survey respondents replied that surf allows them to save at least one third of the time that they would otherwise have spent on manually collecting and analyzing user reviews.
among these participants subjects claim that with the summaries the saved time ranges between and while .
out of of participants a rmed that our approach allows to save of time.
only one subject stated that summaries do not allow to save any time.
we noticed that among the apps having less than reviews out of of subjects declared that our approach allows to save more than of time but amongtable aggregated answers from participants the apps having more than reviews we observe a degradation of this result.
indeed for such apps the subjects who claim as the saved time are just out of .
a possible explanation for this degradation could reside in the fact that for apps with more reviews surf generates longer summaries which need more time to be read and analyzed and this could in uence the perception of the real gain of time enabled through the approach.
these results are related to the time saving capability of surf perceived by developers .
however this measure is fairly subjective and a more quantitative evaluation is required.
thus in order to avoid a biased estimation of these results we discuss the validation times declared by the subjects in the survey q2 in table and compare them with the execution times required by surf to generate the summaries.
the execution times have been measured through a bit intel celeron dual core cpu t3500 .
ghz machine with gb ddr3 ram running windows .
on average the tool spent about .
seconds per review.
indeed for analyzing each app that in average has reviews surf requires an average execution time of minutes and seconds.
the summaries involved in experiment i contain in total sentences and each summary contains on average around sentences.
considering that the average number of sentences is survey participants spent an average validation time of .
seconds per sentence since the average validation time in the experiment is minutes and seconds .
this means that when supported by surf a developer spent for extracting analyzing and validating each user feedback an average total time of .
seconds i.e.
.
seconds .
seconds .
guzman et al.
demonstrated that for a ne grained manual analysis of user reviews in the best case developers spent hours.
we can infer that validators in spent an average time of seconds for analyzing each user review feedback.
thus when supported by surf the time required by developers for analyzing each user feedback decreases by at least .
from to .
seconds .
from a qualitatively point of view .
of subjects considered the provided summaries highly enough useful and comprehensible see q3 in table and .
of participants a rmed that analyzing user feedback with the provided summaries is easy see q4 in table .
experiment ii results .
the results of experiment iitable raw data of the questionnaire concerning the evaluation of surf summaries.
content adequacy response category of ratings exp i exp ii a is not missing any information.
.
.
b is missing some information but the missing information are not necessary to have an overview of users needs25 .
c is missing some very important information .
d not sure .
.
conciseness response category of ratings exp i exp ii a has no unnecessary information.
.
.
b has some unnecessary information.
.
.
c has a lot of unnecessary information.
d not sure .
expressiveness response category of ratings exp i exp ii a is easy to read and understand.
.
.
b is somewhat readable and understandable.
.
.
c is hard to read and understand.
.
.
empirically con rm most of the results obtained in experiment i. however contrary to experiment i only out participants .
declared that our approach allows to save of time.
in this case the time saving capability of surf perceived by developers is again subjective and a more quantitative evaluation is needed.
we notice that in experiment ii a when analyzing user feedback using google pages during a time slot of minutes each of the subjects extracted on average .
feedbacks useful for software maintenance.
vice versa in experiment iib when analyzing user feedback using surf s summaries in the same time subjects extracted in average many more feedbacks i.e.
.
instead of .
.
this means that for extracting a single useful feedback without the summaries the participants spent on average .
seconds while for analyzing and validating a single feedback extracted through surf the subjects spent on average .
seconds.
for the two apps involved in the experiment ii i.e.
lifelog beta and sketch beta there were in total user reviews and the tool had a total execution time of minutes and seconds for generating the two summaries.
thus considering that in this experiment the average execution time required by the tool for each review is .
seconds we can conclude that when supported by surf the time required by participants for extracting and analyzing each user feedback decreases from .
to .
.
.
seconds i.e.
.
.
it s important to highlight that out of i.e.
of feedbacks manually extracted by subjects also appear in the summaries automatically generated by surf.
in summary we can conclude that rq2 a developers consider the summaries generated by surf useful manually extracted feedback appears also in the automatic generated summaries and comprehensible.
surf helps to prevent more than half of the time required by developers for analyzing users feedback and planning software changes.
.
.
rq2 b results experiment i results.
table shows for each app each row and each app topic each column the amount of sentences misclassi ed by surf according to the judgment of respondents as well as the total sentences appearing in the summary.
the validation task performed by the survey participants highlights the very high classi cation accuracy of surf which is .
.
out of of subjects considered the summaries content to be adequate out of participants a rmed that summaries we provided have no loss of information while of them declared that even if there were a possible information loss our summaries still provide a complete overview of user needs see table .
on the contrary subjects out of believed that some important information does not appear in the summaries e.g.
pictures or videos of the original reviews that could explain the context for reproducing bugs .
about conciseness .
of subjects out of declared that summaries do not contain unnecessary information see table while subjects out of .
said that they contain just some unnecessary information e.g.
sentences that do not provide any useful information for developers .
as shown in table .
out of of subjects consider the provided summaries readable and easy to understand out of or somewhat readable and understandable out of .
for example participants say about the summaries generated by surf .
.
.
it s simple useful.
.
.
.
.
.
everything is very compact and we have a good overview.
.
.
.
.
.
the summaries are very concise and easy to read.
.
.
.
.
.
the information is more on less reorganized and expressed in form of change requests which i nd very useful.
.
.
.
.
.
it is very clear and hierarchically organized.
.
.
.
.
.
the summary is self explanatory.
.
.
.
.
.
the use of pop ups allows a developer to trace the entire user comment.
.
.
.
subjects .
believed that the summaries are hard to read and understand e.g.
.
.
.
it is presented in a very old fashioned html style .
moreover .
of subjects considered surf useful in a working context see q13 in table .
.
.
if we have more huge feedbacks i think this system is useful.
i think this can be used for not only reviewing details of each feedback but also understanding statistical bug request trends.
.
.
.
.
.
the tool you propose is very useful to highlight the most useful reviews.
without your tool reviews are just reviews not requests.
.
.
.
experiment ii results .
.
of participants out of declared that the provided summaries are lacking some important information see table like screenshots posted by beta tester useful for reproducing certain bugs i.e.
out of participants complain about this issue .
however screenshots are not allowed in the user reviews of the most popular mobile distribution platforms and our approach was originally conceived to collect data from these platforms and not from pages hosted on a general purpose social network i.e.
google .
as shown in table .
of the subjects out of a rmed that summaries do not contain unnecessary information while participants i.e.
.
said that they contain some unnecessary information e.g.
sentences not providing any useful information for developers or duplicate data .
regarding the expressiveness .
of the participants out of claimed the summaries are readable and understandable see table .
furthermore all the subjects generally considered the summaries highly useful for better understanding user needs see q13 in table .
in summary we can conclude that rq2 b according to the survey participants the summaries generated by surf are reasonably correct adequate concise and expressive.
moreover they are also considered useful in a real working context .feedback of survey participants.
some of the participants suggested to extend surf s functionality to make them even more readable for instance .
.
.
i would improve the report design with a more readable interface and by including extra information.
.
.
.
.
.
to make the summary more immediate would need to enter a statistical graph for all topics.
.
.
.
.
.
a better graphic a graphic report of the amounts of bugs requests info etc a navigation bar or menu hierarchically expandable categories folders some categories lters and a more e cient visual classi cation of the distributions and quality of the feedbacks.
.
.
.
.
threats to validity this section outlines potential threats to the validity of our study.
internal validity .
these threats concern confounding factors that could in uence our results.
the main threat to internal validity in our study is that the assessment is performed on data provided by human subjects hence it could be biased.
to counteract this issue we selected di erent subjects in our study who i meet di erent proles in the eld of software development ii have di erent cultural backgrounds i.e.
they come from di erent nations and iii of them were the original developers of apps in our dataset see section .
.
moreover in the experiment i the validation times are manually reported by the subjects they could entail imprecisions.
for alleviating this issue we provided precise timing instructions to participants.
another issue consists in the preventive assignment of some scores.
we statically assign some of the values on the basis of certain observations.
further investigation is needed to establish whether di erent distributions of these values enable better results.
external validity .
these kinds of threats relate to the generalizability of our ndings.
our experiments are small in scale i.e.
apps out of millions and the chosen apps may not be representative.
to mitigate this issue we investigated user reviews of apps belonging to di erent app categories and mined di erent online platforms see section .
.
speci cally to assess the robustness of surf in classifying and summarizing user feedback we considered reviews that contain di erent vocabularies and were written by di erent user audiences users of these apps i interact with devices and technologies in di erent ways ii belong to di erent age groups and iii have di erent expectations.
in experiment ii participants validated the summaries for a total time of minutes.
speci cally the availability of sony mobile participants in performing experiment ii was only minutes of which i minutes were spent explaining the purpose of the study sharing the materials and grouping participants in group and group ii minutes were used for conducting experiment ii a and experiment ii b and iii minutes were available for answering the questionnaire.
manually analyzing the user feedback for minutes is not su cient to generalize our ndings.
however in experiment i participants had all the time they needed to validate recommended feedback and analyze reviews.
it is important to highlight that the gain times rq a obtained when using surf are almost identical in experiment i and experiment ii.
in future work we are interested in replicating the study involving additional developers of other companies to consolidate our ndings.
finally only one of the apps in our dataset is not free karaoke singme .
commercial apps may have di erentreview patterns.
in the future for alleviating this threat we plan to extend the investigation to the user reviews of more commercial apps.
.
related work several researchers have focused on mining and analyzing app data with the goal of deriving important information to help developers evolve their apps .
first of all harman et al.
introduced the concept of app store mining and identi ed important correlations between the customer rating and the download rank of apps .
pagano et al.
investigated the correlation between reviews and ratings while bavota et al.
presented a study revealing a direct relationship between the rating of an app and the fault proneness of the underlying api used by the developers .
recently approaches have been proposed for automatically mining requirements from app reviews ?
.
for instance chandy et al.
used a latent class graphical model clustering reviews to identify spam .
gu et al.
on the other hand focused on identifying positive reviews using a pattern based parsing approach to extract opinions and summarize reviews .
guzman et al.
performed sentiment analysis to extract coherent features from reviews that relate to requirements evolution tasks .
numerous researchers have also applied natural language processing and text retrieval techniques towards automating the extraction of useful content from app reviews.
speci cally iacob et al.
used latent dirichlet allocation to extract feature requests from user reviews while carreno et al.
use topic modeling to extract common topics in a corpus of user reviews and summarize each topic with a few sentences .
chen et al.
presented a computational framework which automatically groups prioritizes and visualizes informative reviews .
likewise maalej et al.
classi ed app reviews into bug reports feature requests user experience and ratings .
panichella et al.
proposed an approach that combines natural language parsing sentiment analysis and text analysis techniques through a machine learning ml algorithm in order to detect sentences in user reviews that are important from a maintenance perspective.
we rely on this approach for performing the intention classi cation described in section .
.
.
most of the automated approaches discussed above perform an automatic classi cation or clustering prioritization of user reviews according to speci c topics e.g.
bugs features etc.
and as pointed out by gu et al.
they are based on a bag of word assumption without considering sentence structures and semantics .
our research represents a cross section of the eld of issue categorization and classi cation summarization of natural language corpora or source code in the speci c context of mobile app reviews.
from an engineering point of view our work extends the line of research on mining requirements from app reviews and it is novel for three main reasons.
first of all we present urm a conceptual model that takes into account both the users intentions when giving feedback to developers and the aspect concerning the app which is the subject of the review to model users needs in terms of software maintenance and evolution tasks that are important for developers.
moreover we propose a novel approach namely surf which combines sophisticated summarization techniques for generating according to urm summaries ofthousands of user reviews in form of an interactive structured andcondensed agenda of future change tasks .
finally we evaluate the impact of the generated summaries in a real scenario where developers analyzed feedback in user reviews of real mobile apps with the support of surf s summaries.
to the best of our knowledge only the work by gu et al.
is closely related to ours.
indeed gu et al.
proposed a summarization framework called surminer which classi es reviews into ve categories and extracts aspects or opinions in sentences.
the approach by gu et al.
considers aspect evaluation sentences from which it extracts aspect opinion pairs associates a sentiment value to each pair and provides developers with positive and negative opinions about each aspect.
our approach is based on urm which models the user feedback as explicit maintenance tasks considering a broader set of review types see table i and ii .
thus it does not only deal with feedback reporting user opinions about aspects that are modeled in the information giving category .
surf s summaries present to developers also feature requests and bug reports related to speci c parts of an app.
.
conclusion surf s ability to summarize thousands of app reviews in form of an interactive structured and condensed agenda of recommended software changes represents a signi cant step forward on the cutting edge of app review mining.
surf assists developers in e ectively and quickly understanding user needs and substantially reduces the time required for both the manual analysis of user feedback and the planning of software changes.
through surf a developer can quickly become aware of requests and issues reported by the users for instance to add a new option in the ui to x a stability issue or to protect the privacy of the user all without analyzing hundreds of useless reviews.
we assess the robustness and suitability of urm rq1 as well as the usefulness of surf rq2 in a working context by conducting two experiments involving real word apps and subjects having di erent pro les in the eld of software development.
in future work we plan on making the summarization interface more interactive using more e ective visualizations of feedback distributions.
moreover we are interested in i improving the classi cation capabilities of surf by adding new natural language heuristics ii adding internationalization as currently only english reviews are supported and iii implementing on top of surf a mechanism able to recognize which part of the source code needs to be changed in the app to perform the change tasks suggested by surf.
we nally plan to integrate a prioritization mechanism in surf to help developers focus on the most relevant tasks.
.