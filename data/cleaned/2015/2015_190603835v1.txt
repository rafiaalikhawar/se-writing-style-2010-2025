sar learning cross language api mappings with little knowledge nghi d. q. bui school of information systems singapore management university dqnbui.
phdis.smu.edu.sgyijun yu department of computing communications the open university y.yu open.ac.uklingxiao jiang school of information systems singapore management university lxjiang smu.edu.sg abstract to save effort developers often translate programs from one programming language to another instead of implementing it from scratch.
translating application program interfaces apis used in one language to functionally equivalent ones available in another language is an important aspect of program translation.
existing approaches facilitate the translation by automatically identifying the api mappings across programming languages.
however these approaches still require large amount of parallel corpora ranging from pairs of apis or code fragments that are functionally equivalent to similar code comments.
to minimize the need of parallel corpora this paper aims at an automated approach that can map apis across languages with much lessa priori knowledge than other approaches.
the approach is based on an realization of the notion of domain adaption combined with code embedding to better align two vector spaces.
taking as input large sets of programs our approach first generates numeric vector representations of the programs including the apis used in each language and it adapts generative adversarial networks gan to align the vectors in different spaces of two languages.
for a better alignment we initialize the gan with parameters derived from api mapping seeds that can be identified accurately with a simple automatic signature based matching heuristic.
then the crosslanguage api mappings can be identified via nearest neighbors queries in the aligned vector spaces.
we have implemented the approach sar named after three main technical components in the approach in a prototype for mapping apis across java and c programs.
our evaluation on about million java files and million c files shows that the approach can achieve and mapping accuracy in its top and top api mapping results with only automatically identified seeds more accurate than other approaches using the same or much more mapping seeds.
keywords software maintenance language mapping word2vec syntactic structure program translation permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse nier may june gothenburg sweden copyright held by the owner author s .
publication rights licensed to the association for computing machinery.
acm isbn .
.
.
.
introduction migrating software projects from one language to another is a common and important task in software engineering.
to support the process various migration tools have been proposed.
a fundamental challenge faced by such tools is to translate the library apis of one language to functionally equivalent counterparts of another.
often much manual effort is required to define the mappings between the respective apis of two languages.
several studies have addressed this api mapping problem such as mam staminer deepam and api2api .
mam and staminer require as input a large body of parallel program corpora which contain functionally equivalent code that use apis in both languages in order to mine the mappings.
thus they rely heavily on the availability of bilingual projects that implement the same functionality in two or more languages which is not easy to find for any pair of languages.
although they rely on similar function names to reduce manual effort needed to identify parallel data many functions with similar names may be actually functionally different degrading the quality of training data and final mapping results.
deepam maps api sequences to sequences based on the text descriptions for the sequences.
its intuition is that two api sequences across languages may be mapped to each other if their text descriptions are similar.
this approach does not need api mapping seeds but requires many similar text descriptions across programs written in different programming languages whose availability can affect the mapping results.
api2api uses a vector space transformation method inspired by mikolov et al.
but it still requires many api mapping seeds from an external source java2csharp to map apis across languages.
in this paper we propose an approach that can map apis across languages while alleviating the shortcoming of existing approaches.
we realize that the underlying goal of state of the art techniques is essentially to find a transformation that can align two different domains in our context the two vector spaces for apis in two different languages .
api2api is also an instance of this idea to learn an optimal transformation matrix between two vector spaces while requiring much parallel training data.
however empirical evidence of existing approaches suggest that collecting the training data is an expensive process that requires either availability of manual inspection or high quality documentations.
this has led to the following research question we aim to answer in this paper can a model be built to minimize the need of parallel data to map apis across languages?
.
we realize that the api mapping problem may be addressed by techniques based on generative adversarial training with the assistance of a pre trained model.
given large code bases in twoarxiv .03835v1 jun 2019icse nier may june gothenburg sweden nghi et al.
languages it is likely that certain similarities between the code bases can be exploited to discover apis of similar functionality across languages without manually specifying parallel corpora.
such knowledge of similar functionalities may not be big enough for a complete mapping model but it is small enough to afford human validation.
once validated the knowledge can be transferred through adversarial training techniques to maximize the alignment between the two languages which results in better api mappings.
our approach for api mapping works in the following way it takes in a large number of programs in two languages and generates a vector space representing code and apis in each language via a word embedding technique adapted from previous studies it adapts domain adaption techniques to transform and align the two vector spaces for the two languages with mainly three technical components seeding adversarial training and refinement and it utilizes nearest neighbors queries in the aligned vector spaces to identify the mapping result of each api.
we name our approach sar after the three main technical components in the domain adaption step.
we have implemented the approach in a prototype tailored for java and c and evaluated and compared it with the state of theart techniques such as staminer deepam and api2api .
we have evaluated the prototype on a dataset of more than java projects containing approximately .
million files and c projects containing approximately files.
our evaluation results indicate that the approach can achieve and accuracy in its top and top api mapping results with only automatically identified seeds more accurate than other approaches using the same or much more mapping seeds.
in addition we also identify about more api mappings between the java and c sdks than other approaches.
the main contributions of this paper are as follows we propose sar a new approach based on domain adaption techniques to transform and align different vector spaces across languages with the assistance of a seeding adversarial learning and refinement method.
to the best of our knowledge we are the first to apply the adversarial training techniques for the api mapping task.
we adapt the adversarial training techniques in a number of ways to improve its alignment of the vector spaces we use nearestneighbor queries to identify possible mapping candidates for better alignment we use a similarity based model selection criteria and reduce the need of known api mappings during the training of our model and we use the procrustes algorithm to find the exact solution of the mapping matrix.
we have implemented the approach and evaluated it with a corpus containing millions of java and c source files via an extensive empirical evaluation on different components of our approach we demonstrate its advantages against other api mapping approaches in producing more accurate mappings with much fewer seeds that can be automatically identified.
the rest of the paper is organized as follows.
section discusses studies in the literature closely related to this paper section presents the background about vector space mapping and adversarial learning section presents our approach in detail section evaluates our approach to demonstrate its effectiveness and discuss its limitations and section concludes with possible future work.
related work this section briefly reviews related work on cross language program translation and relevant techniques.
cross language program translation.
for the problem of crosslanguage program translation much work has utilized various statistical language models for tokens phrases or apis .
a few studies also used word embedding for api mapping and migration e.g.
but our work does not need large number of manually specified parallel corpora or mapping seeds.
tools for translating code among specific languages in practice e.g.
java2csharp also often dependent on manually defined rules specific to the grammars of individual languages while our approach alleviates the need of language specific rules.
mam and staminer rely on the availability of bilingual projects that implement the same functionality in two or more languages.
deepam requires many similar text descriptions across programs written in different programming languages whose availability can affect the mapping results.
api2api requires many api mapping seeds from java2csharp to map apis across languages.
the idea of our approach is most similar to api2api while we combine seed based and unsupervised domain adaptation techniques to reduce the need of mapping seeds.
relevant techniques.
for the techniques used to represent model learn source code many studies exist for building various statistical language models of code for various purposes in recent years .
when it comes to what models to use for code there is still much room for improvement.
hellendoorn et al.
showed that simpler code learning models e.g.
n gram with caches of code locality and hierarchy may outperform complex deep neural network models.
while other studies e.g.
demonstrate that more grammatical and semantic code features at various levels of abstraction can be useful for more accurate models.
these studies provoke us to perform code embedding with structural information and in future to explore more semantic information for code embedding.
however existing studies using domain adaption techniques for api mapping and translation still require the creation of mapping seeds .
to transform vector spaces studies in nlp on sentence comparison and translation involve variants of bilateral models to align the contents but they require parallel corpora in two languages.
recent progresses in domain adaption alleviate the need of parallel corpora .
in an application to image learning domain adaptation through gan has shown benefit to transfer the models from other dataset as pre training models when training on smaller dataset which provides the technical foundation for our work.
background the goal of domain adaptation is to produce a mapping matrix as an approximation of the similarities between vectors in the two spaces.
this section gives a brief overview of two methods for domain adaptation seed based or unsupervised.
apart from the two input vector spaces the seed based method also requires a set of seeds as the parallel training data to learn the matrix while unsupervised method does not the mapping matrix can be obtained through adversarial learning assuming that similarity exists between the distributions of vectors in the two spaces.sar learning cross language api mappings with little knowledge icse nier may june gothenburg sweden .
seed based domain adaptation given two sets of embeddings have been trained independently on monolingual data seed based domain adaptation is to learn a mapping using the seeds s.t.their translations are close in a shared vector space.
such an idea has been explored for word translation in nlp and api2api adapts it to learn api mappings.
formally given two vector spaces x x1 .
.
.
xn andy y1 .
.
.
ym containing nandmembeddings for two languages l1 andl2 and a set sof seeds of api embedding pairs xsi ysi si s we want to learn a linear mapping wbetween the source and the target space such that w xsiapproximates ysi.
in theory wcan be learned by solving the following objective function w ar min w m rd d w x s ys where dis the dimension of the embeddings m rd dis the space of d dmatrices of real numbers xs xsi xand ys ysi ycontain the embeddings of the apis in the seeds which are matrices of size d s .
instead of approximating a solution using traditional stochastic gradient descent method used in api2api there exists an analytical procrustes problem solved by xing et al.
which has a closed form solution of the mapping matrix derived from the singular value decomposition svd of yxt w ar minw w x s ys uvt with u vt sv d ysxts the advantage of a closed form solution is that one can get the exact solution which is better than the approximate solution of gradient descent and is faster in computation.
with the mapping matrix w one can use yx w xto map a query vector x. the vector yxis the mapping or adaptation of x in the target space.
.
unsupervised domain adaptation adversarial learning has been successfully used for domain adaptation in an unsupervised manner.
in particular the generative adversarial network achieves this goal by a model which comprises a generator and a discriminator as two inter playing components.
a generator network that aims to learn real data distribution and produce fake data to fool the other component so called the discriminator the discriminator network that acts as a classifier which aims to distinguish the generated fake data from the real data.
the two components are trained in a minimax fashion and would converge when the generator has maximized its ability to generate fake data so similar to the real data that the probability for the discriminator to make a mistake would be1 .
conneau et al.
use this idea as a variant for the machine translation task which achieves significantly better results than other baselines of machine translation which would require no parallel data to train the networks.
the generator in this case is a mapping matrix w which can simply be seen as a set of parameters that need to be learned and the discriminator is a feed forward neural network.
we want to find a matrix was an approximation of the mapping between the two vector spaces xandy.
in the adversarial learning setting we aim to optimize two parameters one is the discriminator s parameters denoted as d the other is the mapping matrix w. our goal is to find the optimal value of figure approach overview two sets of parameters which results that we have two objective functions in the adversarial learning setting.
discriminator objective.
given the mapping w the discriminator parameterized as d is optimized by this objective function ld d w n i 1lo p d source w xi m i 1lo p d source yi where p d source v is the probability that a vector voriginates from the source embedding space as opposed to an embedding from the target space .
mapping objective.
given the discriminator d the mapping w aims to fool the discriminator s ability of predicting the original domain of an embedding by minimizing this objective function lw w d n i 1lo p d source w xi m i 1lo p d source yi learning algorithm.
the discriminator dand the mapping w are optimized iteratively to minimize ldandlw respectively by following the training procedure of adversarial networks proposed by goodfellow et al.
our approach combining the virtues of seed based and unsupervised adversarial methods described in the background our domain adaptation approach can approximate two spaces of vectors with minimal parallel corpora.
although unsupervised adversarial learning method does not require any seed as parallel data the distributions of vectors i.e.
embeddings in the two spaces may not be similar.
therefore it is our hypothesis that the performance could be improved by initializing the unsupervised adversarial learning method with a small set of seeds taken from the seed based domain adaptation and by generating the rest of api mappings in the following two steps from large code corpora in two different languages we create two vector spaces for apis by adapting word embedding technique for code.
from such corpora we derive a small set of mappings based on a simple text similarity heuristic seeicse nier may june gothenburg sweden nghi et al.
code embedding in figure the two vector spaces along with the mapping seeds are transformed by a mapping matrix to get aligned with each other.
this step comprises three sub steps seeding adversarial learning and refinement see domain adaptation in figure .
for any given api ain the source language and its continuous vector representation x we can map it to the other domain space by computing yx w x. then one can find the top k nearest neighbors of yxin the target vector space using cosine similarity as the distance metric and finally can retrieve the list of apis in the target language that have the same embeddings as the top k nearest neighbors.
the list of apis can then be used as the mapping results for a see cross space near neighbor query in figure .
the following subsections detail these sub steps.
.
code embedding via word embedding we first parse source code files into abstract syntax trees ast using srcml for both java and c projects.
then we extract from individual functions the code sequences and perform a normalization.
the normalization enriches the code sequence with structural semantic information extracted from parsing which constitutes two steps filtering out noisy tokens.
tokens are considered noisy if they are not api tokens.
to leverage as much structural information as possible language keywords and ast node types are still kept for code embedding.
converting raw api tokens into signatures.
this step reduces the variance of vocabulary existing in the source code.
for example one may extract the list.add method from the java.util.list class or from the com.google.common.collect.list in an external thirdparty library.
even though these two apis have the same class and method names their usages and semantics are different.
to handle such cases we propose this additional step to convert a raw api token to its signature in qualified name format package .
class .
method .
below shows an example of the normalization for the code token sequence list.add list.add if list.addall else hashmap.put return java.util.list.add java.util.list.add if java.list.addall else java.util.hashmap.put return from the corpora of code sequences we use the skip gram word2vec model to train the embedding of each token.
given a large corpus as the training data the tokens appearing in the same context would usually have their embeddings close by distance in the vector space.
.
domain adaptation our domain adaptation comprises three steps seeding adversarial training and refinement hence the abbreviation sar of our approach .
seeing sar from outside as a black box it receives two vector spaces and a set of seeds as input and generates a mapping figure domain adaptation steps to align two vector spaces matrix was output.
internally each step of sar is a different way to improve the mapping matrix which receives the matrix output from the previous step as input and produces the improved version of it as output.
we assign w1 w2andw3as the output matrix for the three steps respectively.
figure summaries the domain adaptation procedure as a whole.
the rationale for each step is described as follows the seeding step to initialize a mapping matrix between the two vectors spaces based on some prior knowledge i.e.
seeds the adversarial learning step to re use the knowledge learned from the seeding step as an initializer for adversarial training in order to maximize the similarity between the two vector spaces or two distributions and the refinement step to make the mapping matrix better and reach its optimal state.
.
.
seeding.
after code embedding two vector spaces are obtained to produce a mapping matrix that approximates the two vector spaces by using the knowledge from mapping seeds in a dictionary.
notice that by a simple signature based comparison to identify apis having the same signature name one can identify many high quality mapping candidates to be used as the seeds without any human effort to verify because developers often use the same name for the same functionality even when they are in different languages.
having the dictionary dobtained in addition to the two vector spaces xandy the initial mapping matrix produced is w1by solving the equation in section also see seeding in figure .
this seeding step can be seen as a function a which will receive these three inputs and produces a transforming matrix w1such thatw1 a x y d .
internally asolves the optimization problemsar learning cross language api mappings with little knowledge icse nier may june gothenburg sweden described in section given the three inputs.
.
.
adversarial learning.
the quality of the matrix w1learned in previous step is limited by the number of seeds one can provide which results in an approximation between the source and target domains.
in this case the knowledge learned for w1can be seen as a pre trained model and can be reused for the other model.
formally given the two original vector spaces x x1 .
.
.
xn andy y1 .
.
.
ym containing nandmapi embeddings obtained from the code embedding step we want to find the matrix w2 tomaximize the approximation of the mapping between the two vector spaces.
we use the adversarial learning to achieve this goal.
we build the adversarial learning network comprises of two steps the mapping matrix w2and a discriminator network as described in section .
.
our goal is to find the optimal value of w2and d discriminator parameters we achieve this by training the adversarial network with the objective functions as described in section .
to find w2and d. the key difference with the general adversarial setting described section .
is that we do not initialize w2randomly as one usually does when training a neural network.
instead we use w1as a pretrained model to initialize for the w2so that w2is initialized with some good knowledge even if it is small see adversarial learning in figure .
this step is essential to improve the performance of the api mapping results.
model selection criteria.
to train the adversarial networks like any other neural network architecture we need a validation set to select the best model for the prediction step.
the validation set is used to minimize overfitting when training the neural network.
concretely for each training epoch one needs to evaluate against the validation dataset to pick the model that has the highest validation accuracy through training.
our goal is to use as little parallel data as possible to build the model.
in practice one only has a very small number of seeds inferred from the signature based matching or in the worst case one cannot infer any seed to have data for validation.
as such it is impractical to use a parallel dataset as a validation set to train neural networks in the adversarial learning step i.e.
involving additional prior knowledge.
to address this issue we propose to use a model selection using an unsupervised criteria that quantifies the closeness of the source and target embedding spaces.
specifically we consider them a set of k most frequent source apis and multiply them with the mapping matrix wto generate a target api mapping for each of them.
we then compute the average cosine similarity between these deemed mapping and use their average as a validation metric.
.
.
refinement for better alignment.
the adversarial approach tries to align all words irrespective of their frequencies.
however rare tokens have embeddings that are less updated and are more likely to appear in different contexts in each corpus which makes them harder to align .
to address this problem we use the method proposed in to infer a list of mapping candidates using only the most frequent tokens.
moreover other heuristics are introduced to infer another candidate set of mapping based on the threshold of cosine similarity which can be used as another synthetic dictionary that can combine with the top k frequency mapping candidates.
following the step shown in it is possible to build a set ofmapping candidates using w2just learned with adversarial training.
assume that one can induce a combined set of mapping candidates from different heuristics above and the quality of the combined set is good then this set of candidates should be used to learn a better mapping and consequently an even better set of candidates for the next iteration.
the process can repeat iteratively to obtain a hopefully better mapping and candidates set each iteration until some convergence criteria are met.
formally the refinement step receives w2from the previous adversarial learning step along with the two original embeddings xandyto produce the next w3iteratively see refinement in figure .
specifically we produce the mapping candidates for refinement based on two heuristics top k frequency conneau et al.
shows that by taking the top k frequent words and their nearest neighbors in the transformed vector spaces it can provide high quality mapping candidates because the most frequently used words are likely to be the same across languages.
therefore we can use the top k frequent api names to induce the seeds for the refinement.
cosine similarity threshold since finding api mappings in the aligned vector space is essential to finding apis close enough in the vector space all api pairs similar enough in the vector space aligned by adversarial learning can be good candidates for the refinement step.
in this work we use the cosine similarity as the metric to measure how similar two vectors are.
we note that notall apis in a language can have a mapping in another language.
in the empirical case study we show how a good threshold is found in section .
.
.
therefore we can infer two sets of synthetic mapping candidates from the above heuristics.
in fact there are different ways to merge them into one single set as they can overlap as e.g.
the union of the two sets the intersection of the two sets.
the matrix w3in this step is the final output of the domain adaptation process.
when it comes to the step to produce the mapping from the source query the embeddings of the query will be multiplied with w3in order to obtain corresponding mappings in the target language.
empirical evaluation we have conducted extensive empirical evaluations on our approach in various settings to answer the following research questions rq1 compared to related methods is our approach more effective in identifying api mappings?
rq2 how well do different combinations of refinement heuristics improve the performance?
rq3 how do the seeds overlapping effect on the performance?
rq4 what is the impact of each component in our approach on the performance?
.
dataset we use the java giga corpus data described by allamanis et al.
.
it involves approximately java projects from github and contains approximately .
millions of files.
for c we clone the projects on github that have at least star and collect c projects with about files.
as the main advantage of our approach there is no need toicse nier may june gothenburg sweden nghi et al.
table example of seeds from the signature based matching heuristic java c java.lang.
string.equals system.
string.equals java.util.
list.remove system.collections.generic.
list.remove java.util.
random.nextdouble system.
random.nextdouble java.lang.
math.round system.
math.round java.io.
file.exists system.io.
file.exists specify which code in java is functionally equivalent to which code in c .
for each function in a file we traverse the ast of the function to extract the api call sequences.
for java we get a corpus containing .
million code sequences for c we get a corpus containing .
million code sequences.
for evaluation we take method api mappings and class api mappings defined in java2csharp as the ground truth for evaluating our approach against the baselines.
.
implementation we adapt gensim in nlp to produce the embeddings of tokens for the java and c corpora.
we use the same settings used by mikolov et al.
during the training stochastic gradient descent with a default learning rate of .
negative sampling with samples skip gram with a context window of size and a subsampling rate of value 1e .
evaluation metrics.
we define the top k accuracy as the evaluation metric throughout the experiments.
the top k accuracy is defined as follow for a test jdk api j sar produces a resulting list.
if the true mapping api in c .net for j is in the top k resulting list we count it a hit.
if not we count it a miss.
top k accuracy is computed as the ratio between the number of hits and the total of hits and misses for a given ground truth test set.
we use this simplified metric for easier comparison with other approaches.
in real world uses one may retrieve a list of api mapping results given a query and better use other information retrieval metrics such as mean average precision map or mean reciprocal rank mrr as the evaluation metrics.
code embedding.
from the two code corpora we scan through all pairs of apis in the two corpora to produce a set of seeds using the signature based matching heuristic.
we got seeds for this step.
table shows examples of the seeds.
among these seeds we found that seeds overlap in mappings of the ground truth.
then we apply the code embedding step on the corpora get the source embedding and target embedding we use them along with the seeds as the input for the domain adaptation process.
domain adaptation.
for the seeding step we find w1by using the procrustes solution in equation with the three inputs source embedding x java target embedding y c and seeds.
this step gives us the mapping matrix w1.
we implement the adversarial learning by using pytorch .
we use momentum gradient descent method to search for the optimal transformation matrix.
we use the unsupervised model selection criteria proposed in section .
.
to select the best model by choosing the top frequent api token pairs e.g top frequent token in the source is aligned with top frequent token in the target as the validation set then we extract the w2from the model.
figure shows three figure unsupervised model selection criteria table api mappings baselines index baselines k folds seeds top top top random seeds api2api .
.
.
.
.
.
.
.
.
.
.
.
random seeds sar .
.
.
.
.
.
.
.
.
.
.
.
k fold api2api1 fold .
.
.
folds .
.
.
folds .
.
.
folds .
.
.
k fold sar1 fold .
.
.
folds .
.
.
folds .
.
.
folds .
.
.
signature based api2api .
.
.
.
.
.
.
.
.
.
.
.
signature based sar .
.
.
.
.
.
.
.
.
.
.
.
different lines the discriminator accuracy which is the accuracy in classifying the samples from the source and target embeddings the api mapping accuracy which is the accuracy when using the model to evaluate against the pairs validation set and the average cosine similarity of all the pairs.
as shown the criteria correlate well to the mapping accuracy.
from w2resulting from the adversarial training we obtain the final w3by performing the refinement step on the basis of two heuristics in section .
.
.
for the top n frequency heuristics we choose top frequent tokens for the synthetic dictionary as suggested in .
for the second similarity threshold rule we use .
as the threshold as shown in section .
.
we found that this number balances coverage and precision of api mappings well.
our source code and experimental results can be accessed at the anonymous repository1.
.
evaluation .
.
rq1.
effectiveness of sar in mining api mapping.
the first question we want to answer is how effective our approach in identifying api mappings from the two vector spaces.
we compare sar with api2api staminer and deepam.
learning cross language api mappings with little knowledge icse nier may june gothenburg sweden result summary.
index in table uses api mappings automatically selected by the signature based matching heuristic and test against the ground truth mappings.
index uses mappings selected randomly from the ground truth set and test against the rest.
the performance of sar in term of top k accuracy is shown.
as one can see in both cases the top accuracies are above and the top accuracies are above .
compare to api2api.
the method used in api2api is corresponding to the seeding step in our domain adaptation process which finds a mapping matrix by solving the equation given a large set of seeds.
we use the top k accuracy as the evaluation metric.
table shows the top kaccuracy of our approach when comparing to api2api in various settings.
first we compare api2api with sar using the seeds coming from two different sources the mappings defined by java2csharp and the mappings inferred from the signature based matching.
here we described the variances as results shown in table indicating that our approach can use much fewer number of seeds compared to api2api but still achieve better results.
select randomly we select a subset of mappings rrandomly from mappings in the ground truth and test against the rest r mappings.
concretely r and k fold we divide the mappings into k 4folds and perform the variants of five fold cross validation while kfolds are used as training data the other kfolds are used as testing data select by signature we use mappings inferred by method signature and select randomly a varying number of them as the training data and test against the remaining mappings in the ground truth.
the process repeats for using different folds as the training data for both api2api and we take the average accuracy are some observations from the results using the same number of seeds either using the seeds from random k fold or signature based we get significantly better results than api2api for every setting.
our approach only needs signature based seeds to get a comparable result for top or better results top and top with api2api that uses manually crafted seeds.
when using all of the signature based seeds our approach gets significantly better results than api2api top improves top improves and top improves .
compare to staminer and deepam.
we follow the details described in staminer and deepam to measure how well sar performs in mining api mappings for class api and method api.
in java an api element by definition can be a class a method or a field in the class and it must belong to a package or the namespace in case of c .
as such the goal in this task is to measure the performance the class and method api mapping task one by one for each api of each package i.e.
to see which package has the best performance for api mappings so called to mappings.
for the method api mapping we use the method ground truth mapping described in section .
for evaluation.
for the class api mapping we use the class ground truth mapping described in section .
for evaluation.
we follow the details described indeepam to choose only the apis under the packages as shown in table column package so that the total number of method api mapping left is remaining from ground truth method api mappings and the total number of class api mapping remaining from ground truth class api mappings .
adapting sar for class level api mapping is relatively easy one can remove the method part of a qualified api signature token so that only the package and class parts of the token are retained in the code sequences.
then code embedding for the api sequence can be derived as the embedding of the class level api along with other keywords from the asts.
we do this for both languages.
to select mapping seeds by api signatures we first infer the mappings from signatures at the class level then follow a similar domain adaptation process from apis at the method level.
one could not run staminer and deepam directly because they require parallel data aligned function body for staminer and aligned code and text description for deepam for training.
therefore we had to compare to them by extracting the reported performance numbers from their papers.
this is also how deepam compared itself to staminer.
we use the f score as the performance metric to measure accuracy in this evaluation.
it is defined as f 2pr p r where precision p tp tp fp and recall r tp tp fn .
tp refers to the number of true positives which is the number of api mappings that are in both result datasets and the groundatasets tn refers to the number of true negatives which is the number of api mappings that are neither in the returned results nor in the ground truth datasets fp refers to the number of false positives which represents the number of result mappings that are not in the ground truth set fn refers to the number of false negatives which represents the number of mappings in the ground truth set but not in the results.
table shows the comparison results of our mined api mappings with staminer sta and deepam deepa .
columns class mapping and method mapping list results of comparing api classes and methods respectively.
as one can see for the f score our approach has better results than those of deepam and staminer at the level of both classes and methods with much fewer seeds while deepam needs to use millions of similar api sequence descriptions and staminer needs to use ten of thousands of pairs of parallel data.
newly found api mappings.
more interestingly we found a lot more new api mappings than other studies in our actual code corpora.
for each of the api in java we query the top nearest neighbors in c and manually verify the mappings.
we enforce the threshold .
as mentioned in section .
.
for this task.
we found new sdk api mappings that can complement the tool java2csharp.
comparing to mam new mappings staminer new mappings api2api new mappings we found a sufficiently larger number of mappings and our newly found apis also overlap with the apis in these baselines.
in table we show some interesting examples of such newly found api mappings whose name do not match exactly using traditional approaches.
our list of newly found java c apis mappings can be accessed at this anonymous github repository .
may june gothenburg sweden nghi et al.
table accuracy of mapping when compares with staminer and mam packageclass mapping method mapping precision recall f score precision recall f score sta deepa sar sta deepa sar sta deepa sar sta deepa sar sta deepa sar sta deepa sar java.io .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
java.lang .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
java.math .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
java.net .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
java.sql .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
java.util .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table examples of newly found apis in java and c java c java.io.datainputstream.readint system.io.binaryreader.readuint16 java.awt.graphics2d.fillrect system.drawing.graphics.fillrectangle javax.swing.text.jtextcomponent.setcaretposition system.windows.controls.richtextbox.caretposition java.lang.byte.parsebyte system.sbyte.parse java.lang.double.longbitstodouble system.bitconverter.int64bitstodouble java.net.datagramsocket.isconnected system.net.sockets.socket.connect java.awt.geom.affinetransform.inversetransform system.drawing.drawing2d.graphicspath.transform java.io.datainputstream.readdouble system.io.binaryreader.readdouble java.net.serversocket.accept system.net.sockets.socket.acceptasync table accuracy using various similarity thresholds thresholdcoverage accuracy top top top top .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rq2.
effect of different refinement approaches.
effects of cosine similarity threshold.
in this section we measure the effect of different ways to combine the seeds for the refinement step.
we want to measure the effects of cosine similarity threshold in order to choose a good one for the second heuristic in the refinement step.
since threshold is a part of the refinement the domain adaptation step only comprises of two steps seeding and adversarial learning.
once the threshold is found we use it for the refinement in the other experiments.
then we produce the mapping for each source query in the ground truth mappings.
for each mapping produce we obtain the cosine similarity between the query and the result mapping.
we choose a threshold to filter out the mapping that has the cosine similarity lower than the threshold then we measure the accuracy of the left mappings.
in table the column coverage means the percentage of ground truth apis that have mappings in the candidate selection results when choosing a specific cosine similarity threshold.
the column accuracy means the top kaccuracy in identifying the mapping given a cosine similarity threshold as a condition to identify.
the results show that our approach in these experiments has higher mapping accuracy but lower coverage with respect to the ground truth set when the similarity threshold increases.
it is therefore a trade off to have higher accuracy in the expense of coverage.
for the other experiments that involve the cosine similarity threshold in the refinement we choose .
as the threshold as this number is balanced between the coverage and the accuracy.
effects of different combinations of refinement heuristics.
obtained .
as a good threshold to identify correct mappings we use this number for the cosine similarity threshold heuristictable different ways to combine refinement heuristics refine method top top top top k .
.
.
cosine .
.
.
union top k cosine .
.
.
intersection top k cosine .
.
.
table effect of overlapping seeds baselines seeds top top top ours removed overlapping seeds .
.
.
ours all seeds .
.
.
in the refinement step.
what we measure is the impact of the two refinement heuristics on the performance either using only one of them or combine them together.
the domain adaptation also comprises of only seeding and adversarial learning.
after adversarial learning we use different combinations of refinement heuristics to measure the effect of each heuristic.
we use the ground truth mappings from java2cssharp as the test set.
the results in table show that taking the intersection between the top k frequency and the cosine threshold heuristic results in the best performance.
this implies that the cosine threshold has an effect to filter out poor top k frequency synthetic seeds thus making the refinement better in overall.
.
.
rq3 effect of overlapping seeds.
as mentioned in section .
we found seeds from the signature based matching heuristic as the training data however of them overlap with the ground truth set.
these overlapping seeds could affect the final performance even though these seeds are part of our endto end approach.
to analyze the effect of these overlapping seeds we also remove them from the training data using the remaining seeds to evaluate.
comparing the results to those using all seeds as the training data table shows that accuracy suffers a few percents but the overall score is still fairly good.
.
.
rq4 effect of each component.
we performed an ablation study of domain adaptation to measure the performance of individual components as well as their combinations .
note that for the refinement component since section .
.
shows that using the intersection of top k and cosine threshold leads to better results than union we refer refinement to those of intersection of top k and cosine performance.
here are some observations from the results seeding is the most important step for the domain adaptation to works well e.g even with a small set of seeds which is a very small knowledge it sets up a basis for the adversarial learning to improve the performance significantly.sar learning cross language api mappings with little knowledge icse nier may june gothenburg sweden table ablation study effects of each component baselines seeds top top top seeding adv25 .
.
.
.
.
.
.
.
.
.
.
.
seeding refine25 .
.
.
.
.
.
.
.
.
.
.
.
seeding25 .
.
.
.
.
.
.
.
.
.
.
.
refine .
.
.
adv refine .
.
.
adv .
.
.
adversarial learning is essential in improving the performance e.g comparing seeding with seeding adv the top accuracy is improved by on average.
without the adversarial learning either the seeding or the refinement step alone or combine these two together does not achieve good performance.
refinement alone does not achieve any good result because the initial input matrix was completely random that cannot be refined to anything better using the adversarial learning alone achieves some reasonable results e.g top top .
further with the refinement step top improves to top becomes .
these can be seen as the results of unsupervised domain adaptation without any initial seeds.
.
explainability analysis of the results we performed various explainability analyses of our model in varying configurations to obtain some insights about our method.
from the results we show that our approach performs significantly better than api2api in every perspective.
an interesting question one may ask is why does this approach perform better than api2api ?
.
although theoretically adversarial learning maximizes the similarity between two distributions it is still useful to explain this phenomenon using analysis of the results.
.
.
effect of refinement on frequent vs rare tokens.
we note that the frequency of an api token could affect the quality of the mapping result i.e more frequent tokens could affect performance more than the less frequent ones.
with this assumption the refinement of the mapping matrix tries to improve the mapping by using frequent tokens as the anchor.
to measure the effect of the refinement on the frequent tokens and rare tokens we ranked the ground truth mappings in java2csharp by the frequency of the source apis i.e.
the java jdk apis.
then we use our model to produce the mapping results against the top which is a subset of frequent tokens and bottom which is a subset of rare tokens.
to ensure a fair comparison we use the non overlapping seeds in section .
.
to train the domain adaptation procedure.
the results in table show the following observations mapping accuracy decreases while increasing top k frequent tokens in the evaluation set in either setting.
this implies that token frequency does affect on the mapping result table effect of refinement on frequent vs. rare tokens baselines ground truth eval sizeaccuracy top top top with refinetop .
.
.
bottom .
.
.
without refinetop .
.
.
bottom .
.
.
the refinement step can improve the result of both the frequent tokens and rare tokens although the impact is bigger on frequent tokens e.g.
improved by for top and only for bottom .
.
.
retrieved results comparison.
to evaluate our approach qualitatively we retrieved c api methods from sample queries in java sdk.
table shows the resulting top c apis for four queries java.util .collection .add java.io.file .exists javax .swin .text .jtextcomponent .setcaretposition and java.util .concurrent .atomic .atomicinte er.
etanddecrement .
they are ordered by increasing difficulty in finding a mapping.
for the first query we can see that both api2api and our approach can successfully select the correct top mapping the other results are also related.
this case can be considered as easy for both approaches to performing well.
for the second query both approaches can achieve a good exact mapping but for the other results our approach can generalize all of the results under the system .io .file class while there are some less related results in the top produced by api2api e.g system .w eb .errorformatter .resolvehttpfilename .
the third query token ranks the 204th in the embedding table3.
as discussed earlier embedding quality of rare tokens is not as good as those of frequent tokens.
therefore it is more difficult to find an exact mapping for such a query.
even so our approach can still rank a correct mapping at the third place system .w indows .controls .richtextbox .caretposition while api2api produce totally unrelated results.
for the last query even though there has no mapping in c by the ground truth the retrieved results are still reasonably close.
the query in this case is an api for an atomic operation which is related to thread handling.
our approach can generalize the result mappings to the system .threadin apis in c while the results from api2api are totally unrelated.
this experiment shows that adversarial learning can maximize the similarity between the two distributions so that similar apis are clustered together.
.
.
api clustering ability.
we perform an additional analysis at the package level to show the ability of adversarial learning to cluster similar apis under the same package e.g.
java.io apis in java should be close to system .io apis in c .
to define the ground truth of the aligned packages we refer to the ground truth mappings from java2csharp datasets when more than of apis in a package in one language have the corresponding of apis in another language we say that these two packages are aligned.
for example we say that the package java.io and system .io are aligned because of java.io apis in java is aligned with some system .io apis in c .
in total we can derive pairs of aligned 3the order of the token embedding provided by word2vec is proportional to the frequency of the token icse nier may june gothenburg sweden nghi et al.
table retrieved api mapping results from sample queries produced by sar and api2api.
sar api2api java.util.collection.add system.collections.objectmodel.collection.add system.collections.generic.list.add system.collections.generic.list.add system.collections.generic.list.get system.collections.objectmodel.collection.clear system.collections.generic.list.remove system.collections.generic.list.contains system.collections.objectmodel.collection.add system.collections.generic.dictionary.add system.collections.idictionary.getenumerator javax.swing.text.jtextcomponent.setcaretposition system.windows.controls.richtextbox.clip system.drawing.image.getframecount system.web.ui.webcontrols.datagrid.pagesize system.media.soundplayer.playsync system.windows.controls.richtextbox.caretposition system.web.ui.webcontrols.calendar.weekenddaystyle system.windows.forms.contextmenustrip.suspendlayout system.configuration.xmlutil.strictskiptonextelement system.windows.controls.richtextbox.caretbrush system.media.soundplayer.playlooping java.io.file.exists system.io.file.exists system.io.file.exists system.io.file.appendtext system.web.errorformatter.resolvehttpfilename system.io.file.delete system.io.file.openread system.io.fileinfo.lastwritetime system.io.compression.zipfile.openread system.io.file.getattributes system.io.compression.zipfile.extracttodirectory java.util.concurrent.atomic.atomicinteger.getanddecrement system.threading.interlocked.decrement system.directoryservices.searchresultcollection.getenumerator system.threading.readerwriterlockslim.enterwritelock system.directoryservices.searchresultcollection.dispose system.threading.interlocked.increment system.runtime.serialization.objectidgenerator.hasid system.threading.eventwaithandle.openexisting system.collections.generic.queue.copyto table average cosine similarity comparison baselinesaverage score per package java.io java.math java.net java.sql java.util api2api .
.
.
.
.
sar .
.
.
.
.
packages java.io system .io java.math system .math java.net system .net java.sql system.data.sqlclient and java.util system .collections .generic .
using these data we first compute the pairwise cosine similarity scores of all pairs of apis under each pair aligned packages then take the average of the scores.
we do this for both api2api and our approach.
table shows that the average scores produced by our approach are significantly better than those of api2api which implies that our approach has the ability to cluster the similar group of apis together.
threats to validity and limitations the goal of domain adaptation is to use as little knowledge as possible for any pair of languages.
however we only perform the experiments on java and c in this paper because it is not easy to find a good and large enough evaluation dataset for other pairs of languages.
we leave this task in the future.
while unsupervised adversarial learning method does not require any seed as parallel data there is a risk that the distributions of vectors embeddings in the two spaces are not so similar.
through our experiments it is confirmed that the performance could be improved further by initializing the unsupervised adversarial learning method with a small set of seeds taken from the seed based domain adaptation and by generating the rest of api mappings.
one limitation of our approach is that we can only generate single api mapping instead of an api sequence mapping.
both api2api and ours share such a limitation.
in api2api they use the new mappings mined from the tool as the input for an externalmachine translation tool phrasal to generate the mapping for api sequences.
in the future we can also feed the newly found mapping apis from our tool to phrasal as inputs.
we mainly use a simplified top k accuracy metric to measure our performance against the api2api.
in real world use cases other information retrieval based metrics such as map and mrr may have less bias in evaluating the list of api mappings.
we leave this for the future.
conclusion future work we have proposed a domain adaptation approach named sar to automatically transform and align the vector spaces used to represent two different languages and apis used therein.
we adapt code embedding and adversarial learning techniques with a seeding and refinement method to implement our approach.
the approach can identify api mappings across different programming languages.
our evaluation shows that the mappings between java and c apis identified by our approach can be more accurate than other approaches with just mapping seeds that can be easily identified by an automatic simple signature based heuristic and it helps to identify hundreds of more api mappings between java and c sdks.
domain adaptation methods are useful for other software engineering tasks that involve two different domains targeted by transferred learning such as cross language program classification code summarization cross language project bug prediction.
these tasks may benefit from the proposed approach when little curated data is available.
other se tasks that are challenging due to lack of data such as the out of vocabulary oov problem for learning and modeling fast evolving software code may also benefit from our domain adaptation approach because the embeddings of oov words may be approximated on the fly by adapting the known embeddings of their contextual or similar words in different languages.
in the future we will explore thesesar learning cross language api mappings with little knowledge icse nier may june gothenburg sweden variants of applications.