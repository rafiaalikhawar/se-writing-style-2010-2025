themis automatically testing software for discrimination rico angell brittany johnson yuriy brun and alexandra meliou university of massachusetts amherst amherst massachusetts usa rangell bjohnson brun ameli cs.umass.edu abstract bias in decisions made by modern software is becoming a common and serious problem.
we present themis an automated test suite generator to measure two types of discrimination including causal relationships between sensitive inputs and program behavior.
we explain how themis can measure discrimination and aid its debugging describe a set of optimizations themis uses to reduce test suite size and demonstrate themis effectiveness on open source software.
themis is open source and all our evaluation data are available at see a video of themis in action ccs concepts software and its engineering software testing and debugging keywords software fairness discrimination testing fairness testing software bias testing themis automated test generation acm reference format rico angell brittany johnson yuriy brun and alexandra meliou.
.
themis automatically testing software for discrimination.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction software plays an important role in making decisions that shape our society.
software decides what products we are led to buy who gets financial loans what a self driving car does which may lead to property damage or human injury how medical patients are diagnosed and treated and who gets bail and which criminal sentence .
unfortunately there are countless examples of bias in software.
translation engines inject societal biases e.g.
she is a doctor translated into turkish and back into english becomes he is a doctor .
youtube is more accurate when automatically generating closed captions for videos with male than female voices .
facial recognition systems often underperform on female and black faces .
in amazon permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
decided not to offer same day delivery to predominantly minority neighborhoods .
and the software us courts use to assess the risk of a criminal repeating a crime exhibits racial bias .
bias in software can come from learning from biased data implementation bugs design decisions unexpected component interactions or societal phenomena.
thus software discrimination is a challenging problem and addressing it is integral to the entire software development cycle from requirements elicitation to architectural design to testing verification and validation .
even defining what it means for software to discriminate is not straightforward.
many definitions of algorithmic discrimination have emerged including the correlation or mutual information between inputs and outputs discrepancies in the fractions of inputs that produce a given output known as group discrimination or discrepancies in output probability distributions .
these definitions do not capture causality and can miss some forms of discrimination.
to address this our recent work developed a new measure called causal discrimination and described a technique for automated fairness test generation .
this tool demonstration paper implements that technique for the group and causal definitions of discrimination in a tool called themis v2.
building on an early prototype .
this paper focuses on the tool s architecture test suite generation workflow and efficiency optimizations section and its user interface section .
section places themis in the context of related research and section summarizes our contributions.
themis automated fairness test generation figure describes the themis architecture and fairness test suite generation workflow.
themis consists of four major components input generator cache error bound confidence calculator and discrimination score calculator.
themis uses the input schema of the system under test to generate test suites for group or causal discrimination.
themis generates values for non sensitive attributes uniformly randomly and then iterates over the values for sensitive attributes.
this process samples equivalence classes of test inputs.
themis later uses the system s under test behavior on these sampled subsets of the equivalence classes to compute the discrimination score.
using a cache to ensure no test is executed multiple times themis executes the system under test on the generated tests.
themis iterates this test generation process generating more tests within the equivalence classes until the confidence in the error bound satisfies the user specified threshold and then outputs the discrimination score.
the final confidence bound is within the specified threshold though themis can also produce its exact measure.
themis focuses on two measures of discrimination group and causal.
to help explain the two measures consider a simple loan esec fse november lake buena vista fl usa rico angell brittany johnson yuriy brun and alexandra meliou themis user interfacethemis engineinput generator inputs input schema type of discrimination to measure acceptable error bound desired confidence bound outputs discrimination score confidence in error bound system under test i odatabasecacheerror bound confidence calculatorno generate more test inputsto reach confidenceconfidence in error bound reached?yesdiscriminationscorecalculatorequivalence classesinput spacenon sensitiveattributegeneratorsensitiveattributeiteratorsensitiveattributeassignmentnon sensitiveattributeassignmentsubsets of equivalence classes of test inputs figure the themis architecture and fairness test suite generation workflow.
program that decides if loan applicants should be given loans.
loan inputs are each applicant s name age bracket 40or race green purple income bracket savings employment status and requested loan amount the output is approve or deny .
group discrimination is the maximum difference in the fractions of software outputs for each sensitive input group.
for example loan s group discrimination with respect to race compares the fractions of green andpurple applicants who get loans.
if ofgreen and of purple applicants get loans then loan s group discrimination with respect to race is .
with more than two races the measure would be the difference between the largest and smallest fractions.
group discrimination with respect to multiple input attributes compares the crossproduct of the attributes e.g.
for race and age bracket there are four groups and .
software testing enables a unique opportunity to conduct hypothesis testing to determine statistical causation between inputs and outputs.
it is possible to execute loan on two individuals identical in every way except race to verify if the race causes an output change.
causal discrimination is the frequency with which equivalences classes of inputs recall figure contain at least two inputs on which the software under test produces different outputs.
for causal discrimination each equivalence class contains inputs with identical non sensitive attribute values but varied sensitive attribute values.
for example loan s causal discrimination with respect to age and race is the fraction of equivalence classes that contain a pair of individuals with identical name income savings employment status and requested loan amount but different race or age for which loan approves a loan for one but not the other.
exhaustively testing software can measure its group and causal discrimination but it is infeasible in practice.
the rest of this section describes three optimizations previously proved sound themis uses to reduce test suite size.
applying these optimizations to real world software see section reduced test suite sizes by on average times for group discrimination and times for causal discrimination .
the more software discriminates the greater the reduction in test suite size.
sound pruning.
the number of possible executions grows exponentially with the number of input attributes being tested for discrimination.
however the group and causal discrimination definitions are monotonic if software discriminates over threshold with respect to a set of attributes x then the software also discriminates over with respect to all supersets of x see theorems .
and .
and their proofs in .
this allows themis to prune its test input space.
once themis discovers that software discriminates against x it can prune testing all supersets of x. further causal discrimination always exceeds group discrimination with respect to the same set of attributes see theorem .
and its proof in so themis can prune its test input space when measuring both kinds of discrimination if software group discriminates with respect to a set of attributes it must causally discriminate with respect to that set at least as much.
these observations and their formal proofs allow themis to employ a provably sound pruning strategy algorithm in .
adaptive sampling.
themis approximates group and causal discrimination scores through sampling done via iterative test generation recall figure .
sampling in themis is adaptive using the ongoing score computation to determine if a specified bound of error with a desired confidence level has been reached.
themis generates inputs uniformly at random using an input schema and maintains the proportion of samples evidencing discrimination computing the bound of error for that proportion.
test caching.
themis may generate repetitive tests tests relevant to group discrimination are also relevant to causal discrimination and tests relevant to one set of attributes can also be relevant to another set.
this redundancy in fairness testing allows themis to exploit caching to reuse test results without re executing tests.
872themis automatically testing software for discrimination esec fse november lake buena vista fl usa using themis to discover and debug discrimination themis is a standalone application written in python.
themis is open source this paper describes themis version .
.
this section uses a simple example loan implementation figure 2a to demonstrate themis.
themis automatically generates tests to detect and measure group and causal discrimination.
themis inputs are a path to the executable for the software to be tested an input schema and what to measure.
to test loan for discrimination the user specifies the loan input schema either via themis gui figure 2b or via a configuration file.
the input scema includes every input attribute to the software to be tested and the range of values it can take on e.g.
sex male female .
currently themis handles categorical inputs such as race income brackets etc.
to specify what to measure the user selects group or causal discrimination or both a set of input attributes with respect to which to test an acceptable error bound the desired confidence in that bound and a maximum acceptable discrimination threshold.
themis allows saving and loading these settings for later use.
themis generates and executes a test suite to perform the specified measurements.
if it finds discrimination the user can choose to explore that discrimination further.
themis displays the details of observed group and causal discrimination differently.
figure 2c shows the group discrimination details themis found for loan when asked to measure discrimination with respect to sex race and income.
loan group discriminates with respect to income .
of the time above the .
threshold loan approved .
of loan applicants who make between and and of applicants who make more than .
as this example illustrates not all discrimination may be undesirable.
approving loans based on income may very well be a desirable behavior.
it is up to the themis user to decide which input attributes need to be tested for discrimination.
figure 2d shows the causal discrimination details themis found forloan .
in addition to the discrimination similar to the group measurements for income themis also found .
causal discrimination with respect to race.
themis lists the tests that exhibit the causal discrimination.
for example loan approves a loan for a male orange candidate with income between and but denies a loan for a blue candidate who is otherwise identical.
these causal pairs allow investigating the source of discrimination by tracing through the executions on two or more relevant inputs that exhibit the different behavior and to potentially debug the problem.
as themis implements an existing automated fairness test suite generation technique we now briefly summarize the earlier evaluation of that technique.
evaluated on a benchmark of eight open source software systems and two financial datasets available at automated fairness test suite generation was effective at discovering both group and causal discrimination.
causal discrimination sometimes captured bias that group discrimination missed.
for example one of the discriminationaware decision tree implementations trained not to discriminate with respect to gender exhibited causal gender discrimination of .
.
learning based systems can find ways to discriminate1 if race is green or orange if income approve else deny else if income deny else approve a sample loan implementation.
b loan input schema.
c themis view of group discrimination results.
d themis view of causal discrimination results.
figure themis tests a loan implementation a using an input schema b and finds group c and causal d discrimination.
873esec fse november lake buena vista fl usa rico angell brittany johnson yuriy brun and alexandra meliou against certain individuals in one way and certain other individuals in another way such that those two discrimination instances cancel out with respect to group discrimination.
causal discrimination however properly captures such bias.
sometimes avoiding discrimination against one attribute may increase discrimination against another.
for example the evaluation showed that training not to discriminate with respect to gender can lead to a significant increase in discrimination against race.
forcing additional constraints on machine learning may have unexpected consequences making themis discrimination testing even more important.
themis optimizations recall section reduced test suite sizes by on average times for group discrimination and times for causal discrimination .
related work discrimination shows up in many software applications e.g.
advertisements hotel bookings and image search .
meanwhile software is entering domains in which discrimination could result in serious negative consequences including criminal justice finance and hiring .
software discrimination may occur unintentionally e.g.
as a result of implementation bugs as an unintended property of self organizing systems as an emergent property of component interaction as conflicting logic from multiple developers changes or as an automatically learned property from biased data .
themis focuses on two measures of discrimination group and causal.
group discrimination is a generalization of the caldersverwer cv score used frequently in prior work on algorithmic fairness particularly in the context of fair machine learning .
many other definitions exist.
the group discrimination definition can be generalized to rich subgroups .
another defintion defines discrimination by observing that a better input is never deprived of the better output .
that definition requires a domain expert to create a distance function for comparing inputs.
causal discrimination which themis measures goes beyond prior work by measuring causality .
fairness in machine learning research is similarly moving toward causal measures of discrimination e.g.
counterfactual fairness .
fairml uses orthogonal projection to co perturb attributes which can mask some discrimination but find discrimination that is more likely to be observed in real world scenarios.
fairtest uses manually written tests to measure four kinds of discrimination scores the cv score and a related ratio mutual information pearson correlation and a regression between the output and sensitive inputs.
by contrast themis generates tests automatically and also measures causal discrimination.
reducing discrimination in machine learning classifiers and selection algorithms is important work that is complementary to ours.
we focus on measuring discrimination via software testing not developing methods for removing it.
themis can be used to manually debug discrimination bugs and thus remove discrimination.
work on formal verification of non discrimination is similarly complementary to our testing approach.causal relationships in data management systems can help explain query results and debug errors by tracking and using data provenance .
for software systems that use data management such provenance based reasoning may aid testing for causal relationships between input attributes and outputs.
our prior work on testing software that relies on data management systems has focused on data errors whereas this work focuses on testing fairness.
unlike other automated test generation tools e.g.
randoop and evosuite themis processes test results to compute discrimination scores.
prior tools are not designed for measuring discrimination instead satisfying testing goals such as maximizing coverage.
this leads to diverse test suites .
by contrast e.g.
to measure causal discrimination themis has to generate pairs of similar not diverse inputs unlikely to be produced by other tools.
contributions we have demonstrated themis an open source implementation of an automated test generation technique for testing software for causal and group discrimination.
themis employs three optimizations which significantly reduce test suite size.
such test suite generation is effective at finding discrimination in open source software.
overall themis is the first automated test generator for discrimination testing and serves both as a useful tool for practitioners and a baseline for future research.
acknowledgment this work is supported by the national science foundation under grants no.
ccf iis cns and ccf1763423.