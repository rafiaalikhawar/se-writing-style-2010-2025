malscan fast market wide mobile malware scanning by social network centrality analysis y ueming wu xiaodi li deqing zou wei yang xin zhang hai jin national engineering research center for big data technology and system cluster and grid computing lab services computing technology and system lab big data security engineering research center school of computer science and technology huazhong university of science and technology wuhan china school of cyber science and engineering huazhong university of science and technology wuhan china shenzhen huazhong university of science and technology research institute university of texas at dallas abstract malware scanning of an app market is expected to be scalable and effective.
however existing approaches use either syntax based features which can be evaded by transformation attacks or semantic based features which are usually extracted by performing expensive program analysis.
therefor in this paper we propose a lightweight graph based approach to perform android malware detection.
instead of traditional heavyweight static analysis we treat function call graphs of apps as social networks and perform social network based centrality analysis to represent the semantic features of the graphs.
our key insight is that centrality provides a succinct and faulttolerant representation of graph semantics especially for graphs with certain amount of inaccurate information e.g.
inaccurate call graphs .
we implement a prototype system malscan and evaluate it on datasets of benign samples and malicious samples.
experimental results show that malscan is capable of detecting android malware with up to accuracy under one second which is more than times faster than two state of the art approaches namely mamadroid and drebin .
we also demonstrate the feasibility of malscan on market wide malware scanning by performing a statistical study on over million apps.
finally in a corpus of dataset collected from googleplay app market malscan is able to identify zero day malware including malware samples that can evade detection of existing tools.
index t erms lightweight feature android malware api centrality market wide i. i ntroduction the explosive growth of android devices and applications app for short has brought in several android markets and spurred the growth of android malware.
millions of apps have been installed by android users around the world from various app markets.
up to the end of the third quarter of the number of new malware apps had an increase of over percent compared to the same period last year1.
stopping the spread of malware primarily relies on the effort from the app markets since they are the first place to provide installations of android apps.
therefor market wide mobile malware scanning can be the primary task to prevent the fast malware spreading.
malware scanning depends on the techniques for detecting android malware.
existing mobile malware detection approaches extract program features to distinguish benign apps from malware.
however many of the techniques can be easily evaded by obfuscation because these techniques are lack of semantic and contextual information of the program behaviors.
to overcome the challenge several systems have been proposed to focus on distilling an app s program semantics into a graph representation and detecting malware by matching the graphs.
nevertheless these graphbased techniques such as droidsift and apposcopy have two main limitations.
first graph matching is typically time consuming because a graph often contains thousands of nodes.
for instance the average running time of droidsift and apposcopy to analyze an app is .
seconds and seconds respectively.
additionally these graph based systems conduct graph matching based on similarity to graphs of existing malware making the systems perform poorly on new malware instances due to the constant evolution of android malware .
therefor these graph based techniques are not scalable and realistic to complete market wide malware scanning.
to further enhance the state of the art techniques approaches such as mamadroid tried to use coarsergranularity information e.g.
using package level information instead of method level information and divide graphs into subgraphs e.g.
using multiple pairwise invocation relationships instead of a call graph representing all invocation relationships .
it has validated the robustness against android malware evolution and increase the lifetime of the trained model.
however the average running time of an app is .
seconds table viii and figure which is not applicable for malware scanning of an app market.
additionally the subgraphs e.g.
the pairwise invocations cannot fully reflect dependencies among method calls thus the approaches are lack of key information to distinguish some of the malicious apps from benign ones.
moreover such approach requires a sizable amount of memory when performing classification because of its large feature sets by extracting all pairwise invocations .
another state of the art technique for malware 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
scanning is massv et which abstracts the uis of an app as a directed graph where each node is a view and each edge is a relationship description.
it has validated the high efficiency on malware scanning however it can only detect repackaged malware and can cause a false negative when the app is a new malware.
to address these limitations in this paper we propose malscan a semantic preserving market wide malware scanning system that can accurately detect a malicious android app in at least .
seconds and at most .
seconds on average table viii and figure .
to preserve the program semantic information while perform fast processing on all the information we regard the function call graph as a complex social network and perform centrality analysis to represent the semantic features of the graph.
both call graphs and social networks are static graphs representing dynamic behaviors.
call graphs represent the program behaviors and each edge of a call graph may represent multiple function invocations.
social networks represent the social behaviors and each edge of a social network may represent multiple social communications.
centrality measures the importance of a function in the whole function call graph and reflects the structural attribute properties of the graph.
because of its succinct representation centrality works well on graphs with inaccurate information making centrality a perfect candidate to represent graphs obtained from low cost program analysis e.g.
context and flow insensitive analysis .
by performing centrality analysis on each sensitive api method malscan provides a balance between abstracting graph details to defend against obfuscation and preserving semantic information to distinguish between malware and benign apps.
we evaluate malscan from seven perspectives over malware spanning from january to december to better observe the robustness of malscan against the evolution in android malware.
our first four experiments focus on the effectiveness of malware detection robustness against evolution of android apps robustness against adversarial attack and runtime overheads of malscan .
in addition we conduct a statistical study to investigate the scalability of malscan on real world app market e.g.
google play .
we totally crawl over million apps information and the analysis result suggests that malscan is capable of scanning malware on googleplay app market.
furthermore we utilize and combine the different centrality experimental results on malware detection.
the experimental result indicates that it can indeed improve the effectiveness on android app classification.
finally we demonstrate the capability of malscan in detecting real world zero day malware.
specifically in a corpus of apps collected from google play app market malscan is able to identify zero day malware including malware samples that can evade detection of existing tools .
in summary this paper makes the following contributions we propose a novel lightweight method to perform classification on android malware by analyzing the centralityof sensitive api calls within a function call graph of an app.
we design and implement a prototype system malscan a novel automatic and efficient system that can perform classification on large scale android malware with high accuracy.
we conduct a comprehensive evaluation using benign samples and malicious samples.
experimental results show that malscan can complete the classification of an app in .
seconds on average with up to accuracy.
paper organization.
the remainder of the paper is organized as follows.
section ii presents the preliminary study on degree centrality of android apps.
section iii introduces our system.
section iv reports the experimental results.
section v discusses the future work.
section vi describes the related work.
section vii concludes the present paper.
ii.
p reliminary study on centrality a social network is a social structure made up of a set of social actors such as individuals or organizations sets of dyadic ties and other social interactions between actors2.
the source code of an app is made up of a set of functions and there are several call relationships between them.
if we regard functions as actors and the call relationships as social interactions the function call graph of an app can be regarded as a social network.
centrality concepts were first developed in social network analysis which quantify the importance of a node in the network and have the potential to unveil the structural pattern of the network.
centrality measures are very useful for network analysis and much work has been proposed to use centrality measures in different areas such as biological network co authorship network transportation network criminal network affiliation network .
there have been proposed many different centrality measures such as degree centrality katz centrality closeness centrality harmonic centrality betweenness centrality percolation centrality cross clique centrality dissimilaritybased centrality .
on the one hand centrality measures can indicate the importance of a node within a network and have the potential to unveil the structural patterns and behaviors.
on the other hand malware usually invokes sensitive api calls to perform malicious activities.
therefor we perform a preliminary study to investigate a question can the centrality of sensitive api calls reflect the difference between benign apps and malicious apps?
in other words is there a significant difference between the centrality of sensitive api calls in benign apps and malicious apps?
to answer the proposed question we first randomly select benign apps and malicious apps from androzoo then the call graphs are extracted by using static analysis.
given a call graph we perform centrality analysis only for network.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i summary of results of one way anova with .
for degree centrality values of sensitive api call connectivitymanager .getactivenetworkinfo in benign apps and malicious apps.
ss sum of squares df degree of freedom ms mean square f calculated f value p calculated p value f crit critical value of f anov a single factor summary groups count sum average variance degree centrality in benign apps .
.
.08e degree centrality in malicious apps .
.
.51e anov a source of variance ss df ms f p value f crit between groups .
.
.
.3e .
within groups .
.8e total .
s66 s101 s110 s105 s103 s110 s77 s97 s108 s119 s97 s114 s101 s45 s48 s46 s48 s48 s49 s48 s46 s48 s48 s48 s48 s46 s48 s48 s49 s48 s46 s48 s48 s50 s48 s46 s48 s48 s51 s48 s46 s48 s48 s52 s48 s46 s48 s48 s53 s68 s101 s103 s114 s101 s101 s67 s101 s110 s116 s114 s97 s108 s105 s116 s121 a api connectivitymanager.getactivenetworkinfo s66 s101 s110 s105 s103 s110 s77 s97 s108 s119 s97 s114 s101 s48 s46 s48 s48 s48 s48 s46 s48 s48 s50 s48 s46 s48 s48 s52 s48 s46 s48 s48 s54 s48 s46 s48 s48 s56 s68 s101 s103 s114 s101 s101 s67 s101 s110 s116 s114 s97 s108 s105 s116 s121 b api linearlayout.init s66 s101 s110 s105 s103 s110 s77 s97 s108 s119 s97 s114 s101 s45 s48 s46 s48 s48 s49 s48 s46 s48 s48 s48 s48 s46 s48 s48 s49 s48 s46 s48 s48 s50 s48 s46 s48 s48 s51 s48 s46 s48 s48 s52 s48 s46 s48 s48 s53 s48 s46 s48 s48 s54 s68 s101 s103 s114 s101 s101 s67 s101 s110 s116 s114 s97 s108 s105 s116 s121 c api textview.init s66 s101 s110 s105 s103 s110 s77 s97 s108 s119 s97 s114 s101 s45 s48 s46 s48 s48 s48 s53 s48 s46 s48 s48 s48 s48 s48 s46 s48 s48 s48 s53 s48 s46 s48 s48 s49 s48 s48 s46 s48 s48 s49 s53 s48 s46 s48 s48 s50 s48 s48 s46 s48 s48 s50 s53 s68 s101 s103 s114 s101 s101 s67 s101 s110 s116 s114 s97 s108 s105 s116 s121 d api activity.finish fig.
the degree centrality distributions of sensitive api calls between benign apps and malicious apps the node representing security sensitive methods by using a list of security sensitive methods .
we then conduct a preliminary frequency analysis to check whether centrality can indeed suggest the inherent differences between benign apps and malicious apps.
we select the top frequentlyinvoked sensitive api calls as the test object.
due to the limitation of the page we only show a portion of our results in figure .
from the results presented in figure we find that the degree centrality of sensitive api calls can be considerably different between benign apps and malicious apps.
to obtain more determinate results we apply analysis of v ariance anova to research the difference of these centrality values between benign apps and malicious apps.
here null hypothesis h0is that the centrality of sensitive api calls between benign apps and malicious apps are similar and there is no significant difference in their means.
we apply oneway anova to test whether we can reject or accept the null hypothesis.
p value is the probability when the null hypothesis h0is true after performing statistical test with a pre determined probability we select .
in our tests .
if the calculatedtable ii p values of degree centrality in benign apps and malicious apps in figure .
api connectivitymanager .getactivenetworkinfo a p i linearlayout.init a p i textview.init api activity.finish api call api api api api p value .3e .20667e .05623e .9192e p value is below then the null hypothesis h0is rejected.
table i depicts the summary results by performing one way anova on the centrality values in figure a .
as shown in table i the average value of degree centrality of connectivitymanager .getactivenetworkinfo in malicious apps is .
while is .
in benign apps.
particularly the pvalue in table i is .3e which is extremely less than .
by this we can reject the hypotheses h0.
in other words the alternate hypothesis h1 i.e.
the difference of degree centrality ofconnectivitymanager .getactivenetworkinfo between benign apps and malicious apps is significant is accepted.
we also apply one way anova on the other dataset in figure .
however due to the limited page we only show the p values in table ii3.
from the results shown in table ii all the p values are extremely less than .
which indicates that the degree centrality of sensitive api calls can reflect the inherent difference between benign apps and malicious apps.
therefor based on the observation we build a model and propose a lightweight android malware detection system by analyzing the centrality of sensitive api calls within a call graph.
iii.
s ystem architecture a. system overview as shown in figure malscan s operation goes through three main phases static analysis centrality analysis and classification .
static analysis this phase aims at extracting the function call graph of an app based on static analysis where each node is a function that can be an api call or a user defined function.
centrality analysis after obtaining the call graph of an app we then compute the centrality of sensitive api calls 3the summary results of figure b to d are available in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
g41 g88 g81 g70 g87 g76 g82 g81 g3 g38 g68 g79 g79 g3 g42 g85 g68 g83 g75 g74 g72 g87 g39 g72 g89 g76 g70 g72 g44 g39 g11 g12 g29 g3 g78 g11 g3 g86 g20 g3 g15 g86 g21 g15 g15 g74 g72 g87 g39 g72 g89 g76 g70 g72 g44 g39 g15 g15 g86 g81 g3 g12 g11 g3 g19 g3 g15 g19 g3 g15 g15 g3 g78 g3 g15 g15 g19 g3 g12 g54 g72 g81 g86 g76 g87 g76 g89 g72 g3 g36 g51 g44 g86 g41 g72 g68 g87 g88 g85 g72 g3 g57 g72 g70 g87 g82 g85 g3 g38 g72 g81 g87 g85 g68 g79 g76 g87 g92 g38 g79 g68 g86 g86 g76 g73 g76 g72 g85 g54 g87 g68 g87 g76 g70 g3 g36 g81 g68 g79 g92 g86 g76 g86 g38 g72 g81 g87 g85 g68 g79 g76 g87 g92 g3 g36 g81 g68 g79 g92 g86 g76 g86 g38 g79 g68 g86 g86 g76 g73 g76 g70 g68 g87 g76 g82 g81 g50 g53 fig.
system architecture of malscan d1 d2 d3 d4 degree centrality k1 k2 k3 k4 katz centrality c1 c2 c3 c4 closeness centrality h1 h2 h3 h4 harmonic centrality d1 d2 d3 d4 k1 k2 k3 k4 c1 c2 c3 c4 h1 h2 h3 h4 concatenate centrality d1 k1 c1 h1 d k2 c2 h2 d k3 c3 h3 d k4 c4 h4 average centralityfeature vectors of four individual centralities feature vectors of added two centralities fig.
illustration of the construction of average centrality and concatenate centrality within the graph.
the output of this phase is the feature vector.
classification in the final phase given the feature vector we can accurately and efficiently classify the app as either benign or malicious by using a machining learning classifier.
b. static analysis and centrality analysis in this paper we aim at proposing a graph based marketwide malware scanning system which requires high efficiency on app processing and graph analysis.
therefor we conduct low cost program analysis e.g.
context and flow insensitive analysis to extract succinct function call graphs based on an android reverse engineering tool androguard .
as api calls are used by the android apps to access operating system functionality and system resources they can be used as representations of the behaviours of android apps.
particularly android malware usually invokes some security related api calls to perform malicious activities.
for instance getdeviceid can get your phone s imei and getline1number can obtain your phone number.
therefor in order to characterize malicious behaviors we focus on these security related api calls namely sensitive api calls on the basis of the results reported by pscout which consist of sensitive api calls.
in centrality analysis we pay attention to extract the centrality of sensitive api calls.
there have been proposed several definitions of centrality in a social network for example degree centrality of a node is the fraction of nodes it is connected to.
the degree centrality values are normalized by dividing by the maximum possible degree in a simple graph n where n is the number of nodes in the graph.
cd v deg v n note that deg v is the degree of node v. katz centrality is a generalization of degree centrality.
degree centrality measures the number of direct neighbors and katz centrality measures the number of all nodes that can be connected through a path while the contributions of distant nodes are penalized.
let abe the adjacency matrix of a graph under consideration.
ck i summationtext k 1n summationtext j 1 ak ij note that the above definition uses the fact that the element at location i j ofakreflects the total number ofkdegree connections between nodes iandj.
the value of the attenuation factor has to be chosen such that it is smaller than the reciprocal of the absolute value of the largest eigenvalue of a. closeness centrality of a node is the average length of the shortest path between the node and all other nodes in the graph.
its normalized form is generally given by the previous value multiplied by n where nis the number of nodes in the graph.
cc v n summationtext yd t v note that d t v is the distance between nodes vandt.
harmonic centrality reverses the sum and reciprocal operations in the definition of closeness centrality.
ch v summationtext t negationslash v1 d t v n note that d t v is the distance between nodes vandt andnis the number of nodes in the graph.
betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes.
cb v summationtext s negationslash v negationslash t epsilon1v st v st note that stis total number of shortest paths from node sto nodetand st v is the number of those paths that pass through v. the betweenness may be normalized by dividing through the number of pairs of nodes not including v which for directed graphs is n n and for undirected graphs is n n where n is the number of nodes in the graph.
given a call graph we then compute the centrality of sensitive api calls.
some sensitive api calls that are not contained in this call graph are represented as in the feature vector.
we select total four different centrality measures which are degree centrality katz centrality closeness centrality and harmonic centrality to commence our experiments.
we exclude betweenness centrality due to the low efficiency on centrality analysis it requires about seconds to complete extracting the centrality of a call graph while the number of nodes is .
additionally it is generally to measure authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the importance of a vertex in a social network by combining multiple centrality measures.
therefor we construct another two types of centrality by integrating the four individual centrality measures the one is to calculate the average value of the former four centrality measures average centrality and the other is to concatenate the former four centrality measures concatenate centrality .
figure shows the construction of these two types of centrality.
in our experiments the dimension of the feature vector of four individual centrality measures is the total number of sensitive api calls reported in pscout which is .
therefor as shown in figure the dimension of the feature vector of average centrality and concatenate centrality is and respectively.
c. classification our final phase focuses on classification i.e.
labeling apps as either benign or malicious.
to this end we select three different classification algorithms nearest neighbor nn nearest neighbor nn and random forest to complete classification.
these three classifiers are implemented by using a python library scikit learn .
for the random forest we adopt the default parameters to commence our experiments4.
each model is trained by using feature vectors obtained from a training dataset and then performing classification on a testing dataset.
all the experimental results are presented in section iv by performing fold cross validations on our datasets.
iv .
e xperimental ev alua tion in this section we conduct seven experiments to check malscan s capability on detecting android malware.
specifically we first evaluate the effectiveness of malscan by classifying datasets that are developed during the same year.
then we examine the robustness of malscan in two ways the first is to classify newer samples by models trained on old datasets the second is to detect adversarial samples crafted by adversarial attack.
next we present the runtime performance ofmalscan .
then we validate the feasibility of malscan on market wide malware scanning.
finally we introduce the combination of different centrality measures of malscan on malware scanning and demonstrate the ability on detecting zero day malware.
a. datasets and metrics datasets used to evaluate malscan include total samples which are available in github5 by this researchers can conduct reproducible experiments.
we crawled these apk files from androzoo which currently contains over nine millions apk files and each of which has been detected by several different antivirus products in virustotal .
our final datasets include benign apps and malicious apps.
in addition the time period of our datasets ranges from january to december by this we can conduct a more detailed evaluation to verify how robust malscan is to 4more detailed information of parameters are available in the official website iii summary of datasets used in our experiments dataset benign malware total average size mb .
.
.
.
.
.
.
.
total .
table iv descriptions of used metrics in our experiments metrics abbr definition true positive tp samples correctly classified as malicious true negative tn samples correctly classified as benign false positive fp samples incorrectly classified as malicious false negative fn samples incorrectly classified as benign true positive rate tpr tp tp fn false negative rate fnr fn tp fn true negative rate tnr tn tn fp false positive rate fpr fp tn fp accuracy a tp tn tp tn fp fn precision p tp tp fp recall r tp tp fn f measure f1 p r p r changes in android apps.
table iii lists the summary of our datasets.
to evaluate malscan we conduct experiments by performing fold cross validations using the datasets.
additionally we use the widely used metrics as shown in table iv to measure the effectiveness of malscan .
note that all experiments are compared with following two state of the art android malware detection systems mamadroid and drebin .
mamadroid a state of the art android malware detection system which leverages the sequences of abstracted function calls obtained from a call graph to build a behavioral model and uses it to extract features to conduct classification.
drebin a state of the art android malware detection system which performs a broad static analysis for extracting as many features as possible from an app and embeds them in a joint vector space to classify malware.
b. detection effectiveness we first evaluate how well malscan on detecting malware by training and testing using samples that are developed in the same year.
to this end we conduct experiments on eight datasets as depicted in table iii by performing fold cross validations.
table v presents the detection results achieved by malscan mamadroid and drebin on each dataset respectively.
the results include f measure and accuracy for each experiment.
in order to verify the effectiveness of different centrality measures on detecting android malware we first conduct four individual centrality experiments.
additionally it is generally to measure the importance of a vertex in a social authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v experimental results of malscan mamadroid and drebin classification with datasets from the same year dataset metrics f1 a f1 a f1 a f1 a f1 a f1 a f1 a f1 a degree .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
closeness .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
harmonic .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
katz .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
concatenate .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mamadroid .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
drebin .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
network by combining multiple centrality metrics.
therefor we add another two experiments by integrating the former four individual centrality measures as shown in figure .
in a word we evaluate malscan by conducting six experiments on each dataset per year.
as shown in table v we see that for each dataset malscan can maintain a high f measure and accuracy above for all six experiments.
in addition the detection performances vary according to the selected centrality measures.
for instance the f measure is .
when we choose degree centrality to conduct classification on dataset while is .
when we select katz centrality.
this observation is mainly due to that the definitions are different between selected centrality measures.
particularly the results of concatenate centrality is generally better than other centralities which is because of the more comprehensive features in concatenate centrality figure .
compared to mamadroid malscan can achieve better performance on all datasets in terms of accuracy.
as for f measure it outperforms mamadroid on most of datasets except dataset.
the f measure is .
when we conduct classification by using degree centrality measure on dataset while mamadroid can achieve .
f measure.
however the maximum f measure and accuracy of six experiments in malscan is all above mamadroid .
such results indicate that malscan can obtain better performance than abstraction based approach when performing classification on dataset in the same year.
this happens because the abstraction of api calls can cause some false positives for instance api calls telephonymanager .getdeviceid and smsmanager .sendtextmessage can be abstracted into the same package and family which are android.telephony and android respectively.
through the results shown in table v we can see that the f measure and accuracy of another compared android malware detection method drebin both take up .
which are almost the same as the maximum f measure and accuracy ofmalscan on dataset.
however it performs poorer than malscan on most of datasets.
in general malscan can achieve higher f measure and accuracy on datasets from to .
particularly it significant outperforms drebin on and datasets.
in addition the maximum f measure and accuracy of six experiments in malscan on all datasets are all above drebin .
it is reasonable that malscan performs better than drebin because malscan considers the program structural semantics while drebin ignores them.table vi mean values of f measure and accuracy of malscan mamadroid and drebin to perform classification on newer samples by training an old dataset testing gap one year two year three year four year metrics f1 a f1 a f1 a f1 a degree .
.
.
.
.
.
.
.
closeness .
.
.
.
.
.
.
.
harmonic .
.
.
.
.
.
.
.
katz .
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
concatenate .
.
.
.
.
.
.
.
mamadroid .
.
.
.
.
.
.
.
drebin .
.
.
.
.
.
.
.
in conclusion malscan can achieve better performance than mamadroid and drebin on detecting malware by training and testing using samples that are developed in the same year.
c. robustness against android evolution for testing the resilience of malscan on the evolution of android apps.
we create four scenarios and conduct experiments on each scenario the f measure and accuracy of malscan mamadroid and drebin are presented in figure .
in the first scenario each system is trained by using dataset and then classify the samples from to .
in the second scenario we use the samples randomly selected from datasets before i.e.
and datasets as the training data and test the samples from to .
similar to the previous scenarios the training samples in the third scenario are randomly selected from datasets before and the testing samples are from to .
our final scenario includes randomly chosen training samples from datasets before and conduct classification on the samples in and .
in order to show more clearly in figures we only present the average and concatenate experimental results of malscan in figure .
in scenario one as shown in figure a and e the f measure and accuracy of malscan is above mamadroid and drebin when conduct testing on and datasets.
however the f measure and accuracy of malscan and drebin both drop a lot when the year of testing dataset increases to .
this is because the sensitive api calls between 6to maintain the coherence of the number of these four scenarios the number of training samples in the last three scenarios are the same as dataset which are benign apps and malicious apps.
7more detailed results can be available in the following website authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
s50 s48 s49 s50 s50 s48 s49 s51 s50 s48 s49 s52 s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s49 s48 s50 s48 s51 s48 s52 s48 s53 s48 s54 s48 s55 s48 s56 s48 s57 s48 s49 s48 s48 s70 s45 s109 s101 s97 s115 s117 s114 s101 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 a scenario one s50 s48 s49 s51 s50 s48 s49 s52 s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s50 s48 s51 s48 s52 s48 s53 s48 s54 s48 s55 s48 s56 s48 s57 s48 s49 s48 s48 s70 s45 s109 s101 s97 s115 s117 s114 s101 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 b scenario two s50 s48 s49 s52 s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s50 s48 s51 s48 s52 s48 s53 s48 s54 s48 s55 s48 s56 s48 s57 s48 s49 s48 s48 s70 s45 s109 s101 s97 s115 s117 s114 s101 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 c scenario three s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s52 s48 s53 s48 s54 s48 s55 s48 s56 s48 s57 s48 s49 s48 s48 s70 s45 s109 s101 s97 s115 s117 s114 s101 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 d scenario four s50 s48 s49 s50 s50 s48 s49 s51 s50 s48 s49 s52 s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s52 s48 s53 s48 s54 s48 s55 s48 s56 s48 s57 s48 s49 s48 s48 s65 s99 s99 s117 s114 s97 s99 s121 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 e scenario one s50 s48 s49 s51 s50 s48 s49 s52 s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s53 s48 s54 s48 s55 s48 s56 s48 s57 s48 s49 s48 s48 s65 s99 s99 s117 s114 s97 s99 s121 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 f scenario two s50 s48 s49 s52 s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s52 s48 s53 s48 s54 s48 s55 s48 s56 s48 s57 s48 s49 s48 s48 s65 s99 s99 s117 s114 s97 s99 s121 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 g scenario three s50 s48 s49 s53 s50 s48 s49 s54 s50 s48 s49 s55 s50 s48 s49 s56 s53 s48 s53 s53 s54 s48 s54 s53 s55 s48 s55 s53 s56 s48 s56 s53 s57 s48 s57 s53 s49 s48 s48 s65 s99 s99 s117 s114 s97 s99 s121 s40 s37 s41 s84 s101 s115 s116 s105 s110 s103 s68 s97 s116 s97 s115 s101 s116 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s98 s105 s110 h scenario four fig.
the f measure and accuracy of malscan mamadroid and drebin to perform classification on newer samples by training an old dataset and dataset vary a lot and both malscan and drebin are based on the analysis of sensitive api calls.
in scenario two all the methods can both achieve high f measure and accuracy when testing on dataset as shown in figure b and f .
drebin is able to maintain the best performance on datasets from to .
in scenario three the f measure and accuracy of malscan and drebin both drop a lot when the year of testing dataset increases from to .
the reason is the same as scenario one for instance the overlap ratio8of sensitive api calls invoked by malware samples between the training dataset and dataset is while drop to when the year of testing dataset increases to .
in other words almost half of sensitive api calls in the malware samples of dataset do not appear in the malware samples of training dataset which can cause a high false negative.
in the last scenario both malscan and mamadroid can obtain better performance than drebin the f measure and accuracy of concatenate experiment of malscan are higher than mamadroid and drebin on all testing datasets.
to research the overall performance on detecting newer samples by using an old dataset for training we present the mean values of f measure and accuracy of malscan mamadroid and drebin on detecting newer datasets whose time period ranges from one year to four year.
table vi presents the results.
as for f measure malscan is able to maintain the best performance when the time period between testing dataset and training dataset differs by one year.
however when the 8given two sets of sensitive api calls s1ands2 the overlap ratio between s1ands2is defined as o s1 s2 s1 s2 max s1 s2 .table vii attack details in this evaluation attack terms descriptions attack scenariothe adversary knows both the feature set and the training set and also has access to the target system as a black box attack algorithm modified jsma attack dataset dataset attack detectors1nn malscan random forest mamadroid and svm drebin time period increases to two year three year and four year mamadroid performs slight better than the others.
this is mainly due to the abstraction of api calls which is more resilient to the evolution of android apps.
as for accuracy malscan can achieve better performance than mamadroid and drebin on detecting newer samples by training an old dataset.
in general compared with mamadroid and drebin malscan can obtain an approximate good effect on detecting newer samples by using an old dataset.
d. robustness against adversarial attack in order to research the robustness of malscan to adversarial samples we leverage a recent and state of the art adversarial attack tool to complete our evaluation.
it can calculate the perturbations modify source files and rebuild the modified apk file to craft adversarial samples of android malware.
due to open source of all the datasets and algorithms of mamadroid drebin and malscan the adversary has access to all datasets.
as for machine learning models i.e.
classifiers they can be obtained by reimplemented the algorithms presented in corresponding papers.
therefor our attack scenario authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
s68 s101 s103 s67 s108 s111 s72 s97 s114 s75 s97 s116 s122 s65 s118 s101 s67 s111 s110 s77 s97 s77 s97 s68 s114 s101 s48 s46 s48 s48 s46 s50 s48 s46 s52 s48 s46 s54 s48 s46 s56 s49 s46 s48 s69 s118 s97 s115 s105 s111 s110 s82 s97 s116 s101 s68 s101 s103 s58 s68 s101 s103 s114 s101 s101 s67 s108 s111 s58 s67 s108 s111 s115 s101 s110 s101 s115 s115 s72 s97 s114 s58 s72 s97 s114 s109 s111 s110 s105 s99 s75 s97 s116 s122 s58 s75 s97 s116 s122 s65 s118 s101 s58 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s58 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s58 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s58 s68 s114 s101 s98 s105 s110 fig.
the evasion rate of adversarial examples crafted by jsma on malscan mamadroid and drebin in this section is that the adversary knows all datasets and also has access to the target detector as a black box.
specifically we choose our dataset as our test object to commence the evaluation.
moreover the attack algorithm we select is modified jsma which crafts adversarial examples by using the forward derivatives of the classifier9.
as described in mamadroid can achieve better performance when it adopts random forest to detect malware.
as for malscan we find that 1nn is able to maintain better effectiveness on detecting malware.
therefor the selected machine learning classifiers in this section are 1nn random forest and svm formalscan mamadroid and drebin respectively.
table vii summarizes these attack details.
figure presents the evasion rate i.e.
fnr of crafted malware samples on malscan mamadroid and drebin .i t shows that all the three systems can be evaded easily by these crafted adversarial samples.
this happens because the attack launched by is a tailor made attack it can make corresponding changes to the attack steps according to the different algorithms implemented by classifiers until the crafted adversarial sample can be misclassified by the detector or exit when the number of iterations reaches the set threshold.
however the attack cost of drebin is the lowest since it only extracts syntax features and the adversary only needs to add code containing the required features e.g.
restricted api calls but never being invoked or executed.
as for mamadroid and malscan it requires more cost for the adversary to complete the attack due to the consideration of program semantics.
for instance the added features are not the simple restricted api calls but calls from some callers to some callees.
moreover when the adversary attacks on average or concatenate centrality detector the attack cost is more since they are constructed by integrated four individual centrality measures.
therefor the adversary must consider four different algorithms of centrality extraction for crafting adversarial samples.
in conclusion malscan mamadroid and drebin are not robust enough in the face of tailor made adversarial attack.
9more detail procedures are in .
s48 s50 s48 s48 s48 s48 s52 s48 s48 s48 s48 s54 s48 s48 s48 s48 s56 s48 s48 s48 s48 s49 s48 s48 s48 s48 s48 s49 s50 s48 s48 s48 s48 s48 s49 s48 s50 s48 s51 s48 s52 s48 s53 s48 s54 s48 s55 s48 s68 s101 s103 s114 s101 s101 s67 s108 s111 s115 s101 s110 s101 s115 s115 s72 s97 s114 s109 s111 s110 s105 s99 s75 s97 s116 s122 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s84 s111 s116 s97 s108 s82 s117 s110 s84 s105 s109 s101 s79 s118 s101 s114 s104 s101 s97 s100 s40 s115 s41 s83 s97 s109 s112 s108 s101 s83 s105 s122 s101 s40 s110 s111 s100 s101 s115 s41 fig.
total runtime overheads of malscan on different sample size by using random forest e. runtime overhead in this section we estimate the runtime overhead of malscan mamadroid and drebin by using a dataset randomly selected from our samples which consists of benign apps and malicious apps.
the average number of nodes in these benign samples and malicious samples are and respectively.
given a new app malscan consists of three main phases to analyze it function call graph extraction feature extraction and classification.
the runtime overheads of these three main phases are illustrated in table viii.
it is required an average of .
seconds to construct the call graph for a given apk file.
in feature extraction the runtime overhead differs according to the selected centrality measures.
on average .
seconds is required for degree centrality extraction which is considerably less than the other three centrality measures.
to extract the average centrality and concatenate centrality of a node within a graph we first need to obtain the four individual centrality values figure .
therefor it takes both about an average of .
seconds to construct the feature vector for average centrality and concatenate centrality.
given a feature vector we can perform classification by using a trained machine learning model10.a s shown in table viii the runtime overhead of classification varies according to not only the selected centrality measures but also the selected classification algorithms.
as for centrality measures the runtime overhead of concatenate centrality is higher than the other five centrality measures.
this happens due to that the dimension of feature vector of concatenate centrality is four times longer than the other five centrality measures.
moreover as shown in figure the total running time to process an app by malscan is generally positive related to the 10in this evaluation we trained machine learning models by using these apps.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table viii the average runtime overhead seconds of malscan mamadroid and drebin on different phases phases graph construction feature extractionclassification 1nn 3nn randomforest svm degree .
.
.
.
.
closeness .
.
.
.
harmonic .
.
.
.
katz .
.
.
.
average .
.
.
.
concatenate .
.
.
.
mamadroid .
.
.
.
.
drebin .
.
size of app s function call graph.
the more nodes in a call graph the longer the running time.
overall given a new app as shown in figure malscan can classify it as either benign or malicious in an average of .
seconds when we select degree centrality to construct the feature vector and random forest as the classification model.
we also evaluate the runtime overhead of mamadroid and drebin on detecting an app.
as for mamadroid it consists of three main procedures to complete the classification.
in the first step for constructing a more precise call graph of an app mamadroid performs heavyweight program analysis to ensure contexts and flows preserved.
so it takes an average of .
seconds to extract the call graph per app in our samples.
whereas it takes about .
seconds to finish the feature extraction step per app.
in the final step classification using random forest is fastest .
seconds on average.
therefor as shown in figure in total mamadroid consumes about .
seconds for a complete classification on an app from our samples when we select random forest as the classification model.
as for drebin on the one hand it extracts features not only from disassembled code but also from the manifest.
on the other hand these features include several complex features e.g.
network address and the number of extracted features is more than .
therefor it takes about .
seconds on average to extract features per app in our samples.
given a feature vector drebin consumes about .
seconds to flag it as either benign or malicious by using a svm classifier.
in conclusion malscan is tremendously scalable and efficient than mamadroid and drebin .
f .
market wide case study table ix the number and average size of apps collected from google play app market year apps average size mb .
.
.
.
.
.
.
.
total .
to validate the feasibility of malscan on malware scanning of google play scale android app stores we conduct s48 s46 s55 s49 s46 s55 s55 s50 s46 s50 s57 s50 s46 s49 s55 s52 s46 s57 s50 s52 s46 s57 s50 s49 s54 s53 s46 s54 s51 s56 s50 s46 s54 s57 s68 s101 s103 s67 s108 s111 s72 s97 s114 s75 s97 s116 s122 s65 s118 s101 s67 s111 s110 s77 s97 s77 s97 s68 s114 s101 s48 s50 s48 s52 s48 s54 s48 s56 s48 s49 s48 s48 s49 s50 s48 s49 s52 s48 s49 s54 s48 s49 s56 s48 s68 s101 s103 s58 s68 s101 s103 s114 s101 s101 s67 s108 s111 s58 s67 s108 s111 s115 s101 s110 s101 s115 s115 s72 s97 s114 s58 s72 s97 s114 s109 s111 s110 s105 s99 s75 s97 s116 s122 s58 s75 s97 s116 s122 s65 s118 s101 s58 s65 s118 s101 s114 s97 s103 s101 s67 s111 s110 s58 s67 s111 s110 s99 s97 s116 s101 s110 s97 s116 s101 s77 s97 s77 s97 s58 s77 s97 s77 s97 s68 s114 s111 s105 s100 s68 s114 s101 s58 s68 s114 s101 s98 s105 s110 s84 s111 s116 s97 s108 s82 s117 s110 s116 s105 s109 s101 s79 s118 s101 s114 s104 s101 s97 s100 s40 s115 s101 s99 s111 s110 s100 s115 s41 fig.
total average runtime overheads of malscan by using random forest mamadroid by using random forest and drebin by using svm a statistical research of app size in google play app market from androzoo .
we collect some information of apps in google play app market which includes sha256 package name apk size and dex date.
as shown in table ix the time period of these apps is from january to december and the total number of collected apps in google play app market is .
table ix presents the average size of collected apps in different year it can be seen that the average size of apps will grow larger over time and the average size of total apps is .
mb.
we also introduce the cumulative distribution functions cdfs of the size of these apps in google play app market and our randomly selected apps.
as shown in figure in general apps in google play app market are slightly larger than apps in malscan randomly selected apps .
because malscan is a graph based malware scanning system and the total running time to process an app is generally positive related to the size of app s function call graph figure .
therefor we randomly select apps from collected apps and conduct static analysis to extract the function call graphs.
after obtaining the function call graphs we then gather the size of these graphs.
specifically from the results presented in table x the average graph size of apps in google play app market is about .
times larger than in malscan .
in addition the ratio of average running time formalscan to complete classification on google play 11all the collected information of these apps can be available in the following website authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table x the average size of apps in google play app market and apps used in malscan average runtime overheads to complete classification on these apps by using random forest and the size of ratio between google play and malscan apps average size mb average size nodes average runtime overheads second degree closeness harmonic katz average concatenate google play .
google play .
.
.
.
.
.
.
malscan .
.
.
.
.
.
.
ratio gp ms .
.
.
.
.
.
.
.
fig.
cdfs of the app size mb in google play app market apps and malscan apps and malscan is almost around .
.
such result also indicates that the total running time of malscan to process an app is generally positive related to the size of app s function call graph.
therefor when we adopt malscan to perform malware scanning on google play app market the average runtime overhead to process an app may be around one second when we select degree centrality to form the feature vector.
such high efficiency suggests that malscan can enable frequent market wide scanning of google play scale android app markets.
g. combination of centrality measures in this section we propose the combination of different centrality measures on detecting android malware.
as discussed in the former experiments we totally select four individual centrality measures and add another two centrality measures figure .
in reality these six experimental results can be complementary for instance we can use majority voting to flag an app as either benign or malicious.
in other words an app we consider as malware when it is reported to be malicious by one or more of the six centrality experiments.
for testing the feasibility of majority voting we conduct an experiment on scenario one in section iv .c.
we leverage a trained model by using dataset and test on datasets from to respectively.
the thresholds to flag an app as malicious in ourtable xi the f measure and true positive rate tpr ofmalscan by adopting majority voting mamadroid and drebin to perform classification on datasets by using dataset for training testingyear metrics f1 tpr f1 tpr f1 tpr degree .
.
.
.
.
.
closeness .
.
.
.
.
.
harmonic .
.
.
.
.
.
katz .
.
.
.
.
.
average .
.
.
.
.
.
concatenate .
.
.
.
.
.
v ote .
.
.
.
.
.
v ote .
.
.
.
.
.
v ote .
.
.
.
.
.
v ote .
.
.
.
.
.
v ote .
.
.
.
.
.
v ote .
.
.
.
.
.
mamadroid .
.
.
.
.
.
drebin .
.
.
.
.
.
experiment are one v ote two v ote three v ote four v ote five v ote and all v ote .
as shown in table xi12 when we select two as the threshold to flag an app as either benign or malicious i.e.
an app we consider as malware when it is reported to be malicious by two or more of the six centrality experiments the f measure is highest among all the experimental results.
particularly the true positive rates tprs improve significantly when we adopt majority voting to detect malware.
when we train a model by using dataset and perform classification on dataset the true positive rate is at most .
if we select concatenate centrality to form the feature vector.
however the true positive rate can increase to .
when we adopt majority voting to flag an app as either benign or malicious.
in other words we can detect at most .
malware in dataset when we employ majority voting.
in addition parallel processing is a feasible choice when we conduct six centrality experiments.
therefor the runtime overhead of majority voting can be only slightly longer than concatenate centrality experiment.
h. detection of zero day malware to validate the capability of malscan on detecting realworld zero day malware we use our dataset to train classifiers by adopting 1nn algorithm we totally obtain six classifiers according to the six different centrality measures .
12due to the limited page other detailed results are available in the website authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
next we crawl apps from google play app market and feed them to the trained classifiers.
we leverage majorityvoting method to flag an app as benign or malicious and we consider an app as malware when it is reported to be malicious by two or more of the six centrality experiments.
among these apps malscan reports of them as being malicious.
to investigate whether these apps are malware we upload them to virustotal to analyze each of them.
among these apps of them are reported as malware by at least one antivirus scanner.
of the remaining apps we manually inspect them.
our manual inspection shows that of these apps contains highly suspicious behaviours it contains dangerous level permissions reads device s memory and cpu information and writes many sensitive data into several log files.
in an effort to check more deeply we leverage a state ofthe art android app analysis system which combines static and dynamic analysis for reporting detailed risky behaviours .
from the reported results we can see that the app executes shell code which can demonstrate that it is indeed a malware.
in conclusion malscan is able to find 1813zero day malware among google play apps of them is not reported as malware by existing tools .
v. d iscussion in our work we totally select four individual centrality measures and add another two centrality measures to perform classification on android apps.
we plan to test the capability of more different centrality measures on detecting android malware.
although the robustness of malscan against tailormade adversarial attack is low it can be used as the first line of defense because of the high efficiency on malware scanning.
after filtering most of malware other more computational intensive and robust approaches can be used as the second line of defense.
by this we can save more times and resources.
moreover since most android malware detection systems are closed source we only compare malscan with two open source systems i.e.
mamadroid and drebin .
we will conduct detailed comparative analysis on more systems in our future work.
vi.
r ela ted work there has been many proposed approaches on android malware detection that rely on syntax features or program semantics .
drebin uses a broad static analysis to extract as many features as possible from an app and embeds them in a joint vector space to classify malware.
however it only searches for the presence of particular strings such as some restricted api calls rather than considers the program semantics.
so it can be easily evaded by attacks on syntax features .
droidapiminer conducts frequency analysis to identify certain api calls commonly used by malware and then performs a simple data flow analysis to extract features to complete classification.
13detailed information can be available in the following website it suffers from feature explosion because it cannot generalize its feature space.
droidsift extracts the weighted contextual api dependency graph to solve the malware deformation problem based on static analysis.
apposcopy utilizes static analysis to extract the data flow and control flow properties of an app to identify its malware family.
however both droidsift and apposcopy suffer from heavy runtime overhead they consume .8s and 275s to analyze an app respectively.
mamadroid leverages the sequences of abstracted function calls obtained from a call graph to build a behavioral model and uses it to extract features to conduct classification.
this approach is more resilient to api changes and is more robust to the evolution of android apps.
however it sustains some limitations the one is that it can be easily evaded by the selfdefined packages that looks similar to android s google s or java s packages the other is that it requires a sizable amount of memory on classification because of its large feature sets and the extraction of call graph .
different from the previous work to avoid the heavy computation overhead of graph matching malscan regards the function call graph of an app as a complex social network and then extracts the centrality of sensitive api calls to construct the feature vector.
given a feature vector malscan can accurately flag it as either benign or malicious.
vii.
c onclusion in this paper we present a lightweight android malware detection method based on centrality analysis of sensitive api calls.
we implement an automated android malware detection system malscan and conduct a comprehensive evaluation in datasets of apps.
experimental results indicate that malscan is capable of detecting android malware in an average of .
seconds with up to accuracy which is more than times faster than two state of theart approaches namely mamadroid and drebin .
we also demonstrate the feasibility of malscan on market wide mobile malware scanning by performing a statistical study on over millions apps from google play app market.
moreover in a corpus of dataset collected from google play app market malscan is able to identify zero day malware including malware samples that can evade detection of existing tools in virustotal .