boosting operational dnn testing efficiency through conditioning zenan li state key lab of novel software technology nanjing university nanjing china lizenan smail.nju.edu.cnxiaoxing ma state key lab of novel software technology nanjing university nanjing china xxm nju.edu.cnchang xu state key lab of novel software technology nanjing university nanjing china changxu nju.edu.cn chun cao state key lab of novel software technology nanjing university nanjing china caochun nju.edu.cnjingwei xu state key lab of novel software technology nanjing university nanjing china jingweix nju.edu.cnjian l state key lab of novel software technology nanjing university nanjing china lj nju.edu.cn abstract with the increasing adoption of deep neural network dnn models as integral parts of software systems efficient operational testing of dnns is much in demand to ensure these models actual performance in field conditions.
a challenge is that the testing often needs to produce precise results with a very limited budget for labeling data collected in field.
viewing software testing as a practice of reliability estimation through statistical sampling we re interpret the idea behind conventional structural coverages as conditioning for variance reduction.
with this insight we propose an efficient dnn testing method based on the conditioning on the representation learned by the dnn model under testing.
the representation is defined by the probability distribution of the output of neurons in the last hidden layer of the model.
to sample from this high dimensional distribution in which the operational data are sparsely distributed we design an algorithm leveraging cross entropy minimization.
experiments with various dnn models and datasets were conducted to evaluate the general efficiency of the approach.
the results show that compared with simple random sampling this approach requires only about a half of labeled inputs to achieve the same level of precision.
ccs concepts software and its engineering software testing and debugging computing methodologies neural networks .
keywords software testing neural networks coverage criteria corresponding author.
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author s .
esec fse august tallinn estonia copyright held by the owner author s .
acm isbn .
reference format zenan li xiaoxing ma chang xu chun cao jingwei xu and jian l .
.
boosting operational dnn testing efficiency through conditioning.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction deep learning has gained great success in tasks that are intuitive to human but hard to describe formally such as image classification or speech recognition .
as a result deep neural networks dnns are increasingly adopted as integral parts of widely used software systems including those in safety critical application scenarios such as medical diagnosis and self driven cars .
effective and efficient testing methods for dnns are thus needed to ensure their service quality in operation environments.
recent efforts on dnn testing have aimed at generating artificial adversarial examples which resembles the debug testing of human written programs that aims at finding error inducing inputs.
however the fundamental difference between dnn models and human written programs challenges the basic concepts and wisdoms for debug testing.
for example the inductive nature of statistical machine learning and the no freelunch theorem imply that an oracle for a dnn model independent of its operation context is senseless.
the fact that dnn performance is measured statistically also diminishes the importance of individual error inducing inputs.
contrastingly this paper focuses on the operational testing of dnn i.e.
testing a previously trained dnn model with the data collected from a specific operation context in order to determine the model s actual performance in this context.
although operational testing for conventional software has been extensively studied the challenge of operational dnn testing is not well understood in the software engineering community.
a central problem here is that it can be prohibitively expensive to label all the operational data collected in field.
for example a surgical biopsy may be needed to decide whether a radiology or pathology image is really malignant or benign.
in this case the labeling effort for each single example is worth saving.
thus it is crucial to test dnn esec fse august tallinn estonia zenan li xiaoxing ma chang xu chun cao jingwei xu and jian l efficiently i.e.
to precisely estimate a dnn s actual performance in an operation context but with a limited budget for labeling data collected from this context.
we propose to reduce the number of labeled examples required in operational dnn testing through carefully designed sampling.
the conventional wisdom behind structural coverages for testing human written programs is re interpreted in statistical terms as conditioning for variance reduction and applied to the sampling and estimation of dnn s operational accuracy.
the key insight is that the representation learned by a dnn and encoded in the neurons in the last hidden layer can be leveraged to guide the sampling from the unlabeled operational data.
it turns out that conditioning on this representation is effective and works well even when the model is not well fitted to the operation data which is a property not enjoyed by naive choices such as stratifying by classification confidence.
to realize the idea one must select a small fraction from the operational data but with sufficient representativeness in terms of their distribution in the space defined by the outputs of neurons in the last hidden layer.
this is difficult because the space is highdimensional and in which the operational data themselves are sparsely distributed.
we solve this problem with a distribution approximation technique based on cross entropy minimization.
the contributions of this paper are a formulation of the problem of operational dnn testing as the estimation of performance with a small sample and a proposal for efficient testing with variance reduction through conditioning as a generalization of structural coverages.
an efficient approach to operational dnn testing that leverages the high dimensional representation learned by the dnn under testing and a sampling algorithm realizing the approach based on cross entropy minimization.
a systematic empirical evaluation.
experiments with lenet vgg and resnet show that compared with simple random sampling this approach requires only about a half of labeled inputs to achieve the same level of precision.
the rest of this paper is organized as follows.
in section we discuss the problem of operational dnn testing and how to improve its efficiency.
section is devoted to the conditioning approach to efficient dnn testing and section to the empirical evaluation of the approach.
we then review related work in section before concluding the paper with section .
operational testing of dnns in this section we briefly introduce dnn examine the problem of testing dnns as software artifacts in operation context and then discuss the insights for and the challenges to efficient dnn testing.
.
deep neural network a deep neural network dnn is an artificial neural network ann with multiple intermediate hidden layers.
it encodes a mathematical mapping from inputs to outputs with a cascading composition of simple functions implemented by the neurons.
figure is a simple example of neural network.
the existence of activation functions makes the model nonlinear.x1 x2n n n 3n n n n 4o1 o2w11b1 w12b1 w13 b1hidden layers input layer output layer ni jwj ixj bi figure a simple neural network to approximate the intricate mapping hidden in the training examples a dnn model has its parameters weights wi jand biases bi gradually adjusted to minimize the averaged prediction error over all the examples.
what a dnn actually learned is a posterior probability distribution denoted as p y x .
for example for a k classification problem dnn will give kposterior probability functions p y i x i .
.
.
kfor the given input x. the predicted label for this input is the class corresponding to the maximum posteriori probability i.e.
f x arg max ip y i x .
.
the dnn testing problem when a previously trained dnn model is adopted as an integral part of a software system deployed in a specific environment it may drastically underperform its expected accuracy.
there can be different causes such as under fitting or over fitting of the model to the training data set or the data distribution discrepancy between the training set and the data emerged in the operation context.
the latter is especially nasty and often encountered in practice.
therefore as any software artifact a dnn model must be sufficiently tested before being put into production.
dnn testing is different from traditional software testing aiming at identifying error inducing inputs.
dnn implements a kind of inductive reasoning which is fundamentally different from human written programs based on logic deductions.
as a consequence for a trained dnn there does not exist a certain and universal oracle for testing.
elaborately the testing of dnns has to be statistical contrasting to human written programs with certain intended behaviors as a statistical machine learning model a dnn offers only some probabilistic guarantee i.e.
to make probably correct prediction on most inputs it concerns .
in fact mispredictions on a small portion of inputs are expected and in some sense intentional in order to avoid overfitting and maximizing generality.
holistic up to now there is no viable rationale interpretation of dnns internal behaviors on individual inputs at the level of neurons .
this means that dnns are essentially blackboxes although their computation steps are visible.
also a detected fault with a specific input is hardly helpful for debugging the dnn.
operational moreover testing of dnns without considering their operation context is meaningless.
this is implied by the no free lunch theorem which says that considering all 500boosting operational dnn testing efficiency through conditioning esec fse august tallinn estonia possible contexts no machine learning algorithm is universally any better than any other .
so generally the task of testing a dnn as a software component is giving a previously trained dnn model and a specific operation context to decide how well the model will perform in this context which is expressed statistically with the estimated accuracy of prediction1.
this task should be easy if we had enough labeled data that well represent the operation context and suffice accurate estimation.
however in practice although unlabeled data can be collected from the operation environment labeling them with highquality is often expensive.
for example considering an application scenario of ai aided clinical medicine where a hospital is going to adopt a dnn model to predict mri images to be malignant or benign.
suppose that the model is previously trained by a foreign provider with its proprietary dataset and thus the hospital needs to gauge it against native patients and local equipment settings.
the hospital may collect a lot of images by scanning patients and volunteers but labeling them is much more expensive because not only advanced human expertise but also some complicated laboratory testings and even intrusive biopsies are required.
therefore a central problem of dnn testing is how to accurately estimate dnns performance in their operational context with small size samples2of labeled data.
or in other words given a budget of cost in labeling examples how to make the estimation of a dnn s performance as accurate as possible .
figure illustrates the process of efficient operational dnn testing.
the goal is that with some sophisticated test data selection one only needs to label a small portion of operational data to achieve enough precision for the estimation of operational accuracy.
selecteddatasettraining datasetunlabeledoperationaldatasetdnntraining trained dnn modeltest data selectionlabelingoperationaltestingestimatedoperationalaccuracypossible divergencesize reduced figure efficient operational dnn testing .
improving testing efficiency through conditioning clearly the above description of dnn testing resembles statistical sampling and estimation whose efficiency can be improved with variance reduction techniques .
in the following we briefly introduce the simple random sampling as the baseline estimation method and then discuss how conditioning can help with some 1in this paper we consider only accuracy that is the proportion of examples for which the model predicts correctly.
however the proposed method is generally applicable to other performance measures.
2there is a common mistake of regarding a sample of more than elements as large enough .
as we will see in section we often need much more.inspirations from the coverage oriented testing of conventional programs.
sampling is the process of selecting a group of individuals from a population in order to study them and estimate the property of population.
specifically suppose that there is a fixed parameter e that needs to be estimated where x d rd is a random variable corresponding to the observed data and h d ris the model of interest.
for example when estimating the accuracy of a dnn model his defined as h x 1if the dnn correctly predicts x s label and 0otherwise.
note that h x is also a random variable.
an estimator of is denoted by the symbol .
the basic sampling method is simple random sampling srs .
srs draws i.i.d.
replications x1 .
.
.
xndirectly from the population.
the i.i.d.
condition requires that each individual is chosen randomly and entirely by chance such that each individual has the same probability of being chosen and each set of nindividuals has the same probability of being chosen for the sample as any other set of nindividuals.
and the estimator is computed as the average of each estimate h x1 h xn n. this is an unbiased estimator i.e.
e .
the efficiency of srs is expressed statistically by its variance var nvar .
despite of its simplicity srs is quite effective in practice if we can draw a sufficiently large i.i.d.
sample.
without further information about h x we can hardly improve over srs.
an often used strategy is conditioning i.e.
to find a random variable or vector z on which ideally h x strongly depends and leverage the law of total variance var e var .
intuitively the law says if we interpret h x with z the variance of h x can be decomposed to those not explained by z the first term on the right hand and those due to z the second term .
note that e itself is a function on z and e e .
so we can sample from z s distribution and estimate e instead of h x taking the advantage that the former has a smaller variance than the latter.
if we can make a complete sample of z i.e.
covering all possible values ziforz the variance of our estimation will be only those introduced in estimating e for each zi.
this is exactly what stratified sampling does.
furthermore if the value of h x is fully determined by z we will have zero variance.
these two ideal conditions are hard to satisfy especially in complex scenarios such as operational software testing.
however the insights are clear to improve the efficiency of testing as estimation we need to identify an observable factor zthat affects h x the performance accuracy as much as possible so that the variance ofh x conditioned under each ziis minimized and to draw as representative as possible samples for z so that the uncertainty due to zcan be well handled.
these two aspects can be conflicting in practice.
intuitively the more precise zthe interpretation for h x is the finer grained it has to be and the harder it can be sufficiently represented by 501esec fse august tallinn estonia zenan li xiaoxing ma chang xu chun cao jingwei xu and jian l a small size sample.
it is crucial to strike a good balance between them with a deliberately chosen z. it is inspiring to use this viewpoint to examine the structural coverage directed testing of conventional programs despite of the difference that structural coverages are mainly used to identify error inducing inputs.
the efficacy of a structural coverage comes from thehomogeneity of inputs covering the same part of a program either all or none of them induces an error so that testing efficiency can be improved by avoiding duplications and thediversity of inputs indicated by coverage metrics so that testing completeness can be improved by enforcing a high coverage to touch corner cases in which rare errors may hide.
evidently these two heuristics resemble the two insights of conditioning.
in this sense the conditioning techniques for testing efficiency improvement can be regarded as a generalization of structural coverages in conventional white box software testing.
however it turns out to be challenging to apply this idea to the testing of dnns because of theblackbox nature of dnns .
there is no obvious structural features like program branches or execution paths of human written programs that intuitively provide the needed homogeneity and moreover thecurse of dimensionality .
for dnns a powerful interpreting factor zfor model accuracy is often a high dimensional vector which makes it very difficult to represent with a small size sample but without huge uncertainty.
we will discuss how to meet these challenges in the next section.
efficient dnn testing methods first let us state our problem of efficient operational dnn testing more specifically problem.
givenma trained dnn model and sa set of nunlabeled examples collected from an operation context instead of labeling all these nexamples select and label a subset t swith a given size budget n t n and use tto estimate the accuracy of m ons with an as small estimation error as possible.
leveraging the information provided by mands we try to achieve efficient estimation through conditioning.
we first discuss confidence based stratified sampling css which is simple but unfortunately fragile and limited to classifiers.
then we present cross entropy baed sampling ces that conditions on representations learned by m and approximates the distribution of sthrough cross entropy minimization.
.
confidence based stratified sampling as discussed earlier the key to improve the estimation efficiency is to find a random variable zthat is strongly correlated to the accuracy of the model m and whose distribution is easy to sample.
a natural choice is the confidence value c x provided by someclassifier models when predicting the label for x. obviously predictions with a higher confidence will be more likely to be correct ifthe classifier is reliable.
since the confidence is a bounded scalar we can divide its range intoksections and stratify the population sintokstrata s1 sk accordingly.
thus the probability of an example belonging to sj ispj sj s j k. from each stratum sj randomly taking njelements such that k jnj n. then the accuracy of monsis estimated as acc k j 1pje k j 1pj njnj i 1h xj i !
.
a simple strategy is to use proportional allocation i.e.
nj pj n and then the estimation of accuracy becomes accprop n h xj i .
however although safe proportional allocation may be suboptimal.
the variance of a stratified estimator is the sum of variances in each strata var acc k j 1var h x x sj .
it can be further reduced if we allocate more examples in strata that are less even.
specifically we can guess that a stratum with a lower confidence should fluctuate more in the accuracy.
so we should take more examples from those low confidence strata.
the optimal stratification and example allocation depend on the actual distribution of the variance of h x conditioned on the confidence which is not known a priori .
they have to be determined according to experience and pilot experiments.
unfortunately css is not robust for operational dnn testing.
it performs very well when the model is perfectly trained for the operation context.
however as shown in our experiments reported in section when the model is not well fitted to the operational data set its performance drops drastically.
note that this unfavorable situation is the motivation for operational dnn testing.
it is not difficult to see the reason when the model predicts poorly in the operation context the confidence values it produces cannot be trusted.
in addition confidence values are not readily available in regression tasks.
therefore we propose another variance reduction method based on behaviors of neurons.
.
cross entropy based sampling a better choice for the condition random variable zis the output of neurons in the last hidden layer.
it is often viewed as a learned representation of the training data that makes the prediction easier .
the rationale behind this choice is manifold.
first although not necessarily comprehensible for human the representation is more stable than the prediction when the operation context is drifting.
this is supported by well known transfer learning practices where only the softmax layer is retrained for different tasks .
second the dnn prediction is directly derived from the linear combination of this layer s outputs and thus it must be highly correlated with the prediction accuracy.
finally the correlation between the neurons in this layer is believed to be smaller than those in previous layers which facilitates the approximation of their joint distribution that will be used in our algorithm.
502boosting operational dnn testing efficiency through conditioning esec fse august tallinn estonia for a trained dnn model mwe consider its last hidden layer lconsisting of mneurons denoted by ei i .
.
.
m. we divide dei the output range of each neuron ei into kequal sections dei .
.
.
dei k and define function fei x jif the output of eifor input xbelongs to dei j j k. hence the conditional variable zis a vector z1 .
.
.
zm zi .
.
.
k i m. let sz1 ... zm x s fei x zi i m be a subset of swhose elements are mapped onto z z1 .
.
.
zm by the model.
the probability distribution ps z ofz is defined with the operational data set sas ps z1 .
.
.
zm sz1 ... zm s .
however considering the high dimensionality of z it is challenging to take a typical sample tfrom the whole test set saccording to z s distribution not to mention applying stratified sampling.
note that we cannot use artificial examples generated according to z s distribution because we need to evaluate the model s accuracy on real data in the given operation context.
now the problem is how to select a small size sample from a finite population which itself is sparsely distributed in a high dimensional space such that the sample is as representative as possible for the population.
to this end we propose to select a typical3sample tby minimizing the cross entropy between ps z andpt z min t s t nce t h ps pt z ... k mps z logpt z where pt z1 .
.
.
zm tz1 ... zm t .
in this high dimensional case the minimization is hard to compute directly partially because of the sparseness of sandtinz s space.
fortunately it is observed that a dnn typically reduces the correlation among neurons in the last hidden layer thus we can take an approximation by assuming that they are independent of each other in computing the minimization.
in this case we can minimize ce t through minimizing ce t the average of the cross entropy between ps z andpt z on each dimension min t s t nce t m i 1 k zi 1pei s zi logpei t zi m where pei s zi x s fei x zi s .
andpei t zi is defined similarly.
furthermore the optimal solution of ce t is achieved when ps z pt z z .
.
.
k m .
therefore the estimator for 3cf.
shannon s concept of typical set .model accuracy e is given by acc z ... k mpt z e z ... k mpt z z tz1 .
.
.
zmh x tz1 ... zm x th x t n i 1h xi n. note that this estimator is unbiased according to equation .
to solve the optimization problem of equation we propose an algorithm algorithm similar to random walk .
elaborately we first randomly select pexamples as the initial sample set t and repeatedly enlarge the set by a group q ofqexamples until we exhaust the budget of n. at each step q is selected from l randomly selected groups minimizing the cross entropy.
algorithm test input selection input original unlabled test set s dnnm the budget nfor labeling inputs.
output selected test set t t n for labeling.
selecting randomly pexamples as the initial test set t. while t ndo randomly select lgroups of examples q1 .
.
.
ql.
each group contains min q n t examples.
choose the group that minimizes the cross entropy i.e.
q min qice t qi i .
.
.
l. t t q .
end while finally there is an intrinsic connection between structural coverage and the cross entropy in equation .
structural coverages actually assume the probability of ps z to be a constant.
in this case minimizing ce t becomes maximizing z ... k mlogpt z which equals to maximizing z ... k mpt z .
since z ... k mpt z it is to even the distribution pt z which is actually to maximize the coverage so that more instances ofzare covered.
evaluation cautious readers may have noted that our approach leverages several heuristics and approximations including the stableness of the representation learned by a dnn model despite of the possible drift of its operational data the independence between the outputs of neurons in the last hidden layer and the optimization through random walk in algorithm .
thus a systematic empirical evaluation is needed to validate the general efficacy of the approach.
in the following we first briefly introduce the implementation of our css and ces approaches then discuss some different situations faced by operational dnn testing and how experiments are designed accordingly.
after that we present the results of the experiments which unanimously confirm that our approach greatly improves testing efficiency.
503esec fse august tallinn estonia zenan li xiaoxing ma chang xu chun cao jingwei xu and jian l .
implementation we implemented our approach using tensorflow .
.
and keras .
.
dl frameworks.
the code along with additional experiment results4 can be found at for css we use an optimal setting achieved through pilot experiments.
the population is partitioned into three strata.
the examples of the whole operational dataset with the highest confidence are assigned to the first stratum the next to the second stratum and the lowest to the third stratum.
to draw a sample with size n we take n n and n examples from the three strata respectively.
for the ces approach that conditions on representation we set k the number of sections for each neuron to .
this is not necessarily the best number but is reasonable considering the tens to fewhundreds examples are expected to be sampled.
in implementing algorithm we select p 30initial examples and enlarge the set byq 5examples in each step.
the number of random groups examined in each step lis set to .
these parameters are fixed in all experiments except for those with very small operational test sets to be detailed in section .
.
.
further optimizations of these parameters is possible but the above values are already sufficient for achieving a significant efficiency improvement over srs.
.
experiment design generally operational dnn testing is to detect the performance loss of a dnn model when used in a specific operation context.
here we assume that the model is well trained with its training set and do not explicitly consider the problems usually addressed in the training process such as under fitting or over fitting.
hence the performance loss is likely to be caused by polluted training set .
the training set is mutated by an accident or malicious attack and thus a mutated model is generated.
different system settings .
for example the model might be trained with high resolution examples but used to classify low resolution images due to the limitation of the camera equipped in the system.
different physical environment .
for example the lightening condition may vary in the operation environment.
in addition we need to consider the differences in the purpose classification or regression the scale of dnn models and the size of unlabeled operational test sets.
with these considerations as shown in table we designed experiments in total which varied in the training sets the dnn models the operational testing sets and thus the actual operational accuracies.
dnn models with very different structures were used in these experiments.
table lists the numbers of their layers and neurons.
the first group of experiments no.
results to be discussed in section .
.
were designed to study the effect of a polluted training set and to see whether the conditioning approaches were robust 4besides those discussed this section additional experiments were carried out to validate the superiority of the last hidden layer over other layers as the learned representation to condition on in ces to explore whether the surprise value can be used as an alternative for the confidence in css and to examine the relative efficiency of ces over srs with relatively bigger samples.
due to the page limit we cannot include them in this paper.table experiment settings and e value results no.trainmodeloperational actual e value set test set acc.
ces srs mnistlenet mnist93.
.
lenet .
.
lenet .
.
4mutant1a79.
.
5mutant2a77.
.
6mutant3a79.
.
drivingdave origdriving90.4b0.
dave drop .8b0.
dave origpatch88.3b0.
dave drop .5b0.
dave origlight89.8b0.
dave drop .5b0.
imagenetvgg 19imagenet72.
.
resnet .
.
vgg 19resolution63.
.
resnet .
.
17mutant1alenet 5mnist .6c0.443c mnist .4c0.375c 19driving dave origpatch .9c0.594c patch .3c0.549c athe three mutated models are trained by changing the labels of training data respectively.
bsince the steering angle is a continuous value we use mse mean squared error as the accuracy.
cit is the mean value of experiments with different randomly selected test sets.
table layers and neurons of dnn models modellenet dave vgg resnet orig drop neurons layers when the actual accuracy varied.
the second group of experiments no.
section .
.
simulated different physical environment conditions and the third group no.
section .
.
were for different system settings.
in these experiments we compared the mean squared errors mse acc of different estimated accuracy accfrom different estimators.
they were the srs estimator section .
the css estimator section .
and the ces estimator section .
.
each experiment was repeated times on each sample size of .
.
.
.5the mean square error was computed as1 50 50 i acci acc where acciandaccwere the estimated and actual operational accuracy respectively.
note that because all these estimators are unbiased themse can be regarded as the estimation variance whose square root i.e.
the standard deviation is plotted in the figures and .
5focusing on estimation with small size samples here we only give results up to sample size .
however an additional experiment presented at our code website demonstrated that the relative efficiency of ces over srs is quite stable when the sample size grew up to .
504boosting operational dnn testing efficiency through conditioning esec fse august tallinn estonia sample size1 .
.
.
.
standard deviation of accuracysrs css ces a lenet sample size0.
.
.
standard deviation of accuracysrs css ces b lenet sample size0.
.
.
.
.
standard deviation of accuracysrs css ces c lenet sample size2 standard deviation of accuracysrs css ces d mutant sample size0 standard deviation of accuracysrs css ces e mutant sample size2 standard deviation of accuracysrs css ces f mutant figure results of experiments with mnist the final group of experiments no.
section .
.
were designed to see whether our approach still worked in cases that only a small number of unlabeled operational examples were available.
we tested cases of taking a sample of size from an operational test set of size and of taking a sample of from .
in addition to the visual plotting of standard deviations we also computed the relative efficiency of two estimators as e 2 2 the ratio between their variances.
considering that the variance of an srs estimator is inversely proportional to the sample size eactually indicates the rate of sample size reduction.
we list the averaged evalues the smaller the better of ces to srs for each experiment in the last column of table .
experiments with the imagenet dataset no.
were conducted on a linux server with two core .20ghz xeon e5 cpus gb ram and nvidia gtx 1080ti gpus and other experiments were on a linux laptop with an .20ghz i7 8750h cpu gb ram and a nvidia gtx 1050ti gpu.
for a feeling about the computational cost of sampling with ces we observed that to select out a sample of size it took .27s for mnist lenet .50s for driving dave orig and .09s for imagenet vgg .
.
experiment results .
.
experiments with the mnist dataset.
experiments were conducted on the mnist dataset that is widely used in machine learning research.
it is a handwritten digit dataset consisting of pixel training images and testing images in 10classes.
with this dataset we trained three lenet family models lenet lenet and lenet .
experiments tested the ideal situation where the training set and testing set were both original.
experiments the models were trained with mutated training set but tested with the original testing images as the operational dataset.
the mutated training set is obtained by exchanging the labels of training data.
from the first row of figure we can see that when there was no divergence between the training data and the operational data the css estimator performed particularly well achieving a .
.
and .198average efficiency relative to srs for the three models tested respectively.
however the second row of figure tells a completely different story.
in this case the training set had been mutated and css performed very bad with a .
.
and2.919average relative efficiency respectively.
clearly the css estimator is not robust to the divergence between the training data and operational data.
on the contrary the ces estimator consistently outperformed srs in both cases.
from figure and the relative efficiency values listed in table we can see that ces only required about a half of labeled data to achieve the same level of precision of estimation.
the result of experiment also suggests that when the operational accuracy was very high about the benefit of our ces diminished to a level saving about labeling effort.
however this should not be a problem because the accuracy is high and in this situation we can switch to css if needed.
505esec fse august tallinn estonia zenan li xiaoxing ma chang xu chun cao jingwei xu and jian l sample size0.
.
.
.
.
.
.
standard deviation of accuracysrs ces a original dataset dave orig sample size0.
.
.
standard deviation of accuracysrs ces b parts blocked dave orig sample size0.
.
.
standard deviation of accuracysrs ces c light modified dave orig sample size0.
.
.
.
.
.
standard deviation of accuracysrs ces d original dataset dave drop sample size1 standard deviation of accuracysrs ces e parts blocked dave drop sample size0.
.
.
.
.
standard deviation of accuracysrs ces f light modified dave drop figure results of experiments with driving dataset .
.
experiments with the driving dataset.
the next experiments were conducted on the driving dataset6.
it is the udacity selfdriving car challenge dataset containing training and testing examples.
this is a regression task that predicts the steering wheel angle based on the images captured by a camera equipped on a driving car.
two pre trained dnns dave orig and davedropout were used in our experiments.
a original b darkened c blocked figure mutations of the driving test data experiments and tested the ideal situation where the operational test set is untouched.
experiments used mutated operational data simulating unideal physical environment conditions with the two methods used by deepxplore occlusion by small rectangles simulating an attacker blocking some parts of camera experiments and and lighting effects for simulating different intensities of lights experiments and .
figure illustrates the mutations.
figure plots the mean squared errors of ces and srs for each of the experiments.
because the regression models did not provide confidence values the css estimator could not be applied in these experiments.
the results show that ces also worked well for regression tasks and achieved .
.
relative efficiency w.r.t.
srs when divergence between training data and operational data existed.
.
.
experiments with the imagenet dataset.
the imagenet dataset is chosen for the last experiments.
it is a large collection of more than .
million images as the training data and other as the test data in classes.
two large scale pre trained models viz.
vgg and resnet were taken as the subject.
again experiments and used the original test set as the operational data.
experiments and used low resolution images as the operational testing data which were obtained by downsampling the images in the original test set as shown in figure .
the results of these experiments as shown in figure are consistent with previous ones.
it is quite impressive that the ces achieved a standard deviation of a little more than with only labelled examples considering the classes of images.
it indicates that our ces method also performed well for large dnn models with or without the divergence between training data and operational data.
506boosting operational dnn testing efficiency through conditioning esec fse august tallinn estonia a original b downsampled figure mutation of the imagenet test data .
.
experiments with small operational datasets.
we randomly selected and examples from the mnist test set and the driving test set as unlabeled operational test set.
then srs and our ces were applied to reduce them to and for labeling respectively.
considering the small sizes of the operational test sets for better numerical precision in the calculation of equation we minimizing dkl pt ps instead of dkl ps pt ce t h ps .
we also set the number of initial test set p 5in algorithm to fit the small sample sizes.
table relative efficiency in case of small test sets no.
average standard deviation maximum .
.
.
.
.
.
.
.
.
.
.
.
each of the experiments was repeated for times and the averages standard deviations and maximums of relative efficiency are presented in table .
we can see that our approach still achieved average relative efficiency of .
.
.
we also found that in almost all cases our approach outperformed srs despite of the expected fluctuation in efficiency.
the result indicates that our approach worked well even in the case that only a very small operational dataset was available.
related work the operational dnn testing addressed by this paper is in line with conventional operational testing for software reliability engineering but with different subjects constraints and solutions.
the work is also complementary to a line of recent research on dnn testing especially those hunting for adversarial examples leveraging structural coverages.
in the following we briefly discuss these related work and highlight how our works differs from them.
.
operational software testing the purpose of software testing for human written programs can be either fault detection i.e.
to find one or more error inducing inputs for the program under test or reliability assessment i.e.
to estimate the chance that the program will err in an operation environment.
frankl et al.
named these two kinds of testing debug testing andoperational testing respectively.
both of them are integral partsof software reliability engineering but they are different in philosophy and technology.
operational testing emphasizes maximal improvement of system reliability in operation with limited testing resources so it focuses more on bugs encountered more often in the operation context.
debug testing aims at finding as many bugs as possible without explicit consideration of their occurrences in operation.
the rationales include the belief that software as logic products should be correct in all contexts the unavailability of precise operational profile and the need for finding rare bugs for systems demanding high reliability.
note that as discussed in section .
these rationales do not apply to dnns.
technically operational testing heavily uses statistics and randomization.
a common theme is to minimize the variance of the estimator for reliability through optimal allocations of testing cases e.g.
.
for debug testing a frequent research topic is the automatic generation of test cases with the information provided by the program itself or its models.
particularly structural coverage criteria are used to guide the test case generation .
recently russo et al.
showed that further reliability improvement can be achieved by combining the strengths of operational testing and structural coverages .
in addition b hme called for a general statistical framework for software testing and proposed to view software testing as species discovery in order to borrow results from ecological biostatistics .
our work is similar to conventional operation testing in minimizing estimation variance with limited testing resources.
but we do not have operation profiles except for the unlabeled operational data.
instead of using adaptive estimate allocate test iterations we leverage the representation information of the dnn under testing to achieve efficient sampling in one step.
in a sense our conditioning on representation can be viewed as a generalization of the idea behind structural coverages in debug testing as discussed earlier.
.
dnn testing here we consider the testing of well trained dnn models as software artifacts but not the validation step in training process .
.
.
structural coverage criteria.
recently there is an increasing interest in software testing of machine learning programs .
especially some authors proposed several structural coverage criteria for dnn testing borrowing the concept of structural coverage criteria for human written programs .
the basic idea is to generate artificial examples to cover corner cases of the dnn model that usual test inputs are unable to touch in the spirit of debug testing.
the efficacy of the criteria were illustrated by the adversarial examples found in the testing.
an adversarial example is a slightly perturbed example that fools the dnn model.
the criteria can be roughly classified into three types neuron activation coverage deepxplore defines the neuron coverage of a neural network as the percentage of neurons that are activated by the given test set.
it uses this coverage joint with gradient based optimization to search for adversarial examples.
the weakness of this criterion is that it can be saturated with a small number of test inputs.
neuron output coverage deepgauge proposes finer grained criteria to overcome the weakness.
it divides the output of each 507esec fse august tallinn estonia zenan li xiaoxing ma chang xu chun cao jingwei xu and jian l sample size1.
.
.
.
standard deviation of accuracysrs ces a original dataset.
vgg sample size2 standard deviation of accuracysrs ces b original dataset resnet sample size1 standard deviation of accuracysrs ces c downsampling vgg sample size1 standard deviation of accuracysrs ces d downsampling resnet figure results of experiments with imagenet neuron into kequal sections and defines the k multisection coverage as the number of chunks that have been covered by the test set.
in addition the authors also considered the touch of outputs out of the ksections with neuron bound coverage .
neuron combination coverages the problem of coverage saturation can also be solved with the combination of neuron states.
deepcover and deepct use this strategy.
inspired by the mc dc coverage deepcover proposes a family of coverage metrics that are very fine grained.
in addition a recent proposal measures the surprise value of an input as the difference in dnn behavior between the input and the training data and use surprise coverage to guide the search for error inducing inputs .
we are skeptical about structural coverage criteria for dnn debug testing .
as explained in section .
and section .
dnn testing needs to be statistical holistic and operational.
in addition the homogeneity diversity wisdom of coverage oriented testing is broken by the fact that adversarial examples are pervasively distributed over the input space partitioned by these criteria .
despite of the difference in purpose one may wonder whether these coverage measures would help in accuracy estimation.
we believe these scalar measures are not informative enough for improving sampling efficiency.
to verify this we experimented with the surprise value in a similar way as css and it turned out to be ineffective.
the result can be found at our code website.
.
.
dnn testing for other purposes.
dnn testing is also conducted for purposes other than searching for adversarial examples .
for example tensorfuzz develops a coverage guided fuzzing cgf method to quickly find numerical errors in neural networks.
deepmutation builds a dnn mutation testing framework.
it simulates potential defects of dnns through training data mutations and program mutation.
mode conducts model state differential analysis to determine whether a model is over fitting or under fitting.
it then performs training input selection that is similar to program input selection in regression testing.
deeproad uses generative adversarial networks gans to automatically generate self driving images in different weather conditions for dnn testing.it is part of our future work to investigate whether our technique can be used for these purposes.
conclusions a crucial premise for a trained dnn model to work well in a specific operation context is that the distribution it learned from the training data is consistent with the operation context.
operational testing must be carried out to validate this premise before adopting the model.
although deep learning is generally considered as an approach relying on big data the operational testing of dnns is often constrained by a limited budget for labeling operational examples and thus it must take a small data approach that is statistically efficient.
in this paper we exploit the representation learned by the dnn model to boost the efficiency of operational dnn testing.
it is interesting to see that although we cannot trust any result produced by the model beforehand recall the unreliability of confidence in section .
we can still make use of its reasoning despite of its opaqueness.
the empirical evaluation confirmed the general efficacy of our approach based on conditioning on representation which reduced the number of labeled operational examples by about a half.
another interesting observation is that the homogeneity diversity wisdom of structural coverage guided testing can be generalized to conditioning for variance reduction in reliability estimation.
the importance of this generalization lies in its potential application to the testing of hybrid systems consisting of both dnns and human written programs.
there are many possible optimizations left for future work such as further variance reduction through adaptive importance sampling and reducing the volume of unlabeled operational data.
however the most important thing to us is how to reformulate the concept of bug and debug in a statistical holistic and operational way that is required for dnns.