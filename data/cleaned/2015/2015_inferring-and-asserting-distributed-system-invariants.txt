inferring and asserting distributed system invariants stewart grant university of british columbia vancouver bc canada stewbertgrant gmail.comhendrik cech university of bamberg bamberg germany hendrik.cech gmail.comivan beschastnikh university of british columbia vancouver bc canada bestchai cs.ubc.ca abstract distributedsystemsaredifficulttodebugandunderstand.akey reason for this is distributed state which is not easily accessible andmustbepiecedtogetherfromthestatesoftheindividualnodes in the system.
we proposedinv anautomatic approachto helpdevelopers of distributedsystemsuncovertheruntimedistributedstatepropertiesoftheirsystems.dinvusesstaticanddynamicprogramanalysestoinferrelationsbetweenvariablesatdifferentnodes.forexample inaleaderelectionalgorithm dinvcanrelatethevariable leaderatdifferent nodes to derive the invariant nodesi j leaderi leaderj.
this can increase the developer s confidence in the correctness of theirsystem.thedevelopercanalsousedinvtoconvertaninferred invariant into a distributed runtime assertion on distributed state.
we applied dinv to several popular distributed systems such as etcd raft hashicorp serf and taipei torrent which have between .7k and 144k loc and are widely used.
dinv derived useful invariants for these systems including invariants that capture the correctness of distributed routing strategies leadership and key hashdistribution.wealsouseddinvtoassertcorrectnessoftheinferred etcd raft invariants at runtime using these asserts to detect injected silent bugs.
introduction developing correct distributed systems remains a formidable challenge .onereasonforthisisthatdeveloperslackthetools to help them extract and reason about the distributed state of their systems .
the state of a sequential program is well defined stack and heap easy to inspect with breakpoints and can be checked for correctness with assertions.
however the state of a distributed execution is resident across multiple nodes and it is unclearhowtobestcomposethesenodestatesintoacoherentpicture let alone check these distributed node states for correctness.
in this work we propose a program analysis tool chain called dinvfor inferring likely data properties or invariants between variablesatdifferentnodesinadistributedsystem andforchecking these distributed invariants at runtime.
dinv inferredinvariantshelpdevelopersreasonaboutthedistributedstateof theirsystemsinvarious ways.inparticular they can confirm expected relationships between variables separated permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
a network to improve developer confidence in their system s correctness.
forexample consideratwo phasecommitprotocol inwhich the coordinator first queries other nodes for their vote and if all nodes including the coordinator voted commit then the coordinator broadcasts a tx commit otherwise it broadcasts a tx abort.
at the end of this protocol all nodes should either commit or abort.
to check if several non faulty runs of the system are correct a developercanexaminethedinv inferreddistributedstateinvariants for this set of executions.
in this case they can check whether dinv mined the invariant coordinator.commit replicai.commit for each replica iin the system.
this would mean that the commit stateacrossallnodeswasidenticalatconsistentsnapshotsofthe system.theycanalsousedinvtoaddaruntimeassertiontocheck this invariant in future runs.
dinvisthefirst automated end to endtooltoinferdistributed system state invariants.
the closest prior work by yabandeh et al.
requires developers to manually identify variables to log instrument their systems identify distributed cuts and so on.
dinv automates the entire process and requires minimal input from the developer.dinvisalsocomplementarywithmanyexistingtoolsfor checking distributed systems like modist and d3s .
these tools expect the developer to manually specify properties to check dinv can make these tools easier to use.
finally dinv includes a light weight and probabilistic assertion mechanism that can detect invariant violations with low controllable overhead.
dinvworksbyfirststaticallyinstrumentingthesystem scode either automatically or with user supplied annotations.
dinv uses staticprogramslicingtocapturethosevariablesthataffectorare affected by network communication at eachnode.
during system execution dinv instrumentation tracks these variables collects their concrete runtime values tags them with a vector timestamp andlogsthevaluesateachnode.oncethedeveloperhasdecided thatthesystemhasrunlongenough e.g.
executionofatestsuite theyrundinvonthegeneratedlogs.dinvusesvectortimestampsin thelogstocomposedistributedstates andthenmergesthesestates using three novel strategies into a series of system snapshots.
dinvthenusesaversionofthedaikontool toinferlikelydistributed stateinvariantsoverthetrackedvariablesinthemergedsnapshots.
our approach with dinv is pragmatic it does not require the developer to formally specify their system and it scales to large productionsystemsandlongexecutions.althoughdinvusesdynamic analysis which is incomplete dinv cannot reason about executions it does not observe we believe that it is useful because mostdistributedsystemsdeveloperstodayusedynamicanalysis tochecktheirsystems e.g.
withtesting and wehavebeenable to use dinv to infer and assert useful properties in several large systems.
acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden stewart grant hendrik cech and ivan beschastnikh instrumentation network usage detectorvector clock injection system execution mining distributed statedaikoninput go code detecting invariantssystem execution global state extraction global state groupingdetected invariantsruntime invariant assertions figure overview of the invariant inference steps in dinv.
we evaluated dinv by using it to study four systems written in go coreos s etcd taipei torrent groupcache andhashicorpserf .wetargeteddifferentsafetyandliveness properties in these large systems and ran dinv on a variety of executions of each system.
for example etcd uses the raft con sensus algorithm and we checked that the raft executions weinducedsatisfythestrongleaderprinciple logmatching and leader agreement.
we used dinv over several iterations to infer distributedstateinvariantsthatconfirmedthattheexecutionswe studied satisfy each of the targeted properties.
we also used dinv s assertionmechanismtocatchbugsinjectedintoraftthatsilently violate three key raft invariants.
we evaluated dinv s overhead and found that it can instrument etcd raft in a few seconds and that logging annotations in a raft cluster of nodes induced a system slowdown and used 1kb s of extra bandwidth.
to summarize this paper makes the following contributions we propose a static analysis technique for automatically detecting variables which comprise distributed state.
weproposeahierarchyofthreestrategiesforgroupingglobal system states snapshots for invariant inference and show that eachstrategyisusefulfordetectingdifferenttypesofdistributed invariants.
wedescribearuntimedistributedinvariantcheckingmechanism based onreal time snapshots.we evaluateits efficacyby using it to find injected silent bugs in etcd raft.
we implemented the above techniques in dinv an open source tool andevaluateitonavarietyofcomplexandwidelyused go systems that have between .7k and 144k loc.
distributed state background in this section we overview our model of distributed state and how it can be observed from the partially ordered logs of an execution.
weconsiderasystemcomposedofanumberof nodes eachof which has an independent thread of execution.
during a node s execution each instruction is an eventand anevent instance refers to a specific event wesometimes use eventfor both .
the stateof a nodeataneventinstanceisthesetofvaluesforallthevariables resident in the nodes memory.
the state of a node can be recorded by writing the variable values to a log.
event instances on a single node are totally ordered.
in this paper we consider message passing systems in which sending and receiving events create a partial ordering of event instances across nodes.
dinv uses vector clocks to establish this happened before ordering .
using a log of vector clocks causal chains of events from an execution can be analyzed.
we use log to refer to the sequence of node states paired with vector clocksgenerated in a single execution of the system.
section .
discusses howdinvautomaticallyinstrumentssystemstowritestates and vector clocks to a log.
mostofdinv sanalysesrunonalogproducedbyasystemafterit has executed.
detecting meaningful distributed invariants requires determiningvalidcombinationsofconcretenodestatestousefor inference.
for a given log a consistent cut is a set of events such that if an event ehappened before event f according to vector clocks then if fis in the cut eis also part of the cut.
local states on the frontier of a cut form a global state.
the complete set of global states which occur during the execution of a system forma lattice.
a point in this lattice is an n tuple of local node states composing a single global state.
a lattice edge connects two global states h i f happened before h again according to vector clocks andthevectorclocktimestampsof andhareseparatedby asingleincrementinlogicaltime.a groundstate isaglobalstatein whichallmessagessentupuntilthatpointhavealsobeenreceived i.e.
nomessagesareinflight .sections3.2and3.3coverdinv s global state analysis.
next we detail dinv s design.
dinv design automatically inferring distributed invariants requires resolving three research challenges what state should be logged and when?
how to infer distributed invariants from logged state?
how to enforce inferred distributed invariants?
dinv s analyses are a step by step procedure for solving these challenges figure .
this section details each step of the analysis.
.
system instrumentation challenge what state should be logged and when?
determining state to log is difficult because state with interesting distributed propertiesishardtoidentify.forexample somestateislocaltothe node and is unaffected by other nodes in the system distributed invariantsoversuchstateareuninteresting.weproposeandusethe following heuristic interesting distributed state must have dataflow to or from the network.
variables which interact with the network can be detected staticallyusingprogramslicing .forwardslicesrootedatnetworkreads andbackwardslicesfromnetworkwritesidentifythe affectedandaffectingvariables respectively.wedevelopedaninterprocedural slicing library for go and use go s networking conventions to statically identify network reads and writes.
variables contained in slices rooted at network calls comprise the set of network interacting variables.
figure lists partial code from serf that implements the swim protocol .
we will use this example throughout the paper.
logging point l1 on line logs variables authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
inferring and asserting distributed system invariants icse may june gothenburg sweden instrumentation strategy location choice variables choice function entrances exits auto auto network calls auto auto user defined annotations manual manual or auto table instrumentation strategies and the control automatic manual offered by each strategy for selecting state logging location and the set of logged variables.
transitivelyaffectedbythenetworkreadonline3.affectedvariables are underlined in the listing.
when should state be logged?
network interacting variables may be used throughout a system s codebase.
invariant inference depends crucially onwhere inthe codethe valuesof thesevariables are logged logging at different points may produce wildly differentinvariants.forexample toinferthemutualexclusioninvariants variablesmustbeloggedinsideacriticalsection ifnot thenthecaptured state would not reflect that the node ever executed a critical section a critical omission!
dinv provides developers with three mechanisms to control wheretologstate .twooftheseautomatethechoiceof locations function entrances exits or network calls and choice of state allnetworkinteractingvariables .thethirdstrategyprovides the developer with fine grained control over where and what state to log.
figure2illustratestwologgingannotations l2a dump annotation line6 and l1aparameterizeddump statement line12 .
the first logs distributed state when a pingis received the second logs state before checking for timeouts.
trackingpartialorder.
vectorclocksareacanonicalmeansfor recording the happens before relation in a distributed system .
dinv automatically instruments any go program that uses go s networking netlibrary with vector clocks.
it does this by detecting usesofthe netlibraryandbymutatingtheabstractsyntaxtree.dinv supportscommonprotocolslikeip udp tcp rpc andipc.vector clocksareappendedor stripped fromnetworkpayloads andthe original function isexecuted on the instrumentedarguments.
for example anetworkwritelike conn.write buffer istransformed into dinv.write conn.write buffer .
insummary oursolutiontochallenge1istologonlythevariables that interact with the network at automatically generated and user specified lines of code.
next we explain how the logs generated by the execution of instrumented nodes are analyzed to infer distributed invariants.
.
extracting global states challenge how to infer distributed invariants from logged state?
anomnipotentobservercanestablishatotalorderonalldistributed events.
in practice an ordering that is feasible to derive is the happens before relation definedbythecausalprecedenceofsent and received messages.
the happens before is a partial ordering on events.
we use latticeto refer to this ordering.
eachpointinthelatticeisaconsistentcut definedinsection2 of theloggedexecution andtheentirelatticeisthesetofallconsistent cuts .
figure relates a message sequence diagram to its1 func s serfnode serf conn udpconnection for true msg conn.read switch msg.type case ping conn.writetoudp ack msg.sender break case gossip s.events append s.events msg.event timeout s.checkfortimeouts switch timeout.type case ping conn.writetoudp ping timeout.node break18 case gossip gossip s.events break21 dump dinv.dump l1 msg.type msg.sender msg.event s.events l2 l1 figure code excerpt from serf with underlined network interacting variables contained in the forward slice from conn.read on line .
line l2 and line l1 are example instrumenting annotations.
node node ping ack b a ...... ... figure a message sequence diagram of an execution of code in figure with two nodes their local vector clocks paired with each event in brackets and message deltascomputed for each event .
b partial lattice of theexecution.
each vertex displays the corresponding vectortime and ground states are bolded.
corresponding lattice for an execution of the code in figure with two nodes.
a lattice point is a global state composed of an n tuple of local states.
global invariants or invariants that hold across multiple nodes are testable by extracting the corresponding global state from a log and asserting the invariant on that state.
due to its generality this lattice analysis incurs significant complexity.
in the worst case an execution without messages with n nodes and eevents will produce a lattice of size en.
becausetestinginvariantsonanexponentialnumberofstatesis infeasible we propose ground states as a heuristic for reducing thenumberoflatticepointstotestforinvariants.a groundstate isa consistent cut with the additional constraint that all sent messages authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden stewart grant hendrik cech and ivan beschastnikh node node l1 acknode node timeout ping l2 l1 l1l1 timeout l1gossip timeoutgossip l1global state globalstate globalstate figure execution of swim code from figure .
node responds to a pingfrom node .
concurrently node propagates node s gossip message.
l1 l2mark local logged state messages events in figure .
dashed lines markthreeglobalstates whicharealsogroundstates eachann tuple of the closest local node states above the dashed line.
have been received i.e.
no messages are in flight .
intuitively a groundstate isaline throughamessage sequencediagramwhichdoes not cross any messages see global states in figure .
analyzing just the ground states reduces completeness a global statewhichisnotagroundstatemayviolateaninvariant.however distributed algorithms are typically specified with invariants over ground states when the system has acquiesced and all messages havebeenprocessed.aswell inferenceongroundstates reduces analysistimebyordersofmagnitude andprovidesaqualitysample of global states in real systems1.
a variety of lattice construction algorithms exist .
longrunningexecutionsoflooselycommunicatingsystemscaneasily generate lattices larger than main memory.
to avoid this dinv utilizesasequentialantichainalgorithmwhichgeneratesalattice levelbylevel.generatinglevel nofalatticerequiresalogandlevel n of the lattice.
this allows dinv to flush most of the lattice to disk as just two levels must be maintained in memory.
ground states are computable with a linear scan of both a log andlattice.first alogisscannedanda deltaof sent received number of messages is calculated per local event on each node.for example if by some event ea node had sent3 messages and received message e s delta is .
the lattice is then scanned and for each global state the deltas of each local event in a global state are summed.
a sum of identifies a ground state e.g.
figure .
lost messages pose a theoretical threat to ground state analysis a single message loss rules out future ground states.
in practice lost messages do not affect receiver s state and are functionally equivalenttolocaleventsatthesender.dinvhandlesexecutions withlostmessagesbydetectinglostmessagesusingvectorclock timestamps and omitting them from the ground state computation.
thefirstcomponentofoursolutiontothechallengeofinferring distributed invariants from logged state is to infer invariants over 1forcompletenessdinvallowsuserstoanalyzeallglobalstatesatthecostsubstantial runtime.
all results in this paper are based on ground state analysis.as sr to node node l1 ping l2node node l1gossipl1global state as sr to node node node node l1gossipl1global state sr to l1ackl1 node node node node l1gossip l1global state l1 l1gossipas figure5 themergingoflocalstatesfrom3globalstatesby our three grouping strategies.
on the left each global state corresponds to a dashed line from figure .
logged localstatesaremarkedbygraycircles eachcontainsthevariableslisted on figure line .
on the right are group id s fromtherespectivestrategiesas all states sr send receive and to total order.
highlighted is a group of states merged by all states.inboldisagroupofstatesmergedbysend receive.
groundstates.wechoosethisapproachbecauseitisscalableand makes no assumptions about the system.
.
strategies to group global states some invariants hold globally at all times but others hold dur ing protocol specific event sequences and between select nodes.
without apriori system knowledge many possible combinations of global states may support or refute an invariant.
the possible combinations of global states in a lattice is 2en which is intractable to analyze andalsolargelyredundant forrealsystems.ourgoalistoautomatically tease apart distinct protocols group the global states in which they executed and infer invariants on the group.
our solutionreliesontheobservationthatmanyprotocolsarespecified ascausalchainsofevents.oneofourresearchcontributionisaset of strategies for grouping global states for invariant inference group all global states together all states groupstatesbysendingandreceivingnodepairs send receive group states by totally ordered message sequences total order figure is an example execution showing two protocol specific causal chains ping ack andgossip gossip included are three globalstates shownasdashedlines .figure5furthershowshow the global states in figure are decomposed by each strategy and how the state tuples with matching identifiers highlighted aregrouped for further invariant inference.
we will use figure to explain each strategy.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
inferring and asserting distributed system invariants icse may june gothenburg sweden all statesstrategy.
themostgeneralformofsysteminvariant isonewhichholdsonallobservablestate andbetweenarbitrary nodes.all states merges all local states of a global state together regardless of causality between them.
each local state has an id nodeid.lo id wherenodeiddenotesthenodeandlogiddenotes theloggingstatement.amergedsetofsuchidsformagroupid .
merged local states which share a groupid are grouped together for invariant inference.
figure5shows all states as mergingalllocalstatesfromthe three global states.
the groupid resulting from all states merging globalstate1 is noothermergedstate shares this groupid.
merged states from global state and global state highlighted in orange share the groupid n0.l1 n1.l1 n2.l1 n3.l1 .
groupids produce multiple groups when there is more than one logging statement per node.
using the logid as a component of a groupid ensures that each merged state in a group containsexactlythesamevariables asseparateloggingstatements neednotcontainthesamesetofvariables.invariantsinferredby the all states strategy are the strongest as they hold across the largest sample of observable states and across all nodes.
send receivestrategy.
manyprotocolsdictatethebehaviorbetweenpairsofnodes.thesend receivestrategymergestogether thestatesofdirectlycommunicatingpairsofnodes.send receive groupidshavetheform wherenodep communicated with nodeqandlo iexecuted before the communication and lo jexecuted after the communication.
for example in figure5globalstate1send receive sr hastwogroups thefirst group correspondingtothe pingbetweennode0 and node and group captures the gossipmessage.
anadvantagetothisstrategyisthatpropertieswhichholdafter communication but not at all times are tested on subsets ofstates.
invariants like those that depend on eventual consistency require thisfor detection.
fromour running example each swim node maintains a list of eventswhich are synchronized with gossip messages.iftheinvariant n1.events n2.events wastestedonthe asgroup from figure highlighted in orange the invariant would beviolatedbecauseinglobalstate2node1hasnotyetreceived thegossipmessage.
in contrast if the same invariant was tested on the send receive group from global state it would hold because node synchronized its events after receiving the message.
total order strategy.
fine grained protocols such as leader election dictate a causal behavior across multiple nodes.
the totalorder strategy merges the local states from causal chainsof communicating nodes.
the groupid for this strategy has the form suchthatthelocalstate nodep.lo i happenedbefore nodeq.lo j andallintermediate nodeid lo id pairs.
infigure 5the tomerged group from global state is the result of merging all local states along the gossipmessages causal path from node to node .
total order has thesame ability assend receive to detecteventual consistency in serf but it detects it in a stronger context.
in the case of global state group the invariant n1.events n2.events n3.events would be inferred.
our complete solution to challenge is to infer invariants on groups of global states merged by one of three strategies.
these strategiesencodeheuristicsinformedbybestpracticesindistributed1func assertleadershipagreement bool 2fori i len assert.node i i is a nodeid index 3ifassert.getvar i leader !
assert.getvar i leader return false 5return true listing a distributed assertion that checks that allnodes in a cluster agree on the leader.
systemdesignandradicallydecreasethespaceofpossiblegroupings.
dinv further scales its analysis by for example discarding identicalloggedinstancesofnodestateswhichspanseparateglobal states since these provide no new information.
next we explain how dinv infers invariants using a modified version of daikon.
.
inferring distributed invariants daikon isdesignedforsequentialsystemanddoesnotsupport inferenceoverpartiallyorderedcollectionsofstateswithdisjoint variable sets.
further daikon includes templates for binary and ternaryinvariants anddoesnotsupportn aryinvariantsnecessary for distributed specifications.
dinv uses daikon by presenting it with a synthetic program point that corresponds to a distributed state.
however in a sequential program the same variables are always present at each programpointandmergedstatesmaybecomposedofdifferentsets of variables from various logging points.
our solution reviewed in section .
is to only merge states with identical sets of variables2.
we also added several n ary templates such as equality to daikon.
inferred invariants span the local state of all nodes.
for example the group from figure s global state 3towould have the invariant node1.events node2.events node3.events ratherthanthetwobinaryinvariants.thisreduces effort in comprehending relations spanning more than two nodes.
.
asserting inferred invariants challenge howtoenforceinferreddistributedinvariants?
dinv includes an assertion library to help developers check inferred distributed safety properties at runtime with user defined assertions listing shows an example .
prior approaches to checking distributed predicates rely on variants of the global snapshot algorithm based on logical clocks .
by contrast dinv uses a light weight real time global snapshot algorithm for assertions.
dinv sreal timeassertionshave3components aroundtriptime rtt estimator a physical clock synchronizer and an assertion algorithm.therttestimatorperiodicallypingsothernodesand computes an estimate of rtt betweeneach node.
to synchronize clocks dinv uses the berkeley algorithm .
theassertionalgorithmworksasfollows.whenanode aexecutes an assertion statement ablocks until the asserted predicate isresolved.first usingtherttestimator aschedulesastatesnapshot time tthat is in the future by the largest rtt from ato the othernodes.next asendsasnapshotrequestwithtime tandrequestedvariablenamestoallnodes.onreceivingarequestfrom 2as a more advanced heuristic dinv also supports analysis of intersections of logged variables authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden stewart grant hendrik cech and ivan beschastnikh a an o d ebcreates a thread that sleeps until t. oncebreaches t thisthreadsnapshotsthevaluesoftherequestedvariablesand sends these to a. onceahas received all the snapshots it needs it evaluates the asserted predicate.
scheduling snapshots with physical clocks even if they are synchronized has the disadvantage that the resulting snapshot may violatethe happens beforerelation e.g.
node csnapshotsits state sendamessagetonode d andthen dsnapshotsitsstate .toavoid inconsistent snapshots dinv uses vector clock timestamps to determine if a snapshot represents a ground state for the system.
if not dinv logs the failure to assert skips the assertion and will retry the assertion the next time it is reached.
due to blocking semantics asserts in frequently executed code canreduceperformance.dinvallowsdeveloperstomitigatethis withprobabilistic assertions which execute with some user defined probability.
we expect developers to use high probabilities duringtesting forprecision andlowprobabilitiesinproduction for performance.
our solution to challenge three is a mechanism for probabilistic distributed assertions.
implementation dinvisimplementedin8k3linesofgocode .ithasbeentested on ubuntu .
.
and relies on go .
.
dinv implements an optimized version of the vector clock algorithm and uses a manually constructed database of wrapper functions for go s net.
we use go s ast library to build traverse and mutate the ast of aprogramforinstrumentation.dinv sstateinstrumentationbuilds oncontrolanddataflowalgorithmsingodoctor .dinvuses daikon version .
.
.
applying dinv to complex systems inthissectionwedescribeourmethodologyforevaluatingdinv on complex systems in section .
for our example we use serf s eventually consistent group membership property.
prior to this evaluationwehadnoknowledgeaboutthesystemsweanalyzed with the exception of the paper describing raft .
in the evaluation we used dinv in concert with documentation and source code to understand each system.
we highlight four techniques we used and that we believe make dinv more usable.
when applying dinv to a new codebase we used its completely automated facilities to survey the systems invariants to learn about its behavior.
initially we instrumented serf4 which injected 400loggingstatementsthatlogged20 40variableseach.weranthe system s test suite and processed the logs with dinv.
the merging strategies parsed the log into groups of global states.
in aggregate across all groups daikon inferred approximately million invariants many of which related constants.
we used the massive set of invariants to identify variables relevant to consistency.
usinglinenumbersasindexesintothesourcecodewe refined our analysis to a smaller set of functions.
a second execution resulted in groups and a total of invariants.
all states invariantsfalsifiednodestateequality sowe 3all loc counts in the paper were collected using cloc test code is omitted from the counts.
4serfusesencodersandrequiredthemanualadditionof20lines oneforeachsending and receiving line of code.deduced that consistency did not hold globally at all times.
we examinedsend receive invariantsforafine grainedviewofthesystem.
the updates were fully observable by monitoring assignments to a single structure which maintained the cluster s health so we composed dump statements to instrument this structure.
running thetestsagain whileloggingonlythecluster shealth resultedin25groups withatotalof40inferredinvariants.
send receive and totalorderoutputs were composed of the desired equality invariants between the node states.
in our evaluation of dinv with three other complex systems we followed the above approach of iterativly refining the logged variables to zero in on properties over key distributed state.
togenerateassertionsfromtheinferredinvariantswemanually wrotebooleanfunctions e.g.
listing1 tocheckaninvariantacross nodesatruntime.section7describesourexperienceswithusing the assertion mechanism.
evaluation inferring invariants in the following section we use dinv to analyze four systems hashicorpserf ourrunningexample groupcache taipeitorrent and coreos s etcd .
we describe each system and theirproperties andreportoninvariantsdetectedbydinvandwhat they tell us about the correctness of the system.
table overviews the invariants we targeted in our study.
experimental setup.
all inference experiments were run on an intel machine with a core i5 cpu and 8gb of memory run ning ubuntu .
.
all applications were compiled using go .
forlinux amd64.experimentswererunonasinglemachineusing a mixture of locally referenced ports and iptable configurationstosimulateanetwork.runtimestatisticswerecollectedusing runlim formemoryandtiminginformation andiptablesfor tracking bandwidth overhead.
.
analyzing the swim protocol in serf serf is a system for cluster membership failure detection and event propagation.
serf has .3k loc and is used by hashicorp inseveral majorproducts.serf buildsona gossipprotocollibrary based on swim .
each swim node maintains an array of all other nodes liveness state alive suspectedordead.asuspectedfailureisgossipedwhen heartbeatmessagesarenotacknowledged andcompletefailures dead is gossiped once a specified subset of nodes suspect a failure.
throughout this process node state updates are attached topings ping reqs and acks and spread by gossip messages to en sure eventual consistency.
a receiving node japplies updates it receives only if it does not have more recent information.
dinvcan observe this property by logging state changes i.e.
node i sets node k s state to alive.
thesend receive strategy inferred the invariant nodei.stateofk nodej.stateofk for all pairs of nodesiandj.
further the total order strategy inferred the invariantnodei.stateofk nodej.stateofk nodem.stateofk onalltransitivesequencesofgossipmessagesonclustersupto4 nodes.
the detection of this invariant required all orderings of gos sip propagation to occur many times and thus required the longest executions and analysis time.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
inferring and asserting distributed system invariants icse may june gothenburg sweden system and targeted propertydinv inferred invariant description serf eventual consistency nodesi j nodestate i nodestate j nodes distribute membership changes correctly.
groupcache key ownership nodesi j i nequalj ownedkeys i ownedkeys j nodes are responsible for disjoint key sets.
kademlialog.
resource resolution requestr summationtext msgs forr log peers all resource requests must be satisfied in no more than o lo n messages.
kademlia minimal distance routing keyk n od ex if xor k x minimal then xstoreskdht nodesstores avalue onlyif itsid hasthe minimalxordistance in the cluster to the value id.
raftstrong leader principle followeri len leader log len i s log all appended log entries must be propagated by the leader.
raftlog matching nodesi j i fi log j log x c i log j log if two logs contain an entry with the same index and term then the logs are identical in all previous entries.
raft leader agreementif nodei s.t.ileader then j nequali jfollower if a leader exists then all other nodes are followers.
table invariants listed by system their corresponding distributed state invariants and descriptions.
ininstrumentingnetworkcalls twocodepathshadtobeconsidered onefortcpandoneforudp.dinv sautomaticinstrumentationworkedforudp.inthetcpcasecustomstreamdecoding prevented automatic instrumentation instead we wrote loc to insert extract vector clocks.
we setup an execution environment where nodes were periodicallypartitionedtoforcefrequentpropagationofmembership updates.
observing nodes in an execution with such partitions resulted in dinv inferring all invariants.
we were also able to observe and gather similar results about serfs behavior in more complex executions i.e.
round robin partitions.
anexecutionwith100partitionswasrunningfor24minutes5 and produced .
mb of log files which dinv analyzed in less than 2minutes.theresultsandlackofcontradictinginvariantsleaves us confident that serf s update dissemination is correct.
.
analyzing groupcache groupcacheisanopensourcegoimplementationofmemcached writtenin1786loc .groupcachenodesactasbothclientsand servers for key requests.
like memcached groupcache assigns key ownership to nodes but nodes hold no state apart from multiple caches.eachnodeisresponsibleforauniquesetofkeyswhichit owns exclusively.
groupcacherequiresuserstoprovidea getterfunctionwhich maps keys to values.
getmessages are encoded using protobuf andexchangedoverhttp.protobufencapsulatesgo sstandard networkinglibrary makingthecallsinvisibletodinv svectorclock instrumentation.wemanuallyaugmentedthehttpheaderwith vector clocks which required additional loc.
because of groupcache s static key partitioning the invariant nodei.keys nequalnodej.keysholds globally at all times and not on precise protocol specific sequences.
our groupcache test program wasrunonconfigurationsof2 8nodes eachofwhichrequested 2kkeys.dinvdetectedthecentralkeydistributionproperty see table with each merging strategy.
here all states provides the 5serf was given seconds after and before each partition to detect and propagate membership changes.strongest evidence of the invariants correctness as the key ownership can be demonstrated to hold on all observable global states.
thekeyownershippropertywasquicklyidentifiedwithdinv byathird yearundergraduatestudent whohadlittleexperience with dinv or groupcache.
.
analyzing taipei torrent taipei torrentisanopensourcebittorrentclientandtracker.its client program uses nictuku s implementation of the kademlia distributed hash table dht protocol to resolve peer and resource queries .
taipei torrent and nictuku are implemented in .8k and .9k loc respectively.
kademlia uses a virtual binary tree routing topology structured on unique ids to resolve resources and peers.
peers maintain routinginformationaboutasinglepeerineverysub treeonthepath from their location to the rootof the tree.
kademlia has primary types of messages storeandfind value.
storeinstructs a peer to store a value.
find value resolves requestsforstoredvalues.apeer sresponsetoa find value query is the list of peers on its sub tree with the closest xor distances to the requested value.
find value is executed iteratively on the peer list until the value is found.
these queries are resolved within o lo peers where peers is the total number of peers.
weautomaticallyinjectedvectorclocksintotaipei torrentin3s.
manual logging functions were used because variables containing routing information were not readily available.
we introduced our owncounterwith2linesofcodetotrackthenumberof find value messages propagated in the cluster.
taipei torrent has sparse communication between nodes the result is a large space of partial orderings.
lattices built from the traces of taipei torrent consisted of million points.
log analysis took upwards of minutes with an upper limit of hours requiring frequent writes to disk as thelatticeexceededavailablememory.dinvsucceededinanalyzingtheseexecutions althoughthecommunicationpatternwasa challenge for our techniques.
lattice inflation limited our analysis to executions with at most peers.
kademlia specifies that peers must store and serve resources withtheminimumxordistancetotheirids.further find value authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden stewart grant hendrik cech and ivan beschastnikh requestsmustresolvetotheminimumdistancepeer.totestthecorrectnessof find value requestsweaddeda5linefunctionwhich output the minimum distance of the peers and resources in the routing table and logged it.
to test routing we ran clusters with peers using a variety of topologies by controlling peer ids.
we logged state after the results of a find value request were added to a peer srouting table.on eachexecution wefound that peersi j peeri.min distance peerj.min distancein all total order groups.
this invariant in conjunction with o lo n message bound provides strong evidence for the correctness of nictuku s implementation of kademlia.
.
analyzing etcd raft etcdisadistributedkey valuestorewhichreliesontheraftconsensusalgorithm .raftspecifiesthatonlyleadersserverequests andfollowersreplicatealeader sstate.followersuseaheartbeat to detect leader failure starting elections on heartbeat timeouts.etcd is used by applications such as kubernetes fleet andlocksmith makingthecorrectnessofitsconsensusalgorithmparamounttolargetechcompaniessuchasebay.etcdraft is implemented in 144k loc.
etcdusesencoderstowrapnetworkconnections somanualvector clock instrumentation was required.
log analysis took between 15s.
etcd was controlled using scripts.
one to launch a clusters of nodes another to partition nodes and one to issue a 30s ycsb a workload put get requests .
strong leadership .
an integral property of raft is strong leadership onlytheleadermayissueanappendentriescommandtothe rest of the cluster.
this property manifests itself in a num ber of data invariants.
a leader s log should be longer than thelog of each follower.
further the leader s commit index and logtermshouldbelargerthanthatofthefollowers.weloggedcom mit indices and the length of the log.
in each case the invari ant leader.lo size follower.lo size andleader.commitindex follower.commitindex was detected by the send receive strategy.
log matching.
raft asserts if two logs contain an entry with the same index and term then the logs are identical in all entries upto the given index .
this property is hard to detect explicitly because it requires conditional logic on arrays.
we were able to detectthatinallcases nodei.commitindex nodej.commitindex nodei.lo nodej.lo nodei.lo nodej.lo up to the all states grouping.
this shows that if any two nodes have the same log index and the value at that index match theirentirelogsmatch thisisevidenceofthelogmatchingproperty.
leadership agreement .
at most one leader can exist at a time in an unpartitioned network and all unpartitioned members ofa cluster must agree on a leader after partitioning.
by logging leadershipstatevariableswhenleadershipwasestablished wewereabletoderivethat nodeistate leader jnodej.leader nodei j nequali nodej.state follower.theseinvariantsweredetectedin bothsend receive andtotal order groups.thisindicatesthatafter thepartitionoccurred allnodesagreeonaleader andthatallnodes but the leader are followers.
strongleadership logmatching andleadershipagreementare invariants ofa correctraft implementation.
bychecking their exis tence weproducedstrongevidenceforthecorrectnessofetcdraft.raft invariant loc p .
p .
p .
strong leadership .
.
.
leadership agreement .
.
.
log matching .
.
.
table3 loctoimplementandtime sec todetectaninvariant violation with probabilistic asserts.
further wehaveshowndinv sabilitytodetectusefulproperties over distributed state of large and non trivial system.
evaluation asserting invariants dinv inferred invariants can be used for comprehension.
however they can also be converted into assertion predicates to find regressionerrorsatruntime.herewedetailhowweusedthedinvassert mechanism to check the inferred etcd raft invariants at runtime.
wedevelopeddistributedassertionsforeachofetcd sinvariants.
we then evaluated the ability of these assertions to find bugs by using them with buggy versions of raft.
for this we manuallycreated three bugs each of which violates one of the three raft invariants.
all bugs cause a violation withoutcausing raft to crash or impact its ability to serve client requests.
that is each bug produces silent errors and is difficult to detect.
strong leadership bug.
in raft only the leader may issue the command to append entries to a replicated log.
in our two line bug an unauthorized follower broadcasted append entries and committedtoitsownlog.raft salgorithmtoleratesthisbugbecausethe leader has authority to overwrite followers logs.
however oncealeaderhaswrittentodiskinaterm thesystemexpectsthatall followers logs are synchronized.
etcd does not verify synchronizationsothebugcausestheleadertoperpetuallyissuelogcorrectionmessagestothebuggyfollower.theinvariantforstrongleadership isthattheleaderslogsizeisgreaterthanallthefollowers logsif the leader has committed in the current term.
leadership agreement bug.
ifaleaderexistsinagiventerm allnodesmustagreeonthisleaderforleadershipagreementtohold.
we introduced a line bug which caused followers to randomly select a leader from their list of peers post election.
etcd continues to execute with this bug.
however followers periodically time out waitingformessagesfromafalseleaderandinitiateanewelection.
in a non buggy execution if any two nodes agree on a leader for a given term then they agree on the same leader.
logmatching bug.
logmatchingiscriticaltoetcd sfaulttolerance.iflogmatchingdoesnotholdetcd skeyvaluestorereturns inconsistent results depending on which node is the leader.
etcd assumesthatalllogentrieswrittentodiskarecorrect.toviolate the log matching invariant we injected a line bug to corrupt acommitted log at a random place and time.
with this bug etcdexecutes as normal and assumes that the nodes logs are correct.
the log matching invariant states that if any two nodes have an entrywiththesameterm index anddata thenallpriorlogentries match.
table shows our experimental results.
assertions for above invariants ranged in size.
log matching was the most complex loc checking it requires a comparison of logs from every pair of nodes.
the assertion iterates through all pairs of node logs checking themfor inconsistencies.
theother two assertions were expressed in under loc.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
inferring and asserting distributed system invariants icse may june gothenburg sweden number of annotationsexecuted annotationslog size mb runtime s runtime overhead .
.8k .
.
.
.6k .
.
.
14k .
.
.
28k .
.
.
85k .
.
.
261k .
.
.
table impact of dinv annotations on raft performance.
weranraftwitheachbugandusedassertionswithprobabilities of .
.
and .
.
we measured the average time delay between the instant a bug was injected and when it was detected.
we found that all asserts found the bugs but they took longer with lower probabilities.consideringtheseverityofthesebugs webelievethat thedelayofafewmoresecondstodetecttheproblemisreasonable given no other alternative .
we discuss the associated decrease in overhead with using probabilistic assertions in section .
.
evaluation dinv overhead dinv imposes severaloverheads.
theseinclude thetime toinstrumentthesystem runtimeandnetworkoverheadsduetologging and injected vector clocks and the running time of the dynamic analysis that dinv must perform on the collected logs.
this section details these overheads.
static analysis runtime.
to benchmark the performance of dinv sstaticanalysis detectingnetworkingcalls addinglogging code etc weusedetcdraft whichcontains144klocandthousandsofvariables.wemeasuredinstrumentationtimewithincreas ingcountsofrandomlylocated dumpannotations.instrumentation timeremainedconstantat3suntil4kannotationsatwhichpointit increased slightly to .2s.
at 64k annotations far beyond practical use runtime was .7s.
logging overhead.
logging state at runtime slows down the system.
we instrumented etcd raft with increasing number of logging statements each one logging variables.
we benchmarked aclusterwith3nodes andaycsb aworkload.eachclusterwas run times and we averaged the total running time.
table shows a linear relationship between the number of logging statementsand runtime.
in practice just two annotations were sufficient to detecttheraftinvariants.theaverageexecutiontimeofasingle logging statement is microseconds.
in our local area network with a round trip time of .05ms while running etcd with second timeouts we can introduce approximately 50k logging statements per node before perturbing the system.
bandwidthoverhead.
vectorclocksintroducebandwidthoverhead.
each entry in dinv s vector clocks timestamp has two bit integers one to identify the node and the other is the node s logical clock timestamp.
the overhead of vector clocks is a product of thenumberofinteractingnodesinanexecutionandthenumber of messages bits nodes messa es .
to evaluate bandwidth overhead in a real system we executed etcd raft using the setup abovewhilevaryingthenumberofnodes.thebandwidthsofall nodes was aggregated together for these measurements.
we found that adding vector clocks to raft slowed down the broadcast ofheartbeats and caused a reduction in bandwidth of 10kb s for all nodes in a node cluster.
at nodes and above the bandwidthsystem runtime s raft log mb raft analysis s gcache log mb gcache analysis s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table generated dinv log size and dinv s dynamic analysis running time for varying system run times for two systems etcd raft and groupcache gcache .
overhead grew linearly with an overhead of 1kb s for nodes and 10kb s for nodes.
dynamicanalysisruntime.
dinv sdynamicanalysisruntime is affected by the size of the log and the number of nodes in the execution.
to measure its performance versus the length of execution we analyzed etcd raft and groupcache.
we exercised them byissuing10requestspersecondtoeachsystem.todemonstratehowdinv sanalysisperformswithregardtothelengthofexecution we analyzedtheresultinglogsof3nodeclusters whichwererunfor intervals in increments of 30s.
results in table show that dinv s log analysis scales linearly with system running time.
to measure how analysis time is affected by the number of nodes in an execution we ran etcd for 30s exercising it with 10client requests per second and running clusters with increasingnumber of nodes.
our results show that dinv s runtime growsexponentially with the number of nodes.
we measured analysistimes of 25s 75s and 725s for logs containing and nodes respectively.
dinv s runtime is exponential in the number of nodes due to the exponential growth of partial orderings our analysis techniquescompute.thisindicatesthatdinviscurrentlylimited to analyzing distributed systems with a small number of nodes.
.
distributed assertions overhead weevaluatedtheoverheadofdinv sassertionmechanismonmicrosoftazure.thesetupconsistedof4vms 3serversand1client allrunningubuntu16.
.theservervmshad3.5gbofmemory and a single core capable of performing iops.
the client was used to saturate the servers and had cores and 56gb of memory and could perform iops.
below we measure the end to end latency of client requests to the etcd cluster.
we established a baseline using unmodified etcd.
the system wasexercisedat3loadlevels and200clientrequestper second eachtestwasrunfor100s.eachassertinsection7wasrun underthesameconditions.assertionswereplacedinetcd sinner event loop which executed on every received message and timer event.onaverage5eventsoccurredperclientrequest.wealsoranexperimentswithprobabilisticassertswithtwoprobabilities p .
andp .01andmeasuredthemedianslowdowninclientrequest response times.
thegreatestslowdownwasincurredwhenassertswereplacedat bottleneckprogrampoints.forexample assertingstrongleadership withp .0causeda52xslowdown aseachclientrequestwasforced to wait for multiple asserts to execute.
using p .
reduced this to .5x and p .
reduced it further to .02x.
leader agreement and logmatchingassertswereperformedbyfollowers whicharenot authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden stewart grant hendrik cech and ivan beschastnikh onthecriticalpathforclientrequestprocessing.bothassertions with p .
introduced only a .09x slowdown.
discussion effort in using dinv.
in our evaluation we considered four large distributed systems none of which we were familiar with prior to this study.
in each case we used all of the resources available to us papers source code documentation to understand the desired system properties and to interpret dinv s output.
as we had no prior knowledge about the four systems in our evaluation and weresuccessfulininferringinterestingproperties weareconfident that with proper training developers would be able to similarly instrument their systems.
althoughwedidnotformallyevaluatedinvwithdevelopers we do have two pieces of anecdotal evidence that dinv is not difficult touse.first graduatestudentswithnopriordistributedsystems backgroundsuccessfullyuseddinvontheirsystemsinadistributed systems course.
second groupcache section .
was analyzed by anundergraduatestudentwhowasfamiliarwithdistributedsystemsbutnotwithdinv.afterinstallinggroupcacheandbecomingfamiliarwithitstestsuit hewasabletoisolatethekeydistribution invariant withina workday.
althoughanecdotal webelieve these experiences indicate that dinv is usable by developers.
we plan to evaluate dinv with developers in our future work.
dynamic analysis.
dinv infers likelyinvariants because it is a dynamic analysis approach that only considers a finite set of system behaviors.
the inferred invariants are not a verification of the system but they could be used for runtime checking as we demonstrated in section or to bootstrap verification .
executionscontainingfailures .dinv sinferencepipelinewas designed to infer invariants from executions with no node failures.
dinv sassertionmechanism however candetectinvariantviolations even when failures occur.
related work mining distributed systems information.
dinv is a specification mining tool that builds on daikon which cannot mine distributed state invariants on its own.
daikon has been previously used to assist a theorem prover in v erifying distributed algorithms by running over simulated execution traces .
the closest work to dinv is work by yabandeh et al.
who inferalmost invariantsindistributedsystems invariantsthatare true in most cases.
they also build on daikon but the process ofidentifyingvariablestolog instrumentingthesystem piecing together distributed cuts and composing logs from different nodes isamanualprocess.ourapproachactuallyinstrumentsthesystem computes ground states and also checks invariants at runtime.
other approaches that mine distributed concurrent specifications produce symbolic message sequence graphs to group ma chines into classes based on similarities in their communicationpatterns ltl properties relating events between nodes and infer communicating finite state machines .
this prior work focuseson eventsandcantraceitsrootsbacktocookandwolf s original work that noted concurrency as a challenge .
none of these techniques can detect distributed dataproperties.otherworkminesavarietyofdistributedsysteminformation for se purposes.
for example some work uses mining to detect dependencies anomalies and performance bugs .
other analysis of distributed systems.
dynamicanalysisof distributed systems has yielded several tools to aid developers.
for example distia implements impact analysis googles dapper analyzes traces to produce call graphs and performance information and lprof instruments java bytecode with synchronized timestamps and uses logs to infer temporal properties.
twopriortoolsusedaikontoderiveinvariantsofnetworkedand concurrentsystems.invarscope detectsinvariantsinjavascript applications but does not generalize beyond client server systems.
udon infersdatainvariantsofmulti threadedprogramswhere program state is shared between threads.
monitoring systems such as fay and pivot tracing use dynamic instrumentation for real time diagnosis of distributed systemsbyactivatingtracepointsatruntime.thesetoolsdonot infer properties from the traces they capture.
formalmethodsfordistributedsystems.
unlikerecentmethodsthatusetheoremprovingtosynthesizecorrectsystemsbycon struction ourworkisimmediatelyapplicabletoexisting productionsystems.previousworkalsoconsiderscheckingexistingsystemimplementationsdirectly orcheckingsystem properties at runtime or during program replay .
dinvalsoincludesaruntimeassertioncheckingmechanism.but incontrasttopriorworkliked3s dinv smechanism schedules node state snapshots using synchronized physical clocks and uses probabilistic assertions to decrease overhead.
more fundamentally previously work assumes that a developer can andiswillingto specifypropertiesoftheirsystem.bycontrast dinvdoesnotrequirethedevelopertoformallyspecifytheirsystem and aims at elucidating the runtime properties of the system.
conclusion distributed state is a key element of distributed systems that im pacts consistency performance reliability and other system features.however distributedstateisdifficulttoteaseout understand andcheck.wepresentedanovelautomatedanalysisapproachto identify distributed state instrument it and record it at runtime combineitusingthreedifferentstrategies and useit to infer likely distributed state invariants.
we also introduced a lightweight probabilistic assertion mechanism to check distributed state invariants at runtime using real time snapshots.
we realized our approach in dinv a tool for systems written in go.weevaluateddinvwithfourcomplexandwidelyusedsystems.ourevaluationdemonstratesthatdinvcaninfercriticalcorrectness properties of these systems and that dinv assertions can detect silentviolationsoftheseproperties.forexample dinvdetecteda violation of each of the three invariants of etcd raft in under 7s with assertion overhead of just .02x.
dinv is an open source tool .