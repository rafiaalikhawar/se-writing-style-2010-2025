rotten green tests julien delplanque st phane ducasse guillermo polito andrew p .
black and anne etien univ.
lille cnrs centrale lille inria umr cristal f lille france rmod inria lille france dept of computer science portland state university oregon usa firstname .
lastname inria.fr apblack pdx.edu abstract unit tests are a tenant of agile programming methodologies and are widely used to improve code quality and prevent code regression.
a green passing test is usually taken as a robust sign that the code under test is valid.
however some green tests contain assertions that are never executed.
we call such tests rotten green t ests .
rotten green tests represent a case worse than a broken test they report that the code under test is valid but in fact do not test that validity.
we describe an approach to identify rotten green tests by combining simple static and dynamic call site analyses.
our approach takes into account test helper methods inherited helpers and trait compositions and has been implemented in a tool called drt est .drt est reports no false negatives yet it still reports some false positives due to conditional use or multiple test contexts.
using drt est we conducted an empirical evaluation of real test cases in mature projects of the pharo ecosystem.
the results of the evaluation show that the tool is effective it detected tests as rotten green tests that contain assertions that are not executed.
some rotten tests have been sleeping in pharo for at least years.
i. i ntroduction agile methodologies such as extreme programming promote unit testing as a key tenant of the software development process.
executing a test suite after each change to the software helps to ensure that new functionality works and that the old functionality remains working that is it helps to avoid software regressions .
tests are based on the execution of assertions that check that the system under test satisfies some property for example that a method returns a certain value or that certain data is written to a stream.
developers value green tests i.e.
tests that are passing because they provide assurance that the software is working as expected.
the software engineering research community has developed many techniques to help assess the quality of test cases.
some approaches are based on mutation testing others on code coverage or on repairing tests .
several researchers have worked on test smells following fowler et al.
s code smells .
huo et al.
present oraclepolish an approach and tool to identify brittle or unused test inputs.
pinto et al.
performed an analysis of the evolution of test suites they mention repaired added and removed tests.
our concern in this work is with rotten green tests tests that were intended by their designer to execute some assertions but that do not actually do so.
such tests are insidious because they pass and they contain assertions they therefore give theimpression that some useful property is being validated.
in fact rotten green tests guarantee nothing they give a bogus feeling of confidence.
rotten green tests can be seen as a new kind of test smell.
our approach to identify rotten green tests is based on a combination of a simple static analysis and dynamic monitoring of method execution at call site granularity.
we determine whether or not a test is rotten even in presence of helper methods and trait compositions .
our approach does not report false negatives but can report false positives in the case of conditional tests and tests that are reused in multiple contexts.
while our implementation and validation occur in pharo rotten green tests are not specific to pharo we encourage replication of our approach in other languages.
the contributions of the paper are the identification of rotten green unit tests as passing tests that contain assertions that are not executed and which therefore give the developers false confidence a simple and precise approach that finds rotten green tests by combining static and dynamic call site analyses and a detailed analysis of the results of applying this approach to several large systems containing a total of tests.
section ii provides a small introduction to pharo the language in which the projects that we analysed are written.
section iii describes the anatomy of a unit test and compares rotten tests with smoke tests.
section iv introduces the approach we developed to automatically identify rotten tests.
section v presents the experiment we used validate our approach and its results.
section vi describes the implementation of drtest .
section vii discusses some aspects of our approach and future work.
section viii presents related work.
ii.
p haro synt ax in a nutshell we use plain pharo code in this article rather than attempting to translate our real examples into another language.
pharo as a smalltalk descendant is a conceptual subset of java it can be considered to be a java without a static type system that makes heavy use of closures.
everything in pharo is an object and every object is an instance of a class.
each class inherits from a single superclass.
all methods are public virtual field access is like java s protected but fields are never visible to other classes in the package.
in addition a class can be composed from traits reusable groups of method definitions .
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fields and local variables are read by using their name and written using for assignment.
method invocations use space rather than a dot and colon introduces an argument so receiver methodn1 arg1 n2 arg2 is equivalent to the java syntax receiver.methodn1n2 arg1 arg2 hence self assert s size equals corresponds to this .assertequals s.size in pharo periods separate statements and are the equivalent of java semi colons.
a return statement uses the caret construct if there is no explicit return the receiving object is returned by default.
thus the following pharo statement weather israining ift rue iffalse is equivalent to this java statement return if weather.israining this .takeumbrella else this .takesunglasses this code snippet also illustrates the use of messages and lexical closures delimited by to implement control flow such as conditionals and loops this is ubiquitous in pharo.
parameters to a closure are introduced by a colon and terminated by a vertical bar s o is a closure that adds to its argument.
closures are executed by sending them the message value s o value returns .
strings are delimited by single quotes while comments use double quotes this is a comment .
the notation classname methodname is not part of pharo itself but is used to designate a method when writing about pharo we will use it here.
iii.
t he problem of rotten green tests before defining rotten green tests we first describe the basics of unit testing and then briefly explain smoke tests to help distinguish them from the topic of this article.
a. unit tests unit tests are commonly composed of a test fixture which sets up the system to be tested one or more stimuli which exercise the component under test and one or more assertions that verify some expected property .
listing shows a trivial sunit test that checks that a set should not contain the same object twice.
1sett est testsetduplication s local variable definition 3s set new.
fixture 4s add .
5s add .
stimulus 6self assert s size equals .
assertions 7self assert s includes .
listing the parts of a unit test.
in the example the fixture is the code that declares and initializes sto contain here the fixture is inline but it can also be factored out into a setup method.
the stimulus is the second addition of 1tos the assertions then verify the property that scontains 1just once.
provided that all of the assertions are true this test will pass and we say that it is green .
if a false assertion is executed for example if the set does not detect the duplicate and its size is2 the test will fail it will be yellow .
if an error occurs during the running of the test for example if set new signals an exception then the test will be red .
b. smoke tests it is common practice to use unit testing frameworks to execute so called smoke tests whose purpose is to check that the feature under test can be run without emitting blue smoke that is that the test ran without raising an unexpected exception .
listing is an example of such a test.
1sett est testsetaddsmoket est s 3s set new.
fixture 4s add .
5s add .
stimulus listing a smoke test.
in its simplest form a smoke test may contain no assertions at all as in this illustration.
this is the way that we use the term smoke test in the remainder of this article.
smoke tests are useful because if they are green they provide a fast but cursory check that the feature concerned can be considered for further testing.
conversely if a smoke test is red there is a serious issue that should be addressed rapidly.
smoke tests are not the concern of this article nothing that follows should be construed as advocating either for or against the use of smoke tests.
nevertheless we do need to distinguish a smoke test that by design contains no assertions from a rotten green test which by accident executes no assertions.
c. rotten green tests consider an empty test a test method that contains no code at all no fixture no stimulus and no assertion.
if it is treated as a passing test it will increase the number of green tests without providing any value.
empty tests are bad because they may help to convince a developer that the software is working correctly when in fact they guarantee nothing.
empty tests do occur perhaps as the remains of a testwriting session that was never finished.
fortunately they are easy to spot and eliminate.
a much more insidious problem is caused by a test that does contain a valid fixture stimulus and assertion but nevertheless does not execute any assertion.
listing shows a real example from pharo issue .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
1tprintonsequencedt est testprintondelimiter astream result allelementsasstring 3result .
4astream readwritestream on result.
5self nonempty printon astream delimiter .
6allelementsasstring result findbetweensubstrings .
7allelementsasstring withindexdo el i self assert el equals self nonempty at i asstring listing a rotten green test.
at first glance this looks like a fine test of the collection printon delimiter method.
lines and create a fixture i n this case they set up astream .
then comes the stimulus w e send the message printon astream delimiter to the collection resulting from self nonempty causing it we hope to write its elements to astream delimited by commas.
the remainder of the test is intended to make assertions .
lines look as though they are parsing the contents of astream and asserting that the elements found written on it are the same as those in the original collection.
this test is passing so we know that everything is ok right?
wrong!
the programmer who wrote this test misunderstood the way that streams work.
the string in the variable result can never be modified by writing to astream .
indeed the name result is misleading because result is initialized to the empty string and never changes.
consequently result findbetweensubstrings will answer an empty collection and the withindexdo block which contains the only assertion will never be executed.
we could put any assertion into this block and the test would still run green.
we believe that such a test is worse than no test at all.
first the rotten test does not guarantee any property of the method under test.
second it wastes time remember that we want our tests to run quickly.
third in the case of no test at all test coverage statistics would reveal that the printon delimiter method is not being exercised and the developers would at least be aware that their testing is inadequate.
instead a rotten test exercises the method without actually testing it.
rotten green tests give developers a false sense of security coverage may be good and the tests may be green but in fact no properties are being checked other than the property that the code runs without signaling an error .
moreover when developers look at such a test superficially they will probably not spot that there is a problem.
the example in listing was first reported as a bug in february .
however because the test was green the bug was easy to overlook and the bug report was closed without any action being taken.
as of the end of this rotten green test was still present in pharo.
iv .
i dentifying rotten green tests once we accept that rotten green tests are bad it is natural to ask how we can detect them.
one might think that all that is necessary is to detect tests that makes no assertions but thiswon t let us distinguish smoke tests from rotten green tests.
neither can we assume that any test that contains no assertions is a smoke test many tests make assertions indirectly through the use of helper methods see section iv a .
tests that use assertions in helper methods are not smoke tests even though they contain no assertions.
to clarify the discussion we will use the following terms.
anassertion primitive is a method provided by the unittesting framework that performs the actual check.
in pharo there are assertion primitives implemented in theasserting protocol of class t estasserter for example assert assert equals and deny .
we do not consider fail fail and signalfailure to be assertion primitives because these methods are designed to be invoked conditionally.
similarly we exclude skip and skip which allow one to skip the test without signalling a failure.
atest method is a method identified as containing a test by the unit testing framework.
in pharo test methods are zero argument methods defined in a subclass of t estcase whose names start with test .
ahelper method is a method that makes an assertion directly by invoking an assertion primitive or indirectly by invoking another helper method but that is not a test method.
developers frequently write application specific test helper methods as we discuss in subsection iv a. arotten test is a test that passes contains assertions either directly or indirectly through a helper but in which at least one assertion is not executed.
a. helper methods it is common practice for developers to factor out assertions into helper methods.
listing shows an example for numerical checks.
helper methods affect our analysis in two ways we need to know which tests invoke helper methods when we look for rotten tests and we must take into account the possibility that helper methods as well as tests might be rotten.
1numericalt ests assert actual isroughly desired within eps 2self assert actual desired abs eps description actual result actual is not even roughly equal to desired listing a helper method for approximate arithmetic for a real world example of a helper method we turn to the pillar editing platform .
pillar s test suites use helper methods such as assertwriting includest ext which is defined in the superclass of pillar s test classes prdocumentwritert est .
this method factors out the writing of an html element and the checking of the emitted html.
listing shows its definition and a sample of its use.
1prdocumentwritert est assertwriting item includest ext str result 3result self write item.
4self assert result includessubstring str authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
6prhtmlwritert est testanchor item 8item pranchor new name foo .
9self assertwriting item includest ext id foo listing a helper method for html generation like a rotten test a helper method might fail to make an assertion in some or all situations helper methods might be rotten too.
any approach to detecting rotten green tests should also detect rotten helper methods.
moreover since the action of a rotten method may depend on the context e.g.
the test fixture and the arguments to the helper method we need to record the specific test that exposes the issue because a helper might work fine in one context but be rotten in another.
b. classifying tests there are three situations that a rotten test analysis should identify.
good tests.
a test passes and contains some assertions either directly or indirectly though helper methods and all these assertions are executed the test is good.
rotten tests.
a test passes and contains assertions either directly or indirectly but at least one of its assertion is not executed the test or the helper method is rotten.
smoke tests.
a test contains no assertions either directly or indirectly it is a smoke test.
note that distinguishing between good and rotten requires some dynamic analysis because we need to ascertain whether an assertion is executed .
in contrast distinguishing between rotten and smoke requires some static analysis because we need to ascertain whether the test contains assertions.
listing shows that detecting rotten tests requires an analysis with call site granularity.
testabc is a test method and helper and secondhelper are helper methods because helper invokes secondhelper and secondhelper invokes an assertion primitive.
in addition testabc contains a valid assertion self assert true .
while this assertion is correctly executed the call to method helper will not be executed and as a consequence the assertion in secondhelper will not be executed.
thus this test contains one assertion that is not executed which makes it rotten by our definition.
this example shows that the analysis should distinguish between the different assertion call sites.
1rottent est testabc t est method false ift rue self assert true .
5rottent est helper indirect helper self secondhelper 8rottent est secondhelper direct helper self assert x listing a rotten test that neglects to invoke its helper method but nevertheless makes a valid assertion.c.
combining static and dynamic analyses our analysis performs the following steps.
step identification of assertion primitives.
we build the set of assertion primitives by manually tagging all the methods of the unit test framework that make assertions.
this set depends only on the test framework.
step identification of helper methods.
to compute the set of helper methods we statically build a call tree of all the self and super sends from the test method under analysis.
building this tree takes care of chains of method invocations as far as the assertion primitives.
the tree is then pruned to remove all branches that do not lead to an assertion primitive.
all methods remaining in the pruned tree except the root and the leaves are helper methods.
step call site instrumentation.
we instrument all the call sites to assertion primitives and helpers in the test method and helper methods.
that is to say we set up a mechanism that allows us to determinate if after the execution of a method all the call sites of assertion primitives and helpers have been executed.
if a call site has not been executed the mechanism gives us the corresponding ast node.
step test execution.
we execute each test method including inherited test methods one at a time while monitoring a the outcome of the test pass fail or error and b whether each assertion primitive and helper call site has been executed.
because we are looking for rotten green tests we consider only passing tests.
a method test or helper is considered to be rotten when it contains a call site for an assertion primitive or a helper but this assertion or helper was not invoked during test execution.
step report generation.
drtest s final report has to take into account the way that methods are reused in the test hierarchy.
a test method may be defined in a superclass and executed in a subclass.
in general the test fixture and thus the meaning of the test will be different in each place in which it is dynamically used so we must report the class of the test as well as the method.
helper methods are designed to be used by many test methods if a helper method is rotten the test invoking it should be reported so that the programmer can understand the scenario in which the helper fails to make an assertion.
the dynamic analysis that we use has call site granularity we know whether each call site has been executed or not.
therefore we can identify a method in which one assertion primitive was executed while another one was not.
this is why we do not have false negatives.
d. example explained to explain how the approach works we use the examples from listing and .
in listing the call on line is executed.
however the call self helper on line is not executed.
because method helper has been identified as a helper method by the static analysis testabc is determined to be rotten.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
found rotten tests project description pack.
classes test tests helpers missed missed context fully classes fail skip dependent rotten compiler ast model and compiler of pharo.
aconcagua model representing measures.
buoy v arious package extensions calypso pharo ide.
collections pharo collection library.
fuel object serialization library.
glamour ui framework.
moose software analysis platform.
petitparser2 parser combinator framework.
pillar document processing platform.
polymath advanced maths library.
postgresql postgresql parser.
renoirst dsl to generate css.
seaside web application framework.
system low level system packages telescope visualisation framework.
zinc http library.
t able i characterization of projects under analysis.
1rottent est testdef t est method self badhelper.
self assert true .
5rottent est badhelper indirect helper false ift rue 8rottent est secondhelper direct helper self assert x listing a rotten helper method badhelper makes no assertions in listing the assertion in line is executed and the method helper is executed.
however the assertion in secondhelper is not executed because the condition at line is always false .
therefore helper is determined to be rotten as istestdef .
v. v alida tion and resul ts in this section we describe what we found when we applied our approach for detecting rotten tests to some real software projects.
section v a characterizes the projects whose tests we analysed.
section v b presents a classification of the rotten tests we found and section v c summarises our results.
finally section v d describes the results from each project and describes some of the rotten tests that we found.
a. characterizing the projects under analysis pharo is an open source language with a growing community and a large number of mature projects.
we have run drtest on pharo subsystems.
table i characterizes the analysed projects.
for each project we show a short description of its purpose the number of packages the number of classes in all these packages the number of test classes the number of tests the number of helpers and the number of rotten tests in each of fourcategories which will be described in the next subsection.
note that first three categories are not exclusive so a rotten test can appear in more than one of them.
we have chosen these projects because of their heterogeneity.
they are quite different in terms of purpose packages classes tests and use of helpers.
b. rotten test categorisation not all rotten tests are equal.
some follow certain patterns and are simple to address while others are complex and hard to characterise.
we originally planned to classify rotten tests based on structural properties such as the use of helper methods and hierarchy.
however such criteria turned out not to be interesting because they depend on a developer choice and are not related to the cause of the rotten test.
instead we performed a manual analysis and assessment of each of the rotten tests identified by drtest see subsection v d .
the results of this analysis led to the emergence of four categories missed fail missed skip context dependent assertions and fully rotten tests .
while our approach definitively identified assertions that were not executed we interpret the two first categories missed fail and missed skip as false positives because they show that the developer misused the assertion primitives provided by the test framework and not that the test was misleading.
we present each category and discuss our findings.
a missed fail this category contains tests where the developer passed false to the assert primitive to force the test to fail.
this is illustrated in listing line where the detect ifnone iterator is used to find an element satisfying the predicate passed as first argument.
when there is no element satisfying this predicate the second argument another closure is executed.
so the test is written to fail if no suitable object is found in the collection.
1tsequencedelementaccesst est test0fixturesequencedelementaccesst est 2self morethan4elements.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
3self assert self morethan4elements size .
4self subcollectionnotin detect each self morethan4elements includes each not ifnone .
7self elementnotinforelementaccessing.
8self deny self morethan4elements includes self elementnotinforelementaccessing .
9self elementinforelementaccessing.
self assert self morethan4elements includes self elementinforelementaccessing listing a missed fail on line .
to make this test fail the developer used self assert false instead of self fail this is a misuse of the testing framework.
our approach classifies this test as rotten because the assertion primitive called from line in the closure argument of detect ifnone is not executed.
we consider this case to be a false positive because if the developer had written self failwe would not have classified this test as rotten.
identifying this pattern amongst the rotten tests is easily automated using a static analysis.
b missed skip several test methods contain guards to stop their execution early under certain conditions.
such a pattern is useful when reusing test suites with different fixtures either to avoid a particular configuration or to avoid running some test depending on the developer s environment.
listing shows such a case where the expression thiscontext method hassourcecode guards execution of the test.
1occontextt empmappingt est 2testaccessingargofouterblockfromanotherdeepblock actual check the source code availability to do not 5fail on images without sources 6thiscontext method hassourcecode ift rue .
8actual outerarg outerarg asstring.
innerarg innerarg asstring.
thiscontext tempnamed outerarg value innervalue.
value outervalue.
self assert actual equals outervalue listing a rotten test in compiler.
the guard clause lines and does not implement what is described in the comment above the ift rue should be an iffalse .
to show the intent to avoid running a test the testing framework provides the primitives skip and skip but the test in listing does not use them.
drtest therefore detects the test as rotten again because of a misuse of the testing framework.
to be explicit and help the code s maintainer the guarded expression should be self skip rather than self .
note that the writer of this test got the condition backwards this illustrates the danger of guarded tests that return rather than skip .i ti s possible to detect this pattern automatically too by searching for tests containing a return statement.
c context dependent assertion the missed skip category is a special case of tests that contain context dependent assertions.
such tests contain conditionals with different assertions in the different branches.
the boolean expressions in these conditionals are either checking that some properties of the environment are fulfilled or in the case of a helper checking some condition on the helper s parameters.
listing shows a context dependent call to one assertserializationidentityof helper or another assertserializationequalityof depending on the platform on which the test is being executed.
1flbasicserializationt est testcharacter t est character serialization.
if the code is less than the same instance is used.
but if it is bigger new ones are created.
3self assertserializationidentityof a.
4flplatform current isspur ift rue self assertserializationidentityof character value .
japanese hiragana a iffalse self assertserializationequalityof character value .
japanese hiragana a .
7self assertserializationequalityof character allcharacters.
8self assertserializationequalityof array with a with character value .
listing a rotten test in the fuel framework one of the two blocks of the conditional on lines is not executed.
while code like this is not bad per se identifying such context dependent tests still provides the developer with important feedback.
indeed conditional logic and large fixture are two test smells that can have a detrimental effect on test effectiveness and maintenance.
one resolution for this problem is to split the conditional test into two separate tests.
d fully rotten tests this last category describes tests that do not execute one or many assertions and do not fall into any of the three previous categories.
fully rotten tests are caused by actual bugs or logic errors in the test.
an example of such a logic error that we have found multiple times is performing an assertion inside a loop that iterates over an empty data structure.
the error in this case is usually located in the code filling the data structure.
listing is an example of such a rotten test.
c. experiment results of the projects we analysed of them contained rotten tests in one or more of the categories.
across the tests in these projects we found missed fail rotten tests 23missed skip rotten tests context dependent rotten tests and fully rotten tests.
all in all rotten tests were found remember a test can belong to more than one category .
d. detailed project analysis in this section we discuss the results for each project and some of the more interesting rotten tests.
a compiler this is the default compiler used in pharo distributions.
one rotten test is due to a guard clause a context dependent assertion .
the four fully rotten tests were intended to identify a bug related to the dynamic bytecode rewriting of boolean expressions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in smalltalk it is normally impossible to make a nonboolean object implementing the boolean interface act like a boolean in an expression.
the reason is that common boolean methods e.g.
ift rue and are compiled to optimized bytecodes that raise an exception when they are evaluated for non booleans.
however pharo dynamically catches this exception and rewrites the bytecodes using a de optimization that actually sends the message to the receiver object.
these rotten tests concern the validation of this feature.
a bug in the bytecode rewrite process induced an early return from these test methods.
the return appears before the assertion is executed and this leads to the tests passing without executing any assertion.
listing shows the source code of one of these methods both in its original form and as it is dynamically rewritten.
what this case shows is that it can be extremely difficult to see that an assertion is not executed.
1mustbebooleant ests testand original mybooleanobject 3mybooleanobject mybooleanobject new.
4self deny mybooleanobject and 6mustbebooleant ests testand rewritten mybooleanobject 8mybooleanobject mybooleanobject new.
mybooleanobject and listing a rotten test in compiler in its original form and as it is decompiled after bytecode rewriting.
b calypso this is pharo s new code browser.
it allows developers to navigate through view and edit packages classes and methods.
drtest found missed fail tests which contained conditionally executed sends of t estasserter assert description with false as the first argument similar to listing .
c collections these packages provide the core data structures of pharo .
drtest found missed skip rotten tests.
tests in collections are written in a generic way in a super test class.
however some tests can not be executed on all subclasses.
to skip them on some test subclasses a guard clause is used to test the class in which the test is executed.
this guard clause if the condition is true leads to a return of self to interrupt the execution of the test.
these missed skip rotten tests are due to these early returns which should be replaced by calls to the skip method.
drtest identified context dependent rotten tests.
some of those tests could be easily re written to avoid executing assertions in conditionals.
finally drtest identified fullyrotten tests all of which contain a bad usage of the stream api.
these methods test the ability of a collection to be serialized on a stream.
they do this by creating a stream writing a collection on the stream and reading what was written on the stream during step and comparing it to the expected result.
however because of a misuse of the stream api the assertion inside the comparison loop is never executed.
an example of such rotten test is shown in listing .
1tprintt est testprintelementson astream result allelementsasstring tmp 3result .
4astream readwritestream on result.
5tmp orderedcollection new.
6self nonempty do .
7self nonempty printelementson astream.
8allelementsasstring result findbetweensubstrings .
to allelementsasstring size do i self assert tmp occurrencesof allelementsasstring at i allelementsasstring occurrencesof allelementsasstring at i .
listing a rotten test in pharo collections.
on line rather than looking for substrings in result the test should look in astream contents .
d fuel this is the official library for serializing objects onto the disk.
it contains context dependent rotten tests with assertions that depend on the platform on which the tests are executed.
an example of such a test can be seen in listing .
e moose this package provides tools and libraries to analyse data and software.
it contains one missed fail rotten test and one fully rotten test.
the latter can be seen in listing .
when the argument of the assert self packagep5fullreferee namespacescope is executed it raises an error.
then the should raise method catches the error .
thus assert is never called.
the use of assert in this test is wrong it is never executed and even if it were it would not get a boolean as argument.
it is sufficient to write self should self packagep5fullreferee namespacescope raise error .
1famixselfloopscopet est testfamixpackagenamespacescope 2self should self assert self packagep5fullreferee namespacescope raise error.
listing a rotten test in moose.
an error is raised during the evaluation of the argument of assert s o assert is not executed.
f petitparser2 a parser combinator framework.
it contains one fully rotten test which is defined in the superclass of the class in which it is rotten.
this test uses the class hierarchy and inheritance between test classes to drive the execution of tests.
it iterates on the subclasses of the test class but some classes such as pp2smalltalkparsert ests h a v e no subclasses.
thus the assert description in the loop is never executed.
listing shows the rotten test.
g pillar this is a markup syntax and associated tools to write and generate documentation books and slides.
one fully rotten test was found in this project.
this test is defined in the superclass of the class in which it is rotten.
listing shows the rotten test.
prt extwritert est is the subclass of prdocumentwritert est in which this test is rotten.
when the test is executed in the context of prt extwritert est class self figurebegin is always empty.
the test is thus rotten.
h seaside this is a framework for building server side web applications.
drtest found missed fail rotten tests and missed skip rotten tests.
finally drtest identified fullyrotten test see listing .
it turns out that it is in fact a misclassified missed skip rotten test.
looking at the implementation of usecompileusenewcompiler during listing authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
1pp2smalltalkgrammart ests testcompleteness this test asserts that all subclasses override all test methods.
subclasses 4subclasses self class allsubclasses.
... 6subclasses do subclass self class testselectors do selector self assert selector testcompleteness or subclass selectors includes selector description subclass printstring does not test selector printstring listing rotten test in petitparser2.
the class in which this method is rotten has no subclass.
1prdocumentwritert est testfigurewithoutlabelandwithoutcaptionexport item result 3item prfigure new reference file picture.png yourself.
6result self write item.
7self figurebegin ifnotempty self deny result includessubstring self figurebegin listing rotten test in pillar.
in prt extwritert est the expression self figurebegin is always empty.
we can see that the block provided as parameter might not be executed if self supportsswitcht onewcompiler answers false .
this explains why the deny and assert intestcompilebytearraywithcachesource are not executed and the rotten test is classified as fully rotten by drtest .
i system this project contains the tests for the system packages of pharo.
drtest detected one missed skip rotten test and context dependent rotten tests.
those are due to early returns and context dependent assertions needed to implement these tests generically in a superclass.
1wapharofilelibraryt est testcompilebytearraywithcachesource library data file source expected 3self usecompileusenewcompiler false during self deny grpharoplatform current usebytearrayliterals.
... source wa t estingfiles sourcecodeat demojpeg.
expected demojpeg ...truncated... asbytearray .
self assert source greasestring expected ensure listing rotten test in seaside.1wapharofilelibraryt est usecompileusenewcompiler aboolean during ablock oldvalue 3self supportsswitcht onewcompiler iffalse .
... rest of the helper... listing helper with guard clause taking a block as parameter.
vi.
i mplement a tion the implementation of drtest has two key aspects the detection of helper methods and the tallying of assertion primitive and helper methods call site executions.
helper methods are detected through static analysis we traverse the class hierarchy starting from the test case class that interests us and select all methods that arrive at an assertion primitive through a chain of self sends.
our analysis does not support helper methods that might be defined in classes outside the test case class hierarchy see section vii b .
for the dynamic analysis the detection of invoked methods is done by instrumenting assertion primitive call sites in the test methods and helper methods.
the following subsections provide more details.
a. detecting helper methods drtest detects helper methods through static analysis.
we first collect all test methods between the analysed test class and the root of test case hierarchy.
then we use an ast interpreter to abstractly execute each of those test methods and perform the transitive closure of self super message sends.
this abstract interpreter recursively visits the entire method ast.
every time it encounters a message send if it is a self send it recursively visits it otherwise it is ignored.
finally we propagate the fact that a helper method invokes an assertion primitive.
this results in a list of helper methods that directly or indirectly call assertion primitives.
b. detecting assertion primitive execution assertion primitive execution is detected through bytecode instrumentation.
we instrument every assertion primitive callsite and helper call site to mark with a flag if they were executed.
we use the reflectivity library to do this instrumentation by adding annotations on the ast nodes that represent the interesting call sites.
c. the case of traits in pharo traits enable developers to reuse tests across several sub hierarchies the collection tests are a good example .
the situation with traits is the same as with helper methods a trait method may show up as being rotten only for a certain trait use.
this can happen because trait composition and inheritance can change the test fixture.
a test method may also be defined in a trait and reused by several classes.
since traits are implemented by flattening methods in the using class identifying test and helper methods coming from authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a trait does not involve any change in our implementation.
however we have decided to extend our tool to show for each method not only the class where it is installed but also its origin.
in this way drtest will show better information to developers so they can make better decisions.
vii.
d iscussion and future work a. early returns in failing tests we decided to ignore skipped tests in our analysis because by definition a skipped test will never be executed and thus neither will its assertions.
skipped tests are reported by the testing framework s user interface as a separate category and are not interpreted as passing tests.
another important point is that skips implemented using a return statement rather than a send of skip may occur not only at the beginning of a test but also in the middle after some assertions have executed.
in such a case the test will be reported as passing.
we consider such a test as rotten because depending on the context it may execute different sets of assertions.
such rotten tests should be refactored into two separate tests one for each condition.
b. location and characterisation of helper methods a limitation of our approach is that our static analysis discovers only those helper methods that are in the hierarchy of the analysed test case.
if the helper methods are located in a utility class then they will not be detected.
although not the focus of this article we thought that it would be interesting to measure whether developers factor helper methods out of test classes and into superclasses.
table ii measures the degree to which this occurs in the projects that we analysed.
we computed the number of levels of the class hierarchy between a call to a helper and its definition.
a distance of means that the helper is defined in the same class as the test a distance of means that the helper is defined in the superclass of the class of the test etc.
across all the projects we found a minimum distance of all projects using helpers define some of them directly in the test class that uses them and a maximum distance of .
we see that helpers are most often used in the class in which they are defined distance .
providing a general solution to identify helper methods in other hierarchies is difficult.
first pharo is a dynamicallytyped language like python ruby and javascript and inferring the class of non self sends statically is a complex task .
second programmers may use reflection again this renders static analysis ineffective .
an alternative to static analysis is to dynamically trace the full system as opposed to just the test cases and then to determine statically for each new method executed whether or not it contains a call to an assertion primitive.
we leave this solution as future work.
viii.
r ela ted work software testing is an active area of research researchers have looked at improving the quality of tests but we are not aware of any prior work that identifies rotten green tests.project compiler aconcagua buoy calypso collections fuel glamour moose petitparser2 pillar polymath postgresql renoirst seaside system telescope zinc 0helperroot subcla1 helper de finition distance t able ii for each project under analysis the number of helper calls for the five distances of helper definition observed.
a t est analysis herzig et al.
present an approach based on machine learning to detect false test alarms false test alarms are integration tests that are directly linked to code defects but fail due to external factors such as hardware failing to give access to a test resource.
identifying false test alarms is key since they demand the attention of engineers and require manual checking to determine that they are false alarms.
v era perez et al.
present a novel analysis of pseudotested methods.
these are methods that are covered by tests yet no test case fails when the method body is removed.
this intriguing concept was named in by niedermayr et al.
who showed that such methods are systematically present even in well tested projects with high statement coverage .
pseudo tested methods are only indirectly related related to rotten tests one possible cause of a pseudo tested method is that all the tests of that method are rotten.
the existence of pseudo tested methods shows that it is important to improve tool support for testing.
huo et al.
present an approach based on taint analysis to identify brittle or unused test inputs.
they monitor the flow of controlled and uncontrolled inputs along data and control dependencies.
our approach focuses on monitoring assertion execution.
from that perspective it is complementary.
mockus et al.
present an approach to determine test effectiveness.
their analysis compares test coverage prior to a release with the number of reported failures after that same release.
we agree with schuler and zeller that simple coverage is not enough especially in presence of dynamic dead code .
both approaches identify tests that make no assertions.
their approach is good for improving code coverage by identifying program elements that are not in the dynamic backward slice of any assertion.
our approach does not look for coverage improvement.
we do not look for statements that are not covered by tests.
we assure programmers that when a test is green its assertions have been executed.
our analysis is less general but also much simpler to implement which we hope will make it more likely to be adopted.
b mutation testing mutation testing is one of the earliest techniques used to improve test quality and robust508 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ness .
several researchers have used mutation testing to improve branch coverage .
tillmann et al.
use symbolic execution to find inputs for parametrized unit tests that achieve high code coverage.
they turn existing unit tests into parametrized unit tests and generate entirely new parametrized unit tests that describe the behavior of an existing implementation.
baudry et al.
present a bacteriological approach to mutation testing.
other approaches focused on other attributes of test quality.
baudry et al.
also worked on improving test for diagnosis criterion they propose a new attribute called the dynamic basic block to improve the location of faults.
for fault localization the usual assumption is that test cases satisfying a chosen test adequacy criterion are sufficient to perform diagnosis.
this assumption is verified neither by specific experiments nor by intuitive considerations.
c repairing broken tests daniel et al.
present reassert a tool that repairs broken tests.
they define broken tests as tests that turn red because the domain code is changed.
they propose various repair strategies such as replacing asserted values inverting relational operators and replacing some common method calls.
by definition rotten tests are not broken tests because they are green.
d t est smells several works have focused on test smells our approach is related to such work since rotten tests can be seen as test smells too.
however none of the existing catalogs mention rotten tests.
deursen et al.
present a list of bad test smells and their associated cures.
they do not mention rotten green tests as a smell.
v an rompaey et al.
propose a heuristic metric based approach to identify some test smells the general fixture and eager tests .
they present testq a tool based on static analysis to detect test smells .
bavota et al.
present an empirical analysis to assess the presence of test smells and their impact on software maintenance.
reichhart et al.
propose testlint a rulebased tool to detect static and dynamic test smells.
based on a large corpus of tests and a literature analysis they collected and proposed a list of test smells.
some of the proposed smells are advanced.
for example their analysis identifies commented out assertions they check if uncommenting them leads to valid tests.
rotten tests were not part of their list.
rotten tests often exhibit conditional logic smells .
in this paper we describe how such a smell can be detected automatically.
more recently bowes et al.
identified testing principles to capture the essence of testing goals.
they identified that principles are not covered by existing test smells and they proposed some metrics to identify such smells.
e smoke t ests the term smoke test is used in various ways by different authors.
waletzky et al.
say that the terms smoke test and build v erification test are sometimes used interchangeably but prefers to treat smoke tests as the subset of build v erification tests that are extremely fast to run and are the prelude to more thorough testing.
other authors use the term smoke test to include tests that make assertions.
for example memon and xie discuss generating thousands of tests that contain sequences of simulated gui events and using various test oracles to check that the state of the gui isas expected which they still call smoke tests.
f t est effectiveness a large body of research has been carried out to assess the effectiveness of tests at detecting faults.
to mention a few mockus et al.
conducted a multiple case study on two dissimilar industrial projects they found that in both projects an increase in test coverage is associated with a decrease in field reported problems when adjusted for the number of pre release changes.
inozemtseva and holmes conducted a large and intensive study showing that code coverage is not strongly correlated with test suite effectiveness.
because of this they suggest that coverage while useful for identifying under tested parts of a program should not be used as a quality property.
gligoric et al.
perform a study showing that branch coverage and intraprocedural acyclic path coverage are the criteria that perform the best when comparing inadequate test suites.
g t est selection other work focuses on the selection of the tests to be run.
for example when a change is made to the software it is desirable to re run those tests that are most likely to be invalidated by the change.
beszedes et al.
propose to use code coverage for test selection to maximise the test surface.
blondeau et al.
analyse the problem of test selection surfaces in an industrial context.
h system robustness for test input dependability using a different approach poulding and feldt increase unit test quality they automatically generate new invalid and atypical inputs for an application tests.
they assess the impact of the choice model on the input variability using a global probability distribution over all the inputs that could be emitted.
rotten tests are not related to the quality of invalid or atypical inputs.
they are linked to unsatisfied test context and conditional logic that confused developers.
in the spirit of jcrasher and system robustness shahrokni et al present robustest a framework to generate automatically tests for timing issues.
i broken test sorting often unit test frameworks present failed tests in an arbitrary order but developers want to focus on the most specific ones first.
gaelli et al.
propose a partial order of unit tests corresponding to a coverage lattice of sets of covered method signatures .
when several unit tests in this coverage lattice fail the tool guides the developer to the test invoking the smallest set of methods.
ix.
c onclusion we have identified the existence of rotten green tests that is tests that pass and contain assertions but for which all assertions are not executed.
such tests are worse than no tests at all because they give developers false confidence in the system under tests.
we have described an algorithm that identifies rotten green tests based on a combination of static and dynamic analysis.
we presented a tool drtest which implements the proposed approach.
it distinguishes rotten green tests from smoke tests which also execute no assertions but do so by design .
we report on the rotten tests found in large open source projects based on an analysis containing a total of more than tests.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.