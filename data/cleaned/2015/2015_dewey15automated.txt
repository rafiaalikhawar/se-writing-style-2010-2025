automated data structure generation refuting common wisdom kyle dewey lawton nichols ben hardekopf university of california santa barbara fkyledewey lawtonnichols benh g cs.ucsb.edu abstract common wisdom in the automated data structure generation community states that declarative techniques have better usability than imperative techniques while imperative techniques have better performance.
we show that this reasoning is fundamentally flawed if we go to the declarative limit and employ constraint logic programming clp the clp data structure generation has orders of magnitude better performance than comparable imperative techniques.
conversely we observe and argue that when it comes to realistically complex data structures and properties the clp specifications become more obscure indirect and difficult to implement and understand than their imperative counterparts.
we empirically evaluate three competing generation techniques clp korat and udita to validate these observations on more complex and interesting data structures than any prior work in this area.
we explain why these observations are true and discuss possible techniques for attaining the best of both worlds.
i. i ntroduction automated data structure generation helps to test code that interacts with or operates on data structures.
techniques for automated data structure generation have been studied for over a decade from testera to korat astgen and udita .
testera was based on the declarative language alloy essentially an interface to a lower level sat solver its declarative nature made simple data structures easy to specify but its performance was disappointing.
succeeding techniques are based on the imperative language java explicitly in order to gain better performance.
these latter techniques also extend java semantics in various ways to include declarative features such as nondeterminism explicitly in order to preserve the usability1benefits of declarative languages.
this progression of research has led to the common wisdom in the automated generation community that imperative implies better performance and that declarative implies better usability .
in this paper we demonstrate that the common wisdom is flawed at best and backwards at worst we show that declarative techniques can be much faster than imperative ones and argue that imperative techniques can have much better usability than declarative ones.
we arrive at this conclusion in two steps.
recall that the current state of the art korat this work was supported by nsf ccf .
usability refers to the ease of use and learnability of a human made object in this case how easily data structures can be specified.
2the exact meanings of the terms imperative and declarative are a constant source of debate though we define imperative to mean the presence of mutable state and loops and declarative to mean the lack of these features.and udita modify the java language semantics to include various declarative features.
we argue first that extrapolating these features to their logical conclusion leads us to constraint logic programming clp which can be used to subsume and extend the current state of the art in data structure generation.
we show that an off the shelf clp engine allows for declarative specification and generation of data structures that easily outperforms existing techniques such as korat and udita.
clp is an inherently declarative approach and thus we show that declarative techniques have superior performance to imperative ones and we explain why this fact is true.
the current state of the art data structure generation techniques have been evaluated on relatively simple data structures such as sorted linked lists and red black trees.
for these data structures the clp specification is concise and elegant being at least on par with specifications in korat and udita in terms of usability.
thus when looking at the kinds of data structures that have historically been used to evaluate automated generation clp is the clear winner.
however our work goes beyond these simple data structures to encompass more complex structures and properties.
for example we specify and generate red black trees with the additional property that inserting a given element guarantees the tree must be rebalanced.
this property takes into account not just the invariants that define the data structure but also models the imperative operations on that data structure i.e.
insertion and rebalancing .
we also specify and generate more complex data structures such as skip lists splay trees and b trees these structures have cycles probabilistic properties and other features that make them more challenging than the simpler structures that previous work has evaluated.
based on our subjective experience we observe that for these kinds of structures and properties the clp specifications become complex and cumbersome they are no longer concise and elegant and closely follow the definition of the data structure but instead are obscure and indirect and difficult to define understand and debug.
we support this observation via metrics which act as proxies for complexity and argue why this declarative difficulty arises.
these two insights lead us to the conclusion stated earlier generating data structures using clp an extreme point on the declarative side of the declarative imperative spectrum is much faster than using techniques such as korat and udita that are based on java and that use its imperative features however in our experience korat and udita are generallymore usable than clp for specifying complex data structures and properties.
this insight and the reasoning behind it leads to some interesting ideas about future work that may eventually allow us to combine the benefits of both approaches.
in summary the contributions of this paper are we describe the progression of past research on automated data structure generation and its tension between declarative and imperative techniques.
section ii we argue that the logical extrapolation of the trends contained in that research is constraint logic programming and explore why clp can be a good fit for this problem for simple data structures.
section iii we examine more complex properties that data structures might have and explain why clp can be lacking in terms of usability when dealing with these properties.
section iv we describe seven specific data structures that exhibit a range of the above properties three structures that were evaluated in previous work and four that have never been automatically generated before this work.
also we describe for each data structure an additional property that yields a particularly interesting part of the search space for that data structure.
section v we specify and generate all of the data structures described above using clp korat and udita objectively comparing these approaches in terms of performance and subjectively comparing them in terms of usability.
section vi we use our insights into the advantages and disadvantages of the different generation strategies to describe some possible interesting avenues for future work in combining their benefits.
section vii ii.
r elated work we focus on general black box data structure generation techniques as opposed to more specialized white box techniques e.g.
.
we observe that prior work in the space of black box data structure generation has been moving between the imperative and declarative realms for over a decade in an attempt to find a sweet spot that makes the proper tradeoffs.
we present the relevant prior work in chronological order in order to make this phenomenon clear.
testera is the starting point for related work and represents a highly declarative mindset.
testera allows users to specify data structures and operations on said data structures via invariants encoded in alloy a specification language intended for bounded model checking.
alloy translates these specifications into a sat instance which is ultimately passed to a sat solver.
with testera satisfying sat assignments correspond to data structures which are valid according to the provided specification.
while testera allows for concise and precise definitions it has two major drawbacks it is disconnected from downstream testing and it has poor and unpredictable performance.
disconnected means that the data structures for automated testing are specified and generated in a languagewhich is entirely separate from what is actually being tested.
that is while we may be interested in testing a red black tree implementation in java we must specify it in alloy first and then translate the resulting structure into a java datatype.
in terms of performance sat is an np complete problem and sat solvers are ill suited for generating many satisfying solutions as opposed to a single satisfying solution.
in addition it can be challenging to diagnose performance issues surrounding a sat solver .
a notable step after testera is korat which addresses the disconnect and performance issues with testera.
korat allows users to write predicates directly in java represented as pure functions that take a data structure and return a boolean value these functions are written to return true iff the provided structure meets the necessary invariants.
korat uses these predicates effectively in reverse to generate interesting data structures.
the predicates themselves use imperative features such as loops and assignment but the overall specification strategy i.e.
providing predicates that accept interesting structures and using those predicates to generate said structures is declarative in nature.
assuming the downstream program under test is in java korat s strategy allows for data structures to be passed directly to the test harness without any sort of translation.
additionally korat sees much better performance than testera does with experiments typically running several orders of magnitude faster.
while korat addresses the major drawbacks of testera a slew of problems remain which were inherited from testera.
most important of these to our discussion is the fact that korat s search strategy is fixed and outside of control of the user.
this means that users cannot adjust the order in which data structures are enumerated which is a desirable property to have .
in response specifically to the search strategy issues astgen was developed.
astgen is aimed towards data structure generation problems which are best phrased in terms of how construction should occur as opposed to what is considered interesting.
this is a radical departure from prior work since it fundamentally promotes an imperative approach to data structure generation as opposed to a declarative approach.
the authors argue that certain problems such as enumerating grammars are well suited to this approach.
most importantly the authors make strong claims about performance best illustrated through the following quote the imperative approach makes the generation faster since no search is necessary.
to allow for imperative generation astgen offers an iterator like interface in java where the key methods are hasnext does the generator have more elements.
next get the next available element.
reset reset the generator to the beginning.
via the above interface users can directly generate data of interest with explicit control over the order in which such data is produced.
the downside of this interface is that it makes operations painfully explicit and generating any sort of nested data structure requires carefully constructing multiple iterator classes and passing instances along to where they are needed.while astgen theoretically overcomes the issues surrounding korat s search strategy the authors do not demonstrate any sort of utility for this ability.
more importantly while performance is frequently used to justify the clear loss of usability astgen is never evaluated against korat or any other prior techniques .
as such astgen s apparent performance benefits are never demonstrated so it is unclear if they exist at all.
in response to the severe loss of usability in astgen udita was developed which attempts to recover some of the usability of korat in an astgen style framework.
a key observation made in the udita work is that the sort of imperative generators used in astgen can be seen as a means of nondeterministically enumerating a stream done in a very explicit way.
building from this insight the authors simplify the generator interface in astgen down to a single method generate which returns a single data structure in a nondeterministic fashion.
this is made possible by executing the generators on java pathfinder a special java virtual machine which imparts a nondeterministic semantics to java.
an additional benefit of using java pathfinder is that users can take advantage of assume statements pedantically ignoreif statements which semantically allow execution to proceed only if a given boolean condition evaluates to true .
via assume programmers can generate data structures of interest given only a predicate that tests validity as with korat.
in this way udita harmonizes the imperative generators of astgen with the declarative style of korat.
in addition to its usability benefits udita saw performance benefits as well.
udita was around faster than astgen on large benchmarks though around slower on smaller benchmarks that astgen completed in under seconds.
this performance benefit was unexpected considering the fact that udita diverges from the imperative model which was assumed in astgen to be unquestionably high performance.
our own evaluation in section vi shows that udita is substantially slower than korat and thus transitively this means astgen is even slower still this observation implies that much the the work done after korat has actually regressed the state of the art.
while udita addresses the sort of usability issues seen in astgen and even sees performance improvements this all comes at a substantial cost significantly increased complexity both from the user s and the implementer s standpoint.
users must understand both astgen and java pathfinder just to pick up udita and they must also be comfortable with nondeterministic execution.
as for the implementation in addition to the use of java pathfinder the authors found it necessary to implement some key optimizations such as lazy instantiation of data structures in order to make generation feasible.
for these reasons udita is is unappealing for data structure generation there are simply too many moving parts and the barrier to entry is high.
the most recent related work is senni et al.
who propose using clp as an alternative to korat and perform a preliminary evaluation that shows clp s promise.
this workis not in the mainstream but it anticipates our argument for clp to a certain extent.
however senni et al.
are primarily interested in research on source to source prolog optimizations rather than examining clp for automated data structure generation.
thus their paper does not attempt to explore the connections between prior work such as korat and udita and clp it does not fully explore the utility of clp and its capabilities and it leaves a number of issues unaddressed that previous work has shown are important.
first they do not investigate clp s ability to customize search strategies which was a major impetus for astgen and which is a strength of clp that senni et al.
do not exploit at all.
second they trivialize the problem of being disconnected from downstream testing something that clp shares with testera which was a major impetus for korat.
finally senni et al.
only evaluate a few low complexity data structures with small bounds all of which were seen in prior work these structures do little to fully showcase and exercise the abilities of clp.
because they did not push the abilities of clp they did not observe any of the shortcomings of clp that we enumerate in this paper and do not have our central insight that refutes the common wisdom about data structure generation.
iii.
a dvantage of clp p erformance constraint logic programming clp languages have features such as built in nondeterministic search value unification and arithmetic constraint solvers.
we argue that not only are these features useful for data structure generation but that many of the features in prior work such as korat astgen and udita are ad hoc approximations of a subset of these features.
because clp engines have been heavily optimized over decades to make these features efficient e.g.
clp has a natural performance advantage over the prior work.
in addition the prior work requires extensive changes and or additions to the java semantics making their implementation complex clp can take advantage of off theshelf engines without any modification.
as an example of how clp can be used for data structure generation consider a sorted linked list.
here is the clp program that can generate an infinite number of such structures sorted .
sorted .
sorted a b sorted .
sorted l write l fail.
this example uses standard prolog syntax where clauses are delimited with periods can be understood as reverse implication comma stands for logical conjunction and labels starting with capital letters are logical variables.
there are three clauses followed by a query the first clause states that an empty list is a sorted list the second states that a singleelement list is a sorted list the third states that a multi element list is sorted if the first two elements in are ascending order and the rest of the list is sorted.
execution of this program starts at the query in the last line.
the semantics of clp dictate that the expression sorted l will find some structure that satisfies the predicate sorted and bind it to the variable lusing unification.
it will then write that structure to output and finally fail.
failure automatically triggers backtracking for nondeterministic search and so the engine will backtrack to the sorted l expression and find a different satisfying structure bind it to l write it to output and fail again.
this process will continue indefinitely outputting an infinite stream of satisfying structures.
we now go over the various features of clp that are useful for data structure generation and compare them with features that are present in prior work.
nondeterministic search.
from a high level all prior work can be seen as techniques to nondeterministically generate data structures of interest in a given space.
in practice they all employ various form of backtracking algorithms.
in testera ultimately the nondeterministic generation is done by sat solvers which use backtracking algorithms for search .
unlike clp however traditional sat solvers e.g those described in are ill suited for generating many satisfying solutions necessitating modifications to the core solving algorithm for the sake of efficiency .
in korat a backtracking algorithm is added on top of the jvm and used to search the space of all structures to find ones that match a predicate defined by the user.
udita actually modifies the semantics of java using java pathfinder which uses backtracking to make java execution nondeterministic.
these techniques all build nondeterministic search into the infrastructure hiding it from the user astgen in contrast forces the user to explicitly encode the nondeterministic search into the specification of the data structure being generated.
nondeterministic execution is a core feature of clp semantics and has been discussed in the literature since very early on .
as such there have been literally decades of work on making this feature efficient in clp engines.
the declarative nature of clp i.e.
without assignments or mutation helps to make these engines as efficient as they are nondeterminism in an imperative setting as in the prior work is both less efficient and more complex.
search strategy control.
all prior work has employed bounded exhaustive generation i.e.
defining a finite space and generating all structures within that space.
while boundedexhaustive search has merit there are other search strategies that can be useful.
random search iterative deepening and various hybrid approaches have been used in the past to good effect for other types of automated generation .
as such restricting search to a single strategy is overly limiting.
astgen and udita allow for some coarse grained control over the order of data structure generation but clp can easily exceed this low bar.
clp naturally employs a depth first search strategy and the order in which structures are generated can be adjusted by modifying the order of clause definitions.
bounded exhaustive search can be achieved by common built in routines such as swi prolog s call with depth limit function which adds auser defined recursion bound to the search.
random testing is possible again via common built in routines such as maybe p where p is the probability that the maybe expression will fail and trigger backtracking.
all of these strategies are composable with each other for example we can easily define an iteratively deepened random search within some maximum bound.
this ability to vary the search strategy with such ease is a major advantage of clp for data structure generation compared to related work.
equality constraint propagation.
senni et al.
observe that udita s lazy data structure instantiation optimization can be seen as a particular cp constraint propagation solution strategy .
we observe that this udita optimization in fact behaves just like the logical variables available in typical clp engines which allow for the propagation of equality constraints .
semantically logical variables start in a special uninstantiated state wherein the variable has no specific value.
uninstantiated variables can be aliased with each other essentially putting variables into the same equivalence class.
logical variables can later become instantiated with particular values and all aliased variables will automatically have that same value.
to better illustrate this phenomenon consider the following code x y y .
this code aliases the logical variables xandywith the expression x y then sets both of them to the value 1with the expression y .
this behavior bears striking similarity to the lazy instantiation optimization in udita which only instantiates variables when operations specific to a given data structure are performed on them and allows for uninstantiated variables to be aliased.
in this way udita is attempting to emulate the logical variables already available in clp engines though in an ad hoc manner.
disequality constraint propagation.
in korat blind search is avoided by observing what sort of sub structure caused a data structure to be rejected by the user defined predicate.
this information is retained in a way that prevents further data structures with identical sub structure from being generated.
we observe that in effect this strategy allows korat to propagate disequality constraints which prevent the generation of invalid sub structures.
in udita certain optimizations related to isomorphism breaking are implemented in a manner which is similar to disequality constraint propagation.
this observation is made directly by the authors in demonstrating the correctness of the details of their generation algorithm.
while disequality constraints are somewhat non standard in clp languages it is still compatible with clp .
we do not make use of this fact in our evaluation but our results show that clp s standard equality constraints significantly outperform korat s disequality constraints.
arithmetic constraint propagation.
the hallmark of clp engines is the ability to reason about symbolic arithmetic via high performance arithmetic constraint solvers.
while all of the prior work allows for generation of data structures witharithmetic invariants with the exception of senni et al.
this capability is handled via a generate and filter approach.
that is instead of asking a constraint solver to deliver numbers which satisfy some given arithmetic constaints one must instead try all numbers in a range and filter out those which did not satisfy applicable arithmetic invariants.
not only is this inefficient it forces the data structure generator to reason about data structure shape and contents simultaneously which can be problematic.
for example consider the problem of generating sorted lists of length to n. in general there are only n unique list shapes in this space though a potentially infinite number of list structures when content is taken into account.
if the tester desires to test only structures with particular shapes without regard to contents the space is quite small.
however the need for contents can blow up the search space in a completely uninteresting direction.
with clp it is possible to reason about shape and contents independently and to request only a single satisfying solution for a list of any given length.
with the prior techniques it would be necessary to tweak various bounds in an ad hoc manner just to get a single solution and this sort of tweaking does not scale to arbitrary data structures.
iv.
d isadvantage of clp u sability for simple data structures such as singly linked lists and binary search trees clp can specify the appropriate structure invariants simply and concisely.
in fact the specifications tend to mirror the invariants closely in a very elegant way as with the sorted list example in section iii.
the prior work in data structure generation has only looked at these sorts of simple structures and thus clp seems like a clear winner for both performance and usability.
however when we extend our evaluation to more complex structures and properties we see that while clp still has much better performance than prior work its usability suffers in manner that has not been noted before in the literature.
we elaborate on these issues below.
cycles and node sharing.
consider the problem of generating a tree data structure with parent pointers imparting a type of cycle in the data structure.
such cyclical structures cannot be directly specified in clp.
the problem is that the fundamental unit of data structure creation in clp known simply as a structure does not permit cycles.
the solution is to employ indirection e.g.
to label nodes with identifiers and maintain a mapping from identifiers to nodes and to use those identifiers to describe the data structure s shape rather than the actual nodes themselves.
this strategy adds additional complexity to the specification and obscures the connection to the data structure invariants being specified.
if any part of the structure may contain a cycle then indirection must be used for the entire data structure specification.
to illustrate this problem consider the simple update of a parent pointer.
ideally to update node n1 s parent to be node n2we need only write n1.parent n2 which corresponds closely to the expected implementation.
however with clp we instead need to write something similar to the following in pseudocode removeedgesannotatedwith edges getnode nodes n1 parent addedgeannotatedwith edges getnode nodes n1 getnode nodes n2 parent .
where getnode removeedgeannotatedwith and addedgeannotatedwith perform map lookups and updates to observe and change the underlying representation of the graph.
this style is clearly much more verbose than the original making clp a poor choice for the representation of cyclical data structures.
a similar problem holds for data structures that are daglike i.e.
where a single sub structure may be referenced via multiple paths in the data structure.
clp naturally tends to divide its generation into independent sub structures in a bottom up manner.
clp will recursively build a series of independent sub structures then bind them together into the overall structure.
this strategy does not work if those substructures are not independent e.g.
if they need to all refer to the same elements.
in this case the clp specification must explicitly build the to be shared elements and explicitly pass them down through all of the recursive calls so that each sub structure will be referencing the same elements.
again this adds complexity and obscures the connection to the data structure invariants being specified.
imperative operations.
clp does not offer imperative style for andwhile loops which is inconvenient when it comes to representing imperative operations which use these features.
more importantly while most clp engines do allow for limited forms of imperative reassignment these operations behave in a manner which is incompatible with data structure generation.
that is while these operations can be easily used to verify if a given data structure satisfies a property they cannot be used to generate such satisfying data structures.
attempts to do so will silently produce incorrect results or otherwise bizarre behavior ultimately because these operations break the otherwise logical semantic model of clp.
this issue forces the user to rewrite such operations in a functional manner often using a store passing style that adds yet another layer of indirection.
this extra indirection is fraught with the same sort of issues as described previously with cycles and is highly undesirable from a usability standpoint.
the need for data structure specifications that model imperative operations is novel to this work and it is a direct consequence of our modeling of advanced properties on complex data structures.
metaprogramming.
most clp engines support metaprogramming through the call instruction which structure values to behave as calls to clauses i.e.
code .
in many ways call is comparable to the notorious eval construct in languages like javascript as it fundamentally allows for dynamicallygenerated code to be executed.
metaprogramming can beused to parameterize computations and minimize redundant code which we have found very useful in specifying the data structures and properties described in section v. however it is an error prone technique with substantial performance drawbacks.
in fact because of performance concerns it is common practice in the clp community to write macros which expand code using metaprogramming into code that does not use metaprogramming which imparts complexity .
in general it is desirable to avoid metaprogramming both from a correctness and performance perspective but we have found in our own specifications that it is almost impossible to avoid metaprogramming without significantly bloating the specifications.
we have thus reluctantly come to the conclusion that metaprogramming is an unfortunate must have.
downstream testing.
with clp assuming the downstream code we want to test is not itself in clp there is a disconnect between the generated structures and the appropriate datatype in the language of the application being tested.
this means that some sort of translation layer between the data structures in clp and the data structures in the system under test must be in place which adds complexity and could potentially harm the performance gains afforded by clp.
this issue was one of the motivating factors behind korat and is of importance for any practical data structure generation technique.
v. d ata structures and properties in this section we describe the seven data structures on which we evaluate the competing data structure generation techniques.
in addition to the baseline data structure definitions we also describe for each data structure an additional property that targets an interesting part of the space of such structures these additional properties stress both the performance and usability of the competing generation techniques.
three of the structures have been evaluated in prior work and are included here for comparison sorted linked lists redblack trees and heaps.
four of the structures have never been evaluated for generation before this work image grammars skip lists splay trees and b trees.
the additional properties for all of the structures including the three structures seen in prior work are novel to this work.
here we informally describe the structures and properties.
the exact predicates that we use to specify them are available in the supplementary materials3 including the korat udita and clp specifications.
sorted linked lists.
a sorted linked list is a linked list whose nodes are ordered according to their contents in this case integers.
both udita and senni et al.
generated these data structures.
additional property we target lists where each integer element is separated by at most a value of k. for example a valid list for k would be .
red black trees.
a red black tree is a type of balanced binary search tree that is commonly used as an efficient pllab under the downloads link.representation for sets and maps.
korat udita and senni et al.
all showed that they could generate red black trees with varying degrees of success.
additional property we target red black trees such that inserting a given element is guaranteed to cause rebalancing.
intuitively this means that given an element value we generate red black trees such that if the given element is inserted into the tree it will cause a rebalance to occur.
this property requires us to encode the insertion and rebalancing operations in our predicate that describes acceptable red black trees.
heaps.
a heap is a type of balanced binary tree that is commonly used to represent priority queues efficiently.
while heaps are usually described as trees they are often backed by arrays in imperative settings and thisis the underlying representation that we use as well.
as with red black trees korat udita and senni et al.
all generated these structures.
additional property we target heaps which require exactly log2noperations on dequeue where nis the number of nodes in the heap.
such data structures show worst case behavior and are interesting not only for testing but for benchmarking.
this property requires encoding the dequeue operation in the predicate describing acceptable heaps.
image grammars.
many data representations can be explained in terms of grammars.
astgen heavily focused on grammars and claimed that declarative approaches were generally ill suited to generating structures that obey grammars.
while there is prior work on generating programs from context free grammars we instead focus on the contextsensitive grammar of the ani image format .
bugs in parsers for this grammar have been historically costly .
we observe that the ani grammar is not well documented and that there are several edge cases where it is unclear if a parser should accept or reject a given image.
for our standard definition of ani images we avoid these edge cases.
additional property we target specifically those edge cases that we avoid in the standard definition.
in other words we target ani images that are guaranteed to contain at least one edge case.
these edge cases are arate subsection is named list which introduces a parsing ambiguity with another image component with the same name.
aninfolist subsection has size which should not be possible with valid data.
thetitle orauthor field holds a non printable character.
the image contains no icons indicated with an icon length of which are core components of the image.
thejifrate is which corresponds to an animation that would move infinitely fast.
skip lists.
a skip list is a special dag like representation of a linked list that allows for multiple elements in a list to be traversed in a single operation.
the consequence of this on peformance is that inserting an element into a sorted linkedlist can be performed in o logn unlike the typical o n .
of special interest is that these data structures rely on probabilistic features and thus do not have deterministic shapes.
additional property we target skip lists where fewer than k of the elements have the maximum height.
the observation this property is based on is that the smaller this percentage becomes the less likely the data structure is in practice due to the probabilistic features of the skip list algorithm and thus we are more likely to generate what can be considered an edge case.
ideally we would like a very small percentage though this percentage also influences the number of elements in the tree.
to keep list sizes manageable we use k .
splay trees.
splay trees are a type of binary search tree which are known for their imperative implementation.
the value of these trees is in their ability to reconfigure themselves so that from an intuitive standpoint elements which are freqently accessed are cheaper to access than others.
central to this reconfiguration is a splay operation that moves a given element to the root of the tree via a series of modifications.
additional property due to the splay operation the shape of a splay tree can vary widely between different operations which call splay .
for testing we are interested in particularly dramatic changes to the tree s shape.
specifically we want to generate trees for which the following two properties hold in conjunction where nis the total number of nodes in the tree the tree contains at least one node at depth greater than d1 log2 n e. if asplay operation is performed on any single node in the tree all nodes in the tree would have depth d1 log2 n e. the aforementioned properties define splay trees which can become more balanced via some particular use of splay .
generating such splay trees would be useful for testing any optimization scheme based on this observation.
b trees.
b trees are a complex tree based data structure which are used heavily in databases and filesystems.
given the fact that these are so popular at base system levels it is useful to be able to generate these automatically for testing purposes.
additional property we take a similar approach as with red black trees generating trees which would experience nodesplitting given some particular value to insert.
vi.
e valuation we evaluate and compare korat udita and clp for performance scalability and usability.
the overall goal of our evaluation is to back our claims that clp is a highperformance technique but one which can be unwieldy to use especially with respect to imperative operations.
a. experimental methodology we have specified basic versions of the seven data structures described in section v and also advanced versions containing the additional properties in each of korat udita and clp.
to be concise we uniformly refer to these versions as datastructures .
the basic data structure is referred to as basic and the version of the data structure with the additional property is referred to as special .
to measure performance we record the time each technique takes to generate all structures within a given set of bounding values.
while evaluations in prior work report bounds as a single uniform value n we observe that this does not reflect reality for even the simplest of data structures.
that is for all the data structures involved there are multiple distinct bounding values that must be specified.
therefore we report all of the bounding values used for each data structure.
a description of these bounding values is provided in table i. henceforth we will refer to these bounding values via commaseparated lists of integers where the integer s position reflects which bound is being referred to in table i and the integer value is the actual bound.
for example with basic sorted lists the bounds would mean a maximum of two nodes and a maximum element value of three.
additionally we set k for special sorted lists see section v and we ensure that we insert an element distinct from the tree contents for special red black trees and special b trees.
to measure scalability we break the performance results into three separate groupings based on small medium and large bounding values.
for clp we chose gnu prolog as our engine due to its public availability and high performance.
as a proxy for usability we record the approximate amount of time needed to specify the given data structure for an author already familiar with the particular generator language being used.
in cases where code was already provided by the authors of korat and udita we consider this to have taken minutes.
where possible we used publicly available code for our specifications.
in all cases we specify the additional property for a given data structure after the data structure itself was specified which often helped to reduce the amount of time necessary to specify the additional property.
secondary to the amount of time needed to specify the data structures are various measures which act as proxies for code complexity.
for each of the techniques we record lines of code loc .
we record the maximum number of variables ever in scope at once which gives an idea of the amount of state manipulation and passing that is required to specify a given data structure and we argue is correlated with the amount of state the programmer must reason about.
for korat and udita we record the number of conditionals treating loops as conditionals along with the maximum nesting depth for conditionals.
the observation here is that conditionals especially deeply nested conditionals signify complex control flow.
this measure does not have a direct translation in clp which is why it is only recorded for the java based korat and udita.
for clp we measure the maximum call graph strongly connected component scc size after implications are refactored into clauses which provides a measure of how many levels deep mutual recursion becomes.
for example a maximum scc size of four means that there are four clauses that contain mutually recursive