tool choice matters javascript quality assurance tools and usage outcomes in github projects david kavaler university of california davis dmkavaler ucdavis.eduasher trockman university of evansville asher.trockman gmail.combogdan v asilescu carnegie mellon university vasilescu cmu.eduvladimir filkov university of california davis filkov cs.ucdavis.edu abstract quality assurance automation is essential in modern software development.
in practice this automation is supported by a multitude of tools that fit different needs and require developers to make decisions about which tool to choose in a given context.
data and analytics of the pros and cons can inform these decisions.
y et in most cases there is a dearth of empirical evidence on the effectiveness of existing practices and tool choices.
we propose a general methodology to model the timedependent effect of automation tool choice on four outcomes of interest prevalence of issues code churn number of pull requests and number of contributors all with a multitude of controls.
on a large data set of npm javascript projects we extract the adoption events for popular tools in three task classes linters dependency managers and coverage reporters.
using mixed methods approaches we study the reasons for the adoptions and compare the adoption effects within each class and sequential tool adoptions across classes.
we find that some tools within each group are associated with more beneficial outcomes than others providing an empirical perspective for the benefits of each.
we also find that the order in which some tools are implemented is associated with varying outcomes.
i. i ntroduction the persistent refinement and commoditization of software development tools is making development automation possible and practically available to all.
this democratization is in full swing e.g.
many github projects run full devops continuous integration ci and continuous deployment cd pipelines.
what makes all that possible is the public availability of high quality tools for any important development task all usually free for open source projects .
linters dependency managers integration and release tools etc.
are typically well supported with documentation and are ready to be put to use.
often there are multiple viable tools for any task.
e.g.
the linters eslint andstandardjs both perform static analysis and style guideline checking to identify potential issues with written code.
this presents developers with welcome options but also with choices about which of the tools to use.
moreover a viable automation pipeline typically contains more than one tool strung together in some order of task accomplishment each tool performing one or more of the tasks at some stage in the process.
and the pipelines evolve with the projects adding and removing components is part andparcel of modern software development.
as shown in table i out of projects gathered in our study use at least one tool from our set of interest adopt more than one tool over their lifetime and projects switcht able i tool adoption summary st a tistics adoption events across projects tool task class per tool per task class daviddependency management20 bithound gemnasium codecov coverage2 codeclimate coveralls eslint linter7 jshint standardjs note total projects under study projects which adopt tools under study projects use different tools in the same task class from one tool to another which accomplishes a similar task.
having multiple tools available for the same task increases the complexity of choosing one and integrating it successfully and beneficially in a project s pipeline.
what s more projects may have individual context specific needs.
a problem facing maintainers then is given a task or a series of tasks in a pipeline which tools to choose to implement that pipeline?
organizational science particularly contingency theory predicts that no one solution will fit all .
unsurprisingly general advice on which tools are best is rare and industrial consultants are therefore busy offering devops customizations .
at the same time trace data has been accumulating from open source projects e.g.
on github on the choices projects have been making while implementing and evolving their ci pipelines.
the comparative data analytics perspective then implores us to consider an alternative approach given sufficiently large data sets of ci pipeline traces can we contextualize and quantify the benefit of incorporating a particular tool in a pipeline?
here we answer this question using a data driven comparative contextual approach to customization problems in development automation pipelines.
starting from a large dataset of npm javascript packages on github we mined adoptions of three classes of commonly used automated quality assurance tools linters dependency managers and code coverage tools.
within each class we explore multiple ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
tools and distill fundamental differences in their designs and implementations e.g.
some linters may be designed for configurability while others are meant to be used as is.
using this data and informed by qualitative analyses sec.
ii b of issue and pull request pr discussions on github about adopting the different tools we developed statistical methods to detect differential effects of using separate tools within each class on four software maintenance outcomes prevalence of issues as a quality assurance measure code churn as an indication of setup overhead number of pull requests for dependency managers as an indication of tool usage overhead and number of contributors as an indication of the attractiveness of the project to developers and the ease with which they can adapt to project practices .
we then evaluated how different tools accomplishing the same tasks compare and if the order in which the tools are implemented is associated with changes in outcomes.
we found that projects often switch tools within the same task category indicating a need for change or that the first tool was not the best for their pipeline.
only some tools within the same category seem to be generally beneficial for most projects that use them.
standardjs coveralls and david stand out as tools associated with post adoption issue prevalence benefits.
the order in which tools are adopted matters.
ii.
j ava script continuous integra tion pipelines with different quality assurance tools in this work we study how specific quality assurance tool usage tool choice and the tasks they accomplish are associated with changes in outcomes of importance to software engineers in the context of ci.
we focus on ci pipelines used by javascript packages published on npm javascript is the most popular language on github and npm is the most popular online registry for javascript packages with over published packages.
while there is hardly any empirical evidence supporting the tool choices there has been much research on ci pipelines.
we briefly review related prior work and discuss tool options for our task classes of interest.
a. ci tool configuration and tool integration ci is a widely used practice by which all work is continuously compiled built and tested multiple ci tools exist .
the effect of ci adoption on measures of project success has been studied with prior work finding beneficial overall effects .
as part of ci pipelines third party tools for specific tasks are often used.
though the notion of using a pre built solution for common development tasks is attractive these tools are often far from being usable off the shelf.
hilton et al.
describe multiple issues that developers experience when configuring and using ci including lack of support for a desired workflow and lack of tool integration developers want powerful and highly configurable systems yet simple and easy to use.
however xu and zhou find that over configurable systems hinder usability.
it has also been shown that developers often don t use the vast majority ofconfiguration options available to them .
misconfigurations have been called the most urgent but thorny problems in software reliability .
in addition configuration requirements can change over time as older setups become obsolete or must be changed due to e.g.
dependency upgrades .
the integration cost for off the shelf tools has been longstudied in computer science .
however much of this research is dated prior to the advent of ci and large socialcoding platforms like github.
thus we believe the topic of tool integration deserves to be revisited in that setting.
b. tool options within three quality assurance task classes there are often multiples of quality assurance tools to accomplish the same task and developers constantly evaluate and choose between them.
e.g.
a web search for javascript linters turns up many blog posts discussing options and pros and cons .
we consider sets of tools that offer similar task functionality as belonging to the same task class of tools.
specifically we investigate three task classes commonlystudied in prior work e.g.
and commonly used in the javascript community linters dependency managers and code coverage tools.
below to better illustrate fundamental differences in tools within the same task class and note tradeoffs when choosing between tools we intersperse the description of the tools with links to relevant github discussions.
to identify these discussions we searched for keyword combinations e.g.
eslint jshint on github using the github api more details in section iv identifying relevant issue discussion threads that explicitly discuss combinations of our tools of interest.
one of the authors then reviewed all matches removed false positives and coded all the discussions the emerging themes were then discussed and refined by two of the authors.
linters.
linters are static analysis tools commonly used to enforce a common project wide coding style e.g.
ending lines with semicolons.
they are also used to detect simple potential bugs e.g.
not handling errors in callback functions.
there has been interest in the application of such static analysis tools in open source projects and ci pipelines .
within npm common linters include eslint jshint and standardjs .
qualitative analysis of issue threads discussing linters suggests that project maintainers and contributors are concerned with specific features discussions and ease of installation with some recommending specific linters based on personal preferences or popularity favoring those that they have used before without much justification e.g.
.
two categories of linters emerge first eslint and jshint are both meant to be configured to suit a project s specific needs and preferences among them eslint is more highly configurable as it allows for custom plugins.
second standardjs is meant to be a drop in solution to linting it is not meant to be further configured.
1surprisingly across projects in our data which adopted tools only issue threads explicitly discuss combinations of our tools of interest.
this relatively low frequency suggests a lack of deliberateness in choosing tools which underscores the need for this study especially given our findings.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
eslint shigh configurability makes it a common choice discussions .
ten projects switched to eslint or jshint after using older linters which lack this functionality in fact four view eslint as a combination of older linters .
on the other hand increased configurability comes with increased overhead as noted in one discussion eslint has so many options often leads to insignificant debates about style... this makes the project harder to maintain and hinders approachability .
in contrast standardjs addresses the problem of optionality encouraging projects to adopt a standard configuration .
one developer notes it may be more convenient for contributors who are more likely to be familiar with the standard style .
however another developer finds it less appealing as they simply don t agree with those default settings .
overall ten of the found issues included some form of discussion between contributors and maintainers.
this indicates at least partly that some developers are interested in discussing the pros and cons of linter choice.
based on these anecdotes we hypothesize that a complex project with project specific code style requirements may be more likely to use eslint orjshint while a smaller project may opt for standardjs because it is easier to set up.
while highly configurable tools like eslint andjshint require more effort in configuration they may nullify disputes over code formatting and facilitate code review.
standardjs requires less initial effort from project maintainers and may be appealing to new contributors who are familiar with the common code style.
however since its adoption likely entailed less configuration effort maintainers may need to spend more time on code review or address more issues about code styling.
coverage tools.
coverage tools executed locally or as part of a ci pipeline compute and report code coverage typically line coverage a measure of test suite quality.
popular javascript coverage tools include istanbul andjscover .
if used as part of a ci pipeline the coverage results are often sent to a third party service such as coveralls orcodeclimate which archive historical coverage information and offer dashboards.
third party tools require some additional configuration over simply using a coverage reporter locally and may result in more issues and pull requests as code coverage information is made available or reacted to by the community.
out of ten issue discussions about coverage tools in four developers talked about not wanting to spend too much time on configuration .
one developer states that the sole purpose of such a tool is to get the coverage badge i.e.
to signal that the project cares about test coverage .
as expected two developers state that they use a certain coverage tool simply because they are more familiar with it .
it seems developers want to invest minimal effort in coverage services but seem unaware that some services require more overhead than others a yml config file seems silly when all that you want is coverage states a developer .
coveralls and codecov are considered very focused services generally providing as per the issue discussions only coverage without additional features.two developers claim that coveralls is unreliable inspiring a switch to codecov which is said to have a better user experience .
codeclimate provides other services besides coverage e.g.
linting and developers may be confused by its high configurability codeclimate isn t sure if it wants to be a ci or not states a developer .
dependency managers.
dependency management services help keep dependencies up to date and secure.
they make the status of dependencies visible to project maintainers encouraging updates after new releases through automated notifications.
dependency management is a popular research topic .
david gemnasium and snyk are examples of such tools that require manual intervention their results are often displayed as badges on a project s readme file.greenkeeper a github bot provides automatic dependency updates at each new release.
it creates a pr with the updated dependency which in turn triggers the ci service to run the build.
this allows the project maintainers to immediately see if an upgrade will incur significant work.
if the build does not break then it is likely safe to merge the update.
hence projects must choose if they would like manual or automatic dependency upgrades.
intuitively manual upgrades may require more work for developers due to upgrading and testing.
however automatic upgrades increase the number of prs to review and may cause notification fatigue .
dependency managers face similar trade offs between configurability and ease of use although we found only five issues discussing them.
our analysis suggests that developers value ease of installation e.g.
low configuration overhead over other concerns e.g.
one developer prefers david since it s a specific easy to install service as opposed to bithound which as another developer notes provides dependency management as well as numerous extra features .
in summary most projects decide which tool to select based on personal preference popularity or implementation cost.
however we find very little explicit discussion of tool choice within issue discussions suggesting that tool choice may not be very deliberate.
iii.
r esearch questions we investigate three main research questions rq how often do projects change between tools within the same task class?
rationale.
github projects have varying needs and thus likely require configurable tools.
however given the complexity of tool configuration in general and the requirement of keeping configurations up to date developers may need to change between tools within a given task class over time.
rq 1studies the stability of the projects quality assurance pipelines.
rq are there measurable changes in terms of monthly churn pull requests number of contributors and issues associated with adopting a tool?
are different tools within an equivalence class associated with different outcomes?
rationale.
despite great research interest in ci pipelines there is a distinct lack of empirical evidence regarding tool authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
choice and associations with outcomes of interest.
prior work has shown that there are costs when integrating pre built tools in general but the empirical evidence for differential costs within task classes is lacking especially in the choiceladen github open source setting.
rq 2focuses on these costs.
to drill into our second research question we investigate specific subquestions rq2.
is linter tool adoption associated with lower monthly churn?
rationale.
prior work has reported that javascript developers use linters primarily to prevent errors .
we expect a decrease in monthly churn after linters start being used because the linter may catch issues that would have otherwise had to be changed after the initial commit.
rq2.
i sstandardjs associated with more monthly contributors?
rationale.
as discussed in section ii standardjs is meant as a drop in solution implementing standard javascript style guidelines while eslint is more customizable but more complex.
developers potential surprise or frustration with unusual style guidelines may impact how engaged they remain with a project.
therefore we expect that other variables held constant projects using more standard javascript style guidelines would be more attractive to developers who in turn may remain engaged longer.
rq2.
are coverage tools associated with immediate overhead measured by monthly churn and prs?
rationale.
test coverage has been described as important for software quality .
however achieving full coverage is difficult and relies on an appropriate test suite.
thus we hypothesize that the adoption of a coverage reporting tool is associated with an immediate overhead cost as coverage tools likely require moderate to large infrastructure and test suite changes to facilitate their reporting.
rq2.
are dependency managers associated with more monthly churn and prs?
rationale.
prior work found that dependency management is according to one developer one of the most significantly painful problems with development .
the authors report that developers identified strategies that can be roughly categorized as quick scheduled or reactive .
we note that the existence of a dependency management tool may not affect monthly churn and prs for projects with a scheduled update policy i.e.
a systematic method for reviewing dependencies but may affect those with quick or reactive policies as they are more likely to be sensitive to such a tool s warnings dependencies should be updated and developers may do this through prs additionally reflected in churn.
rq2.
are the studied tools associated with issue prevalence?
rationale.
since the tools studied are quality assurance tools developers may hope that adopting a tool will decrease the prevalence of reported issues either right away or over time.
a reduction in the number of issues can give developers more time to provide new features maintain existing ones etc.if we observe such an association it can provide evidence for the argument that all else being equal the tool associated with fewer issues should be adopted.
rq are certain tool adoption sequences more associated with changes in our outcomes of interest than others?
rationale.
lastly we investigate whether integration cost is incurred differently given prior tool adoptions.
for example implementing a coverage tool before a linter may incur less integration cost than in the opposite order as coverage tools are accompanied by large test suites if there are systematic issues with a test suite a linter may point them out.
thus if the test suite is not initially written according to a linter s guidelines there may be additional cost associated with integrating a linter as the improper formatting must be fixed.
iv .
d ata a n d methods here we discuss practical matters of data collection including extraction of tool adoption events task class creation negative control data model building and interpretation.
a. data collection to determine when projects incorporate linters and coverage services into their ci pipelines we downloaded build information from travis ci the most popular ci service on github.
while such tools may be declared in a project s .travis.yml configuration file they are often delegated to shell scripts task runners makefiles or the npm package.json file making discovery non trivial.
hence instead of scanning repositories for evidence of tool use we scan the travis ci build logs.
travis ci allows projects to define a build matrix which specifies the configuration for numerous jobs .
the build matrix could specify that only one or more than one jobs involve running linters or coverage services.
hence for the most popular npm projects we had access to we queried the travis ci api to download build information e.g.
date status and if the build was started via push or pull request as well as all associated jobs.
this resulted in .
million rows of build metadata and .
million rows of corresponding jobs.
to determine the time of tool adoption we downloaded all associated job logs per project at monthly intervals it would have been intractable to download all .
million logs.
the granularity in adoption dates in our data is one build per month per project.
in total we downloaded logs for projects.
we say that a project is using a given tool in a given month if any of the job logs contain corresponding traces.
we scanned the job logs case insensitively for instances of the tool names eslint jshint standard coveralls codecov codeclimate occurring on lines starting with or i.e.
lines corresponding to the execution of a command.
this heuristic is necessary since otherwise we would detect many false positives e.g.
for projects that include dependencies with names including eslint that do not actually run eslint as part of the ci build.
the adoption date of a tool is the first month that has a job log containing an execution of that tool.
dependency managers however are not typically included in the ci pipeline.
instead we determine their adoption date from the corresponding badge on a project s readme .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
for project level github measures e.g.
number of commits number of prs we use data from the march dump of ghtorrent aggregated monthly and join this to our tool adoption data.
for our monthly authors outcome we look at the authors for all commits in a given month and project recording how many are unique.
we also mined .
million issues for npm projects from ghtorrent the remaining projects had no issues and the corresponding .
million issue comments from ghtorrent s mongodb raw dataset.
b. choosing tools that do not overlap we want to model both tool adoption and tool switching within task classes if it occurs.
identifying precise tool removals from travis ci job logs is prone to false positives as projects may and according to manual review of our data do move a given tool run outside of the travis ci job itself instead running it as e.g.
a pre ci job step.
the difficulty of parsing ci job logs has been noted by prior work .
instead to infer tool removal we carefully chose the task classes and popular competing tools within them which should not be used simultaneously.
thus we say that a tool is removed when another tool within the same task class is adopted.
we believe this is a reasonable decision as e.g.
we find it unlikely that a project will use two different linters simultaneously thus the adoption of a new linter likely coincides with the removal of the old one.
this is partially supported by our qualitative analysis.
this assumption affects most data shown in table i therefore we acknowledge it as a threat to validity.
c. on negative controls and interventions in standard experimental design with intervention e.g.
clinical trials there are often two basic groups a negative control group which receives no intervention and a treatment group which receives an intervention.
however with observed data as on github generally these two groups do not exist a priori the intervention is observed rather than induced in a specific group with proper prior randomization.
in order to view tool adoption as an intervention we first form an analogous negative control group otherwise our data would consist of only those projects that receive an intervention.
lacking a proper negative control reduces statistical power as there exists no group to compare to the treatment group.
thus our data also consists of projects which do not adopt any of the tools under study providing a form of negative control.
d. time series modeling we use linear mixed effects regression lmer to measure the relationship between our outcomes dependent variables and our explanatory variables of interest under the effect of various controls.
lmer can be seen as an extension of standard ordinary least squares ols linear regression allowing for the addition of random effects on top of standard fixed effects .
fixed effects in an lmer can be interpreted the same way as coefficients in an ols regression.
random effects are used to model often sparse factor groupings that may imply a hierarchy nesting within the data or to controlformultiple observation of the same subjects in time.
in ols regression multiple observation can lead to multicollinearity which can limit inferential ability .
lmer explicitly models correlation within and between random effect groupings thus reducing the serious threat of multicollinearity when modeling longitudinal data using other methods.
in our data we use a random effect grouping for each project.
ideally one would be able to fit a separate ols regression to each project inspecting individual coefficients to test hypotheses.
however data within project groups are often sparse i.e.
many projects have too few data points to model individually in a statistically robust manner.
in an ols regression one may attempt to combine data from all projects and model project groups as a factor.
however this approach is also vulnerable to sparsity for similar reasons.
thus random effects or e.g.
lasso regression can be used as a form of shrinkage .
each random effect group level is shrunk towards its corresponding grand mean analogous to standard regularization strategies .
one can interpret this as fitting a separate ols regression to each project with a constraint the project level intercept for each individual ols regression is a deviation from the mentioned grand mean .
this is distinctly different than fitting a standard ols regression to each project as this random effect constraint allows us to model projects with far fewer data points than would be allowed when fitting separate ols regressions due to the aforementioned constraint.
we model the effect of tool adoption over time across github projects for multiple outcomes of interest.
to do this we use a model design analogous to regression discontinuity design rdd using mixed effects regression .
in this work we treat each tool adoption event as an intervention analogous to administering a drug treatment in a clinical trial.
rdd is used to model the extent of a discontinuity in a group at the moment of intervention and lasting effects post intervention.
the assumption is that if an intervention has no effect on an individual there would be no significant discontinuity in the outcome over time the post intervention trajectory would be continuous over the intervention time.
thus rdd allows us to assess how much an intervention changes an outcome of interest immediately and over time via change in the trajectory slope .
fig.
shows real data for monthly pull requests centered at the time of eslint adoption.
note the discontinuity at the time of intervention which serves as motivation for the applicability of rdd here.
fig.
depicts the four theoretical cases for an intervention in our model design.
prior to an intervention there is some trend depicted as a dashed line.
at the moment of intervention there can be a positive or negative immediate effect discontinuity in addition to a positive or negative change in slope.
thus our design can be expressed as yij i itime ij iintervention ij itimeafterintervention ij icontrols ij epsilon1ij wherejis a particular project and iindexes the set of observations for a given project with the ability to have authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
.
time centered on eslint adoptionmonthly pull requests fig.
.
eslint intervention for prs centered at projects sampled.
basic ols trajectories fitted before after intervention shown as dashed lines.
discontinuity slope discontinuity slope discontinuity slope discontinuity slope .
.
.
.
.
.
.
.
.
.
timeoutcome fig.
.
four theoretical basic cases for longitudinal mixed effects models with interventions.
intervention times marked in red.
multiple time varying controls.
note that we are not limited to a single intervention as expressed above we can and do have multiple tool adoption events within the same model.
the time ijvariable is measured as months from the start of a project to the end of our observation period intervention ijis an indicator for time occurring before or after tool adoptions and time after intervention ijis a counter measuring months since the tool adoption.
in mixed effects regression these parameters are aggregated across all groups projects yielding a final coefficient useful for interpretation.
this rdd formulation along with the usage of mixed effects regression allows for the simultaneous analysis of many projects all with varying observation periods i.e.
differing project lifetimes with interventions occurring at different points in time i.e.
not necessarily aligned .
this flexibility allows for complex and statistically robust analysis with simple interpretations.
for points before the treatment holding controls constant the resulting regression line has a slope of i after the treatment i i. the effect of an intervention is measured as the difference between the two regression values of yij pre and post treatment equal to i. this design is used in different models to examine rq .to examine rq we build 3additional longitudinal models with observed tool adoption sequences modeled as a factor along with the same controls used in our prior models.
the general form of the model for each outcome of interest is yij i itime ij icurrent toolsequence ij icontrols ij epsilon1ij where current tool sequence ijis a cumulative factor indicating what tools have been adopted for a given project until then including a random intercept for each project j. e.g.
if project jadopts tool ain month bin month and cin month withaandcin the same task class our data would have observations for project j with a tool sequence of a a b a b b c where a bis repeated in month 3andais removed in month 4ascbelongs to the same task class.
using this formulation we can analyze whether an association exists between a given tool sequence and our outcomes of interest.
in both formulations we include additional fixed effects multiple iestimated as controls e.g.
project size popularity and age.
the idea of variable significance in lmer is greatly debated in statistics mostly due to the lack of classical asymptotic theory as used for inference in e.g.
ols regression .2these issues can be dampened by large sample sizes as we have but not completely avoided.
we report significance using satterthwaite approximation for denominator degrees of freedom for regression variable t tests implemented using the lmertest r library .
this is considered a reasonable approach when sample sizes are large .
we account for multicollinearity by considering only fixedeffect control variables with vif variance inflation factor less than5 as having many fixed effects along with a complex design structure can introduce issues in model estimation.
in addition lmer is sensitive to extreme valued parameters and having many fixed effects can require heavy re scaling making it difficult to interpret results.
to avoid extreme values in the fixed effects we log transform the churn variable which has high variance.
we trim outliers by removing the top of values in each considered variable to further avoid potential high leverage issues.
due to the large variance in our outcomes of interest there is a risk of spurious significant discontinuities a significant discontinuity at intervention time may be found due to mere noise in the signal.
thus we smooth each outcome using moving average smoothing with a window of 3months.3in addition we remove data for the exact month in which the tool is adopted.
these two steps act to combat the risk of spurious discontinuities and should not negatively affect our results as we are interested in the trend of each outcome over time rather than the absolute high variance value for a particular month.
we report psuedo r2values as described by nakagawa and schielzeth called marginal r2and conditional r2.
the marginal r2can be interpreted as the variance described by the fixed effects alone conditional r2as the variance described by both fixed and random effects.
as our data set consists of projects after filtering for described issues we expect 2examples of this debate abound listed