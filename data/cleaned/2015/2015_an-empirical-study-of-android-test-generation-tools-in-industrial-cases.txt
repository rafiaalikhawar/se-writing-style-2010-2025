an empirical study of android test generation tools in industrial cases wenyu wang university of illinois at urbana champaign usa wenyu2 illinois.edudengfeng li university of illinois at urbana champaign usa dli46 illinois.eduwei yang university of texas at dallas usa weiyang.utd gmail.com yurui cao university of illinois at urbana champaign usa yuruic2 illinois.eduzhenwen zhang yuetang deng adazhang tencent.com yuetangdeng tencent.com tencent inc. chinatao xie university of illinois at urbana champaign usa taoxie illinois.edu abstract user interface ui testing is a popular approach to ensure the qualityofmobileapps.numeroustestgenerationtoolshavebeen developed to support ui testing on mobile apps especially for android apps.
previous work evaluates and compares different test generation tools using only relatively simple open source apps whilereal worldindustrialappstendtohavemorecomplexfunctionalities and implementations.
there is no direct comparison among test generation tools with regard to effectiveness and easeof use on these industrial apps.
to address such limitation we studyexistingstate of the artorstate of the practicetestgeneration tools on widely used industrial apps.
we directly compare the tools with regard to code coverage and fault detection ability.accordingtoourresults monkey astate of the practicetool from google achieves the highest method coverage on of appswhose methodcoverage datacanbe obtained.of all68apps under study monkey also achieves the highest activity coverageon apps while stoat a state of the art tool is able to triggerthe highest number of unique crashes on apps.
by analyzingthe experimental results we provide suggestions for combining differenttestgenerationtoolstoachievebetterperformance.we alsoreportourexperienceinapplyingthesetoolstoindustrialapps under study.
our study results give insights on how android ui testgenerationtoolscouldbeimprovedtobetterhandlecomplex industrial apps.
ccs concepts software and its engineering software testing and debugging permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september montpellier france copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
android ui testing test generation empirical study acm reference format wenyuwang dengfengli weiyang yuruicao zhenwenzhang yuetang deng and tao xie.
.
an empirical study of android test generation tools in industrial cases.
in proceedings of the 33rd acm ieee international conference on automated software engineering ase september3 montpellier france.
acm newyork ny usa 11pages.
introduction as the fast pace of android app development and evolution continues effectivequalityassuranceforindustrialandroidappsbecomesincreasinglynecessaryanddemanding.userinterface ui testing aiming to uncover potential app defects e.g.
crashing by mimicking human interactions has long been an important approach to ensure the quality of android apps before their de livery to end users.
to facilitate ui test automation the android developertoolkitfromgoogleprovidesmonkey anautomatic test generation tool that sends randomly generated ui event se quences to an app under test.
in addition researchers have alsoproposed various test generation tools to automate android ui testing .
theseindustrialoracademictestgenerationtoolsallshowsatisfactory performanceaccording to their ownrespective evaluation on various open source or industrial apps.
table 1shows the statisticsofsubjectsusedforevaluatingexistingandroidtestgeneration tools publishedinmajorsoftwareengineeringconferences .the lastrowofthetablealsoshowsastudyconductedbychoudharyetal.
in2015bycomparingdifferentandroidtestgenerationtools.
inthetable coveragecomparison denotedas emp.comp.
shows thenumbersofopen sourceapps denotedas opn.
andindustrialapps denotedas ind.
usedinevaluatingtheproposedtool scapability in terms of code coverage or and fault detection against othertools .bydefault thecodecoverageiscomparedacrosstools.
we mark with these entries where both code coverage and fault detectionarecompared.
case ind.
showsthenumbersofindustrial apps used in case studies for the proposed tools.
these case studies does notreport code coverage or compare the proposed tool against other related previous tools.
the sole purpose of these authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france w. wang d. li w. yang y. cao z. zhang y. deng and t. xie table overview of existing android test generation tools and their evaluation subjects tool study venueemp.
comp.
case ind.
opn.
ind.
a3e oopsla diamond28 acteve fse droidbot icse c dynodroid fse guiripper ase monkey sapienz issta stoat fse swifthand oopsla wctester fse ind triangle100 study by ase both code coverage and fault detection are compared across tools on open source apps.
diamond only code coverage is measured whereas fault detection is not measured on industrial apps.
triangle the tool is compared with only monkey but no other tools.
studies is to evaluate the proposed tools applicability on industrialandroidtestingtasks byreportingtheresultsof onlyfault detection .oneexceptionthereisa3e whichisevaluatedon only code coverage without on fault detection note that no tool comparison is conducted there .
asshownintable thereexists nocomparisonamongexistingtoolsoverindustrialappsintermsofbothcodecoverage and fault detection .
subjects for empirical tool comparison in the emp.comp.
column includeonlyopen sourceapps withone exception wctester where the proposed tool is compared withonly onebaselinetool monkey ononly oneindustrialapp wechat .
in addition although the case study evaluation of a few toolsincludesindustrialapps inthe caseind.
column notool comparisonisconductedthere andnocasestudiesonindustrial apps measureboth codecoverage and faultdetection.
thereexist agapandyetastrongneedtoinvestigateandcomparehowwell theseproposedtoolsperformonindustrialappsthat incontrastto open source apps are usually much more complex with regard tofunctionalitiesandimplementations bettermaintainedand tested and with much larger user bases and higher impacts.
to fill this gap and give practitioners and researchers insights on how existing tools perform on industrial apps in this paper we present the first empirical study that conducts comparison among existingtoolsonindustrialappsintermsofbothcodecoverageand fault detection.
in particular we investigate how existing available state of the art or state of the practice test generation tools performon68widely usedindustrialappsintermsofcodecoverage method and activity coverage and fault detection the number ofdistincttriggeredcrashes .theseappsspanacross30different categories and each of these apps has at least million installs according to googleplay .
we empirically studythe coverage and fault detection results to gain insights on each tool s strengths and weaknesses.
we also study how to efficiently combine some of these tools to achieve better code coverage or fault detection capabilities on testing industrial apps.
we also report our experience in applying these tools to testing tasks for industrial android apps.inthispaper ourempiricalstudyprovidesappdevelopersand toolresearchers vendors withinsightsonthe strengthsandweaknessesofexistingtestgenerationtools helpingthemimprovetheir tools design and implementation and their handling of realistic tasks for industrial apps.
in particular we address four main research questions in our study rq1 whatisthecodecoverage methodcoverageandactivity coverage achieved by each test generation tool under study on applicable industrial apps?
rq2 how many unique crashes can each test generation tool trigger on each applicable industrial app?
what are the causes of these crashes?
rq3 how to efficiently combine multiple test generation toolsonapplicableindustrialappstoachievebettercoverage and fault detection than applying these tools individually?
rq4 how much effort does it require to set up each test generation tool for testing industrial apps?
werundifferenttestgenerationtoolsunderstudyonselected industrialappstostudytheeffectivenessofthesetools.accordingto ourresults monkeyachievesthehighestmethodcoverageon22of 41appswhosemethodcoveragedatacanbeobtained.ofall68apps understudy monkeyalsoachievesthehighestactivitycoverageon35apps whilestoatisabletotriggerthehighestnumberofunique crashes on apps.
togain betterunderstandingofthe toolperformanceon industrialapps similartoapreviousstudymethodology werank each covered method activity or triggered unique crash in eachindustrial app based on the number of test generation tools that havecoveredthemethod activityortriggeredtheuniquecrash.for instance a method activity or unique crash is considered rank 1if only one test generation tool has covered the method activity ortriggeredtheuniquecrash.ourresultsshowthat onmanyindustrial apps monkey has the highest numbers of rank methods andactivities andstoatisabletotriggerthehighestnumbersof rank 1uniquecrashes.ouranalysisalsoprovidessuggestionsfor combining multiple tools for better coverage and or fault detection than applying these tools individually.
in summary this paper makes the following main contributions empiricalinvestigationontheeffectivenessofexistingavail ableandroiduitestgenerationtoolswhenbeingappliedon industrial apps.
detailedanalysisofthecoverageresultsachievedbyeach tool to provide insights on the strengths and weaknesses of thetestgenerationtoolsunderstudyandonhowtobetter leverage these tools.
hands on experience report of applying multiple state ofthe art or state of the practice test generation tools on complex industrial apps.
a strong implication that testing researchers for androidtest generation tools should empirically compare a newly proposed tool with related previous tools on industrial apps besidesopen sourceapps going beyondthecurrentcommon research practice of comparing tools on onlyopen source apps.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an empirical study of android test generation tools in industrial cases ase september montpellier france background in this section we present an overview of the android app components and the android os architecture.
.
android app components from the view of users and other apps an android app consists of fourtypesofcomponents activities intentfilters services and content providers .
activities.
activitiesaredesignedtoshowuiscreensconsistingof setsoflayoutsanduiwidgets e.g.
buttons .widgetsareassociated withsetsofattributes e.g.
sizesandpositions andcanbeboundto callbackmethodstohandleuievents e.g.
shortclicks .anactivity is typically used for a single specific scenario such as logging in and user registration.
intent filters.
intents are messaging objects used by components withinanapporacrossdifferentappstocommunicatewitheach other.
intent filters are used to allow only the designated intent types to be received and processed by the components.
launching an app for instance is achieved by sending a specific intent to the app main activity that intercepts such intents.
services.services are intended to perform tasks in the background without being attached to a ui screen.
downloading tasks forexample areusuallyimplementedusingservicestoavoidblocking the usage of app main functionalities.
content providers.
content providers enable apps to expose and manage a globally shared set of data.
for instance a user s contact informationisstoredinanandroidsystemappandmaybeaccessed by other apps using the specific content provider.
.
android os architecture theandroidosisanopen sourcelinux basedsoftwarestack .
androidappsrunwithinindividualsandboxes namelyinstances of the dalvik virtual machine dalvikvm on top of native libraries and the linux kernel.
android frameworks which are responsible for low level functionalities of the android os includinguiandactivitymanagement alsorunwithininstancesofthe dalvikvmandcanbereachedbyappsviaandroidapis.system apps are pre installed on the android os to provide users with basic features such as phone calling and sms sending.
the java source code of an android app is compiled into dexcode andsubsequentlypackagedasanandroidpackage apk file along with other resource files.
developers are also allowed to write their app libraries in c c as native libraries and invoke them through the java native interface jni .
the app can then be installed on a compatible android os.
at runtime the system s dalvikvm reads the app s dex code and executes it.
starting from android4.
theandroidruntime art isincludedwiththeandroid os where the art translates and optimizes dex code to native machine code during installation to enable faster execution.
notethatbothdalvikvmandarthavethe 64kreferencelimitation i.e.
therecannotbemorethan65 536methodsinasingle .dexfile thatcontainsanapp sdex code duetothedesignofthedalvikvm instruction set.
android provides multidex support to mitigate this limitation.
selection of android test generation tools we choose state of the art or state of the practice ui test generation tools for our study.
monkey is the official test generation tool shipped with all android devices while the rest are all publishedattopvenuesofsoftwareengineering.weselecttoolsthat are applicable on at least half of the industrial apps under study.
table2presentsanoverviewofthetestgenerationtoolsthatwe examine and our decision on tool selection.
inthissection wefirstpresenttheselectedtoolsforourstudy.
wethenillustratetheexcludedtoolsalongwiththereasonswhy these tools cannot be included for the study.
.
selected tools under study .
.
monkey.
monkey is a purely randomized android test generation tool from google that generatespseudo random streamsofuievents e.g.
clicks touches andgesturesonui and limitedtypesofsystem levelevents suchasvolumecontrols to unmodified android apps.
monkey is the most widely used tool in industrialsettingsduetoitsapplicabilitytoavarietyofapplication settings e.g.
easeof use andcompatibility with differentandroid platforms .
.
.
wctester.
to inherit the advantages of monkey while addressingitsmajorlimitations thewechatteamdevelopsanewap proach incorporatingthreemainstrategies.first wctester findsandtriggersonlyenabledeventsoneachuiscreen.second wctester focuses on generating events with higher chances to change current ui states.
third wctester considers the ui statehistory and avoids repetitions during exploration.
the new approachleadstosignificantperformanceimprovementsontesting the wechat app one of the most popular messenger apps in the world with over billion monthly active users .
.
.
sapienz.
sapienz isanevolutionary testing based testgenerationtoolforandroiduitesting.itleveragesagenetic algorithm toevolvegeneratedseedinputsequencestosearch fortheoptimizedtestsuitescontainingshortinputsequenceswhile maximizingcodecoverageandfaultrevelation.
pre definedinput sequences i.e.
motifgenes areleveragedtocomplementtherandom exploration and provide local exercise for different types of uiwidgets.
stringresources insideapps areextracted asseedsfor text inputs.
multi level instrumentation is supported to accommodatevariousapps.testsuitescanbeevaluatedsimultaneouslyon multiple devices to speedupthesear chprocess.
.
.
stoat.
stoat isauitestgenerationtoolforandroid apps with model basedevolutionary testing.it constructsa probabilistic ui state transition model through dynamic explorationand optional static analysis at the first stage.
it then evolves themodel to search for the optimized model with regard to comprehensivefitnessscoresoftheconcreteinputsequencesderivedfromgibbssampling onmodels.codecoverage modelcoverage and test suitediversityarereflectedinthefitnessscore.system level events are also randomly injected to further enhance the testing effectiveness.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france w. wang d. li w. yang y. cao z. zhang y. deng and t. xie table overview of android test generation tools under study tool open sourceno need of modification exploration strategyno need of app source code app platform monkey random wctester random sapienz evolutionary stoat model based evolutionary droidbot model based a3e depth first systematic instrumentation is optional for sapienz and stoat.
.
.
droidbot.
droidbot is a programmable light weight and model based android ui testing tool.
it generates ui guided test inputs based on a state transition model constructed on thefly.italsoallowsdeveloperstowritetestingscriptstocustomize the exploration strategy.
detailed testing reports are provided after eachtesttohelpdevelopersunderstandapps behavior.droidbot has received over stars on github at the time of writing.
.
.
a3e depth first.
a3e includes a systematic testing tool i.e.
a3e depth first that performs a depth first search strategy during exploration.
such a search strategy mimics user actions and aims to thoroughly cover app functionalities.
another strategy namedtargeted exploration is alsoproposed for fast directexploration of activities as opposed to the general purpose exploration thataims forhighercode coverageorfault detection ina3e.the strategy is based on high level control flow graphs capturing activity transitions and constructed by performing static dataflow analysis on apps bytecode.
.
excluded tools and reasons this section describes the android test generation tools that are published in top venues but are not included in our study.
wefurther provide reasons why these android test generation tools are not applicable for the study.
.
.
dynodroid.
dynodroid is a guided random testing toolthatgeneratesuseruieventsandsystem levelevents.byinstrumentingtheandroidos dynodroidcomputesthesetofrelevant events that can execute code of the app under test.
furthermore dynodroid generates more system level events than monkey such as incoming phone calls and geolocation changes.
reason.dynodroidworksononlyemulatorswithandroidos version2.3duetotherequirementofinstrumentingtheandroid platform and the authors of dynodroid publish only the instru mented version for android .
.
very few industrial apps under our study still support such an outdated android system that was released in .
.
.
guiripper.
guiripper isamodel basedtestingtool.
it constructs a finite state machine fsm model of the ui and performsthedepth firstsearch dfs explorationstrategy.tobuild the model guiripper instruments the apk file of the app under test and dynamically analyzes the app ui to obtain relevant events related to ui widgets.
the tool then systematically traverses the appui generatingandexecutingobtainedrelevanteventswhen new states are encountered.reason.wefailtoadaptguirippertorealdevices notethatonly a binary version of guiripper for the windows os is available .
in addition evenonemulators guiripperworksononlyandroid4.
and it fails to process most industrial apps under study.
.
.
swifthand.
swifthand isamodel basedtestingtool.
it features a specialized active learning algorithm to approximate a modeloftheappundertesttoguideexplorationintounexplored parts of the app s state space.
unlike traditional active learning algorithmssuchas l suchdesignminimizesthenumberof restartsduringexploration.swifthandrequirestoinstrumentthe apk file of the app under test to obtain the app ui information during testing.
reason.due to possible implementation defects swifthand fails on most of the industrial apps under study during instrumentation witherrormessagessuchas arrayindexoutofboundsexception orsimplyneverfinishesinstrumentationandproducesgib sized log files.
.
.
acteve.
acteve is a concolic testing t o o lf o r androidapps.byinstrumentingboththeandroidsdkandtheapp undertest actevesymbolicallytrackseventsfromtheoriginating points e.g.
tap coordinates on screen to the code handling theevents.
such approach limits the search space for feasible eventsand avoids generating redundant inputs.
the tool also identifies read only or ineffective events to further reduce the sizes of event sequences.
reason.similar todynodroid acteve works ononly android .3anditrequirestoinstrumentboththeandroidsdkandandroid apps.
study methodology in this section we present our study methodology including the industrial appselection coverage crashmeasurement andstudy setup.
.
industrial app selection wechoosetoobtainindustrialappsfromgoogleplay theofficial android app market by google with huge user base.
we sample multipletop recommendedappswiththehighestnumbersofdown loadsfromeachcategory andmanagetoharvest68industrialapps that are compatible with android .
the most recent version of androidsupportedbymostofthetop recommendedappsandall test generation tools under study.
note that the wechat app is specificallyexcludedduetothefactthatwctester oneofthetools authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an empirical study of android test generation tools in industrial cases ase september montpellier france table overview of industrial apps under study and their applicability on selected test generation tools app name version category install login method activityapplicability m. w. sa.
st. d. a. abs .
.
health fitness 10m accuweather .
.
free weather 50m adobe acrobat .
.
productivity 100m amazon kindle .
.
.
books reference 100m autolist .
.
auto vehicles 1m autoscout24 .
.
auto vehicles 10m best hairstyles .
beauty 1m cnn .
news magazines 10m crackle .
.
entertainment 10m duolingo .
.
education 100m es file explorer .
.
productivity 100m evernote .
productivity 100m excel .
.
productivity 100m facebook .
.
social 1b filters for selfie .
.
beauty 1m flipboard .
.
news magazines 500m floor plan creator .
art design 5m fox news .
.
news magazines 10m g.p.
books .
.
books reference 1b g.p.
music .
.
music audio 1b g.p.
newsstand .
.
news magazines 1b gmail .
.
communication 1b go launcher z .
personalization 100m goodrx .
.
medical 1m google .
.
tools 1b google calendar .
.
business 500m google chrome .
.
communication 1b ibispaint x .
.
art design 10m instagram .
.
social 1b instar .
.
art design 5m line camera .
.
photography 100m marvel comics .
.
comics 5m match .
.
dating 10m mcdonald .
.
food drink 10m merriam webster .
.
books reference 10m messenger .
.
communication 1b mirror beauty 1m my baby piano .
.
parenting 5m nfl .
.
sports 50m nike run club .
.
health fitness 10m nook .
.
.
books reference 10m officesuite .
.
business 100m onenote .
.
business 100m photos .
.
photography 1b pinterest .
.
lifestyle 100m quizlet .
.
education 10m realtor.com .
.
house home 10m sing!
.
.
music audio 100m sketch .
.a.
.
art design 50m speedometer .
auto vehicles 1m spotify .
.
music audio 100m ted .
.
education 10m the weather chnl.
.
.
weather 50m ticketmaster .
.
events 5m translate .
.
tools 500m tripadvisor .
.
food drink 100m trivago .
.
travel local 10m uc browser .
.
communication 500m watchespn .
.
sports 10m wattpad .
.
books reference 100m waze .
.
.
maps navigation 100m webtoon .
.
comics 10m wish .
.
shopping 100m word .
.
productivity 100m yelp .
.
food drink 10m youtube .
.
video player editor 1b zedge .
.
personalization 100m zillow .
.
house home 10m under study is specifically optimized for the app and could potentiallycausebiasintheresult.wealsomanuallyregisteraccounts forappsthatrequireloggingintoaccesstheirmajorfunctionalities.
inaddition appsrequiringspecial sensitiveinformation e.g.
banking or related to real world services e.g.
taxi calling are skipped to minimize undesirable side effects in the study.
table3showsthedetailedinformationofeachselectedindustrial app and its applicability on the selected test generation tools.
install shows the number of installs of the app according to google play.
login denotes whether logging in is required by the app for its majority functionalities to be available.
method indicates the numberofmethodsineachappasreportedbytheinstrumentation tool for which indicates that wedo not instrument the appfor methodcoverage moredetailsareavailableinsection .
.
activity showsthenumberofactivitiesineachappasextractedfrom androidmanifest.xml .
in the availability header section m. w. sa.
st. d. and a. stand for monkey wctester sapienz stoat droidbot anda3e depth first respectively.notethatthe sameabbreviationconventionisusedinsubsequentanalysis.as showninthetable mostoftheselectedappshavemorethan100 million installs while each app has at least million installs.
these apps span across different categories and are popularly used by androiduserseveryday.suchfactorsdistinguishtheseindustrial apps from open source apps which often have only a few users and very limited functionalities.
.
coverage crash measurement for code coverage measurement we use ella to instrument all theindustrialapps andcollectstatisticsofmethodcoverageduring testing.
to avoid potential issues by dual instrumentation i.e.
instrumentation duplicately conducted by both ella and a test gen eration tool under study to collect method coverage we share the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france w. wang d. li w. yang y. cao z. zhang y. deng and t. xie ella collectedmethodcoverageinformationwithtestgeneration tools that need the coverage information during testing instead of lettingthetoolsinstrumenttheappagain.notethatwefocuson coverage of only java code without considering the native code because android apps main functionalities are typically implementedinjava5.inpractice wefindthatellafailstoinstrumentsome large industrial apps under study due to the 64k reference limitation of dalvikvm see section .2for details andsome successfullyinstrumentedappsfailtorunproperlyonandroiddevices due toself protection mechanisms.
toavoid potential biason app selection caused by instrumentation we still keep all these apps in the study without collecting their method coverage information.
table3alsoshowswhethereachappisactuallyinstrumentedinthe experiments as indicated by the method column.
in addition we measureactivitycoveragebyperiodicallymonitoringtheactivity stackonthetestingdeviceandextractingallactivitynamesfrom androidmanifest.xml in each app.
for crash measurement we monitor the logcat on target devicesduringtestingandrecorderrormessagesrelatedtostack traces.
we filter out stack traces that are not related to the app under test by checking whether the app s package name is present.
only unique stack traces are counted achieved by hashing all codelocationsineachstacktrace insteadoftheentirestacktrace which might contain environment related information .
.
study setup we run each test generation tool continuously for hours on each of their applicable industrial apps under study.
note that for stoat we follow the settings described in the tool s corresponding paper byallocating1hourformodelconstructionand2hours formodelevolution.
each test i.e.
acombinationof onetestgenerationtool andoneapplicableapp isrun3times tocompensate potentialinfluencebroughtbyrandomnessduringtesting.alltests of the same app are run on the same device.
for the fairness of comparison when we run each test the tool is allowed to use only onedevice.forappsrequiringloggingintoexposemostoftheir functionalities wechoosetomanually log intotheseappsbefore eachtestbeginsinordertofacilitatein depthtesting notethatthecodecoveragebeforethetestbeginsisnotincludedintheanalysis .
in addition the original implementation of sapienz clears app data beforeevaluatingeachinputsequence revertingtheeffortsofmanual logging in.
in order to set up a normalized testing environment while keeping the tool s original design as much as possible we modify the tool so that it backs up the app data right after manual logging in and later restores the app data instead of clearing them.
weconductourstudyonofficialandroidx86emulatorsand4 realphones allrunningandroid4.
.eachemulatorisconfigured with cpu cores gib of ram and gib of sd card.
for each appunderstudy iftheappsupportsx86devices itistestedona standardemulatoreachtime otherwise itistestedonacertainrealphone.apps dataandmodificationstothesdcardareallreverted aftereachtest.suchdesignservesasanefforttokeepthetesting environmentefficient unified yetversatiletoallowtestingvarious industrial apps.
note that android arm emulators are not useddue to their poor performance which could potentially limit thepower of test generation tools given a bounded amount of time.accordingtoourobservationduringtesting mostx86emulators seldomuseupalldedicatedcpucores indicatingtheirgoodperformance.
also note that we modify each tool s implementation in onlytwosituations adaptingthetooltoourtestingenvironment or dealing with an easy to fix implementation defect that prevents the tool from functioning properly with reference to the tool s corresponding document or paper .
code coverage results on industrial apps in this section we answer rq1 what is the code coverage achieved byeachtestgenerationtoolunderstudyonapplicableindustrialapps by measuring and comparing the method and activity coverage achievedbyeachtestingtoolonindustrialappsinourexperiments.
table4shows the statistics ofmethod and activity coverageon each app achieved by each test generation tool under study after hours of testing.
in a table cellindicates that the corresponding toolisnotapplicableonthecorrespondingindustrialapp dueto instrumentationortoolapplicabilityissues .tablecellswithgray backgroundsindicatethehighestvaluescomparedwithothertools for the same app and coverage type and multiple tools might have the same highest coverage on the same app as shown by multiple tablecellswithgraybackgroundsforthesameappandcoverage type .
all coverage percentage numbers are the averaged values of repetitions and are rounded to the nearest integer.
note that due to the number rounding there might be two tools achieving the samepercentagenumberbutonlyonehavingthegraybackground.
also note that we use the same convention in subsequent analysis.
ascanbeseenfromtable monkeymanagestogainthehighest method coverage on of apps whose method coverage data canbeobtained althoughthetooldoesnotachievemuchhigher methodcoveragecomparedwithothertools especiallysapienz on multiple apps.
sapienz comes after monkey by gaining the highest method coverage on apps while other tools perform the best with regard to method coverage on fewer than three apps.
such findingisdifferentfromtheevaluationresultsonopen sourceappsconductedbytheauthorsofsomeofthesetools.accordingtotheseauthors evaluationresults theyfindthattheirtoolsachievehigher code coverage on more apps compared with monkey.
it can also be seenthatnotoolmanagestocovermorethan50 ofmethodsonany app with the only exception being sapienz on the app floor plan creator .inaddition themajorityofthetoolsachievelessthan30 ofmethodcoverageonmostappsevenafter3hoursoftesting.such findings suggest that there is still much space for improving these toolsonindustrialapps.anotherinterestingfindingisthatanapp s larger code base is not necessarily more difficult to be covered.
for example the app spotify hasover 000methods andsapienz manages to cover of these methods.
however the app google playmusic abbreviatedas g.p.music hasabout23 000methods but none of tools cover more than of these methods.
such resultalsosuggeststhatdifferentindustrialappsmighthavevery different characteristics even under the same category.
the statistics of activity coverage are similar to those of method coverage.monkeygains thehighestactivitycoverageon35ofall apps including ties i.e.
there are apps on which monkey has the same activity coverage as another tool while sapienz authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an empirical study of android test generation tools in industrial cases ase september montpellier france table statistics of code coverage fault detection on industrial apps by test generation tools under study app namemethod coverage activity coverage of unique crashes m. w. sa.
st. d. a. m. w. sa.
st. d. a. m. w. sa.
st. d. a. abs 51300accuweather adobe acrobat amazon kindle autoscout24 autolist best hairstyles cnn crackle duolingo es file explorer evernote excel 01100facebook filters for selfie flipboard 40floor plan creator fox news g.p.
books g.p.
music g.p.
newsstand go launcher z gmail goodrx google 0google calendar google chrome 220instagram line camera 200marvel comics match mcdonald merriam webster messenger 00000mirror my baby piano nfl nook nike run club officesuite onenote 20100photos pinterest quizlet sing!
40sketch speedometer spotify 00000ted the weather chnl.
ticketmaster 3translate tripadvisor uc browser webtoon 210watchespn wattpad waze wish 02200word 00000yelp youtube zedge zillow ibispaint x instar realtor.com trivago gains the highest activity coverage on apps also including ties .
wctester comes after sapienz by having the highest activity coverageon15apps including3ties .suchfindingsuggeststhat wctester might be better at breadth first exploration than at indepth exercising.
it can also be seen that although the overall activity coverage is higher than the method coverage on industrial apps under study many of the apps still have very low activity coverage.
a possible explanation is that many of the apps main functionalitiesareactuallynotreached.thus itmightbehelpful toprioritizeunexploredfunctionalitiesinordertobettersaturate the coverage of industrial apps.
to better understand the tools coverage performance we investigate into each tool s behavior over time during testing.
figures and2showthetrendofaveragemethodandactivitycoverageby each test generation tool under study with regard to the elapsed time during testing.
note that we average the coverage percentagenumbersofdifferentappsinsteadofcountsofmethodsoractivities to avoid imbalanced influence by apps in different sizes.
as shown in figure sapienz almost always has the highest average method coverage although its advantage over monkey becomes smaller astimegoesby.whenitcomestotheactivities asshowninfigure2 monkeyconstantlyhashigheraverageactivitycoveragethan sapienz.thesetwotoolsbothhavemuchhighercoveragethanthe remainingfourtools.thetwotoolsalsogainnewcoveragefaster than all other four tools on average leading to more significant advantages over time.
it can also be seen that a3e depth first abbreviatedas a3e hascomparableorhigheraveragecoveragewith wctester stoat and droidbot at the beginning of testing.
however a3e depth firstalmoststopsgainingnewcoverageafterthat.
according to our observation during testing such result might be caused by the tool s outdated implementation which often causes the tool to hang completely see section 8for more discussion .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france w. wang d. li w. yang y. cao z. zhang y. deng and t. xie .
.
.
.
.
.
.
.
.
.
180coverage elapsed time min monkey wctester sapienz stoat droidbot a3e figure trend of average method coverage of industrial apps achieved by test generation tools under study .
.
.
.
.
.
.
180coverage elapsed time min monkey wctester sapienz stoat droidbot a3e figure trend of average activity coverage of industrial apps achieved by test generation tools under study fault detection results on industrial apps in this section we answer rq2 how many unique crashes can each test generation tool trigger on each applicable industrial app and what are the causes of these crashes by showing the statistics of unique crashes triggered by each testing tool on industrial apps in our experiments.
table4showsthenumberofuniquecrashestriggeredbyeach test generationtool oneach applicableindustrial app understudy.
note that each number reports the total number of unique crashes triggered by the tool on the app after repetitions.
as shown in table4 stoat triggers the highest numbers of unique crashes on apps outperforming all other tools.
sapienz triggers the highest numbersofuniquecrasheson19apps whilemonkeyaccomplishes so on apps.
other three tools trigger the highest numbers of uniquecrashesonfewerthan10apps.also thenumbersofunique triggeredcrasheshavemuchhigherdeviationsacrossdifferenttools for the same app compared with method and activity coverage.
itissomewhatsurprisingtoseethatthefault detectionstatistics differfromthemethodandactivitycoveragestatistics.aimingto understand the differences we manually investigate into a case involving stoat and a case involving sapienz and examine the details of crashes with the findings as below.
stoatontheapp photos .stoathasthehighestnumberofunique crashes on this app.
stoat triggers many nullpointerexception s duringstartingofactivitiesthattakean intent seesection .1for details as input.
meanwhile monkey and other tools trigger other typesofexceptionsincluding arrayindexoutofboundsexceptionandstackoverflowerror .
stoat s triggering these crashes during activitystartingmightbenefitfrominjectingsystem levelevents during testing.
sapienz on the app wattpad .
this combination has much more unique crashes than any other combinations.
we find that sapienz triggersnumerous sqliteexception sonthisappforeachofthe three runs.
the exception causes are mostly about querying onmultiple non existent tables in the app s sqlite database.
as theapp seems to heavily rely on the sqlite database but does notproperly handle related exceptions these fatal sql queries arefrequently triggered from multiple locations of the app causingdifferent stack traces.
none of other tools is able to trigger suchnumber of exceptions during testing.
a possible explanation isthat triggering such crashes requires special preconditions e.g.
forciblyterminatingtheappduringinitialization whichinvolves sqloperationsforcreatingthesetables thatothertoolsmightnot be able to create.
rank analysis on experiment results inthissection inordertoprovideadditionalinsightsforanswering rq3 how to efficiently combine multiple test generation toolson applicable industrial apps to achieve better coverage and fault detection we measure and analyze the statistics of rank method and activity coverage plus rank unique crashes achieved by each test generation tool on industrial apps in our experiments.
we also analyze the results from previous sections to answer rq3.
arank nmethod activityoruniquecrashindicatesthatthereare ntestgenerationtoolsbeingabletocoverthemethod activityor unique crash .
specifically a rank method activity or unique crashindicatesthatonlyonetestgenerationtoolunderstudycovers the method activity or trigger the unique crash in at least one run of our experiments.
for each tool under study the numbers of itscoveredrank 1methods activitiesandtriggeredrank 1unique crashes reflect the tool s unique value to testing an app.
table5shows the statistics of rank methods activities and unique crashes on applicable industrial apps by the test generation tools under study.
a table cell with m n indicates that on the correspondingapp the rank 1methods activitiesoruniquecrashes covered by the corresponding test generation tool account for m percent of covered methods activities or triggered unique crashes by all the six test generation tools and allof the tool s covered methods activities or triggered unique crashes are npercent of coveredmethods activitiesortriggereduniquecrashesbyallthe sixtestgenerationtools.withsuchdefinition weknowthatona specificapp iftestgenerationtoola smethod activityorunique crash statistic is a b and tool b s method activity or unique crash statistic is c d by running both tool a and tool b i.e.
combining toolaandtoolb wecouldachieveatleast max a d b c percent coverage of methods activities or unique crashes that are covered or triggered by all the six test generation tools.
asshownintable formanyindustrialappsunderstudy combining monkey and sapienz facilitates good saturation of covering the app code as they together contribute to over of all covered methods by all the six tools on these apps.
these two tools alsohave the highest numbers of rank covered methods on many authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an empirical study of android test generation tools in industrial cases ase september montpellier france table statistics of rank methods activities and unique crashes on industrial apps by test generation tools under study app name of rank covered methods of rank covered activities of rank unique crashes m. w. sa.
st. d. a. m. w. sa.
st. d. a. m. w. sa.
st. d. a. abs accuweather adobe acrobat amazon kindle autoscout24 autolist best hairstyles cnn crackle duolingo es file explorer evernote excel facebook filters for selfie flipboard floor plan creator fox news g.p.
books g.p.
music g.p.
newsstand go launcher z gmail goodrx google google calendar google chrome instagram line camera marvel comics match mcdonald merriam webster messenger mirror my baby piano nfl nook nike run club officesuite onenote photos pinterest quizlet sing!
sketch speedometer spotify ted the weather chnl.
ticketmaster translate tripadvisor uc browser webtoon watchespn wattpad waze wish word yelp youtube zedge zillow ibispaint x instar realtor.com trivago apps.whenitcomestoactivities combiningmonkeywithsapienz and orstoatseemstobeagoodoptionformostoftheapps because monkeyhasthehighestnumbersofcoveredactivities regardless of ranking on many apps while sapienz and or stoat can be good complements when monkey is not able to cover most activities.
for fault detection combining stoat with sapienz and or monkey seems to be more effective for most of the apps as stoat has the highestnumbersofuniquecrashes regardlessofranking onmanyappswhilesapienzand ormonkeycanbegoodcomplements.such suggestion is consistent with the results of manual investigation from section where we find that stoat and sapienz monkey can triggerverydifferenttypesofcrashes.also accordingtothefact that wctester is designed for wechat the tool might be a good complementwithmonkeywhentheappundertestinvolvessimilarscenariosasthoseinwechat e.g.
chatting social andinformationbrowsing .
rank activity statistics also show hints on this suggestion wctester covers the highest numbers of unique activities on facebook messenger pinterest and tripadvisor .
all these apps share similar usage scenarios with some functionalities of wechat.
experience in applying test generation tools on industrial apps inthissection weanswerrq4 howmucheffortdoesitrequiretoset up each test generation tool for testing industrial apps by reporting our experience on setting up each test generation tool under study and applying them on selected industrial apps.
we additionallyreport our experience with ella and the android framework illustrated in section .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france w. wang d. li w. yang y. cao z. zhang y. deng and t. xie .
test generation tools monkey.
as the built in test generation tool shipped with each androiddevice monkeycanbeinvokeddirectlyusingtheandroid debug bridge shell interface.we spend no efforton setting up monkey for industrial apps under study.
wctester.
due to defects in the uiautomator python wrapper beingused wctesteroftenhaltsduringexplorationand produceserrormessagessuchas rpcservernotconnected .we spend about hours investigating and fixing the defects and after that wctester becomes much more stable.
sapienz.theoriginalimplementationofsapienzsupportsonly emulators.
given that many apps under study include native libraries compiled against only arm processors we modify the tool s implementation to add support for real devices.
since the toolistestedononlyandroid4.4andneedstoinstall motifcore tothesystempartition formaximumcompatibility wedowngrade all real devices to android .
and acquire the root privileges onall of them.
we also modify the tool s implementation so that itrestores the app data to the point right after manual logging ininstead of clearing them.
finally we spend more than hours getting sapienz to work in our settings.
stoat.
theoriginal implementation ofstoat has multipleissues withourtestinginfrastructure.forexample itforciblykillsalljava and adb processes on the underlying computer to clean up the environment unexpectedlyterminatingourtoolsformonitoring thetesting.
stoatalso usestheproblematic uiautomator python wrapper.overall wespendabout10hoursinvestigatingandfixing the implementation of stoat.
droidbot.droidbotneedstorunitsclientappundertheaccessibility mode which requires granting the privilege manually in androidsystemsettings.
we alsosometimesencountererrormessages such as please enable droidbot manually in accessibility settings evenifthetoolworksinpreviousruns.overall wespend about hours writing a script to mitigate this issue.
a3e depth first.a3e depth first has several issues in the implementation such as not being able to click buttons with labels containing special characters.
due to the outdated implementation andtheneedofrunningthetargetappunderitsinstrumentation the tool causes many apps to crash at beginning preventing them frombeingtested.italsohangsduringexplorationforunknown reasons even after we try to fix this issue.
we spend about hours trying to fix the issues for the tool.
note that we have already submitted bug reports on most of theprecedingpatchestotheseexistingtoolsfortheoriginaltool authorstoimprovethequalityandrobustnessofthesetools.additionally due to the fact that some tool issues appear only after the experiments have lasted for some time it takes a lot of manual effortstoinspecttheexperimentresultstofindoutsuchissues and the wasted time of running these experiments with these issues still existing in the tools adds up to tens of hours.
.
ella and the android framework ella.ellahasmultipleimplementationissuesindifferentmodules.
in addition the tool s original implementation does not supportinstrumenting apps with multidex enabled which is commonlyused by large industrial apps.
we spend more than hours fixing the issues and adding multidex support to ella.
androidframework.weevenencounteranissueinthesystem framework on android .
.
specifically the issue in the uiautomatorframeworkcausestheservicetostopworkingwhenthereisanyspecialcharacter e.g.
anemojiicon onthescreen.wefixtheissuebymodifyingthecorrespondingandroidsourcecodeplusrecompil ingandreplacingtheuiautomatorframework uiautomator.jar .
we spend about hours addressing this issue.
threats of validity themainthreattoexternalvalidityistherepresentativenessofthe studiedsubjects i.e.
thedegreetowhichthestudiedindustrialapps and tools arerepresentative of true practice .
ourcurrent tool set containsonlysixtestgenerationtoolsduetonotbeingabletoapply other test generation tools on most industrial apps under study.
however thesesixtoolsarestate of the artonesthatarealready comparedwithmorestate of the art orstate of the practicetools suchasmonkey whichispopularlyusedinindustry.thesethreats could be reduced by more experiments on wider types of subjects in future work.
the threats to internal validity are instrumentation effects that can bias our results.
issues in ella s handling of the apps binarycode faults in our modification of the existing tools or in our experiment scripts etc.
might cause such effects.
to reduce these threats we manually inspect traces of our experiments for sample apps.
in addition we are not able to obtain method coverage for abouthalfoftheindustrialappsunderstudyduetoella sfailing to instrument these apps or these apps not running normally after instrumentation.
we also try coverage collection tools based on soot and they simply fail or cause problems on more apps.
we arenotawareofothertoolsthatcanflawlesslyinstrumentthese large complex and closed source apps.
also it might cause bias to the selection of apps if we simply discard these apps that fail to be instrumented.
conclusion in this paper we have presented an empirical study of existing android test generation tools applicability on industrial apps.
we directly comparethe tools with regardto code coverageand faultdetectionability.byanalyzingthestudyresults weprovidesugges tionsforcombiningdifferenttestgenerationtoolstoachievebetterperformance.wealsoreportourexperienceinapplyingthesetools toindustrialappsunderstudy.ourstudyresultsgiveinsightson howandroiduitestgenerationtoolscouldbeimprovedtobetter handle industrial apps.
ourstudyresultsofferastrongimplicationthattestingresearchers for android test generation tools should empirically compare a newly proposed tool with related previous tools on industrial apps besidesopen source apps going beyondthe current common research practice of comparing tools on onlyopen source apps.