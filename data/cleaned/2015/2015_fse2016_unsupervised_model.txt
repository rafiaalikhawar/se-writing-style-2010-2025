effort aware just in time defect prediction simple unsupervised models could be better than supervised models yibiao y ang1 yuming zhou1 jinping liu1 y angyang zhao1 hongmin lu1 lei xu1 baowen xu1 and hareton leung2 1department of computer science and technology nanjing university china 2department of computing hong kong polytechnic university hong kong china abstract unsupervised models do not require the defect data to build the prediction models and hence incur a low building cost and gain a wide application range.
consequently it would be more desirable for practitioners to apply unsupervised models in e ort aware just in time jit defect prediction if they can predict defect inducing changes well.
however little is currently known on their prediction e ectiveness in this context.
we aim to investigate the predictive power of simple unsupervised models in e ort aware jit defect prediction especially compared with the state of the art supervised models in the recent literature.
we rst use the most commonly used change metrics to build simple unsupervised models.
then we compare these unsupervised models with the state of the art supervised models under cross validation time wise cross validation and across project prediction settings to determine whether they are of practical value.
the experimental results from open source software systems show that many simple unsupervised models perform better than the state of the art supervised models in e ort aware jit defect prediction.
ccs concepts software and its engineering !risk management software development process management keywords defect prediction changes just in time e ort aware .
introduction recent years have seen an increasing interest in just in time jit defect prediction as it enables developers to identify defect inducing changes at check in time .
a defectinducing change is a software change i.e.
a single or several corresponding author zhouyuming nju.edu.cn.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa c acm.
isbn .
.
.
.
commits in a given period of time that introduce one or several defects into the source code in a software system .
compared with traditional defect prediction at module e.g.
package le or class level jit defect prediction is a ne granularity defect prediction.
as stated by kamei et al.
it allows developers to inspect an order of magnitude smaller number of sloc source lines of code to nd latent defects.
this could provide large savings in e ort over traditional coarser granularity defect predictions.
in particular jit defect prediction can be performed at check in time .
this allows developers to inspect the code changes for nding the latent defects when the change details are still fresh in their minds.
as a result it is possible to nd the latent defects faster.
furthermore compared with conventional non e ort aware defect prediction e ort aware jit defect prediction takes into account the e ort required to inspect the modi ed code for a change .
consequently e ort aware jit defect prediction would be more practical for practitioners as it enables them to nd more latent defects per unit code inspection e ort.
currently there is a signi cant strand of interest in developing e ective e ortaware jit defect prediction models .
kamei et al.
leveraged supervised method i.e.
the linear regression method to build an e ort aware jit defect prediction model.
to the best of our knowledge this is the rst time to introduce e ort aware concept into jit defect prediction.
their results showed that the proposed supervised model was e ective in e ort aware performance evaluation compared with the random model.
this work is signi cant as it could help nd more defect inducing changes per unit code inspection e ort.
in practice however it is often time consuming and expensive to collect the defect data used as the dependent variable to build supervised models.
furthermore for many new projects the defect data are unavailable in which supervised models are not applicable.
di erent from supervised models unsupervised models do not need the defect data to build the defect prediction models.
therefore for practitioners it would be more desirable to apply unsupervised models if they can predict defects well.
according to recent studies simple unsupervised models such as the manualup model in which modules are prioritized in ascending order according to code size are e ective in the context of e ort aware defect prediction at coarser granularity.
up till now however little is known on the practical value of simple unsupervised models in the context of e ort aware jit defect prediction.
the main contributions of this paper are as follows permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa c acm.
... .
we investigate the predictive e ectiveness of unsupervised models in e ort aware jit defect prediction which is an important topic in the area of defect prediction.
we perform an in depth evaluation on the simple unsupervised techniques i.e.
the unsupervised models in section .
under three prediction settings i.e.
cross validation time wise cross validation and across project prediction .
we compare simple unsupervised models with the stateof the art supervised models in recent literature .
the experimental results show that many simple unsupervised models perform signi cantly better than the state of the art supervised models.
the rest of this paper is organized as follows.
section introduces the background on defect prediction.
section describes the employed experimental methodology.
section provides the experimental setup in our study including the subject projects and the data sets.
section reports in detail the experimental results.
section examines the threats to validity of our study.
section concludes the paper and outlines directions for future work.
.
background in this section we rst introduce the background on just intime defect prediction and then e ort aware defect prediction.
finally we describe the existing work on the application of unsupervised models in traditional defect prediction.
.
just in time defect prediction the origin of jit just in time defect prediction can be traced back to mockus and weiss who used a number of change metrics to predict the probability of changes to be defect inducing changes.
for practitioners jit defect prediction is of more practical value compared with traditional defect predictions at module e.g.
le or class level.
as stated by kamei et al.
the reason is not only because the predictions can considerably narrow down the code to be inspected for nding the latent defects but also because the predictions can be made at check in time when the change details are still fresh in the minds of the developers.
in particular in traditional defect predictions after a module is predicted as defect prone it may be di cult to nd the speci c developer who is most familiar with the code to inspect the module to nd the latent defects.
however in jit defect predictions it is easy to nd such a developer to inspect the predicted defect prone change as each change is associated with a particular developer.
in the last decade mockus and weis ss study led to an increasing interest in jit defect predictions.
sliwerski et al.
studied defect inducing changes in two open source software systems and found that the changes committed on friday had a higher probability to be defect inducing changes.
eyolfson et al.
studied the in uence of committed time and developer experience on the existence of defects in a software change.
their results showed that changes committed between midnight and 4am were more likely to be defect inducing than the changes committed between 7am and noon.
yin et al.
studied the bug xing changes in several open source software systems.
they found that around .
to .
bug xing changes for post release bugs were defect inducing and the concurrency defects were the most di cult to be xed.
kim et al.
used numerous features extracted from various sources such as change metadata source code and change log messages to buildprediction models to predict defect inducing changes.
their results showed that defect inducing changes can be predicted at recall and precision on average.
.
effort aware defect prediction although the above mentioned results were encouraging they did not take into account the e ort required for quality assurance when applying the jit defect prediction models in practice.
arisholm et al.
pointed out that when locating defects it was important to take into account the cost e ectiveness of using defect prediction models to focus on veri cation and validation activities.
this viewpoint has been recently taken by many module level defect prediction studies .
inspired by this viewpoint kamei et al.
applied e ort aware evaluation to jit defect predictions .
in their study kamei et al.
used the total number of lines modi ed by a change as a measure of the e ort required to inspect a change.
in particular they leveraged linear regression method to build the e ort aware jit defect prediction model called the ealr model .
in their model the dependent variable was y x effort x where effort x was the e ort required for inspecting the change xandy x was ifxwas defect inducing and otherwise.
the results showed that on average the ealr model can detect of all defect inducing changes when using of the e ort required to inspect all changes.
as such kamei et al.
believed that e ort aware jit defect prediction was able to focus on the most risky changes and hence could reduce the costs of developing high quality software .
.
unsupervised models in traditional defect prediction in the last decades supervised models have been the dominant defect prediction paradigm in traditional defect prediction at module e.g.
le or class level .
in order to build a supervised model we need to collect the defect data such as the number of defects or the labeled information buggy or not buggy for each module.
for practitioners it may be expensive to apply such supervised models in practice.
the reason for this is that it is generally time consuming and costly to collect the defect data.
furthermore supervised defect prediction models cannot be built if the defect data are unavailable.
this is especially true when a new type of projects is developed or when historical defect data have not been collected.
compared with supervised models unsupervised models do not need the defect data.
due to this advantage recent years have seen an increasing e ort devoted to apply unsupervised modeling techniques to build defect prediction models .
in practice however it is more important to know the e ort aware prediction performance of unsupervised defect prediction models.
to this end many researchers investigate whether unsupervised models are still e ective when taking into account the e ort to inspect the modules that are predicted as defect prone .
koru et al.
suggested that smaller modules should be inspected rst as more defects would be detected per unit code inspection e ort.
the reason was that the relationship between module size and the number of defects was found not linear but logarithmic indicating that defect proneness increased at a slower rate as module size increased.
menzies et al.
used the manualup model to name the smaller modules inspected rst strategy by koru et al.
in their experiments 158menzies et al.
did nd that the manualup model had a good e ort aware prediction performance.
their results were further con rmed by zhou et al.
s study in which the manualup model was found even competitive to the regular supervised logistic regression model.
all these studies show that in traditional defect prediction unsupervised models perform well under e ort aware evaluation.
.
rresearch methodolody in this section we rst introduce the investigated independent and dependent variables.
then we describe the simple unsupervised models under study and present the supervised models which will be used as the baseline models against.
next we give the research questions.
after that we provide the performance indicators for evaluating the e ectiveness of defect prediction models in e ort aware jit defect prediction.
finally we give the data analysis method used in this study.
.
dependent and independent variables the dependent variable in this study is a binary variable.
if a code change is a defect inducing change the dependent variable is set to and otherwise.
table summarization of change metrics metric description ns number of subsystems touched by the current change nd number of directories touched by the current change nf number of les touched by the current change entropydistribution across the touched les i.e.
n k 1pklog2pk where n is the number of les touched by the change and pk is the ratio of the touched code in the k th le to the total touched code la lines of code added by the current change ld lines of code deleted by the current change lt lines of code in a le before the current change fix whether or not the current change is a defect x ndev the number of developers that changed the les age the average time interval in days between the last and the change over the les that are touched nuc the number of unique last changes to the les exp developers experience i.e.
the number of changes rexp recent developer experience i.e.
the total experience of the developer in terms of changes weighted by their age sexp developer experience on a subsystem i.e.
the number of changes the developer made in the past to the subsystems the independent variables used in this study consist of fourteen change metrics.
table summarizes these change metrics including the metric name the description and the source.
these fourteen metrics can be classi ed into the following ve dimensions di usion size purpose history and experience.
the di usion dimension consists of ns nd nf and entropy which characterize the distribution of a change.
as stated by kamei et al.
it is believed that a highly distributed change is more likely to be a defectinducing change.
the size dimension leverages la ld and lt to characterize the size of a change in which a larger change is expected to have a higher likelihood of being a defect inducing change .
the purpose dimension consists of only fix.
in the literature there is a belief that a defect xing change is more likely to introduce a new defect .
the history dimension consists of ndev age and nuc.
it is believed that a defect is more likely to be introduced by a change if the touched les have been modi ed by more developers by more recent changes or by more unique last changes .
the experience dimension consists of exp rexp and sexp in which the experienceof the developer of a change is expected to have a negative correlation to the likelihood of introducing a defect into the code by the change.
in other words if the current change is made by a more experienced developer it is less likely that a defect will be introduced.
note that all these change metrics are the same as those used in kamei et al.
s study.
.
simple unsupervised models in this study we leverage change metrics to build simple unsupervised models.
as stated by monden et al.
to adopt defect prediction models one needs to consider not only their prediction e ectiveness but also the signi cant cost required for metrics collection and modeling themselves.
a recent investigation from google developers further shows that a prerequisite for deploying a defect prediction model in a large company such as google is that it must be able to scale to large source repositories .
therefore we only take into account those unsupervised defect prediction models that have a low application cost including metrics collection cost and modeling cost and a good scalability.
more speci cally our study will investigate the following unsupervised defect prediction models.
for each of the change metrics except la and ld we build an unsupervised model that ranks changes in descendant order according to the reciprocal of their corresponding raw metric values.
this idea is inspired by koru and menzies et al.
s nding that smaller modules are proportionally more defect prone and hence should be inspected rst .
in our study we expect that smaller changes tend to be more proportionally defect prone.
more formally for each change metric m the corresponding model isr c m c .
here crepresents a change and ris the predicted risk value.
for a given system the changes will be ranked in descendant order according to the predicted risk valuer.
in this context changes with smaller change metric values will be ranked higher.
note that under each of the above mentioned simple unsupervised models it is possible that two changes have the same predicted risk values i.e.
they have a tied rank.
in our study if there is a tied rank according to the predicted risk values the change with a lower defect density will be ranked higher.
furthermore if there is still a tied rank according to the defect densities the change with a larger change size will be ranked higher.
in this way we will obtain simple unsupervised models that have the worst predictive performance theoretically in e ort aware just in time defect prediction .
in our study we investigate the predictive power of those worst simple unsupervised models.
if our experimental results show that those worst simple unsupervised models are competitive to the supervised models we will have con dence that simple unsupervised models are of practical value for practitioners in e ort aware just in time defect prediction.
as can be seen there are simple unsupervised models which involve a low application cost and can be e ciently applied to large source repositories.
.
the supervised models the supervised models are summarized in table .
these supervised models are categorized into six groups function lazy rule bayes tree and ensemble .
the supervised models in the function group are the regression models and the neural networks.
lazy are the supervised models based on lazy learning.
rule and tree respec159tively represent the rule based and the decision tree based supervised models.
ensemble are those supervised ensemble models which are built with multiple base leaners.
the naive bayes is a probability based technique.
in kamei et al.
using the linear regression model to build the e ortaware jit defect prediction model i.e.
the ealr model .
the ealr model is the state of the art supervised model in e ort aware jit defect prediction.
besides we also include other supervised techniques i.e.
the models in table except the ealr model as the baseline models.
the reasons are two folds.
first they are the most commonly used supervised techniques in defect prediction studies .
second a recent literature using most of them except for the random forest to revisit their impact on the performance of defect prediction.
table overview of the supervised models family model abbreviation function linear regression ealr simple logistic sl radial basis functions rbfnet network sequential minimal smo optimization lazy k nearest neighbour ibk rule propositional rule jrip ripple down rules ridor bayes na ve bayes nb tree j48 j48 logistic model tree lmt random forest rf ensemble bagging bg lmt bg nb bg sl bg smo and bg j48 adaboost ab lmt ab nb ab sl ab smo and ab j48 rotation forest rf lmt rf nb rf sl rf smo and rf j48 random subspace rs lmt rs nb rs sl rs smo and rs j48 in this study we use the same method as kamei et al.
to build the ealr model.
as stated in section .
y x e ort x was used as the dependent variable in the ealr model.
for the other supervised models we use the same method as ghotra et al.
.
more speci cally y x was used as the dependent variable for these supervised models.
consistent with ghotra et al.
we use the same parameters to build these supervised models.
for example the k nearest neighbor requires k most similar training example for classifying an instance.
in ghotra et al.
found that k performed best than other options i.e.
and .
as such we also use k to build the k nearest neighbor.
in ealr model kamei et al.
used the under sampling method to deal with the imbalanced data set and then removed the most highly correlated factors to deal with collinearity.
in consistence with kamei et al.
s study we use exactly the same method to deal with imbalanced data set and collinearity.
.
research questions we investigate the following three research questions to determine the practical value of simple unsupervised models rq1 how well do simple unsupervised models predict defect inducing changes when compared with the state of theart supervised models in cross validation?
rq2 how well do simple unsupervised models predict defect inducing changes when compared with the state of thefigure code churn based alberg diagram art supervised models in time wise cross validation?
rq3 how well do simple unsupervised models predict defect inducing changes when compared with the state of theart supervised models in across project prediction?
the purposes of rq1 rq2 andrq3are to compare simple unsupervised models with the state of the art supervised models with respect to three di erent prediction settings i.e.
the cross validation time wise cross validation and across project prediction to determine how well they predict defect inducing changes.
since unsupervised models do not leverage the buggy or not buggy label information to build the prediction models they are not expected to perform better than the supervised models.
however if unsupervised models are not much worse than the supervised models it is still a good choice for practitioners to apply them because they have a lower building cost a wider application range and a higher e ciency.
to the best of our knowledge little is currently known on these research questions from the viewpoint of unsupervised models in the literature.
our study attempts to ll this gap by an in depth investigation into simple unsupervised models in the context of e ort aware jit defect prediction.
.
performance indicators when evaluating the predictive e ectiveness of a jit defect prediction model we take into account the e ort required to inspect those changes predicted as defect prone to nd whether they are defect inducing changes.
consistent with kamei et al.
we use the code churn i.e.
the total number of lines added and deleted by a change as a proxy of the e ort required to inspect the change.
in kamei et al.
usedacc andpoptto evaluate the e ort aware performance for the ealr model.
acc denotes the recall of defectinducing changes when using of the entire e ort required to inspect all changes to inspect the top ranked changes.
popt is the normalized version of the e ort aware performance indicator originally introduced by mende and koschke .
thepoptis based on the concept of the code churn based alberg diagram.
figure is an example code churn based alberg diagram showing the performances of a prediction model m. in this diagram the x axis and y axis are respectively the cumulative percentage of code churn of the changes i.e.
the percentage of e ort and the cumulative percentage of defect inducing changes found in selected changes.
to compute popt two additional curves are included the optimal model and the worst model.
in the optimal model and the worst model changes are respectively sorted in decreasing and ascending order according to their actual defect densities.
according to poptcan be formally de ned as popt m area optimal area m area optimal area worst 160here area optimal and area worst is the area under the curve corresponding to the best and the worst model respectively.
note that both acc andpoptare applicable to supervised models as well as unsupervised models.
.
data analysis method figure provides an overview of our data analysis method.
as can be seen in order to obtain an adequate and realistic assessment we examine the three rqs under the following three prediction settings times fold cross validation time wise cross validation and across project prediction.
times fold cross validation is performed within the same project.
at each fold cross validation we rst randomize the data set.
then we divide the data set into parts of approximately equal size.
after that each part is used as a testing data set to evaluate the e ectiveness of the prediction model built on the remainder of the data set i.e.
the training data set .
the entire process is then repeated times to alleviate possible sampling bias in random splits.
consequently each model has prediction e ectiveness values.
time wise cross validation is also performed within the same project in which the chronological order of changes is considered.
this is the method followed in .
for each project we rst rank the changes in chronological order according to the commit date.
then all changes committed within the same month are grouped into the same part.
assume the changes in a project are grouped into n parts we use the following approach for the time wise prediction.
we build a prediction model m on the combination of part i and part i and then apply m to predict changes in part i and i i n .
as such each training set and test set will have changes committed with two consecutive months.
the reasons for this setting are four fold.
first the release cycle of most projects is typically 8weeks .
second it can make sure that each training set and test set will have a gap of two months.
third two consecutive months can make sure that each training set will have enough instances which is important for supervised models.
forth it allows us to have enough runs for each project.
if a project has changes of n months this method will produce n prediction e ectiveness values for each model.
across project prediction is performed across di erent projects.
we use a model trained on one project i.e.
the training data set to predict defect proneness in another project i.e.
the testing data set .
given nprojects this method will produce n n prediction e ectiveness values for each model.
in this study we use six projects as the subject projects.
therefore each prediction model will produce prediction e ectiveness values.
note that the unsupervised models only use the change metrics in testing data to build the prediction models.
in this study we also apply cross validation time wise crossvalidation and across project prediction settings to the unsupervised models.
this allows the unsupervised models to use the same testing data as those supervised models thus making a fair comparison on their prediction performance.
when investigating rq1 rq2 andrq3 we use the bh corrected p values from the wilcoxon signed rank test to examine whether there is a signi cant di erence in the prediction e ectiveness between the unsupervised and supervised models.
in particular we use the benjamini hochberg bh corrected p values to examine whether a di erence is statis tically signi cant at the signi cance level of .
.
if the statistical test shows a signi cant di erence we then use the cli s to examine whether the magnitude of the di erence is practically important from the viewpoint of practical application .
by convention the magnitude of the di erence is considered trivial j j small j j moderate j j or large .
furthermore similar to ghotra et al.
we use scottknott test to group the supervised and unsupervised prediction models to examine whether there exist some models outperform others.
the scott knott test uses hierarchical cluster analysis method to divide the prediction models into two groups according to the mean performance i.e.
the popt and theacc with respect to di erent runs for each model .
if the di erence between the divided groups is statistically signi cant scott knott will recursively divide each group into two di erent groups.
the test will terminate when groups can no longer divided into statistically distinct groups.
.
experimental setup in this section we rst introduce the subject projects and then describe the data sets collected from these projects.
.
subject projects in this study we use the same open source subject projects as used in kamei et al.
s study .
more speci cally we use the following six projects to investigate the predictive power of simple unsupervised models in e ort aware jit defect prediction bugzilla bug columba col eclipse jdt jdt eclipse platform pla mozilla moz and postgresql pos .
bugzilla is a well known web based bug tracking system.
columba is a powerful mail management tool.
eclipse jdt is the eclipse java development tools which is a set of plug ins that add the capabilities of a full featured java ide to the eclipse platform.
mozilla is a well known and widely used open source web browser.
postgresql is a powerful open source object relational database system.
as stated by kamei et al.
these six projects are large well known and long lived projects which cover a wide range of domains and sizes.
in this sense it is appropriate to use these projects to investigate simple unsupervised models in jit defect prediction.
.
data sets the data sets from these six projects used in this study are shared by kamei et al.
and are available online.
as mentioned by kamei et al.
these data are gathered by combining the change information mined from the cvs repositories of these projects with the corresponding bug reports.
more speci cally the data for bugzilla and mozilla were gathered from the data provided by msr mining challenge.
the data for the eclipse jdt and platform were gathered from the data provided by the msr mining challenge.
the data for columba and postgresql were gathered from the o cial cvs repository.
table summarizes the six data sets used in this study.
the rst column and the second column are respectively the subject data set name and the period of time for collecting the changes.
the third to the sixth columns respectively report the total number of changes the percentage of defectinducing changes the average loc per change and the number of les modi ed in a code change.
as can be seen for each data set defects concentrated in a small percentage times fold cross validation dataset across project predictiontime wise cross validation supervised model unsupervised modelordered dataset n months supervised model unsupervised modelrandomized dataset folds supervised model unsupervised modelevaluate evaluatei from to i from to n evaluate evaluate evaluate evaluate...fold fold n dataset...fold fold ntrain all folds except i test fold i train fold i and i test fold i and i train project i test project j figure overview of the three prediction settings table summarization of studied data sets defect mean modi ed inducing loc per les per project period change change change change bug .
.
col .
.
jdt .
.
pla .
.
moz .
.
pos .
.
of changes around of all changes .
.
experimental results in this section we report the experimental results.
.
times fold cross validation figure and figure respectively employ the box plot to describe the distributions of poptandacc obtained from times fold cross validation for the supervised models and simple unsupervised models for the overall result over the six data sets.
for each model the box plot shows the median the horizontal line within the box the 25th percentile the lower side of the box and the 75th percentile the upper side of the box .
in figure and figure the horizontal dotted lines respectively represent the median performance of the best supervised model.
in particular there are blue red and black box plots.
a blue box indicates that the corresponding simple model performs signi cantly better than the best supervised model according to the wilcoxon signed rank test i.e.
the bh corrected p value is less than .
and the magnitude of the di erence between the corresponding simple model and the best supervised model is not trivial according to cli s i.e.j j .
a red box indicates that the corresponding simple model performs signi cantly worse than the best supervised model and the magnitude of the di erence between the corresponding simple model and the best supervised model is not trivial.
a black box indicates that the di erence between thecorresponding simple model and the best supervised model is not signi cant or the magnitude of the di erence between the corresponding simple model and the best supervised model is trivial.
from figure and figure we have the following observations.
first according to popt the best supervised model is the ealr model which performs signi cantly better than all the other supervised models.
however the nd exp rexp sexp unsupervised models have a performance similar to the ealr model and the nf entropy lt ndev age nuc unsupervised models perform signi cantly better than the ealr model.
second according to acc the best supervised model is also the ealr model which performs signi cantly better than the other supervised models except the rbfn model.
however the ndev nuc unsupervised models perform similar to the ealr model and the nf entropy lt age unsupervised models perform signi cantly better than the ealr model.
figure and figure respectively present the results from scott knott test.
in figure and figure the y axis is the average performance.
the blue labels indicate simple unsupervised models.
the dotted lines represent groups divided by the scott knott test.
all models are ordered by their mean ranks over the six di erent projects.
as can be seen all models in the rst group are the unsupervised models.
the best supervised model i.e.
the ealr model is in the second group.
this indicates that those unsupervised models in the rst group signi cantly outperform the best supervised model.
table a and table b respectively summarize the medianpoptandacc for the best supervised model i.e.
the ealr model and the best four simple unsupervised models obtained from times fold cross validation.
in each table for each simple unsupervised model we show how often it performs signi cantly better denoted by p or worse denoted by than the best supervised model by the wilcoxon s signed rank test.
the row avg reports the average median over the six data sets.
the row w t l reports 162figure times fold cross validation performance comparison in terms of popt figure times fold cross validation performance comparison in terms of acc figure scott knott test for times fold cross validation in terms of popt figure scott knott test for times fold cross validation in terms of acc the number of data sets for which the simple unsupervised model obtains a better equal and worse performance than the best supervised model.
in particular an entry in deep gray background indicates a large improvement and an entry in light gray background indicates a moderate improvement in terms of the cli s .
table times fold cross validation best supervised model vs. unsupervised models rq1 a popt ealr nf nuc lt age bug .
.
.
.
.748p col .
.811p0.822p0.842p0.856p jdt .
.716p0.679p0.781p0.747p pla .
.698p0.640p0.748p0.707p moz .
.614p0.552p0.644p0.621p pos .
.727p0.714p0.805p0.768p avg .
.
.
.
.
w t l b acc ealr nf entropy lt age bug .
.
.
.
.
col .
.
.608p0.626p0.687p jdt .
.370p0.445p0.523p0.476p pla .
.369p0.470p0.459p0.385p moz .
.213p0.326p0.375p0.244p pos .
.
.
.519p0.464p avg .
.
.
.
.
w t l from table we have the following observations.
first according to popt the best four simple unsupervised models have average median poptranging from .
to .
thus exhibiting a to improvement over the ealr model average median popt .
in particular the age unsupervised model performs signi cantly better than the ealr model in all six data sets and the magnitudes of the di erence are large in ve data sets.
second according to acc the best four simple unsupervised models have average medianacc ranging from .
to .
thus exhibitinga to improvement over the ealr model average medianacc .
.
in other words when using the same e ort to inspect the changes these simple unsupervised models can detect to more defect inducing changes than the best supervised model.
overall the above observations suggest that simple unsupervised models could perform better when compared with the state of the art supervised models in e ort aware jit defect prediction under times fold cross validation.
.
time wise cross validation figure and figure respectively employ the box plot to describe the distributions of the poptand theacc obtained from time wise cross validation.
from figure and figure we have the following observations.
first according to popt the best supervised model in the time wise cross validation is the ealr model which outperforms all the other supervised models.
however the entropy ndev sexp unsupervised models have a performance similar to the ealr model and the nf lt age nuc unsupervised models perform signi cantly better than the ealr model.
second according to acc again the best supervised model is the ealr model.
however the nf entropy ndev rexp sexp models have a performance similar to the ealr model and the lt age unsupervised models perform signi cantly better than the ealr model.
figure and figure respectively present the results from the scott knott test for the supervised and unsupervised models with respect to poptandacc obtained from time wise cross validation.
from figure we can see that both the rst group and second group consist of two simple unsupervised models.
the best supervised model i.e.
the ealr model is in the third group in which six unsupervised models are also included.
this indicates that many simple unsupervised models are similarly to or even better than the best supervised model in terms of popt.
from figure we can see that that the rst group consists of two simple unsu163figure time wise cross validation performance comparison in terms of popt figure time wise cross validation performance comparison in terms of acc figure scott knott test under time wise cross validation in terms of popt figure scott knott test under time wise cross validation in terms of acc pervised models.
the best supervised model i.e.
the ealr model is in the second group in which seven unsupervised models are also included.
this indicates that many simple unsupervised models are similarly to or even better than the best supervised model in terms of acc .
table time wise cross validation best supervised model vs. unsupervised models rq2 a popt b acc ealr lt age ealr lt age bug .
.721p0.661p0.
.449p0.375p col .
.732p0.786p0.
.440p0.568p jdt .
.709p0.685p0.
.452p0.408p pla .
.717p0.709p0.
.432p0.429p moz .
.651p0.638p0.
.363p0.280p pos .
.742p0.731p0.
.432p0.426p avg .
.
.
.
.
.
w t l table a and table b respectively summarize the medianpoptandacc for the best supervised model i.e.
the ealr model and the best two simple unsupervised models obtained from time wise cross validation.
as can be seen the lt age unsupervised models perform signi cantly better than the ealr model in all six data sets regardless of whetherpoptoracc is considered.
as indicated by the cells in grey background there is a moderate to large improvement in most cases in terms of the cli s .
according to popt the best two simple unsupervised models exhibit more than improvement over the ealr model average median popt .
.
according to acc the best two simple unsupervised models exhibit more than improvement over the ealr model average median acc .
.
in other words when using the e ort to inspect all the changes these simple unsupervised models can detect more defect inducing changes than the ealr model.
overall the above observations suggest that simple unsupervised models could perform better when compared with the state of the art supervised models in e ort aware jitdefect prediction under time wise cross validation.
.
across project prediction figure and figure respectively employ the box plot to describe the distributions of poptandacc obtained from across project prediction for the supervised models and simple unsupervised models over the six data sets.
from figure and figure we have the following observations.
first according to popt the earl model performs signi cantly better than all the other supervised models.
however the nd exp rexp unsupervised models have a performance similar to the ealr model and the nf entropy lt ndev age nuc sexp unsupervised models perform signi cantly better than the ealr model.
second according to acc the ealr model is also perform signi cantly than all the other supervised models.
however the ndev nuc exp rexp sexp unsupervised models have a performance similar to the ealr model and the nf entropy lt age unsupervised models perform signi cantly better than the ealr model.
figure and figure respectively present the results from scott knott test for the supervised and unsupervised models with respect to poptandacc obtained from acrossproject prediction.
from figure we can see that all the four models in the rst group are simple unsupervised models.
the best supervised model i.e.
the ealr model is in the second group in which six simple unsupervised models are also included.
this indicates that many simple unsupervised models are similarly to or even better than the best supervised model in terms of popt.
from figure we can see that the models in the rst second and third groups are all simple unsupervised models.
the best supervised model is in the fourth group in which four simple unsupervised models are also included.
this indicates that many simple unsupervised models are similarly to or even better than the best supervised model in terms of acc .
table shows the poptandacc for the best supervised 164figure across project prediction performance comparison in terms of popt figure across project prediction performance comparison in terms of acc figure scott knott test under across project prediction in terms of popt figure scott knott test under across project prediction in terms of acc model i.e.
the ealr model and the best two simple unsupervised models obtained from across project prediction.
the rst and the second columns are respectively the training project and the testing project.
the row avg reports for each prediction model the average poptandacc .
table across project prediction best supervised vs. unsupervised models rq3 train testealr lt age poptacc p optacc p opt acc bug col .
.
.
.
.
.
jdt .
.
.
.
.
.
pla .
.
.
.
.
.
moz .
.
.
.
.
.
pos .
.
.
.
.
.
col bug .
.
.
.
.
.
jdt .
.
.
.
.
.
pla .
.
.
.
.
.
moz .
.
.
.
.
.
pos .
.
.
.
.
.
jdt bug .
.
.
.
.
.
col .
.
.
.
.
.
pla .
.
.
.
.
.
moz .
.
.
.
.
.
pos .
.
.
.
.
.
pla bug .
.
.
.
.
.
col .
.
.
.
.
.
jdt .
.
.
.
.
.
moz .
.
.
.
.
.
pos .
.
.
.
.
.
moz bug .
.
.
.
.
.
col .
.
.
.
.
.
jdt .
.
.
.
.
.
pla .
.
.
.
.
.
pos .
.
.
.
.
.
pos bug .
.
.
.
.
.
col .
.
.
.
.
.
jdt .
.
.
.
.
.
pla .
.
.
.
.
.
moz .
.
.
.
.
.
avg .
.
.
.
.
.
from table when comparing the simple unsupervised models against the ealr model we have the following observations.
first the lt unsupervised model has a larger performance value in all entries for both poptandacc .
sec ond the age model has a larger performance value in all entries forpopt.
foracc except one entry i.e.
the underlined .
the age model also has a larger performance value.
on average see the avg row the two best simple unsupervised models exhibit more than improvement in terms of poptand exhibit more than improvement in terms ofacc .
overall the above observations suggest that simple unsupervised models could be better than the state of the art supervised models in e ort aware jit defect prediction under across project prediction.
table overall performance simple unsupervised models vs. the best supervised model cv tw cv ap popt acc p opt acc p opt acc ns nd nfppp pp entropypp pp ltpppppp fix ndevp p agepppppp nucp p p exp rexp sexp p p signi cantly better and the magnitude is not trivial signi cantly worse and the magnitude is not trivial the di erence is not signi cant or magnitude is trivial table summarizes the main results from section .
section .
and section .
.
as can be seen the best two simple unsupervised models are the lt age models as they perform signi cantly better than the best supervised model under all of the three prediction settings in terms of both poptandacc .
in addition the nf entropy ndev sexp models perform similarly to or better than the best supervised model.
thus we have a strong evidence to support that many simple unsupervised models perform well compared 165with the state of the art supervised models in e ort aware jit defect prediction indicated by poptandacc .
the above nding is very surprising as it is in contrast with our original expectation that the state of the art supervised models should have a better performance.
the reason for this expectation is that the state of the art supervised models exploiting the defect data i.e.
the label information to build the prediction model.
from the above nding we believe that it is a good choice for practitioners to apply simple unsupervised models in practice due to the low building cost and the wide application range.
.
threats to validity in this section we analyze the most important threats to the construct internal and external validity of our study.
.
construct validity the dependent variable used in this study is a binary variable indicating whether a change is defect inducing.
our study used the data sets provided online by kamei el al.
.
as stated by kamei et al.
they used the commonly used szz algorithm to discover defect inducing changes.
however the discovered defect inducing changes may be incomplete which is a potential threat to the construct validity of the dependent variable.
thus the approaches to recovering missing links are required to improve the accuracy of the szz algorithm.
indeed this is an inherent problem to most if not all studies that discover defect inducing changes by mining software repositories not unique to us.
nonetheless this threat needs to be eliminated by using complete defect data in the future work.
the independent variables used in this study are the commonly used change metrics.
for their construct validity previous research has investigated the degree to which they accurately measure the concepts they purport to measure .
in particular each change metric has a clear de nition and can be easily collected.
in this sense the construct validity of the independent variables should be acceptable.
.
internal validity there are two potential threats to the internal validity.
the rst potential threat is from the speci c cut o value used for the performance indicator i.e.
the acc .
in our study .
is used as the cut o value for the computation for recall of defect inducing changes.
the reasons are two fold.
first .
was used in kamei et al.
s study this enable us to directly compare our result with them.
second .
is the most commonly used cut o value in the literature.
however it is unknown whether our conclusion depends on the chosen cut o value.
to eliminate this potential threat we re run all the analyses using the following typical cut o values .
.
and .
.
we found our conclusion remained no change.
the second potential threat is from the gap between the training and the test set in time wise cross validation.
in this study we use two months as the gap.
however it is unknown whether our result depend on this gap.
in order to eliminate this potential threat we re run all the analysis using the other gaps and .
we found our conclusion remained no change.
.
external validity the most important threat to the external validity of this study is that our results may not be generalized to othersystems.
in our experiments we use six long lived and widely used open source software systems as the subject systems.
the experimental results drawn from these subject systems are quite consistent.
furthermore the data sets from these systems are large enough to draw statistically meaningful conclusions.
we believe that our study makes a signi cant contribution to the software engineering body of empirical knowledge about e ort aware jit defect prediction.
nonetheless we do not claim that our ndings can be generalized to all systems as the subject systems under study might not be representative of systems in general.
to mitigate this threat there is a need to use a wide variety of systems to replicate our study in the future.
.
conclusions and future work in this paper we perform an empirical study to investigate the predictive power of simple unsupervised models in e ort aware jit defect prediction.
our experimental results from six industrial size systems show that many simple unsupervised models perform well in predicting defect inducing changes.
in particular contrary to the general expectation we nd that several simple unsupervised models perform better than the state of the art supervised model reported by kamei et al.
.
the experimental results from the investigated six subject systems are quite consistent regardless of whether times fold cross validation time wisecross validation or across project prediction is considered.
our ndings have important implications.
for practitioners simple unsupervised models are attractive alternative to supervised models in the context of e ort aware jit defect prediction as they have a lower building cost and a wider application range.
this is especially true for those projects whose defect data are expensive to collect or even unavailable.
for researchers we strongly suggest that future jit defect prediction research should use our simple unsupervised models as the baseline models for comparison when a novel prediction model is proposed.
our study only investigates the actual usefulness of simple unsupervised models in e ort aware jit defect prediction for open source software systems.
it is unclear whether they can be applied to closed source software systems.
in the future an interesting work is hence to extend our current study to closed source software systems.
.
repeatability we provide all datasets and r scripts that used to conduct this study at .