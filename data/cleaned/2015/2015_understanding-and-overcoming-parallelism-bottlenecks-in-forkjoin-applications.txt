understanding and overcoming parallelism bottlenecks in forkjoin applications gustavo pinto1anthony canino2fernando castor3guoqing xu4y u david liu2 ufpa brazil1suny binghamton usa2ufpe brazil3uc irvine usa4 abstract f ork join framework is a widely used parallel programming framework upon which both core concurrency libraries and real world applications are built.
beneath its simple and user friendly apis f ork join is a sophisticated managed parallel runtime unfamiliar to many application programmers the framework core is a work stealing scheduler handles finegrained tasks and sustains the pressure from automatic memory management.
f ork join poses a unique gap in the compute stack between high level software engineering and low level system optimization.
understanding and bridging this gap is crucial for the future of parallelism support in jvm supported applications.
this paper describes a comprehensive study on parallelism bottlenecks in f ork join applications with a unique focus on how they interact with underlying system level features such as work stealing and memory management.
we identify 6bottlenecks and found that refactoring them can significantly improve performance and energy efficiency.
our field study includes an in depth analysis of a kka a real world actor framework and additional open source f ork join projects.
we sent our patches to the developers of projects and outof the projects that replied to our patches have accepted them.
i. i ntroduction modern java applications predominately run on parallel architectures whose performance and energy efficiency critically depend on efficient thread management.
f ork join is an influential parallel framework at the core of java concurrency design.
it not only provides thread management to numer ous real world applications but also serves as the bedrockfor higher level java concurrent libraries .
the impact of f ork join also goes beyond java applications per se a s several new programming languages continueto operate on java virtual machines jvms and rely on f ork join for thread management.
f ork join is known for its intuitive programming interface particularly suitable forprogramming task parallel and data parallel jobs that have adivide and conquer nature.
f ork join employs a work stealing runtime .
while work stealing provides many benefits in resource utilization and scalability efficient stealing dictates careful coordinationacross the layers of applications runtime systems and theos.
system level performance and energy optimizations forc based work stealing programs are not new but combining work stealing with a java like managedruntime and more importantly reorienting it to applicationprogramming comes with a distinct set of unique challenges thread management work stealing by nature is decentralized coordination where threads coordinate onsystem utilization but thread management decisions aremade by individual threads.
this is in contrast withexisting approaches either lacking coordination e.g.
j a v a thread objects or requiring centralized management e.g.
thread pooling .
synchronization management work stealing presents unique features in managing synchronization and thread states.
unfortunately they conflict with traditional ap proaches such as locks e.g.
synchronized methods and explicit thread state management e.g.
sleep leading to erratic performance surprising to applicationprogrammers.
this problem is exacerbated by the largelegacy code base of java applications and libraries.
data management the java runtime primarily allocates objects in the heap and deallocation is managed by garbage collection.
this is in sharp contrast with c basedwork stealing frameworks where data are routinelyrepresented as arrays of primitive data types.
as a result the allocation and distribution of data among workerthreads plays a pivotal role in application performance.
do these challenges introduce bottlenecks in modern parallel applications running on f ork join?
how severe are these bottlenecks in terms of performance and energy efficiency?
is there generalizable wisdom that can be shared with f ork join programmers to avoid the bottlenecks?
this paper we present the first empirical study to bridge the gap between modern software engineering and work stealing systems in the context of f ork join.
it aims at providing a better understanding as well as raising the awareness ofthe subtleties and common performance pitfalls in f ork join programming through a comprehensive study of character istics and behaviors of real world f ork join applications.
we identify potential bottlenecks against parallelism in theseapplications illustrate their impact on system performanceand energy and demonstrate how such bottlenecks can beovercome through refactoring.
our study follows a unique cross layer approach it is application driven and system aware.
on the one hand we are more interested in how real world applications built on f ork join behave and how their performance can be improved through application level programming rather thanan under the hood system level optimization.
on the otherhand we are aimed at finding the root causes of the bottleneckson the systems stack such as how each bottleneck maypotentially hamper the desired behavior of the work stealingscheduler garbage collector and underlying hardware.
thiscross layer approach advances software engineering by provid978 .
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research765 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i placing fork join in context .
work stealing fine grained dynamic garbage unstructured programmable parallelism allocation collection synchronization thread states fortran no no no no yes uncommon pthread no no uncommon no prevalent prevalent openmp no no uncommon no uncommon uncommon mpi yes yes uncommon no uncommon uncommon cilk yes yes uncommon no uncommon uncommon java threads no no prevalent yes prevalent prevalent x10 yes yes prevalent yes uncommon uncommon haskell yes yes prevalent yes uncommon uncommon fork join yes yes prevalent yes prevalent prevalent ing guidelines for performance improvement and illuminating why programming patterns and performance are intimately linked.
the approach also advances system research by fillinga void of assessing work stealing through an empirical andapplication oriented route taking advantage of the fact that f ork join is the first work stealing framework with a large application developer base.
we take a two pronged approach for our empirical study.
first we conduct a depth oriented study on a kka a sophisticated middleware f ork join based framework.
we identify a bottleneck at the junction of a kka s messaging engine and f ork join and demonstrate an average speedup of .
and up to a .
and an average energy savings of .
up to .
through an in depth refactoring of a kka s core messaging engine.
second we conduct a breadth oriented study through investigating real world f ork join projects from github with a total of 791k loc.
we summarize our findings as a taxonomy of bottlenecks and present across layer analysis on the root causes of these bottlenecks.by removing these bottlenecks the optimized applicationscan produce an average of of performance improvementand of energy savings.
our optimization patches wereconfirmed by the majority of application developers we com municated with.
this paper makes the following contributions we present a comprehensive application driven systemaware empirical study on performance and energy effi ciency of f ork join applications.
we identify bottlenecks latent in f ork join applications analyze their root causes and provide programming patterns for mitigating them.
we develop fjd etector a bottleneck detection and refactoring tool that can perform interactive source code level optimizations of some f ork join applications.
the source code of the tool we have developed as well as all raw data can be found online.
ii.
b ackground we now provide a brief background on the work stealing algorithm its implementation in f ork join and applications built on f ork join.
stealing work stealing was popularized by the cilk language a c like language designed for parallel programming.
in the work stealing runtime each cpu core ismanaged by a worker which is often directly mapped to an os thread.
the computational unit executed by each workeris called a task during whose execution may fork additional tasks.
these tasks are placed on a decentralized per worker queue.
when a worker completes a task it picks up one more from its queue.
when the queue is empty the worker steals a task from the queue of another worker.
in this case we call the stealing worker a thief while the worker whose item was stolen is called a victim.
ultimately workers are joined to compute a result.
observe that the logical parallel processing unit a task is different from the physical parallel unit a worker.
in practice the number of workers physical threads is staticallydetermined often the same as the number of cpu cores whereas the number of tasks logical processing unit farexceeds the number of workers.
work stealing therefore isan instance of fine grained parallelism.
the f ork join framework fork join is java s parallel programming framework with a unique set of features.
table iplaces f ork join in the context of commonly used frameworks and languages.
at its core f ork join is a concrete implementation of work stealing.
the forkjoinpool class is the entry point of the f ork join framework where the programmer can specify the number of workers.
tasks are modeled as sub classes of the forkjointask class recursivetask and recursiveaction.
the two differ in that only the former can return the result of a computation.
upon execution aforkjointask instance may in turn fork additional tasks called child tasks via the fork method.
invoking the join method introduces synchronization between the enclosing taskand its children.
the framework also provides additional util ity methods.
for example method invokeall is syntactic sugar for a fork immediately followed by a join.
method isdone inspects whether a task has completed.
f ork join applications as f ork join runs on the jvm its influence extends beyond applications written in java.a growing number of object oriented languages such asscala are translated to java bytecode and operate on the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
jvm.
such programming languages also take advantage of fork join for thread management.
as our main interest is ondynamic behavior of f ork join performance and energy efficiency in particular we view applications written in these languages also as f ork join applications.
one example is a kka an actor framework written in scala but built on java s f ork join.
scala s programming interface for f ork join is identical to the interface described above with the exception of language specific grammatical differences.
conceptually actors are a message passing frameworkwhere each actor serves as a logical processing unit thatcommunicates with each other.
no two actors share memory leading to benefits such as race condition freedom by design.
a kka has been deployed by companies such as groupon ebay and amazon.
iii.
m ethodology benchmarks table ii shows the applications within the scope of this study.
the first row provides the information for a kka .
we selected this application because a kka is among the largest open source projects built on top of f ork join a kka has been extensively deployed in the real world a kka is a middleware framework rather than an enduser application.
its performance improvement may lead tosignificant impact on a large number of end user applications.
for the breadth oriented study we searched github for the key word forkjoin and selected a set of open sourceprojects covering a wide range of application domains fromsupervisor management to raytracing.
our selection criteriaare they should not be tutorials they should be recent but not currently under rapid changes.
for example we didnot select any projects whose first commit and last commit areboth within months they must be able to compile andrun.
for each project we investigate its source code lookingfor possible bottlenecks.
if the project has tests we executedthe tests that perform f ork join computations otherwise we wrote the tests.
we discarded projects where we were unableto find any bottleneck.
experiments we ran each selected application in a machine with a core core when hyper threading is enabled intel r xeon r e5 cpu .60ghz and 64gb of ddr3 memory running debian kernel .
.
amd64 andoracle hotspot bit server vm build .
b02 mixedmode jdk version .
.
b13 .
the machine has three cache levels l1 l2 and l3 whose sizes are 64kb percore 128kb total 256kb per core 512kb total and 3mb smart cache respectively.
all experiments were performedin the os exclusive mode without any other loads runningsimultaneously.
the default settings of both the os and the jvm were used.
in particular the power management of linux is thedefault ondemand governor which dynamically adjusts cpu core frequencies based on system workloads.
for the jvm the parallel garbage collector is used and just in time jit compilation is enabled.
the initial heap size and maximumheap size were set to be 1gb and 16gb respectively.
hyper threading is enabled and the turbo boost feature is disabled.table ii asample of projects used in this study .locs encompass only non blank and non commented lines of code computed using the cloc program .
projects loc commits bottlenecks akka itemupdown jacer educational scalatuts knn doms transformers forkandjoinutility solitaire mywiki magicsquares ejisto exhibitor cq4j netflixoss javaonebr jadira ecco conflate bazzar base documentindexing csstproto fibonacci mandelbrot solitaire matrices lockedbasedgrid basic blocks warp j7cc lowlatency for all applications other than a kka we ran each benchmark times this is implemented by a top level iterationloop over each benchmark.
the reported data is the averageof the last runs to warm up the jit optimizations .
forour a kka study we ran each benchmark times discarding the first runs.
message passing frameworks such as actors are known to have a higher degree of nondeterminism.
weobserved higher variation in our experiments and choose torepresent results with a larger sample of data.
energy consumption was measured using jrapl a framework that contains a set of apis for profiling java programs running on cpus with running average power limit rapl support.
our energy consumption data includecpu core cpu uncore and dram energy consumption.
iv .
a s tudy on akka in this section we conduct a depth oriented study on potential parallelism bottlenecks latent in f ork join applications with a focus on a kka .
an overview of akka messaging core akka s internal messaging structure is detailed in listing .
messages are en capsulated by an envelope that bundles an abstract message with its sending actor.
messages are sent between actors byforwarding the message to the dispatcher.
messages sent to an actor are queued up in the actor s mailbox which is an instance of a forkjointask.
once scheduled a mailbox task will process allmessages held in its queue one at a time authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
case class envelope message any sender actor class mailbox queue queue extends forkjointask var receiver actor def setactor a actor receiver a def hasmessages boolean ... def isscheduled boolean ... def setscheduled ... def setnotscheduled ... def run boolean processmailbox def processmailbox unit val next queue.next if next !
null receiver.invoke next processmailbox setnotscheduled class dispatcher val pool executorservice def dispatch receiver actor msg envelope val mbox receiver.mbox mbox.enqueue msg if !mbox.isscheduled mbox.setscheduled pool.execute mbox class actor mbox mailbox dispatcher dispatcher def sendmessage msg envelope unit ... dispatcher.dispatch this msg listing .
the core a kka messaging logic classes mailbox dispatcher and actor have additional unrelated methods not shown in the same order that the messages were received.
a message is processed when the message handler defined in the actor hasbeen executed.
note that a mailbox once scheduled may represent a long running task.
furthermore a mailbox handles synchronization via status bits and compare and swap abstractly represented with isscheduled setscheduled andsetnotscheduled.
overall an a kka runtime may consist of a large number of actors and f ork join provides fine grained parallelism for message processing of different actors as illustrated by the mailbox class.
bottleneck centralized pooling fork join as a workstealing runtime in essence features decentralized thread management decisions on task creation execution and migra tion are managed by individual worker threads and there isno centralized control.
for backward compatibility purposes f ork join in addition supports centralized pooling maintaining a centralized task pool where all newly created tasks should be submitted and from which all f ork join workers steal2.
centralized pooling however goes against the spirit of work stealing which may lead to performance penalties.
akka handles mailbox tasks through centralized pooling.
this can be seen in the dispatch method in listing where the mailbox is submitted through execute to the centralized pool.
indeed the mailbox abstraction is a natural design choice considering a kka needs to maintain the semantic guarantee that messages are processed one at a time in the 2for backward compatibility reasons the conceptually centralized pool is implemented as the union of all f ork join worker thread queues.class envelope message any sender actor extends forkjointask var receiver actor def setactor a actor receiver a def run unit receiver.invoke message receiver.mbox.setnotrunning class mailbox queue queue extends forkjointask var receiver actor def setactor a actor receiver a hasmessages isscheduled setscheduled setnotscheduled same as listing def isrunning boolean ... def setrunning ... def setnotrunning ... def run boolean if !isrunning processmailbox else run processmailbox same as listing class dispatcher same as listing class actor mbox mailbox dispatcher dispatcher def sendmessage msg envelope unit msg.setactor this if !mbox.hasmessages mbox.notrunning mbox.setrunning msg.fork else dispatcher.dispatch this msg listing .
refactored a kka messaging logic well preserved order.
the sacrifice to be made is that a task cannot be forked for every message sent de facto foregoing the decentralized nature of f ork join design.
this may lead to performance penalties especially when an actor does not continuously receive a backlog of messages i.e incoming messages do not need to be queued.
centralized pooling is unfriendly to forkjoin for several reasons.
first there is greater synchronization overhead associated with scanning the centralized pool.
second the processingof individual tasks must go through centralized scheduling often delayed compared with the decentralized design.
overcoming the bottleneck we illustrate a modified version of a kka that takes advantage of fork in listing .
our intuition is that when the mailbox is empty we can immediately fork the message handling as a task at message send time.
we transformed the envelope class into a forkjointask which upon run will invoke the handler of the message receiver.
to determine whether the mailbox is empty we introduce a flag isrunning which will be atomically accessed.
when the mailbox is not empty the program defaults to a kka s one at a time message processing.
algorithmically this refactoring may improve performance of a kka programs because it removes the handling of the first actor message from the critical path of actor message handling.
the synchroniza tion introduced by isrunning is per mailbox decentralized in nature.
in our implementation we further enable a light weight tracking on how often mailbox is empty when a message authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii detailed performance sta tistics a kka with f ork join worker threads runtime ms energy j benchmark original custom speedup original custom savings max throughput .
.
.
.
.04x .
.
.
.
.
single ping .
.
.
.
.35x .
.
.
.
.
ping throughput .
.
.
.
.58x .
.
.
.
.
single producer .
.
.
.
.78x .
.
.
.
.
multi producer .
.
.
.
.92x .
.
.
.
.
middle man .
.
.
.
.18x .
.
.
.
.
mediator .
.
.
.
.63x .
.
.
.
.
forkjoin worker threadsspeedup x forkjoin worker threadsenergy saved micromax throughput single pingping throughputsingle producermulti producermiddle manmediator fig.
.
speedup y axis in logarithmic scale and energy saving of refactored a kka implementation is sent to it.
the a kka runtime adaptively switches to the default when the likelihood is small.
table iv akka benchmark configura tions actors messages per actor max throughput single ping ping throughput single producer multi producer middle man mediator performance impact we have modified a kka .
compiled and run with scala .
to incorporate these changes.we have evaluated these changes within a kka with micro benchmarks provided by the actors benchmarking suite with minor changes to include our performance measurements .
ping throughput creates several pairs of actors that ping messages.
single ping is an instance of this general pattern where only one pair of actors with a largenumber of messages are created.
additionally we created twovariations of ping throughput middle man where two pinging actors compete to send messages to a third middle man actor and mediator where a ping messages must first pass through a third actor.
single producer taxes a single actor by sending a large number of messages without waiting.
multi producer spawns application threads that all send messages without waiting to a singleactor.
max throughput spawns application threads that each send messages without waiting to their own actor.
thenumber of actors and messages per actor for each benchmarkare detailed in table iv.
as shown in figure eliminating the centralized pooling bottleneck results in a remarkable improvement in performance and energy efficiency.
we observe an average of .
speedup and .
energy savings in a wide spectrum of set tings.
among them ping throughput single ping andmediator reacted to our refactoring with overwhelmingly positive results.
these three benchmarks capture the scenario when an actor or some actors are able to processmessages without them queueing up and represent the casewhere our experiments confirm that fork leads to performance benefits.
for ping throughput the observed speedup ranges from .
to .
and the energy savings range from .
to .
.
in contrast max throughput single producer and multi producer capture the scenario where an actor will receive messages faster than it canprocess them and represent the case where our experimentsindeed show a mild slowdown.
we will discuss these detailsshortly.
the most intriguing case is perhaps middle man a hybrid case where some running actors are observed to have a backup of messages.
encouragingly middle man has a stable .
to .
speedup and .
to .
energy savings.
a kka as a middleware framework may be subjected to diverse workloads.
refactoring at the level of the core service of a kka cannot nor should it be expected to benefit all workloads.
in our experiments we find our new implementa tion of a kka is effective in the presence of heavy workloads authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in terms of the number of actors the number of messages and the number of workers.
the workload it does not handle wellis the case when the throughput rate of an actor s messagehandler is far below the rate of its message reception.
ourcurrent sampling algorithm partially addresses this issue buta more refined workload characterization is likely needed foran industrial strength a kka re implementation.
we highlight the thread configuration in table iii.
observe that in thecase that we do not perform well the slowdown remains withinthe deviation.
v. a t axonomy of fork join parallelism bottlenecks centralized pooling is an important bottleneck we have discovered for f ork join applications but not the only one.
in this section we summarize additional bottlenecks we have found in our study.
from now on centralized pooling is alsocalled bottleneck .
bottleneck copy on fork for data intensive applications a performance sensitive dimension of design is data distribution i.e.
how data are spread through parallel execution units.
individe and conquer frameworks including f ork join the general strategy is to represent the data as an indexiblestructure e.g.
a potentially multidimensional array which in turn can be partitioned into slices and fed to individual parallel execution units.
this simple process may pose challenges to a f orkjoin programmer.
in particular data in java are often represented as objects and arrays are dynamically allocated.
the combinationeffect of aliasing and shared memory programming impliesthat data distribution by reference at forking time mayintroduce race conditions.
as a conservative approach many f ork join programmers choose to copy data at the forking time.
observe the followingusage of the copyofrange in figure .
import static arrays.
class task extends recursiveaction public task user u ... protected void compute if u.length n local u else int split u.length user u1 copyofrange u split user u2 copyofrange u split u.length invokeall new task u1 new task u2 fig.
.
example of copying data over sub tasks beyond the obvious consequences such as memory bloat excessive copying turns out to be uniquely un friendly to f ork join for a number of reasons.
as a fine grained parallelism framework most tasks are completedwithin milliseconds.
copying upon fork implies the dominat ing growth of short lived objects creating a severe burdenfor garbage collection.
the cascaded division common in f ork join applications means that data are copied at every level of recursion potentially leading to an o log n growth inmemory.
in contrast copying for flat data partitioning can onlylead to a constant growth in memory.
unlike copying with threadsenergy j magicsquares 32copied shared threadstime sec magicsquares fig.
.
a comparison on energy and performance with varying numbers of threads before and after copies are removed in magicsquares.
flat data partitioning where all allocations are done once and for all a strategy somewhat friendly for the memory allocatordue to batching copying with cascaded data partitioning leadsto frequent yet intermittent allocation requests hamperingperformance.
among the programs we have studied we found occurrences of this bottleneck in f ork join programs.
fixing the bottleneck requires simple modification of the source code that shares the input data structure and letssubtasks work on distinct regions of the data structure.
fig ure shows the energy gains from fixing this bottleneck.
fig.
.
energy savings when removing the copy on f ork bottleneck.
from left to right projects are itemupdown jacer educational scalatuts knn netflixoss doms transformers forkandjoinutility exhibitor solitaire javaonebr mywiki ejisto cq4j and magicsquaresclearly the energy consumption is reduced in allthe refactored programs.
theaverage reduction in energyconsumption is .
.
theexecution time decreasesproportionally.
interestingly out of the analyzedprojects cross the barrierof energy savings.
however of the analyzed projectshave energy savings of lessthan .
for the projectsabove the minimumenergy saving was .
foritemupdown and the maximum was .
for magicsquares .
after inspecting these projects we haveobserved that the amount of energy savings is related tothe width of forking.
that is the more the program createsredundant copies of the data structure the more effective ourrefactoring is.
figure shows the comparison results before and after eliminating copies for magisquares a computational benchmark for computing the magic square puzzle .
the dataparallel computation is based on the number of permutationsavailable which represents all possible rows columns anddiagonals.
each parallel task attempts to construct a matrixwhose first row is the permutation and whose first columnis another permutation that begins with the same entry andcontains no other duplicate entries.
the algorithm attemptsto find sum permutations to fill in the remaining rows andcolumns.
when sharing the data structure we saved theprogram from creating additional data structures withinteger data type leading to a .
energy saving whenrunning with threads.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
enabled by jrapl we also report our results on the hardware component level for dram cpu and uncore respectively.
we observed that roughly the same amount of cpuenergy was consumed before and after removing the copies i.e.
.
joules and .
joules respectively .
however thedifference is more obvious when the energy consumptions ofdram and uncore are compared.
due to the excessive objectcreation dram and uncore of the original version consume1.
and .
more energy than the optimized version.
since copy on f ork creates large volumes of small shortlylived data structure objects it is interesting to understand how different gc algorithms may impact our results we conductedexperiments over gc options in hotspot a serialgc the stop the world serial collector b parallelgc the parallel collector c paralleloldgc the parallel collector with data compression d concmarksweepgc concurrent mark sweep collector and e g1gc the garbage first collector.
figure shows the results for magicsquare.
for almost all algorithms the fix can speed up gc by .
copy referenceenergy consumption j abcdecopyreferencetime sec abcde fig.
.
a comparison of gc costs magicsquares threads gc algorithms are a serialgc b parallelgc c paralleloldgc d parallelnewgc e g1gc .
bottleneck copy on join the counterpart of copy on f ork iscopy on join after having joined on its subtasks a task must usually combine the results of the subtasks into a result for the larger problem.
consider the program in figure extractedfrom the cq4j benchmark.
protected list t compute int size datasource.size if size fork size return computedirectly else list t result new arraylist t int mid size recursivefilteringtask t first new recursivefilteringtask t filter datasource.
sublist mid first.fork recursivefilteringtask t second new recursivefilteringtask t filter datasource.
sublist mid size second.fork result.addall first.join result.addall second.join return result fig.
.
example of joining data with sub tasks.
as one reader might observe this particular code snippet suffer from the same bottleneck previously explained creating sublists of the current data structure .
however this benchmark also presents a different bottleneck.
at theend of the execution an expensive operation addall is invoked to copy merge collections.
copy on join has many negative consequences similarly to copy on f ork with oneadditional unique drawback since joining in a work stealing system is implemented by barriers copy on join increases the wait time at barriers particularly unfriendly for energy consumption.
note that this is an established fact barrier wait at the low level is either implementedas spin locks or context switch both of which can leadto energy waste without contributing to program progress.
fig.
.
energy savings after removing the copy on join bottleneck.
from left to right projects are cq4j ejisto javaonebr exhibitor conflate.we have found occurrences of this bottleneck in the pro grams studied.
a fix of this bot tleneck is similar to that of copy on f ork a shared data structure can be passed into subtasks to carry results.
after applyingthese changes in programs wehave achieved overall energy savings.
the results areshown in figure .
bottleneck scattered data we next investigate the impact of data locality on performance and energy consumption.
an important pattern we found is that the execution of a taskfollows the sequence of ababababc where aperforms memory copies for a subtask bforks the subtask and cdoes the computation of the current task.
figure shows a code snippet of this case extracted from benchmark csstproto.
protected r compute if len recursivetask r task createtask from return task.invoke else forkjointask r tasks new forkjointask for int i i len i forkjointask r task createtask from i task.fork tasks task r result tasks .join tasks null for int i i len i r next tasks .join tasks null result merge result next return result fig.
.
example of scattered data.
this pattern has impact on energy consumption and performance for several reasons.
first the copy operation has thepotential of polluting caches increasing the chance of memoryround trips.
second the number of context switches might alsoincrease due to the sparse task creations.
a possible solutionto this problem is to create a list of tasks and during the for loop add each new task object to the list.
after the execution of the for loop one might call the invokeall method which is responsible for forking and joining all tasks in the list.with this fix we have observed an energy saving of .
forcsstproto.
regarding cache behavior we observed that theoriginal implementation had a .
cache misses whereasthe fix reduced it to .
.
we also observed a reductionon context switches from to .
yet the numberof branch misses is also reduced from .
of all branches authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
to .
which we believe is due to the boilerplate code used our initial example invokeall eliminates the first for loop then reducing the overall number of branches and as a consequence the number of branch misses.
bottleneck exacting intra task synchronization as locks play a central role in java shared memory programming and metadata representation unstructured synchronization i.e.
object locks is pervasive in java applications.
synchronization occurs via invoking synchronized methods or code blocks or using popular concurrent library classes such ascountdownlatch.
improving performance and energy effi ciency for systems where unstructured synchronization is theonly mechanism to achieve concurrency safety such aspthreads or the java thread library is a well understood topic.
mixing unstructured synchronization in a structured parallel system such as work stealing leads to additional subtle interactions between the application runtime and the os.
whenunstructured synchronization happens in the middle of the task execution it effectively stalls stealing from that worker.unfortunately the stalled worker cannot forgo the current taskand select another task from its deque even if there aremany other task items in it because tasks on the deque ina work stealing system carry inherent logical dependencies analogous to stack frames.
at best the worker itself can becontext switched by the os.
observe however even thoughthere may be thousands of tasks in the work stealing runtime the number of workers the jvm representation of os threads is few typically smaller than the number of cpucores.
in other words os level context switch may at best helpother applications in a time sharing environment but will not contribute to improving the performance or energy efficiency of the application itself.
the most principled solution to avoid the bottleneck is to eradicate unstructured synchronization from java.
there is encouraging progress in recent java development to supportasynchronous abstractions such as futures .
however it may take time before java practitioners fully embracethese features .
in this study we investigate into legacyprograms attempting to understand how unstructured syn chronization is used in the real world.
overall we found 7occurrences of this bottleneck.
surprisingly we found in asignificant number of projects an easier solution exists manysynchronizations are simply to implement exact computations which can be safely relaxed without creating any impact on correctness .
we illustrate this bottleneck with benchmark mandelbrot.
a mandelbrot is a mathematical set ofpoints whose boundary is a distinctive two dimensionalfractal shape.
each parallel task works on a set of points andthesynchronized block is then used when a task needs to render the fractal image.
this is done by calling the setrgb method available on the bufferedimage class as showed in figure a .
4we used the perf linux tool to calculate cache misses context switches and branch misses.if !isbenchmarking mb.isliverendering synchronized mb.lock mb.renderimage.setrgb j i color.getrgb mb.repaint a threadsenergy j mandelbrot 32synchronized unsynchronized threadstime sec mandelbrot b fig.
.
example of an hidden over synchronization a and a comparison of energy and performance with and without synchronization b on mandelbrot figure b shows the results for this benchmark in this benchmark a task has a range of values of which it should work on.
for our input data width height thebenchmark creates a total of tasks.
as we can see thereis a great difference between the synchronized version and theunsynchronized one.
on average the unsynchronized versionconsumes less energy then its counterpart faster .
after inspecting the implementation we observed that the method setrgb is already synchronized so there is no need to use another synchronization construct to wrap up thissingle method call.
in fact we could not find any visibledifference between the images generated by executions withand without the synchronization.
we sent the modified sourcecode as a patch to its developer who then acknowledged theover synchronization and accepted our patch.
.
bottleneck sleepy workers a more extreme case but along the same line of intra task synchronization is the use of thread.sleep during task execution.
just as the previous bottleneck the invocation of this thread management primitive stalls stealing and explicitly requests os contextswitches.
from a logical perspective the intention of theprogrammer may be to put the task to sleep but unfortunately the work stealing runtime will place the worker to sleep.
as described earlier the worker cannot forgo the sleep inducing task and pick up other tasks from its deque neither can the idlecpu core help other workers of the same application.
what isworse is that unless the os has other applications running anidle core under the widely used on demand governor wouldput the core in a low power state which later needs a longtime to wake up.
in a work stealing runtime where competitiveperformance is of its first priority user level sleeping is oftenmore detrimental than beneficial.
we found occurrences ofthis bottleneck.
thectask benchmark presents the worst scenario of this bottleneck.
during the sequential execution this benchmarkputs every current task to sleep for a second.
figure showsthis impact on both performance and energy consumption.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
threadsenergy j ctask 32sleep no sleep threadstime sec ctask fig.
.
a comparison on energy and performance with and without thread sleeping for varying numbers of threads in ctask.
the sleep construct creates significant penalties in both performance and energy consumption.
the execution without sleep can be .
more energy efficient than the execution with and reduction in running time .
after inspecting the source code we observed that the developerused sleep to force the program to wait for a result fromanother computation.
however this sleep is unnecessary sincethe computation on which the sleep is waiting is a synchronous operation.
vi.
d etecting refactoring opportunities some bottlenecks can be detected and refactored automatically.
as a proof of concept we have built a tool named fjd etector capable of automatically detecting and refactoring copy related bottlenecks as explained in bottleneck .
a.fjd etector fjd etector works as a plugin for eclipse ide.
it performs source code analysis on f ork join programs focusing on programs with divide and conquer data parallelism.
we check if the f ork join computation is operated on a data structure such as array orarraylist.
since most of thearraylist methods provide accesses over arrays our approach handles them in a similar way.
f ork join computations are usually described in terms of inner classes where the data is passed through the inner class constructor.
hence foreach parameter of the constructor we inspect a if it is a datastructure b if it is splitted and copied inside the compute method and c if the variables containing the copy results are passed into new instances of the task class.
we identify potential divide and conquer programs through pattern matching f ork join scompute method body.
specifically we are looking for a branching statement that fallsinto one of three patterns.
sequential computation in the if block and parallel computation in the else block parallel computation in the if block and sequential computation in theelse block and sequential computation in the if block plus a return at the end of the block and the parallel computation in the remainder of the method.
fjd etector is not able to work with f ork join classes structured in a different manner.
once a bottleneck is confirmed by the developer fjd etec tor performs a set of transformations on the f ork join code.
our transformations remove copies by computing indices foreach subtask and letting them work on distinct regions of thesame shared data structure.table v the benchmarks selected .c olumns add and del indica te the number of additions and deletions applied by fjd etector .
rep?
means replied ?
and acc?
means accepted ?
.
t he symbols check and mean respectively accepted not accepted and no response .
projects add del rep?
acc?
savings itemupdown .
jacer check check .
educational .
scalatuts check check .
knn check check .
netflixoss .
doms transformers check .
forkandjoinutility check check .
exhibitor check .
solitaire .
javaonebr check check .
mywiki .
ejisto check check .
cq4j .
magicsquares check .
b.fjd etector results we have applied fjd etector to of the benchmarks listed in iii.
the benchmarks were selected due to the presence of bottleneck v .
table v lists the selected benchmarks.
we assess fjd etector in terms of the following evaluation questions eq1.
is our approach useful ?
eq2.
how intrusive is fjd etector ?
results of eq1 to answer eq1 we have sent modified versions of the benchmarks to their developers as patches.
if these matches are useful they will eventually be merged intothe benchmarks.
to assess the intrusiveness of fjd etector we measured the number of lines of code that fjd etector adds to and removes from the benchmarks in order to refactorthem.
a large number of modifications makes the code harderto understand and modify for its developers.
with fjd etector instances of refactorings were performed over projects.
we sent these modified versions as patches to the owners of the corresponding repositoriesvia the pull request feature of github.
on table v columns replied?
and accepted?
flag the projects that have repliedand accepted our patch.
projects have replied showing anintention to accept our patch.
one project is no longer activelymaintained doms transformers .
for the remaining projects that replied of them have already accepted andmerged our patches.
benchmark netflixoss was the only project that closed our pull request with no response.
this particular projectseems to be a fork from another existing project it has 361lines of java code performed by a single developer in asingle commit and does not seem to be maintained anymore.the owners of the remaining projects did not provide anycomments for our patches.
results of eq2 to answer eq2 we measured the number of new statements that were added to and the number existing statements that were deleted from the benchmarks.
a large authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
number of modifications can produce code that is hard to understand and modify.
so a refactoring that results in a smallnumber of modifications is desirable.
overall our approach has added statements and removed ones to the benchmarks.
considering that one of them has instances of bottleneck the meannumber of modifications for each transformation was .8additions and .
deletions.
thus our approach is not veryintrusive.
most of the additions are due to the addition of anew constructor which means that preexisting code e.g.
thecompute method is the target of only a few modifications.
the refactoring of the parallel code added an average .
newstatements.
deletions have different explanations.
for instance most of the deletions on project exhibitor are due rewriting the parallel computation out of the deletions .
initially this project used a more verbose approach iterating throughthe data structure creating and forking each new parallel task and joining them at the end.
we simplified this computationby just using the invokeall method as shown figure .
protected list serverstatus compute for list serverspec sublist lists.partition specs size task task new task exhibitor sublist task.fork tasks.add task for task task tasks statuses.addall task.join protected list serverstatus compute ... int split from to invokeall new task exhibitor specs from split new task exhibitor specs from split to ... fig.
.
fjd etector refactor example.
vii.
r ela ted work parallel programming is a well established topic.
in the last decade efforts have been made on introducing novelprogramming models as well as performance programmer effort satisfaction and error proneness and even energy consumption evaluations.
there exists a considerable number of studies about the characteristics of bugs in modern software systems includingconcurrency bugs performance bugs and morerecently bugs in the cloud .
closely related to thiswork are empirical studies focusing on uses and misuses ofconcurrent libraries have been conducted .
forinstance the java.util.concurrent package in which the f ork join framework resides is the focus of a large scale study covering over projects .
however this workdoes not consider the f ork join framework.
although the work of dig et al.
considered the f ork join frameworkwhen converting sequential code to parallel code the authors did not studied anti patterns related to f ork join usage.
okur et al.
observed that misuses can account for of the overall uses of parallel libraries.
in the worst case these misues can make the code run sequentially instead ofconcurrently.
lin et al.
found that even though java s concurrent collections provide thread safe implementations when composing two or more operations developers canna vely misuse these collections and introduce atomicity violations.
other studies propose tools that correct other commonmistakes e.g.
.
these studies are complementary to ours since none of them focus on the f ork join framework.
a recent study investigated the impact of three threading constructs on application energy consumption one ofwhich is the f ork join framework.
this study found that the energy consumption of a f ork join program is sensitive to the degree of parallelism achievable by the program it outperforms two other concurrent programming models inapplications that are embarrassingly parallel but underper forms in the presence of large numbers of serial operations.this study did not investigate specific bottenecks faced by f ork join applications.
another study most closely related to our own was conducted by dewael et al.
.
in this study the authors analyzed java applications that employ f ork join to understand how real world developers use forkjoin.
however theauthors did not discuss on how the antipatterns identifiedcan be removed.
neither did they analyze the impact of theantipatterns on energy consumption.
viii.
c onclusions this paper describes a comprehensive study on parallelism bottlenecks in f ork join applications.
based on an in depth analysis over a kka together with open source f ork join applications on github we present a taxonomy of bottle necks whose removal and mitigation may lead to performanceimprovement and energy savings.
we sent our patches to thedevelopers of projects and out of the projects thatreplied to our patches have accepted them.
the bottlenecks we have identified in this paper largely group into three categories thread management bottleneck1 data management bottlenecks and and synchro nization management bottlenecks and .
we believe theapplicability of identifying and overcoming these bottlenecksmay go beyond f ork join.
in the future we plan to generalize these findings and investigate their applicability on othermulti threaded language runtimes.
a cknowledgements we would like to thank the reviewers for their valuable comments.
this work is partially supported by cnpq prope sp ufpa facepe apq .
facepe pronex apq .
nsf ccf nsf cns onr n00014 and onr n00014 .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.