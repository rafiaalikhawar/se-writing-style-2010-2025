on the use of delta debugging to reduce recordings and facilitate debugging of web applications mouna hammoudi1 brian burg2 gigon bae1 gregg rothermel1 1university of nebraska lincoln usa mouna gbae grother cse.unl.edu 2university of washington usa burg cs.washington.edu abstract recording the sequence of events that lead to a failure of a web application can be an effective aid for debugging.
nevertheless a recording of an event sequence may include many events that are not related to a failure and this may render debugging more difficult.
to address this problem we have adapted delta debugging to function on recordings of web applications in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure we present the results of three empirical studies that show that recording reduction can achieve significant reductions in recording size and replay time on actual web applications obtained from developer forums reduced recordings do in fact help programmers locate faults significantly more efficiently as and no less effectively than non reduced recordings and recording reduction produces even greater reductions on larger more complex applications.
categories and subject descriptors d. .
testing and debugging debugging aids tracing general terms reliability experimentation keywords delta debugging web applications record replay techniques recording reduction .
introduction users of web applications often experience intermittent annoying failures that reduce their productivity.
when such a failure is encountered a user may be prompted to send feedback to the application s developers.
while better than nothing user provided bug reports frequently omit important details such as clear reproduction steps and expected behavior .
many crash reporting tools supplement user reports with automatically gathered post failure data permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
copyright 20xx acm x xxxxx xx x xx xx ... .
.
e.g.
crash stacks .
this is not without its costs developers can easily be overwhelmed by the large amount of collected data .
even with detailed post failure reports it can be difficult to locate a fault because these reports often lack the specific inputs and execution conditions underlying the failure.
an alternative approach for reporting a failure is to automatically capture the inputs and events that led to it.
macro replay tools such as sikuli coscripter and selenium do this reproducing user inputs for automated testing or sharing automated workflows.
researchers have also been investigating record replay approaches for web applications and javascript programs .
there has also been research on deterministic record replay techniques that capture entire executions while additionally controlling sources of nondeterminism section provides details .
a record replay infrastructure for web applications captures user inputs and events mouse keyboard navigation commands etc.
before they are hit tested or parsed by the browser engine.
during playback inputs and events are re delivered to the browser engine.
a challenge faced by record replay approaches is that the recordings they capture may be lengthy and thus difficult to utilize.
even if an execution is exactly recorded the number of events and program states can render opportunistic debugging strategies ineffective .
captured inputs and events that are not necessary to reproduce a failure are distracting yet no prior work has attempted to reduce the size of recordings of web applications.
in this work we adapt the well known delta debugging algorithm to function on recordings of web applications in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure.
when a user encounters a failure in a web application he or she can record an extended interaction with the application and send it to the application s developers.
the user s recording can then be reviewed and reduced and utilized in bug triage fault localization and regression testing.
alternatively developers can capture recordings of web applications found to contain faults in house and reduce these recordings in order to find and correct the faults.
there are several questions that one might wish to answer about the effectiveness and the efficiency of recording reduction via delta debugging in the domain of web applications but three questions that we believe are particularly important are whether the approach sufficiently reduces recordings whether reduced recordings actually facilitate debugging and whether the approach scales to large and complex web applications.
to address these questions we conducted three empirical studies.
in our first study we sought to determine whether recording reduction via delta debugging can in fact produce potentially effective reductions in recordings when applied to a relatively large number of actual web applications containing actual failures repermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august september bergamo italy c acm.
... .
333ported by their programmers.
the study assessed the degree of reduction that could be achieved by applying recording reduction to faulty web applications obtained from developer forums.
the technique was able to reduce each recording to or less of its original size on average reducing recording sizes to .
of original sizes and reducing replay times to .
of original times.
years of work on fault localization techniques have resulted in numerous empirical studies showing that these techniques can reduce the numbers of statements that could be seen as related to faults.
virtually all papers on this topic have simply assumed that this would help programmers locate faults without directly studying whether it could in fact do so.
parnin and orso recently provided data suggesting the potential folly of that assumption.
over years of work on delta debugging have resulted in empirical studies that show that it can reduce the sizes of input sets or programs but here as well we can find no published studies of whether this can help programmers either.
thus in our second study we asked twenty computer science students to detect failures locate faults and correct them given reduced and unreduced recordings made over web applications.
the results of this study show that reduced recordings statistically significantly decreased the time required by developers to locate and correct faults without adversely affecting their effectiveness in so.
results of our first and second studies show that delta debugging can substantially reduce recordings and facilitate the debugging process but were limited to extracts of code posted by developers online.
our third study addresses scalability issues attempting to ascertain whether delta debugging can successfully reduce recordings of larger more complex web applications.
thus in our third study we applied our recording reduction technique to several larger more complex web applications.
the technique was able to reduce each recording to or less of its original size on average reducing recording sizes to .
of original sizes and reducing replay times to .
of original times.
.
delta debugging for web apps in this section we describe our adaptation of delta debugging for reducing recordings of web application failures.
we then discuss some simplifying assumptions made to facilitate the work.
.
algorithm and implementation selenium and other replay tools enable a user to capture a recordingrof events that lead to the occurrence of a specific failure f. our goal is to reduce recording rto a new recording r0by discarding events1that are not required for failure fto occur.
using r0to reproducef a developer should be able to consider fewer events and less code when attempting to locate the fault responsible for f. our recording reduction approach adapts the delta debugging algorithm presented in reference to operate on recordings of web applications.
at a high level the algorithm operates by repeatedly selecting subsets of the events in a recording and replaying these on the failing application to determine whether these subsets can by themselves reveal the failure.
if so then the algorithm continues to search these subsets for smaller failure revealing subsets until no such subsets can be found.
more precisely the delta debugging algorithm attempts to partition a given recording r0into a number of subsets and test each subset as well as its complement subset until each subset contains 1we use the term event to define the various elements that appear in selenium traces such as keyboard entries inputs and mouse movements in general however our approach is applicable to most varieties of event and input based replay systems.only one change.
if the execution of a subset raises failure f the algorithm treats the failing subset of r0as a new recording and repeats the same procedure until there is no smaller subset causing failuref.
numbers of subsets are determined by a variable n that stands for granularity nis initialized to two at the first iteration but subsequently can change to two if a subset induces failuref reduce to subset to max n if a subset s complement induces failure f reduce to complement or to min jr0j 2n ifn jr0j increase granularity .
otherwise the reduction procedure terminates and returns r0 a1 minimal test case from which no one element can be removed while preserving failure f. a critical building block for any recording reduction algorithm is afailure oracle the ability to tell whether the failure occurs during an execution.
our approach does not depend on any particular oracle for the purposes of our studies we use manually inserted assertions that detect whether a failure occurs.
given a web application with a known failure such assertions can be inserted by software engineers prior to proceeding with recording reduction.
our implementation makes use of recordings generated by selenium.
we chose this platform for several reasons including the facts that it is open source is widely used among web application testers with regard to web application testing and there is a wide availability of ides and abundant web browser control apis that make it easy to record event sequences as a test case and to execute them programmatically.
selenium ide a firefox extension can record a user s actions or events in the firefox browser and save them as a record or a test case in various formats e.g.
html java junit testng ruby rspec and c nunit .
our prototype implementation is written in java and uses the selenium webdriver library2and groovy script engine this allows the tool to execute any portion of a selenium script.
the input to our tool is a test script derived from the recording made with the selenium ide.
the recording reflects the user s interactions with the interface.
each time the user interacts with the interface a selenese command is inserted into the recording once the recording process is completed the final set of selenese commands forms a test script.
each selenese command represents one user interaction with the interface and is denoted by the following tuple action locator value .
the action component of a selenese command indicates the event that is performed on the user interface e.g.
click select during the recording process.
the locator component specifies the user interface element i.e.
input field that the user is interacting with during the recording process.
the value component refers to any input entered by the user within the locator previously specified while recording user interactions.
the first line of every selenium test script consists of the open selenese command which specifies the url of the web page under test.
the action of the open command is open and its locator is that url.
we inserted assertions within our test scripts via the selenium ide to signal the occurrence of a failure during the replay process.
once the recording process ends the test script is exported via the selenium ide as a junit4 webdriver script.
our recording reduction tool is able to replay the user interactions initially recorded based on the sequence of selenese commands contained within this test script.
our approach depends on the ability to replay subsections of a failure recording.
in general delta debugging can generate reduced recordings that are infeasible in that they contain events that cannot be executed.
selenium can start playback from any position in a recording but if the recording ends up being infeasible our im2 334plementation of the playback follows the approach used by delta debugging and outputs unresolved as the test result of the subset.
otherwise each test of a subset outputs pass or fail according to the existence of failure f. future work could analyze dependencies among events and filter out some non feasible sequences before executing them but whether this would reduce the cost of the approach would need to be studied.
finally because recordings of web applications may include certain steps that must be present in order to replay any portion of them e.g.
the input of a username and password our implementation also allows users to flag certain events in a selenium trace as prefix events all such events are ignored during recording reduction ensuring that they remain in any reduced recording.
given that data sources could change between a recording session and a replay session it is possible that some of our test cases could be rendered obsolete.
to address this issue our implementation accepts a url whose http request is established before executing the prefix events .
this feature can be used to invoke other initialization tasks that need to be performed prior to running any subset of events during delta debugging.
for example in study where our object programs utilize databases we use this feature to allow the web application server to rollback to its initial database state and user session state prior to running any event sequence otherwise replay results could be adversely affected.
to construct our implementation of delta debugging we utilized code made available by zeller 3that is based on the algorithm published in .4since the algorithm does not guarantee that each subset is tested only once it may test the same subset multiple times.
we use a cache to store test outcomes to improve efficiency.
.
simplifying assumptions and limitations to function correctly our algorithm depends on the ability of selenium to deterministically reproduce a web application s behavior.
javascript itself is a single threaded language but javascript programs can indeed face sources of non determinism.
while selenium can often control for such non determinism particularly when the application being debugged is under the control of the programmer the debugging this may not always be the case.
in such cases we would need to resort instead to the use of deterministic record replay techniques which as noted in section include approaches that capture entire executions by additionally controlling sources of nondeterminism.
one such approach also noted in section and described in greater detail in section is that of burg et al.
.
as infrastructures such as these mature they should allow applications of reduction algorithms such as ours in scenarios where tools like selenium do not suffice.
the delta debugging algorithm that we utilize relies on trial and error rather than precise tracking of runtime data dependencies to decide which inputs to discard.
data dependencies between tasks are treated as black boxes with better data dependency tracking more inputs could be safely discarded.
tools such as the whyline for java have had great success in leveraging dynamic slicing algorithms to identify a minimal causal chain of events leading up to a specific output or program state.
still as our subsequent studies show the algorithm we have presented can be quite effective at reducing recording sizes.
failure oracles are also critical to the efficacy of our approach.
for example a naive diff oracle might compare the outputs of r 4zeller s implementation omits one efficiency improving step reduce to subset of the algorithm published in reference that does not impact its effectiveness but increases its efficiency and we omit this step as well.andr0and detect that their outputs differ.
however the difference in outputs may have nothing to do with the failure s manifestation.
as noted earlier in this work we depend on engineers to insert assertions that can indicate whether a particular known failure recurs.
however given the varied modalities of web application failures we expect many types of oracles to be applicable in this context.
prior work has focused on text and javascript centric oracles but interactivity oracles and visual oracles may also be useful.
in particular oracles that can identify transition instants the statement or event where a failure manifests are helpful for isolating failures to specific intervals of a recording.
transitionrevealing oracles depend on the capabilities of the underlying replay tool or execution environment.
arya et al.
discuss practical issues in detecting such transitions such as non monotonicity of transition instants and integration with breakpoint debuggers.
finally delta debugging has certain inherent limitations that our implementation of it shares.
when a program contains multiple faults it may be difficult to correctly assess whether a particular failure is being reproduced by a subset of a recording as opposed to a failure prompted by some other cause.
somewhat similarly an attempt to assess whether a failure reoccurs given a subset of a recording may suffer from what we might call coincidental incorrectness a case in which an oracle incorrectly deduces that a given output is incorrect and does represent the recurrence of a failure.
in our empirical studies we were able to determine that the second limitation did not surface in the cases that we considered but in the general case they remain possible.
.
study feasibility our first study examines the following research question rq1 to what extent can delta debugging reduce recordings of web applications?
.
objects of analysis we chose stack overflow as a source for objects of analysis in this study.
in part this choice was made due to the popularity of this forum among web application developers.
as of march stack overflow contained questions related to html questions related to javascript and questions related to css.
we randomly selected objects of analysis from those that developers had posted within stack overflow restricting our selection to cases in which the developers had made both code and replicable descriptions of failures in that code available.
table provides basic data on the objects of analysis along with their functionality size and type of faults contained and citations that indicate where they can be found.
in this study our objects of study consist of client side code embedding faults at the level of html javascript and css.
one might argue that these objects of study are relatively short code extracts that do not reflect the complexity of typical web applications.
nevertheless these code extracts were initially posted by developers on stack overflow which indicates that their developers indeed had difficulties locating and correcting the faults by themselves.
further our objects of study had generated an average of five responses by stack overflow administrators which also attests to the non trivial nature of the faults under consideration.
in addition the objects of study were all developed by different programmers and were diverse in the types of functionality they provide.
5experiment data including selenium recordings transformed test scripts and delta debugging logs for studies and is publicly available at 335table objects of analysis functionality loc fault type host travellers invalid input applicant survey 144checking more checkboxes than allowed apply for a passport missing textbox create an account 86inappropriate dialog box output when the user clicks submit calculate age in days hours min 114invalid formula for calculation create an account 115inappropriate error message play with numbers invalid input shopping cart 30cannot add to items in cart game reorder letters in word faulty button click pool tournament 36unable to delete player from team filling complaint form 49unresponsive to button click submit a form to win a prize header not scrollable schedule an appointment online weekends not disabled prepare for a concert unresponsive page book a trip 105incorrect values listed in dropdown menu 16gather ingredients for a cooking workshop incorrect behavior best used cars in town invalid input course listings invalid form layout 19borrow a book from a library s website invalid input airport pickup information 44allowed form submission even if the user enters a phone number in the email text field rent a car unresponsive datepicker veterinary form 130inverted disable enable input fields dentist form invalid email address waiting list for classes invalid input playing with numbers 118wrong value output due to incorrect calculation phone company customer survey invalid form layout patient form 93form submitted with empty fields insurance account creation two fields left blank financial aid form inappropriate dialog box online shopping invalid amount input .
variables our independent variable consists of the type of recording utilized reduced or unreduced .
as dependent variables we measured the sizes of full and reduced recordings in terms of the numbers of events and the time required to replay the full and reduced recordings.
.
study setup and design to obtain unreduced recordings for each web application we found a sequence of events which when applied to that web application reproduced the failure that had been reported for it.
usually such a sequence had been reported on stack overflow as part of the failure report.
for example one person reported the following code is supposed to display a confirmation box.
if the user clicks the ok button the current time is supposed to be displayed.
but for some reason when i test the button by clicking on it nothing happens.
i need a fresh pair of eyes to tell me what i m missing as to why this code isn t working.
we then applied that sequence of events with selenium operating to capture the unreduced recording.
next we inserted assertions into the applications as described in section to allow the failures to be observed and we applied ourdelta debugging implementation to the unreduced recordings.
to obtain recording replay time we reran selenium on both the original and reduced recordings.
we executed this experiment as well as those reported for study on an os x .
macbook pro with .
ghz intel core i5 and 16gb of ram memory.
.
threats to validity the primary threat to external validity for this study involves our reliance on relatively small client side web pages rather than more complex web applications.
through stack overflow we had access to open source code and failure reports only for web pages extracted from web applications.
thus our study does not take into consideration all of the complexities of modern web applications a limitation that our third study seeks to address .
however all of the pages that we consider are derived from real applications with actual reported faults albeit with only a single fault per application.
a threat to internal validity involves the possibility of faults existing in our delta debugging implementation but this is mitigated to some extent by the fact that we confirmed that both our unreduced and reduced recordings did in fact reveal the same failures.
.
results and analysis table addresses rq1.
the table quantifies the differences in size and replay time observed before and after reduction.
as the data shows each recording was reduced to or less of its original size.
the average size of full recordings was .
events while the average size of reduced recordings was .
events i.e.
on average recording sizes were reduced to .
of the sizes of unreduced recordings .
the average replay time for full recordings was .
seconds while the average replay time for reduced recordings was .
seconds i.e.
on average replay times were reduced to .
of the replay times of unreduced recordings .
to determine whether delta debugging produced statistically significant reductions in recording size and replay time we conducted two tailed mann whitney u tests at significance level .
our null hypotheses were recording reduction does not reduce recording size and recording reduction does not reduce replay time .
the tests indicated that the reductions in size and replay time were both statistically significant with p values .
and .
respectively.
.
discussion the data obtained in our study demonstrates the potential effectiveness of our delta debugging algorithm applied to web applications at least of the class considered in our objects of study.
the approach was able to substantially and statistically significantly reduce recording size and replay time.
potentially such reduced recordings can help programmers debug web applications more cost effectively.
.
study our second study considers the following research question rq2 to what extent does a reduced recording assist programmers in the debugging process given a faulty web application?
.
participants we recruited participants from the population of graduate and undergraduate students enrolled in the department of computer science and engineering at the university of nebraska lincoln by sending an email request to all such students.
from those who responded we excluded persons who lacked experience with html javascript and css and then we selected the first respondents 336table recording size replay time before after reduction size events replay time seconds full recordingreduced recordingpct.full recordingreduced recordingpct.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
from those who remained eligible.
participants were each offered in compensation for their time.
we used a survey to obtain demographic information about participants.
thirteen participants were graduate students and seven were undergraduates all were male.
on a point likert scale six participants rated their experience with web programming as average seven as good six as very good and one as excellent .
participants had a mean of .
years and standard deviation of .
years experience programming web applications using html javascript and css.
participants had learned web programming by themselves four through courses and five through internships.
seven participants were familiar with record and replay infrastructures but only four had used selenium.
the mean number of web applications participants had developed was .
with a standard deviation of .
.
participants rated their debugging skills as good six as very good and two as excellent .
.
variables our independent variable consists of the type of recording utilized reduced or unreduced .
we considered two dependent variables.
our first dependent variable relates to efficiency and measures the wall clock time required by each participant for a given faulty web application to detect the failure locate the fault responsible for the failure and correct the fault.
note that in some cases participants were unable to locate and or correct faults and in those cases times required to achieve these goals are not considered.
the second variable relates to effectiveness and measures whether or not each participant succeeded for a given faulty web application in each of these three things.
.
study setup and design we selected four web applications.
these included a tic tac toe game application of loc in which the fault involved omittedcode and the applications numbered and in table .
here we refer to these as applications a b c and d respectively.
we chose these applications because they were of varying sizes ranging from relatively small loc to much larger loc and each involved different types of failures.
we used unreduced selenium recordings along with reduced recordings derived by the delta debugging algorithm used in study with assertions removed.
we conducted the study in the usability lab at the university of nebraska lincoln.
we organized twenty individual sessions of minutes in order to observe each participant individually.
we began each session with a tutorial about selenium.
next we gave the participant a faulty web application with a statement of how it was expected to work and a recording of the application s behavior in a case where it fails.
we used this to illustrate the process of replaying the recording identifying and recording information on the failure attempting to locate the underlying fault in the application and attempting to correct that fault.
during this process we worked with the participant and answered questions but we did not inform them about the purpose of the study or make any statements related to recording types or lengths.
when each participant was confident enough to proceed we gave them the four faulty web applications two with reduced and two with unreduced recordings.
we counterbalanced the assignment of web applications across all participants such that all possible combinations of web applications and recordings were utilized and distributed as evenly as possible.
we also varied the orders in which participant considered web applications to lessen the impact of learning effects.
with participants each considering two unreduced and two reduced recordings we were able to obtain sets of results relative to each of the two reduced and unreduced recordings of the four web applications.
we asked each participant to run the recording for each web application attempt to detect the failure within that application and if successful at that to attempt to locate the fault responsible for it and correct it.
during this portion of the study we captured screen recordings using the usability testing software morae this enabled us to later revisit the debugging process conducted by each participant.
we did not utilize a think aloud methodology because we wished to measure the time taken to perform tasks and a thinkaloud methodology would render these measurements problematic.
we limited the time allowed for each of the four applications to minutes each to keep the overall task manageable and limit possible fatigue effects.
as the participants proceeded we recorded the times at which they found failures noted faults and concluded that faults had been corrected.
these times provide our data on efficiency.
we also asked each participant to write a brief description of each failure located and each fault they were able to locate.
finally we asked each participant to complete an exit survey to obtain further feedback and comments.
when all sessions were finished we reviewed the participants notes about failures and faults found together with when available the version of the web application they pronounced correct to determine whether they had correctly noted the failures and faults and applied an appropriate correction.
the first author together with a staff programmer not involved with this research each performed this assessment individually and met to compare notes and come to a consensus as to which results were indeed correct.
this provided our data on effectiveness.
.
threats to validity the primary threat to external validity for this study involves the objects of study utilized.
we studied only four such objects a failure detection time b fault localization time c fault correction time figure timing results all of which were client side web pages.
still all four objects of study are actual web applications containing actual faults which their original creators had difficulty correcting so they do represent at least one sub class of actual applications of interest.
a second threat to external validity concerns our participants all of whom were students not practicing professional web application engineers.
however this group of participants does represent one actual non trivial class of persons who program web applications and they are interesting in that respect.
threats to internal validity may involve learning effects we guarded against these by counterbalancing and varying the orders of web applications and recordings.
threats to construct validity involve the use of a minute time limit for debugging tasks times longer than minutes might have allowed participants to locate and correct additional faults.
however a limit needed to be chosen and as discussed in section .
it affected only five instances of fault localization all on unreduced recordings and of fault correction five on unreduced and five on reduced recordings .
it seems likely that while longer limits might have affected differences in effectiveness they would only have accentuated differences in efficiency.
.
results .
.
efficiency figures .
a .
b and .
c use boxplots to present the amount of time required by participants to detect failures locate the underlying faults and correct those faults for each web application using both unreduced .long and reduced .short recordings.
the horizontal axes list the web applications and recording types and the vertical axes indicate time required.
the boxes thus indicate the distributions of times participants required to detect failures locate faults and correct faults across the three graphs respectively.
in the figures we use cumulative numbers to report the times required to locate a fault and correct it because these provide a more appropriate understanding of the total times required to reach the various debugging task checkpoints.
for example for c.long the mean time required to detect the failure was .
seconds the mean time to detect the failure and locate the fault that caused it was seconds and the mean time to detect the failure locate the fault and correct it was seconds.
also as noted earlier we report times only for cases in which participants were able to complete tasks.
for example the boxplot for d.long presents data for only the four participants who corrected the fault in that case.
overall using unreduced recordings participants required longer times to detect failures locate faults and correct them.
considering failure detection this is evident in the boxplots for all four applications.
across all four applications unreduced recordings led to a mean failure detection time of .
seconds with a standard devi table statistical results confidence intervals objdetect failurelocate fault cumul.fix fault cumul.locate fault non cumul.fix fault non cumul.
a b c d ation of .
.
in contrast reduced recordings led to a mean time of .
seconds with a standard deviation of .
.
when the cost of fault localization is added to that of failure identification using unreduced recordings participants again required more time in cases where they succeeded than when using reduced recordings on all four applications.
across all four applications unreduced recordings led to a mean time of .
seconds with a standard deviation of .
whereas reduced recordings led to a mean time of .
seconds with a standard deviation of .
.
when the cost of correcting faults is added on unreduced recordings led to a mean time of .
seconds with a standard deviation of .
.
in contrast reduced recordings led to a mean time of .
seconds with a standard deviation of .
.
hence the time required to repair faults was higher for unreduced recordings than reduced ones.
however the standard deviation was higher at the level of reduced recordings compared to unreduced ones the amounts of time needed to repair faults were spread out over a larger range of values with reduced recordings than with unreduced ones.
to determine whether the differences in times were statistically significant we conducted bootstrap tests on the data obtained for each program s unreduced and reduced recordings.
we chose this test because it is applicable to non normal data distributions and can be utilized on data sets of unequal sizes such as we have for most of our object web applications where fault localization and fault correction are concerned .
our null hypotheses were recording reduction does not increase programmer efficiency with respect to failure detection fault localization fault correction efficiency and we tested these hypotheses for each of the four applications at a confidence level of .
in addition for fault localization and fault correction we examined this hypothesis on both cumulative times and on the times required just for the specific tasks.
table presents the confidence intervals returned by the bootstrap tests.
in all cases these confidence intervals do not cover and in all cases p values were .
allowing us to reject the null hypotheses in all cases.
overall reduced recordings statistically significantly increased programmers efficiency in failure detection fault localization and fault correction considering time cumulatively or non cumulatively.
a failure detection correctness b fault localization correctness c fault correction correctness figure correctness results .
.
effectiveness figures .
a .
b and .
c present the numbers of failures detected faults located and faults corrected by the participants respectively for each of the web applications using both unreduced .long and reduced .short recordings.
the horizontal axes list the web applications and recording types and the vertical axes indicate numbers of faults failures.
the heights of bars indicate for each application the numbers of participants who detected failures located faults and repaired faults correctly across the three graphs.
as figure .
a shows all of the participants were able to detect each of the failures embedded within the applications they considered regardless of recording size.
as figure .
b shows all participants were able to locate the fault embedded within application a. applications b c and d follow a different pattern on these applications participants were not always able to correctly locate the faults.
however they were correct more often when using the reduced recordings.
as figure .
c shows where fault correction is concerned differences in results across recording sizes were even more apparent.
however keep in mind that only participants who located the faults could correct them.
on application b all participants who located the fault were able to correct it with both reduced and unreduced recordings and on application d two participants who located the fault were not able to correct it with reduced or unreduced recordings.
on the other hand for a.long five participants who located the fault were unable to correct it while for a.short only two who located the fault were unable to correct it and for c.long two participants who located the fault were unable to correct it while for c.short only one who located the fault could not correct it.
this suggests that programmers abilities to correct faults may be enhanced by reduced recordings.
since our effectiveness data is categorical and in two cases involves fewer than five data points we used fisher s exact test to assess whether the differences in effectiveness comparing unreduced and reduced recordings in the different tasks were statistically significant.
our null hypotheses were recording reduction does not increase programmer effectiveness with respect to failure detection fault localization fault correction efficiency and we tested these hypotheses for each of the four applications at a confidence level of .
in no case was statistical significance found.
thus we cannot conclude that the numerical differences observed in effectiveness were necessarily caused by reductions in recording size.
we canconclude however that programmers using short recordings were no less effective than those using long ones.
.
discussion we asked our participants to complete exit surveys to help us gain insights into their perceptions of the experience.
partici pants considered their background sufficient given the applications they used while only four felt that their background was not sufficient.
students considered the overall level of difficulty of the web applications average five rated it easy and three considered it difficult .
regarding the students ratings of their overall experience locating faults and fixing bugs considered the task to be of average difficulty three rated it as easy and three considered it difficult .
these numbers lead us to conclude that our participants and the web applications that we selected were overall appropriate for the study.
as previously noted the task of correcting faults involved a higher standard deviation in efficiency results for reduced recordings than for unreduced ones.
figure .
c reveals that participants tended to locate larger numbers of faults correctly when using reduced recordings.
the data corresponds however only to correct repair efforts.
thus the data represent larger numbers of values for reduced recordings than for unreduced recordings increasing the chances for observing variance in that data.
we also noted previously that all the participants were able to locate the fault in application a regardless of recording type reduced or unreduced .
however participants were not all able to correctly locate the faults embedded in applications b c and d. the fault embedded in application a is related to a code omission that could easily be visible to the participants.
however the faults in applications b c and d were related to existing code that was incorrect in a manner that was not necessarily obvious.
therefore locating faults in the latter applications was more challenging for participants.
even though participants were all able to locate the fault in application a they were not all able to repair it particularly when relying on application a s unreduced recording.
due to the omission of code in a the participants had to rewrite the omitted lines of code which was a more error prone operation than correcting lines of code that were already present but erroneous.
participants required less time to locate faults when using reduced recordings giving them more time to make progress towards comprehending and fixing the fault.
table illustrates the number of participants who reached the minute time limit allotted for the fault localization and fault correction stages of tasks.
a comparison of figure .
b with table reveals that no participants ran out of time while attempting to locate faults given reduced recordings though some were incorrect in their assessments of the faults.
in contrast several participants ran out of time when attempting to locate faults using unreduced recordings.
.
analysis of screen recordings the results of our second study show that delta debugging can substantially facilitate the debugging process by decreasing the time 339table current activity as task time limits were reached.
a.l a.s b.l b.s c.l c.s d.l d.s fault localization fault correction needed to locate and correct faults with no reduction in effectiveness.
however these results do not tell us why these effects were achieved.
to attempt to answer this question we analyzed the screen recordings made during the user study.
.
.
unreduced recordings on viewing the screen recordings obtained during the study we noticed that none of the participants provided with unreduced recordings appeared to have a clear idea initially about the input fields or the lines of code responsible for the failure.
they focused on the structure of the code as a whole and used that as a starting point for locating the fault and fixing it.
we were able to enumerate three overall patterns adopted by participants for tackling unreduced recordings.
approach undirected search.
several participants given unreduced recordings spent most of their time scrolling through the code looking for the code responsible for the fault.
some participants spent all available time this never actually locating the fault.
for instance one participant given an unreduced recording of the tic tac toe game application a that was unresponsive when the user clicks on squares within the user interface could not locate the code related to this unresponsiveness.
ultimately he ran out of time.
some participants combined the scrolling process with google searches.
they would scroll up and down considering code and each time they suspected that a portion of code might be incorrect they would perform a google search to verify their speculation.
provided with unreduced recordings however all such participants conducted google searches related to non faulty code.
for instance application b contained a failure related to an inappropriate dialog box.
faced with this failure one participant using an unreduced recording performed google searches that were related to other non faulty and fully functional dialog boxes.
approach code understanding based search.
a second group of participants adopted an approach that consisted of trying to understand the code as a whole and determining its structure prior to locating the fault.
first they attempted to partition the code into a set of blocks differentiating different roles.
one common behavior involved successively selecting specific areas of the code with the mouse.
other participants divided the code into blocks by adding new lines before and after each block in order to differentiate it from the rest of the code.
still others indented blocks of code in a certain manner and some inserted comments clearly separating the code into distinct parts.
we further divide these participants into those who after partitioning code in some fashion used one of two code understanding processes a incremental search and b directed search.
the incremental search approach involves using google searches to verify that each block of code is correct.
once the participants encountered a block of code that they believed was incorrect they focused on that block and attempted to correct it.
for instance application c involves a company arranging pickup information at the airport for a hotel s clients.
the failure is that the user can submit a request even if a phone number is input in the email text field.
one participant partitioned the code for this application into three distinct parts html css and javascript.
then he inserted comments in order to differentiate each block of code from the others.
he further subdivided the javascript portion of the code into three parts namely catching the exception throwing it and sending a confirmation message to the user.
the participant conducted google searches to ensure that the html and css blocks of code satisfied syntax requirements.
he did not detect any inconsistencies in those blocks so he focused his attention on the javascript portion of the code and conducted google searches about the javascript that is responsible for catching errors.
again the participant did not detect any inconsistencies so he conducted google searches about the second subpart of the javascript block that plays the role of throwing errors.
after checking the rules for catching and throwing errors the participant judged that one error was not caught appropriately which he correctly concluded was the fault.
in contrast the directed search approach consists of focusing the search effort directly on one specific block of code considering each line.
for example application d includes a form for renting cars in which one button is unresponsive to the user s mouse clicks.
after one participant replayed the recording he examined the code and divided it into three parts input manipulation button clicks and error handling.
the participant designated the button click portion of the code as being faulty and focused his attention on that block of code ignoring other parts of the code.
all the google searches made by the participant were related to the block of code designated as faulty.
after performing one search over one line of the faulty block the participant indicated correctly that line as being faulty and attempted to correct the fault.
approach exhaustive inspection.
a final group of participants adopted a different approach with unreduced recordings.
instead of directing their attention towards specific parts of the code they inspected each line of code individually to attempt to judge its correctness.
these participants performed many google searches regarding lines of code most of which were completely unrelated to the fault under consideration.
for instance on application b the application in which the failure involved an inappropriate dialog box output after a user clicked on the submit button one participant inspected every line of code even though some were completely unrelated to the submit button behavior.
furthermore the participant conducted a google search about each line of code and ended up running out of time.
.
.
reduced recordings when using reduced recordings participants appeared to have much clearer notions about which region of code to focus on.
they did not spend much time scrolling through the code and viewing its structure instead they directed their attention towards a specific area of the code and worked on that.
some participants also retained keywords after viewing the reduced recordings and performed code searches in order to locate lines embedding those keywords an approach not observed for participants given unreduced recordings.
also participants used google searches that were more directly focused on the occurrence of the fault and the failure.
for instance consider application a the tic tac toe game that was unresponsive to user clicks on squares.
one participant who was given a reduced recording revealing the associated failure directly focused on the area of code embedding the fault.
the participant concluded that the unresponsiveness of clicks was caused by an incorrect calculation at the code level.
also since the incorrect calculation was related to the variable square the participant further narrowed his field of vision by searching the code for the keyword square .
this participant was able to retain the name of the input field whose manipulation seemed to be causing the failure then searched for that keyword at the level of the application.
overall it appears that as postulated in the first paragraph of section .
participants who were given reduced recordings were 340table study objects app downloadsuser ratingsize loc language description and functionalities faq forge .
php css sql javascriptdocument management create faqs manual guides and howtos in a hierarchical structure time clock .
php sql javascripttime management track employee schedule track upcoming vacations manage sign in sheets school mate .
php css javascript sqlschool management admin manage classes users teachers manage grades assignments students access grades submit homework parent check student progress better able to focus on code and relevant keywords than those given unreduced recordings.
it further appears that this is primarily due to the reduction in extraneous information provided by the reduced recordings and the more easily observed connections between inputs that lead to failures and those failures themselves.
.
study scalability our third study considers the following research question rq3 to what extent is delta debugging scalable to large and complex web applications?
.
objects of analysis as objects of study we chose three non trivial applications that had been utilized in prior studies of techniques for testing web applications .
table provides details on the applications illustrating the numbers of time they have been downloaded their user ratings on a scale of to their sizes in non comment lines of code measured using cloc count lines of code 6the languages they utilize and the functionalities they provide.
the downloads and user ratings attest to the popularity of the objects.
we did not have access to actual faults for the web applications we selected so we asked an experienced web application programmer who was not involved in this work and had no information about our intended study to place faults in each of the web applications that were representative of actual faults he had found in practice and that resulted in detectable failures in the applications.
during the fault seeding process this programmer did uncover one actual fault in the first of the programs faq forge .
this seeding process resulted in the provision of six faults for faq forge five faults for school mate and three faults for time clock .
details on the specific faults are shown in table .
.
variables our independent and dependent variables and our process for obtaining values for them are the same as those reported in section .
for study .
.
threats to validity the threats to validity for this study are also similar to those for study with the added external threat to validity posed by our use of seeded faults.
however our use of much larger and more complex web applications does help address the threat to validity posed by our use of smaller applications in the earlier studies.
.
results and analysis table shows the differences in size and replay time observed for each recording before and after reduction.
as the data shows reduced recordings were all less than the size of full recordings.
the average size of full recordings was .
elements while the average size of reduced recordings was .
elements overall study faults app fault description faq forge1adding a 2nd page to an existing one causes both pages to display the content from the 2nd page 2from an existing topic with pages deleting page and adding a new page corrupts the page numbering currently displayed page is labelled page .
3deleting a topic does not result in an actual deletion although the user receives a message confirming the deletion 4creating a 3rd level child page is correctly shown on the current page but results into a file duplication in main 5creating a document followed by clicking on the view document link results into showing the page number as of .
school mate1dysfunctional change student association functionality 2the page number is not displayed on one of the web pages the page generates an error when adding a new term 4the student cannot register for a class although there are seats available 5an announcement is not deleted even if the user clicks on delete time clock1the hours worked report only provides hours rounded down without any decimal portion 2the note entered on login logout is not displayed under the notes column on the page 3the date is not being displayed in the top right corner of the timeclock.php page .
the size of full recordings .
the average replay time for full recordings was .
seconds while the average replay time for reduced recordings was .
seconds overall .
smaller than for full recordings .
the table also shows the total amount of time required by our implementation of delta debugging itself this time ranged from seconds to seconds about minutes with an average of .
seconds about eight minutes .
to determine whether delta debugging produced statistically significant reductions in size and replay time we conducted twotailed mann whitney u tests at significance level .
our null hypotheses were recording reduction does not reduce recording size and recording reduction does not reduce replay time .
the tests indicated that the reductions in size and replay time were both statistically significant with p values of .981e in both cases.
.
discussion the reductions in size and replay time observed in this study are actually larger than those observed in the first study on smaller applications.
reduced recordings do contain on average more elements than those used in the first study particularly on faqforge but they still do eliminate large numbers of elements.
of course we cannot claim without further study that this reduction will en341table recording size and replay time before and after reduction for large complex applications app faulttotal dd timesize elements replay time sec full rec.red.
rec.pct.full rec.red rec.pct.
faq forge1 school mate1 time clock1 able programmers to detect and correct faults more quickly but in view of our second study we are hopeful that it will.
the time required to run delta debugging itself is likely inflated by the fact that our implementation is a prototype and we have not focused on performance in creating it however the execution is fully automated and if it does enable developers to detect and correct faults more efficiently and effectively it is most likely worth it.
.
related work many techniques e.g.
attempt to detect failures at runtime using static or dynamic analyses.
many techniques rely on testing based approaches to detect failures and several such techniques have been explored in relation to web applications e.g.
.
there has also been work on approaches for reproducing failures e.g.
.
such techniques focus on detecting and reproducing failures whereas in this work we focus on helping debuggers localize faults from failures that are already known.
numerous approaches attempt to help developers localize faults in non web based applications using a wide variety of tactics e.g.
.
recent work has also considered fault localization for web application software.
for example ocariza et al.
provide an automated method for localizing dom related javascript faults through dynamic analysis and tracing and slicing of javascript code and artzi et al.
provide a technique for performing fault localization on dynamic php web applications.
the primary difference between these approaches and the approach we present here involves our focus on behavioral recordings and recording reduction.
there has also been work on facilitating the debugging process from the human standpoint.
whyline facilitates debugging by presenting the programmer with an interface that allows them to formulate questions about a program s behavior.
vejovis helps debuggers by suggesting fixes for dom related javascript faults.
our work shares their goal of helping debuggers in their tasks.
strategies closely related to ours are those that attempt to minimize program input program state or test cases that produce failures.
regehr et al.
use test case reduction to detect c compiler bugs and lei and andrews minimize randomly generated failing tests.
most closely related to this work given that we draw on its algorithm is delta debugging which has been utilized in several different scenarios.
zeller and hildebrandt use delta debugging to simplify and isolate input that reproduces failures.
zeller also adapts delta debugging to narrow down failurecauses involving program state.
cleve and zeller use delta debugging to create minimal test cases that produce failures.
burger and zeller minimize java programs focusing on object interactions.
none of these approaches considers web applications or involves recording replaying and reducing recordings at the level of program behavior that we consider in this work we show how delta debugging can be used to reduce recordings of web applications.
there has been considerable research on record replay approaches.
many such approaches have been explored in the context of nonweb software e.g.
without considering recording reduction or targeting web applications.
clause and orso present a technique for recording and replaying program executions that does perform reduction with the aim of helping replicate and correct code that fails in the field.
this technique focuses on compiled code in non web applications and on tracking interactions in the code by intercepting various i o streams.
in contrast we focus on user interactions with web applications captured by tools that monitor information related to events and dom trees.
there has been some research on record replay infrastructures for web based software.
sen et al.
present jalangi a program analysis framework for javascript that incorporates record replay.
andrica and candea present warr a tool that records and replays the interaction between users and web applications.
mickens et al.
present mugshot which allows deterministic replay of javascript programs.
burg et al.
present dolos a deterministic record replay infrastructure for web applications that captures not only user inputs but also network callbacks.
during playback captured inputs are re delivered to the browser engine but new live inputs are suppressed.
asynchronous events such as animation timers and futures are captured and replayed using hooks inside the browser engine.
return values of nondeterministic functions that provide javascript with access to persistent state browser cookies local storage etc.
and environmental data current time screen size etc.
are memoized.
none of this work however has considered recording reduction.
.
conclusions and future work we have presented an approach for recording reduction based on delta debugging that operates on recordings of web applications.
we presented empirical evidence that our approach can significantly reduce the size and replay time of recordings that reduced recordings help programmers debug more cost effectively and that the approach itself can scale to larger more complex applications.
given the results of our studies we intend to next explore the application of the approach in conjunction with deterministic record replay infrastructures and with alternative oracle types.
we also intend to study reduction algorithms that analyze dependencies or incorporate such analyses into the delta debugging approach.
finally we plan to extend the scope of our empirical studies.
.