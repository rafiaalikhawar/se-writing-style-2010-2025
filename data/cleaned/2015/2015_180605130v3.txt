detecting speech act types in developer q uestion answer conversations during bug repair andrew wood university of notre dame notre dame indiana awood7 nd.edupaige rodeghero clemson university clemson south carolina prodegh clemson.edu ameer armaly google mountain view california aarmaly nd.educollin mcmillan university of notre dame notre dame indiana cmc nd.edu abstract t his paper targets the problem of speech act detection in conversations about bug repair.
we conduct a wizard of oz experiment with professional programmers in which the programmers f ix bugs for two hours and use a simulated virtual assistant for help.
t hen we use an open coding manual annotation procedure to identify the speech act types in the conversations.
finally we train and evaluate a supervised learning algorithm to automatically detect the speech act types in the conversations.
in two hour conversations we made annotations and uncovered speech act types.
our automated detection achieved precision and recall.
t he key application of this work is to advance the state of the art for virtual assistants in so f tware engineering.
virtual assistant technology is growing rapidly though applications in so f tware engineering are behind those in other areas largely due to a lack of relevant data and experiments.
t his paper targets this problem in the area of developer q a conversations about bug repair.
ccs concepts so f tware and its engineering !maintaining so f tware keywords speech acts virtual assistant bug repair classi f ication acm reference format andrew wood paige rodeghero ameer armaly and collin mcmillan.
.
detecting speech act types in developer q uestion answer conversations during bug repair.
in proceedings of 12th joint meeting of the european so f tware engineering conference and the acm sigsoft symposium on the foundations of so f tware engineering lake buena vista florida usa nov. esec fse pages.
.
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro f it or commercial advantage and that copies bear this notice and the full citation on the f irst page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author s .
esec fse lake buena vista florida usa copyright held by the owner author s .
.
.
.
.
.
introduction speech acts are spoken or wri t ten actions meant to accomplish a task .
a classic example of a speech act is i now pronounce you husband and wife the speech itself is an action with consequences .
naturally most speech acts in life are less impactful let s go to the movies or please tell me how to f ind my classroom though the principle is the same.
speech acts are key components of conversations that guide the what the speakers do.
while research in sociology has studied speech acts for decades there has been an increase in interest due to the growth of virtual assistants.
virtual assistants such as cortana google now siri etc.
try to carry on a conversation with a human to try to serve that person s request asking for a restaurant recommendation or the time of day.
and while human conversation can seem effortless at times in fact there are several key steps that we do without even being aware we detect when speech acts occur we comprehend the speech act as being a particular type of act e.g.
an information request a command a clari f ication and cra f t an appropriate response.
we understand naturally that the type of act will depend on the context of the conversation and that a piece of dialog may be of more than one type.
virtual assistants must be carefully designed to mimic this process the f irst step is to detect speech acts and classify them by type.
designing a virtual assistant to detect and classify speech acts requires examples of conversations from which to learn what those speech acts are.
t hese conversations must be related to the task for which the assistant is being designed.
for example a study by whi t taker et.
al targets dialog systems for restaurant recommendations and therefore collects examples of conversations in which a human asks for restaurant recommendations.
kerly et.
al targets automated tutoring systems and to do so collects examples of tutoring sessions for a speci f ic subject area.
t he data collected for one application domain is generally not applicable to other domains.
one key accepted strategy for collecting examples of conversations is a user simulation in a wizard of oz experiment .
in a wizard of oz experiment human participants interact with a machine that the participants believe to be automated.
in reality the machine is controlled by human experimenters.
t he participants are asked to use the machine for a particular purpose e.g.
arxiv .05130v3 jul 2018esec fse nov. lake buena vista florida usa andrew wood paige rodeghero ameer armaly and collin mcmillan they ask for a restaurant recommendation .
t he idea is that the experimenters can collect simulated conversations that closely re f lect real world use.
analysis of the conversations reveals what speech acts the participants make and clues as to how to detect them.
today virtual assistants are possible due to major efforts in understanding human conversation though these efforts have largely been con f ined to everyday tasks.
while virtual assistants for so f tware engineering have been envisioned for decades progress is limited largely due to three problems that we target in this paper there are very few experiments with data released of so f tware engineering conversations the speech act types that so f tware engineers make are not described in the relevant literature and there are no algorithms to automatically detect speech acts.
in this paper we conduct a wizard of oz experiment in the context of bug repair.
we then manually annotate the data from this experiment to f ind the speech act types and build and evaluate a detector for these speech acts in conversations.
our target problem domain is a virtual assistant to help programmers during bug repair.
we chose bug repair because it is a common so f tware engineering task and because as previous studies have shown bug repair is a situation in which programmers are likely to ask questions .
we recruited professional programmers to f ix bugs for two hours each while providing an interface to a wizard of oz simulated virtual assistant.
t he programmers interacted with the simulated virtual assistant for help on the debugging task.
we then manually annotated each conversation with speech act types in an open coding procedure see section .
finally we trained a learning algorithm to detect speech acts in the user s side of the conversations and evaluated its performance sections .
across two hour conversations we made annotations and discovered speech act types.
our automated speech act detection algorithm achieved an average of precision and recall.
by releasing this corpus we contribute one of very few woz corpora which are especially rare in the domain of so f tware engineering .
we release all data including conversations annotations and our detection algorithm source code via an online appendix section to promote reproducibility and assist future research in so f tware engineering virtual agents.
problem significance scope t he problem we target in this paper is that models of developer conversations are not described in the literature.
certainly strong efforts in the area of program comprehension have made inroads into our understanding of the types of information that programmers need and how programmers make sense of so f tware problems.
however the nuts and bolts of actual conversations among programmers are still not well understood.
a key component of those nuts and bolts are speech acts as de f ined in the previous section and our goal is to automatically detect these speech acts in conversations.
but detection of speech acts is useful beyond pure academic interest advancements in programmer tool support depend on improved detection of programmer intent.
numerous so f tware engineering tools depend on natural language interfaces such as code search engines navigation tools traceability tools and our target context of automated virtual assistant technology.
t he situation we envision is that a programmerasks an automated virtual assistant a question in lieu of a fellow human programmer and the virtual assistant is expected to provide an answer to that question.
a fundamental part of answering these questions is to detect the types of statements comments etc.
that programmers make when asking and clarifying their questions.
t hroughout this paper we refer to a book by rieser and lemon as both motivation for and rationale behind our work.
t he book provides an excellent summary of the design decisions required for building dialog systems and re f lects the signi f icant momentum in years of research on virtual agents one key theme is that using wizard of oz studies to inform data driven dialog system construction is a highly effective strategy.
t hey point out that while it is possible to design a virtual assistant using manuallycra f ted assumptions about user behavior the existence of annotated simulated dialog via a woz study provides an immense boost to the f lexibility and effectiveness of virtual agent design.
one bene f it is from the increased knowledge scientists gain from studying the dialog while another bene f it is from the ability to use supervised and reinforcement learning algorithms to teach the computer correct behavior even with relatively sparse data.
in this paper we contribute the dataset our manual annotation of the dataset and our analysis of those annotations to the community as a foundation for building be t ter so f tware engineering virtual agents.
t his contribution alone is signi f icant considering that a recent survey by serban et al.
found only four publicly available woz datasets more are held privately suitable for building dialog systems and none related to so f tware engineering.
however we take a further step towards a working virtual agent by building a classi f ier to automatically label the dataset in essence this is a detector for speech act type using supervised learning as chapter of highlights supervised learning is o f ten the f irst technique tried for speech act type detection prior to resorting to more complex approaches .
note that in our manual annotation process we annotated the entire conversation both madeline s and the study participants side .
however during the speech act type detection we only predict the type of speech acts from the participants side of the conversation.
t his is because during the manual annotation process we study not only the participants but the wizards actions also this is for the purpose of laying a groundwork for conversation f low analysis in future work in addition to the academic interest presented in this paper.
but during speech act detection the realistic scenario is that a virtual assistant would never need to classify its own conversation since it would already know the speech act types it generated itself.
it would only need to detect the speech act type of the human user.
background t his section describes four key technologies related to and underpinning our work in this paper automated virtual assistants conversation analysis and modeling studies of program comprehension and text classi f ication.
.
automated virtual assistants automated virtual assistants such as siri cortana and google now are claiming an increasing role in computing for everyday tasks.
t hey simplify duties such as planning meals and f inding music detecting speech act types during developer q a esec fse nov. lake buena vista florida usa and are part of a broader trend towards automated productivity services.
virtual assistants for so f tware engineering have been envisioned for decades with the dream being a system that can mimic the answers that human teammates would give such as a system able to generate on demand developer documentation as responses to source code queries .
we the so f tware engineering research community are still far away from this dream.
nevertheless advancements are being made in that direction.
recently bradley et al.
built devy a virtual agent to help automate programmer tasks.
devy differs from our work in that we seek to understand the structure of programmers conversations to build a system to help programmers learn and recall information rather than automate tasks.
pruski et al.
created tiqi a technique that answers database query questions in the form of unstructured dialog.
ko and myers created whyline which answers questions about program output.
escobaravila et al.
answered unstructured questions by connecting questions to so f tware engineering video tutorials.
a majority of current efforts focus on understanding unstructured so f tware engineering data for a more complete survey we direct readers to arnaoudova et al.
.
but what holds back progress at the moment is an incomplete understanding of how programmers communicate it is not possible to build a tool that participates in this communication without understanding the nature of that communication.
t his understanding can only be completed with conversation analysis and modeling.
.
conversation analysis and modeling conversation analysis and modeling is the task of extracting meaning from human wri t ten or verbal communication.
it usually involves creating a representation of a type of conversation e.g.
restaurant recommendations or technical support calls and then using that representation to predict the f low of the conversation.
a f low of a conversation is how people tend to put information in conversations for example one conversation participant asking does that make sense?
if the other participant is silent a f ter receiving new information.
conversations are typically broken up byturns .
a turn begins every time a speaker begins speaking and can encompass multiple sentences.
conversation analysis and modeling is what allows automated virtual assistants to create human like conversations.
conversation modeling has its roots in sociology and psychology where researchers studied the factors behind conversation f low and form.
t hese o f ten employ qualitative analysis methods to isolate human factors such as social rank or fatigue.
a f ter a signi f icant investment in the 1990s quantitative analysis procedures have been developed to model and predict the types of information that human conversations include in order to create interactive dialog systems.
work in this area has f lourished with representative work including .
for example work by lemon models restaurant recommendation conversations as a markov decision process in which each turn is one of six possible states.
a typical strategy in conversation modeling for discovering speech acts is user simulation in which participants in a study are told that they are interacting with a dialog system which is actually a human acting like a dialog system via a chat program .
t he simulation results in a transcript of a conversation between a human participant and an idealized virtual assistant simulated by the researcher .
t he transcript is an extremely valuable source of information on how the human participant expects to interact with a machine and how the machine should respond.
while rare in so f tware engineering these studies are not unheard of goodrum et al.
perform a woz study to discover what requirements knowledge programmers need related conceptually to requirements gathering woz studies proposed earlier .
.
studies of program comprehension t his paper could be broadly classi f ied as a study in program comprehension how programmers comprehend and communicate about so f tware development and behavior.
typically questions asked by program comprehension literature relate to the mental and physical processes that developers follow .
examples of mental processes include targeting how code is connected .
physical processes include taking of notes and pa t terns of movements of the eyes .
notably roehm et al.
point out that programmers try to avoid program comprehension and look for short cuts whenever possible.
t his f inding is in line with several others that suggest that tool support for comprehension should provide information incrementally and at as high a level as possible and avoid too many low level details .
our vision in this paper is to build a foundation for so f tware engineering virtual assistants to provide information in the order and at the time requested by programmers during a dialog.
.
text classi f ication text classi f ication is an intensely studied area in machine learning and text classi f ication techniques have seen extensive use in so f tware engineering.
a recent book by aggarwal and zhai surveys text classi f ication and mining techniques generally.
so f tware engineering applications are so prevalent that we cannot list them all here though representative examples include .
we use text classi f ication as a component of our speech act detection.
user simulations in this section we describe our user simulation study.
in general a user simulation is an imitation of a conversation between a human and a machine instead of a real machine a researcher stands in for the machine without the human being aware of it .
in this paper our user simulation is the interaction between our participants and an imitated so f tware program.
participants believed the program could automatically assist programmers with tasks.
t hey were informed their participation in this study was helping to improve a virtual assistant program for programmers.
however there was no actual virtual assistant producing answers to the questions asked by the participants.
we manually answered every question.
.
methodology we based our methodology on previous studies of bug repair in so f tware engineering and previous wizard of oz studies in sociology .
we asked the programmers to remotely participate in the study using a provided ubuntu bit virtual machine and the microso f t skype application on their local machine.
we instructed the participants to f ix bugs from pre installed open source javaesec fse nov. lake buena vista florida usa andrew wood paige rodeghero ameer armaly and collin mcmillan projects contained within an eclipse ide workspace on the provided virtual machine.
we instructed the participants to f ix as many bugs as they could within a pre de f ined two hour time frame.
during that time we gave the participants one bug at a time one bug per project.
we asked the participants to avoid using the internet to search for solutions or answers to any questions that they might have and to instead direct their questions to a automated virtual assistant named madeline through the skype platform.
note that this involved two key design decisions informed by rieser and lemon s guide on woz studies for dialog systems chapter of first we used the wri t ten text chat only no voice to limit the scope of the study to developer q a conversations instead of introducing the possibility of voice transcription errors it is necessary to deliberately add noise to woz studies involving voice to simulate actual noise and we felt this would add too many variables considering the already complicated nature of debugging .
second we restricted access to internet resources.
while this may seem to create an unrealistic situation since programmers frequently use stackover f low etc.
it was necessary in order to learn how programmers might use a virtual agent due to a bias in which people might not try a new technology simply because it is unfamiliar and to avoid biases introduced by external tools.
t hese restrictions are o f ten a necessary evil in woz experiments for example of papers surveyed by riek placed substantive restrictions on participant behavior and resources.
during each study two to three of the authors collaborated at all times to respond to the participants.
at least one of the authors had previously f ixed the bugs given to the participants.
t his allowed for quick and intelligent responses to the participants giving the illusion that madeline produced responses automatically.
t his deception typical of the wizard of oz strategy was necessary to ensure the authenticity of the responses.
t he participants were explicitly told that they were communicating with an automated system supervised by humans madeline .
t he participants were told to interact with madeline through skype conversations and also to share their screens for quality assurance purposes.
in reality screen sharing provided the means to prepare responses in real time and was critical for imitating a fully autonomous system.
following rieser and lemon s woz process for dialog systems again chapter of we did not restrict wizards to a script or set of prede f ined speech act types since a goal of our study was to understand what the programmers needed rather than test a prede f ined script.
.
participants we recruited professional programmers to participate in our study.
t hese programmers were recruited through email and an online freelance website called upwork .
t he programmers work at various companies such as ibm golfnow and hyland so f tware while some work as freelancers full time.
note that the programmers recruited are not students but professionals working in industry.
each programmer recruited had familiarity with java before participating in the study.
overall the participants had an average of .
years of experience with java.
t he maximum number of years of java experience was and the minimum was one.
.
t hreats to validity as with most studies this project has a few threats to validity.
first since each experiment was two hours long not including any technical problems it is possible that the participants experienced fatigue.
t his is compounded with any fatigue that they already experienced from their normal work schedule.
t his was mitigated by using a large pool of participants.
another threat came from technical problems with screen sharing.
t he only issue with this however was a possible reduction in response speed but we saw no noticeable reductions in any of the studies.
either through technical problems or participants forge t ting to save them a few screen shares were unable to be stored.
however these stored recording were not actually used in analysis.
finally another threat to validity was our lack of control over whether participants actually refrained from searching for answers over the internet rather than asking our simulated virtual assistant.
participants could have used another device to search the web.
we did not notice any extended lapses in questions or work time from any participants though so we believe most participants followed our searching instructions correctly.
project name bug report t he game board becomes unresponsive.
public gamepane int size basepane basepane this.size size this.basepane basepane setscore this.tilesize tilesizes this.movetime size setprefsize size tilesize size tilesize setlayoutx size tilesize setlayouty size tilesize setstyle quotedbl.var fx background color ffffff quotedbl.var addtile ... thread focusfield new thread new runnable override public void run while !thread.currentthread .isinterrupted if !isfocused try thread.sleep catch interruptedexception e e.printstacktrace requestfocus focusfield.setdaemon true focusfield.start figure a description of a bug in the project with source code.
participants received full copies of the source code however parts have been omitted for space limitations in this f igure.detecting speech act types during developer q a esec fse nov. lake buena vista florida usa .
data collection we provided each participant with an ubuntu bit virtual machine.
we asked the participants to download the virtual machine ahead of the study.
inside the virtual machine we provided eclipse with all the buggy projects.
we also provided a screen recording program called simplescreenrecorder .
we asked each participant to start the screen recording at the beginning of the study and leave the so f tware running until the study had completed.
t he participants then saved and sent the screen recording f ile to us.
we then collected the skype transcript created by the study and removed all identifying information from the conversations.
some participants also sent back the project f iles of the f ixed bugs but these f iles were not used in our analysis.
.
bugs t he java bugs come from different open source projects.
t he project domains include a pacman game a calender application and a pdf f ile merger.
we also selected bugs from commonly used java libraries such as opencsv and apache commons io .
we chose the bugs based on four criteria t he bugs had to be simple enough that they could be solved in a few hours but complicated enough to take at least minutes to solve.
we had to be able to understand the bugs well enough to give meaningful answers to questions during the simulation.
t he user had to be able to reproduce the bug easily.
t he bugs had to be solvable without obscure domain knowledge.
all of the bugs were previously solved and we had the actual solutions on hand throughout each study.
however we also discussed other solutions to the bugs before the user simulations.
t his is because some of the bugs could be f ixed in a variety of ways.
t he bugs were presented individually and randomized for each study.
an example of a bug given to the participants is as follows t he bug in the source code above occurs when a user tries to make a move with the arrow keys.
t he source of the bug is the result of an incorrect fusion of a third party library used for graphics javafx and the structural design of the project.
t he project contains multiple panes which house bu t tons performing different types of actions.
for the sake of simplicity consider there to be only two panes one that displays the board and is controlled by the arrow keys the game pane and another that allows users to save and load games the f ile pane .
both of these panes are vying for focus but for the game to be played the game pane must always have focus.
to ensure this the project s implementation spawns a deamon thread that almost constantly requests focus for the game pane.
t he bug comes from the fact that javafx only allows for one thread called the event thread to make changes to the ui.
when creating the deamon thread the developer uses the t hread type to request focus which javafx interprets as modifying the ui.
t his causes an exception to be raised and for the game to become unresponsive to the arrow keys.
one solution to this bug is to use javafx safe data types to perform the action of the deamon thread.
during studies participants were only provided with the buggy projects and the bug description.we pretending to be madeline while aware of solutions would in no form give a solution to the participants but would only react to questions asked.
participants were incentivized to search the source project for the f iles containing bugs as questions designed to tease solutions out of madeline were met with vague and unhelpful responses i.e.
i am unsure .
a complete list of bugs can be found at our online appendix see section .
.
experiences lessons learned in this section we discuss our experiences and lessons learned while conducting the user simulation study.
we do this to provide some guidance to so f tware engineering researchers who might do studies similar to ours in the future.
one of the biggest lessons we learned was to con f irm that the virtual machine we provided worked on the participant s machine before the study started.
in roughly half of the studies we found ourselves f ixing problems on the participants machines and spending on average an extra minutes f ixing the issues.
t his was problematic as the studies took up more time than originally anticipated which threw off our original study schedule.
we also learned that additional information should be advertised beyond the scope of the study to allow for smooth experiments such as experience with virtual machines or experience with the eclipse ide.
another lesson learned was how to effectively schedule remote studies.
many participants were unable to participate in the study until they returned home from their jobs in the evening.
some had families and wanted to participate in the study even later once their children were in bed.
many of our participants were in different time zones there were days where we would schedule studies at am pm and pm in our time zone.
we learned over time to hire participants overseas where their evening was our work day.
annotations in this section we describe our process for annotating the speech acts from the data collected during the user simulation studies see section .
.
essentially our goal is to determine what the speech acts are and to determine what parts of the conversations are associated with those speech act types.
we also discuss our research questions the rationale for asking them and provide annotation examples.
.
research q uestions t he research objective of this section is to determine how programmers would use a virtual assistant to assist them in f ixing a source code bug.
we seek to see what types of questions programmers would ask a virtual assistant and if those types of questions are consistent across multiple programmers.
rq1do different programmers ask the virtual assistant similar questions for the same bugs?
rq2what types of questions did programmers ask during bug repair?
rq3what type of questions did programmers most frequently ask?
t he rationale behind rq1is that if programmers ask for help and if they ask similar questions for the same bug it is possible to create aesec fse nov. lake buena vista florida usa andrew wood paige rodeghero ameer armaly and collin mcmillan figure t he annotation labels of all transcripts and the occurrences for each label.
each turn can have multiple labels speech act type .
speech acts classi f ication system given training data.
we group the questions to create labels for the training data in rq2.
finally we investigate the most common uses of a potential virtual assistant inrq3to advise future virtual assistant implementations.
.
methodology we used a manual open coding qualitative analysis process adapted from the social sciences to create labels for the conversations we collected.
t hough for the purposes of this paper we follow rastkar et al.
in referring to coding as annotating to prevent conceptual con f licts between sociological coding and computer coding.
q ualitative annotation is becoming more common in so f tware engineering literature and it is important to recognize that while standards and principles exist the nature of qualitative analysis is that each annotation procedure is slightly different based on the needs and circumstances of the study.
in this paper we followed the recommendations of rieser and lemon in a book on creating dialog systems from woz data with one exception noted below.
a metaphor for open coding is unsupervised learning in that the human annotators do not begin with a set of labels our goal is to discover those labels from the data and then assign them to the turns in our data.
practically speaking we did this in three rounds.
t he f irst round of annotation consisted of label creation where we created labels as we saw f it and did not have a predetermined list to choose from.
t he second round consisted of label pruning where we decided what labels could be safely removed or merged.
t he second round became necessary the more progress was made in the f irst round and was due to the complexity of compressing sometimes vague and complex english text down into its major concepts.
t he result of label pruning was a set of well de f ined and disjoint descriptions of english text describing our examples.
t he third and f inal stage of annotating involved reexamining the annotations but instead searching for spelling errors or other small mistakes.
t his round had the effect of ensuring labels were consistent and resolving labels that represented the same concept but used different terminology i.e.
synonyms or were spelled incorrectly.
during any annotation process and especially an open process in which we do not begin with labels the bias of the human annotator becomes a major concern.
t he degree of bias is known asthe reliability of the data and it is an extremely controversial research topic.
one possibility is to follow the lead of carle t ta in calculating kappa agreement from multiple annotators and only accepting agreement above a certain threshold if agreement cannot be achieved the argument goes then more annotators are necessary.
while this is a common procedure it is by no means universally accepted.
as craggs and mcgee wood point out one must decide for oneself based on the intended use of scheme whether the observed level of agreement is sufficient .
likewise they suggest that if a coding scheme is to be used to generate data from which a system will learn to perform similar coding then we should be unwilling to rely on imperfect data .
at the same time it is not an option to merely add more and more annotators until agreement is achieved.
t here has long been a recognized split between expert and naive annotators .
it is not proper to allow naive annotators to have majority rule over the experts.
to be an expert annotator in our study a person would need to have knowledge of the bugs solved in our study so they can understand the conversations and not been a participant in the study.
only the f irst and second authors were both quali f ied and available manual annotation is weeks of effort .
rieser and lemon faced a similar situation and solved it by having a discussion between two annotators for all disagreements followed by independent decision making and calculation of kappa page of .
we differ from this procedure in that we consider our situation to be more unwilling to rely on imperfect data due to the fact that our research questions in section .
and our prediction training in section could be affected by errors.
t herefore for this paper we had two experts annotate all data and solve every disagreement through discussion as disagreements occurred followed by mutual decision making resulting in one set of annotations.
while this mutual process makes it impossible to calculate a reliability metric we felt it was more important to maximize correctness of the annotations.
annotations results in this section we present the results from our annotation process.
we also provide annotation examples following the results.
we note that the programmers asked on average .
questions throughout the two hour user simulation study.
a select few did not ask more than three however these participants were outliers.
t he highestdetecting speech act types during developer q a esec fse nov. lake buena vista florida usa number of questions asked during a user simulation study was and the lowest number of questions asked during a study was .
.
rq1 programmers asking similar questions we found that programmers asked similar questions to one another.
of all the questions asked by the programmers the ones that were consistent across the majority of participants included con f irmation q uestions clari f ication q uestions and api q uestions.
of these three types of questions clari f ication q uestion was asked the most by all programmers.
it was asked a total of times which comprised .
of all questions asked by programmers.
t here were various types of clari f ication questions asked.
some of the clari f ication questions included questions about what the bug report said what questions madeline could and could not answer and clarifying answers from madeline.
t he participants also asked clari f ication questions to con f irm their understanding of the task that they were to complete for the study.
.
rq2 types of questions being asked we found that programmers asked a variety of questions that ranged from system type questions to api and documentation types.
an example of an api question is quotedbl.varwhat methods are in eventyhandler ?
quotedbl.var we also found many programmers asked implementation questions quotedbl.varwhat are valid values for the int direction in pacplayer.java?
quotedbl.var a f ter f inishing the annotation process we were able to narrow down the question annotation types into categories.
t he categories are syntax parameter documentation api clari f ication implementation bug report con f irmation clari f ication and system questions.
figure lists the number of occurrences for each of the speech act types.
in section .
we go into detail with a short example of an annotated conversation.
we also provide all of the annotations on our online appendix see section .
.
rq3 most frequent questions being asked we found programmers asked a few questions signi f icantly more than others.
in figure the speech act type statement has the most occurrences.
we would like to point out that there was another more popular type of question the setup speech act.
since this speech act type is not relevant to the study itself this speech act type was removed from our corpus.
clari f ication q uestion has the highest occurrence out of any question type.
t his label appeared times throughout all transcripts.
many of the participants asked clari f ication questions on the bugs and on the responses madeline gave.
madeline asked clari f ication questions as well when we needed more information from a participant to answer a question.
sometimes the participants would ask questions that needed more detail so that madeline could answer the question.
t he second highest occurrence annotation label for a question type was apiquestion.
t his label occurred times in the transcripts.
t his makes sense as programmers were not allowed to use the internet during the bug repair task and were unfamiliar with the given source code.
.
annotation examples we annotated over two thousand speech acts during the annotation process.
to further explain the previous sections we provide an example of one of the annotations.
t hroughout the data participants asked api questions documentation and implementation questions.
below is a section of a developer conversation.
t his section of the conversation includes implementation questions and clari f ication questions.
at the end of each speech act there is the annotation label for that speech act.
t he annotation is in bold text and is in brackets.
t he speech acts begin with p or m denoting the speaker as a participant or madeline virtual assistant respectively.
p so the bug is that the pacplayer does not face right when the key is released but it is supposed to?
m yes.
he also disappears.
p does he disappear because the alive bool is set to false at the wrong time m i am unsure t hroughout the annotation process we found similar results to the previous example.
however we found programmers asked varying amounts of questions throughout the bug repair task.
t his was evident once deep into the annotation process.
it appeared that the more senior a participant was the less the participant asked for help from the virtual assistant.
t here are three interpretations we derive from these observations.
first the programmers possibly did not want to ask for help and instead wanted to solve the bug without help.
second it is possible that the programmers did not feel comfortable asking questions.
finally the programmers may have assumed that there was no automated virtual assistant and therefore did not ask questions.
we found that programmers o f ten made a statement before asking a question.
it appeared the participants were explaining their thought process before asking a question.
t his occurred about of the time in the user simulation studies.
an example of this is participant f irst i tried sudo apt get install default jre participant it told me it depends on default jreheadless and openjdk jre participant is it possible to set a command line argument for start up of the program?
here the participant makes multiple statements before asking madeline a question.
we did not ask participants to think aloud during this study.
however we observed this phenomenon throughout the user simulations and annotation process.
predicting speech act type our approach for predicting the speech act type is essentially a text classi f ier based on logistic regression.
recall the use case that we envision in section a virtual assistant receives a message and needs to classify that message into one of several categories so thatesec fse nov. lake buena vista florida usa andrew wood paige rodeghero ameer armaly and collin mcmillan it can respond appropriately.
our idea is to train a prediction model then use that prediction model to classify incoming messages.
.
labeled training data supervised machine learning algorithms depend on labeled training data.
we use the labels from section .
.
in that section we manually annotated every turn in every conversation as belonging to one of the speech act types we identi f ied.
in this section we use that data however only turns from the participants side of the conversation not madeline s to match the use case of the virtual agent classifying incoming messages to train a classi f ier to annotate the turns automatically.
note that this is a multi label classi f ication problem because an example consists of a turn annotated with a list of all the speech act types to which that turn belongs.
each speech act turn type is a label so each turn may belong to many labels.
.
attributes we use two types of a t tributes.
first we treat the problem as text classi f ication where each word is an a t tribute.
we calculate the a t tributes as a binary bag of words .
for each example the set of attributes includes either a one or zero for each word depending on if that word occurs in the text of the turn or not.
recent industry track papers came to the conclusion that to maximize potential industrial impact researchers should prioritize simplicity and only move to more complex approaches when absolutely necessary.
we stuck to binary bag of words for this reason.
we also did not do stop word removal or stemming.
we defer word count normalization e.g.
tf idf nlp based solutions advanced preprocessing techniques etc.
to our future work.
as we will explain in section .
the simple approach already achieves reasonable performance.
second we used three shallow features identi f ied by related literature .
t his related literature actually identi f ies over twenty shallow features that complement or replace text classi f ication but many of these are not applicable in our context.
for example many rely on computing entropy over a whole conversation a f ter the fact.
t hat is not possible in our context because we can only know incoming message and previous messages not future messages.
t he three features we used are slen the number of words in the message normalized over all previous messages wc the number of words not normalized and ppau the number of seconds between the message and the previous message.
.
smote we use smote to overcome the problem of unbalanced data.
some of the user speech acts we identi f ied only have a few examples e.g.
we only found eight examples for the parameterquestion type .
t hat presents a problem because the learning process will inevitably classify no turns in that type and still seem to achieve very high accuracy.
smote works by synthesizing examples in small classes from the known examples in those classes.
t he result is that the small classes are f illed with synthesized examples until the data are balanced.
smote has been widely used to resolve situations of unbalanced data generally as well as conversational analysis .
in pilot studies we compared smote to duplicative oversampling and observed slight performance improvements usingsmote.
we used smote only on the training data to avoid biasing the testing set.
.
prediction models we trained a multi label prediction model using the binary relevance procedure.
t he procedure is to create one binary classi f ier for every class.
we used the logistic regression lr algorithm to create each classi f ier.
we also tested naive bayes and support vector machines in pilot studies lr had superior performance to naive bayes and the difference between lr and svm was so slight as to not be worth the much longer training time for svm eight hours versus four minutes .
note that while we built a multi label prediction model we calculated smote using a multi class structure.
t hat is we ran smote once for each category then trained each classi f ier then combined the classi f iers with the binary relevance procedure.
in theory it is possible to run smote in a multi label con f iguration by executing smote on every combination of labels.
however this would necessitate nnruns of smote for ncategories which would be far more expensive.
we also performed parameter tuning for logistic regression across twelve parameters and naive bayes across four parameters.
parameter tuning has been recommended generally when working with se data .
due to space requirements we direct readers to our online appendix and reproducibility package for complete details see section .
.
implementation details we used the toolkit scikit learn to implement our classi f iers and smote imblearn.over sampling.smote .
we implemented the shallow a t tribute calculators ourselves using related work as a guide .
t he hardware was an hp z640 workstation with an e1630v3 cpu and 64gb of memory.
for total clarity we make all implementation scripts and datasets available via our online appendix see section .
evaluation of predictions t his section describes our evaluation of the prediction models we create.
essentially we use a fold cross validation procedure to test the quality of the predictions as well as explore where the predictions are most accurate.
.
research q uestions our research objective is to determine what level of performance we can expect from the prediction models as well as to understand which speech acts are easiest to detect.
rq4what is the performance of our prediction models overall in the multi label con f iguration according to the metrics described in section .
?
rq5for which speech acts do the prediction models have the highest performance?
rq6which a t tributes are the most informative?
t he rationale behind rq 4lies in the application we intend in section the performance of a virtual assistant will be limited by its ability to detect what type of speech act to which an incoming message belongs.
while we do not expect perfect performance wedetecting speech act types during developer q a esec fse nov. lake buena vista florida usa need to at least have an understanding of how much inaccuracy may stem from the detection process.
t he rationale behind rq is similar.
some speech acts are bound to be easier to detect than others.
it is helpful to know which speech acts about which we may be con f ident or others where we are less sure.
in practice it may be necessary to return a message to the user indicating that the virtual assistant is unsure what the user intends and ask the user to clarify.
rq6is useful because the presence of some a t tributes may indicate high con f idence while others may indicate low con f idence.
.
methodology in general we follow a fold cross validation study design.
in a standard n fold design for evaluating classi f iers nexamples are set aside as a testing set while the remaining 1n 1o nexamples are used for training.
t he evaluation is conducted ntimes once for each nthselection of the examples as a testing set.
t hen the evaluation metrics are calculated for each fold and averaged.
we chose as a value for nbecause it ensured that our testing set would not be too small as it might have been with a fold design while still maintaining multiple folds that could be averaged.
t he selection of a testing set is a non trivial exercise in a multilabel dataset in contrast to a single label one.
in a single label dataset it is usually sufficient to randomly selected nof the examples for the testing set.
but in our multi label dataset we need to ensure that the testing set represents the same distribution of labels as the overall dataset.
with only f ive folds it is conceivable that a random selection would give too much weight to one label and this overweighted selection would not be averaged out over a large number of folds.
t herefore we sampled each label in proportion to the number of examples in that label and con f irmed that the distribution of the labels over the testing set was as close as possible to the distribution of labels over the entire dataset.
a f ter we separated the testing and training data we ran smote on the training data only.
if we had executed smote on the entire dataset then divided the data into testing training groups we would have contaminated the testing set with information from the training set.
smote synthesizes examples based on the examples it is given.
if we had run smote on the entire dataset we would have created synthesized examples based on real examples that ended up in testing set.
t herefore we only ran smote on the training set.
t his did increase the execution cost of our experiment slightly since we needed to execute smote f ive times once for each fold a f ter we separated the testing set from the training set .
note also that this methodology is conservative it only uses real examples for the testing set.
we use the results from this conservative approach to answer rq 4and rq to avoid presenting a biased result.
we also use these results to calculate other metrics see the next section to answer rq .
.
metrics we report the metrics precision recall f measure and support to answer rq 4and rq t hese are standard metrics for evaluating classi f iers and have been covered extensively elsewhere for space we do not discuss their details here.
we calculate these metrics for each speech act type i.e.
each label for rq and combine the results for each speech act type to answer rq .
we combine theprecision and recall values for each speech act type with a weighted average where the weights are based on the support for each speech act type.
t he reason is so that the combined values re f lect the size of each label.
a simple average without the weights would be biased by labels that only have a few examples in the testing set.
for rq we calculate f score for the a t tributes.
f score distinguished from f measure the harmonic mean of precision and recall is typically used for feature selection to indicate which features are the most informative.
.
t hreats to validity like all experiments our study carries threats to validity.
t he main sources of threats to validity include the participants in the user simulations the bugs we asked the users to repair and the in f luences of the technology used by the participants e.g.
the ide on the questions they asked.
also it is possible that there are errors in our manual annotation process or in our selection of categories.
while we try to mitigate these risks by following accepted data collection and annotation procedures and by including a relatively large number of participants and different bugs the threat remains that changes in these variables could affect the performance of our classi f iers.
as an additional guard against these risks we release all data via an online appendix for community scrutiny see section .
prediction eval.
results t his section discusses our answers to rq rq including our supporting data and rationale.
table performance metrics calculated for each speech act type some speech act types have been abbreviated .
recall that the averages are a weighted average based on the support for each speech type see section .
.
precision recall f measure support apianswer .
.
.
.
api q uestion .
.
.
.
clarifanswer .
.
.
.
clarif q uestion .
.
.
.
con f irmation .
.
.
.
docanswer .
.
.
.
impl q uestion .
.
.
.
implstatement .
.
.
.
introduction .
.
.
.
stmnt .
.
.
.
system q uestion .
.
.
.
avg total .
.
.
.
.
rq overall performance t he weighted average precision of from our classi f iers was while the weighted average recall was as reported in table .
t hus for an arbitrary incoming message we can expect this classi f ier to correctly identify the speech act type of that message of the time while identifying of the speech acts types to which the message belongs.
if the classi f ier claims that a message of a particular type we can estimate that that claim will be correctesec fse nov. lake buena vista florida usa andrew wood paige rodeghero ameer armaly and collin mcmillan table t he top most informative features for each speech act type calculated by f score.
most features are words but features with the suffix sfare shallow features see section .
.
see section .
for a deeper discussion of this table.
apianswer node if on f inished keyframes constructor values time timeline keyvalue keyframe api q uestion size method have how pane class object an does what clarifanswer compilation con f igurations word trigger supply green appear box clicking bo t tom clarif q uestion need the or other wc sf you this f ix prime bug con f irmation of yes is to thanks slen sf the thank wc sf ok documentanswer byte marks later reading bytes joptionpane external input audio stream implement q uestion face why mark eratosthenes occurs gets arraycopy reason bu t ton clicked implementstatement signature widget funtion hidden drawing waitfor throwing paint timeout jcomponent introduction supervised programmers today hello human am start hi study ready statement seems what slen sf looks f ixed but works was think it system q uestion password there permi t ted lang running way programs kill eclipse how roughly 3rds of the time.
we acknowledge that we cannot evaluate whether these improve over an earlier approach given that we are not aware of an earlier technique for identifying speech acts on our data.
nevertheless we f ind these results to be an encouraging starting point for building a virtual assistant in light of the somewhat bare bones text classi f ication strategy we used binary bag of words see section .
a promising area of future work in our view is to adapt more advanced classi f ication techniques.
.
rq speech act type variations t he performance of our classi f iers varied considerably across different speech act types.
at the high end precision was over and recall over .
at the low end precision and recall dipped below around .
t his observation is important because it means that for some speech act types a virtual assistant can be highly con f ident that the prediction is correct.
as a practical ma t ter a virtual assistant may request the user to repeat a message in different words or ask for other followup information if the classi f ier is not able to place the message into a speech act type with sufficient con f idence.
t his observation is also important from an academic viewpoint because it means that programmers use different types of language to make different types of messages.
in some cases programmers consistently use the same language which is what the classi f ier uses to make good predictions .
in other cases programmers use much more different language it makes the prediction process more challenging but also raises academic questions about what is different about the language which is an area of future work.
we begin to explore this in rq .
.
rq attribute effects table shows the top most informative features for each speech act type.
we make two observations from this data first the shallow features are far more useful for some speech act types than others.
for example con f irmation actions are likely to be short messages so the word count metric wcsf is informative in this case.
t his observation is useful because shallow features are easy to compute so areas where they are informative can be predicted with reasonable accuracy at low cost.
second many of the words are general enough that they are likely to be generalizable beyond the set of bugs we chose even though others are speci f ic to particular domains.
for example the speech act implementationstatement is informed by words like function and signature which are likely to be true across many programming conversations.
but the most informative feature for that action is jcomponent which is aword speci f ic to java and perhaps the domain of programs we study.
it is not likely to appear in every domain.
t herefore one possible mediation is to use placeholder features that count the number of e.g.
domain speci f ic programming words used in a message.
also we note again that we used the binary bag of words model which separates the words from their contexts.
an area of future work is in nlp based recognition such as phrases or n grams.
conclusion our paper makes three contributions to so f tware engineering literature.
first we contribute so f tware engineering conversations with professional developers.
second we created a system of classi f ication for developer speech acts.
we manually detect and classify relevant speech acts in order to contribute to the understanding of developer question answer conversations.
we also provide this annotation classi f ication system on our online appendix for future researchers to use.
t hird we lay the foundation for a virtual assistant by building an automatic speech act classi f ication system.
reproducibility we have made our raw data annotations model and source code available via an online appendix h t tps tinyurl.com yadfpojd for the research community to reproduce or use.