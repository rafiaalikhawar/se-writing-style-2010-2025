codevectors understanding programs through embedded abstracted symbolic traces jordanhenkel universityof wisconsin madison usa jjhenkel cs.wisc.edushuvendu k.lahiri microsoft research usa shuvendu.lahiri microsoft.com ben liblit universityof wisconsin madison usa liblit cs.wisc.eduthomasreps univ.
ofwisconsin madison andgrammatech inc. usa reps cs.wisc.edu abstract withtheriseofmachinelearning thereisagreatdealofinterestin treatingprogramsasdatatobefedtolearningalgorithms.however programsdonotstartoffinaformthatisimmediatelyamenable tomostoff the shelflearningtechniques.instead itisnecessaryto transformtheprogramtoasuitablerepresentationbeforealearning techniquecan be applied.
inthispaper weuseabstractionsoftracesobtainedfromsymbolic execution of a program as a representation for learning word embeddings.
we trained a variety of word embeddings under hundreds of parameterizations and evaluated each learned embedding on a suite of different tasks.
in our evaluation we obtain top accuracyonabenchmarkconsistingofover19 000api usageanalogies extracted from the linux kernel.
in addition we show that embeddingslearnedfrom mainly semanticabstractionsprovide nearly triple the accuracy of those learned from mainly syntactic abstractions.
ccs concepts theoryofcomputation abstraction computingmethodologies machinelearning symbolicandalgebraicmanipulation softwareanditsengineering automatedstaticanalysis generalprogramminglanguages softwareverificationandvalidation softwaretestingand debugging keywords word embeddings analogical reasoning program understanding linux acmreference format jordanhenkel shuvenduk.lahiri benliblit andthomasreps.
.code vectors understanding programs through embedded abstracted symbolic traces.
in proceedings of the 26th acm joint european software engineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse november 4 9 lakebuena vista fl usa.
acm new york ny usa 12pages.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 4 9 lake buenavista fl usa associationfor computing machinery.
acm isbn ... .
introduction computer science has a long history of considering programs as dataobjects .withtheriseofmachinelearning therehas been renewed interest in treating programs as data to be fed to learningalgorithms .however programshavespecialcharacteristics includingseveral layers ofstructure suchas a program s context freesyntacticstructure non context freenameandtype constraints and the program s semantics.
consequently programs do not start off in a form that is immediately amenable to most off the shelflearningtechniques.instead itisnecessarytotransform the program to a suitable representation before a learning techniquecan be applied.
thispapercontributestothestudyofsuchrepresentationsinthe contextof wordembeddings .wordembeddingsareawell studied methodfor convertinga corpusof natural languagetext to vector representations ofwords embedded into a low dimensionalspace.
thesetechniqueshavebeenappliedsuccessfullytoprogramsbefore but different encodings of programs into word sequencesarepossible andsomeencodingsmaybemoreappropriatethanothersas the inputto aword vector learner.
the high level goalsof our work can be statedas follows deviseaparametricencodingofprogramsintowordsequences that i canbetunedtocapturedifferentrepresentationchoices on the spectrum from mainly syntactic to mainly semantic ii is amenable to word vector learning techniques and iii can be obtainedfrom programs efficiently.
wealsowishtounderstandtheadvantagesanddisadvantagesof ourencodingmethod.
5 8summarizetheexperimentsthatwe performedto provideinsightonhigh level goal ii .
we satisfy high level goals i and iii by basing the encoding onalightweightform of intraprocedural symbolicexecution .
we base our technique on symbolic execution due to the gap between syntax e.g.
tokens or abstract syntax trees asts and the semantics of a procedure in a program.
in particular token basedtechniquesimposeaheavyburdenon the embedding learner.
for instance it is difficultto encode the differencebetweenconstructions suchas a band !
a !
b via a learned low dimensional embedding .
ourmethodis intraprocedural sothatdifferentprocedures can be processedinparallel.
esec fse november4 lake buena vista fl usa j. henkel s.lahiri b. liblit andt.reps ourmethodis parametric inthesensethatweintroducea levelofmappingfromsymbolic executiontracestotheword sequencesthatareinputtotheword vectorlearner.
wecall theseabstraction mappings orabstractions although strictly speakingtheyarenotabstractionsinthesenseofabstract interpretation .
differentabstractionmappingscanbe usedtoextractdifferentwordsequencesthatareindifferent positionsonthespectrumof mainly syntacticto mainly semantic.
we have developed a highly parallelizable toolchain that is capable ofproducingaparametricencodingofprogramstowordsequences.
forinstance wecanprocess311 670proceduresinthelinuxkernel1 in4hours 2usinga64 coreworkstation 4cpuseachclockedat .
ghz running centos7.
with252gb ofram.
after we present our infrastructure for generating parametric encodings of programs as word sequences there are a number ofnaturalresearchquestionsthat we consider.
first we explore the utility of embeddings learned from our toolchain research question are vectors learned from abstracted symbolic traces encoding usefulinformation?
judgingutilityisadifficultendeavor.natural languageembeddings have the advantage of being compatible with several canonical benchmarks for word similarity prediction or analogy solving .inthedomainofprogramunderstanding no such canonical benchmarks exist.
therefore we designed a suiteofovernineteenthousandcodeanalogiestoaidintheevaluationofour learnedvectors.
next weexaminetheimpactofdifferentparameterizationsof our toolchain by performing an ablation study.
the purpose of this study isto answer the following question researchquestion2 whichabstractionsproducethebest program encodings for word vector learning?
there areseveral examplesof learningfrom syntacticartifacts such asasts or tokens.
thesuccess of suchtechniques raises the question of whether adding a symbolic execution engine to the toolchainimproves the qualityofour learnedrepresentations.
researchquestion3 doabstractedsymbolictracesat the semanticendofthespectrumprovidemoreutilityastheinput toaword vector learningtechniquecomparedtoonesatthe syntactic end ofthe spectrum?
becauseoursuiteofanalogiesisonlyaproxyforutilityinmore complexdownstreamtasksthatuselearnedembeddings wepose one more question 1specifically we used a prerelease of linux .
corresponding to commit fd7cd061adcf5f7503515ba52b6a724642a839c8 in the github linux kernel repository.
2during trace generation we exclude only vhash update fromcrypto vmac.c due to its size.research question can we use pre trained word vector embeddings onadownstream task?
the contributionsofour work can be summarizedas follows wecreatedatoolchain fortakingaprogramorcorpusofprogramsandproducingintraproceduralsymbolictraces.thetoolchain isbasedondockercontainers isparametric andoperatesinamassivelyparallelmanner.oursymbolic executionengineprioritizes the amount of data generated over the precision of the analysis in particular no feasibility checking is performed and no memory modelisusedduringsymbolic execution.
we generated severaldatasets ofabstractedsymbolictraces from the linux kernel.
these datasets feature different parameterizations abstractions and are stored in a format suitable for off the shelf word vector learners.
wecreatedabenchmarksuite ofover19 000api usageanalogies.
we report on severalexperiments using thesedatasets in 5 weachieve93 top 1accuracyonasuiteofover19 analogies.
in 6 weperformanablationstudytoassesstheeffectsof differentabstractionsonthe learnedvectors.
in 7 we demonstrate how vectors learned from mainly semantic abstractions can provide nearly triple the accuracy ofvectors learnedfrom mainly syntactic abstractions.
in 8 welearnamodelofaspecificprogrambehavior which error a trace is likely to return and apply the model in a casestudytoconfirmactualbugsfoundviatraditionalstatic analysis.
our toolchain pre trained word embeddings and code analogy suite are all available as part of the artifact accompanying this paper details are given in .
organization.
theremainderofthepaperisorganizedasfollows 2providesanoverviewofourtoolchainandapplications.
3details the parametric aspect of our toolchain and the abstractions we use throughout the remainder of the paper.
4briefly describes word vectorlearning.
5 8addressourfourresearchquestions.
9considersthreatstothevalidityofourapproach.
10discusses related work.
11describes supporting materials that are intended to helpothersbuildonour work.
12concludes.
overview ourtoolchainconsists ofthree phases transformation abstraction and learning.
as input the toolchain expects a corpus of buildable c projects a descriptionof abstractionsto use and a word vector learner.asoutput thetoolchainproducesanembeddingofabstract tokenstodouble precisionvectorswithafixed user supplied dimension.
we illustrate this process as applied to the example in fig.
.
phase i transformation.
thefirstphaseofthetoolchainenumerates all pathsin each source procedure.
we begin by unrolling and truncating eachloop sothat its body is executed zero or one time therebymakingeachprocedureloop freeatthecostofdiscarding many feasible traces.
we then apply an intraprocedural 164codevectors understandingprograms through easts esec fse november4 lake buena vista fl usa intexample buf alloc if buf !
bar buf free buf return else return enomem figure anexample procedure callalloc assume alloc !
callbar alloc callfree alloc return a trace 1callalloc assume alloc return enomem b trace figure traces from the symbolic execution of the procedure infig.
called alloc retneq alloc called bar called free a abstractedtrace 1called alloc reteq alloc reterror enomem b abstractedtrace figure resultofabstracting thetwotraces infig.
2b symbolic executor to each procedure.
fig.
2shows the results of this processas appliedto the example code infig.
.
phase ii abstraction.
givenauser definedsetofabstractions thesecondphaseofourtoolchainleveragestheinformationgleaned from symbolic execution to generate abstracted traces.
one key advantageofperformingsomekindofabstractionisadrasticreduction in thenumber of possible tokens that appear in the traces.
considerthetransformedtraceinfig.
2b.ifwewanttounderstand therelationshipbetweenallocatorsandcertainerrorcodes then we might abstract procedure calls as parameterized tokens of the formcalled callee comparisons of returned values to constants as parameterized reteq callee value tokens and returned error codes as parameterized reterror code tokens.
fig.
3shows the result ofapplyingtheseabstractionsto the traces from fig.
.
phase iii learning.
ourabstractedrepresentationdiscardsirrelevant details flattens control flows into sequential traces and exposes key properties in the form of parameterized tokens that capture domain information such as linux error codes.
these qualities make abstracted traces suitable for use with a word vector learner.
word vector learners place words that appear in similar contexts close together in an embedding space.
when applied to naturallanguage learnedembeddingscananswerquestionssuchas king is to queen as man is to what?
answer woman.
our goalisto learn embeddings that can answer questionssuch as if a lock acquired by calling spin lock is released by callingspin unlock thenhowshouldireleasealockacquired by calling mutex lock nested ?
that is called spin lock is tocalled spin unlock ascalled mutex lock nested is to what?
answer called mutex unlock .
whicherrorcodeismostcommonlyusedtoreportallocation failures?
that is which reterror code is most related to reteq alloc ?
answer reterror enomem .
which procedures and checks are most related to alloc?
answers called free retneq alloc etc.
the remainder of the paper describes a framework of abstractions and a methodology of learning embeddings that can effectivelysolvetheseproblems.alongtheway wedetailthechallenges that arise in applying word embeddings toabstract path sensitive artifacts.
abstractions one difference between learning from programs and learning from naturallanguageisthesizeofthevocabularyineachdomain.in natural language vocabulary size is bounded e.g.
by the words in a dictionary ignoring issues like misspellings .
in programs the vocabularyisessentiallyunlimited duetoidentifiernames there areahugenumberofdistinctwordsthatcanoccurinaprogram.
to address the issue of vocabulary size we perform an abstraction operation on symbolic traces so that we work with abstracted symbolic traces when learningwordvectors from programs.
.
abstracted symbolic traces we now introduce the set of abstractions that we use to create abstracted symbolic traces.
selected abstractions appear in the conclusions of the deduction rules shown in fig.
.
the abstractionsfallintoafewsimplecategories.the called callee and accesspathstore path abstractions can be thought of as events thatoccurduringatrace.abstractionslike reteq callee value and errorserve to encode the status of the current trace they provide contextual information that can modify the meaning of an event observed later in the trace.
near the end of the trace the reterror code retconst value andpropret callee abstractions provide information about the result returned at the end of the trace.takentogether thesedifferentpiecesofinformationabstract thetrace however theabstractedtraceisstillarelativelyrichdigest ofthe trace sbehavior.
withtheabstractionsdescribedabove wefoundthatthelearned vectors were sub optimal.
our investigation revealed that some of the properties we hoped would be learned required leveraging contextual information that was outside the window that a wordvectorlearnerwascapableofobserving.forexample tounderstand the relationship between a pair of functions like lockandunlock a word vectorlearnermustbeabletocopewithanarbitrarynumber ofwordsoccurringbetweenthefunctionsofinterest.suchdistances are a problem because lengthening the history given to a wordvectorlearneralsoincreasesthecomputationalresourcesnecessary to learn goodvectors.
165esec fse november4 lake buena vista fl usa j. henkel s.lahiri b. liblit andt.reps callfoo called foo callbar foo paramto bar foo callfoo obj callbar obj paramshare bar foo assumefoo reteq foo obj foo.bar baz accesspathstore foo.bar return c c err codes reterror err codes errorreturnc c nelementerr codes retconst c returnfoo propret foo propret ptr err error figure example derivationsforselected abstractions functionstart lock obj lock call lock foo alloc call alloc if foo !
retneq alloc obj baz accesspathstore baz bar foo paramto bar alloc call bar else reteq alloc unlock paramshare unlock lock obj lock call unlock return enomem reterror enomem error functionend unlock paramshare unlock lock obj lock call unlock return retconst functionend figure sample procedure with generated abstractions shownas comments due to the impracticality of increasing the context given to a word vector learner we introduced two additional abstractions paramtoandparamshare .theseabstractionsencodetheflowofdata inthetracetomakerelevantcontextualinformationavailablewithout the need for arbitrarily large contexts.
as shown in the abstractions that encode semantic information such as dataflow facts end up adding the most utility to our corpus of abstracted traces.
this observation is in line with the results of allamanis et al.
who found that dataflow edges positively impact the performance ofalearnedmodelondownstream tasks.
weaugmenttheabstractionsshowninfig.
withthefollowing additional abstractions which are similar to the ones discussed above retneq callee value retlessthan callee value andothers arevariantsofthe reteq callee value abstractionshownin fig.
.
functionstart andfunctionend are abstractionsintroduced at the beginningandend ofeachabstractedtrace.
accesspathsensitive path issimilarto accesspathstore it encodesanycomplexfieldandarrayaccessesthatoccurin assumestatements.
.
encoding abstractionsas words wenowturntohowtheencodingoftheseabstractionsaswords and sentences to form our trace corpus can impact the utilityof learned vectors.
to aid the reader s understanding we use a sample procedure and describe an end to end application of our abstractionsandencodings.
fig.5showsasampleprocedurealongwithitscorresponding abstractions.
the number s before each abstraction signify which of the two paths through the procedure the abstraction belongs to.
to encode these abstractions as words we need to make careful choices as to what pieces of information are worthy of being represented as words and how this delineation affects the questions we can answer using the learnedvectors.
forinstance considerthe retneq alloc abstraction.there areseveralsimplewaystoencodethisinformationasasequence ofwords retneq alloc alloc neq retneq alloc alloc neq 0 retneq alloc alloc neq retneq alloc alloc neq 0 each of these four encodings comes with a different trade off.
thefirstencodingsplitstheabstractionintoseveralfine grained words which in turn reduces the size of the overall vocabulary.
this approach may benefit the learned vectors because smaller vocabulariescanbeeasiertoworkwith.ontheotherhand splitting the information encoded in this abstraction into several words makessomequestionsmoredifficulttoask.forexample itismuch easiertoask whatis mostrelated to allocbeing notequal tozero whenwehavejustasingleword alloc neq 0 tocapturesuch ascenario.
in our implementation we use the fourth option.
it proved difficulttoaskinterestingquestionswhentheabstractionswerebroken downintofine grainedwords.thisdecisiondidcomewiththecost ofalarger vocabulary.3encodingsfor therestofour abstractions are shown in fig.
.4the sentences generated by applying these encodings to fig.
5are showninfig.
.
word2vec word2vec is a popular method for taking words and embedding themintoalow dimensionalvectorspace .insteadofusinga one hot encoding where each element of a vector is associated withexactlyoneword word2veclearnsadenserrepresentation 3we mitigate theincrease invocabulary size from constructions like alloc neq 0 by restricting the constants we look for.
our final implementation only looks for comparisonsto constants in the set .
4because it is not possible to have paramshare x y orparamto x y without a called x following them the abstractions paramshare x y and paramto x y areencoded as yto avoid duplicating x.
166codevectors understandingprograms through easts esec fse november4 lake buena vista fl usa matchabstraction with called x x paramto x x paramshare x x reteq x c x eq c retneq x c x neq c ... propret x ret x retconst c ret c reterror e ret err codes functionstart start functionend end error err accesspathstore p !
p accesspathsensitive p ?
p figure encoding ofabstractions start lock alloc alloc neq 0 !
baz alloc bar lock unlock ret 0 end a trace start lock alloc alloc eq 0 lock unlock err ret enomem end b trace figure traces for fig.
5generated by the encoding from fig.
thatcapturesmeaningfulsyntacticandsemanticregularities and encodes theminthe cosine distancebetween words.
forourexperiments weusedglove duetoitsfavorableperformancecharacteristics.
glove works byleveraging the intuition thatword wordco occurrenceprobabilitiesencodesomeformof meaning.aclassicexampleistherelationshipbetweentheword pair ice and steam and the word pair solid and gas.
gas and steamoccurin thesamesentencerelatively frequently compared to the frequency with which the words gas and ice occur in the same sentence.
consequently the following ratio is significantly less than pr gas ice pr gas steam if instead welookatthefrequencyofsentenceswithbothsolid andicecomparedtothefrequencyofsentenceswithbothsolidand steam we find the opposite.the ratio pr solid ice pr solid steam is much greater than .
this signal is encoded into a large cooccurrencematrix.glovethenattemptstolearnwordvectorsfor whichthedot productoftwovectorsisclosetothelogarithmof theirprobability ofco occurrence.
rq1 are learnedvectorsuseful?
researchquestion 1askedwhethervectorslearnedfromabstracted symbolic traces encode useful information.
we assess utility via three experimentsoverword vectors.each of the followingsubsectionsdescribes andinterpretsone experiment indetail.
.
experiment codeanalogies an interesting aspect of word vectors is their ability to express relationships between analogous words using simple math and cosine distance.
encoding analogies is an intriguing byproduct of a good embedding and as such analogies have become a common proxyfor the overallquality of learnedwordvectors.
no standard test suite for codeanalogies exists so we created such a test suite using a combination of manual inspection and automated search.
the test suite consists of twenty different categories each of which has some number of function pairs that have been determined to be analogous.
forexample consider mutex lock nested mutex unlock and spin lock spin unlock these are two pairs from the lock unlock categorygivenintab.
.wecanconstructananalogyby takingthesetwopairsandconcatenatingthemtoformtheanalogy mutex lock nested is tomutex unlock asspin lock is to spin unlock .
byidentifyinghigh levelpatternsofbehavior and finding several pairs of functions that express this behavior we createdasuite that contains 042code analogies.
tab.1listsourcategoriesandthecountsofavailablepairs along with a representative pair from each category.
tab.
1also provides accuracymetricsgeneratedusingthevectorslearnedfromwhatwe willrefer to as the baseline configuration 5whichabstracts symbolictracesusingalloftheabstractionsdescribedinin .weused a grid search over hundreds of parameterizations to pick hyperparametersforourword vectorlearner.fortheresultsdescribed in this section we used vectors of dimension a symmetric windowsizeof50 andavocabulary minimumthresholdof1 to ensure that the word vector learner only learns embeddings for words that occur a reasonable number of times in the corpus of traces.
we trained for iterations to give glove ample time to find goodvectors.
in each category we assume that any two pairs of functions are sufficiently similar to be made into an analogy.
more precisely we form a test by selecting two distinct pairs of functions a b and c d fromthesamecategory andcreatingthetriple a b c to givetoananalogysolverthatisequippedwithourlearnedvectors.
theanalogysolverreturnsavector d andweconsiderthetest passed if d dand failed otherwise.
levy and goldberg present the following objective to use when solving analogies with word vectors d argmax d v a b c cos d b cos d a cos d c results.
the accuracy column of tab.
1shows that overall accuracyontheanalogysuiteisexcellent.ourembeddingsachieve 5thebaselineconfigurationisdescribedinmoredetailin whereitisalsocalled configuration .
167esec fse november4 lake buena vista fl usa j. henkel s.lahiri b. liblit andt.reps table analogy suite details type category representative pair ofpairs passingtests total tests accuracy calls store16 store32 .
calls add remove ntb list add ntb list rm .
calls create destroy device create device destroy .
calls enable disable nv enable irq nv disable irq .
calls enter exit otp enter otp exit .
calls in out add in dtd add out dtd .
calls inc dec cifs in send inc cifs in send dec .
calls input output ivtv get input ivtv get output .
calls join leave handle join req handle leave req .
calls lock unlock mutex lock nested mutex unlock .
calls on off b43 led turn on b43 led turn off .
calls read write memory read memory write .
calls set get set arg get arg .
calls start stop nv start tx nv stop tx .
calls up down ixgbevf up ixgbevf down .
complex retcheck call kzalloc neq 0 kzalloc .
complex reterror prop write bbt lt 0 ret write bbt .
fields check check ?
dmaops ?
dmaops altera dtype .
fields next prev !.task list.next !.task list.prev .
fields test set ?
at current !
at current .
totals .
greater than top accuracy on thirteen out of the twenty categories.thelearnedvectorsdotheworstonthe retcheck call category where the top accuracy is only .
this category is meanttorelatethecheckingofthereturnvalueofacallwiththecall itself.
however we often find that one function allocates memory whileadifferentfunctionchecksforallocationsuccessorfailure.
for example a wrapper function may allocate complex objects but leave callers to check that the allocation succeeds.
because our vectorsarederivedfromintraproceduraltraces itissensiblethat accuracysuffers for interproceduralbehaviors.
bycontrast ourvectorsperformextraordinarilywellonthe ret error prop category top .
this category represents cases where an outer function i performsan inner call ii detectsthat ithasreceivedan errorresult and iii returns propagates that errorresultastheouterfunction sownreturnvalue.unlikeforthe retcheck call category thenatureofthe reterror prop categoryensuresthatboththecheckandthereturnpropagationcanbe observedinintraproceduraltraces withoutlosinganyinformation.
.
experiment simple similarity one of the most basic word vector tasks is to ask for the knearest vectorstosomechosenvector usingcosinedistance .weexpect theresultsofsuchaquerytoreturnalistofrelevantwordsfromour vocabulary.oursimilarityexperimentswerebasedontwotypes ofqueries i givenaword findtheclosestword and ii givena word find the five closestwords.ret new ... priv bo if !ret ret pin priv bo ... if !ret ret map priv bo if ret unpin priv bo if ret ref null priv bo figure excerpt from nv17 fence.c .
names have been shortened to conserve space.
similar pairs.
we identified the single most similar word to each word in our vocabulary v. this process produced thousands of interesting pairs.
in the interest of space we have selected four sampleswhicharerepresentativeofthevarietyofhigh levelrelationshipsencodedinour learnedvectors6 sin mul andcos mul dec stream header anddec stream footer rx b frame andtx b frame 6the artifact accompanying this paper includes a full listing of these pairs ordered by cosine similarity.
168codevectors understandingprograms through easts esec fse november4 lake buena vista fl usa nouveau bo new eq 0 andnouveau bo map7 the last pair is of particular interest because it expresses a complexpatternofbehaviorthatwouldbeimpossibletoencodewithout some abstraction of the path condition.
the last pair suggests that there is a strong relationship between the function newreturning0 which signals a successful call and then the subsequent performance of some kind of map operation with the mapcall.
to gain a deeper understanding of what the vectors are encoding we searchedforinstances of thisbehaviorin the original sourcecode.
we foundseveral instancesofthe pattern showninfig.
.
the code in fig.
8raise a new question why isn t pinmore closelyrelatedto new eq 0 ?weperformedadditionalsimilarity queries to gain a deeper understanding of how the learned vectors have encodedthe relationship between new pin andmap.
first wecheckedtoseehowsimilar pinistonew eq 0 .we found that pinis the fourth most related word to new eq 0 which suggests that a relationship does exist but that the relationship between new eq 0 andpinis not as strong as the one between new eq 0 andmap.
looking back at the code snippet and remembering that several more instances of the same pattern canbefoundinseparatefiles weareleftwiththefactthat pin directly follows from the successful new.
therefore intuition dictatesthat pinshouldbemorestronglyrelatedto newthanmap.
the disagreement between our intuition and the results of our word vector queriesmotivatedusto investigate further.
by turning to the traces for an answer we uncovered a more complete picture.
in traces newco occurs with pin.
in traces newco occurs with map.
if we look at traces that donotcontain a call to new there are 354traces that have nocallto new butstillhaveacallto pin.incontrast only traces have no call to new but still have a call to map.
finally wehaveadefinitiveanswertotheencodinglearnedbythevectors it is indeed the case that newandmapare more related in our corpus of traces because almost every time a call to mapis made acorrespondingcallto newprecedesit.ourintuitionfooledus because the snippets of source code only revealed a partial picture.
top similar words and the challenge of prefix dominance.
anothersimilarity basedtestistotakeawordandfindthetop k closest words in the learned embedding space.
ideally we d see words that make intuitive sense.
for the purpose of evaluation we picked two words affs bread a function in the afs file system that reads a block and kzalloc a memory allocator.
for each target word we evaluated the top most similar words for relevance.intheprocess wealsouncoveredaninterestingchallenge when learning over path sensitive artifacts which we call prefix dominance .
ourcorpusofsymbolictracescanbethoughtofasacorpusof execution trees.
in fact in the implementation of our trace generator the traces only exist at the very last moment.
instead of storingtraces westoreatreethatencodes withoutunnecessary 7in the following text and in fig.
we remove the nouveau bo prefix to conserve space.table top 5closest words to affs bread andkzalloc affs bread kzalloc affs bread neq 0 kzalloc neq 0 affs checksum block kfree affs sb volume affs free block snd emu10k1 audigy write op affs brelse ?
output amp duplication theinformationgainedfromsymbolicallyexecuting a procedure.
if we think about the dataset of traces as a dataset of trees each of which holds many traces that share commonprefixes we begin to see that learning word vectors from traces is an approximation oflearningdirectlyfrom the executiontrees.
the approximation of trees by traces works in the sense that we can use the traces to learn meaningful vectors but the approximation is vulnerable to learning rare behaviors that exist at the beginningofaprocedurewhosetrace treehasmanynestedbranches.
theserarebehaviorsoccuronlyonceintheoriginalproceduretext andcorrespondingexecutiontree butarereplicatedmanytimesin the traces.
in a procedure with significant branching complexity a single occurrence of rare behavior can easily overwhelm any arbitrary number ofoccurrences of expectedbehavior.
in tab.
we see two words affs bread andkzalloc and the five most similar words to each of them.
word similarity has captured many expected relationships.
for example the fact that kzalloc is most commonly checked to be non null kzalloc neq 0 andthenalso kfreediswhatwewouldexpect given the definition of an allocator.
similarly we can see that affs bread is also checked to be non null checksummed freed released etc.
however in addition to these expected relationships the last three entries for kzalloc seem out of place.
these unexpectedentriesare presentinthetop answerbecauseofprefix dominance.
we searched our traces for places where kzalloc and the three unexpected entries in the table co occur.
we found one function with paths being our budget for the number of traces we are willing to generate via symbolic execution for a single procedure of which have several instances of the patternkzalloc followed by snd emu10k1 audigy write op .
this one function with its multitude of paths overwhelms our dataset andcausesthewordvectorstolearnaspuriousrelationship.
prefix dominance also explains the strong associations between kzalloc and volume and?
output amp .
ontheotherhand affs bread isrelativelyunaffectedbyprefix dominance.
examining our traces for the affsfile system that contains this function we found that no procedures had an overwhelmingnumberofpaths.therefore weneverseeanoverwhelming number of affs bread usage patterns that are rare at the sourcelevel but common inour setof traces.
169esec fse november4 lake buena vista fl usa j. henkel s.lahiri b. liblit andt.reps .
experiment queries via word vector averaging wordvectorshavethesurprisingandusefulabilitytoencodemeaning when averaged .
we devised a test to see if our learned vectors are able to leverage this ability to capture a relationship between allocation failure andreturning enomem .
to understand whether our word vectors are capable of answering such a high level question we evaluated their performance on increasingly targeted queries represented by averaged vectors .
each query was restricted to search only for words in the subspace of the embedding space that contains kernel error codes.
narrowingtothesubspaceoferrorcodesensuresthatweareonlylooking at relevantwords andnot at the wholevocabulary.
results.
weidentifiedtwentydifferentfunctionsthatactasallocators inthe linux kernel.
first for each such allocator we took its word vector a and queried for the closest vector to a in the subspace of error codes .
this method found the correct error code only twice out of twenty tests i.e.
accuracy .
second we asked for the vector closest to an average vector that combined the vector for the allocator aof interest and the vector errfora generic error a err .this queryfound the correct enomemcode fourteen times out of twenty i.e.
accuracy .
third instead of averaging the allocator s avector with err we triedaveraging awiththe vector for the special endtoken that signals the end of a trace.
seeking the error code closest to a end 2found the correct result for sixteen of twenty test cases i.e.
accuracy .
the fact that this method outperforms ourpreviousqueryrevealsthatthecalltoanallocatorbeingnear the end of a trace is an even stronger signal than the errtoken.
finally we mixed the meaning of the allocator the error token and the end of trace token by averaging all three a err end .
the error code whose vector is closest to thisqueryisthecorrect enomemcodeforeighteenofthetwenty tests i.e.
accuracy .
the steadily increasing performance indicates that targeted queries encoded as average word vectors can indeedbe semantically meaningful.
the effectiveness of these queries and the results from .
and 5. supportapositiveanswertoresearchquestion learned vectors doencode useful information aboutprogram behaviors.
rq2 ablation study inthissection wepresenttheresultsofanablationstudytoisolate theeffectsthatdifferentsetsofabstractionshaveontheutilityof thelearnedvectors.weusedthebenchmarksuiteof19 042codeanalogies from 5to evaluate eight different configurations.
we scored each configuration according to the number of analogies correctlyencodedbythewordvectorslearnedforthatconfiguration i.e.
we report top results .
8the errwordisaddedtoanytracethatreturnseither i theresultofan err ptr call or ii aconstant lessthan zero that isalso a known error code.consequently a vector erris learned for the word err.
.
.
.
.
.
.
.
.
oov failed passed figure9 ablationstudy top 1analogyresultsforeightconfigurations baseline with up to one individual abstraction class removed .
the vocabulary minimum was and thenumberoftraining iterationswas .
in addition to the baseline configuration from .
we partitioned the abstractions into six classes9and generated six new embeddings eachwithone of the sixabstractionclasses excluded.
we also used one more configuration in which stop words were included.innaturallanguageprocessing stopwordsarewordsthat are filtered out of a processing toolchain.
sometimes these are the most common words in a language but any group of words can be designated as stop words for a given application.
in our context stopwordsarefunctionnamesthatoccuroften butaddlittlevalue to the trace.
examples are builtin expect and automatically generated compiletime assert s. we evaluatedthe following eightconfigurations baseline allabstractionsfrom baselinewithout paramtoandparamshare baselinewithout reteq retneq etc.
baselinewithout accesspathstore andaccesspathsensitive baselinewithout propret reterror andretconst baselinewithout error baselinewithout functionstart andfunctionend and baselinewithstop wordsincluded.
fig.9comparestheaccuracyoffortheseeightconfigurations.
bluebars indicate the number of tests in the analogy suite that passed redindicates tests that failed and brownindicates outof vocabulary oov tests.
configuration had the most outof vocabulary tests in this configuration we do not have words like!
next and!
prev which leaves several portions of the analogy suite essentially unanswerable.
thus we count out ofvocabulary tests as failedtests.
to create a fair playing field for evaluating all eight configurations we chose a single setting for the hyper parameters that wereusedwhenlearning wordvectors.wereducedthethreshold for how often a word must occur before it is added to the vocabulary from to .
the latter parameter which we refer to as 9except for called whichwas used in allconfigurations.
170codevectors understandingprograms through easts esec fse november4 lake buena vista fl usa thevocabulary minimum significantly impacts performance by forcingtheword vectorlearnertodealwiththousandsofrarelyseenwords.tounderstandwhywemustsetthevocabularyminimum to zero effectively disabling it consider the following exampletrace called foo paramshare foo bar called bar .in configuration whereweignore paramshare wewouldencode thistraceasthesentence foo bar .inconfiguration thissame traceisencodedas foo foo bar .thefactthatsomeabstractions caninfluencethefrequencywithwhichawordoccursinatracecorpusmakesanyword frequency basedfilteringcounterproductive to our goalofperformingafair comparison.
we also lowered the number of training iterations from to to reduce the resources required to run eight separate configurations of our toolchain.
these changes are responsible for thechangeinthetop 1accuracyofthebaselineconfigurationfrom .
intab.
1to .
infig.
.
in fig.
one clearly sees that configuration the one without any dataflow based abstractions suffers the worst performance degradation.configuration whichomitsaccess path basedabstractions has the second worst performance hit.
these results indicate that dataflow information is critical to the quality of learned vectors.
this conclusion further confirms findings by allamanis etal.
regardingtheimportanceofdataflowinformationwhen learningfrom programs.
fig.9also reveals that removing state abstractions reteq retneq etc.and error haslittleeffectonquality.however these abstractionsstilladdusefultermstoourvocabulary andthereby enlarge the set of potentially answerablequestions.
without these abstractions someofthe questionsin 5wouldbe unanswerable.
these results support the following answer to research question2 dataflow basedabstractionsprovidethegreatestbenefitto word vectorlearning.theseabstractions coupledwithaccess pathbasedabstractions providesufficientcontexttoletaword vector learner create useful embeddings.
adding abstractions based on path conditions or other higher level concepts like error adds flexibility without worsening the quality of the learned vectors.
therefore we recommendincludingtheseabstractions as well.
rq3 syntacticversussemantic nowthatwehaveseentheutilityofthegeneratedcorpusforwordvector learning and the interplay between the abstractions we use 6 wecompareourrecommendedconfiguration from 6 withasimpler syntactic basedapproach.
we explored several options for a syntactic based approach againstwhichtocompare.intryingtomakeafaircomparison one difficultythatarisesistheamountofdataourtoolchainproducesto use for the semantics basedapproach.
if we were to compare configuration against an approachbasedon asts or tokens there would be a large disparity between the paucity of data available totheast token basedapproachcomparedtotheabundanceof dataavailabletotheword vectorlearner anast ortoken based approach wouldonly have one data pointper procedure whereas thepath sensitiveartifactsgatheredusingconfiguration provide theword vector learnerwithhundreds ifnotthousands ofdata pointsper procedure.
semantic syntactic85.
.
passed failedoov figure10 top 1analogyresultsforsyntacticversussemantic abstractions.
the vocabulary minimum was and the numberoftraining iterationswas .
tocontrolfor thiseffect andavoidsuch adisparity we instead comparedconfiguration againstaconfigurationofourtoolchain thatusesonly syntactic abstractions i.e.
abstractionsthatcanbe appliedwithoutanyinformationobtainedfromsymbolicexecution.
thus the syntactic abstractionsare functionstart andfunctionend accesspathstore path and called callee .
the rest of our abstractions use deeper semantic information such asconstantpropagation dataflowinformation or thepathconditionfor agiven trace.
using only the syntactic abstractions we generated a corpus of traces and then learned word vectors from the corpus.
we compared the newly learned word vectors to the ones obtained with configuration .fig.10clearlyshowsthatsemanticabstractions arecrucial togivingthecontextnecessaryforsuccessfullearning.
even if we assess performance using only the analogies that are in vocabulary for the syntactic based approach we find that the syntactic based approach achieves only about accuracy which isabouthalf theaccuracyofvectorslearnedfrom mainly semantic abstractions.
theseresultssupportanaffirmativeanswertoresearchquestion3 abstracted traces that make use of semantic information obtained via symbolic execution provide more utility as the inputtoaword vectorlearnerthanabstractedtracesthatuseonly syntactic information.
rq4 use indownstream tasks research question 4asks if we can utilize our pre trained wordvector embeddings onsomedownstream task.
to address this question we selected a downstream task that models bug finding repair and code completion in a restricted domain error codemisuse.wechoseerror codemisusebecause it allows us to apply supervised learning.
because there are only a finitenumberofcommonerrorcodesinthelinuxkernel wecan formulateamulti classlabelingproblemusingtracesgeneratedvia our toolchainandour pre trainedword vector embeddings.
to build an effective error code misuse model we gathered a collectionoffailingtraces tracesinwhichthe errtokenoccurs .
wethenconstructedadatasetsuitableforsupervisedlearningasfollows we took each trace from configuration 10and removed the 10thedataflowabstractionspresentin werecreatedtoaidword vectorlearners for thisexperiment weuseconfiguration to excludethoseabstractions.
171esec fse november4 lake buena vista fl usa j. henkel s.lahiri b. liblit andt.reps lastthreeabstracttokens namely err ret e and end weusedthe ret e tokenasthelabelforthetrimmedtrace.we sampled a subset of traces from this large trace collection to use for training our model.
thisdatasetisagoodstartingpoint butfeedingittoamachinelearningtechniquethatacceptsfixed lengthinputsrequiresfurther processing.topreprocessthedata wekeptonlythelast100tokens ineachtrace.wethentookthetrimmedtraces andusedourlearned word vectorembeddingto transformeachsequenceof words into a sequence of vectors of dimension .
if originally a trace had fewer than100tokens we paddedthe beginningofthetrace with the zero vector.
we paired each of the trimmed and encoded traces with its label which we derived earlier .
finally to complete the preprocessingofthedatasetweattachedaone hotencodingofthe label.
tocollectachallengingtestsettoevaluateourlearnedmodel weturnedtorealbug fixingcommitsappliedtothelinuxkernel.
wesearchedforcommitsthatreferencedan incorrectreturn in their description.
in addition we leveraged min et al.
s list of incorrect return codes fixed by their juxta tool.
next we generatedabstractedsymbolictracesbothbeforeapplyingthefixing commitandafter.finally wekeptthetracesgeneratedbeforeapplyingthe fixthat afterthe fix had changedonly inthe errorcode returned.usingthisprocess wecollected68traces from15unique functions thathadbeen patchedto fixan incorrectreturn code.
using the preprocessed dataset we trained a model to predict theerrorcodethateachtraceshouldreturn.weusedarecurrent neural network with long short term memory lstm .
we evaluated the trained model using our test set in two different ways bugfinding weuseourlearnedmodeltopredictthethree mostlikelyerrorcodesforeachtraceinourtestset.ifagiven traceinitiallyendedintheerrorcode a butwaspatched to return the error code b we check to see if the incorrect aerrorcode isabsent from our model stop predictions.
repair suggestion weagainusethelearnedmodeltopredict thethreemostlikelyerrorcodesforeachtraceinthetestset.
this time we determine the fraction of traces for which the correcterrorcode i.e.
b ispresentinthetop 3prediction madebythe model.
in evaluation we found that the learned model identified an incorrecterrorcodein57ofour68tests.thisresultispromising because it suggests that there is enough signal in the traces of encoded vectors to make good predictions that could be used to detectbugsearly.
in evaluation we observed that the learned model had a top accuracy of .
meaning that the correct error code is amongourtopsuggestedfixesformorethanthreefourthsofthe buggy traces.
this result is a strong indicator that the learned vectors and abstracted symbolic traces are rich enough to make high level predictions that could be used to augment traditional ides with predictive capabilities.
such a feature could operate like autocomplete butwithanawarenessofwhatothercontributors 11we exclude traces that included the ret ptr err token because these traces do not haveanassociated errorcode.havedoneandhowtheir presumablycorrect codeshouldinfluence new contributions.
this feature would be similar to the existing applications of statistical modelingto programming taskssuch as autocompletion .
theseresultssupportanaffirmativeanswertoresearchquestion4 ourpre trainedword vectorembeddingscanbeusedsuccessfullyondownstreamtasks.theseresultsalsosuggestthatthere aremanyinterestingapplicationsforourcorpusofabstractedsymbolic traces.
learning from these traces to find bugs detect clones oreven suggestrepairs are allwithin the realmof possibility.
threats to validity there are several threatsto the validity of our work.
we leverage a fast but imprecise symbolic execution engine.
it is possible that information gained from the detection of infeasible pathsandtheuseofamemorymodelwouldimprovethequality of our learned vectors.
in addition it is likely that a corpus of interproceduraltraces wouldimpact our learnedvectors.
we chose to focus our attention on the linux kernel.
it is possiblethatlearninggoodword embeddingsusingartifactsderived from the linux kernel does not translate to learning good wordembeddingsforprogramsingeneral.tomitigatethisrisk wemaximizedtheamountofdiversityintheingestedproceduresbyingestingthe linux kernel withallmodularandoptionalcode included.
our analogies benchmark and the tests based on word vector averaging are only proxies for meaning and as such only serve as an indirectindicator of the qualityof the learned word vectors.
in addition wecreatedthesebenchmarksourselves andthusthere is a risk that we introduced bias into our experiments.
unfortunately wedonothavebenchmarksasextensiveasthosecreated throughout the years in the nlp community.
similar to mikolov etal.
wehopethatourintroductionofasuitablebenchmark willfacilitatecomparisonsbetweendifferentlearnedembeddings inthe future.
related work recently severaltechniqueshaveleveragedlearnedembeddingsfor artifacts generated from programs.
nguyen et al .
leverage word embeddings learned from asts in two domains to facilitate translation from java to c .
pradel and sen use embeddings learned from custom tree based contexts built from asts to bootstrapanomalydetectionagainstacorpusofjavascriptprograms.
guetal.
leverageanencoder decoderarchitecturetoembed wholesequencesintheir deepapi toolforapirecommendation.
api2api by ye et al .
also leverages word embeddings but it learns the embeddings from api related natural language documentsinstead ofan artifact deriveddirectlyfrom sourcecode.
moving toward more semantically rich embeddings defreez et al.
leverage labeled pushdown systems to generate rich traces which they use to learn function embeddings.
they apply theseembeddingstofindfunctionsynonyms whichcanbeusedto improve traditional specification mining techniques.
alon et al .
learn from paths through asts to produce general representations ofprograms in theyexpanduponthisgeneralrepresentationby 172codevectors understandingprograms through easts esec fse november4 lake buena vista fl usa leveragingattentionmechanisms.ben nunetal .
utilizeanintermediate representation ir to produce embeddings of programs that are learned from both control flow and data flow information.
venturingintogeneralprogramembeddings thereareseveral recent techniques that approach the problem of embedding programs or moregenerally symbolic expressions trees inunique ways.usinginput outputpairsastheinputdataforlearning piech et al.
and parisotto et al .
learn to embed whole programs.
usingsequencesoflivevariablevalues wangetal .
produce embeddingstoaidinprogramrepairtasks.allamanisetal .
learn toembed wholeprograms viagatedgraphrecurrent neural networks gg rnns .allamanisetal .
approachthemorefoundationalproblemoffindingcontinuousrepresentationsofsymbolic expressions.mouetal .
introducetree basedconvolutionalneuralnetworks tbcnns anothermodelforembeddingprograms.
pengetal .
provideanast basedencodingofprogramswith the goal of facilitating deep learning methods.
allamanis et al .
give a comprehensive survey ofthese techniques andmany other applicationsofmachine learningto programs.
we are not aware of any work that attempts to embed traces generatedfromsymbolicexecution.onthecontrary fowkesand sutton warnofpossibledifficultieslearningfrompath sensitive artifacts.
we believe that our success in using symbolic traces as theinput toa learnerisduetothe additionofpath conditionand dataflow abstractions the extra information helps to ensure that a complete picture isseen even inapath sensitive setting.
in the broader context of applying statistical nlp techniques toprograms therehasbeenalargebodyofworkusinglanguage modelstounderstandprograms tofindmisuses andto synthesize expressionsandcode snippets .
experimentalartifacts ourc2ocaml tool which performs a source to source transformation during the compilation of c projects to generate inputs to our lightweight symbolic execution engine is available at .
ourlightweightsymbolicexecutionengine lsee isalsoavailableat .
additionally weprovidetoolstodemonstrateourexperimentsat .
thisartifactallowstheusertorunourtoolchainend to endona smaller open source repository redis .
the artifact uses pre built docker images to avoid complex installation requirements.
our raw data two sets of learned vectors and a full collection of abstractedsymbolic traces are alsoincludedinthis artifact.
conclusion the expanding interest in treating programs as data to be fed to general purposelearningalgorithmshascreatedaneedformethods to efficiently extract canonicalize and embed artifacts derived from programs.
in this paper we described a toolchain for efficiently extracting program artifacts a parameterized framework of abstractionsforcanonicalizingtheseartifacts andanencodingof theseparameterized embeddings ina format thatcan be usedby off the shelf word vector learners.
ourworkalsoprovidesanewbenchmarktoprobethequalityof word vectors learned from programs.
our ablationstudy used thebenchmarktoprovideinsightaboutwhichabstractionscontributed themosttoourlearnedwordvectors.wealsoprovidedevidence that mostly syntactic abstractions are ill suited as the input to learningtechniques.lastly weusedthesetoolsanddatasetstolearn a model of a specific program behavior answering the question which error is a trace likely to return?
and applied the model inacasestudytoconfirmactualbugsfoundviatraditionalstatic analysis.