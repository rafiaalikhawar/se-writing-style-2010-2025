remi defect prediction for efficient api testing mijung kim jaechang nam jaehyuk y eon soonhwang choi and sunghun kim department of computer science and engineering software r d center the hong kong university of science and technology samsung electronics co. ltd hong kong china suwon south korea mjkimab jcnam hunkim cse.ust.hk jaehyuk.yeon soonhwang.choi samsung.com abstract quality assurance for common apis is important since the the reliability of apis affects the quality of other systems using the apis.
testing is a common practice to ensure the quality of apis but it is a challenging and laborious task especially for industrial projects.
due to a large number of apis with tight time constraints and limited resources it is hard to write enough test cases for all apis.
to address these challenges we present a novel technique remithat predicts high risk apis in terms of producing potentialbugs.
remiallowsdeveloperstowritemoretestcases for the high risk apis.
we evaluate remion a real world industrial project tizen wearable and apply remito the api development process at samsung electronics.
our evaluation results show that remipredicts the bug prone apis with reasonable accuracy .
f measure on average .
the results also show that applying remito thetizen wearable development process increases the number of bugs detected and reduces the resources required for executing test cases.
categories and subject descriptors d. .
software quality assurance keywords defect prediction api testing quality assurance .
introduction in modern software industry it is common to develop and deploy apis which implement common functionalities that can be used as a form of library for third party systems.
therefore it is important to ensure the reliability of these apis while developing them in house because any uncovered bugs in apis will quickly become widespread among the third party systems.
this work was supported by samsung electronics software r d center.to ensure the reliability of apis testing is a common and important practice during api development processes.
one of the common testing methodologies for apis under development is to create a certain number of unit test cases for each api e.g.
two test cases for each api .
however this methodology is neither efficient nor effective especiall y in industrial projects because there are a large number of apis and not every api is equally bug prone.
if we could identify high risk apis developers could test more on these high risk apis.
for example api developers can elaborately allocate resources to detect more bugs more quickly from the high risk apis.
in this paper we present a novel defect prediction technique called remi risk evaluation method for interface testing that is targeted for testing apis in the development process.
remimeasures the risk of apis in terms of producing potential bugs.
using a machine learning classifier random forest remibuilds a prediction model that predicts bug proneness of apis and ranks the apis with respect to the predicted buggy probability.
the api ranks returned by remican be used during a quality assurance qa process in industry.
practitioners can effectively allocate their limited resources to high ranking apis first.
we demonstrate our technique remiwith an industrial apidevelopmentprojectfor tizen wearable whichhasbeen actively developed by samsung electronics samsung .
our empirical evaluation measures the performance of the apilevel defect prediction and the efficiency of the qa process for api testing at samsung.
the results of our evaluation show that the prediction accuracy of remiis reasonable .
f measure on average .
the results also show that applyingremito the software development process increases the number of bugs detected and reduces the cost required for executing test cases.
overall using our tool samsung has improved the effectiveness and the efficiency of the api testing process for tizen wearable .
this paper makes the following contributions a novel approach to api level defect prediction that is targeted for api testing.
an empirical evaluation to assess remi.
successfully applying remiin an industrial api development project for efficient api testing.
.
related work defect prediction on industrial projects software defect prediction has been applied in industrial projects for software quality assurance .
ostrand et al.
pre figure overview of our approach.
dicted the density of defects in two large software systems from at t .
they found that of files with the highest predicted defect number actually detected and of defects .
nagappan et al.
predicted post release defects in a binary file on five microsoft products by using complexity metrics .
engstr om et al.
proposed a regression test selection technique that selects test case s associated with those source code files that have been predicted to be bug prone in the past and stored in a cache .
lewis et al.
applied two defect prediction approaches fixcache and rahman algorithms to two google projects at the file level and found that the defect prediction needs to be improved for the front line developers.
unlike these techniques that work at a file level granularity remiis designed for the practical use of defect prediction techniques at the api level our study shows that remican be effectively applied to the api development process in practice.
api testing on industrial projects there are several existing techniques that assess the reliability of apis used in industry by creating crash inducing inputs for apis .
shelton et al.
from microsoft adopted an automatic testing framework ballista and customized it to assess the robustness of win32 apis .
they used the exceptional values returned by ballista as inputs for win32 apis.
similarly hoffman et al.
investigated a way to generate domain specific boundary values and used them as crashinducing inputs for java apis used in two java components developed by bell labs .
jorgensen et al.
developed a technique for api testing by combining model based testing and category partitioning and conducted an empirical study on a microsoft windows application .
unlike these techniques that create test inputs for apis remifocuses on efficient resource allocation during the qa process.
.
our approach figure depicts the overview of our approach.
remihas four inputs the source code of the apis a software repository containing revision history the function call graph for the source code of the apis and bug history associated with the apis.
remioutputs a list of ranked apis along with the buggy probability.
remiconsistsoffiveprocessingphases collectingmetrics aggregatingmetricsattheapi level labelingapis building a prediction model and ranking the apis.
first remicollects two types of metrics at the functionlevel the source code metrics and the development process metrics to prepare for a data set used for the phase of aggregating metrics at the api level.
because an api consists of a set of functions remifirst collects the metrics for each function reachable from the api.
table presents the metrics used in remi.
for source code metrics remicollects metrics regarding mccabe s cyclomatic complexity volume e.g.
lines of code and object oriented features e.g.
table metrics in remi.
for a detailed description of each metric refer to the metrics for c c in and .
type metrics code metricsaltcountlineblank altcountlinecode altcountlinecomment countinput countline countlineblank countlinecode countlinecodedecl countlinecodeexe countlinecomment countlineinactive countlinepreprocessor countoutput countpath countsemicolon countstmt countstmtdecl countstmtempty countstmtexe cyclomatic cyclomaticmodified cyclomaticstrict essential knots maxessentialknots maxnesting minessentialknots ratiocommenttocode process metricsnumcommits totallocdeleted avglocdeleted maxlocdeleted totallocadded avglocadded maxlocadded totallocchurn avglocchurn maxlocchurn totaldistinctauthors numfixes coupling between object classes .
also remicollects development process metrics from the source code repository such as the number of commits the various measures of lines of deleted added changed code and the number of previous fixes .
second to predict defects at the api level remiaggregates the metrics collected at the function level to the apilevel with respect to the calling context of an api.
to acquire metrics at the api level remiutilizes the function call graph for the source code of apis and summates the metric values associated with the functions within the calling context of an api.
we choose to aggregate metrics in this way because in order to perform the prediction for the api instances the metrics at the api level need to take into account those metrics associated with the functions reachable from the api.
remisummates the metric values of the functions invoked in a various given call depth.
for example if the call depth given is remisummates the metrics up to the immediate children of each api in the call graph.
third as the last step to prepare for the data set used to build the prediction model remilabels each api as buggy or clean.
to do this remiuses bug fix history which is a typical labeling method for defect prediction studies .
specifically we label apis based on the existence of past bugs identified in previous release candidates rc .
there are two types of bug data that we use to identify bugs in previous rcs.
first we use the information from commit messages containing the fix related keywords e.g.
fix or resolve in the source code repository .
if there are any fix commits that make changes in functions within the calling context of an api remilabels the api as buggy.
second we use history of the bugs identified by executing the test suite in the past.
if at least one test failure associated with an api is recorded in the same period of collecting metrics remilabels such api as buggy otherwise as clean.
using these two types of past bug data if an api is labeled as buggy by either fix commits or test results we regard the api as buggy.
fourth when the metrics at the api level and labeled apis are ready remibuilds a prediction model using random forest .
since defect prediction performance may vary based on various machine learners and project datasets it is difficult to decide the best learner .
for apis of tizen wearable random forest leads to the best prediction performance in our repeated experiments1.
for this reason 1in our experiments we used the weka framework with thetable results for rq1.
representative prediction results of tizen wearable rc2.
packagesdepth depth all precision recall f measure precision recall f measure package .
.
.
.
.
.
package .
.
.
.
.
.
average .
.
.
.
.
.
we use random forest for remiin our empirical study.
finally remiranksapiswithrespecttothebuggyprobability of the prediction outcomes.
remiconsiders that apis predicted at a higher buggy probability are likely to be more risky.
this rank information is used by the qa team to decide which apis need more testing effort.
.
ev aluation to evaluate remi we investigate two research questions rq1 how accurately can remipredict buggy apis?
rq2 how useful is remifor api testing in the actual api development process?
to set up our experiments we applied remitotizenwearable which is implemented in c c at samsung.
among the functional packages containing approximately apis found in tizen wearable we applied remito packages containing approximately apis in which at least one bug exists or no build error occurs while releasing a release candidate rc .
we conducted the defect prediction in three consecutive rcs from rc2 to rc4 in which remibuilds a prediction model from the data collected in rc n andpredictsbugsforthedatacollectedinrc n .
for example remibuilds a prediction model using data collected in rc2 and predicts bug prone apis for rc3.
based on the prediction results the qa team performs api testing by allocating its resources to those bug prone apis.
we repeat this process for rc4.
.
rq1 prediction accuracy wemeasureprecision recall andf measureforeach tizenwearable package.
table presents the prediction results from package and in tizen wearable rc2 and their average results.
since the rcs in the experiments were the outcome of intensive qa activities most packages in rcs were already reliable.
thus most packages except for package and have no buggy apis based on our labeling methods.
we build the prediction model using the data collected from rc1 and predict the defects in rc2 using the model.
precision represents how many of the apis predicted as buggy are correct.
recall measures how many buggy apis are predicted from all actual buggy apis.
since precision and recall have a trade off we compute f measure which is a harmonic mean value of precision and recall.
table also presents the prediction results by different depths of the call graph used during the metric aggregation section .
we report the prediction results of depth and all for each package.
depth all means we summate metrics using all descendant functions of an api in the call graph.
in package remiachieves the high f measure in both of depths and all .
and .
respectively .
although the f measure in package is not as high as that in package we observed that of the buggy apis were ranked in the top of apis by remi.
this result indicates that default options for random forest logistics simple logistics lmt j48 naive bayes bayes network and smo .table results of rq2 for the test development phase.
m indicates resources consumed for modifying existing tests.
n indicates resources for developing new te sts.
resources bug detect ability test bugs version man day apicases detected rc2w oremi m w remi .
n rc3w oremi .
m w remi .
n our ranking is effective in prioritizing the riskier apis so that the samsung qa team can allocate its resources for these riskier apis first.
table shows the different prediction results in depth and depth all.
the packages may have more complexity in terms of the source code and process metrics when comparing the metric values in depth all to those in depth .
since higher metric values are likely to entail more apis being predicted as bug prone the prediction results in depth all have a better f measure .
of package in depth all than that in depth .
aggregating metrics at the apilevel with a deeper call graph depth may be helpful and lead to a better prediction performance.
to investigate the impact of different call graph depths we applied depth and all for all packages in tizen wearable .
however we observed that the deeper depth did not always lead to the better results as seen in depth all of package .
one possible reason is that defects of apis reside in functions invoked in different depths of the function call graph.
we plan to identify the reasonable depth before building a prediction model as future work.
.
rq2 effects on api testing weevaluatetheeffectsof remionapitestingwhen remi is applied during the api development process.
in particular we measure the bug detectability of the test suite and the allocated resources in two phases of api testing test case development and test case execution.
table presents the results for the test case development phasebeforeandafterapplying remi.
thefirsttwocolumns list subject versions applied with and without remi.
before applying remi the qa team at samsung modifies a subset of existing test cases that is affected by api specification changes or that contains invalid test inputs.
after applyingremi samsung arranged an additional qa team whose members develop new test cases for risky apis predicted by remi.
for this experiment we define risky apis to those apis predicted with a or higher buggy probability because the qa team at samsung confirmed that is the mostsuitablecut offforeffectivelyputtingqaeffortintheir rc release cycles.
note that we conduct the experiment for rc3 at the same development starting point where all detected bugs by the modified and created test cases in rc2 have been fixed.
therefore rc3 is developed on one single version rather than two different versions for with and without remi.
column presents the allocated testing resources in man days.
a man day in samsung is defined as eight hours of work per person.
columns and show the number of apis and the number of the corresponding test cases respectively.
the last column represents the number of detected bugs by the test cases.
the result in table indicates that remiimproves the effectiveness of the api testing as newly developed test case stable results of rq2 for the test execution phase.
resources bug detect ability testdetected detection version man hour run bugs rate rc2w oremi .
.
.
w remi .
.
rc3w oremi .
.
.
w remi .
.
for the risky apis to detect additional bugs.
although more resources are consumed by the group using remidue to the large percentage of buggy apis during the early version rc2 remireduces the use of a considerable amount of resources in rc3 .
and .
man days in rc2 and rc3 respectively .
this resource reduction is possible because the number of apis predicted as buggy by remiis much less than that in rc2.
this implies that the reliability of tizen wearable improved after the intensive testing during rc2 by developing new test cases for those highly ranked apis in terms of risk.
table presents the results for the test suite execution phase.
column shows the consumed resources in manhours which are defined as test executions per hour because testers at samsung are advised to execute test cases in minutes.
the last three columns present the number of test cases executed test run the number of detected bugs after test run and the bug detection rate to test run respectively.
to illustrate the efficiency of remi we execute the test cases that are associated with the top of total apis ranked by remi.
for the fair comparison we randomly select the same number of test cases for test run without remi.
because of the randomness of selecting test cases we repeat this times .
as shown in table the bug detectability of the test suite executed using remioutperforms the one without remiin terms of the number of detected bugs with statistical significance wilcoxon signed rank test p .
for both rc2 andrc3 .
inparticular thebug detectionratewith remiis higher than the one without remiin rc2 .
vs .
.
this result demonstrates that remiis effective in ranking bug prone apis by placing more bug prone apis at higher rankings.
however the bug detection rate with remiin rc3 is not as high as that in rc2.
this is because the prediction performance for rc3 is not as accurate as that in rc2 due to the incorrect labeling caused by the build errors of the source code.
this incorrect labeling also affects the low prediction accuracy for package in table .
however despite this exceptional case the number of detected bugs with remiis still higher than without remiwith statistical significance.
therefore adopting remican improve the efficiency of the test execution by increasing the bug detectability of the test suite with the reduced amount of resources.
.
lessons learned conclusion during the case study of remiat samsung we received three distinct comments from five qa members who performed testing activities using remi.
from theses comments we learned the following lessons.
first the list of risky apis provided before conducting qa activities is helpful for testers to allocate their testing effort efficiently especially with tight time constraints.
second in the process of applying remi overheadsariseduringthetoolconfiguration and executions approximately to .
hours .
finally it isdifficult to collect the bug information to label buggy clean apis without noise.
the performance of remi s prediction model is affected by the quality of api labels.
to make remimore reliable the method for labeling apis needs to be improved.
overall our case study demonstrates that remiis effective in improving the efficiency of qa activities but there is still room for improvement such as simpler configuration process and better labeling quality.
inconclusion afterpredictingthetopriskyapisby remi practitioners could efficiently manage resources needed during the qa process.
after applying remitotizen wearable rcsatsamsung theqamembersidentifiedadditionalbugs by developing test cases for risky apis.
additionally given the same amount of resources when executing test cases the qa members could detect more bugs using remi.
as a result the qa process of tizen wearable was improved.
the qa team in samsung plans to improve remimore and to apply it to other software projects.
remialso can be applied to open source api development projects and this remains as our future work.
.