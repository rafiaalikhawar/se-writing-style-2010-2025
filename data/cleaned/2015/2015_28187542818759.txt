views on internal and external validity in empirical software engineering janet siegmund norbert siegmund and sven apel university of passau germany abstract empirical methods have grown common in software engineering but there is no consensus on how to apply them properly.
is practical relevance key?
do internally valid studieshave any value?
should we replicate more to address the tradeoffbetween internal and external validity?
we asked the communityhow empirical research should take place in software engineering with a focus on the tradeoff between internal and external validity and replication complemented with a literature review aboutthe status of empirical research in software engineering.
wefound that the opinions differ considerably and that there isno consensus in the community when to focus on internal orexternal validity and how to conduct and review replications.
i. i ntroduction empirical research in software engineering came a long way.
from being received as a niche science the awareness of its importance has increased.
in empirical studies were found in about of papers of major venues and conferences while in recent years almost all papers of icse esec fse and emse reported some kind of empirical evaluation see section iii .
thus the amount of empirically investigated claims has increased considerably.
with the rising awareness and usage of empirical studies the question of where to go with empirical software engineering research is also emerging.
new programming languages techniques and paradigms new tool support to improve debugging and testing new visualizations to present information emerge almost daily and claims regarding their merits need to be evaluated otherwise they remain claims.
but how should new approaches be evaluated?
do researchers focus on internal validity and control every aspect of the experiment setting so that differences in the outcome can only be caused by the newly introduced technique?
or do they focus on external validity and observe their technique in the wild showing a real world effect but without knowing which factors actually caused the observed difference?
both options maximizing internal or maximizing external validity have their benefits and drawbacks which we illustrate by the example of evaluating the influence of using a new tool on the performance of beginning programmers the first option maximizing internal validity allows researchers to exclude almost all influencing factors so that they can observe in a highly controlled setting whether the new tool improves one aspect of the every day work of beginning programmers.
this way researchers can draw sound conclusions about the reasons of improvement or degradation but at the cost of generalizability.
with the second option maximizing external validity researchers can observe whether the tool has any effect on different types of developers in an every day setting external balance internal would show no value to se community without internal validity the results cannot be trusted you might get a more reliable result but the result could not be used to explain anything about the real world we first need to clearly control confounding factors before eventually being able to generalise include two studies one maximizing internal validity and the other maximizing external fig.
.
preferences for internal vs. external validity among program committee and editorial board members.
but at the cost of not being able to unambiguously understand why the new tool affects the work flow maybe it is just because it is new.
there is an inherent tradeoff in empirical research do we want observations that we can fully explain but with a limited generalizability or do we want results that are applicableto a variety of circumstances but where we cannot reliably explain underlying factors and relationships?
due to the options different objectives we cannot choose both.
deciding for one of these options is not easy and existing general guidelines for example by wohlin or juristo and moreno are too general to assist in making this decision.
with our work we want to raise the awareness of this problem should we focus on internal or external validity?
should we focus on one first and then on the other?
should we balance both kinds of validity not maximizing one?
in the end every time we are planning an experiment wemust ask ourselves do we ask the right questions?
for example is it better to ask principal questions such as whether static type systems ease program comprehension compared to dynamic type systems or is it better to ask broadly which commonly used programming languages are more superior in what circumstances?
do we want pure ground research or applied research with immediate practical relevance?
is there even a way to design studies such that we can answer both kinds of questions at the same time or is there no way aroundreplications i.e.
exactly repeated studies or studies that deviate from the original study design only in a few well selected factors in software engineering research?
in the remainder of this paper we present the results of a literature review to evaluate the kind and extent of empirical methods used in software engineering and to get an impression of the role of internal and external validity and replications sec.
iii followed by example studies maximizing one or the ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee icse florence italy other sec.
iv .
thereupon as a main contribution of this paper we present the results of an online survey among programcommittee and editorial board members key players in their field of major software engineering venues regarding theirperception and opinion on how to address the tradeoff between internal and external validity sec.
v to vii .
in a nutshell we found large differences in the opinions regarding the importance of internally and externally validstudies and a lack of awareness of the tradeoff between the two which we illustrate in figure .
furthermore many survey participants are aware of the need of replication but thereis substantial disagreement about the kind and extent of the delta that is necessary for proper replication.
thus there are different reviewer expectations to a paper and there are no proper guidelines for reviewing a paper in this hindsight.
our research is meant to stimulate researchers across multiple areas rethinking their expectations and standards of empirical research including educating young software engineeringresearchers assisting researchers in evaluating their work helping reviewers in judging the soundness of a research paper and providing guidelines for planning empirical research.
in summary we make the following contributions overview of the state of the art of empirical software engineering in three major empirical software engineering venues with a focus on the role of internal and external validity and replication.
overview of the opinions of the key players of thesoftware engineering community based on a survey among program committee and editorial board members of major software engineering venues.
suggestions on how to conduct empirical research in software engineering.
a discussion of open issues meant to initiate a discussion in the community.
all data from the literature review and survey are available on a supplementary web site de spl janet ese .
as an overarching theme of our work let us quote david parnas on software engineering research it is time to stop exploring and start experimenting.
ii.
r elated work there is considerable work concerned with the status of empirical research or guidelines on how to conduct empirical research in the area of software engineering.
however weare not aware of any work surveying program committee or editorial board members to assess their opinion and suggestions addressing the tradeoff between internal and external validity in empirical software engineering.
guidelines there is a long history of advocating and evaluating empirical research in software engineering.
asearly as basili and others published guidelines onempirical research comprising a framework to describe experimental work.
furthermore basili proposed the goalquestion metric approach to guide researchers in defining theirresearch goals such that the context of an experimental setting is clearly described .
kitchenham and charters proposed guidelines on how to conduct systematic surveys in softwareengineering following guidelines from medical research .
kitchenham and others complement this research with a study on systematic literature reviews and on the repeatability of systematic literature reviews .
ko and others present guidelines for conducting controlled experiments to evaluate software engineering tools with human participants .
these guidelines arrange research activities along ten steps including recruitment and training of participants as well as task design.
siegmund and schuman provide an overview of confounding parameters that influence the outcome of an experiment and that need to be controlled for .
sj berg and others make the case for more realistic settings in software engineeringresearch stressing the role of funding to pay professional developers .
furthermore they report on current problems of empirical research for which the lack of practical relevanceis still an issue among others .
as solutions they suggest to give more competence to empirical researchers e.g.
bytraining or to improve the collaboration between industryand academia.
their vision of empirical research in to years is striving for more practical relevance more synthesis of knowledge and more theory building.
tichy and others reported on the status of experimental research in software engineering compared to optical engineering and neural computation concluding that there is only little empirical research in software engineering .
consequently tichy stated that computer scientists should experiment more .
he also provided guidelines for reviewing empirical research which describe common arguments that reviewers use to reject a paper and explanations for why these are not valid for rejection .
replication there is considerable work in the direction of replication i.e.
a repetition of an experiment under similar conditions but with specified variation such as a new sample .
basili and others stated that too many studies tend to be isolated and are not replicated either by the same researchers or by others .
they describe a framework for categorizing related studies which can then be viewed in context rather than viewing each study in isolation.
shull and others describe the role of exact and conceptual replication in software engineering which both are standard in behavioral science but not in software engineering research as our literature review and our survey show.
juristo and vegas describe the role of nonexact replications explaining that exact replications are almostimpossible to conduct in software engineering research because the context is so complex e.g.
how techniques were applied as well as the knowledge of participants and how they weretrained .
thus many researchers give up e.g.
or do not publish their efforts because of contradicting results.
to improve this situation juristo and vegas suggest to loosen the restrictions for the exactness of replication studies so that some obstacles of replication studies can be removed.
status of empirical research do all these guidelines and insights affect the status of empirical research?
there is evidence that the amount of empirical research has increased while sj berg and others found that in major softwareengineering venues from to only .
of the papers reported a controlled experiment this fraction has increased in recent years for example as observed by ivarsson icse florence italyand gorschek in the domain of requirements engineering .
in our literature review we found a large number of papers that conducted some sort of empirical evaluation section iii .
but does that also count for the quality?
ivarsson and gorschek found that in requirements engineering the rigor of empirical studies has improved but practical relevance has not .
nagappan and others found that selected subject systems cover a wide range of different dimensions such as team size and project size which positively affects external validity .
sj berg and others as well as dyb a and others noted among others that the reports of empirical studies oftenlack important details.
for example threats to validity are often vague and unsystematic despite the numerous guidelines on how to describe empirical studies .
kampenes and others analyzed the conduct of quasi experiments and found that the design analysis and reporting can be improved .
thus despite the long history of advocating empirical research in software engineering there is still much room for improvement which zannier and others nicely phrased iven the numerous clear and repeated messages of which date back almost years and provide results that date even further in history we must ask ourselves at what point will the message become clear?
our work in particular the analysis of the survey results strives for making this message clearer.
iii.
s tate of the art al iterature review as not being addressed by previous work cf.
section ii we conducted a literature review of three of the major empirical software engineering venues to get an overview of the currentstatus of empirical research in software engineering.
our sampleconsisted of all full technical papers of icse esec fse to and emse to the major venues in empirical software engineering.
while this selection is limited it still gives a good impression of the state of the art.
we manually examined each paper regarding the use of empirical methods recruitment of human participants students or professionals replication and presentation of validity.
to this end we skimmed each paper and searched with a set of keywords1.
in figure we provide an overview of the process and findings of the literature review.
first we determined whether an empirical method e.g.
case study controlled experiment was applied which happened in overwhelmingly papers.
this seems like a large increase compared to the .
that sj berg and others foundabout years earlier but to be fair they only included controlled experiments with human participants.
second we also determined whether a study was conducted with or without human participants.
of all papers recruited human participants and had no human participants but evaluated other properties such as performance or test coverage.
now we can draw a more close 1keywords empirical student profession developer subject participant human repeat replicat further.
24replication108 88no differentiation of val.
threats empirical study1 fig.
.
fraction of empirical studies that meet certain criteria.
numbers in circles represent the absolute number of papers to which the circle area is proportional.
gray numbers refer to the paragraphs in the text.
comparison with the sj berg study indicating that the human factor is nowadays considered as more important.
third there is a diversity in the selection of human participants.
of the studies involving human participants recruited students professionals and both.
in papers the participants were not specified closer and in study researchers used mechanical turk.
thus relying on professional programmers is not the exception.
fourth we determined whether a paper reported on a replication of the papers did not conduct a replication did a replication.
of these did an internal replication i.e.
by the same group and reported on an external i.e.
by another group replication one paper was not clear on the kind of replication .
this result suggests that replication studies especially external ones are underrepresented in software engineering.
fifth we looked at the discussion of validity.
of the papers using an empirical method surprisingly papers did not explicitly mention threats to validity at all.
in papers authors discussed threats to validity but did not differentiate between internal or external or otherkinds of validity.
in a few papers the discussion wasnot explicit but hidden in a paragraph of the discussion orconclusion.
the remaining papers differentiated between different kinds of validity mostly internal external construct and conclusion validity .
while this result may be biased by the selection of venues i.e.
conferences and journal conferences impose a strict page limit to which the discussion of validity is often sacrificed it neverthelesssuggests that there is considerable room for improvement in the discussion and documentation of threats to validity.
to summarize empirical research seems to be an integral part of software engineering research nowadays.
however from the methodological point of view the individual standards differ considerably.
icse florence italyiv .
m aximizing internal or external validity to illustrate the merits of internal and external validity as well as to provide a foundation for the survey we introduce two studies as running examples one maximizing internal the other maximizing external validity.
a study set up that maximizes internal validity was designed by hanenberg .
he evaluated whether static type systems as compared to dynamic type systems influence development time.
to control for confounding parameters which might influence the result beside the merits of the two kinds of type systems he developed a language and corresponding ide solely for thepurpose of this experiment.
the language and ide differed onlyin the type system used nothing else.
furthermore hanenberg recruited students with similar programming experience asparticipants and let them implement two small tasks.
in thishighly controlled setting he was able to conclude that a difference in the performance of student programmers is caused only by the type system nothing else.
however how can one generalize the result of such a controlled experiment?
should developers switch to another type system?
obviously giving a recommendation is difficult because the setting of hanenberg s experiment was artificial.
so how about another setting in which several different professional developers from different companies work with different programming languages on every day tasks?
r ohm and others used such a setting to observe how professional programmers work with source code .
while being realistic this set up has a lot of confounding parameters that havenot been controlled for such as the complexity of the task programming language and programming experience of the developers.
thus while such a general setting produces general potentially practically relevant results it is unclear how the results emerged many factors could have affected the results both kinds of study setting are viable and lead to interesting results but which one is preferable and in which situation?
we conducted an online survey among program committee and editorial board members to provide answers to this and related questions.
v. s urvey setup in this section we give a detailed overview of our survey following the guidelines provided by jedlitschka and others .
a. objective with our survey we targeted several research objectives ro1assess the awareness of the community of the tradeoff between external and internal validity.
ro2assess the opinion of the community regarding how to address this tradeoff.
ro3assess the opinion of the community regarding the role of replication.
these objectives emerged from discussions with researchers at different conferences and workshops as well as from reviews of empirical research papers.
we experienced that sometimes 2we refer to objectives rather than questions to avoid any confusion with the actual questions of the survey.there is a lack of appreciation for internally valid studies and that external validity or practical relevance of a study is seen as most important.
thus we assess the awareness of the community ro as well as their suggestions on how to address this tradeoff ro .
furthermore in other disciplines replicating studies is a commonly accepted way to address this tradeoff in medicine or physics only replicated results are accepted.
thus we asked the community about what they think of replication to address this tradeoff in software engineering research ro .
b. participants as participants we contacted the program committee and editorial board members of major empirical softwareengineering venues.
we decided to balance venues with empiri cal focus and venues with a general software engineering focus to reduce the bias toward empirically interested researchers.
this way we can assess the opinion of renowned researchers and experts of their area empirical and not empirical .
clearly the key players shape the future of software engineeringresearch by deciding on the acceptance of research papers guide young researchers and advise funding agencies.
to ensure that the participants have been reviewing current papers we extracted the e mail addresses of members active in the years to from the following venues ase automated software engineering ease evaluation and assessment in software engineering ecoop object oriented programming emse empirical software engineering esec fse foundations of software engineering esem empirical software engineering and measurement gpce generative programming icpc program comprehension icse software engineering icsm software maintenance oopsla object oriented programming tosem software engineering and methodology tse software engineering on average a participant was on the program committee or editorial board of .
different venues with a minimum of and a maximum of different venues.
c. questionnaire and conduct we designed a questionnaire that covers several aspects of empirical research in particular focusing on internal and external validity and replication.
we included severalclosed questions for each of which we additionally askedthe participants to elaborate on their decision.
furthermore we included several open questions asking for suggestions for example do you have any suggestions on how empirical researchers can solve the dilemma of internal vs. external validity of empirical work in computer science?
.
all questions were optional.
to ensure that the participants knew what a highly internally and highly externally valid study looks like we described a research question inspired by hanenberg s study sec.
iv and two settings to evaluate the corresponding research question one maximizing internal validity and one maximizing external validity.
in table i we list all survey questions and map them to our research objectives.
icse florence italytable i questions of the survey to answer the research objectives ro .
b efore the questions we described a research question and two scenarios for its ev aluation one maximizing internal and the other external v alidity .
ro questions answer options which option would you prefer for an evaluation?
we asked this question two times for human and non human studies circlemax.
internal validity circlemax.
external validity circleno preference would it be a reason to reject a paper that does not choose your favorite option?
circleyes circleno in your opinion what is the ideal way to address research questions like the one outlined above?
open did you recommend to reject a paper in the past mainly for the following reasons?
boxint.
validity too low boxext.
validity too low for research questions like the one presented above fp vs. oop do you prefer more practically relevant research or more theoretical ground research?
circleapplied circlebasic circleno preference have you changed how you judged a paper regarding internal and external validity?
circleyes circleno what do you think about a reviewing format with several rounds but with publication guarantees?
open do you have any suggestions on how empirical researchers can solve the dilemma of internal vs. external validity of empirical work in computer science?open during your activity as a reviewer how often have you reviewed a replicated study?
circlenever circlesometimes circleregularly in general how were the replications rated by you... by your fellow reviewers?
circleaccept circleborderline circlereject during your activity as a reviewer did you notice a change in the number of replicated studies?
circleyes increase circleyes decrease circleno do you think we need to publish more experimental replications in computer science?
circleyes circleno as a reviewer of a top ranked conference would you accept a paper that as the main contribution ... ...exactly replicates a previously published experiment of the same group?
circleyes circleno circlei do not know ...exactly replicates a previously published experiment of another group?
circleyes circleno circlei do not know ...replicates a previously published experiment of the same group b u tincreases external validity?
circleyes circleno circlei do not know ...replicates a previously published experiment of another group b u tincreases external validity?
circleyes circleno circlei do not know ...replicates a previously published experiment of the same group but increases internal validity?
circleyes circleno circlei do not know ...replicates a previously published experiment of another group b u t increases internal validity?
circleyes circleno circlei do not know we used surveygizmo for our survey.
in may we e mailed each program committee and editorial board member and asked them to complete the survey within three weeks.
of the people we contacted completed the questionnaire leading to the typical response rate.
some members preferred to have all questions on one page so we created an according version for them.
vi.
r esults and discussion to analyze the answers of the survey we used an open card sorting technique .
to this end we looked for higherorder themes in the open answers of participants for eachquestion.
overall we spent open questions hours per question hours on categorizing answers.
we identified several categories per question several of them occurred across questions.
instead of discussing all identified categories we structure this section along our research objectives.
for each objective we present descriptive statistics of the closed questions if applicable followed by a summary of the categories we found with as minimal interpretation as possible to separate data analysis from interpretation .
on the supplementary web site we provide all identified categories per question including their frequency of occurrence.
we conclude this section with an interpretation and insights we gained.
a. ro awareness of the tradeoff between external and internal validity.
for this objective there are no closed questions so we directly start with the categories we identified in the free text responses of the participants.
categories the responses show a mixed picture.
in particular we found answers indicating that participants are aware of this issue but also statements lacking this awareness.awareness of tradeoff participants stated that both kinds of validity should be balanced which we found times across all questions related to ro .
unawareness of tradeoff by contrast we also found a profound lack of awareness regarding the tradeoff.
one reviewer stated s he would reject a paper that describes a study that maximizes internal validity because it ould show no value at all to se community .
another participant stated that his her opinion regarding the kind of validity changed such that s he now can appreciatestudies with external validity more and that s he has come to loathe ivory tower toy examples .
other interesting insights we also often found that reviewers stated that it depends on different aspects forexample on the research question on the study subjects or on the claims indicating that the kind of validity plays a minor role in judging the merits of a study.
human and non human studies there is a disagreement on whether for human and non human studies the same ordifferent criteria regarding validity should be applied.
the reasons for different criteria lie among others in the effort of human studies non human experiments are be able to scale up to realistic situations at reasonable cost in contrast to human experiments.
they lie in the bias caused by human studies removing humans from the exercise reduces the challenges for internal validity.
in that context knowing how general the approach was would seem a more important issue to address.
or researchers should maximize internal validity for non human studies because this is possible in the first place icse florence italy systems unlike humans can be inspected and explained fully.
we can produce extremely precise theories about the behavior of software that we create and we should.
.
arguments in favor of applying the same criteria for human and non human studies arise among others from the fact that adoption for industry is the key point of software engineering research assess the potential for industrial adoption.
or that independent of the kind of study both kinds of validity are necessary to get a thorough understanding we need both studies and possibly more to get a thorough understanding .
interestingly one even stated the equality of human and nonhuman studies as ground truth it makes no difference with or without humans!
we are talking about software technologies... .
consequences the magnitude of difference in the opinions surprised us starting from the view that internally valid studies would have no value to software engineering research to the view that only a combination of internally and externally valid studies lets us understand a problem in detail.
what can be learned from this result is that researchersshould be aware that there isa tradeoff and that both kinds of validity add valuable information to our body of knowledge.
furthermore we would like to point researchers to the fact that there are strong differences in opinions of key players in software engineering research.
if there is no consensus some might not even be aware of this situation it is difficult toproperly shape the future of software engineering research.
currently getting a paper on a study published seems like a game of chance if authors get a reviewer who is not open to the kind of study that authors report on chances are that the reviewer will argue strongly against the paper possibly leading to the rejection of a methodologically sound study.
generally speaking there are no transparent community standards on empirical research.
on the contrary different program committee and editorial board members have strongly different opinions about internal and external validity without even knowing it.
this is partly reflected in the large number of participants stating that the kind of study depends on several factors.
one participant even stated that it also depends on the resources of the authors of the paper ...what resources did the authors have?
what i expect from a paper out of cisco is different from a paper out of a university.
exaggerating this statement it could mean that it is ok to recruit students as participants in studies conducted by researchersat universities because they lack the resources to recruit professionals however studies conducted by or in companies such as cisco should recruit professionals because they have according resources.
clearly knowing the authors would help in understanding certain tradeoffs regarding resources but would also prohibit conducting double blind reviews which is currentie n01020304047 ie n gpnfrequency a b c fig.
.
frequency distribution of answers.
a b which option would you prefer for an evaluation?
a human studies b non human studies.
nternal validity xternal validity o preference.
c do you prefer more practically relevant research or more theoretical ground research?
round research ractical research o preference.
practice for several conferences such as sigcse or ecoop .
overall these different opinions show the fundamental need for a community agreed standard on how to conduct empirical research in software engineering.
key insights there is a mixed degree of awareness of the tradeoff between internal and external validity.
the opinions on how to handle the tradeoff differ to a large extent.
there are different points of view whether the same or different criteria regarding internal and external validity for human and non human studies should be applied.
there are no transparent community standards for handling the tradeoff between internal and external validity.
b. ro opinion of the community regarding how to address this tradeoff.
descriptives in figure we show the answers to the three closed questions for ro .
they indicate a tendency toward externally valid studies with practical relevance.
categories again we got a mixed picture of which questions researchers should ask but with a clear preference for externally valid studies.
we also found that several reviewers prefer balancing internal and external validity.
the most important reason to favor external validity is practical relevance external validity is very important since it provides indications about the potential for industrial adoption.
leave the ivory tower.
if actual insights for people s lives are supposed to be the outcome of research it better be applied to such problems.
experience from professional developers seems more relevant.
these and further statements indicate that external validity and practical relevance are seen as equivalent.
however this is not entirely true as we will discuss shortly.
in addition to focusing on one study some participants stated that researchers should replicate studies or conduct multiple studies on the same topic as inspired by other sciences.
for example to declare the discovery of the higgs boson particle many replications had to be conducted.
icse florence italy3 consequences external validity vs. practical relevance several participants equated external validity with practical relevance leading us to two interpretations first external validity describes how the results obtained in one experimental setting can be applied to different settings for example to different programming languages tasks or participants.
many answers indicate that a study conducted with professional programmersautomatically has higher external validity than a study with students.
however if researchers use professional programmers in their every day work the results cannot necessarily be applied to students in a university context.
thus a practically relevant study does not necessarily have high external validity.
instead practical relevance is described by the term ecological validity .
admittedly we might have slightly influenced our participants by the way we asked the questions as we discuss in section viii.
second studies involving students are not seen aspractically relevant because the results are applicableto professionals only to a limited extent.
while it istrue that much research is conducted to improve thelife of the professional programmer also students orbeginning programmers are an important populationto be studied especially when they have considerable programming experience.
furthermore there are studies showing that in certain scenarios students are comparable to professionals .
practical impact of studies second some participants stated that studies should have an immediate practical impact my preference towards external validity is only slight.
i am worried that maximizing internal validity easily creates overly academic papers that provide little impact.
.
thus a single study is not seen as a piece of the puzzle but each study needs to immediately lead to general conclusions.
some reviewers suggested to look at the standards in others sciences specifically referring to replications being common.
have hundreds thousands of participants over several years and address very narrow issues e.g.
is medicine x better than y .
we don t see there studies that use participants are done in months and attempt to answer questions of the caliber is ct better than mri .
.
looking at other sciences it is certainly advisable to get away from the view that a single study must provide a definite answerto a substantial research question.
instead combining differentkinds of studies for example a case study to explore hypotheses and controlled experiments to evaluate these hypotheses is a feasible strategy to address the tradeoff.nsr010203040506069 abr abr id n ynfrequency a b c d e fig.
.
frequency distribution of answers.
a how often have you reviewed a replication?
ever ometimes egularly.
b c how were the replications rated... b ...by you?
c ...by others?
ccept orderline eject.
d did you notice a change in the number of replicated studies?i increase ecrease o e do you think we need to publish more experimental replications in computer science?
es o. key insights there is a misconception of the relation between external validity and practical relevance.
a single study is not seen as piece of the puzzle but requires immediate practical impact this is in contrast to the view that studies provide incremental insights into a complete big picture.
replication studies have proved successful in other sciences and should be considered more in softwareengineering research.
c. ro opinion of the community regarding the role of replication descriptives in figure we show the answers to the closed questions regarding ro .
in essence many participants think that there are too few replications in our field.
categories even though replications are common in other sciences to increase the credibility of results they arenot as accepted in software engineering.
for example some participants stated that there should always be something novel in a study one even stated getting a publication accepted that doesn t contribute anything but a new experiment while assessing the same question not even adding artifacts is a good example of hunting for publications just for the sake of publishing.
come on.
however the majority of the participants stated that we need more replication in software engineering showing awareness of this issue.
they gave several reasons for the need and the lack of replication as we discuss next.
delta of a replication participants who appreciate replication studies said that replications are useful as long as they add information to the body of knowledge.
however participants do not agree on what add information means.
ingeneral there are many different points of view regarding howto conduct a replication.
many participants say that a replicationshould add something new or improve an aspect of the original study for example not to make the same methodologicalmistakes again.
some say that a replication should increase icse florence italyexternal validity of a study while others state that internal validity should be increased or that at least the replication has to be done by a different group.
so apparently there is no agreement on the delta of a replication compared to its original study in the community.
reasons for lack of replications the participants mentioned several reasons for why we do not see many replications in software engineering research including that they are difficult to publish and that incentives platforms guidelines standards as well as replication packages are missing i have seen few replications and perform myself a few because they are too difficult to publish there will always be a dumb reviewer to say this is not novel!
... it seems that replication is rarely done since it is costly hard to do often not all details tools software or datasets involved in an earlier study are available and it carries a low impact factor at least in certain venues .
i am not sure though would be appropriate for conferences.
a replication study is appropriatefor conference if new findings arise.
i think that journals are the right outlets for replication studies.
interestingly one participant in our survey stated that eplications are common .
several rounds of reviewing one question in our survey was what do you think about a reviewing format with several rounds but with publication guarantees?
that is the paper is guaranteed to be published independent of the results if the authors conduct a further sound empirical evaluation that improves either internal or external validity.
we got mixed reactions to this suggestion some stating that only the quality of the conducted study counts multiple rounds is a good idea but approving publication must be based on the quality of the research and presentation.
it should not be related to the outcome of the study.
others fear a degradation of quality and that authors will abuse this publication guarantee regrettably my experience is that some authors will undermine this process.
it isn t viable.
nevertheless while the participants fear that the process will be misused decreasing research quality they are not against it in general.
suggestions of our participants to implement this process include providing templates for authors reviewers papers and the process itself.
consequences mismatch the participants mostly agree that there should be more replication in our field but they also argued thatthis is unlikely to happen.
a possible resume is that reading a replication is boring and there is no payoff forneither the authors nor the reviewers.
it seems that there is a certain hypocrisy in that everybody agrees that replications are important but not many researchers want to conduct read and accept them as two participants nicely stated i think that this is a big problem in our discipline.
however in my experience people are inclined to say that replications are important but then reject replication studies for not presenting new problems questions.
i say yes but like everyone else i know i wouldn t actually like to do so.
so it probably won t happen even though we pay lip service to it.
interestingly we could observe a related pattern in our survey see figure b and c the number of participants stating that they would accept a replication b is as high as the number of participants stating that fellow reviewers tend to reject a replication c .
we see three possible explanations first this contradiction might be caused by the possible selection biasin our sample in that mostly participants with an affinity to empirical research responded to our survey which may tend to accept a replication while the majority who did not respond tend to reject it.
second the distribution of answers might indicate a certain mismatch of views or even hypocrisy in thatthe participants believe they tend to accept more replication thantheir fellow reviewers which however might be a biased view.
third participants have different expectations about how to conduct a replication leading to disagreement in the process ofreview.
in any case to increase the appreciation of replications we need to encourage and motivate reviewers to rate them more positively independently of the novelty of the results.
incentives for replication several participants made suggestions on how to change the current situation to support replication in essence we need to create incentives for authors and reviewers.
many participants have the impression that authors do not want to do replications because they expect difficulties getting them accepted.
also reviewers do not want to review replications because there is nothing new to learn.
furthermore reviewing a replication also means more workfor reviewers who would also have to look at the original study to give an informed recommendation about the quality and delta of the replication.
however program committee and editorial board work is already a rather ungrateful job and increasing the workload for reviewers is unlikely to improve the situation.
thus without incentives for authors and reviewers it is unlikely that we will see more replications.
a suggestion of some participants is to have a special platform for replication.
there is already a workshop series specifically for replication replication in empirical software engineering research reser .
but this does not appear to be sufficient because our participants stated that we need more replication and workshops typically have limited impact.
furthermore a designated workshop series gives reviewersthe opportunity to reject a replication that does not have novel or contradicting results based on the argument that it is out of scope and better fits to reser.
this way unpopular replications become banished from mainstream venues so that they will still face a niche existence.
to make replications more accepted there needs to be a place for them in renowned conferences and journals.
this could mean a special track issue or paper categories.
however the community is certainly icse florence italywell advised to be honest to itself who wants to attend a session about replication studies of which the results may be well known?
thus accepted replications may face a difficult role in conferences.
some of our participants mentioned that replications should be published in journals whereas conferences are for presenting novel results.
a special track at renowned software engineering venues such as icse could raise the awareness for the value of replications.
a second suggestion was that there should be standards or guidelines for reviewers and authors on how to rate replications.
for example for a replication the methodological soundness should have more influence on acceptance than the novelty of the results i.e.
it would be ok to confirm a previous result .
this way we can counteract the expectation of exciting results it depends whether the findings contradict the previous ones .
authors could instead focus on the soundness of the study design and they should provide replication packages so that a lack of information does not hinder researchers to replicate astudy.
one of the participants suggested to consult a memberof the original team to conduct the replication because many details of the experiment are not properly described or not published.
in fact there is an experience report of a group of researchers who originally planned to replicate a study but due to the difficulties they encountered despite conversing with theoriginal team they could not conduct the replication .
instead they published their experience with exact replication.
standards on which information to share in which way also learning from other disciplines will help authors when replicating other studies.
third there are considerable differences in the expectation of the delta a replication must provide.
should a replication study count as such only when conducted by a group different than the original group or only when it adds new information or improve the methodology?
is it enough to change the sample to different students subjects or the room daytime of a conduct?
we cannot give an answer to these questions based on the survey and we do not think that there is a general answer because software engineering incorporates numerous different subfields of different maturity and with different requirements.
for example measuring performance is different than a human based study on program comprehension.
thus each subcommunity needs to define its own standards which need to be communicated clearly to the authors and reviewers.
finally our suggestion of multiple reviewing rounds with publication guarantees given sufficient quality of a study received mixed reactions mostly because of the fear of degrading the quality of research and of undermining the whole process.
with a well defined review and publication process we might mitigate this problem as one participant stated sounds interesting but has to be outlined and studied in detail.
key insights there is a certain mismatch in the participants viewon replication studies most participants appreciate replications but see that they are hard to conduct and publish.
neither researchers nor reviewers seem to like to conduct read or accept replications.
there is disagreement on the delta a replication must provide.
suggestions to improve the situation include setting up special platforms and guidelines for reviewers and authors which need to be defined and communicated by the respective subcommunity.
vii.
f urther insights in addition to answering our research objectives we gained several further insights we want to share with the community.
a. paper experiment?
an interesting point that came up across all questions was whether a study should map to to a paper or whether there should be an n to m mapping in particular multiple replication studies making up a single substantial paper.
during our analysis we learned that we and others tend tothink of a study and a paper as interchangeable concepts.
however is this really the way to go?
one participant asked excuse me but are we discussing science and the way it should be done or how to prepare papers to be accepted?
this issue indicates that empirical research in software engineering has come to a point that when designing a study researchers also think in terms of getting a corresponding paperpublished.
but this is not just a problem in software engineering but in many more disciplines as the slogan publish or perish describes.
a possible solution to this dilemma is exercised by plos one in which the evaluation of the worthiness of a result is left to the reader the purpose of the review process is quality assurance such that the conclusions drawn from a study are justified.
b. internal external v alidity vs. artificiality practicality there seems to be a misconception about the relationship of validity and practicality.
we believe that the reasons lie in the close relationship of both concepts an internally valid study is only rarely realistic because many confounding parameters need to be controlled for which easily results in artificial experiment settings.
externally valid studies often are realistic because the lack of control for confounding parameters can lead to several different values for them e.g.
novice to expert programmers .
however internally valid studies can also be realistic and produce generalizable results for example if the selected programming language shares similar properties with other often used programming languages.
thus we should avoid equating external validity with practicality and internal validity with artificiality if internally valid studies are only seen as artificial toy examples or ivory tower research it is hard to raise the appreciation for internally valid studies which icse florence italyare an important way to understand effects in depth.
likewise if only externally valid studies are accepted how can we ever pinpoint the precise factors causing the observed effect?
c. software engineering engineering discipline?
we were surprised by the number of answers stating that participants expect a practical impact of each study because software engineering is an engineering discipline rather than science practically relevant studies are inherent to software engineering.
however there is no reason for why softwareengineering research should not follow standards of naturalscience where internally valid studies can add valuable information to our knowledge base.
maybe the view of software engineering solely as engineering discipline which is still discussed is one reason for the lack of appreciation of internally valid and replication studies?
d. empirical research not for its own sake several participants expressed their concern not to do empirical research only for its own sake.
in a world where publishing papers decides over careers there is certainly the danger that people start conducting replication studies just for increasing their publication count.
given that replication studies will be more accepted in the future one could imagine that it is quite easy to grab low hanging fruit by replicating existing studies.
which degree of replication is healthy?
in some sense we trade confidence in our results by conducting replication studies with the danger of being swamped by studies that have been conducted only of their own sake.
viii.
t hreats to validity a. internal v alidity there is a possible selection bias as it may be that only those program committee and editorial board members responded who have experience with empirical research.
this could mean that there is a bias toward the awareness of the tradeoff betweeninternal and external validity and the appreciation for internally valid and replication studies.
thus one could assume that the software engineering community as whole is less aware and appreciative of these issues.
while the selection bias is relevant it does not affect the big picture that there are many different opinions about which the researchers are not necessarily aware.
another threat arises from the rosenthal effect the wording of the questions might have influenced the participants for example regarding the misconception of external internal validity vs. artificiality practicality and the relation between studies and papers or n m .
hence both insights might not follow to this extent from our sample but we believe they would have occurred anyway only one participant mentioned that external validity and practicality are not the same and one other stated that we should not design studies to get papers accepted.
b. external v alidity reflecting on the insight about the mapping from studies to papers we revisited our design decisions for the survey.
admittedly we also thought about how these decisions wouldaffect acceptance chances especially contacting only programcommittee and editorial board members which threatens external validity.
we cannot say whether the big picture would change when including further researchers.
but as our goal was to get insights from the key players we sufficiently controlled this threat with respect to the scope of our study.
ix.
c onclusion as empirical research has grown common in software engineering it is time to agree on how it should be conducted and how to address the tradeoff between internal and external validity and replications.
reviewing papers that were recently published in major software engineering venues we found that presented an empirical study but only discussed threats to validity and only differentiated betweendifferent kinds of validity.
given that we include emseas major empirical software engineering journal this is an alarmingly high number of authors who do not seem to be aware of the threats to validity to their study.
to get a deeper understanding of the view of the community s key players we asked program committee and editorial board members of major software engineering venues about theiropinions on these and related issues.
we found that many reviewers are not aware of the tradeoff between internal and external validity but at the same time have strong opinions onmaximizing one kind of validity which indicates a lack of community standards on conducting and reviewing empirical studies.
this leads to the situation that getting a paper accepted is a game of chance rather than based on quality or value added to the community.
interestingly a considerable number of participants stated that only externally valid studies best with immediate practical impact have value.
regarding the role of replication we also found a mismatch most participants wish to see more replications but at the same time are reluctant to conduct read or accept them.
apparently in software engineering there is a lack of incentives for conducting replication studies e.g.
low impact low acceptance chance high effort and a lack of standards on how to design and review replications in particular on the delta of a replication .
thus software engineering does not seem to be comparable to other engineering or social disciplines.
so we must ask ourselves how can we shape and promote empiricalsoftware engineering if we cannot agree on what it should like?
having made these different points of view explicit we hope that they initiate a discussion in the community and provide a starting point for guidelines and standards of empirical software engineering both for authors and reviewers.
finally we would like to stress that our goal is not to judge or offend any reviewers or authors.
on the contrary we highly appreciate the time and effort the participants took to answer our questions which documents their interest in this issue.