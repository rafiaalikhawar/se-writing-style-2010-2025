cross project defect prediction using a connectivity based unsupervised classifier feng zhang1 quan zheng1 ying zou2 and ahmed e. hassan1 1school of computing queen s university canada 2department of electrical and computer engineering queen s university canada feng quan ahmed cs.queensu.ca 2ying.zou queensu.ca abstract defect prediction on projects with limited historical data has attracted great interest from both researchers and prac titioners.
cross project defect prediction has been the main area of progress by reusing classifiers from other projects.
however existing approaches require some degree of ho mogeneity e.g.
a similar distribution of metric values be tween the training projects and the target project.
satisfy ing the homogeneity requirement often requires significanteffort currently a very active area of research .
an unsupervised classifier does not require any training data therefore the heterogeneity challenge is no longer anissue.
in this paper we examine two types of unsupervised classifiers a distance based classifiers e.g.
k means and b connectivity based classifiers.
while distance based un supervised classifiers have been previously used in the defect prediction literature with disappointing performance connectivity based classifiers have never been explored before in our community.
we compare the performance of unsupervised classifiers versussupervisedclassifiersusingdatafrom26projectsfrom three publicly available datasets i.e.
aeeem nasa and promise .
in the cross project setting our proposed con nectivity based classifier via spectral clustering ranks asone of the top classifiers among five widely used supervisedclassifiers i.e.
random forest naive bayes logistic regres sion decision tree and logistic model tree and five unsupervised classifiers i.e.
k means partition around me fuzzy c means neural gas and spectral clustering .
in the within project setting i.e.
models are built and applied on the same project our spectral classifier ranks in the second tier while only random forest ranks in the first tier.
hence connectivity based unsupervised classifiers offer a viable so lution for cross and within project defect predictions.
ccs concepts software and its engineering software verification and validation software defect analysis permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citationon the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permissionand or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c circlecopyrt copyright held by the owner author s .
publication rights licensed to acm.
isbn .
.
.
.
project!heterogeneity supervised classifierbuildtraining projects defect proneness unsupervised classifier figure illustration of the heterogeneity challenge.
keywords defectprediction heterogeneity cross project unsupervised spectral clustering graph mining .
introduction a defect is an error in a software system that causes a system to behave improperly or produce unexpected re sults.
fixing defects typically consumes about of thetotal budget of a software project .
such cost can be significantly reduced if defects are fixed in an early stage .
hence defect prediction modelsare often used to prioritize quality improvement and defectavoidance efforts.
however defect prediction is not widely adopted in industry .
the barriers include the cost of collectingup to date training data e.g.
defect data the low generalizability of prediction models and thelack of automated tooling for the prediction process .
moreover many companies lack the needed resources and technical expertise to prepare data for building defect prediction models .
a typical solution i.e.
cross project prediction is to apply defect prediction models that are built using data from other training projects using supervised classifiers .
as illustrated in figure the major challenge in crossprojectprediction comes from the heterogeneitybetween thetraining projects and the target project .
the heterogeneity may be caused by diverse development settings e.g.
varyinguserrequirementsanddeveloperexperiences .
it is common that the distribution of metric values of software entities e.g.
files or classes exhibits significant differences across projects with varied contexts e.g.
s i z ea n d programming language .
another heterogeneity chal2016 ieee acm 38th ieee international conference on software engineering lenge in cross project prediction as pointed out recently by nam and kim is that different projects may have dif ferent sets of metrics all together.
to mitigate such challenges an unsupervised classifier could be used.
as shown in figure such classifiers donot require any training data and are therefore by naturefree of the challenges that are due to heterogeneity of the training and target projects.
however distance based unsupervised classifiers e.g.
k means have shown disappointing performance for within project defect prediction e.g.
.
in this study we propose to apply a connectivity based unsupervised classifier that is based on spectral clustering .
unlike distance based unsupervised classifiers thatpartition the data based on euclidean distance spectral clustering considers the connectivity among all entities andtherefore has many advantages .
in defect prediction the connectivity among software entities can be determined by their similarity in metric values.
our key intuition for ex ploring spectral clustering is that defective entities tend tocluster around the same neighbourhoods i.e.
c l u s t e r s a s observed as well by by menzies et al.
and bettenburg et al.
in their work on local prediction models.
to evaluate the feasibility of using unsupervised classifiers for cross project prediction we perform an experimentusing three publicly available datasets i.e.
aeeem nasa and promise that include projects intotal.
our major findings are presented as follows unsupervised classifiers underperform supervised clas sifiers in general.
however a connectivity based un supervised classifier i.e.
via spectral clustering can compete with supervised classifiers.
inthecross projectsetting ourproposedspectralclustering based classifier achieves a median auc value of .
and ranks as one of the top classifiers.
in the within project setting our spectral clusteringbased classifier ranks in the second tier the same as three commonly used supervised classifiers i.e.
logistic regression logistic model tree and naive bayes .
the random forest classifier appears in the first rank.
a deeper investigation confirms our intuition that de fective entities have significantly stronger connections with other defective entities than with clean entities.
as a summary we propose to tackle cross project predictions from a different perspective i.e.
using a connectivitybased unsupervised classifier.
our spectral classifier is relatively simple the implementation with lines of r codeis provided in appendix a .
moreover our spectral classifier is unsupervised therefore it can be applied on a project without training data.
paper organization.
section presents the background and related work.
in section we describe details of our spectral classifier.
experimental setup and case study results are presented in sections and respectively.
section closely examines the defect data in order to better understand the strong performance of our spectral classifier.
the threats to validity of our work are discussed in section .
we conclude the paper and provide insights for future work in section .software metricsunsupervised clusteringcluster cluster 2cluster labelerdefective cleanunsupervised classifier figure a typical process to do defect predictionusing an unsupervised classifier.
.
background and related work in this section we first present the related work on crossproject defect prediction and unsupervised defect prediction.
we then describe essential backgrounds on unsupervised classifiers.
.
cross project defect prediction prior attempts for cross project defect prediction often resulted in disappointing performance e.g.
.
the major challenge is the heterogeneous distribution ofmetric values between the training projects and the target project .
to reduce the heterogeneity in cross project defect prediction there are two major approaches using a model derived from training entities that are most similar to the entities in the target project e.g.
for instance he et al.
propose to filter the training set based on distributional characteristics e.g.
mean and standard deviation of both the training and the target sets.
turhan et al.
propose to perform nearest neighbour filtering nnfiltering .
transforming the metrics of both the training projects and the target project to increase their similarity e.g.
.
forinstance ma et al.
applytransfer naive bayes tnb nam et al.
use transfer component analysis tca chen et al.
use double transfer boosting dtb model.
our previous work proposes a context aware rank transformation approachthat transforms software metrics based on project contexts e.g.
programminglanguageandprojectsize and builds a universal defect prediction model that achievescomparable performance as within project models.
anotherchallengeincross projectdefectpredictionisthat thesetofmetricsisoftendifferentamongprojects.
namandkim propose an approach to deal with heterogeneous sets of metrics between the training projects and the target project.
.
unsupervised defect prediction unsupervised defect prediction predicts defect proneness without requiring access to training data.
as illustratedin figure a typical process for predicting defects usingan unsupervised classifier has two steps clustering soft ware entities into kclusters usually two clusters and labelling each cluster as a defective or clean cluster.
however thereexistsalimitednumberofstudiesintheliterature 310on unsupervised defect prediction.
one reason is that unsupervised classifiers usually underperform supervised ones e.g.
random forest and logistic regression in terms of their predictive power.
an initial attempt to use unsupervised defect classifiers is by zhong et al.
who apply k means and neural gas clustering in defect prediction.
zhong et al.
observe that a neural gas classifier outperforms k means in terms of predictive power but runs slower.
however their approach requires one to specify the expected number of clusters andinvolves experts to determine which cluster contains defec tive entities i.e.
label the cluster .
catal et al.
propose to use metric values to label the clusters.
bishnu and bhattacherjee propose to apply quad trees to initialize the cluster centres of k means clustering.
in addition to kmeans clustering based classifiers abaei et al.
propose to use self organizing maps som and yang et al.
propose to apply the affinity propagation clustering algorithm.recently nam and kim proposed to label the clusters using thresholds on selected metrics.
.
background on unsupervised classifiers unsupervised classifiers make use of clustering methods.
clustering is a common way to explore groups of similar entities.
frequently applied clustering methods include hierarchical clustering and k means.
hierarchical clustering produces clusters based on the structure of a similarity or dissimilarity matrix.
k means clustering is used to cluster high dimensional data that are linearly separable .
in recent years spectral clustering has become one of the most effective techniques for clustering .
unlikedistance based classifiers e.g.
k means clustering that divide a data set based on euclidean distance spectral cluster ing partitions a data set based on the connectivity between its entities.
spectral clustering is performed on a graph consisting of nodes and edges.
in the context of defect prediction each node represents a software entity e.g.
fi l eo r class .
each edge represents the connection between soft ware entities and its weight is measured by the similarity of metric values between its two ends.
similarity definition.
a widely used similarity is the dot product between vectors of two nodes iandj as shown in equation .
w ij xi xj m summationdisplay k 1aikakj wherexiandxjdenote the metric values of software entities iandj respectively aikis the value of the kthmetric on the ithsoftware entity and mis the total number of metrics.
from the geometric perspective the similarity wijcan be interpreted as xi xj xi xj cos ij where xi and xj are the norms and ijis the angle between two vectors.
it is the length of the projection of one vector onto the other unit vector.
from a correlation perspective the similarity wijis basically the unnormalized pearson correlation coefficient between nodes iandj.
each element in vector x irepresents a metric value.
it is unnormalized since it makes little sense to normalize the values across metrics belonging to the same software entity.
the similarity wijcan be positive negative or zero.
a positive value indicates a positive correlation between two software entities and a negative value indicatesa negative correlation.
a value of zero indicates that there is no linear correlation.
it is meaningless to study the self algorithm spectral clustering based classifier for defect prediction input a matrix with rows as software entities and columns as metrics.
output a vector of defect proneness of all software entities.
normalize software metrics using z score.
construct a weighted adjacency matrix w. calculate the laplacian matrix lsym.
perform the eigendecomposition on lsym.
select the second smallest eigenvector v1.
perform the bipartition on v1using zero.
label each cluster as defective or clean.
circle of a software entity therefore we set the self similarity i.e.
a l lw ii t oz e r o .
spectral clustering steps.
a popular algorithm for spectral clustering is to minimize the normalized cut .the normalized cut is a disassociation measure to describe the cost of cutting two partitions in a graph .
this algorithm partitions a graph into two subgraphs to gain high similarity within each subgraph while achieving low similarity across the two subgraphs.
the input for spectral clustering is a weighted adjacency matrix that stores the similarity between each pair of nodes in the graph.
there are three major steps in the algorithm computing the laplacian matrix from the weighted adjacency matrix where the laplacian matrix is a widely used matrix representation of a graph in graph theory performing an eigendecomposition on the laplacian matrix selecting a threshold on the second smallest eigenvector to obtain the bipartitions of the graph.
.
our spectral classifier in this section we describe details on our spectral clustering based classifier see algorithm .
the rimplementation of our spectral classifier consists of lines of code see appendix a .
.
preprocessing software metrics software metrics have varied scales.
hence software metrics are often normalized before further processing .
for instance nam et al.
find that applying z score to normalize software metrics can significantly improve the predictive power of defect prediction models.
the advantageofz score is that a normalized software metric has a mean value of zero and a variance of one.
our spectral classifier uses the z score for the normalization of each metric.
we use y jto denote a vector of values of thejthmetric in a project.
then yj a1j ... a nj t wherenis the number of entities in the project and aijis the value of the jthmetric on the ithsoftware entity.
the vectoryjis normalized as yj yj yj sj where yjis the average value of yjandsjis the standard deviation of yj.
this step corresponds to line in algorithm .
.
spectral clustering we now describe the three steps for spectral clustering.
step .
the first step is to calculate the laplacian matrix lsym.
the symmetric laplacian matrix lsymis derived from the adjacency matrix wthat stores the similarity between each pair of software entities.
the adjacency matrix wis computed directly from the normalized software metrics i.e.
line in algorithm .
in spectral clustering there is usually an assumption that all values of the similar ity are non negative .
hence we set all negative w ijto zero.
the symmetric laplacian matrix lsymis calculated usinglsym i d 2wd i.e.
line in algorithm where the matrix iis the unit matrix with size n the matrixdis a diagonal matrix of row sums of w a n dd diag d ... d 2n where d i summationtextn j 1wij .
step .
the second step is to perform the eigendecomposition on the symmetric laplacian matrix lsym i.e.
l i n e4 in algorithm .
eigenvalues will always be ordered increasingly .
we follow the normalized cut algorithm byshi and malik and use the second smallest eigenvectorfor clustering i.e.
line in algorithm .
we use v 1to denote the second smallest eigenvector of lsym.
step .
the third step is to separate all entities into two clusters.
shi and malik propose to apply a particularthreshold such as zero or median on the second smallesteigenvector v .
if the median is used then of entities are predicted as defective.
inspecting of entities re quires significant effort.
hence we adopt zero as the thresh old value of v i.e.
line in algorithm to create two non overlapped clusters.
we use v1ito denote the ithvalue ofv1 where i ... n a n dnis the total number of software entities in the given project.
the value v1icorresponds to the eigenvalue of the ithsoftware entity.
all entities with v1i create a cluster called cpos and all entities with v1i create the other cluster called cneg.
in the following subsection we describe how to determinewhether cluster c poscontains defective entities or cluster cnegdoes.
.
labelling defective cluster the last step i.e.
line in algorithm of applying the spectral clustering based classifier in defect prediction is to label the defective cluster.
we usecdefective to denote the cluster that contains defective entities only and use ccleanto represent the cluster that contains clean entities only.
to determine whether cposorcnegis the defective cluster cdefective we use the following heuristic for most metrics software entities containing defects generally have larger values than software entities without defects.
this heuristic is based on our field s extensive empirical observations on the relationship between software metrics and defect proneness.
for instance gaffney find that larger files have a higherlikelihood to experience defects than smaller files.
kitchen hamet al.
report that more complex files are more likely to experience defects than files with lower complexity.
similar findings are also observed in many other studies e.g.
.
with this heuristic in mind we use the average row sums of the normalized metrics of each cluster to determine whichcluster is defective.
the row sum is the sum of all metricvalues of the same entity.
we compute the average row sum of all entities within each cluster i.e.
either c posorcneg .
the cluster with larger average row sum is considered asthe cluster containing defective entities.
we label all entitieswithin this cluster as defective i.e.
c defective and all the remaining entities as clean i.e.
cclean .
however the aforementioned heuristic does not necessarily work for all kinds of metrics.
for instance in the case where smaller values indicate less chance of defects the aforementioned heuristic should be reversed.
we suggestpractitioners to derive the appropriate heuristic based ontheir set of metrics.
.
experiment setup in this section we present the experimental setup to evaluate the performance of our spectral classifier.
.
corpora we examined data from three commonly studied datasets aeeem nasa and promise .
the threedatasets are publicly available and have been used exten sively in defect prediction studies e.g.
.
abrief description on each dataset and our selected metricsare presented as follows.
d1.the aeeem dataset was prepared by d ambros et al.
to compare the performance of different sets of metrics.
accordingly theaeeemdatasetcontainsthe most number of metrics.
in particular it has met rics including product process previous defect met rics and entropy based metrics.
all projects in the aeeem dataset have identical software metrics.
we use all metrics in our study.
d2.the nasa dataset was collected by the nasa met rics data program.
shepperd et al.
observe that the original nasa dataset contains many repeated andinconsistent data points and they clean up the nasadataset.
inthisstudy weusethecleanednasadatasetthat is available in the promise repository.
in the nasa dataset projects do not share the same setofmetrics.
forinstance projectkc3has39metrics while project jm1 has metrics.
since supervisedclassifiers require exact the same sets of metrics weonly select the metrics that are common across all of the studied nasa projects.
d3.the promise dataset was prepared by jureczko and madeyski .
it contains open source java projectsand has object oriented metrics.
in the promise dataset projects do not have the same set of metrics.
hence we select the metricsthatarecommonacrossallofthe10studiedpromiseprojects.
in general the selected projects have diverse size i.e.
having to instances and varied percentage of defective entities i.e.
ranging from .
to .
.
the summary of all selected projects is presented in table .
more details about these metrics can be found on the correspond ing website of each dataset.
312table an overview of the studied projects.
dataset project o fe n t i t i e sdefective aeeemeclipse jdt core .
equinox .
apache lucene .
mylyn .
eclipse pde ui .
nasacm1 .
jm1 .
kc3 .
mc1 .
mc2 .
mw1 .
pc1 .
pc2 .
pc3 .
pc4 .
pc5 .
promiseant v1.
.
camel v1.
.
ivy v1.
.
jedit v4.
.
log4j v1.
.
lucene v2.
.
poi v3.
.
tomcat v6.
.
xalan v2.
.
xerces v1.
.
average .
.
performance measure there are many performance measures such as precision recall accuracy f measure and the area under the receiver operating characteristic curve auc .
however a cut off value on the predicted probability of defect proneness is required when computing precision recall accuracy and f measure.
the default cut off is .
which may not be thebest cut off value in practice .
on the other hand theauc value is independent of a cut off value and is not impacted by the skewness of defect data.
lessmann et al.
and ghotra et al.
suggest to use the auc value for better cross dataset comparability.
hence we select the auc measure as our performance measure.
when computing the auc measure a curve of the false positive rate is plotted against the true positive rate.
accordingly the auc value measures the probability that a randomly chosen defective entity ranks higher than a ran domly chosen clean entity.
an auc value of .
implies that a classifier is no better than random guessing.
a larger auc value indicates a better performance.
in particular gorunescu advises the following guideline to interpretthe auc value .
to .
as excellent prediction .
to .
as a good prediction .
to .
as a fair prediction .
to .
as a poor prediction and .
to .
as a failedprediction.
.
classifiers for comparison to find if our spectral classifier is applicable for defect prediction in a cross project setting we compare its perfor mance with nine off the shelf classifiers.
we not only select supervised classifiers but also choose distance based unsu pervised classifiers.
for supervised classifiers we select five classifiers that have been commonly applied to build defect prediction models.
the five classifiers are random forest rf naive bayes nb logistic regression lr decision tree j48 and logistic model tree lmt .fordistance basedunsupervisedclassifiers wechoosefour classifiers that have been previously used in the defect pre dictionliterature .
thefourclassifiersinclude k means clustering km partition around me pam fuzzy cmeans fcm and neural gas ng .
these classifiers arebased on euclidean distance therefore employ a differentclustering mechanism than spectral clustering sc .
.
scott knott test to compare the performance across the large number of datasets we apply the scott knott test using the confidence level i.e.
.
.
the scott knott test can overcome the issue of overlapping multiple comparisons that are obtained from other tests such as the mann whitney u test .
the scott knott test has been used in defect prediction studies to compare the performance across differentclassifiers .
the scott knott test recursively ranks the evaluated classifiers through hierarchical clustering analysis.
in each iteration the scott knott test separates the evaluated classifiers into two groups based on the performance measure i.e.
t h e auc value .
if the two groups have statistically significantdifference in the auc value the scott knott test executesagain within each group.
if no statistically distinct groupscan be created the scott knott test terminates .
.
case study results in this section we present our research questions along with our motivation approach and findings.
rq1.
how does our spectral classifier perform in cross project defect prediction?
motivation.
unlike supervised classifiers unsupervised classifiers do not have to deal with the challenge of heterogeneity between the training projects and the target project.
while distance based classifiers e.g.
k means clustering underperform supervised classifiers connectivity based unsupervised classifiers have not been explored in our com munity.
hence it is of significant interest to investigate ifconnectivity based classifiers particularly via spectral clus tering can provide comparable performance as supervisedclassifiers in the context of cross project defect prediction.approach.
to address this question we need to get the performance of all studied classifiers for each project.
for each classifier all entities of the target project are used to obtain its performance.
supervised classifiers require a training project.
all supervised classifiers under study require the exact same setof metrics between the training and the target projects.as the three studied datasets i.e.
aeeem nasa and promise have different sets of metrics we make crossproject defect prediction within the same dataset.
for each target project we select all other projects from the same dataset for training.
for instance if the target project is eclipse jdt core then each supervised classifier is used to build four models using each of the remaining projects within the same dataset i.e.
equinox apache lucene mylyn and eclipse pde ui respectively.
we compute the average auc values of these four models to measure the performance of the corresponding classifier on the target project since it is unknown which model performs the best on the target project prior to the prediction.
313sc rf fcm lmt nb lr ng km pam dt0.
.
.
.
.
.
.
.85aeeemauc sc rf nb lmt fcm pam lr ng km dt0.
.
.
.
.
.
.
.85nasaaucsc nb rf lmt lr km ng fcm pam dt0.
.
.
.
.
.
.
.85promiseauc sc rf nb lmt lr fcm pam km ng dt0.
.
.
.
.
.
.
.85all projectsauc figure the boxplots of auc values of all studied supervised blue labels and unsupervised red labels classifiers for the abbreviations see sec tion .
.
different colors represents different ranks red yellow green blue .
unsupervised classifiers do not require training projects.
we directly apply the studied unsupervised classifiers on thetarget project.
when do clustering we create kclusters.
we setk for clustering since this setting yields the best performance in defect prediction e.g.
.
in the resulting two clusters one cluster is labelled as defective and the other cluster is labelled as clean using the heuristic that is described in section .
.
to compare the predictive power among all classifiers we apply the scott knott test with the confidence level torank all classifiers across projects within the same dataset.
we examine the scott knott ranks per dataset.
furthermore we perform one large scott knott run where we inputall the auc values for all the classifiers across all datasets.
findings.
our spectral classifier achieves good results for defect prediction in the cross project setting.in general our spectral classifier significantly outperforms all other unsupervised classifiers and it has slightlybetter performance than the best supervised classifier under study i.e.
r a n d o mf o r e s t .
our spectral classifier ranks the first in all the three studied datasets.
the colors in figure illustrate the ranks of all classifiers.
the boxplots show the distribution of the aucvalues of each classifier under study.
classifiers with boxplots of the same color are ranked at the same tier.
the per formances of classifiers in the same tier are not statistically distinct.
among all supervised and unsupervised classifiers only two supervised classifiers i.e.
random forest and logistic model tree are in the same ranking tier across all thethree datasets as our spectral classifier.
t h ee x a c ta u cv a l u e so ft h et o pf o u rc l a s s i fi e r s i.e.
o u r spectral classifier random forest naive bayes and logisticmodel tree on each project are presented in table .
in particular the median auc values of the top four classifierstable the auc values of the top four classifiers in cross project defect prediction bold font highlightsthe best performance .
dataset project sc rf nb lmt aeeemeclipse jdt core .
.
.
.
equinox .
.
.
.
apache lucene .
.
.
.
mylyn .
.
.
.
eclipse pde ui .
.
.
.
nasacm1 .
.
.
.
jm1 .
.
.
.
kc3 .
.
.
.
mc1 .
.
.
.
mc2 .
.
.
.
mw1 .
.
.
.
pc1 .
.
.
.
pc2 .
.
.
.
pc3 .
.
.
.
pc4 .
.
.
.
pc5 .
.
.
.
promiseant v1.
.
.
.
.
camel v1.
.
.
.
.
ivy v1.
.
.
.
.
jedit v4.
.
.
.
.
log4j v1.
.
.
.
.
lucene v2.
.
.
.
.
poi v3.
.
.
.
.
tomcat v6.
.
.
.
.
xalan v2.
.
.
.
.
xerces v1.
.
.
.
.
median .
.
.
.
across all projects under study are .
.
.
and .
respectively.
we observe that distance based unsupervised classifiers e.g.
k means do not perform as well as supervised classifiers.
the poor performance of these distance based classifiers may explain why unsupervised classi fiers are not widely applied in defect prediction.
in summary our results clearly show that applying connectivity based unsupervised classification is a promising di rection to tackle the heterogeneity challenge in cross projectdefectprediction.
ourconnectivity basedunsupervisedclas sifier is based on spectral clustering.
we suspect that the success of spectral clustering is because defective entities are more similar to other defective entities than other cleanentities in terms of the values of their various software met rics.
such intuition is supported through recent work by menzies et al.
and bettenburg et al.
on local defect prediction models.
our spectral classifier performs the best among all studied classifiers that include five supervised and five unsupervised classifiers.
therefore applying the connectivity based unsupervised classification is apromising direction to tackle the challenge of heteroge neous data in cross project defect prediction.
rq2.
does our spectral classifier perform well in within project defect prediction?
motivation.
in comparison to a cross project setting the chance of experiencing heterogeneous training and target data is much lower in a within project setting.
as unsupervised classifiers can save significant effort in defect data collection we are interested to find if our connectivity based 314table the average auc values of the top five classifiers in both cross project cp and within project settings wp .
the column diff shows the difference between cross project models and within project models.
dataset projectrf lr sc lmt nb cp wp diff cp wp diff cp wp diff cp wp diff cp wp diff aeeemeclipse jdt core .
.
.
.
.
.
.
.
.
.
.
.
.
.
equinox .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
apache lucene .
.
.
.
.
.
.
.
.
.
.
.
.
.
mylyn .
.
.
.
.
.
.
.
.
.
.
.
.
.
eclipse pde ui .
.
.
.
.
.
.
.
.
.
.
.
.
.
nasacm1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
jm1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
kc3 .
.
.
.
.
.
.
.
.
.
.
.
.
mc1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
mc2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mw1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
pc1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
pc2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
pc3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
pc4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
pc5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
promiseant v1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
camel v1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ivy v1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jedit v4.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
log4j v1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lucene v2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
poi v3.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tomcat v6.
.
.
.
.
.
.
.
.
.
.
.
.
.
xalan v2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
xerces v1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
median .
.
.
.
.
.
.
.
.
.
.
.
.
.
table ranks of all studied classifiers for withinproject defect prediction based on evaluations.
overall classifier median average standard ranks rank rank deviation rf .
.
2lr .
.
sc .
.
lmt .
.
nb .
.
.
fcm .
.
4pam .
.
.
ng .
.
dt .
.
km .
.
.
unsupervised classifier i.e.
the proposed spectral classifier can still compete with supervised classifiers in a withinproject setting.approach.
to evaluate the performance of supervised classifiers in a within project setting the essential step is to sep arate all entities of a project into two sets.
one set is fortraining a model and the other one is the target set to applythe model.
both supervised and unsupervised classifiers are applied on the same target set of entities.
the only difference is that supervised classifiers require an additional stepto build a model from the training set of entities.
to create the training and target sets we apply a two fold cross validation i.e.
a random split that has been previously applied in the defect prediction literature .
for a random split each classifier is evaluated twice the first half is used as the training data while the other half is used as the target data and the second half is used as the training data while the first half is used as the target data.
to deal with the randomness of sampling we repeatthe random splits for times i.e.
times of two foldcross validation .
in total evaluations are performed for each classifier on each project.
to get the performance of each classifier on each project we compute the average auc value of the total evaluations.
to find statistically distinct ranks of all classifiers we follow the approach of ghotra et al.
and perform a double scott knott test.
the double scott knott test ensures arobust ranking of all classifiers across projects regardless o ft h e i re x a c ta u cv a l u e s .
t h efi r s ts c o t t k n o t tt e s ti sperformed on each individual project to rank all classifiersbased on their auc values for that particular project.
theobtained ranks are used in the second run of the scott knott test to yield a global ranking of all classifiers across all studied projects.
findings.
generally speaking in a within project setting supervised classifiers outperform unsupervised classifiers .
there is only one unsupervised classifier i.e.
our spectral classifier among the top five classifiers.
the detailed rankings are presented in table including the global ranks of all classifiers across all projects and the statistics i.e.
median average and standard deviation ofthe ranks of each classifier as obtained in the first scottknott test on the results of evaluations.
in particular our spectral classifier has a median rank of and is ranked in the same tier as three widely used classifiers i.e.
logistic regression logistic model tree and naive bayes.
the actual auc values of the top five classifiers i.e.
r a n dom forest logistic regression our spectral classifier logis tic model tree and naive bayes on each project are presented in table .
the auc values in both cross project andwithin project settings are shown as well as their difference i.e.
the auc value in a within project setting minus the auc value in a cross project setting .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
defect ratiodifferencerf lr ourslmt nb figure the regression lines of the performance difference of the top five classifiers between a withinproject setting and a cross project setting over the ratio of defects of each project.
the dotted line isthe horizontal base line.
our spectral classifier achieves almost the same predictive power between cross project and within project settings across all projects under study ass h o w ni nt a b l e4 .
the size of the target project in a within project setting is only half of that in a cross projectsetting highlighting that our spectral classifier tends to berobust when the size of the target project changes.
a within project model can sometimes significantly underperform a cross project model although a within project model generally outperforms a crossproject model.
for example looking at table and for project ivy v1.
the top four supervised classifiers experi enceadowngradedperformancewhenchangingfromacrossproject setting to a within project setting.
in particular the random forest classifier achieves an auc value of .
in across project setting but yields a lower auc value of .67in a within project setting.
we conjecture that the decreasein performance when changing to a within project setting is caused by the low ratio of defects i.e.
the low percentage of defective entities in the target project.
for instance project ivy v1.
has a ratio of defects of .
with only16 defective entities.
similar observations are noted in other projects such as apache lucene and pc2 .
supervised classifiers tend to experience a performance decrease if the ratio of defects becomes lower.
to illustrate the relationships between the performance of each classifierand the ratio of defects we plot regression lines of the performance difference of the top five classifiers over the ratio of defects in figure .
in comparison to supervised classifiers our spectral clustering based classifier is more robust acrossa varying ratio of defects.
one possible reason is that supervised classifiers experience a significant class imbalance problem on these projects while our spectral classifier isan unsupervised approach therefore has no issue of class imbalance.
we conjecture that for projects with a low ratioof defects our spectral clustering based classifier may be more suitable than the supervised classifiers.
in a within project setting our spectral classifier ranks in the second tier with only random forest ranking in the first tier.
however our spectral classifier may be more suitable for projects with heavily imbalanced i.e.
very low percentage of defective entities defect data.
.
why does it work?
in this section we present an in depth analysis to understand why our spectral classifier which is a connectivity based classifier achieves good results in defect prediction.as aforementioned spectral clustering separates all entitiesin a project based on the connections among entities.
weconjecture that software entities may reside within two so cial network like communities one community is formu latedbydefectiveentities and2 theotheroneisestablishedby clean entities.
.
essential definitions community definition.
wedefineacommunityasaset of members i.e.
software entities that have much strongerconnections with each other than with members from other communities.
a connection is basically an edge in a graph as mentioned in section .
.
we define an edge betweenentitiesiandjusing equation .
e ij wij where wij 1ifwij a n d1 wij 0otherwise.
as described in section .
wijrepresents the similarity or the correlation between entities iandj.h e n c e eijequals to if there is a positive correlation between entities iand j.w ed e n o t et h es e to fa l le d g e sa se thene eij .
we construct the community as follows.
for each project we partition the entities into two sets based on their defect proneness.
we use vdto denote the set of actual defective entities and vcto denote the set of actual clean entities.
a software entity can be either defective or clean.
hence there is no overlap between vdandvc and the union of vd andvccontains all entities within the same project.
connectivity measurement.
we define degdd the total degree of all defective entities using equation .
we definedegdd the total degree of all clean entities using equation .
similarly we define degcd the total number of edges between each pair of defective and clean entities using equation .
deg dd summationdisplay i vd summationdisplay j vdeij j negationslash i degcc summationdisplay i v c summationdisplay j vceij j negationslash i degcd summationdisplay i vc summationdisplay j vdeij to measure the connectivity among entities within vdor vc or between vdandvc we further define the ratio of edges i.e.
connections as follows.
dd degdd vd vd cc degcc vc vc cd degcd vc vd to illustrate the computation we present an example in figure .
there are three defective and four clean entities.each defective entity has connections to all other two defective entities.
hence deg dd 6and dd .
.
similarly we can get degcc 8and 316clean entities defective entities cc dd cd figure illustrating example of computing the ratio of edges i.e.
dd cc cd .
cc .
and degcd 2a n d cd .
.
.
hypotheses for each project we compute the ratios dd cc a n d cdbased on the actual defect proneness.
to compare the connectivity among entities across all projects under study we test the following hypotheses h0 there is no difference in the ratios of connections from defective entities to other defective entities dd and clean entities cd.
h02 there is no difference in the ratios of connections from clean entities to other clean entities cc and defective entities cd.
hypotheses h01andh02are two sided and paired since each project has three unique values dd cc a n d cd.t o test the hypotheses we apply paired mann whitney u testusing the confidence level i.e.
.
.
we further compute the cliff s as the effect size to quantify the difference.
both the mann whitney u test and the cliff s are non parametric statistical methods and do not requirea particular distribution of assessed variables.
an effect sizeis large if cliff s .
.
.
empirical findings we observe that in general the connections between defective and clean entities are weaker than the con nection among defective entities and the connections among clean entities.
table presents the detailed values of our three measures i.e.
cc cd a n d dd f o re a c h project.
for instance in project eclipse jdt core theratio of connections among defective entities dd .
.
the ratio of connections among clean entities cc .
.
these two ratios are significantly greater than the ratio ofconnections between clean and defective entities which is cd .
.
defective entities have significantly stronger connections with other defective entities than with clean entities.
the pvalue of the mann whitney u test is is .20e when comparing the ratios ddand cdacross all projects.
the difference is large as the corresponding cliff s is .
.
.
similarly clean entitieshave significantly stronger connections with other clean entities than with defective entities i.e.
t h e p value of the mann whitney u test is .55e .
the difference is also large as cliff s is .
.
.
as a summary our observation indicates that either defectiveorcleanentitiesaresimilarintermsofmetricvalues buttable the values of cc cd a n d ddfor each project.
bold font highlights the minimum valueper row .
dataset project cc cd dd aeeemeclipse jdt core .
.
.
equinox .
.
.
apache lucene .
.
.
mylyn .
.
.
eclipse pde ui .
.
.
nasacm1 .
.
.
jm1 .
.
.
kc3 .
.
.
mc1 .
.
.
mc2 .
.
.
mw1 .
.
.
pc1 .
.
.
pc2 .
.
.
pc3 .
.
.
pc4 .
.
.
pc5 .
.
.
promiseant v1.
.
.
.
camel v1.
.
.
.
ivy v1.
.
.
.
jedit v4.
.
.
.
log4j v1.
.
.
.
lucene v2.
.
.
.
poi v3.
.
.
.
tomcat v6.
.
.
.
xalan v2.
.
.
.
xerces v1.
.
.
.
median .
.
.
defective and clean entities are less likely to experience sim ilar metric values.
in other words there roughly exist twocommunities based on defect proneness.
entities within thesame community have stronger connections than cross com munities.
this may be the reason as to why the proposedconnectivity based unsupervised classifier i.e.
o u rs p e c t r a l classifier achieves empirically good results in defect prediction.
there roughly exist two communities of entities a defective community and a clean community of entities.
within community connections are significantlystronger than cross community connections.
.
threats to v alidity in this section we describe the threats to validity of our study under common guidelines by yin .
threats to conclusion validity concern the relation between the treatment and the outcome.
the major threat isthat we only compare our approach with off the shelf clas sifiers.
future work should explore state of the art cross project defect classifiers.
unfortunately the implementation of such specialized classifiers are rarely available and oftenrequire a considerable amount of setup making them hardfor practitioners to easily adopt.
hence we chose to compareagainst commonly used and readily available classifiers.
threats to internal validity concern our selection of subject systems and analysis methods.
we select projectsthat have been commonly used in the defect prediction lit erature.
these projects are from different domains include both open source and industrial projects and have different sets of metrics.
however evaluating our approach on a 317large scale of projects is always desirable.
nevertheless our findings raise a very poignant point about the importance ofexploring connectivity based unsupervised classifiers in fu ture defection prediction research.
moreover the simplicityof our approach makes exploring it in future studies as avery lightweight and simple step to perform.
threats to external validity concern the possibility to generalize our results.
our approach only requires softwaremetrics that can be computed in a standard way by publiclyavailable tools.
however only metrics that are collected in the three data sets are applied in our experiments.
replica tion studies using different sets of metrics may prove fruitful.
threats to reliability validity concern the possibility of replicating this study.
all the three studied data sets are publicly available.
moreover the rimplementation of our approach is provided in appendix a. .
conclusion as new or small projects do not have sufficient training data cross project defect prediction has attracted great in terest from both researchers and practitioners e.g.
.
the major challenge in cross projectdefect prediction is the heterogeneity between the trainingprojects and the target project e.g.
different distributions of metric values and different sets of metrics .
this study brings a new insight to tackle this challenge using connectivity based unsupervised classifiers.
unsupervised classifiers do not require any training data and therefore have no issue of heterogeneity.
apart from distance based unsupervised classifiers e.g.
k means clustering the connectivity based unsupervised classifiers assume that de fective entities tend to cluster around the same area a similar intuition as the recent work on local prediction models by menzies et al.
and bettenburg et al.
.
toevaluatetheperformanceof our proposedspectral classifier we perform experiments using projects from three publicly available datasets i.e.
aeeem nasa and promise .
the results show that the proposed connectivity based unsupervised classifier i.e.
o u rs p e c t r a l classifier achieves impressive performance in a cross project setting.
specifically our spectral classifier ranks as one of the top classifiers among five supervised classifiers e.g.
r a n dom forest and five unsupervised classifiers e.g.
k means .
in a within project setting our spectral classifier ranks in the second tier the same as three widely used supervised classifiers e.g.
logistic regression logistic model tree and naive bayes with random forest as the only classifier in the first tier.
as a summary our contributions are as follows demonstrating that connectivity based unsu pervised classification particularly via spectral clustering performs well in a cross project setting.our experiments show that our connectivitybased unsupervised classifier via spectral clustering can achieve similar or better performance than severalcommonly used supervised and unsupervised classifiers.
we believe that unsupervised classification holds greatpromiseindefectprediction especiallyinacrossproject setting and for highly skewed within project settings.
demonstrating the existence of two defectiveand clean separated communities of softwareentities based on the connectivity between theentities in each community.
we believe that this observation highlights the importance for the softwareengineering research community to explore more ad vanced techniques for unsupervised defect predictioninstead of current strong reliance on supervised classi fiers.
appendix a. r implementation of our spectral classifier in listing we present the rimplementation of our spectral classifier.
listing rimplementation of our approach.
1spectral clustering based classifier function a normalize software metrics.
norma apply a function x x mean x sd x construct the weighted adjacency matrix w. w norma t norma set all negative values to zero.
w set the self similarity to zero.
w w diag diag w construct the symmetric laplacian matrix lsym.
dnsqrt diag sqrt rowsums w i diag rep nrow w lsym i dnsqrt w dnsqrt perform the eigendecomposition.
ret egn eigen lsym symmetric true pick up the second smallest eigenvector.
v1 dnsqrt ret egn vectors v1 v1 sqrt sum v1 divide the data set into two clusters.
defect proneness v1 label the defective and clean clusters.
rs rowsums norma if mean rs mean rs defect proneness v1 return the defect proneness.
defect proneness