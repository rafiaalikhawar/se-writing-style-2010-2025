test suite parallelization in open source projects a study on its usage and impact jeanderson candido luis melo marcelo d amorim federal university of pernambuco pernambuco brazil jbc5 lhsm damorim cin.ufpe.br abstract dealing with high testing costs remains an important problem in software engineering.
test suite parallelization is an important approach to address this problem.
this paper reports our findings on the usage and impact of test suite parallelization in open source projects.
it provides recommendations to practitioners and tool developers to speed up test execution.
considering a set of popular java projects we analyzed we found that of the projects contain costly test suites but parallelization features still seem underutilized in practice only .
of costly projects use parallelization.
the main reported reason for adoption resistance was the concern to deal with concurrency issues.
results suggest that on average developers prefer high predictability than high performance in running tests.
i. i ntroduction dealing with high testing costs has been an important problem in software engineering research and industrial practice.
several approaches have been proposed in the research literature to address this problem with the focus mainly on test suite minimization prioritization reduction and selection .
in industry the focus has been mainly on distributing the testing workload.
evidence of this are the google tap system and the microsoft cloudbuild system which provide distributed infrastructures to efficiently build massive amounts of code and run tests.
building in house server clusters is also a popular mechanism to distribute testing workloads.
for example as of august the test suite of the groupon pwa system which powers the groupon.com website included over 19k tests.
to run all those tests under 10m groupon used a cluster of computers with cores each .
at large organizations the alternative of renting cloud services or even building proprietary infrastructures for running tests is a legitimate approach to mitigate the regression testing problem.
however for projects with modest or nonexistent budgets and yet relatively heavy testing workloads this solution may not be economically viable.
for these cases the use of commodity hardware is an attractive solution for running tests.
the proliferation of multi core cpus and the increasing popularization of testing frameworks and build systems which today provide mature support for parallelization enable speedups through increased cpu usage see section ii .
these two elements demand for cost effective test execution and supply of relatively inexpensive testing infrastructures inspired us to investigate test suite parallelization in practice.this paper reports on an empirical study we conducted to analyze the usage and impact of low level parallelization to speed up testing in open source projects.
this is a relevant problem given the tremendous popularity of open source development and regression testing research .
note that parallelization is complementary to other approaches to mitigate testing costs such as safe test selection and continuous integration .
the dimensions of analysis we considered in this study are i feasibility ii adoption iii speedup and iv tradeoffs.
the dimension feasibility measures the potential of parallelization to reduce testing costs.
in the limit parallelization would be fruitless if all projects had short running test suites or if the execution cost was dominated by a single test case in the suite.
the dimension adoption evaluates how often existing open source projects use parallelization schemes and how developers involved in costly projects not using test suite parallelization perceive this technology.
it is important to measure resistance of practitioners to the technology and to understand their reasons.
the dimension speedup evaluates the observed impact of parallelization in running times.
finally the dimension tradeoffs evaluates the relationship between speedups obtained with parallelization and issues that arise when running tests in parallel including test flakiness .
we briefly summarize our findings in the following.
feasibility.
to assess how prevalent long running test suites are we selected popular java projects from github containing maven build files .
section iv details our methodology to select subjects and to isolate our experiments from environmental noise.
results indicate that nearly of the projects take at least 1m to run and of the projects take at least 5m to run.
considering the projects with test suites taking longer than a minute to run the average execution time of a test suite was 9m.
results also show that test cases are typically short running typically taking less than half a second to run.
furthermore we found that only in rare cases few test cases monopolize the overall time to run a test suite.
adoption.
we considered two aspects in measuring technology adoption.
first we measured usage of parallelism in opensource projects.
then we ran a survey with developers to understand the reasons that could explain resistance to using the technology.
considering only the projects whose test suites take longer than a minute to run we found that only .
of them use parallelism.
we also contacted developers from a .
c ieeease urbana champaign il usa technical research838 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
selection of costly projects that did not use parallelization to understand the reasons for not using parallelization.
dealing with concurrency related issues e.g.
the extra work to organize test suite to avoid concurrency errors and the availability of continuous integration services were the most frequently answered reasons for not considering parallelization.
speedups.
we used two setups to measure speedups.
in one setup we measured speedups obtained on projects that run test suites in parallel by default.
in the other setup we evaluated how execution scales with the number of available cores in the machine.
considering the first setup results indicate that the average speedup of parallelization was .53x.
although we found cases with very high speedups e.g.
.8x for project jcabi we also found cases where the speedups were not very significant.
considering the scalability experiment we noticed perhaps as expected that parallelization obtained with forking jvms scales with the number of cores but the speedups are bounded by long running test classes.
tradeoffs.
test flakiness is a central concern when running tests in parallel.
dependent tests can be affected by different schedulings of test methods and classes.
this dimension of the study measures the impact of different parallel configurations on test flakiness and speedup.
overall results indicate that configurations that fork jvms do not achieve speedups as high as other more aggressive configurations but they manifest much lower flakiness ratios.
our observations may trigger different actions incentivize forking.
forked jvms manifest very low rates of test flakiness.
developers of projects with long running test suites should consider using that feature which is available in modern build systems today e.g.
maven .
break test dependencies.
non forked jvms can achieve impressive speedups at the expense of sometimes impressive rates of flakiness.
breaking test dependencies with electrictest for example to avoid flakiness is advised for developers with greater interest in efficiency.
refactor tests for load balancing.
the configuration with forked jvms scales better when the test workload is balanced across testing classes.
automated refactoring could help balance the workload in scenarios where developers are not willing to change test code but have access to machines with a high number of cores.
improve debugging for build systems.
while preparing our experiments we found scenarios where maven s executions did not reflect corresponding junit s executions.
those issues can hinder developers from using parallel testing.
better debugging support for build systems could help on that.
the artifacts we produced as result of this study are available from the following web page testsuite parallelization .
ii.
p arallel execution of testsuites figure illustrates different levels where parallelism in test execution can be obtained.
the highest level indicatesparallelism obtained through different machines on the network.
for instance using virtual machines from a cloud service to ... high low ... threadscpusmachines... fig.
.
levels of parallelism.distribute test execution.
the lowest levels denote parallelism obtained within a single machine.
these levels are complementary the lowest levels leverage the computing power of server nodes whereas the highest level leverages the aggregate processing power of a network of machines.
this paper focuses on low level parallelism where computation can be offloaded at different cpus within a machine and at different threads within each cpu.
this form of parallelism is enabled through build systems spawning processes in different cpus and testing frameworks spawning threads in one given cpu .
it is important to note that a variety of testing frameworks provide today support for parallel test execution e.g.
junit testng and nunit as to benefit from the available power of popular multi core processors.
in the following we elaborate relevant features of testing frameworks and build systems for parallelization.
we focused on java maven and junit but the discussion can be generalized to other language and tools.
a. testing frameworks the list below shows the choices to control parallelism within one java virtual machine jvm .
these options are offered by the testing framework e.g.
junit .
sequential c0 .
no parallelism is involved.
sequential classes parallel methods c1 .
this configuration corresponds to running test classes sequentially but running test methods from those classes concurrently.
parallel classes sequential methods c2 .
this configuration corresponds to running test classes concurrently but running test methods sequentially.
parallel classes parallel methods c3 .
this configuration runs test classes and methods concurrently.
notice that an important aspect in deciding which configuration to use or in designing new test suites is the possibility of race conditions on shared data during execution.
data sharing can occur for example through state that is reachable from statically declared variables in the program or through variables declared within the scope of the test class or even through resources available on the file system and the network .
considering data race avoidance configuration c1 is preferable over c2 when it is clear that test methods in a class do not manipulate shared state which can be challenging to determine .
similarly c2 is preferable over c1 when it is clear that several test methods in a class perform operations involving shared data.
configuration c3 does not restrict scheduling orderings.
consequently it is more likely to manifest data races during execution.
note that speedups depend on several factors including the test suite size and distribution of test methods per class.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
b. build systems forking os processes to run test jobs is the basic mechanism of build systems to obtain parallelism at the machine space see figure .
for java based build systems such as maven and ant this amounts to spawning one jvm on a given cpu to handle a test job and aggregating results when jobs finish.
the list below shows the choices to control parallelism through the build system e.g.
maven .
forked jvms with sequential methods fc0 .
the build system spawns multiple jvms with this configuration assigning a partition of the set of test classes to each jvm.
test classes and methods run sequentially within each jvm.
forked jvms with parallel methods fc1 .
with this configuration the build system forks multiple jvms as fc0 does but it runs test methods concurrently as c1 does.
note from the listing that forking can only be combined with configuration c1 see section ii a as maven made the design choice to only accept one test class at a time per forked process.
maven offers an option to reuse jvms that can be used to attenuate the potentially high cost of spawning new jvm processes on every test class if reuse is enabled and also to achieve test isolation if reuse is disabled .
plugin groupid org.apache.maven.plugins groupid artifactid maven surefire plugin artifactid configuration forkcount 1c forkcount reuseforks true reuseforks parallel methods parallel threadcount threadcount configuration plugin fig.
.
configuration fc1 on maven.
example figure shows a fragment of a maven configuration file known as pom.xml highlighting options to run tests using the parallel execution mode fc1.
maven implements this feature through its surefire junit test plugin .
with this configuration maven forks one jvm per core forkcount parameter and uses five threads threadcount parameter to run test methods parallel parameter within each forked jvm.
maven reuses created jvms on subsequent forks when execution of a test class terminates reusefork parameter .
iii.
o bjects of analysis we used github s search api to identify projects that satisfy the following criteria the primary language is java1 the project has at least stars the latest update was on or after january 1st and the readme file contains the string mvn.
we focused on java for its popularity.
although there is no clearcut limit on the number of github stars to define relevant projects we observed that one hundred stars was enough to eliminate trivial subjects.
the third criteria serves to skip projects without recent activity.
the 1in case of projects in multiple languages the github api considers the predominant language as the primary language.
stars pushed mvn 20in readme sort stars fig.
.
query to the github api for projects that use java contains at least stars has been updated on january 1st or later contains the string mvn in the readme file.
fourth criteria is an approximation to find maven projects.
we focused on maven for its popularity on java projects.
important to highlight that as of now the github s search api can only reflect contents from repository statistics e.g.
number of forks main programming language it does not provide a feature to search for projects containing certain files e.g.
pom.xml in the directory structure.
figure illustrates the query to the github api as an http request.
the result set is sorted in descending order of stars.
we used the following methodology to select projects for analysis.
after obtaining the list of potential projects from github we filtered those containing a pom.xml file in the root directory.
then considering this set of maven projects we executed the tests for three times to discard those projects with issues in the build file and non deterministic results observed from sequential executions.
as of august 25th our search criteria returned a total of subjects.
from this set of projects projects were not maven or did not have a pom.xml in the root directory projects were not considered because of environment incompatibility e.g.
missing dbms projects were discarded because of flaky tests .
a flaky test is a test that passes or fails under the same circumstances leading to non deterministic results.
as some of our experiments consist of running tests on different threads we ignored these projects as it would be impractical to identify whether a test failed due to a race condition or some other source of flakiness.
from the remaining projects with deterministic results we eliminated projects with or more failing tests as to reduce bias.
for the remaining projects with failing tests we used the junit s ignore annotation to ignore failing tests.
our final set of subjects contains projects.
figure summarizes our sample set.
fig.
.
we fetched popular projects hosted on github.
from this initial sample we ignored projects without maven support with missing dependencies projects with flaky tests and projects had at least of failing tests.
we considered projects to conduct our study.
to run our experiments we used a core i7 .
ghz intel processor machine with eight virtual cpus four cores with two native threads each and 16gb of memory running ubuntu .
lts trusty tahr bit version .
we used java and maven .
.
to build projects and run test suites.
to process test results and generate plots we used python bash r and ruby.
all source artifacts are publicly available for replication on our website .
this includes supporting scripts and the full list of projects.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iv.
e valuation we pose the following research questions organized by the dimensions of analysis we presented in section i. feasibility rq1.
how prevalent are time consuming test suites?
rq2.
how is time distributed across test cases?
adoption rq3.
how popular is test suite parallelization?
rq4.
what are the main reasons that prevent developers from using test suite parallelization?
speedups rq5.
what are the speedups obtained with parallelization in projects that actually use it ?
rq6.
how test execution scales with the number of available cpus?
tradeoffs rq7.
how parallel execution configurations affect testing costs and flakiness?
a. feasibility rq1.
how prevalent are time consuming test suites?
to evaluate prevalence of projects with time consuming test suites we considered the projects appearing in figure .
figure illustrates the script we used to measure time.
we took the following actions to isolate our environment from measurement noise.
first we observed that some test tasks called test unrelated tasks e.g.
javadoc generation and static analyses that could interfere in our time measurements.
to address that potential issue we inspected maven execution logs from a sample including a hundred projects prior to running the script from figure .
the tasks we found were ignored from execution lines .
furthermore we configured our workstation to only run essential services as to avoid noise from unrelated os events.
the machine was dedicated to our experiments and we accessed it via ssh.
in addition we configured the isolcpus option from the linux kernel to isolate six virtual cpus to run our experiments leaving the remaining cpus to run os processes .
the rationale for this decision is to prevent context switching between user processes running the experiment and os related processes.
finally to make sure our measurements were fair we compared timings corresponding to the sequential execution of tests using maven with that obtained with junit s default junitcore runner invoked from the command line.
results were very close.
the main loop lines of the script in figure iterates over the list of subjects and invokes maven multiple times lines .
it first makes all dependencies available locally line compiles the source and test files line and then runs the tests in offline mode as to skip the package update task enabled by default line .
after that we used a regular expression on the output log to find elapsed times line .
we ran the test suite for each subject three times reporting averaged execution times in three ranges tests that run within a minute short tests that run in one to five minutes medium 1maven skips drat.skip true dmaven.javadoc.skip true djacoco.skip true dcheckstyle.skip true dfindbugs.skip true dcobertura.skip true dpmd.skip true dcpd.skip true 6for subj in subjects do cd subjects home subj mvn clean dependency go offline mvn test compile install dskiptests maven skips compile.log mvn test o fae maven skip testing.log cat testing.log grep text total time tail n 15done fig.
.
bash script to measure time cost of test suites.
for each subject we fetch all dependencies compile the source and test files and execute the tests in offline mode ignoring non related tasks.
test unrelated tasks are omitted.
long medium short groupnumber of projects a long medium short0.
.
.
.
.
grouptime cost in minutes b fig.
.
a number of projects in each cost group and b distribution of running times per cost group.
and tests that run in five or more minutes long .
figure a shows the number of projects in each group.
as expected long and medium projects do not occur as frequently as short projects.
however they do occur in relatively high numbers.
figure b shows the distribution of execution time of test suites in each of these groups.
note that the y ranges are different.
the distribution associated with the short group is the most unbalanced right skewed .
the test suites in this group ran in or less seconds for over of the cases.
considering the groups medium and long however we found many costly executions.
nearly of the projects from the medium group take .
or more minutes to run and nearly of the projects from the long group take minutes to run.
we found cases in the long group were execution takes more than minutes to complete as can be observed from the outliers in the boxplot.
it is important to note that we under estimated running times as we missed test modules not enabled for execution in the root pom.xml.
for instance the project apache.maven surefire runs all unit tests in a few seconds.
according to our criteria this project is classified as short but a closer look reveals that only smoke tests are executed in this project by default.
in this project integration and system tests which take longer to run are only accessible via custom parameters which we do not handle in our experimental setup.
we enabled such parameters for this specific project and observed that testing time goes to nearly minutes.
for simplicity we considered only the tests executed by default.
from the testable projects successfully executed all tests and reported some test failure.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
mediumlong 0246time cost in secs fig.
.
distribution of test case time per project.
from these subjects only subjects have more than of failing tests .
on average .
answering rq1 we conclude that time consuming test suites are relatively frequent in open source projects.
we found that of the projects we analyzed i.e.
nearly in every projects take at least minute to run and of them take at least minutes to run.
rq2.
how is time distributed across test cases?
section iv a showed that medium and long running projects are not uncommon accounting to nearly of the projects we analyzed.
research question rq2 measures the distribution of test costs in test suites.
in the limit if cost is dominated by a single test from a large test suite it is unlikely that parallelization will be beneficial as a test method is the smallest working unit in test frameworks.
figure shows the time distribution of individual test cases per project.
we observed that the average median times see dashed horizontal red lines were small namely .08s for medium projects and .16s for long projects and the standard deviations associated with each distribution were relatively low.
high values of are indicative of cpu monopolization.
we found only a small number of those.
the highest value of occurred inuber chaperone a project from the long group.
this project contains only tests of which take less than .5s to run one of which takes nearly 3s to run two of which take nearly 11s to run four of which takes on average 3m to run and two of which take 8m to run.
for this project .
of the execution cost is dominated by of the tests without these two costly tests this project would have been classified as short running.
we did not find other projects with such extreme time monopolization profile.
project facebookarchive linkbench is also classified as longrunning and has the second highest value of .
for this project however cost is distributed more smoothly across tests of which .
take more than 1s to run with the rest of the tests running faster.
figure a shows the difference in the distribution of test suite sizes across groups.
this figure indicates that long projects have a higher median and much higher average number of test cases.
furthermore we noted a strong positive correlation between running time and number of test on projects in the long group.
considering the medium group the correlation between these two variables was weak.
figure b illustrates long medium groupsnumber of test cases x102 a long medium number of test cases x102 time cost in minutes b fig.
.
a size of test suites b size versus running time of test suites.
the regression lines between these the variables test suite cost and number of test cases.
to sum we observed that for projects with long running test suites running time is typically justified by the number of test cases as opposed to the cost of individual test cases.
answering rq2 overall results indicate that projects with a very small number of tests monopolizing end to end execution time were rare.
time most often is distributed evenly across test cases.
b. adoption rq3.
how popular is test suite parallelization?
to answer rq3 we used projects from the medium and long groups where parallelization can be more helpful.
we used a dynamic and a static approach to find manifestations of parallelism.
we discuss results obtained with these complementary approaches in the following.
dynamic checking to find dynamic evidence of parallelism we ran the test suites from our set of projects to output all key value pairs of maven parameters.
to that end we used the option xto produce debug output and the option dskiptests to skip execution of tests.
we skipped execution of tests as we observed from sampling that only bootstrapping the maven process suffices to infer which parallel configuration modes it uses to run the tests.
it is also important to point that we used the default configurations specified in the project.
we inferred parallel configurations by searching for certain configuration parameters in log files.
according to maven s documentation a parallel configuration depends either on the parameter parallel to define the parallelism mode within a jvm followed by the parameter threadcount or the parameter forkcount2to define the number of forked jvms.
as such we captured for each project all related key value pairs of maven parameters and mapped those pairs to one of the possible parallelization modes.
for instance if a given project contains a module with the parameter forkcount 1c forkcount the possible classifications are fc0 or fc1 depending on the presence and the value of the parameter parallel .
if the parameter parallel is set to methods the detected mode will be fc1.
large projects may contain several test suites distributed on different 2this parameter is named forkmode in old versions of maven surefire.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
maven modules potentially using different configurations.
for those cases we collected the maven output from each module discarding duplicates as to avoid inflating counts for configuration modes that appear in several modules of the same project.
for instance if a project contains two modules using the same configuration we counted only one occurrence.
considering our set of projects we found that only of those projects had parallelism enabled by default with only configurations c2 c3 and fc0 being used.
configurations c3 and fc0 were the most popular among these cases.
note that these results under approximate real usage of parallelism as we used default parameters in our scripts to spawn the maven process.
that decision could prevent execution of particular test modules.
table i shows the projects we identified where parallelism is enabled by default in maven.
column subject indicates the name of the project column table i subjects with parallel test execution enabled by default .
group subject ofmodemodules medium californium c2 medium chaos c2 long flink fc0 long log4j2 fc0 long javaslang c3 medium jcabi c3 long jet fc0 long mahout fc0 long mapdb c3 medium opennlp fc0 medium rultor c3 medium takes c3 long vavr c3 of modules indicates the fraction of modules containing tests that use the configuration of parallelism mentioned in column mode .
we note that considering these projects the modules that do not use the configuration cited use the sequential configuration c0.
for example three modules from log4j2 use sequential configuration.
it came as a surprise the observation that no project used distinct configurations in their modules.
static checking given that the dynamic approach cannot detect parallelism manifested through the default configuration of projects we also searched for indications of parallelism in build files.
we parsed all pom.xml files under the project s directory and used the same approach as in our previous analysis to classify configurations.
we noticed initially that our approach was unable to infer the configuration mode for cases where the decision depends on the input e.g.
parallel parallel.type parallel .
for these projects the tester needs to provide additional parameters in the command line to enable parallelization e.g.
mvn test dparallel.type classesandmethods .
to handle those cases we considered all possible values for the parameter in this case parallel.type .
it is also important to note that this approach is not immune to false negatives which can occur when pom.xml files are encapsulated in jar files or files downloaded from the network.
consequently this approach complements the the dynamic approach.
overall we found projects manifesting parallelism with this approach.
compared to the set of projects listed in table i we found four new projects namely google cloud dataflowjavasdk using configuration c3 mapstruct using configuration fc0 t sne java using configuration fc0 and urbanairship datacube using configuration c3 .
curiously we also found that project jcabi rultor and takes were not detected using this methodology.
that happened because these projects loaded a pom.xml file from a jar file that we missed.
considering the static and dynamic methods together we found a total of distinct projects using parallelism corresponding to the union of the two subject sets.
answering rq3 results indicate that test suite parallelization is underused.
overall only .
of costly projects out of use parallelism.
rq4.
what are the main reasons that prevent developers from using test suite parallelization?
to answer this research question we surveyed developers involved in a selection of projects from our benchmark with time consuming test suites.
the goal of the survey is to better comprehend developer s attitude towards the use of parallelism as a mechanism to speedup regression testing.
we surveyed developers from a total of projects.
from the initial list of project we discarded projects that we knew a priori used parallelization and projects that we could not find developer s emails from commit logs.
from this list of projects we mined potential participants for our study.
more precisely we searched for developer s name and email from the last commits to the corresponding project repository.
using this approach we identified a total of eligible participants.
finally we sent plain text e mails containing the survey to those developers.
in total developers replied but we discarded three replies with subjective answers.
considering projects covered by the answers a total of projects .
of the total were represented in those replies.
note that multiple developers on each project received emails.
in one specific case one developer worked in multiple projects and we consider it as a different answer.
we sent the following set of questions to developers how long does it take for tests to run in your environment?
can you briefly define your setup?
do you confirm that your regression test suite does not run in parallel?
select a reason for not using parallelization a i did not know it was possible b i was concerned with concurrency issues c i use a continuous integration server d some other reason.
please elaborate.
considering question we confirmed that execution time was compatible with the results we reported in section iv a .
furthermore of the participants indicated the use of continuous integration ci to run tests with of these participants reporting that test suites are modularized and those modules are tested independently in ci servers through different parameters.
those participants explained that such practice helps to reduce time to observe test failures which is the goal of speeding up regression testing.
a total of participants answered that they do run tests in their local machines.
note authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
however that ci does not preclude low level parallelization.
for example installations of open source ci tools e.g.
jenkins in dedicated servers would benefit from running tests faster through low level test suite parallelization.
considering question the answers we collected indicated to our surprise that six of the projects execute tests in parallel.
this mismatch is justified by cases where neither of our checks static or dynamic could detect presence of parallelism.
a closer look at these projects revealed that one of them contained a pom.xml file encapsulated in a jar file similar case as reported in section iv b in one of the projects the participant considered that distributed ci was a form of parallelism and in four projects the team preferred to implement parallelization instead of using existing features from the testing framework and the build system in two projects the team implemented concurrency control with custom junit test runners and in two other projects the team implemented concurrency within test methods.
note that considering these four extra cases ignored two distributed ci cases the usage of parallelization increases from .
to .
.
we do not consider this change significant enough to modify our conclusion about practical adoption of parallelization rq3 .
considering question the distribution of answers was as follows.
a total of .
of the developers who answered the survey did not know that parallelism was available in n a d cba fig.
.
summary of developer s answers to survey question .maven option a .
of developers mentioned that they did not use parallelism concerned with possible concurrency issues option b .
of developers mentioned that continuous integration suffices to provide timely feedback while running only smoke tests i.e.
short running tests locally option c and .
of developers who provided an alternative answer option d mentioned that using parallelism was not worth the effort of preparing the test suites to take advantage of available processing power.
a total of .
of participants did not answer the last question of the survey.
the pie chart in figure summarizes the distribution of answers.
answering rq4 results suggest that dealing with concurrency issues i.e.
the extra work to organize test suite to safely explore concurrency was the principal reason for developers not investing in parallelism.
other reasons included availability of continuous integration services and unfamiliarity with the technology.
c. speedups rq5.
what are the speedups obtained with parallelization in projects that actually use it ?
to answer rq5 we considered the subjects from our benchmark that use parallelization by default see table i .
we compared running times of test suites with enabled parallelization as configured by project developers and without parallelization.
it is important to note that there are no observedtable ii speedup or slowdown of parallel execution tp over sequential execution ts .
d efault parallel configuration of maven is used .
highest slowdown speedup appears in gray color .
group subject ts tp ts tp medium californium .45m .40m .04x medium chaos .51m .47m .03x medium flink .79m .57m .59x long log4j2 .24m .21m .00x medium ja vaslang .18m .82m .20x medium jcabi .76m .30m .20x long jet .26m .67m .25x long mahout .38m .15m .51x long mapdb .06m .58m .17x medium opennlp .30m .55m .36x medium rultor .30m .27m .52x medium takes .00m .19m .53x long v avr .26m .25m .45x av erage .53x failures in either execution.
table ii summarizes results.
lines are sorted by project names.
columns group and subject indicate respectively the cost group and the name of the project.
column ts shows sequential execution time and column tp shows parallel execution time.
column ts tp shows speedup or slowdown.
as usual a ratio above 1x indicates speedup and a ratio below 1x indicates slowdown.
results show that on average parallel execution was .
times faster compared to sequential execution.
three cases worth special attention log4j2 chaos and takes .
we note that parallel execution in log4j2 was ineffective.
we found that maven invokes several test modules in this project but the test modules that dominate execution time run sequentially by default.
this was also the case for the highlighted project californium .
no significant speedup was observed in chaos a project with only three test classes of which one monopolizes the bulk of test execution time.
this project uses configuration c2 which runs test classes in parallel but runs test methods declared in each class sequentially.
consequently speedup cannot be obtained as the cost of the single expensive test class cannot be broken down with the selected configuration.
finally the speedup observed in project takes was the highest amongst all projects.
this subject uses configuration c3 and contains test methods distributed nearly equally among test classes with a small number of test methods.
furthermore several methods in those classes are time consuming.
as result the cpus available for testing are kept occupied for the most part during test execution.
answering rq5 considering the machine setup we used the average speedup observed with default configurations of parallelization was .53x.
rq6.
how test execution scales with the number of available cpus?
this experiment evaluates the impact of making a growing number of cpus available to the build system for testing.
for authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
this reason we used a different machine with more cores compared to the one described in section iii.
we used a xeon e5 2660v2 .20ghz intel processor machine with virtual cpus cores with two native threads each and 256gb of memory running ubuntu .
lts trusty tahr bit version .
this experiment spawns a growing number of jvms in different cpus using parallel configuration fc0.
we selected subject mapdb in this experiment as it represents the corestime m fig.
.
scalability.case of a long running test suite see table ii with test cases distributed across many test classes .
recall that a test class is the smallest unit that can be used to spawn a test job on a jvm and that we have no control over which test classes will be assigned to which jvm that the build system forks.
figure shows the reduction in running times as more cpus contribute to the execution.
we ran this experiment for a growing number of cores ... .
the plot omits results beyond cores as the tendency for higher values is clear.
we noticed that improvements are marginal after three cores which is the basic setup we used in other experiments.
this saturation is justified by the presence of a single test class org.mapdb.waltruncat containing test cases that take over two minutes to run.
answering rq6 results suggest that execution fc0 scales with additional cores but there is a bound on the speedup that one can get related to how well the test suite is balanced across test classes.
d. tradeoffs this dimension assesses the impact of using distinct parallel configurations on test flakiness and speedup.
increased parallelism can increase resource contention leading to concurrency issues such as data races across dependent tests .
flakiness and speedup are contradictory forces that could influence the decision of practitioners about which parallel configuration should be used for testing.
note that section iv c evaluated speedup in isolation.
rq7.
how parallel execution configurations affect testing costs and flakiness?
to answer this research question we selected different subjects ran their test suites against all configurations described in section ii and compared their running times and rate of test flakiness.
we used the sequential execution configuration c0 as the comparison baseline in this experiment.
to select subjects we sorted projects whose test suites run in 1m or more by decreasing order of execution time and selected the first fifteen projects that use junit .
or later.
the rationale for this criteria is to ensure compatibility with parallel configuration since older versions of junit does not support parallel testing.
we ran each project on each configuration for three times.
overall we needed to reran test suites times times 3x6 configurations on each project.
given the low standarddeviations observed in our measurements we considered three reruns reasonable for this experiment.
it is worth mentioning that we used custom junit runners as opposed to maven to run the test suites with different parallel configurations see section ii .
after carefully checking library versions for compatibility issues and comparing results with junit s we observed that several of maven s executions exposed problems.
for example maven incorrectly counts the number of test cases executed for some of the cases where test flakiness are observed.
these issues are categorized and documented on our website and can be reproduced with our scripts.
to address those issues we implemented custom test runners for configurations c1 c2 and c3and for configurations fc0 and fc1 we implemented a bash script that coordinates the creation of jvms and invokes corresponding custom runners.
as to faithfully reflect maven s behavior in our scripts we carefully analyzed the source code of the maven surefire plugin.
we implemented test runners using the parallelcomputer class from junit .
we used maven log files to identify test classes to run and used the maven dependency plugin to build the project s classpath with the command mvn dependency build classpath .
once we find the tests suite to run and the corresponding classpath we invoke the test runners mentioned above on them.
we configured this experiment to run at most three jvms in parallel.
recall that in our setup see section iii we limited our kernel to use only three cores and reserved one core for os related processes.
to ensure that our experiments terminate recall that deadlock or livelock could occur we used the timeout command configured to dispatch a killsignal if test execution exceeds a given time limit.
finally we save each execution log and stack traces generated from junit to collect the execution time the number of failing tests and to diagnose outliers in our results.
table iii summarizes results ordered by subject s name.
values are averaged across multiple executions.
we did not report standard deviations as they are very small in all cases.
as to identify the potential causes of flakiness we inspected the exceptions reported in execution logs.
we found that in most of the cases flakiness was caused by race conditions approximately .
of the failures were caused by a null dereference and .
were caused by concurrent access on unsynchronized data structures.
cases of likely broken test dependencies were not as prevalent as race conditions .
of the total eofexception .
filesystemalreadyexistsexception .
and bufferoverflowexception .
.
results suggest that anticipating race conditions to schedule test executions would have higher impact compared to breaking test dependencies using a tool such as electrictest .
the projects with flakiness in all configurations were aws sdk googlecloud and moquette .
it is worth highlighting the unfortunate case of moquette which manifested more than flaky tests in every configuration.
considering time it is noticeable from the averages perhaps as expected an increasing speedup from configuration c1toc3and from configuration fc0 tofc1.
it is also worth mentioning that some combinations authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii speedup versus flakiness fail .
c onfiguration c0denotes the comparison baseline .
columns tandnindicate time and number of tests respectively .
other columns show speedup and percentage of failing tests in different configurations compared to c0.
subjectc0 c1 c2 c3 fc0 fc1 t n speedup fail speedup fail speedup fail speedup fail speedup fail activiti .9m .1x .
.5x .
.9x .
.9x .
.1x .
aws sdk java core .7m .0x .
.5x .
.7x .
.9x .
.5x .
bucket4j .0m .5x .3x .
.2x .
.3x .7x facebook linkbench .1m .0x .6x .
.0x .7x .6x googlecloud dataflow java sdk .6m .2x .
.7x .
.8x .
.8x .
.8x .
inria spoon .3m .2x .
.7x .
.6x .
.8x .8x .
jcabi github .6m .1x .7x .8x .0x .9x jctools core .6m .5x .6x .0x .8x .0x mapdb .2m .5x .7x .8x .7x .
.4x .
moquette .7m .6x .
.4x .
.3x .
.5x .
.3x .
spring cloud function .8m .4x .
.3x .
.6x .
.1x .9x .
stream lib .1m .9x .2x .4x .
.7x .6x stripe java .3m .8x .
.3x .
.5x .
.7x .6x .
tabulapdf java .4m .2x .
.1x .2x .
.0x .2x .
urban airship datacube .3m .9x .
.7x .
.9x .
.0x .9x .
average .0m .
.6x .
.5x .
.7x .
.9x .
.2x .
manifested slowdown instead of speedup.
recall that parallel execution introduces the overhead of spawning and managing jvms and threads.
overall results show that of flakiness have been reported in of the 5x15 pairs of project and configuration we analyzed of the total .
in for of the projects flakiness was not manifested in any combination pairs.
we noticed with some surprise that the average speedup of configuration c1was higher compared to fc1 indicating that it is not always the case that using more cpus pays off.
important to note that the cost of spawning new jvms can be significant in fc1.
answering rq7 overall results indicate that the test suites of .
of the projects we analyzed could be run in parallel without manifesting any flaky tests.
in some of these cases speedups were significant ranging from 1x to .8x.
v. d iscussion this paper reports our finding on a study to evaluate impact and usage of test suite parallelization enabled by modern build systems and testing frameworks.
this study is important given the importance to speedup testing.
note that test suite parallelization is complementary to alternative approach to speedup testing see section vii .
the observations we made in this study trigger multiple actions incentivize forking.
forked jvms manifest low rates of test flakiness.
for instance in fc0 only of projects manifest flakiness and excluding the extreme case of moquette projects manifest flaky tests in low rates .
to .
.
developers of projects with long running test suites should consider using that feature which is available in modern build systems today e.g.
maven .
break test dependencies.
non forked jvms can achieve impressive speedups at the expense of sometimes impressive rates of flakiness.
breaking test dependencies to avoidflakiness and take full advantage of those options is advised for developers with greater interest in efficiency.
refactor tests for load balancing.
forked jvms scales better with the number of cores when the test workload is balanced across testing classes.
to balance the workload automated user oblivious refactoring can help in scenarios where developers are not willing to change test code but have access to machines with a high number of cores.
improve debugging for build systems.
while preparing our experiments we found scenarios where maven s executions did not reflect corresponding junit s executions.
those issues can hinder developers from using parallel testing.
better debugging infrastructure is important.
this study brings to light the benefits and burdens of test suite parallelization to improve test efficiency.
it provides recommendations to practitioners and developers of new techniques and tools aiming to speed up test execution with parallelization.
vi.
t hreats to validity the main threats to validity of this study are the following.
external validity generalization of our findings is limited to our selection of subjects testing framework and build system.
to mitigate that issue we selected subjects according to an objective criteria described in section iii.
it remains to evaluate the extent to which our observations would change when using different testing frameworks and build systems.
also some of the selected subjects contain failing tests.
test failures may reduce the testing time due to early termination or even inflate the time e.g.
test waiting indefinitely for an unavailable resources .
to mitigate this threat we eliminated subjects with flaky tests and filtered projects with at least of the tests passing.
only of our subjects have failing tests.
we carefully inspected our rawdata to identify and ignore these failures with junit s ignore annotation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
internal validity our results could be influenced by unintentional mistakes made by humans who interpreted survey data and implemented scripts and code to collect and analyze the data.
for example we developed junit runners to reproduce maven s parallel configurations and implemented several scripts to automate our experiments e.g.
run tests and detect parallelism enabled by default in the subjects .
all those tasks could bias our results.
to mitigate those threats the first two authors of this paper validated inspected each other to increase chances of capturing unintentional mistakes.
construct validity we considered a number of metrics in this study that could influence some of our interpretations.
for example we measured number of test cases per suite distribution of test costs in a suite time to run a suite etc.
in principle these metrics may not reflect the main problems associated with test efficiency.
vii.
r elated work regression testing research has focused mostly on test suite minimization prioritization reduction and selection .
most of these techniques are unsound i.e.
they do not guarantee that fault revealing tests will be considered for testing .
the test selection technique ekstazi is an example of a sound regression testing technique.
it conservatively computes which tests have been impacted by file changes.
a test is discarded for execution if it does not depend on any changed file dynamically reachable from execution.
important to note that regression testing techniques including test selection is complementary to test suite parallelization.
electrictest is a tool for efficiently detecting data dependencies across test cases.
dependency tracking is important as to avoid test flakiness when parallelizing test suites.
electrictest observes reads and writes on global resources made by tests to identify these dependencies at low cost.
we remain to investigate the impact of electrictest to reduce flakiness in unrestricted test suite parallelization.
the use of the single instruction multiple data simd design has been previously explored in research to accelerate test execution .
the simd architecture as implemented in modern gpus for instance allows the execution of a given instruction simultaneously against multiple data.
for that reason in principle one test could be ran simultaneously against multiple inputs provided that multiple test inputs exist associated to that one test.
recent work explored that idea to speedup test execution of embedded software using graphic cards.
although benchmarks indicate superior performance compared to traditional multicore cpus the use of the technology in broader settings is limited.
for example execution of more general programs can violate the simd s lock step assumption on the control flow of threads.
this violation would affect negatively performance.
furthermore handling complex data is challenging in simd .
the approach is promising when multiple input vectors exist for each test and the testing code heavily manipulates scalar data types.
the datasets used in those papers satisfied those constraints.google and microsoft have been creating distributed infrastructures to efficiently build massive amounts of code and run massive amounts of tests.
those scenarios bring different and challenging problems such as deciding when to trigger the build under multiple file updates .
although such distributed systems are targeted to extremely large scale code and test bases the same ideas can be applied to handle the build process of large albeit not as large projects.
for example gambi et al.
recently proposed cut a tool to automatically parallelize junit tests on the cloud.
the tool allows the developer to control resource allocation and deal with the project specific test dependencies.
note that test suite parallelization is complementary to these high level parallelism schemes.
continuous integration ci services such as travis ci are becoming widely used in the open source community .
accelerating time to run tests in ci is important as to reduce the period between test report updates.
modulelevel regression testing for example can be helpful in that setting.
it is important to note that test failures are more common in ci compared to an overnight run or a local run for instance.
this can happen because of semantic merge conflicts for instance.
as such effect can impact developer s perception and tolerance towards failures we are curious to know if developers would be willing to receive more frequent test reports at the expense of potentially increasing failure rates due to flakiness caused by parallelism.
viii.
c onclusions testing is expensive.
despite all advances in regression testing research dealing with high testing costs remains an important problem in software engineering.
this paper reports our findings on the usage and impact of test execution parallelization in open source projects.
multicore cpus are widely available today.
testing frameworks and build systems that capitalize on these machines also became popular.
despite some resistance observed from practitioners our results suggest that parallelization can be used in many cases without sacrificing reliability.
more research needs to be done to improve automation e.g.
breaking test dependencies refactoring test suites enforcing safe test schedules as to safely optimize parallel execution.
the artifacts we produced as result of this study are available from the following web page acknowledgment jean derson and luis are supported by facepe scholarphips ibpg .
and ibpg .
respectively.
this work was partially supported by cnpq grants and .