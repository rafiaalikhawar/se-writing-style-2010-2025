natural software revisited musfiqur rahman dharani palani peter c. rigby department of computer science and software engineering concordia university montr eal qu ebec canada musfiqur.rahman mail.concordia.ca d palan encs.concordia.ca peter.rigby concordia.ca abstract recent works have concluded that software code is more repetitive and predictable i.e.
more natural than english texts.
on re examination we find that much of the apparent naturalness of source code is due to the presence of language specific syntax especially separators such as semi colons and brackets.
for example separators account for of all tokens in our java corpus.
when we follow the nlp practices of eliminating punctuation e.g.
separators and stopwords e.g.
keywords we find that code is still repetitive and predictable but to a lesser degree than previously thought.
we suggest that syntaxtokens be filtered to reduce noise in code recommenders.
unlike the code written for a particular project api code usage is similar across projects a file is opened and closed in the same manner regardless of domain.
when we restrict our n grams to those contained in the java api we find that api usages are highly repetitive.
since api calls are common across programs researchers have made reliable statistical models to recommend sophisticated api call sequences.
sequential n gram models were developed for natural languages.
code is usually represented by an ast which contains control and data flow making n grams models a poor representation of code.
comparing n grams to statistical graph representations of the same codebase we find that graphs are more repetitive and contain higherlevel patterns than n grams.
we suggest that future work focus on statistical code graphs models that accurately capture complex coding patterns.
our replication package makes our scripts and data available to future researchers .
index t erms basic science entropy language models statistical code graphs stackoverflow i. i ntroduction language modelling is a popular approach in the field ofstatistical machine translation smt and natural language processing nlp .
the growing popularity of this approach has resulted in the application of language modelling techniques in diverse fields.
in the field of software engineering language modelling has revealed power law distributions and an apparent naturalness of software source code .
although the term naturalness is vague it has been expressed mathematically with statistical language models .
in essence language models trained on a large corpus assign higher naturalness to previously seen code while assigning lower naturalness to unseen or rarely seen code.
for example campbell et al.
showed that language models mark code which is syntactically faulty as unlikely orless likely than code without syntax errors.
the goal of this paper is to revisit the natural code hypothesis in new contexts.
as in nlp different programming tasks will require different tuning and cleaning of a corpus.
for example if the goal is to create an english grammar correction tool then stopwords such as the are necessary.
in contrast if the goal isto extract news topics then stopwords must be removed as these dominant tokens will introduce noise and reduce the quality of predictions.
analogously if the goal is to find syntax errors then the corpus must include syntaxtokens.
in contrast if the goal is to recommend multi element api usages then syntaxtokens will dilute predictions.
for example hindle et al.
did not remove syntaxtokens and in their autocompletion model they suggest a syntaxtoken approximately of the time.
as a result a recommender tool would suggest an obvious separator before a useful token such as an api call.
in this work we examine the repetitive behaviour of source code for multiple programming languages we determine the impact of syntaxtokens on repetition we quantify how repetitive api usages are and we compare the repetitiveness of n grams vs graph representations of code.
we examine each topic in the following four research questions.
rq1 replication how repetitive and predictable is source code?
we reproduce hindle et al.
s work to ensure that our dataset is large and diverse enough to test the naturalness hypothesis in new contexts.
we also examine 6additional programming languages c c javascript python ruby and scala.
rq2 repetitive syntax how repetitive and predictable is code once we remove syntaxtokens?
in nlp it is standard practice to remove punctuation and stopwords .
we examine the contribution of three types of syntaxtokens to the language distribution separators such as bracket and semi colon keywords such as if andelse and operators such as plus and minus signs.
rq3 api usages how repetitive and predictable are java api usages?
frameworks and apis provide reusable functionality to developers.
unlike the code written for a particular project api code is similar across projects.
for example a file is opened and closed in the same manner whether it is used in banking or healthcare.
we examine only java api tokens and determine how repetitive and predictable their usage is.
given the large and successful literature on api usage recommendations and autocompletions we suspect that api elements may be more repetitive and predictable than general program code.
rq4 statistical code graphs how repetitive and predictable are graph representations of java code?
an n gram language model assumes that the current token can be predicted by the sequence of n 1previous tokens.
however compilers and humans do not process programs sequentially.
in the case of compilers parse trees or syntax ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i corpus size in tokens.
the total of tokens is held constant across languages.
language files total tokens unique tokens java .
c .
c .
javascript .
python .
ruby .
scala .
trees are generated to provide abstract representations of code.
eyetracking studies of developers reading code show a nonlinear movement along the control and data flow of the program .
we extract the graph based object usage model groum from java programs and compare how repetitive graphs of nodes sizes and are with equivalent sized n grams from the same java programs.
the remainder of this paper is structured as follows.
in section ii we describe our data.
in sections iii iv v and vi we report the results of our experiments for each of the research questions.
since we extract different tokens and graphs we describe the extraction methodology in section in which it is used.
in section vii we discuss limitations of our work and threats to validity.
in section viii we position our work in the context of the literature.
in section ix we summarize our contribution and conclude the paper.
we also publicly release a replication package which includes all processed n gram and graph data as well as the scripts used in our processing pipeline.
ii.
d ata sources project source code we create our source code corpus from open source projects on github.
as a starting point we select the java and python project used in a prior study .
to ensure that we processed a consistent number of tokens for each language between 20m and 25m tokens we added java and python projects as well as projects from additional programming languages.
these projects were selected from the most popular projects on github for each language.1for all the projects we examine only the master branch.
since each research question requires the source code to be processed differently e.g.
n grams vs graphs we describe the extraction methodology when we answer each question.
the list of projects scripts and the processed n grams and graphs can be found in our replication package .
a summary for each programming language is shown in table i. english texts following hindle et al.
we process the gutenberg corpus.
we use a subset of the gutenberg corpus which includes over .4k english works .
the corpus represents a range of styles topics and time periods making gutenberg a diverse corpus.
to make a comparable technical english corpus we process stackoverflow posts that discuss programming tasks in english for each programming language.
stackoverflow text 1top github projects per language extract posts from stackoverflow by removing code and keeping only the english text.
furthermore we use the following constraints to reduce noise and poorly constructed english when selecting posts we only use posts which are the accepted answer.
each post has at least 10positive votes.
the corresponding question post has at least 1positive vote.
we take posts which have at least characters of english text and exclude the code snippet and any code words in the text.
this ensures that our corpus has sufficient english tokens.
although we exclude code words we take only posts that contain a code snippet to ensure that the discussion is about code and not for example about the configuration of an ide.
to extract the english tokens in stackoverflow posts we extract the necessary data body without code with a python html library.
we merge the posts into a single file and perform the nlp process steps of stemming lematization lexicalization and stopword removal.
the scripts and output data can be found in our replication package .
iii.
r eplica tion rq1 how repetitive and predictable is source code?
we replicate the work of hindle et al.
to ensure that the data we sample produces similar results.
we also examine c c javascript python ruby and scala.
we want to understand if the language and programming paradigm influence the repetitive nature of programming.
a. theoretical background and methodology we give the definitions of n gram language models cross entropy and selfcrossentropy and describe how we extract n grams.
n gram language model we use the term language model lm to mean the probability distributions over a sequence of ntokens p k k2 ... k n .
a lm is trained on a corpus containing sequences of tokens from the language.
using this lm our goal is to assign high probability to tokens with maximum likelihood and low probability to n grams with lower likelihood.
the primary purpose of modelling a language statistically using lms is to model the uncertainty of the language by determining the most probable sequence of tokens for a given input.
consider a sequence of tokens k1 k2 k3 ... k n kn in a document d. n gram models statistically calculate the likelihood of the nth token given the previous n tokens.
we can estimate the probability of a document based on the product of a series of conditional probabilities p d p k1 p k2 k1 p k3 k1 k2 ...p kn k1 k2 ... k n here p d is the probability of the document and p k i is the conditional probability of tokens.
we can transform the above equation to a more general form which is given below.
p k1 k2 k3 ... k n kn n summationdisplay i 1p ki k1 ... k n authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
this transformation uses the markov property which assumes that token occurrences are influenced only by limited prefix of length n .
furthermore we can consider this as amarkov chain which assumes that the outcome of the next token depends only on the previous n 1tokens .
thus we can write p ki ki n ... k i p ki ki n this equation requires the prior knowledge of the conditional probabilities for each possible n gram.
these conditional probabilities are calculated from the n gram frequencies.
we use these n grams to determine the entropy of a language corpus including source code.
selfcrossentropy hindle et al.
s calculate the average number of bits entropy required to predict the nth token of the n grams in a document.
they use the standard formula for cross entropy which is also the log transformation of perplexity.
they define cross entropy in the context of ngrams.
given a language model m the entropy of a document d with n tokens is h d m nn summationdisplay i 1log2p ki k1...k i they use cross entropy in a unique manner to define selfcrossentropy.
instead of estimating the language model m from another document or corpus they divide a single corpus into folds.
m is then calculated from of the folds and h d m is calculated with dbeing the remaining fold.
the final selfcrossentropy is the average value across all folds.
extracting n grams we replicate hindle et al.
using the same tools and methodology as shown in figure .
we remove the source code comments.
we lexicalize each source file in the project using antlr2to extract code tokens.
then we merge all the lexicalized files to create a corpus.
for example to get the selfcrossentropy of the java language we process all .java files.
then we merge the processed files to create our final corpus.
to calculate the selfcrossentropy a single corpus is split into folds.
ten fold cross validation is used with the probability estimated from of the data and validated on the remaining .
the results are averaged over the test folds.
we use mit language model mitlm toolkit3to calculate the selfcrossentropy for each data set.
mitlm uses techniques for n gram smoothing to deal with unseen n grams in the test fold see hindle et al.
for further discussion .
we calculate the selfcrossentropy for token sequences i.e.
n grams from grams to grams for each programming corpus the gutenberg corpus and english text on stackoverflow corpus.
the processing pipeline for the experiments is shown in figure .
b. replication result how repetitive and predictable is software?
2antlr4 3mitlm 2a shows the replication of hindle et al.
s work including six additional programming languages and stackoverflow posts.
all the programming languages under consideration for this study show the same pattern of selfcrossentropy.
the highest selfcrossentropy is observed for unigram language models.
the value of selfcrossentropy declines significantly for bigram and trigram models.
from grams to grams the selfcrossentropy remains nearly constant.
since we are able to replicate hindle et al.
s results we are confident that our dataset is large and diverse enough to test the naturalness hypothesis in new contexts.
while the pattern is the same the values of selfcrossentropy are substantially different for each language.
with scala being much less repetitive than c .
the difference among languages leads us to conjecture that the syntax of the language artificially reduces its selfcrossentropy.
the natural software hypothesis that code is repetitive and predictable holds across programming languages.
however the degree of repetition varies dramatically among languages.
iv .
r epetitive synt ax rq2.
how repetitive and predictable is code once we remove syntaxtokens?
standard prepossessing steps in nlp involve the removal of stopwords and punctuation .
stopwords including articles e.g.
the and prepositions e.g.
of are removed in information retrieval tasks because they introduce noise in the data set reducing the likelihood of retrieving interesting information unless one is creating a grammar checker .
in our work we examine the impact of three types of syntaxtokens separators such as brackets and semi colons keywords such asif andelse and operators such as plus and minus signs.
our replication package contains the full list of syntaxtokens for each language .
by including syntax in their analysis we suspect that hindle et al.
artificially inflate the naturalness of source code.
without these syntax tokens we hypothesize that raw source code will not be especially repetitive.
in this section we examine the impact of each type of syntaxtoken on the repetitiveness of code.
a. background and methodology for each programming language we examined the language specification to identify the keywords separators and operators.
we calculate the percentage of these syntaxtokens in each programming language.
then we remove syntaxtokens from the corpus and measure the entropy of n grams without the language specific tokens.
we report the change in selfcrossentropy of the n grams after the removal of language specific tokens and answer the following questions what percentage of total tokens are syntaxtokens?
what is the change in selfcrossentropy after removing syntaxtokens?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
pipeline for experiments performed in this study a selfcrossentropy with syntaxtokens.
b selfcrossentropy without syntaxtokens.
fig.
selfcrossentropy is the number of bits required to encode n grams in each programming language.
the left figure is a direct replication of hindle et al.
.
the right figure does not include syntaxtokens and the number of bits required to encode the programming languages is reduced and the variation among them is substantially smaller.
also the number of bits required to encode technical english on stackoverflow similar to code.
how repetitive is code without syntaxtokens compared to general english and to technical english on stackoverflow?
b. results and discussion what percentage of total tokens are syntaxtokens?
stopwords are removed during natural language information retrieval tasks because their high prevalence introduces noise reducing the likelihood of retrieving highvalue information.
when applied to our programming corpora in table ii we see that syntaxtokens account for a high percentage of total tokens.
across the programming languages javascript has the highest number of syntaxtokens at of total tokens while the smallest percentage is for ruby.
separators account for the largest proportion of syntaxtokens between and of all tokens.
the corresponding values for keywords 4for the english corpora we removed the standard stopwords with the nl tk toolkit.are and and for operators and .
the main conclusion is that syntaxtokens dominate the tokens in all programming languages and when included make code look artificially repetitive.
however we are not suggesting that researchers or developers remove all syntaxtokens as there may be cases where it is interesting to predict for example the control structure through the if keywords.
we do feel as we discuss later that statistical graph representations of code that include control flow may be better abstract representations than simply removing syntaxtokens from a corpus.
what is the change in selfcrossentropy after removing syntaxtokens?
when we remove the syntaxtokens and recalculate the selfcrossentropy in table iii we see a dramatic increase in selfcrossentropy and a corresponding decrease in repetitiveness.
for java we see that from grams to grams we need a respective increase of more bits.
after grams we need a nearly constant increase in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able ii percentage of language syntax token.
syntaxtokens dominate the tokens in all programming languages and when included make code look artificially repetitive.
language separators keywords operators total java .
.
.
.
c .
.
.
.
c .
.
.
.
javascript .
.
.
.
python .
.
.
.
ruby .
.
.
.
scala .
.
.
.
bits.
clearly more information is required to encode java programs without the artificially repetitive syntaxtokens.
how repetitive is code without syntaxtokens compared to english?
we investigate the difference in selfcrossentropy between programming languages and english by reporting the number of additional bits necessary to encode english.
hindle et al.
report a maximum average per word entropy of approximately bits for english and bits for java which means that english requires times as many bits while for grams and grams english requires and .
times as many bits.
similarly we find that before removing syntaxtokens we need .
.
.
.
.
more bits for grams to grams for java.
after grams the increase is constant at .
times.
however without syntaxtokens the number of additional bits required is substantially less for java .
.
.
additional bits for gram to grams and remains constant at .
from grams to grams.
this provides further evidence that syntaxtokens clearly account for a large proportion of the repetitiveness in java.
with slight variation in the actual number this result generalizes to the other programming languages in figure 2b.
how repetitive is stackoverflow english?
as we discussed in the data section the gutenberg corpus contains a wide range of english writing styles topics and authors.
in contrast the programming corpora used in our work and that of hindle et al.
s are for single programming languages.
to provide a more comparable english corpora we processed stackoverflow posts related to each programming language.
we find that selfcrossentropy of english on stackoverflow is similar to that of code.
for example java requires .
times as many bits as stackoverflow english to encode grams.
clearly the vocabulary on stackoverflow is very limited.
for grams .
times as many bits are required and this number remains constant at .
for grams to grams.
after grams we see that sequences of token usages are larger in stackoverflow.
this is likely because classes and methods tend to be used together in java.
however compared to the originally reported times as many bits or more bits the removal of syntaxtokens shows a .
to .
times as many bits or to more bits.
this result is consistent across programming languages.
technical discussion in english on stackoverflow have a similar degree of repetition to codec.
concluding discussion on syntaxtokens hindle et al.
were worried by the questions that we ask in this section .
they asked is the increased regularity we are capturing in software merely a difference between the english and java languages themselves?
java is certainly a much simpler language than english with a far more structured syntax.
to answer this question they conducted an experiment were they compared the selfcrossentropy of a single program with the cross entropy of predicting the tokens in one java program with those in other java programs.
they conclude that because the entropy for single programs is lower than the entropy between programs that regularity of software is not an artifact of the programming language syntax.
however in both cases the programs were written in the same language java using the same syntax.
their experiment does not control for simple syntactical regularities in the java language.
in contrast in our study we remove syntaxtokens and find that the regularity of programs drops dramatically.
we conclude that the syntax of programming languages artificially reduces the entropy of software.
our findings suggest that software engineers should follow the nlp practice of removing stopwords and punctuation in this case syntaxtokens to reduce the noise they introduce and to make higher value recommendations.
in natural language processing obvious stopwords are removed as they add noise to prediction models.
syntaxtokens are analogous to the and in natural language.
suggesting and to developers is unnecessary as these syntaxtokens are part of the language specification and are captured by compilation rules.
when developing code recommenders syntaxtokens should be filtered to allow for more insightful recommendations.
v. a p i u sages rq3.
how repetitive and predictable are java api usages?
api code is used across multiple projects in the same manner regardless of the domain of the project.
we extract java api tokens and determine how predictable their usage is.
for example reading from an input stream would have the following java api sequence fileinputstream.fileinputstream fileinputstream.read fileinputstream.close we conjecture that sequences of api elements i.e.
api usages should be more repetitive and predictable than general program code.
a. background and methodology we extract the java api elements from the java platform library standard edition specification .
we remove all tokens from the java corpus which are not part of java standard libraries.
the set of api elements includes package class field and method names the full list and processed corpus can be found in our replication package .
for the java corpus we calculate the selfcrossentropy for the api usage of size to grams.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able iii percentage increase in selfcrossentropy after the removal of syntaxtokens.
for example the selfcrossentropy for java doubles after grams indicating a substantial drop in its repetitive nature.
language gram gram gram gram gram gram gram gram gram gram java .
.
.
.
.
.
.
.
.
.
c .
.
.
.
.
.
.
.
.
.
c .
.
.
.
.
.
.
.
.
.
javascript .
.
.
.
.
.
.
.
.
.
python .
.
.
.
.
.
.
.
.
.
scala .
.
.
.
.
.
.
.
.
.
ruby .
.
.
.
.
.
.
.
.
.
fig.
comparing the java api selfcrossentropy with raw java source code java source code without syntaxtokens and english.
the use of the java api is highly repetitive.
b. results and discussion for api usages figure compares the selfcrossentropy of n gram api usages in java to raw java java without syntaxtokens stackoverflow english and gutenberg.
we find that the selfcrossentropy of the java api is less repetitive and predictable than the raw corpus which contains syntaxtokens.
this result derives from the high proportion of syntaxtoken tokens i.e.
of tokens in java are syntaxtokens.
java that excludes syntaxtokens but includes internal code requires more bits for grams and a consistent more for to grams compared with the java api.
this is likely because the domain specific tokens for example the bankaccount class in a banking application are used much less repetitively than the api code such as string or inputstreamreader classes in standard java libraries.
the corresponding numbers for technical english on stackoverflow are to more bits.
for gutenberg which includes a diverse set of english texts to more bits are required.
these differences are substantially lower thangutenberg and raw java which requires between and more bits to encode the gutenberg corpus.
we conclude that raw java code that contains syntaxtokens is more repetitive than the java api usages likely due to the repetitive use of syntax rules.
in contrast we find that java api is more repetitive than general java code that does not contain syntaxtokens.
the repetitiveness of the java api usages quantifies the truth underlying the large and successful literature on sophisticated api recommendations e.g.
.
api usages are repetitive and predictable.
since api calls are common across programs their usage leads to reliable statistical models that can recommend sophisticated api call sequences.
vi.
s t a tistical code graphs rq4 how repetitive and predictable are graph representations of java code?
most natural language models assume a sequential left toright reading order.
in contrast compilers and humans do not usually process programs sequentially.
in the case of compilers parse trees or syntax trees are generated to provide abstract representations.
eyetracking studies of developers reading code show a nonlinear movement along the control and data flow of the program which differs from natural language reading strategies .
for example developers focus on method signatures and following beacons in the code.
in this section our goal is to measure how repetitive an abstract graph representation of code is and to understand if it has repetitions that cannot be identified with n grams.
a. background and methodology in order to determine how repetitive code graphs are we need a statistical graph extraction technique that is able to satisfy the following requirements extract the code graphs from a large number of projects that may not be able to be compiled due to for example external dependencies.
filter out granular information such as variables and expressions to include only control and data dependencies among class objects and methods in the code graphs.
identify isomorphic code graphs to determine the occurrence frequency of each graph to create statistical graph models of code.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we evaluated the eclipse ast parser and found that it had critical limitations the java project dependencies must be present for each project.
the ast includes lowlevel details such as variable names which would artificially reduce graph frequencies.
statistics on structurally similar asts is not built in.
techniques to identify structural similarities in the code using asts are computationally expensive .
in summary the eclipse ast parser is designed for static analysis but is not appropriate for statistical based recommendations.
in contrast g rou miner was designed to extract graph based object usage models g roums and to calculate efficiently isometric graphs.
below we describe the steps necessary to extract the frequency of java code graphs recoder is used to extract an ast without the need to compile the program .
grou miner transforms the ast for each method body into a g roum .
the nodes in a g roum represent constructors method invocations field accesses and branching points for control structures.
the edges represent temporal data and control dependencies between nodes.
graph induction is used to generate subgraphs of the groum for a specified size in our case and node graphs.
grou miner computes the occurrence frequencies of each g roum .
b. data we use g rou miner to capture the occurrence frequency of each g roum in the java projects used in the previous sections.
in the previous section we found that api code tends to be more repetitive and predictable across multiple projects.
as a result we capture g roums containing api usages from the java platform standard edition specification .
we include groums that contain at least one java api node.
we eliminate groums which contain only control flow structures or only contain internal code.
to perform a fair comparison with ngrams we use the same inclusion and exclusion criteria to filter the n grams tokens see our replication package for the graphs and n grams .
our goal is to study the inherent degree of repetition for the two representations graphs and n grams.
in the previous sections we calculated the selfcrossentropy by predicting the nth token for n grams in fold cross validation.
since graphs are not sequential the most appropriate prediction comparison is unclear.
to avoid this problem we examine the underlying frequency distribution for each set of n grams and n node graphs on the same set of java projects.
this strategy of examining the distribution has been employed in many previous works examining code structure .
the more left skewed the distribution the more repetitive and predictable the representation.
c. results and discussion for statistical java code graphs we collect g roums with and nodes and the corresponding n grams.
we measure the occurrence frequenciest able iv the cumulative proportion of n node graphs and n grams from to in point increments for all usages.
for example the top of the n node graphs account for over of all usages.
the table shows the left skew of the distributions.
cumulative percentage2 node graph3 node graph4 node graph gram gram gram .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fig.
the top of n grams and n node graphs account for the y axis of the usages.
for example the top of the n node graphs account for .
of all usages.
graphs are more repetitive than n gram sequences.
of each g roum and n gram across the java projects.
since graphs represent an abstraction of code we conjecture that on the same code g roums will have a stronger pareto type distribution than n grams i.e.
graphs will be more repetitive and left skewed.
in figure we plot the top of the n grams and n node g roums against the percentage of total n grams and n node g roums respectively.
we see both n grams and n node g roums are highly left skewed.
for example the top of n grams account for for all instances of and grams respectively.
the corresponding value for the top of n node g roums account for of instances of and node graphs respectively.
the top of graphs are percentage points more frequent authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
a g roum representing iteration through a hashmap.
this graph is statistically common and is an abstraction of the code in listings through .
than the top of n grams.
furthermore the drop between nodes and nodes is much less than between grams and grams indicating that graphs remain highly repetitive with increasing size.
table iv shows the complete distribution from to for graphs and n grams.
the column at is represented in the figure but for space reasons we cannot show the graphs as this would represent lines.
the table shows that the pattern remains clear with n nodes being more left skewed than n grams.
we conclude that graph representations are more repetitive than sequential representations.
d. illustration of graphs we have quantitatively determined that g roums are more repetitive and predictable than n grams.
in this section we provide illustrations of why they are more repetitive.
for example an n gram sequence will not capture the relationship between file.open andfile.close because there will always be other tokens such as file.read between these api calls.
although we removed syntaxtokens in this section if they had been included the problem would be exacerbated because syntaxtokens lie between all related api calls.
in contrast g roums will always contain a data dependency edge between file.open andfile.close even when internal classes are present.
the temporal program flow will still be captured by control edges.
listing transactionthroughputchecker.java private void printthroughputreports printstream out out.println throughput reports tx s for map.entry string double entry reports.
entryset out.println t entry.
getkey entry.
getvalue out.println listing globalsessiontrackerstate.java public globalsessiontrackerstate newinstance globalsessiontrackerstate copy new globalsessiontrackerstate copy.logindex logindex for map.entry memberid localsessiontracker entry sessiontrackers.
entryset copy.sessiontrackers.put entry.
getkey entry.
getvalue .newinstance return copy a more complex example from our corpus of java programs illustrates the transformation of separate program code fragments into a common abstract g roum with nodes.
the g roum in figure represents the api usage pattern of iterating through a java.util.hashmap with an enhanced for loop.
the g roum is an abstract representation of the code in listings and as well other classes in the neo4j project.
specifically the g roum contains the data and control flow dependencies between map.entryset map.entry.getkey map.entry.getvalue and an enhanced for loop.
for example in listing the code iterates through a hashmap of tracked client sessions and in listing the code iterates through a hashmap of throughput reports.
below we use the listings to show the important differences between the g roum and n gram models.
abstraction from examining the listings it is clear that no sequential model would consider these code fragments as identical.
there are many internal classes and syntaxtokens between these api elements.
even when only api elements are considered there would be no direct sequence with map.entryset preceding map.entry.getvalue .
this relationship is only captured as a data dependency in a graph.
size the size of the n gram necessary to capture each of these code fragments would be much larger than the node groum .
for example if we include syntaxtokens for the respective listings we need sequences with and tokens to represent the code in the listings.
without syntaxtokens the corresponding number of tokens is smaller but still quite large at and tokens respectively.
graphs are also a more realistic representation of code than sequential n grams because compilers and humans do not process code sequentially.
graphs are more appropriate for statistical code recommendation because they can recommend non sequential relationships that cannot be represented in a sequential model.
g roums capture information about the control and data flow at a higher level of abstraction which makes them a more repetitive representation of code than sequences of tokens.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code is not sequential and researchers should stop using n gram representations that were created for sequential natural languages.
instead graph models that capture control and data flow provide more concise abstract representations of code.
most abstract representations of code are based on rule oriented asts.
we need new probabilistic representations of code to make highlevel complex statistical recommendations of code usage.
vii.
l imit a tions and validity limitations of graphs to extract an ast from a large number of projects we used recoder .
recoder like the ppa tool has known limitations that lead to unknown nodes in a graph.
when a node is unknown we are unable to generate a g roum .
for and node graphs we have .
.
.
of graphs that contain an unknown.
these percentages are inline with the accuracy of the state ofthe art partial programs analysis and code snippets analysis tools .
a second limitation is the computational expense of identifying isomorphic graphs using g rou miner .
in this work we calculated g roum sizes up to nodes.
based on our analysis we have seen that the probability distribution of graphs for node and nodes remain constant indicating that like n grams higher n node graphs exhibit similar degrees of repetition.
furthermore since graphs are at a higher degree of abstraction fewer nodes are necessary to represent the same block of code when compared to sequential n grams.
model limitations there are many natural language models that deal with sequences of tokens n grams skip grams rnn lstm .
the goal of the basic research in this paper is to understand the distribution of tokens in code corpora and to understand if there is enough repetition to make interesting statistical predictions.
the model is less relevant to this work than the repetition in the corpus as neural networks depend upon this repetition as much as linear models.
since models designed for natural language assume a sequential sequence of tokens they are not appropriate for code.
we need to modify the underlying unit for these models and modify them for statistically based abstract graph representations.
reliability and external v alidity by examining a diverse set of languages we increase the generalizability of our results.
furthermore in rq1 our goal was to replicate previous work and to ensure that our data and scripts produced consistent results.
we were successful in this replication increasing the validity of the data used in the novel work in subsequent research questions.
in our replication package we have included all processed n gram and graph data as well as the scripts used in our processing pipeline to allow other researches to validate and extend our work.
limitations of selfcrossentropy in terms of entropy calculations selfcrossentropy is an extension of cross entropy whereby fold cross validation is used to calculate the pertoken average of the probability with which the language model generates the test data .
ideally we would calculate all possible combinations of the next token however asshannon points out this is impractical with o tn where t is the number of unique tokens and n is the total number of tokens in the corpus.
for each language in our corpus there are over 300k unique tokens and million total tokens.
as a result selfcrossentropy serves as a good approximation of entropy.
viii.
r ela ted work research into language entropy.
basic research into understanding redundancy and measuring entropy in languages has a long history.
shannon developed statistical measures of entropy for the english language.
gabel and su noted high levels of redundancy in code.
hindle et al.
continued this work demonstrating that software is highly repetitive and predictable.
recent works have replicated these software findings on a giga token corpus looked at the entropy in local code contexts and applied neural network models .
others have examined repetition at higher granularity including the package class statement level line level and using word categories .
repetition has also be studied in other domains such as android apps .
in each case code has been found to be repetitive and predictable.
in our work question replicates hindle et al.
s work expanding it to multiple programming languages.
we noted differences among programming languages and conjectured that these differences may be due to syntax .
following nlp practices of removing stopwords and punctuation we remove operators separators and keywords and find that without these highly repetitive tokens software is much less repetitive and predictable especially without separators .
while we support the general conclusion that code is repetitive and predictable we find that it is not much more repetitive than english.
this conclusion is important because it will reframe the ease with which statistical predictions about software can be made.
research on code validation and checking.
most existing tools for finding defects and other code faults use static analysis.
recent works have focused on using the statistical properties of the languages to find bugs and to suggest patches.
for example campbell et al.
find that syntax errors can be identified using n gram language models.
researchers also identified bugs and bug fixes in code because buggy code is less natural and has a higher entropy .
santos and hindle used the n gram cross entropy of text in commit messages to identify successfully commits that were likely to make a build fail.
our research confirms that statistical code checking will work much better on syntax or apis than on internal classes because these former types are much more repetitive.
research into recommenders and autocompletions.
modern ides contain an autocompletion feature that usually use the structure of the language to make suggestions.
researchers working on code suggestion have long known intuitively that code is repetitive.
for example textual similarity of program code commit messages and api usage patterns have been exploited to guide developers during authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
their engineering activities.
building on this work zimmermann et al.
used association rule mining on cvs data to recommend source code that is potentially relevant to a given change task.
recent work by azad et al.
has extended this work to make change rule predictions from a large community of similar apps and the code discussed on stackoverflow.
advanced recommendation techniques have used the history of applications and the repetitive nature of programming to recommend code elements to developers.
robbes and lanza filtered the suggestions made by code completion algorithms for example based on where the developer had been working in the past and the changes he or she had made.
bruch et al.
recommend appropriate method calls for a variable based on an existing code base that makes similar calls to a library.
buse and weimer automatically generate code snippets from a large corpus of applications that use an api.
duala ekoko and robillard use structural relationships between api elements such as the method responsible for creating a class to recommend related elements to developers.
works by nguyen et al.
use statistical language models to recommend code accurately.
nguyen and nguyen expanded this work to graphs in order to create recommendations that are syntactically valid.
much of this work focuses on recommending api elements.
our work suggests that api usages are substantially more repetitive and predictable than general code which explains the success of api recommendation approaches.
furthermore we show why graphs are a more appropriate representation of code and we hope this will encourage future researchers to focus on statistical graph abstractions instead of sequential tokens.
research on statistical translation.
recent works have mirrored the success of statistical machine translation in natural languages e.g.
google translate and applied these approaches to translating english to code.
for example swim uses a corpus of queries from bing to align code and english and generates sequences of api usages.
deepapi uses recurrent neural networks to translate aligned source code comments with code to translate longer sequences of api calls.
t2api uses alignments between english and code on stackoverflow to generate a set of api calls.
these calls are then rearranged based on their usage likelihood in existing program graphs.
t2api can generate long graphs of common api usages from english.
our work provides a frame in which to understand these works.
for example the sequences of swim and deepapi tend to be short and simplistic as they are restricted by a left to right processing of tokens.
in contrast t2api which re orders api elements in a graph can produce more complex usages.
ix.
c onclusion our findings confirm previous work that code is repetitive and predictable.
however it is not as repetitive and predictable as hindle et al.
suggested.
we have found that the repetitive syntax of the program language makes software look artificially much more repetitive than english.
for example language specific syntaxtokens account for of the totaljava tokens in our corpus.
we conclude that the researcher must ensure that the corpus is tuned and cleaned for the prediction task.
if the goal is to recommend statistically tokens that are related to complex software engineering tasks for example completing a set of api calls then suggesting syntaxtokens such as semicolons that are encoded as rules in a compiler will simply distract from more interesting recommendations.
we make our scripts n grams and graphs available in our replication package and hope that our work will be used by researchers to select appropriate corpora with sufficient repetition.
for example we conducted a failed experiment to suggest patches based on past fixes using an n gram language model.
had we had our current analysis there would have been little need to conduct the experiment as it would be obvious that internal class tokens and usages were too infrequent to be used successfully in any statistical model.
future work to complement static analysis with statistical models could allow for appropriate recommendations even when a class is used infrequently.
the success of api usage recommendations flows naturally from our findings.
by tuning the vocabulary to api code tokens and examining the usage of these apis element across many programs there is sufficient repetition to make accurate recommendations.
software recommender tools are moving from simple single element autocompletions to multi element non sequential recommendations of code blocks.
our work shows that different representations of code have different degrees of repetition.
graph representations allow for a higher degree of abstraction and the data and control flow allow for non sequential relationships.
furthermore the abstract nature of graphs allows for a more concise representation that reduces the number of noise tokens in code predictions.
most abstract representations of code are based on rule oriented asts.
although this work is basic science and does not involve developing new tools and techniques we suggest that future work should focus on new code representations that are tailored to statistical code suggestion allowing for complex and useful recommendations.