evaluating and improving fault localization spencer pearson jos campos ren just gordon fraser rui abreu michael d. ernst deric pang benjamin keller u. of washington usa u. of sheffield uk u. of massachusetts usa palo alto research center usa u. of porto haslab portugal suspense cs.washington.edu jose.campos sheffield.ac.uk rjust cs.umass.edu gordon.fraser sheffield.ac.uk rui computer.org mernst cs.washington.edu dericp cs.washington.edu bjkeller cs.washington.edu abstract most fault localization techniques take as input a faulty program and produce as output a ranked list of suspicious code locations at which the program may be defective.
when researchers propose a new fault localization technique they typically evaluate it on programs with known faults.
the technique is scored based on where in its output list the defective code appears.
this enables the comparison of multiple fault localization techniques to determine which one is better.
previous research has evaluated fault localization techniques using artificial faults generated either by mutation tools or manually.
in other words previous research has determined which fault localization techniques are best at finding artificial faults.
however it is not known which fault localization techniques are best at finding real faults.
it is not obvious that the answer is the same given previous work showing that artificial faults have both similarities to and differences from real faults.
we performed a replication study to evaluate claims in the literature that compared fault localization techniques from the spectrum based and mutation based families .
we used artificial faults in real world programs.
our results support of the previous claims as statistically significant but only as having non negligible effect sizes.
then we evaluated the same claims using real faults from the programs.
every previous result was refuted or was statistically and practically insignificant.
our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults.
in light of these results we identified a design space that includes many previously studied fault localization techniques as well as hundreds of new techniques.
we experimentally determined which factors in the design space are most important using an overall set of real faults.
then we extended this design space with new techniques.
several of our novel techniques outperform all existing techniques notably in terms of ranking defective code in the top or top reports.
i. i ntroduction a fault localization technique for short fl technique directs a programmer s attention to specific parts of a program.
given one or more failing test cases and zero or more passing test cases a fl technique outputs a typically sorted list of suspicious program locations such as lines statements or declarations.
the fl technique uses heuristics to determine which program locations are most suspicious that is most likely to be erroneous and associated with the fault.
a programmer can save time during debugging by focusing attention on the most suspicious locations .
another use is to focus a defect repair tool on the parts of the code that are most likely to be buggy.
dozens of fault localization techniques have been proposed .
it is desirable to evaluate and compare these techniques both so that practitioners can choose the ones that help them solve their debugging problems and so that researchers can better build new fault localization techniques.a fault localization technique is valuable if it works on real faults.
although some real faults mostly faults in the single small numerical program space have been used in previous comparisons of fault localization techniques the vast majority of faults used in such comparisons are fake faults mostly mutants.
the artificial faults were mutants automatically created by a tool or mutant like manuallyseeded faults created by students or researchers .
artificial faults such as mutants differ from real faults in many respects including their size their distribution in code and their difficulty of being detected by tests .
it is possible that an evaluation of fl techniques on real faults would yield different outcomes than previous evaluations on mutants.
if so previous recommendations would need to be revised and practitioners and researchers should choose different techniques to use and improve.
it is also possible that an evaluation of fl techniques on real faults would yield the same recommendations thus resolving a cloud of doubt that currently hangs over the field.
either result would be of significant scientific interest.
the results also have implications beyond fault localization itself.
for instance it would help to indicate which fault localization approaches if any should be used to guide automated program repair techniques .
this paper compares fault localization techniques on real vs. artificial faults.
techniques that localize artificial faults best donot perform best on real faults.
our experiments are based on previously studied fault localization techniques from the spectrum based and mutation based families.
the contributions of this paper include a replication study that repeats and extends previous experiments comparing fault localization techniques on artificial faults.
we mitigated threats to internal validity by re implementing all the techniques in a single infrastructure and using the same experimental scripts faults and other experimental variables.
our results confirm of previouslyreported comparisons such as ochiai is better than tarantula and refute .
a new study that compares the fault localization techniques on real faults.
the ranking does not agree with any previous results from artificial faults!
of the previous results are reversed for example metallaxis is better than ochiai on artificial faults but ochiai is better than metallaxis on real faults.
the other of the results are statistically insignificant for example dstar is better than tarantula on artificial faults but on real faults there is no significant difference between the two techniques.
these results indicate that artificial faults ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
e.g.
mutants are not an adequate substitute for real faults for the task of evaluating a fault localization technique.
an explication of the design space of fault localization techniques.
previous work made different sometimes undocumented choices for factors other than the formula.
we exhaustively evaluated all these factors.
we found that formula which most papers have exclusively focused on is one of the least important factors.
we also added new factors to the design space thereby creating new hybrid fault localization techniques that combine the best of previous techniques.
an evaluation of all the fl techniques generated by the design space with respect to how well they localize real faults.
we found new techniques that are statistically significantly better than any previous technique though with small effect sizes.
more importantly they do much better in terms of including the correct answer the actual faulty statement within the top or top statements of their output.
our results indicate how to make the most of current approaches and they indicate that significant advances in fault localization will come from focusing on different issues than in the past.
our methodology addresses multi line faults faults of omission and other real world issues both in the design of fl techniques and in the experimental protocol for evaluating them.
a technical report has more examples data analyses.
ii.
e v alua ting fault localiza tion many studies have evaluated and compared fl techniques .
table i summarizes these studies.
the majority of studies revolve around the same set of programs and use largely artificial faults.
this section explains how a fault localization technique s output can be evaluated.
a. evaluation metrics a fault localization technique ttakes as input a program p and a test suite with at least one failing test and it produces as output a sorted list of suspicious program locations such as lines statements or declarations.
for concreteness this paper uses statements as the locations but the ideas also apply to other levels of granularity.
given a fault localization technique tand a program pof sizenwith a single known defective statement d a numerical measure of the quality of the fault localization technique can be computed as follows run the fl technique to compute the sorted list of suspicious statements let nbe the rank of din the list use a metric proposed in the literature to evaluate the effectiveness of a fl technique e.g.
lil t score expense or exam score .1for concreteness this paper uses exam score which is the most popular metric but our results generalize to the others.
the exam score is n n where nis the number of statements in the program.
the score ranges between 0and1 and smaller numbers are better.
1these scores are different than the suspiciousness score the fl technique may use for constructing the sorted list of suspicious program statements.b.
extensions to fault localization evaluation the standard technique for evaluating fault localization described in section ii a handles defects that consist of a change to one executable statement in the program as is the case for mutants.
to evaluate fault localization on real faults we had to extend the methodology to account for ties in the suspiciousness score multi line statements multi statement faults faults of omission and defective non executable code such as declarations.
ties in the suspiciousness score we assume that the sorting function breaks ties arbitrarily.
when multiple statements have the same suspiciousness score then all of them are treated as being the nth element in the output where nis their average rank .
multi line program statements any fl tool report that is within a statement is automatically converted to being a report about the first line of the smallest enclosing statement.
multi statement faults of real world bug fixes span multiple statements .
our study evaluates the fault localization techniques for three debugging scenarios best case any one defective statement needs to be localized to understand and repair the defect.
worst case all defective statements need to be localized to understand and repair the defect.
average case of the defective statements need to be localized to understand and repair the defect.
note that these debugging scenarios are equivalent for singlestatement faults.
all of our the experimental results are generally consistent for the three scenarios.
faults of omission in of cases a bug fix consists of adding new code rather than changing existing code.
the defective program contains no defective statement but some are missing.
previous studies have not reported whether and how this issue was addressed.
a fl technique communicates with the programmer in terms of the program s representation statements of source code.
a fl technique is most useful if it identifies the statement in the source code the programmer needs to change.
however many fl techniques have a serious limitation they do not rank or report lines consisting of scoping braces such as the final of a method definition even though that would be the best program location to report when the insertion is at the end of a method.
to avoid disadvantaging such techniques we also count the current last statement as a correct report.
a technical report shows examples.
a more serious complication is that the developer inserted the new code at some statement but other statements might be equally valid choices for a bug fix.
consider the following example drawn from the patch for closure in defects4j if n.iscall ... return true if n.isnew ... return true if n.isdelprop return true for node c n.getfirstchild ... ... authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i selected fault localiza tion studies .o ur results bottom uphold previous results on artificial faults but not on real faults .
ref.
lang.
ranking from best to worst programs artif.
real kloc faults faults c tarantula siemens gentsroom c ochiai tarantula siemens gentsroom c ochiai tarantula siemens space gentsroom c barinel ochiai tarantula siemens space gzip sed gentsroom c tarantula concordance c op2 ochiai tarantula siemens space gentsroom c metallaxis ochiai siemens space flex grep gzip gentsroom c ochiai tarantula siemens space nanoxml xml security gentsroom c dstar ochiai tarantula siemens space ant flex grep gzip make sed unix gentsroom c muse op2 ochiai space flex grep gzip sed gentsroom java ochiai tarantula jexel jparsec jaxen commons codec commons lang joda time c javadstar tarantula printtokens printtokens2 schedule schedule2 totinfo jtcas sorting nanoxml xml security32 gentsroom c dstar ochiai tarantula siemens space nanoxml xml security gentsroom this java metallaxis op2 dstar ochiai barinel tarantula musejfreechart closure commons lang commons math joda time this java dstar ochiai barinel tarantula op2 metallaxis musejfreechart closure commons lang commons math joda time gentsroomrepresents manually seeded artificial faults and represents mutation based artificial faults.
the siemens set is printtokens printtokens2 replace schedule schedule2 tcas and totinfo.
the unix set is cal checkeq col comm crypt look sort spline tr and uniq.
the programmer could have inserted the missing conditional before line between lines and or where it actually was inserted.
a fl technique that reports any of those statements is just as useful as one that reports the statement the programmer happened to choose.
for every real fault of omission we manually determined the set of candidate locations at which a code block could be inserted to fix the defect lines and in the example above .
we consider a fault localization technique to identify an omitted statement as soon as any candidate location appears in the fl technique s output.
faults in non ranked statements some fault localization techniques have limitations in that they fail to report some statements in the program.
here are examples non executable code declarations such as a supertype declaration or the data type in a field or variable declaration.
in the defects4j database of real world faults of real faults involve some non executable code locations and involve only non executable code locations.
non mutatable statements the mutation based fl techniques that we evaluate have a weakness in that they only output a list of mutatable statements.
some faulty executable statements are not mutatable due to compiler restrictions.
for example deleting or incorrectly moving a break orreturn statement might cause compilation errors.
in the defects4j database of real world faults of real faults involve a non mutatable yet executable statement.
previous studies on the effectiveness of fault localization have not considered faults in non ranked statements.
we ensure that the ranked list of statements produced by a fl technique always contains every statement in the program by adding any missing statement at the end of the ranking.
multiple defects large real world programs like those in defects4j almost always contain multiple defects coexisting with each other.
however no action is needed to correct for thiswhen performing fault localization as long as the failing tests only reveal one of these defects as is the case in defects4j .
iii.
s ubjects of investiga tion a. fault localization techniques this paper evaluates families of fault localization techniques spectrum based fault localization sbfl techniques for short which is the most studied and evaluated fl technique and mutation based fault localization mbfl techniques for short which is reported to significantly outperform sbfl techniques .
a survey paper lists other types of fault localization techniques .
most fault localization techniques including all that we examine in this paper yield a ranked list of program statements sorted by the suspiciousness score s s of the statement s.a high suspiciousness score means the statement is more likely to be defective that is to be the root cause of the failures.
spectrum based fl techniques spectrum based fault localization techniques depend on statement execution frequencies.
the more often a statement is executed by failing tests and the less often it is executed by passing tests the more suspicious the statement is considered.
this paper considers of the best studied sbfl techniques .
in the following let totalpassed be the number of passed test cases and passed s be the number of those that executed statement s similarly for totalfailed and failed s .
tarantula s s failed s totalfailed failed s totalfailed passed s totalpassed ochiai s s failed s totalfailed failed s passed s op2 s s failed s passed s totalpassed barinel s s passed s passed s failed s dstar s s failed s passed s totalfailed failed s variable .
we used the most thoroughly explored value.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
mutation based fl techniques mutation based fault localization techniques extend sbfl techniques by considering not just whether a statement is executed but whether that statement s execution is important to the test s success or failure that is whether a change to that statement changes the test outcome.
the more often a statement saffects failing tests and the less often it affects passing tests the more suspicious the statement is considered.
the key idea of mbfl is to assign suspiciousnesses to injected mutants based on the assumption that test cases that kill mutants carry diagnostic power.
a test case kills a mutant if executing the test on the mutant yields a different test outcome than executing it on the original program.
our study considered two well known mbfl techniques muse and metallaxis .
each one generates a set of mutants mut s for each statement s assigns each mutant a suspiciousness m m and aggregates the m m to yield a statement suspiciousness score s s .
muse s ranking can be obtained by setting m m failed m f2p p2f passed m where failed m is the number of failing tests that passed with minserted and f2p is the number of cases in the whole program where a mutant caused any failing test to pass.
passed m and p2f are defined similarly.
muse sets s s a v gm mut s m m .
metallaxis uses the same suspiciousness formula as ochiai ochiaif the superscript fdenoting a reference to the formula rather than the sbfl technique for the suspiciousness of each mutant m m failed m totalfailed failed m passed m where failed m is the number of failing tests whose outcomes are changed at all by the insertion of m e.g.
by failing at a different point or with a different error message and totalfailed is the number of tests that fail on the original test suite passed m and totalpassed are defined similarly .
the suspiciousness of statement siss s m a x m mut s m m .
since mbfl requires running the test suite once per possible mutant it is much more expensive than sbfl even with the optimizations described in our technical report that reduced the runtime by more than an order of magnitude running every test necessary to compute every mbfl technique s score on all faults took over cpu hours.
implementation we re implemented all the fault localization techniques using shared infrastructure.
this ensures that our results reflect differences in the techniques rather than differences in their implementations.
we collected coverage data using an improved version of gzoltar .
we collected mutation analysis data using the major mutation framework v1.
.
using all mutation operators it offers.
b. programs we used the programs in the defects4j dataset v1.
.
which consists of real faults from open source projects jfreechart google closure compiler apache commons lang apache commons math mockito and joda time.
for each fault defects4j provides faulty and fixed program versionstable ii how we selected faults for our replica tion studies .
faults action real artif.
consider all faults from defects4j every real fault must correspond to some artificial fault discard faults with deletion only fix discard undetectable artificial faults impose 000h timeout final with a minimized change that represents the isolated bug fix.
this change indicates which lines in a program are defective.
defects4j s patch minimization was performed by three authors of this paper using both automated analysis such as delta debugging and manual analysis to find a minimal patch that they agreed preserved the spirit of the programmer s fix.
given a minimized patch we used an automated analysis to obtain all removed inserted and changed lines but ignoring changes to declarations without an initializer addition and removal of compound statement delimiters curly braces annotations and import statements.
these statements do not affect the program s algorithm or are trivial to add and therefore a fl tool should not report them.
any other statement modified by the patch is a defective statement that a fl tool should report.
to reduce cpu costs we applied each fault localization technique only to the fault relevant classes .
a fault relevant class for a defect is any class that is loaded by any faulttriggering test for that defect.
this optimization is sound and a programmer could use it with little or no effort when debugging a failure.
we did not use slicing impact analysis or other approaches to further localize or isolate the defective code.
c. test suites all investigated fault localization techniques require as an input at least one test case that can expose the fault.
for each real fault defects4j provides a developer written test suite containing at least one such fault triggering test case.
to verify that each artificial fault has at least one fault triggering test case as well we executed the corresponding developerwritten test suite and discarded the of artificial faults that were not exposed by any test case.
this is on the same order as the results of a study finding that of mutants were undetectable of which do not change the program semantics.
iv .
r eplica tion artificial faults one goal of our work is to repeat previous evaluations of fault localization techniques on artificial faults using shared infrastructure between all of the techniques to reflect differences in the techniques not in their implementations.
this section describes the techniques and faults we studied our methodology for assessing them and the results of the comparison.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a. methodology research questions rq which fl techniques are significantly better than which others on artificial faults?
rq do the answers to rq1 agree with previous results?
data artificial faults we used the major tool to generate artificial faults by mutating the fixed program versions in defects4j.
we could have generated an artificial fault for every possible mutation of every statement in the program but many of these artificial faults would be in parts of the program completely unrelated to the corresponding real fault where fault localization might be easier or harder.
therefore we only generated artificial faults for formerly defective statements of the fixed program version that is those that would need to be modified or deleted to reintroduce the real fault.
in more detail each real fault in defects4j is associated with a faulty and a fixed program version.
for each of these pairs of program versions defects4j provides a patch which when applied to the fixed version would reintroduce the real fault.
we call the statements modified or deleted by this patch the fixed statements of the real fault.
we generated artificial faults by mutating the fixed statements.
our methodology of comparing real faults to artificial faults requires that for every real fault there is at least one artificial fault for comparison.
we discarded real faults that did not fit this criterion as shown in table ii.
first we discarded real faults whose fixes only deleted erroneous code so there were no fixed statements and no artificial faults can be generated.
then we discarded artificial faults that are not detected by any tests we also discarded artificial faults that cause the test suite to time out e.g.
because they introduce infinite loops .
this discarded all artificial faults for real faults so we removed those real faults from the study.
some faults that do not introduce an infinite loop nonetheless take a very long time during mutation testing.
we ran our experiments for about hours and discarded faults whose mbfl analysis had not yet completed.
the output of this process was a set of artificial faults corresponding to different real faults each artificial fault existing in a fixed statement of the corresponding real fault and detectable by the same developer written test suite.
we computed the exam score for each artificial fault and fl technique.
experimental design we answered our research questions through the following analyses rq we used three independent complementary evaluation metrics to rank fl techniques from best to worst mean exam score across all artificial faults.
tournament ranking comparing the sets of exam scores of each pair of techniques awarding point to the winner if it is statistically significantly better and ranking by number of points.
mean flt rank using each fault to rank the techniques from to and averaging across all artificial faults.
flt stands for fault localization technique .table iii fault localiza tion techniques sorted by mean exam score or tournament ranking .
w orse is the number of other techniques tha t are sta tisticall y significantl y worse in the tournament ranking .
artificial faults real faults technique exam worse technique exam worse metallaxis .
dstar .
op2 .
ochiai .
dstar .
barinel .
ochiai .
tarantula .
barinel .
op2 .
tarantula .
metallaxis .
muse .
muse .
rq for each pair of techniques compared by prior work table i we determined whether the two techniques distributions of exam scores are significantly different.
for all statistical comparisons between any two techniques in this paper we performed a paired t test for two reasons.
first our experiments have a matched pairs design fault localization results are grouped per defect.
second while the exam scores are not normally distributed the differences between the exam scores of any two techniques are close to being normally distributed.
given that we have a large sample size and no serious violation of the normality assumption we chose the t test for its statistical power.
b. results best fl technique on artificial faults the mean exam score metric and tournament ranking metric were perfectly consistent with each other and produce the ordering shown in the left half of table iii.
the third metric mean flt rank also agrees perfectly with the others except that muse does best by mean flt rank and worst by the other two metrics.
as shown by the peaks for the dotted lines in fig.
mbfl techniques very often rank the artificially faulty statement in the top .
one reason for this is that many artificial faults we generate are caused by reversible mutants mutants that can be exactly canceled by applying a second mutant e.g.
a b a b a b .
reversible artificial faults guarantee that mbfl will consider some mutant in the faulty statement that fixes every failing test and that receives a very high suspiciousness score.
agreement with previous results for each of the pairs of techniques that the prior work in table i has compared we performed a two tailed t test comparing the two techniques scores for artificial faults.
the left column of table iv shows the results of prior comparisons and the middle columns show our results.
notable features include small effect sizes.
all pairs of techniques have statistically significant differences the agree?
column of table iv is unparenthesized.
however the practical differences that is effect sizes are all small or negligible the d column is parenthesized.
all spectrum based techniques except tarantula are nearly indistinguishable in fig.
.
we only see statistical significance because of our large number of artificial faults.
consistency with prior sbfl sbfl comparisons.
our results agree with all previous comparisons between sbfl authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv previousl y reported comparisons and our results for those comparisons in the best case debugging scenario .
the conclusions are the same for all debugging scenarios .
emphasis on whether our study agrees indicates p value p .
p .
p .
.
emphasis on cohen s dindicates effect size large medium small negligible .
the column ci gives the confidence interval for the difference in means.
the column b eq w gives the counts for per defect was the winner better equal to or worse compared to the loser ignoring the magnitude of the difference.
previous comparisons our study on artificial faults our study on real faults winner loser agree?
d eff.
size ci b eq w agree?
d eff.
size ci b eq w ochiai tarantula yes .
insig .
.
barinel ochiai no .
insig .
.
barinel tarantula yes .
insig .
.
op2 ochiai yes .
no .
op2 tarantula yes .
insig .
.
dstar ochiai yes .
insig .
.
dstar tarantula yes .
insig .
.
metallaxis ochiai yes .
no .
muse op2 no .
no .
muse tarantula no .
no .
artificial faults real faults .
.
.
.
.
.
.
.
.
.
.
.
.
.
exam score log scale densityfl technique barinel dstar metallaxis muse ochiai op2 tarantula family mbfl sbfl artificial faults real faults .
.
.
.
.
absolute score log scale density fig.
.
distributions of exam and absolute scores for all fl techniques considering the best case debugging scenario and artificial vs. real faults.
the absolute score is the first location of any defective statement in the suspiciousness ranking of program statements computed by a fault localization technique.
techniques except the claim that barinel outperforms ochiai which our results contradict.
disagreement with prior sbfl muse comparisons.
prior comparisons found muse superior to sbfl techniques.
our results do not support that finding although muse is better on many artificial faults it does much worse on others.
overall the differences are practically insignificant.
v. r eplica tion real faults a. subjects we evaluated the same techniques programs and test suites as described in section iii except that instead of evaluating each technique on the artificial faults described in section iv we evaluate them on the corresponding real faults.
b. methodology our methodology is exactly like that described in section iv a except evaluated on real faults to answer rq which fl techniques are significantly better than which others on real faults?
rq do the answers to rq3 agree with previous results?
c. results best fl techniques on real faults the right hand columns in table iii show the fl technique rankings for real faults produced by either the mean exam score metric or the tournament ranking metric.
the mean flt rank gives almost the same ranking except that metallaxis ranks first instead of nearly last.
metallaxis usually has slightly higher scores than any other technique as shown in fig.
giving it a good flt rank but it also has more extreme outliers than sbfl techniques greatly damaging its mean exam score.
for more details see our technical report .
agreement with previous results the right hand columns of table iv compare our results on real faults to the results of the studies we replicated.
notably authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
insignificant differences between sbfl techniques.
all effect sizes are negligible column d is italicized.
practical significance muse performs poorly.
the only practically significant differences show that muse performs poorly on real faults see fig.
.
this is due to almost no real faults being reversible by a single mutant see section iv b .
d. comparison to artificial faults the most important feature of tables iii and iv is that there is no significant relationship between the results for real and artificial faults.
this suggests that artificial faults are not useful for the purpose of determining which fl technique is best at localizing mistakes that programmers actually make.
another notable feature is that while metallaxis performs best on artificial faults it does worse than spectrum based techniques on real faults.
one reason for this may be that of real world faults involve non mutatable statements which appear last in mutation based techniques suspiciousness rankings.
these outlier scores greatly degrade the technique s mean score.
on real faults metallaxis has the best mean flt rank but one of the worst mean exam scores.
we repeated the analysis of rq1 restricted to different categories of real faults.
again there are no statistically significant relationships between rankings on real and artificial faults even for single line faults which one would expect to be the most similar to mutants or faults of omission on which some fl techniques might perform poorly.
e. controlling for number of samples table iv shows statistically significant results for artificial faults but mostly insignificant results on real faults.
it is possible that the results are insignificant because there are so few data points there are many more artificial than real faults.
to investigate this we averaged results for each artificial fault that corresponds to a single real fault so there are only datapoints for artificial faults.
the results for artificial faults remained statistically significant showing that having data points does not prevent statistically significant results.
furthermore the effect sizes for real faults are negligible for sbfl comparisons and the confidence intervals are tight table iv .
artificial faults are not a good proxy for any single technique the correlation between each technique s performance on artificial faults and real faults is at most moderate mostly weak or negligible.
vi.
e xploring a design space to better understand these differences in performance and their causes we developed a design space that encompasses all of these techniques and evaluated the techniques in that space on our overall set of real faults.
a. subjects all the techniques of section iii a have the same structure for each program element i.e.
statements or mutants count the number of passing failing tests that interact with i.e.
execute or kill that element.
calculate a suspiciousness for each element by applying a formula to the numbers of passing failing tests that interact.
if necessary group those elements by statement and aggregate across the elements suspiciousnesses to compute the statement s suspiciousness.
rank statements by their suspiciousness.
we developed a taxonomy for describing any of these techniques in terms of different parameters formula the formula used to compute elements suspiciousness values e.g.
ochiaif total definition the method for weighting passing failing test interactions in the formula interaction definition what it means for a test to interact with an element i.e.
coverage for sbfl killing for mbfl aggregation definition for mbfl the way of aggregating elements suspiciousness by statement e.g.
max average these parameters are described in more detail below.
for sbfl techniques the elements are simply statements.
a test interacts with a statement by executing it and no aggregation of elements by statement is necessary so the only two relevant parameters are formula and total definition.
the following subsections detail the possible values for each of these parameters.
by taking all sensible combinations of them we arrive at a design space containing techniques.
f ormula we consider the formulas for the sbfl techniques tarantula ochiai dstar barinel and op2 as well as the formula used by muse which can be cast as s s failed s totalfailed totalpassed passed s .
when combined with the appropriate values of the other parameters this formula produces muse s statement ranking.
total definition almost all of the prior fl techniques make use of totalpassed and totalfailed in their suspiciousness formulas representing the numbers of passing failing tests.
muse though recall from section iii a instead refers to p2f and f2p representing the number of mutants killed by passing failing tests.
motivated by the resemblance between these quantities we introduced a parameter that determines whether totalpassed in the fl technique s formula refers to the number of tests or the number of elements interacted with by the tests and similarly for totalfailed .
interaction definition for sbfl there is one clear definition for whether a test interacts with a statement coverage or executing the statement.
for mbfl the definition of whether a test kills a mutant is not firmly established.
muse requires that the mutant change whether the test passes or fails while metallaxis merely requires that the mutant cause any change to the test s output for example change the message of an exception thrown by a failing test .
we used the following framework to describe the spectrum of possible definitions.
a test kills a mutant if it changes the test outcome more specifically if it changes the outcome s equivalence class .w e give ways to define the equivalence classes.
all of them define one class each for pass timeout jvm crash and several classes for exception including assertionerror .
the definitions differ in how they partition exceptions authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
exact exceptions with the same stack trace are equivalent type fields location exceptions with the same type message and location are equivalent type fields exceptions with the same type and same message are equivalent type exceptions with the same type are equivalent all all exceptions are equivalent passfail all exceptions are equivalent to one another and to the time out and crash classes so there are only two possible equivalence classes pass and fail .
metallaxis uses the exact definition.
muse uses the passfail definition.
aggregation definition mbfl computes an aggregate statement suspiciousness s s from the suspiciousnesses of individual mutants by taking either the average like metallaxis or the maximum like muse .
unmutatable statements are not assigned any suspiciousness and therefore do not appear in the technique s ranking.
approximately of defects4j s faults contain at least one unmutatable faulty statement.
this causes mbfl to do quite poorly in the worst case debugging scenario when its goal is to find the position of allfaulty statements.
b. methodology rq which technique in this design space performs best on real faults?
for each of our evaluation metrics exam score fl t rank and debugging scenarios best case average case worst case we identified the technique that performed best averaged across all real faults.
to quantify how often these techniques significantly outperform others we performed pairwise comparisons between them and each of the other techniques using a paired t test.
rq what are the most significant design decisions for a fl technique?
we performed an analysis of variance to determine the influence of the design space parameters the debugging scenario and the defect on the exam score.
in other words we compute how much variance in the exam score is explained by each factor.
c. results what is the best fault localization technique?
for the best case debugging scenario the dstar technique has the smallest mean exam score see table v with more details in dstar is statistically significantly better than almost every other technique in the design space see table vi and its score is almost twice as good as the best mbfl technique which closely resembles metallaxis.
for the other two debugging scenarios barinel and ochiai perform best when instantiated with the number of statements covered definition of totalpassed and totalfailed .
judged by the mean exam score all sbfl techniques are better than the best mbfl technique.
however judged by mean fl t rank this reverses and many mbfl techniques are better than any sbfl technique.
as seen in table vi the best mbfl technique by flt rank for two debugging scenarios uses muse s formula and total definition but metallaxis s aggregation and interaction kill definition.
recall fromtable v best fl techniques per famil y according to mean exam score .
the first column is rank among the techniques .
family formula total def.
interact.
def.
agg.
def exam score best case debugging scenario localize any defective statement sbfl dstarftests .
mbfl ochiaifelements exact max .
worst case debugging scenario localize all defective statements sbfl barinelfelements .
mbfl ochiaifelements exact max .
average case debugging scenario localize of the defective statements sbfl ochiaifelements .
mbfl dstarftests exact max .
table vi pairwise comparison of the best technique per ev alua tion metric with all other techniques in the design space .
b etter than gives the number of comparisons for which the best technique is significantl y better and d gives the a verage effect size .
evaluation metricbest fl technique better than d family formula total inter.
agg.
def.
def.
def.
best case debugging scenario localize any defective statement mean exam score sbfl dstarftests .
mean rank mbfl dstarftests type avg .
worst case debugging scenario localize all defective statements mean exam score sbfl barinelfelements .
mean rank mbfl musefelements exact max .
average case debugging scenario localize of the defective statements mean exam score sbfl ochiaifelements .
mean rank mbfl musefelements exact max .
section iv b that muse s kill definition tied its performance to fault reversibility.
using a different kill definition damages its performance on reversible faults but makes it much better on real faults .
dstar is statistically significantly better than all but four techniques in the design space and the best mbfl technique is statistically significantly better than about of the design space.
dstar is the best fault localization technique in the design space.
however it is statistically indistinguishable from four other sbfl techniques including ochiai and barinel.
which parameters matter in the design of a fl technique?
we analyzed the influence of the different parameters on the exam score.
table vii shows the results indicating that all factors including all fl technique parameters as well as the defect and debugging scenario have a statistically significant effect on the exam score.
it is unsurprising that most of the variance in scores sum of squares column is accounted for by which defect is being localized some faults are easy to localize and some are difficult.
interestingly although prior studies have mostly focused on the formula and neglected other factors we find that the formula has relatively little effect on how well a fl technique performs.
the choice of the formula accounts for no more than of the non defect variation in the exam scores.
furthermore a post hoc tukey test showed that the differences between all formulas are insignificant for sbfl techniques.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vii anov a anal ysis of the effect of all factors on the exam score for real faults .r2gives the coefficient of determina tion .
factor deg.
of freedom sum of squares f value p sbfl r2 .
defect .
debugging scenario .
.
formula .
.
total definition .
insig.
mbfl r2 .
defect .
debugging scenario .
interaction definition .
formula .
.
aggregation definition .
.
total definition .
.
sbfl mbfl r2 .
defect .
debugging scenario .
family .
formula .
.
total definition .
.
all studied parameters have a statistically significant effect on the exam score but the only fl technique parameters with a practically significant effect are family sbfl vs. mbfl and interaction kill definition for mbfl only .
vii.
n ew techniques beyond the quantitative results discussed so far our studies exposed three limitations of mbfl techniques.
i mbfl techniques perform poorly on defects that involve unmutatable statements.
ii mbfl techniques perform poorly when some mutants are covered but not killed.
iii the run time of mbfl techniques is several orders of magnitude larger than for sbfl techniques because mutation analysis requires running the entire test suite many times once per mutant .
we designed several new variants of mbfl to address these limitations by using coverage information to augment the mutation information mcbfl mutant and coverage based fl which increases the suspiciousness of mutants covered by failing tests thus ensuring that mutants covered but not killed by failing tests are more suspicious than ones not even covered mcbfl hybrid failover which uses sbfl to assign suspiciousnesses to unmutatable statements thus placing them more accurately in the ranking than mbfl can mcbfl hybrid avg which averages each statement s mbfl suspiciousness with the suspiciousness calculated by a sbfl technique mcbfl hybrid max which does the same but takes the greater of the two suspiciousnesses and mrsbfl which uses mutation coverage information to replace the kill information in mbfl thus requiring only a single run of the test suite making it as inexpensive as sbfl.table viii percentage of defects whose defective sta tements appear within the top top and top of the techniques suspiciousness ranking .
deb ugging scenario best case dbg.
scen.
worst case dbg.
scen.
avg case dbg.
scen.
technique top top top top top top top top top mcbfl h ybrid avg metallaxis dstar we evaluated these novel techniques on our overall set of real faults considering all debugging scenarios.
overall mcbfl hybrid avg is better than any other technique in all debugging scenarios but the difference in exam score and fl t rank is not practically significant a technical report gives full experimental results .
table viii compares our new mcbfl hybrid avg technique to the best sbfl and mbfl techniques in terms of how often they report defective statements in the top or statements.
this is relevant because a recent study showed that of practitioners consider a fault localization technique to be useful only if it reports the defective statement s within the top of the suspiciousness ranking.
another analysis shows that automatic program repair systems perform best when they consider only the top suspicious statements.
while the sbfl and mbfl techniques perform equally well under this light they complement each other.
this leads our new mcbfl hybrid avg technique to clearly report more defective statements near the top of the suspiciousness ranking than any previous technique.
viii.
t hrea ts to v alidity generalization.
defects4j s data set spans programs written by different developers and targeting different application domains.
our set of real faults is much larger than all previous studies combined less than faults .
nonetheless future research should verify whether our results generalize to other programs and test suites.
based on the consistency of our results so far table iii we believe that artificial faults are not good proxies for real faults for evaluating any sbfl or mbfl techniques.
however slice based or model based techniques see section ix are sufficiently different that our results may not carry over to them.
applicability.
the exam score may not be the best metric for comparing usefulness of fl techniques by humans in one study expert programmers diagnosed faults more quickly with fl tools than without but better exam scores did not always result in significantly faster debugging .
our study revolves around the comparison of fl techniques rather than their absolute performances.
furthermore our mean fl t rank metric is agnostic to whether absolute or relative scores are being compared.
other metrics may be better correlated with programmer performance such as defective statements in the top section vii .
it is unknown which metrics are best for other uses of fault localization such as automated program repair.
even for the use case of human debugging our study authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
yields insights into the construction and evaluation of fl techniques and what user studies should be done in the future.
verifiability.
all of our results can be reproduced by an interested reader.
our data and scripts are publicly available at our methodology builds upon other tools which are also publicly available.
notable examples are the defects4j database of real faults the gzoltar fault localization tool and the major mutation framework ix.
r ela ted work according to a recent survey the most studied and evaluated fault localization techniques are spectrum based slice based model based and mutation based .
for reasons of space we discuss the most closely related work our technical report contains extensive additional discussion.
a. evaluation of fault localization techniques table i