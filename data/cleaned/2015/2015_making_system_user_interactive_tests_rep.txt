making system user interactive tests repeatable when and what should we control?
zebao gao yalan liang myra b. cohen atif m. memon and zhen wang department of computer science university of maryland college park md usa email gaozebao atif cs.umd.edu department of computer science engineering university of nebraska lincoln lincoln ne usa email yaliang myra zwang cse.unl.edu abstract system testing and invariant detection is usually conducted from the user interface perspective when the goal is to evaluate the behavior of an application as a whole.
a large number of tools and techniques have been developed to generate and automate this process many of which have been evaluated in the literature or internally within companies.
typical metrics for determining effectiveness of these techniques include code coverage and fault detection however with the assumption that there is determinism in the resulting outputs.
in this paper we examine the extent to which a common set of factors such as the system platform java version application starting state and tool harness configurations impact these metrics.
we examine three layers of testing outputs the code layer the behavioral or invariant layer and the external or user interaction layer.
in a study using five open source applications across three operating system platforms manipulating several factors we observe as many as lines of code coverage difference between runs using the same test cases and up to percent false positives with respect to fault detection.
we also see some a small variation among the invariants inferred.
despite our best efforts we can reduce but not completely eliminate all possible variation in the output.
we use our findings to provide a set of best practices that should lead to better consistency and smaller differences in test outcomes allowing more repeatable and reliable testing and experimentation.
i. i ntroduction numerous development and maintenance tasks require the automated execution and re execution of test cases and often these are run from the system or user interface perspective .
for instance when performing regression testing on a web application or integrating components with a graphical user interface gui a test harness such as selenium mozmill or unified functional test uft can be used to simulate system level user interactions on the application by opening and closing windows clicking buttons and entering text.
re execution of tests is also needed when performing invariant generation and when experimenting to evaluate the effectiveness of new test generation techniques .
over the past years numerous papers have proposed improved gui and web automation testing debugging techniques and common metrics have been used to determine success such as fault detection time to early fault detection business rule coverage and statement branch and other structural coverage criteria .
test oracles mechanisms that determine whether a test passed or failed for these applications operate at differing degrees ofprecision such as detecting changes in the interface properties or comparing outputs of specific functions or through the observation of a system crash .
the assumption for all of these scenarios however is that these system user interactive tests suits can be reliably replayed and executed i.e.
they produce the same outcome code coverage invariants state unless either the tests or software changes.
recent work by luo et al.
illustrates that many tests do not behave deterministically from the perspective of fault detection what is commonly called flaky behavior in industry .
other research by arcuri et al.
has developed methods to generate unit tests that behave differently in varying environments controlling this by replacing api calls and classes related to the environment with mocked versions.
and there has been attention given to revisiting the expectation of sequence based testing that of test order independence .
in our own experience with gui testing and benchmarking we have learned that the ability to repeat others experimental results with different platforms and or setups is hard to do.
we set out to compile a comprehensive list of factors that should be controlled to ensure testing can be repeated reliably within and across platforms.
unfortunately the harder we tried the more we learned that our assumptions about how well we could control tests were incorrect.
we found that even within the same platform simple changes to system load or a new version of java or even the time of day that we ran a test could impact code coverage or the application interface state.
in this paper we have taken a step back to empirically evaluate the set of factors that we have observed have the largest impact on our results and to examine how and if these can be controlled.
we have designed a large empirical study to evaluate for certain classes of interactive applications those driven by the graphical user interface or gui what extent of variation is seen and which factors matter the most.
we use an entropy based metric to determine the stability of test runs and study this for different test metrics code coverage gui state and invariants .
our results show that the impact is large for many applications when factors are uncontrolled as many as lines of code coverage differ and more than false positives for fault detection may be observed .
despite our ability to control factors and reduce variance there still may exist some that we cannot control thatare application specific or sensitive to minor changes in timing or system load therefore a single run of any testing technique despite fault determinism may lead to different code coverage or even different invariants and that unless one accounts for the normal variance of a subject a single test run may be insufficient to claim that one technique is better than another or even that a fault is really a true fault.
the contributions of this work are .
the identification of a set of factors that impact the repeatability of testing results at three different layers of user interactive applications .
a large empirical study across a range of platforms on a set of open source applications that measures the impact these factors have on different types of test outputs and .
a set of practical steps that one can take when comparing different test runs to ensure higher repeatability.
in the following section we provide a motivating example and discuss related work.
we follow this with our empirical study in section iii followed by results section iv .
finally in section v we conclude and present future work.
ii.
m otivating examples we begin by differentiating three layers illustrated in figure of a user interactive software application user interaction layer we typically use this layer to execute suits and extract information for test oracles behavioral layer to infer properties such as invariants regarding the test execution and code layer for code coverage.
user interaction layer behavioral layer code layer fig.
layers of a user interactive application user interaction layer we use this to interface with the application and run tests.
for example in the android system one could write the following code segment in a system test to programmatically discover and click on the ok button.
u i o b j e c t okbutton new u i o b j e c t new u i s e l e c t o r .
t e x t ok okbutton .
c l i c k we also can use this layer to identify faults since this is the layer that the user sees.
for instance if a list widget fails torender or displays the wrong information then this layer will reveal a fault.
from an automation perspective the properties of the interface widgets can be captured and compared to a correct expected output e.g.
using the following code that gets a handle to the current screen clicks on a list and checks whether the list contains the text android .
s o l o new solo g e t i n s t r u m e n t a t i o n g e t a c t i v i t y s o l o .
c l i c k i n l i s t a s s e r t t r u e s o l o .
s e a r c h t e x t android a s s e r t i o n during our experiments we find variation in the properties of certain interface widgets between runs that would appear as a fault to the test harness and would be a real fault if this happened during manual execution but that is most likely an artifact of automated test execution a false positive.
for instance in the application jedit described later in section iii we found that the widget org.gjt.sp.jedit.gui.statusbar.errorswidgetfactory errorhighlight had empty text in one run but was filled in during other runs.
we believe this had to do with a delay between test steps that was too short when the system load increased preventing the text from rendering to completion before the next test step occurred.
a test case that checks the text in this widget may non deterministically fail during one run and succeed in another.
behavioral layer this layer presents behavioral data such as runtime values of variables and function return values and returning data to an external location or database which may be obtained from the running program via instrumentation hooks.
such data can also be mined analyzed to infer invariants.
invariant detection involves running a set of test cases and inferring from this a set of properties that hold on all test cases.
invariants represents high level functionality of the underlying code and should be consistent between runs.
in our experiments we found differences in the invariants reported.
for instance in the application rachota section iii we found that approximately in two of every ten runs the application started faster than normal and generated the following two extra invariants related to the startup window t h i s .
lbimage o r i g org .
c e s i l k o .
r a c h o t a .
g u i .
startupwindo w .
startupwindow .
lbimage t h i s .
l o a d i n g o r i g org .
c e s i l k o .
r a c h o t a .
g u i .
startupwindo w .
startupwindow .
l o a d i n g not found in the other eight runs.
in each run we used exactly the same set of test cases.
the correct set of invariants is dependent on the speed at which the window opens and again may be an artifact of the system load or test harness factors.
code layer at the lowest level we have the source code.
it is common to measure code coverage at the statement branch or path level to determine coverage of the underlying business logic.
it is very commonly used in experimentation to determine the quality of a new testing technique.
in our experiments we found numerous instances of the same test case executing different code.
in fact this was a large motivating factor for our work.
the harder we tried to deterministically obtain ten runs with the exact code coverage the more we learned that this may not be possible.
we also ruled out factors such as concurrency and multi threading.
the following code shows an example of memory dependent code from the application drjava that we cannot deterministically control with our test harness.
i f whole sb .
append whole sb .
append sb .
append s i z e s .
.
.
in this code lines of stringops.java the memsizetostring method we see code that checks memory usage so that it can create a string object stating the memory size.
it checks for whole block boundaries e.g.
1b 1kb 1mb via whole and constructs the string content accordingly.
because the actual memory allocated to the executing jvm may vary from one run to the next in our experiments one in ten executions covered this code because by chance the space was not equal to a block.
the next code segment is an example of code that we can control by making sure the environment is mimicked for all test cases.
p u b l i c s t a t i c i n t p a r s e p e r m i s s i o n s s t r i n g s i n t p e r m i s s i o n s i f s .
l e n g t h i f s .
c ha r a t r p e r m i s s i o n s i f s .
c ha r a t w p e r m i s s i o n s .
.
.
.
e l s e i f s .
c ha r a t t p e r m i s s i o n s return p e r m i s s i o n s in this code jedit miscutilities class lines the application checks file permissions.
we found that the files did not always properly inherit the correct permissions when moved by the test script and this caused different lines being covered between executions or when time and date were involved as has been described in .
other examples of differing code coverage occurred when opening screens of windows have code that waits for them to settle based on a time .
these are just some examples of the types of differences we found at the various layers when we examined our results.
our aim in this work is to identify how much these impact our testing results and provide ways to minimize these differences.
we next present the set of factors that we believe cause the largest impact on test differences in our applications.
a. factors impacting execution we categorize the factors we believe have the greatest impact on running suits deterministically into four groups .test execution platform.
many of the applications that we test can be run on different execution platforms which includes different operating systems e.g.
windows mac os linux or on different versions of the same operating system e.g.
ubuntu ubuntu .
different operating systems may render system interfaces differently and could have different load times etc.
in addition applications often contain code that is operating system specific.
.application starting state configuration.
many applications have preferences or configuration files registry entries that impact how they start up.
research has shown that the configuration of an application impacts test execution therefore we know that this starting point is important.
even if we always start with a default configuration at the start of testing test cases may change the configurations for future tests.
therefore the program configuration files should be located and restored before each test is run.
.test harness factors.
test harnesses such as selenium contain parameters such as step delays orstartup delays to ensure that the application settles between test steps however this is often set to a default value and or is set heuristically by the tester.
long delays may mean that the application pauses and runs additional code but short delays may not allow completion of some functionality particularly when system load or resources vary between executions.
in early experiments we ran tests on a vm where we can change cpu and memory and have found that reducing memory and or cpu has a large impact on the ability to repeat test execution primarily due to the need for tuning these parameters.
.execution application versions.
if we are running web applications using different web browsers or java programs using different version of virtual machines e.g.
java versus or openjdk versus oracle java we may run into differences due to support for different events and threading policies.
we select a subset of these factors in our experimentation to evaluate them in more detail.
iii.
e mpirical study we now evaluate the impact of factors that we have identified summarized in the previous section on test repeatability in suits via the following research questions.
rq1 to what extent do these factors impact code coverage?
rq2 to what extent do these factors impact invariant detection?
rq3 to what extent do these factors impact gui state coverage?
note that our questions include one for each of the layers.
we start with the lowest code layer code coverage and end with the highest user interaction layer gui state .
a. objects of analysis we selected five non trivial open source java applications with gui front ends from sourceforge.net.
all of these have been used in prior studies on gui testing.
table i shows the details of each application.
for each we show the version the lines of code the number of windows and the number of events on the interface.
rachota is a time tracking tool allowing one to keep track of multiple projects and create customized time management reports.
buddi is a personal financial tool for managing a budget.
it is geared for those with little financial background.
jabref is a