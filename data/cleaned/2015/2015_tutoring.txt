a feasibility study of using automated program repair for introductory programming assignments jooyong yi innopolis university russia j.yi innopolis.ruumair z. ahmed indian institute of technology kanpur india umair cse.iitk.ac.inamey karkare indian institute of technology kanpur india karkare cse.iitk.ac.in shin hwei tan national university of singapore singapore shinhwei comp.nus.edu.sgabhik roychoudhury national university of singapore singapore abhik comp.nus.edu.sg abstract despite the fact an intelligent tutoring system for programming itsp education has long a t tracted interest its widespread use has been hindered by the difficulty of generating personalized feedback automatically.
meanwhile automated program repair apr is an emerging new technology that automatically f ixes so f tware bugs and it has been shown that apr can f ix the bugs of large real world so f tware.
in this paper we study the feasibility of marrying intelligent programming tutoring and apr.
we perform our feasibility study with four state of the art apr tools genprog ae angelix and prophet and programs wri t ten by the students taking an introductory programming course.
we found that when apr tools are used out of the box only about of the programs in our dataset are repaired.
t his low repair rate is largely due to the student programs o f ten being signi f icantly incorrect in contrast professional so f tware for which apr was successfully applied typically fails only a small portion of tests.
to bridge this gap we adopt in apr a new repair policy akin to the hint generation policy employed in the existing itsp.
t his new repair policy admits partial repairs that address part of failing tests which results in improvement of repair rate.
we also performed a user study with novice students and graders and identi f ied an understudied problem while novice students do not seem to know how to effectively make use of generated repairs as hints the graders do seem to gain bene f its from repairs.
ccs concepts applied computing computer assisted instruction so f tware and its engineering so f tware testing and debugging keywords intelligent tutoring system automated program repair t he f irst author did part of this work at national university of singapore.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro f it or commercial advantage and that copies bear this notice and the full citation on the f irst page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permi t ted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci f ic permission and or a fee.
request permissions from permissions acm.org.
esec fse paderborn germany acm.
.
.
.
.
.
.
introduction developing and using intelligent tutoring system for novice programmers has gained renewed a t tention recently .
t he typical goal of an intelligent tutoring system for programming itsp is to f ind bugs in student programs and provide proper feedback for the students to help them correct their programs.
an itsp can also be used to help human tutors deal with many different student programs efficiently.
while an itsp for novice programmers has already existed since at least the early 80s it has not been widely adopted in the education f ield.
t he main difficulty of building an effective itsp is in the high degree of variations of student programs which makes it challenging to automatically generate personalized feedback without requiring additional help from the instructor.
despite this difficulty with the advent of massive open online course mooc and increasing interest in end user programming the need for an effective itsp has never been greater.
with the technological advances made during the last more than three decades since an early prototype system meno ii was introduced it may now be possible to realize the widespread use of itsp.
automated program repair apr is an emerging new technology that has recently been actively researched .
an apr system f ixes so f tware bugs automatically only requiring a test suite that can drive the repair process.
failing tests in the test suite become passing a f ter repair which manifests as a bug f ix.
apr was originally developed to f ix professionally developed large so f tware and an apr tool angelix recently reported and automated the f ix of the heartbleed bug .
in this paper we seek to study the inter play between apr and itsp.
given that student programs are much simpler than professionally developed so f tware applying apr to student programs may seem achievable.
however when we apply four state of theart apr tools namely genprog ae angelix and prophet to student programs obtained from an introductory programming course offered by the third author at indian institute of technology kanpur repairs are generated only for of these programs.
t he remaining about of the student programs in our dataset are not repaired by any of the four tools.
one of the main reasons for a low repair rate is that student programs are o f ten severely incorrect and fail the majority of the tests.
in our dataset of the programs fail more than half of the available tests.
t his is in contrast to the fact that professionalso f tware for which apr was successfully applied typically fails only a small portion of tests.
to rectify an incorrect program that fails the majority of tests it is o f ten necessary to make sizable changes to the program.
indeed about half of the programs in our dataset require more than one hunk of changes to reach the correct programs our dataset contains a corresponding correct program for each incorrect program .
however the current apr tools can f ix only a small number of lines most successful repairs reported in the literature change a small number of lines and some tools such as spr and prophet even restrict the change to a single line.
given these discrepancies it seems infeasible to use apr tools for the purpose of tutoring programming.
difference between bug fixing and program tutoring.
while we report in this paper apr tools weak capability to f ix novice student programs showing a correct program to a student is not necessarily the best way to provide students with feedback.
in fact experienced human tutors show an answer only selectively when students make simple errors such as syntactic errors .
for more complex errors such as semantic errors human tutors in general do not directly correct the error instead they give students hints.
t hat way tutors can help students move toward a correct answer.
partial repairs as hints.
considering this difference between bug f ixing and program tutoring we explore the possibility of using apr tools for the purpose of generating hints for the sake of teaching programming to students.
when student programs fail multiple tests we change the repair policy of apr tools as follows.
given an incorrect student program p a repair candidate p0is returned as a repair if all previously passing tests still pass with p0 and at least one of previously failing tests passes with p0.
we call such as repair a partial repair distinguishing it from the complete repair that passes all tests following the original repair policy of apr.
by comparing a generated partial repair with the incorrect program students can see when a particular test fails or passes which can help a student understand why his or her program fails the test addressed by the partial repair.
since a generated partial repair ris specialized for the tests addressed by r the expected usage of partial repairs is to encourage students to modify their own incorrect program by taking account of the partial repair rather than blindly accepting it.
we note that our partial repair is conceptually similar to the next step hint advocated in the education f ield .
by looking at a next step hint students can make forward progress toward an answer.
in contrast recent automated feedback generation techniques that appeared in the so f tware engineering and programming languages f ields are evaluated under a restricted assumption that student programs are almost correct.
to facilitate the use of partial repairs as hints our modi f ied repair strategy generates one of the following two forms of repairs.
t he f irst kind of a partial repair is if e fsg where sis a modi f ied added deleted statement and eis the guard expression for s. when such a form of a repair is generated the student can obtain a hint about a data f low change by observing the modi f ication addition deletion of s along with an additional hint about when that data f low change is necessary by observing the guard e. t he second kind of a partial repair modi f ies only conditional expressions which gives students a hint about control f low changes.improved feedback rate.
a f ter changing the repair policy allowing partial repairs and the repair strategy feedback rate repair generation rate signi f icantly improves showing improvement.
in about of the programs in our dataset either complete or partial repairs are generated.
by analyzing the remaining cases where repairs are not generated we identify a few common reasons for repair failure the two most common reasons being the need for output string modi f ication and array modi f ication for which the current apr tools are not specialized.
it would be most cost effective to strengthen repair operators that can manipulate strings and arrays in future apr tools.
user study.
a high feedback rate is only one necessary condition for using apr for programming tutoring.
to see whether automatically generated repairs actually help students and graders we perform a user study with students taking an introductory c programming course and teaching assistants tas of the same course part of whose duty is to grade student assignments.
in our user study students problem solving time increases when generated repairs are provided as hints whereas tas grading performance improves.
t his difference seems to be due to that repairs generated by apr tools over f it the provided test suite which is the well known problem in apr .
while tas can in general spot the problems of the incorrect student program based on suggested repairs novice students are likely to be distracted by the overly specialized suggestions.
to transform automatically generated repairs into feedback that can actually help students post processing of generated repairs seems necessary while answering the question about which form of feedback is bene f icial for students remains a future challenge.
note that even if the ideal correct repair for a given student program is available post processing is still necessary to give the student a hint not a solution.
our contributions.
in our feasibility study of using apr for introductory programming assignments we found that t he current state of the art apr tools more o f ten than not fail to generate a repair.
however they can more o f ten than not generate partial repairs that pass part of previously failing tests.
generating partial repairs are analogous to that human tutors guide the students gradually toward the answer by giving them hints.
failure of apr is o f ten due to a few common reasons such as the weak ability of apr tools to change the output string.
automatically generated repairs seem to help tas grade student programs more efficiently.
however novice students do not seem to know how to effectively make use of suggested repairs to correct their programs.
overall it seems feasible to use apr tools for the purpose of tutoring introductory programming given that repairs can be generated more o f ten than not a f ter tailoring apr tools and further improvement seems possible by addressing a few common reasons for repair failure.
to facilitate further research we share our dataset containing real student programs our toolchain implementing the partial repair policy strategy and our user study materials in the following url h t tps github.com jyi itsp.
a summary description is available in section .table characteristics of our dataset lab prog topic lab simple expressions printf scanf lab conditionals lab loops nested loops lab integer arrays lab character arrays strings and functions lab multi dimensional arrays matrices lab recursion lab pointers lab algorithms sorting permutations puzzles lab structures user de f ined data types automated program repair we perform a feasibility study with the following four state of theart apr tools genprog ae prophet and angelix .
t hese four tools similar to the majority of apr tools are testdriven meaning that a modi f ied program p0is considered repaired ifp0passes all tests in the provided test suite.
genprog repeatedly modi f ies the program using genetic programming until it f inds a repair or the time budget is exhausted.
in contrast to genprog where the program is modi f ied in a stochastic fashion the program is modi f ied differently at each run of the tool ae modi f ies the program in a deterministic way by applying mutation operators to the program.
prophet f irst searches for a transformation schema that can be used to repair the program and in the next step it instantiates the transformation schema to generate a repair.
in the second step of schema instantiation prophet uses a repair model learned from successful human patches to prioritize the instantiation similar to human patches.
angelix f irst searches for a set of angelic values for potentially buggy expressions e when these angelic values substitute e all tests are passed.
in the next step angelix synthesizes patch expressions that return the angelic values found in the f irst step.
t hese four apr tools while sharing the goal of generating repairs that pass all tests internally use different repair algorithms and repair operators.
we include these different apr tools in our study to gain holistic understanding of the feasibility of using apr tools for programming tutoring.
dataset t he dataset on which we perform and report our analysis was obtained from an introductory c programming cs course offered at indian institute of technology kanpur iit k by the third author.
t he programs were collected using prutor a system that stores intermediate versions of programs in addition to the f inal submissions.
t his course was credited by f irst year undergraduate students.
one of the major grading component was weekly programming assignments termed lab .
t he assignments were designed around a speci f ic topic every week as described in table so as to test the concepts learned so far.
t he labs were conducted in an environment where we recorded the sequence of submissions made by students towards the goal of passing as many pre de f ined test cases as possible.
multiple a t tempts were allowed with only the last submission being graded.
for each of these labs table t he result of our initial experiment in which the existing apr tools are used out of the box.
t he overall repair rate is .
lab programs fixed repair rate time lab s lab s lab s lab s lab s lab s lab s lab s lab s lab s total s we pick a random sample of pb pc program pairs as our dataset where pbis a version of student program which fails on one or more test cases and pcis a later version of the a t tempt by the same student which passes all the provided test cases.
we exclude from our dataset the instances of pbfailed to be compiled.
t he second column of table shows the number of programs for each lab we include in our dataset.
initial feasibility study how o f ten can the state of the art apr tools f ix incorrect student programs?
a high repair rate of apr is a prerequisite to using apr tools for feedback generation.
as the f irst step of our feasibility study we investigate how well four state of the art apr tools i.e.
genprog ae prophet and angelix f ix the incorrect student programs in our dataset.
for each incorrect program a repair is considered found if one of the four apr tools successfully generates a repair that is a generated repair passes all provided tests of the program.
we run the four apr tools in parallel until either a one of the apr tools successfully generates a repair or b all apr tools fail to generate a repair within a time limit minutes .
we use the default con f iguration of each apr tool with slight modi f ications for prophet to extend the search space of repair .
our experiment was performed on an intel xeon e5 .
ghz processor with ubuntu .
bit operating system and gb of memory.
.
results of initial experiment table shows the results of our initial experiment.
each column represents from le f t to right the lab for which the incorrect programs were submi t ted lab the number of incorrect programs submi t ted to the lab programs the number of incorrect programs in the lab that are f ixed by the apr tools we apply fixed repair rate i.e.
fixed programs in percentage repair rate and average time taken to successfully generate repairs time respectively.
in our experiments repairs are generated only in of the programs in our benchmark and repair rate is below across each individual lab.
meanwhile the average time taken when repairs are found is about minute.
our initial experimental result suggests that a low repair rate is a severe concern.
lab lab lab lab lab lab lab lab lab lab totalgroup high failure rate low failure ratefigure t his plot shows the repair rate of two different groups y axis across each individual lab x axis .
t he high failure rate group consists of the cases in which more than half of the tests fail the given program whereas the low failure rate group consists of the cases in which at least half of tests pass the given program.
repair rate is signi f icantly lower in the high failure rate group to which of the programs in our dataset belong.
.
reasons for low repair rate despite the fact that student programs are simpler than programs wri t ten by professional developers for which apr tools are developed the state of the art apr tools fail to generate repairs for the majority of the incorrect program in our benchmark.
our result suggests that f ixing short student programs is not easier than f ixing developer programs.
what makes automatically f ixing student programs difficult?
answering this question may help us adjust apr to the new challenge posed by student programs.
we observe in our dataset that the following two properties of student programs are likely to make automatically f ixing student programs difficult student programs o f ten fail in a majority of the tests and student programs o f ten require complex f ixes.
we describe them in more detail in the following sections.
.
.
high test failure rate.
student programs are o f ten signi f icantly incorrect and fail the majority of the tests.
in our dataset of the programs fail more than half of the available tests.
t his is in contrast to the fact that professional so f tware for which apr was successfully applied typically fails only a small portion of tests.
high test failure rate is likely to make automated program repair difficult.
figure compares the repair rate between the following two groups of our benchmark programs the high test failure group in which more than half of the tests fail the given program and the low test failure group where at least half of tests pass the given program.
while about half of the programs of the low failure rate group are successfully repaired the repair rate of the high failure rate group is only .
.
.
complex fixes.
t he majority of bugs reported to be successfully repaired by apr tools are cosmetically simple mostly restricted to one line changes of the given buggy program.
still the promise of apr is that it can save developers from manual search for a simple f ix in large so f tware.
to investigate the distribution between simple f ixes one hunk changes and complex f ixes multiple chunk changes in our dataset we compare each incorrect program in our dataset with its correct version.
recall that our dataset contains both an incorrect program and its correct version wri t ten by the same student.
in our dataset about half of the incorrect programs are f ixed by adding more than hunk of changes.
for these programs requiring complex f ixes the repair rate is shown to be lower than the repair rate for the rest of the programs .
tutoring programming our initial experiment reveals that repair rate of the current apr tools for novice student programs is prohibitively low.
does this imply that it is infeasible to use apr for intelligent programming tutoring ipt ?
or given that apr was originally not developed for ipt is it possible to tune up apr for the purpose of ipt?
one big difference between f ixing a bug and tutoring programming is in the different degrees of their interactivity with the users.
tutoring is a highly interactive process between a tutor and a student.
to complete a program a student takes multiple steps of actions and at each step the tutor provides feedback.
t he tutor offers a con f irmatory feedback if the student follows the right track toward a correct solution.
meanwhile if the student goes astray the tutor provides a hint for the student to get the student back on track.
in this highly interactive tutoring process the tutor does not simply show a correct program all at once.
instead the tutor provides for the student a series of feedback to help the student stay on track toward a correct solution.
t his behavior of human programming tutors is recorded in detail in .
intelligent tutoring systems expected to mimic human tutors should provide interactive feedback for the students where each feedback should help the students move to the next step toward a correct solution.
in contrast the ideal of apr is to synthesize a correct bug f ix at once without involving a long feedback loop with the developer.
given this difference between bug f ixing and programming tutoring we believe apr can be used for intelligent tutoring only a f ter it is tailored to the new needs of programming tutoring.
from bug fixing toward tutoring given the difference between apr and ipt described in the previous section the problem of apr and the problem of ipt can be described differently as follows.
de f inition .
automated program repair apr .
given a program pand its speci f ication s the following holds true initially re f lecting the fact that pis buggy p0s.
t he problem of apr is to generate an alternative program p0that satis f ies p0 s. de f inition .
intelligent programming tutoring ipt .
given a program pand its speci f ication swhere p0s the problem of ipt is to generate a series of alternative programs p0 p0 p0 k p0 k p0n p0 n 1that satis f ies the following through an iterative interaction with the student.
for all odd numbers k p0 kis an automatically generated program by the tutoring system and p0 k 1is a program constructed by the student using p0 kas a hint.
k n p0 k p0 k where p0 k p0 k 1denotes that program p0 k 1is closer to the speci f ication sthan p0 k one de f inition of will be described later .
k n p0 k0s p0 n s notice that in ipt the f inal correct version of the program p0 n is sought for through a series of feedback generation represented by p0 kfor all odd numbers k interspersed with student programming represented by p0 k 1for all odd numbers k .
we describe in the following how we tailor apr to ipt.
in particular we tailor test driven apr given that the majority of apr tools use a test driven approach.
in test driven apr a test suite is used as the speci f ication of the program.
t hat is given a test suite tand a buggy program pwhere pdoes not pass all tests in t i.e.
p0t test driven apr generates a repaired program p0satisfying p0 t which denotes p0passes all tests in t. .
tailoring repair policy we tailor itp described in def.
.
to test driven apr as follows.
first we replace in def.
.
speci f ication swith test suite t. t hus intermediate programs p0 kdo not pass all tests in t p0 k0t while they are gradually approaching the f inal version that passes all tests.
meanwhile the partial relation used in def.
.
can be naturally de f ined as follows.
we say p0 k p0 k 1if all tests passed in p0 kalso pass in p0 k .
similar to test driven development the progression of the student can be achieved by gradually passing more tests.
while the number of tests may not be a precise measure of student progression its practicality is high given that tests are widely used in evaluating student programs.
related but orthogonal issues are how to construct an effective test suite for the purpose of ipt and in which order each test should be satis f ied by the program for instance given multiple failing tests tfand an incomplete program which tests among tfshould be addressed f irst?
t hese orthogonal issues are not addressed in this initial feasibility study.
to implement progressive program construction p0 p0 p0n we modify the repair policy of apr as follows.
taking as input a student program p0 k we generate p0 k 1that satis f ies p0 k p0 k .
note that it is not required for p0 k 1to pass all tests unlike in the original apr.
t his different repair policy can be implemented in an apr tool in a straightforward way by generating a partial repair de f ined as follows.
de f inition .
partial repair .
given npositive tests where n andmnegative tests where m a partial repair p0satis f ies the following p0passes all npositive tests and p0passes at least one of mnegative tests.
in comparison we de f ine a complete repair generated in the original apr as follows.
de f inition .
complete repair .
given npositive tests where n and mnegative tests where m a complete repair p0 satis f ies the following p0passes all npositive tests and p0passes all mnegative tests.
t he expected usage of partial repairs is to encourage students to modify their own incorrect program by taking into account thepartial repair as a hint.
in fact a partial repair is specialized for the tests it addresses the tests that turn from negative to positive a f ter the partial repair the student needs to generalize the partial solution shown to him or her.
by comparing a generated partial repair with the incorrect program students can see when a particular test fails or passes which can help a student understand why his or her program fails the test addressed by the partial repair.
.
tailoring repair strategy partial repairs are generated as hints not as solutions.
typical hints partial repairs can provide are as follows.
control f low hints.
students can see that a test can pass by changing the control f low of the program which includes changing the direction of an if conditional skipping over a loop and exiting a loop at a different iteration than before.
data f low hints.
students can see that a test can pass by adding or deleting statements which affects the data f low of the program.
conditional data f low hints.
it is o f ten the case that the data f low of the program should be changed only under a certain circumstance.
in this case statement addition deletion can be guarded with a condition.
t he deleted added statements provide data f low hints while the guard conditions provide control f low hints.
note that a data f low hint can be viewed as a special case of a conditional data f low hint where a statement sis guarded with either false suggesting the deletion of s or true suggesting the addition of s .
to facilitate the use of partial repairs as hints we tailor the repair strategy of apr following algorithm .
our repair strategy searches for a control f low hint and a conditional data f low hint in parallel a data f low hint is the special case of a conditional data f low hint .
t his parallel use of tools is shown in line of the algorithm c.sc o.sc n.sc t.sc r.sc o.sc l.scf i.sc x.sc pb tp tn jj c.sc o.sc n.sc d.scd a.sc t.sc a.scf i.sc x.sc pb tp tn where pb tp and tnrepresent an input buggy program positive tests passing tests and negative tests failing tests respectively.
function c.sc o.sc n.sc t.sc r.sc o.sc l.scf i.sc x.sc and c.sc o.sc n.sc d.scd a.sc t.sc a.scf i.sc x.sc search for a partial repair that can be used as a control f low hint and a conditional data f low hint respectively.
parallel search for a partial repair stops when either a repair is found or the time budget is exhausted.
in function c.sc o.sc n.sc t.sc r.sc o.sc l.scf i.sc x.sc by which a control f low hint is searched for we invoke in parallel two apr tools angelix and prophet both of which have repair operators that can modify the conditional expressions of the if loop statements.
we restrict the repair space only to conditional expression changes when looking for a control f low hint.
meanwhile in function c.sc o.sc n.sc d.scd a.sc t.sc a.scf i.sc x.sc by which a conditional data f low hint is searched for we use the following two step repair process.
in the f irst step we modify the data f low of the program by adding deleting modifying statements such that one of the negative tests becomes positive a f ter the modi f ication.
at this step we do not preserve positive tests that is the modi f ied program piin line may fail some all of positive tests.
however in the second step we re f ine pisuch that the re f ined program pr obtained in either line or passes all positive tests.
more speci f ically our re f inement process takes place as follows.
given a statement sthat is added or deleted in the f irst step we transform sinto if true algorithm partial repair generation using our repair strategy input buggy program pb test suite t output partially repaired program pr .runpbwith tto f ind out positive tests tpand negative tests tn.
tp tn r.sc u.sc n.sc pb t .parallel call.
successful termination of one function termination with a non null value kills the remaining function.
pr c.sc o.sc n.sc t.sc r.sc o.sc l.scf i.sc x.sc pb tp tn jj c.sc o.sc n.sc d.scd a.sc t.sc a.scf i.sc x.sc pb tp tn .function c.sc o.sc n.sc t.sc r.sc o.sc l.scf i.sc x.sc searches for a partial repair changing the control f low of the program using angelix and prophet.
if a partial repair is not found null is returned.
function c.sc o.sc n.sc t.sc r.sc o.sc l.scf i.sc x.sc pb tp tn .set the repair con f iguration such that a partial repair changing the control f low of the program is searched for.
c fcontrol partialg return r.sc u.sc n.sca n.sc g.sc e.sc l.sc i.sc x.sc c pb tp tn jj r.sc u.sc n.scp r.sc o.sc p.sc h.sc e.sc t.sc c pb tp tn end function .function c.sc o.sc n.sc d.scd a.sc t.sc a.scf i.sc x.sc searches for a partial repair changing the data f low and or the control f low of the program.
if a partial repair is not found null is returned.
function c.sc o.sc n.sc d.scd a.sc t.sc a.scf i.sc x.sc pb tp tn .set the repair con f iguration such that a partial repair changing the data f low of the program is searched for.
c fdata partialg .search for a program pithat makes at least one of the tests in tnpass while ignoring tp.tirepresents a set of tests in tnthat pass with pi.
pi ti r.sc u.sc n.scg e.sc n.scp r.sc o.sc g.sc c pb tn jj r.sc u.sc n.scae c pb tn jj r.sc u.sc n.sca n.sc g.sc e.sc l.sc i.sc x.sc c pb tn jj r.sc u.sc n.scp r.sc o.sc p.sc h.sc e.sc t.sc c pb tn .ifpiis found i.e.
pi!
null re f ine pisuch that not only ti but also all the tests in tppass.
t his is achieved by looking for a complete not partial repair that passes all tests in tp ti ifpi!
null then c fcontrol completeg pr r u.sc n.sca n.sc g.sc e.sc l.sc i.sc x.sc c pi tp ti end if .if re f inement with angelix fails i.e.
pr null try with prophet.
ifpi!
null pr null then c fcontrol completeg pr r u.sc n.scp r.sc o.sc p.sc h.sc e.sc t.sc c pi tp ti end if return pr end function fsg or if false fsg respectively.
similarly if a statement sis modi f ied into another statement s0in the f irst step we transform s into if true fs gelsefsg .
t his transformation takes place internally inside the apr tools we modify for this purpose.
t he re f ined program pris obtained by replacing the tautological conditions true or false guarding the added deleted modi f ied statement with different expressions with which prpasses all positive tests and the negative tests addressed in the f irst step.
we invoke multiple apr tools in parallel in the two step repair process of f inding a conditional data f low hint.
in the f irst step we invoke four tools that is genprog ae prophet and angelix for prophet and angelix we turn off the options that allow conditional expression changes .
in the second step where guards are modi f ied we invokeonly prophet and angelix since genprog and ae do not support expression level modi f ications.
.
incremental repair our overall repair algorithm optionally allows incremental repair that is generating a series of partial repairs incrementally.
more speci f ically a new partial repair pi 1is generated based on the previous partial repair pigenerated at the i th iteration.
t he number of passing tests grows as the iteration proceeds and the tests passed bypiare also passed by pi .
t he iteration proceeds until either there is no remaining negative failing test or a partial repair is not found.
a repair obtained through the incremental repair approach can be useful for graders to whom showing as many changes as possible can provide hints about why the student program is wrong.
evaluation we evaluate the feasibility of using our partial repair algorithm for introductory programming assignments.
t he following are our research questions.
rq1 how o f ten are repairs generated when our partial repair algorithm is employed in addition to the complete repair algorithm of the existing apr tools?
a high repair rate is a prerequisite for using apr for introductory programming assignments.
t he current state of the art apr tools fail to generate repairs more o f ten than not as shown in section .
how signi f icantly does a new repair strategy allowing both complete and partial repairs improve repair rate?
rq2 when are repairs not generated even a f ter employing our partial repair algorithm?
if there are common reasons for those cases of repair failure they should be addressed in future tools.
rq3 do tool generated partial repairs help students in f inding a solution more efficiently than when repairs are not shown?
rq4 similarly do tool generated repairs help graders in grading student programs more efficiently than when repairs are not shown?
to investigate our research questions we conduct a tool experiment to address rq1 regarding repair rate repair failure analysis to address rq2 and user study to address rq3 and rq4 .
.
tool experiment we developed a tool that implements our partial repair algorithm on top of the same four existing apr tools as used in our initial experiment.
we apply our tool to the same dataset as used in our initial experiment modulo the incorrect programs for which complete repairs are already generated in the initial experiment.
recall that the purpose of this tool experiment is to investigate how signi f icantly a new repair strategy allowing both complete and partial repairs improves repair rate.
t he experiment was performed on the same environment as used for the initial experiment.
table shows the results of our tool experiment.
as compared to our initial experiment that does not allow partial repairs the overall repair rate increases from to showing about of improvement.
repair rate increases signi f icantly across all labs as shown in figure .
meanwhile the average successful repair time stays as low as seconds.table t he result of an experiment in which partial repairs are sought for in case a complete repair is not found out.
t he overall repair rate is about .
lab programs fixed repair rate time lab s lab s lab s lab s lab s lab s lab s lab s lab s lab s total s lab lab lab lab lab lab lab lab lab lab totalpolicy complete partial complete figure t his plot shows the repair rate in percentage y axis across each individual lab x axis .
t he complete represents the cases in which only complete repairs are counted whereas the partial complete represents the cases in which partial repairs are also allowed in case a complete repair does not exist.
.
repair failure analysis despite the increase of repair rate a f ter allowing partial repairs neither complete repair nor partial repair was generated in of our subject programs.
we compare these of programs with their correct versions to look for common reasons for repair failure.
speci f ically for each defect represented by the buggy version pb and the correct version pc we obtain the ast differences between pband pcusing gumtree an ast differencing tool.
we f irst perform manual inspection of the ast differences to derive a set of common characteristics observed in the differences between pband pc.
t hen we detect other such instances in our dataset using our extension of gumtree where we encode the ast difference pa t terns corresponding to the common characteristics we identi f ied.
we repeat this process until all programs for which repairs are not generated are covered.
note that some programs are labeled with multiple characteristics in this process.table t his table shows the distribution of the difference characteristics of the two programs a buggy program pb and its correct version pc for which neither complete nor partial repair is generated by the apr tools.
pc pb instances portion string array missing function complex control unsupported others empty implementation wrong parameters wrong usage table shows our analysis result.
t he f irst column categorizes the characteristics of the differences between the buggy program pb and its corrected version pc .
t he second and third column show the number of instances and portion of each category respectively by which the table is sorted.
t he following describes each category for pc pbwhich we represent as string t his corresponds to the case where involves changing the string constants used in the program such as adding a missing space or a new line.
it is observed that this category takes the most number of instances of repair failure .
array t his corresponds to the case where involves changes in arrays that include array index changes array size changes adding deleting array access expressions and using array lengths in the program.
missing function t his corresponds to the case where involves adding a function call.
complex control t his corresponds to the case where involves complex control f low changes that include control f low changes in a nested loop and control f low changes in multiple conditionals.
while angelix and prophet can change conditional expressions they do not exhaustively consider all possible control f low changes.
unsupported t his corresponds to the case where pcrequires expressions that cannot be synthesized by the current apr tools such as the expressions involving the modular operator and nonlinear expressions.
empty implementation t his corresponds to the case where the main function of pbis empty or contains only a return statement.
we do not label other characteristics for the programs belonging to this category.
wrong parameters t his corresponds to the case where involves changing multiple parameters of a function call expression.
while angelix can change multiple expressions it does not exhaustively consider all possible combinations.
wrong usage t his corresponds to the case where students use language constructs in a semantically wrong way.
t his includes mistakenly adding a semicolon before a for loop body i.e.
using for ... f...ginstead of for ... f...g using scanf ... x instead of scanf ... x where xrepresents a variable using xwhen x 1is required using x when xis required and using xwhen xis required.
others t his covers the rest of the characteristics.
a programming experience b skill level for c programming figure background of teaching assistants t he fact that the portions of the top two categories string and array take more than suggests that it would be most cost effective to strengthen repair operators that can manipulate strings and arrays in future apr tools.
.
user study we perform a user study with novice students and graders to see whether automatically generated feedback can help students solve the problem on their own.
whether automatically generated feedback can help teaching assistants tas grade submissions efficiently faster grading and effectively only small variation in the marks for similar submissions.
for the student study we selected problems for which we had buggy submissions and the partial repairs generated by our algorithm.
we divided the students into the experimental group for whom the generated repairs are presented and the control group for whom the repairs are not presented.
repairs are presented in the form of a comment around the repaired lines of the buggy submission.
we asked each student to f ix one randomly chosen buggy submission.
t he study was unannounced that is the task was provided as a bonus question along with other regular assignment problems.
t he weight of the f ix task was kept low so that it does not impact the overall grade of the students in the course.
t he participation was voluntary and in total students submi t ted their completed programs students in the without repair group and students in the with repair group out of the students crediting the course.
similarly to estimate the impact of repairs on the grading task we did a study on tas.
tas volunteered for this task out of which f illed in the pre study survey and the post study survey.
figure summarizes the background of these tas which we collected using a pre study survey.
for the study we randomly collected buggy submissions from the subset of our dataset for which our algorithm successfully generates either complete or partial repairs.
t hese buggy submissions correspond to different programming problems.
we asked the tas to grade these submissions based on how close they are to a correct program by f iguring out the bugs and their corresponding repairs.
t he tas were divided into two groups.
t he f irst group was given tasks set a without repair and tasks set b with repairs while the second group was conversely given set a with repairs and set b without repairs.
we s s s s s problemid050010001500200025003000time t akenrepair not provided repair providedfigure time taken by students for bug f ix task p p p p p p p p problemid0100200300400500600700800time t akenrepair not provided repair provided figure time taken by tas for grading task table t he answer frequency from tas for the question how do you categorize the errors of the program based on the suggested repair?
multiple answers are allowed.
category frequency conditionals loops missing character string modi f ications array accesses user de f ined functions missing values in the output library functions others missing whitespace in the output floating point operations compared the time taken and marks assigned by the tas for these tasks.
t he reference marks for these submissions were provided by the instructor who did not participate in the study and did not have access to the repairs.
with tas we also conducted a post study survey to understand the experience of tas with repairs.
figure and figure respectively show the distribution of time taken by the students for the solving task and time taken by the tas for the grading task.
both the f igures are box plots where x axis shows the problem ids and y axis shows the time in seconds.
each box or rectangle represents the f irst and third quartiles withtable t he answer frequency from tas for the question what kind of modi f ications are necessary in the suggested repair to obtain a correct solution?
multiple answers are allowed.
description frequency fix condition for conditionals or loops fix operators insert delete character e.g.
forma t ting the output whitespaces fix constants fix array indices others table analysis of ta grading time.
yes tas correspond to those who answered in the post study survey that repairs were useful while no tas answered conversely.
grading all tas yes tas no tas time without with without with without with sec repair repair repair repair repair repair average .
.
.
.
.
.
median .
.
.
.
.
.
stdev .
.
.
.
.
.
a horizontal line inside indicating the median value.
t he ends of the vertical lines or whiskers on either side of the box represent the minimum and maximum time taken.
from these f igures we can infer that repairs affect novice students and tas differently.
while the problem solving time of the students tends to increase in the group where repairs are shown the grading time of the tas tends to decrease when repairs are shown.
t hat is when repairs are shown the students tend to solve the problems more slowly while the tas tend to grade the problems more quickly.
we conjecture that these opposite trends between novice students and tas are due to their different levels of expertise and the format of feedback.
in our post study survey we asked tas how do you categorize the errors of the program based on the suggested repair?
and what kind of modi f ications are necessary in the suggested repair to obtain a correct solution?
t he results are shown in table and .
in the f irst question the majority of tas identi f ied the errors in loops conditionals see table while in the second question the most number of answers were given to the changes in loops conditionals see table .
t hese results suggest that tas are capable of generalizing suggested repairs that are overly specialized to the tests.
it is likely that this generalization capability of tas helps them f inish the grading tasks more efficiently.
however novice students do not seem to know how to effectively make use of suggested repairs unlike tas.
table shows a closer look at the grading performance of tas.
t he f irst column all tas shows the performance statistics for all tas both without repair and with repair and the second column yes tas and the third column no tas show the statistics for those who said in the post study survey that the suggested repairs are useful and not useful respectively.
half of the tas answered the repairs are useful the yes group and the rest of the half answered s 1s 2s 3s 4s 5s 6s 7s 8s 9s10s11s12s13s14s15 problemid05101520marks awardedinstructor s grade repair not provided repair providedfigure distribution of marks assigned by tas not useful the no group .
given that the average grading time is smaller in the yes group high performers tend to feel more strongly that the suggested repairs are useful.
in both groups the average grading time decreases when repairs are shown.
also notably the standard deviation decreases in both groups indicating that the gap between high performers and low performers becomes narrower when repairs are shown.
figure shows the marks awarded by the tas for randomly picked submissions out of tasks.
t he reference marks for these submissions were provided by the instructor who did not participate in the study and also did not look at the generated repairs used in the study.
in the graph the x axis and y axis show respectively the problem ids and marks awarded between and .
t he overall trends are similar among the group for whom repairs are presented experimental group the group for whom repairs are not presented control group and the independent instructor.
in a majority of the cases the absolute difference between the experimental group and the control group is not much for cases and for cases.
threats to validity in our tool experiments one of the apr tools genprog uses a random algorithm genetic programming which can produce different results for each run.
to mitigate this threat we applied the same seed to genprog in our initial experiment section and the second tool experiment section .
.
also the fact that the rest of the apr tools employed for our experiments ae prophet and angelix use deterministic repair algorithm further mitigates this threat.
our repair failure analysis section .
may be restricted by the difference categories pc pb to which our analysis tool categorizes.
to mitigate this threat we manually inspected the differences and added new categories when the previously used categories were not sufficient.
our dataset while collected from the actual students taking an introductory programming course may not be representative of all student programs.
similarly in our user study participating students and tas may not represent all novice students and graders.
in terms of the programming language ourresults are con f ined to c programs for which apr has been developed most actively.
however our proposed partial repair policy can be applied to other programming languages.
in our user study the experimental se t ting where the participating students are given buggy programs wri t ten by other students is not identical with the actual usage scenario where the students f ix the mistakes they made.
we leave further investigation as future work.
related work many different techniques have been applied to automated feedback generation and each technique has different advantages and disadvantages.
program equivalence checking is used in where behavioral difference between a student program and its reference program differences in input output relations is reported to the student as feedback.
since program equivalence checking is generally undecidable performs equivalence checking in a constrained manner that is when comparing a student program pswith its reference program pr prshould be structurally similar to ps.
such prcan be provided either manually the instructor prepares pr or semi automatically the instructor selects pr from previously submi t ted correct student programs which can be automatically clustered according to their structures .
since this approach based on program equivalence uses a reference program as a speci f ication it can generate feedback even when there is no failing test.
meanwhile the fact that the instructor should validate the correctness of the reference program poses not only a burden to the instructor but also a risk of generating false feedback in the presence of a validation mistake.
a model based approach is used in where an instructorgiven error model describing possible student errors de f ines how the given incorrect program is allowed to be modi f ied.
to search for a correct modi f ication efficiently a program synthesis technique is employed.
while an error model can capture some common student errors and hence can guide feedback generation it also restricts the search for feedback only to the common errors described in the error model.
t he fact that an error model should be prepared beforehand by the instructor is another disadvantage.
static analysis is used in where dependence graphs extracted from a student program psand the reference program pr are compared to each other in order to identify a statement in ps that can potentially cause semantic difference from pr.
as usual in conservative static analysis these approaches can guarantee not to miss a semantic error while as a f lip side an error can be falsely reported.
a learning based approach is used in refazer and deepfix .
refazer learns programs transformation rules from the past program changes similar to where systematic edits similar but not identical changes made in many program locations are learned from the past program changes.
meanwhile deepfix applies deep learning to the correction of syntactic errors that cause compilation failure.
while learning based approaches can complement the existing approaches when the previous submissions of a programming assignment are available their applicability and effectiveness are restricted by the availability and the quality of the previous submissions.
a data driven approach is used in where in order to generate feedback not only the reference program but also a chainof intermediate programs leading to the reference program are exploited.
t he use of intermediate programs makes it more amenable to generate a next step hint since the buggy student program is likely to be closer to one of the intermediate programs than to the reference program.
however the hint space consisting of the reference programs and their intermediate programs is more restricted than the one of apr.
automated program repair apr is fully automatic unlike some approaches requiring additional input from the instructor such as an error model and multiple reference programs from multiple clusters.
in apr it is sufficient to provide a student program and a test suite.
although generated repairs can be imperfect and overly specialized to the provided test suite this issue has been gradually addressed in recent work of apr .
meanwhile fault localization can also be used to provide hints to students as suggested in .
in fact apr also performs fault localization in the sense that apr performs fault localization before synthesizing a f ix.
furthermore students can also see how a previously failing test passes a f ter f ix which provides an additional hint.
t here have been several user studies in the area of program debugging and repair .
unlike these user studies concerning the productivity of professional developers our study is conducted with different target of users that is novice students and graders.
overall our study provides holistic information about the feasibility of using apr for introductory programming assignments including how o f ten repairs are generated why repairs are failed to be generated and how useful generated repairs are for students and graders.
conclusion in this paper we have explored the possibility of using apr as a feedback generation engine of intelligent tutoring systems for introductory programming.
we have performed a feasibility study with four state of the art apr tools genprog ae prophet and angelix and real student programs collected from a course on introductory programming.
although out of the box application of apr tools seems infeasible due to the low repair rate we have shown that repair rate can be boosted by tailoring the repair policy and strategy of apr to the needs of intelligent tutoring.
most notably adopting a partial repair policy akin to the next step hint generation advocated in the education f ield seems effective in terms of improving feedback generation rate.
we have also shown through a repair failure analysis that repair failures are o f ten caused by a few common reasons.
further improvement of feedback generation rate is expected by strengthening repair operators manipulating strings and arrays in future apr tools.
lastly we have shown our user study results performed with novice students and graders tas .
in contrast to the tas who use the suggested repairs as hints to efficiently complete the grading tasks the novice students do not seem to know how to effectively make efficient use of suggested repairs to correct their programs.
we leave as future work a study of effective post processing of repairs to transform them to hints more comprehensible to novice students.