detecting missing information in bug descriptions oscar chaparro1 jing lu1 fiorella zampetti2 laura moreno3 massimiliano di penta2 andrian marcus1 gabriele bavota4 vincent ng1 1the university of texas at dallas usa 2university of sannio italy 3colorado state university usa 4universit della svizzera italiana switzerland abstract bug reports document unexpected software behaviors experienced by users.
to be effective they should allow bug triagers to easily understand and reproduce the potential reported bugs by clearly describing the observed behavior ob the steps to reproduce s2r and the expected behavior eb .
unfortunately while considered extremely useful reporters often miss such pieces of information in bug reports and to date there is no effective way to automatically check and enforce their presence.
we manually analyzed nearly 3k bug reports to understand to what extent ob eb and s2r are reported in bug reports and what discourse patterns reporters use to describe such information.
we found that i while most reports contain ob i.e.
.
only .
and .
explicitly describe eb and s2r respectively and ii reporters recurrently use discourse patterns to describe such content.
based on these findings we designed and evaluated an automated approach to detect the absence or presence of eb and s2r in bug descriptions.
with its best setting our approach is able to detect missing eb s2r with .
.
average precision and .
average recall.
our approach intends to improve bug descriptions quality by alerting reporters about missing eb and s2r at reporting time.
ccs concepts general and reference empirical studies software and its engineering maintaining software keywords bug descriptions discourse automated discourse identification acm reference format oscar chaparro jing lu fiorella zampetti laura moreno massimiliano di penta andrian marcus gabriele bavota and vincent ng.
.
detecting missing information in bug descriptions.
in proceedings of esec fse paderborn germany september pages.
introduction bug reports are meant to collect relevant information about the bugs that users encounter when using software.
the information provided in such reports is intended to help developers diagnose permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany association for computing machinery.
acm isbn .
.
.
.
remove software bugs .
while much of the information in bug reports is structured the main content of a bug report is unstructured that is expressed in natural language .
unstructured natural language content produced by reporters includes the description of software s mis behavior i.e.
observed behavior or ob the steps to reproduce the mis behavior i.e.
steps to reproduce or s2r and the software s expected behavior eb .
previous research indicates these three pieces of information to be highly important for developers when triaging and fixing bugs .
while considered extremely useful reporters do not always include ob eb and s2r in their bug reports.
recently developers from more than one thousand open source projects signed and sent a petition to github remarking that ... issues are often filed missing crucial information like reproduction steps ... .
in addition researchers found that textual descriptions in bug reports are often incomplete superficial ambiguous or complex to follow .
the lack of important information in bug reports is one of the main reasons for non reproduced bugs unfixed bugs and additional bug triage effort as developers have to spend more time and effort understanding bug descriptions or asking for clarifications and additional information .
low quality bug reports are also likely to gain low attention by developers .
as indicated by developers absent and wrong information in bug reports is the predominant cause for delays on bug fixing .
one of the main reasons for incomplete information in bug descriptions is the inadequate tool support for bug reporting .
in the aforementioned github petition developers called for improvements to github s technology to ensure that essential information is reported by users.
this problem extends to other bug tracking systems.
most of these systems capture unstructured natural language bug descriptions through web forms without any content verification or enforcement.
some bug tracking systems e.g.
bugzilla in the mozilla firefox project provide semi structured reporting of natural language information using predefined text templates that explicitly ask for ob eb and s2r.
such a solution is insufficient to address the problem as it does not guarantee that reporters will provide this information as expected.
very little research has been done on detecting the presence absence of ob eb or s2r in bug descriptions.
most of the approaches proposed in the literature are meant to detect other types of information such as source code snippets or stack traces .
the few that detect ob eb or s2r rely on keyword matching such as observed behavior to detect ob or basic heuristics such as enumerations itemizations identification to detect s2r.
unfortunately while simple and straightforward these approaches are suboptimal in accurately detecting such content as they lead to an excessive number of undetected cases i.e.
false negatives .
the goal of our research is two fold i to understand to what extent andhow reporters describe ob eb s2r in bug descriptions esec fse september paderborn germanyo.
chaparro j. lu f. zampetti l. moreno m. di penta a. marcus g. bavota and v. ng and ii to develop and validate an approach to automatically identify if bug reports miss such contents.
our conjecture is that reporters use a limited vocabulary and a well defined set of discourse patterns when describing ob eb or s2r .
if true then we can automatically detect the presence or absence of these patterns with high accuracy.
the converse situation would mean that the automatic analysis of unstructured bug descriptions would be impractical.
to the best of our knowledge no existing research validates or invalidates our conjecture.
the work on bug description analysis has mainly focused on investigating linguistic properties of bug report titles identifying frequently asked questions investigating unwanted behavior types and studying the structure of bug reports .
other work has focused on identifying linguistic patterns in other software engineering sources such as development e mails or app reviews .
little is known about the discourse that reporters use to describe software bugs.
we manually analyzed bug reports from nine systems and found that while most reports contain ob i.e.
.
only .
and .
explicitly describe eb and s2r respectively.
in addition to verify our conjecture we analyzed sentences and paragraphs of a subset of bug report descriptions by using an open coding procedure .
we found that reporters recurrently use discourse patterns to describe ob eb and s2r which means that such contents can be automatically detected.
based on our findings we developed an automated approach for detecting missing eb and s2r in bug report descriptions as they are more likely to be missing called demibud detecting missing information in bugdescriptions.
we developed three versions of demibud based on regular expressions heuristics and natural language processing nlp and machine learning ml .
we empirically evaluated demibud s accuracy in detecting missing eb s2r in a subset of bug descriptions.
the evaluation indicates that demibud with its best setting detects missing eb s2r with .
.
average precision and .
average recall.
demibud can be used either to alert submitters while writing bug reports or as a quality assessment tool for triagers so that they can contact the reporters right away to solicit the missing information while the facts are still fresh in memory.
demibud can also be used to augment existing bug report quality models .
in summary the major contributions of our research are a set of patterns that capture the discourse followed by reporters when describing ob eb and s2r in bug reports an automated approach demibud to detect the absence presence of eb and s2r in bug reports a dataset of labeled bug reports that can be used for replication purposes and future research .
the discourse of bug descriptions the first goal of our research is to understand how essential information about bugs is reported.
to this end we identify the discourse patterns that reporters use to describe ob eb and s2r in bug reports.
discourse patterns are rules that capture the syntax and semantics of the text.
figures and are examples of ob eb and s2r discourse patterns respectively.
to address such a goal we answer the following research questions rqs rq to what extent bug reports contain ob eb and s2r?
this rq investigates if reporters tend to include ob eb and s2r.
this motivates the need for automated detection of such information.rq do bug reporters describe ob eb and s2r in bug descriptions by using a well defined set of discourse patterns?
this rq aims at understanding the discourse followed by reporters to describe ob eb and s2r.
the presence of discourse patterns is essential to automatically identify such contents.
to answer these questions we performed a qualitative discourse analysis of a large set of bug reports from nine software projects based on open coding .
before describing the coding process the coding criteria and the coding results we introduce a set of assumptions and definitions useful for our study.
.
definitions we focus on bug reports i.e.
issues that describe potential software bugs or defects.
we do not code issues describing feature requests enhancements questions ortasks .
in addition our pattern discovery task focuses on the description of bug reports i.e.
bug descriptions and not on titles.
the reason for this is that titles rarely describe completely ob eb or s2r e.g.
they can simply be noun phrases or words referring to the reports topics .
we focus our attention on three types of information in bug descriptions namely observed behavior ob expected behavior eb and steps to reproduce s2r .
we expect to find a set of discourse patterns for the sentences and paragraphs i.e.
the units of discourse of the bug descriptions.
a discourse pattern is a rule that structures a sentence or a paragraph to convey either ob eb or s2r.
this means that a pattern captures the syntax and semantics of sentences and paragraphs.
.
issue sampling we collected a sample set of issues from nine software projects of different types and domains.
these projects rely on different issue or bug trackers to capture potential software bugs found by users.
eclipse firefox httpd and libreoffice use bugzilla as issue tracker hibernate and openmrs use jira docker and wordpress android a.k.a.
wordpress a use github s issues and facebook uses a proprietary issue tracking system.
these projects except for facebook are open source.
to create our issue sample set for the coding task we rely on the issue data set collected by davies et al.
for eclipse facebook firefox and httpd.
this data set is composed of .6k issues randomly sampled from their corresponding issue trackers.
from this data set and the online issue repositories of the remaining projects we performed random sampling making sure to exclude issues that were not bug reports e.g.
feature requests by manually inspecting the type of issue and its comments.
in total we collected bug reports i.e.
reports per project on average including the ones collected by davies et al.
.
from these we used reports for discourse pattern discovery and the remaining ones i.e.
for validation purposes.
we refer to the former data set as the discourse bug reports and to the latter as the validation bug reports .
.
coding procedure we present the coding procedure that we followed to address both rq 1andrq .
while we coded the presence of ob eb and s2r in thediscourse bug reports andvalidation bug reports we only used thediscourse bug reports to infer the discourse patterns.
.
.
discourse pattern coding.
five coders four authors of this paper and one additional coder conducted the sentence and paragraph coding task for the discourse bug reports .
in order to define 397detecting missing information in bug descriptions esec fse september paderborn germany a starting coding framework one of the coders conducted a pilot study on a subset of issues from davies et al.
s issues that were not used in discourse bug reports .
the goal of this task was to analyze the issue descriptions identify the sentences or paragraphs that corresponded to ob eb and s2r and infer the discourse patterns from them.
this task resulted in twelve preliminary discourse patterns with specific textual examples from the issues a set of textual characteristics of the issues and the initial coding criteria.
once the pilot study was completed this person trained the rest of the coders in a minute session that involved discussing the results and some ambiguous sentences.
the discourse bug reports were evenly and randomly distributed among coders to ensure that each coder received a subset of reports for each of the nine projects.
each person coded reports except for one person who coded i.e.
reports per system per person on average .
for each bug report the coders analyzed the bug description and marked each sentence or paragraph as ob eb or s2r.
a sentence paragraph can fall into more than one of these categories at the same time.
then the coders inferred a discourse pattern from each marked sentence paragraph and assigned a code to it.
a code is a label that uniquely identifies a discourse pattern.
note that it is possible to infer more than one pattern from a sentence paragraph.
a catalog of inferred patterns was shared among coders via an online spreadsheet.
in this way all coders were aware of the patterns inferred by each coder and were able to reuse existing patterns or add new ones to the catalog.
when one of the coders identified a new pattern it was included in the catalog and the other coders were notified.
each new pattern was verified by all the coders and disagreements were solved via open discussion.
for each new pattern the existing catalog was inspected for similar patterns and when appropriate with unanimous agreement similar patterns were merged into a new one i.e.
a more general pattern and the existing labels were updated accordingly.
this process was fully iterative and included constant refinement of the pattern catalog as well as discussion of ambiguous cases.
every decision taken during the pattern extraction was representative of the opinion of all coders.
to minimize subjectivity we recruited four additional coders one cs masters student two developers and one business analyst and asked them to code the same reports coded by the first group of coders.
in a minute session one member of the first group trained the new coders on the coding procedure and criteria see section .
.
.
we randomly distributed the reports among the new coders ensuring that each one coded a subset of issues coded by each of the original coders.
the task of the additional coders was to mark the sentences and paragraphs that corresponded to ob eb and s2r.
this time the pattern inference was not part of the task as the iterative and collaborative nature of the pattern coding procedure already aimed at minimizing subjectivity.
in the end each issue from the discourse bug reports was coded by two distinct coders.
the inter coder agreement is discussed in section .
.
.
.
.
validation set coding.
for the remaining bug reports from our initial sample i.e.
the validation bug reports all nine coders were requested to follow the same coding process without pattern inference.
each report was coded by two different coders i.e.
on average issues were assigned to each pair of coders.
the bugswere randomly distributed so that each pair of coders received a subset of issues from each system.
again the coders marked the sentences and paragraphs that corresponded to ob eb and s2r i.e.
no pattern inference this time .
.
.
coding criteria.
we summarize the most important criteria followed by the coders full list in our replication package .
the coders were provided with examples of each criterion.
the coding focused only on natural language nl content written by the reporters as opposed to code snippets stack traces or logs.
however the nl referencing this information was coded.
in addition only explicit mentions of ob eb s2r were labeled.
note that it is possible to infer eb from ob descriptions as the former is usually the opposite of the latter.
such cases were not labeled.
regarding ob uninformative sentences such as the system does not work are insufficient to be considered ob.
there must be a clear description of the observed mis behavior of the software.
code explanations and root causes are not considered ob.
regarding eb solutions or recommendations to solve the bugs are not considered eb.
in some cases imperative sentences such as make targets not automatically filled... may be considered eb according to the context of the bug.
sometimes however these suggest tasks instead of eb.
regarding s2r one or more sentences i.e.
a sentence or a paragraph can describe steps to reproduce.
conditional sentences such as when i click on apache.exe it returns an error like this may be s2r if they provide enough details about how to reproduce the bug.
finally s2r paragraphs may also contain ob and eb sentences.
.
coding results and analysis before reporting and discussing the coding results we briefly summarize the inter coder agreement measurements.
.
.
inter coder agreement.
we analyzed the reliability of the coding process regarding the presence and absence of ob eb and s2r in bug descriptions.
remember that each bug description was coded by two coders.
we measured the observed agreement between coders as well as cohen s kappa k and krippendorff s alpha coefficients.
our analysis reveals high inter coder agreement levels.
coders agreed on the presence or absence of ob in of the cases avg.
k .
.
i.e.
fair agreement the presence or absence of eb in .
of the cases avg.
k .
.
i.e.
substantial agreement and the presence or absence of s2r in of the cases avg.
k .
.
i.e.
moderate agreement .
overall bug reports i.e.
.
had some type of disagreement solved by applying a third person scheme.
we distributed the conflicting reports among the nine coders in such a way that a third coder different from the original two coders would judge and solve the disagreements.
our analysis revealed that the main causes for disagreement were omissions mistakes and in the case of s2r misunderstandings as in several cases it was not clear if single conditional sentences were s2r or not.
.
.
rq presence of ob eb and s2r in bug reports.
table reveals that while most of the bug reports contain ob i.e.
.
only .
and .
of the reports explicitly describe eb and s2r respectively.
.
of the reports contain all three types of information ob eb and s2r .
these results indicate that essential 398esec fse september paderborn germanyo.
chaparro j. lu f. zampetti l. moreno m. di penta a. marcus g. bavota and v. ng information is missing in bug reports and motivate the need for automated detection of such information.
firefox is the system with the highest number of reports having eb and s2r i.e.
.
and .
and having all three types of information.
we attribute this result to the use of predefined templates explicitly asking for this information.
wordpress android has the lowest number of reports with ob.
we observed that screenshots rather than textual descriptions are commonly used in this project.
table number of bug reports containing ob eb and s2r.
project ob eb s2r total docker .
.
.
eclipse .
.
.
facebook .
.
.
firefox .
.
.
hibernate .
.
.
httpd .
.
.
libreoffice .
.
.
openmrs .
.
.
wordpress a .
.
.
total .
.
.
.
.
rq bug descriptions discourse.
our open coding approach resulted in a catalog of patterns that capture the discourse followed by reporters to describe ob eb and s2r.
most of the patterns are sentence level patterns and most of the paragraph level patterns correspond to s2r out of .
we summarize and discuss our pattern catalog and the discourse used for each type of information in bug descriptions the full catalog is available in our replication package .
ob discourse.
we observe that many patterns in our catalog correspond to ob i.e.
or .
see table .
out of these are sentence level patterns and five are paragraph level patterns.
software mis behavior is usually described following a negative discourse.
the six most frequent ob patterns correspond to negative textual content and account for .
of the discourse bug reports that contain ob.
three of these patterns are neg aux verb verb error and neg verb.
the first one is the most frequent one which corresponds to negative sentences containing auxiliary verbs see fig.
.
the second one corresponds to sentences with verb phrases containing error related nouns such as virtualbox gui gives this error from docker and the third one to sentences with non auxiliary negative verbs such as writer hangs on opening some doc docx or rtf files from libreoffice .
we also observed ob positive discourse.
for instance the cond pos pattern represents conditional sentences with positive predicates such as when the merge was completed i saw that the entries in the value coded column remained as they were originally from openmrs trunk .
the but pattern corresponds to sentences containing contrasting terms followed by affirmative predicates such as you require at least letters but our name delupe only consists of from facebook .
the top six most frequent positive discourse patterns account for of the reports describing ob.
overall the top six most frequent negative and the top six positive patterns appear in .
of the ob bug descriptions.
eb discourse.
reporters describe expected behavior using patterns i.e.
.
of our pattern catalog see table .
most ofpattern code s ob neg aux verb description negative sentence with auxiliary verbs rule definitions are not can not does not did not etc.
example from eclipse figure most common ob discourse pattern.
them i.e.
are sentence level patterns.
the most frequent pattern is should see fig.
which represents sentences using the modal terms should or shall .
these types of sentences appear in .
of the reports that describe eb.
other frequent discourse for describing eb is represented by the exp behavior instead of exp behavior expected and would be patterns.
the former exp behavior represents sentences with explicit eb labels such as expected results taken away the dialog box... from firefox instead of exp behavior accounts for sentences using instead of or similar terms such as when you try to schedule a saved draft it is published immediately instead of being scheduled for the future date you select from wordpress android the expected pattern represents sentences using noun phrases or conjugated verbs of the word expect such as the expectation was that objects would be loaded identically regardless of using scrollable results or using get result list from jpa.
from hibernate hhh and would be corresponds to sentences containing would be positive adjective phrases such as it d be optimal if the ux updated to reflect the actual updated follow state for given users blogs from wordpress android .
these five patterns appear in .
of the reports describing eb.
pattern code s eb should description sentence using the modals should or shall with no preceding predicates that use negative auxiliary verbs rule should shall not example should make an attempt to print the date in the language requested by the client from httpd figure most common eb discourse pattern.
s2r discourse.
the steps to reproduce discourse is represented by patterns see table of which are paragraph level patterns.
this means that reporters often use more than one sentence to describe steps to reproduce.
while the most frequent pattern to describe s2r is paragraphs containing a labeled list of actions see fig.
i.e.
it accounts for .
of the reports describing s2r s2r is also expressed using a single sentence.
for example the cond obs pattern corresponds to conditional sentences containing non negative ob predicates such as when saving a new transient entity ... hibernate will generate two insert statements... from hibernate hhh .
in addition the code ref pattern describes sentences with noun phrases and adverbs of location to refer to code scripts or other non natural language information used to reproduce the observed behavior.
an example of this type of sentences is the following statement produces a compilation error in jdt... from eclipse .
the top five most frequent s2r discourse patterns are present in .
of the s2r bug descriptions.
unique discourse patterns.
we found overlap among ob eb and s2r patterns.
either some patterns are equivalent or they are part of others across the three types of information.
specifically we found that the instead of ob pattern is equivalent to the 399detecting missing information in bug descriptions esec fse september paderborn germany pattern code p sr labeled list description paragraph containing a non empty labeled list of sentences that indicate actions.
the label is optional and indicates s2r terms.
the action sentences may be simple or continuous present past sentences or imperative sentences.
the list may contain ob and eb sentences in no particular order.
rule definitions how to reproduce str to replicate steps to reproduce ... .
... example from firefox click on the header for the first tab to switch to that tab.
figure most common s2r discourse pattern.
instead of exp behavior eb pattern.
the cond pos and cond neg ob patterns are part of two s2r paragraph level patterns and three s2r sentence level patterns all related to conditional content .
the imperative eb pattern is part of two s2r paragraph level and two s2r sentence level patterns all related to imperative content .
in the end ob eb and s2r patterns are unique in our catalog see our replication package .
summary.
our discourse analysis revealed that reporters use discourse patterns to describe ob eb and s2r and on average of the instances of ob eb and s2r are described using only .
of the patterns.
.
.
discourse analysis across projects.
table shows how many patterns of every kind we identified in any of the sentences paragraphs of the bug reports for each project.
as seen in the table not all patterns are used in each system.
table number of patterns used to express ob eb and s2r.
project of reports of patterns overall obebs2r docker eclipse facebook firefox hibernate httpd libreoffice openmrs wordpress a overall fig.
depicts the pattern distribution across projects i.e.
each bar indicates the number of patterns that appear in a given number of projects.
out of the patterns in our catalog i.e.
.
patterns appear in only one project each i.e.
.
appear in two to four projects i.e.
.
appear in five to eight projects and patterns i.e.
.
appear in all nine projects.
examples of rare patterns that appear in one project only are impossible which is used to describe ob in ...it s impossible to click the post button... from facebook why first place which is used to describe eb in ...why the outgoing changes didn t appear in the first number of patterns06121824 9obebs2r 12number of projectsfigure distribution of pattern appearance across projects.
place from eclipse and cond then seq which is used to describe s2r in if you enter... then switch... then switch back... from firefox .
examples of frequently used patterns which appear in all nine projects are neg aux verb ob should eb and cond obs s2r these are previously described in section .
.
.
fig.
reveals that the distribution of ob patterns differs from the distribution of eb and s2r patterns.
nearly three quarters of the ob patterns i.e.
or .
appear in more than half of the projects i.e.
in five to nine projects .
in contrast less than a third of the eb patterns i.e.
or .
and less than half of the s2r patterns i.e.
or .
appear in more than half of the projects.
these results indicate that reporters use many patterns to describe ob and they reuse them often across systems.
reporters use far fewer patterns to describe eb and only a third of them are reused frequently.
about half of the s2r patterns are reused frequently across systems.
our previous analysis identified patterns that are used in of the ob eb s2r descriptions on average.
we deepened our analysis to determine how many patterns are used to achieve similar coverage for each system in our sample set.
nearly a third of the patterns i.e.
out .
are used to describe ob eb or s2r in at least of the cases.
among the patterns twenty ob eb and s2r patterns are frequently reused in each project.
3demibud detecting missing information in bug descriptions our first study revealed that while most of the bug reports .
contain ob only .
and .
of the reports explicitly describe eb and s2r respectively.
these results motivate the need for an automatic approach to detect the absence of this information in bug descriptions.
we focus on detecting eb and s2r as ob is described in nearly all bug reports.
our study also indicated that reporters follow a relatively limited set of discourse patterns to describe eb and s2r across systems i.e.
around in each case .
the existence of these patterns confirms our original conjecture and supports our research on automatically identifying missing eb and s2r.
we designed and evaluated three versions of demibud that automatically detect missing eb and s2r in bug descriptions demibudr based on regular expressions demibud h based on heuristics and nlp and demibud ml based on machine learning.
the first two approaches are unsupervised while the third one requires the 400esec fse september paderborn germanyo.
chaparro j. lu f. zampetti l. moreno m. di penta a. marcus g. bavota and v. ng use of a labeled set of bug reports explicitly reporting the ones containing or not eb and s2r.
.
regular expressions based demibud demibud r uses regular expressions to detect if a bug report contains or not eb and s2r.
the regular expressions rely on frequently used words found in our eb and s2r discourse patterns such as keywords explicitly referring to eb or s2r e.g.
expected result behavior or steps to reproduce recreate and keywords commonly used to describe eb i.e.
modal verbs such as should could or must or other terms such as instead of .
for s2r demibud r also detects enumerations e.g.
.
.
etc.
and itemizations e.g.
etc.
.
if any of the sentences or paragraphs of a bug report matches any of the regular expressions then demibud r labels the report as containing eb s2r otherwise demibud r labels it as missing eb s2r.
demibud r extends existing approaches to detect eb s2r .
the full list of regular expressions used bydemibud r can be found in our replication package .
.
heuristics based demibud demibud h uses part of speech pos tagging and heuristics to match sentences and paragraphs to our discourse patterns.
we implemented each one of the patterns in our catalog by using the stanford corenlp toolkit .
for example to detect eb sentences that follow the discourse pattern should demibud h first identifies the clauses of a sentence by finding coordinating conjunctions i.e.
tokens tagged as cc or punctuation characters e.g.
commas and splits the sentence using these tokens.
then for each clause it identifies the modal terms should or shall by processing the tokens labeled as md i.e.
modal .
finally demibud h checks for the absence of any predicate that uses negative auxiliary verbs prior to the modal.
this is done by identifying the adverb not preceded by auxiliary verbs i.e.
the verbs1 do have or be or the modals can would will could or may .
demibud h also checks for the complement after the modal and for some exceptions e.g.
phrases such as should be done .
if any of the clauses satisfy these rules then the sentence is detected as following the should discourse pattern and labeled as an eb sentence.
each pattern implementation is used to classify all sentences paragraphs in a bug description as having or not having eb s2r.
a bug report is labeled as containing eb s2r if at least one sentence paragraph of the bug report matches any eb s2r pattern implementation.
otherwise the bug report is labeled as missing eb s2r.
.
machine learning based demibud demibud ml is based on state of the art approaches in automated discourse analysis and text classification which utilize textual features such as n grams andpos tags i.e.
part of speech tags .demibud ml relies on two binary classifiers one that detects missing eb and another one that detects missing s2r.
textual features.
we use our discourse patterns as features of bug descriptions for classification purposes.
our patterns capture the structure and to some extent the vocabulary of the descriptions.
each eb and s2r pattern is defined as a boolean feature indicating 1a verb is a token labeled with one of the vb xpos tags such as vbd or vbn i.e.
verb in past tense or past participle .table bug reports missing eb s2r validation bug reports .
project missing eb missing s2r total docker .
.
eclipse .
.
facebook .
.
firefox .
.
hibernate .
.
httpd .
.
libreoffice .
.
openmrs .
.
wordpress a .
.
total .
.
the presence or absence of the pattern in any of the sentences paragraphs of a bug report.
we use the pattern implementations of demibud h to produce the pattern features.
eb and s2r features are used in turn by the corresponding classifier i.e.
the one for eb or s2r respectively .
we also use n grams to capture the vocabulary of bug descriptions.
n grams are contiguous sequences of nterms in the text.
we use unigrams bigrams and trigrams where each n gram is defined as a boolean feature indicating the presence or absence of such an n gram in any of the sentences of a bug report.
finally we use pos tags to capture the type of vocabulary used in the bug descriptions.
similar to n grams we use contiguous sequences of n pos tags in the text.
we define pos tags as boolean features indicating the presence or absence of a tag combination in any of the sentences of a bug report.
learning model.
our current implementation of demibudmluses linear support vector machines svms from svm light to classify the bug reports as missing or not missing eb and s2r.
linear svms are robust state of the art learning algorithms for high dimensional and sparse data sets commonly used for text classification based on n grams .
investigating the use of other classifiers is subject of future work.
.
empirical evaluation design we conducted an empirical evaluation with the goal of determining how accurately demibud can detect missing eb and s2r in bug descriptions and comparing the accuracy of the different instances ofdemibud .
the context of our study is represented by the validation bug reports from the nine software projects used for open coding.
this data set is our gold set see table .
the empirical evaluation aims to answer the following research question rq which demibud strategy has the highest accuracy in detecting missing eb and s2r content in bug descriptions?
we describe the methodology we used to answer rq i.e.
text preprocessing approach tuning evaluation settings and metrics.
text preprocessing.
we removed uninformative text that is likely to introduce noise to the detection using different text preprocessing strategies.
specifically we performed code removal i.e.
deletion of code snippets stack traces output logs environment information etc.this was done by using regular expressions and heuristics defined after our observations of the text.
we also performed basic preprocessing i.e.
replacing urls with the url meta token and removing special characters e.g.
punctuation numbers single characters and tokens starting with numbers.
in addition we performed stemming and stop word removal i.e.
401detecting missing information in bug descriptions esec fse september paderborn germany deletion of common articles prepositions or adjectives by using an adapted version of the lemur stop word list .
our replication package contains the preprocessed bug descriptions the list of stop words and the code removal implementation .
when assessing the performance of demibud r anddemibudh we only use code removal as the preprocessing strategy since by design these approaches need special characters e.g.
the ones used for itemizations and enumerations unstemmed vocabulary and stop words e.g.
if when then etc.
.
the performance ofdemibud ml is determined by using the combination of all preprocessing strategies mentioned above.
tuning and evaluation settings.
we used the discourse bug reports to test demibud r anddemibud h .
this data set contains positive and negative instances that allowed us to test and tune our implementations.
since demibud detects the absence of eb s2r anegative instance is a sentence paragraph report that contains an explicit description of eb s2r whereas a positive instance is a sentence paragraph report that misses such a description.
by using the discourse bug reports we determined the patterns that contribute most and least to demibud h s accuracy.
we followed a leave one out strategy for each one of the eb and s2r patterns.
having all the patterns activated we deactivated one pattern at a time and measured demibud h s accuracy i.e.
the f score more details below without using such pattern.
overall we identified three patterns that when deactivated drastically deteriorate the accuracy of demibud h i.e.
the f 1score drops drastically2 .
these patterns are should for eb and labeled list and after for s2r.
conversely we also identified three patterns that drastically improve the accuracy of demibud h when they are deactivated namely can and imperative for eb and code ref for s2r.
these latter three patterns negatively affect demibud h s performance because they occur frequently in sentences that do not describe eb s2r.
hence we call these as ambiguous patterns .
we measured demibud h s accuracy both by using all the patterns and by omitting the ambiguous patterns.
fordemibud ml we performed fold cross validation 10cv using the validation bug reports .
to avoid over fitting we used and of the bug reports for training parameter tuning and testing respectively.
this strategy ensures that all bug reports are used for training parameter tuning and testing.
the testing data set was used to measure demibud ml s accuracy.
to follow 2for space reasons we omit the results of this tuning approach.
however they can be found in our replication package .a realistic approach we performed 10cv independently on the bug reports of each project.
we call this setting within project evaluation .
we used stratified sampling to create the folds thus ensuring that the proportions of negative and positive instances are similar to the proportions of all the reports in the corresponding project remember that a negative instance indicates the presence of eb s2r while a positive instance indicates the absence .
to assess feature generality in demibud ml we also conducted a cross project evaluation in which the bug reports of one project were used for testing and the reports of the remaining eight projects were used for training and parameter tuning approximately and of the reports were used for training and parameter tuning respectively .
in our experiments we tuned the penalty parameter cof the linear svms by using the parameter tuning data set of each fold.
larger cvalues mean higher penalty on errors.
we experimented with the following parameter values .
.
... .
.
we chose the best parameter cby maximizing thef1score of the trained svms to detect missing eb and s2r.
we found that the parameters that lead to the best accuracy fall in the ranges and for eb and s2r respectively.
evaluation metrics.
we use standard metrics in automated classification to measure the accuracy of our approaches namely precision recall and f 1score .
precision is the percentage of bug reports predicted as missing eb s2r that are correct with respect to the gold set i.e.
precision tp tp fp .
recall is the percentage of bug reports missing eb s2r that are correctly predicted as missing eb s2r i.e.
recall tp tp fn .
f score is the harmonic mean of precision and recall which gives a combined measure of accuracy.
intuitively we prefer higher recall as in a practical setting we want demibud to alert reporters whenever eb or s2r is missing in their bug descriptions.
nonetheless we also want demibud to achieve high precision as many false alerts would hinder its usability.
experiments with users are needed to assess acceptable trade offs between recall and precision.
we leave such studies for future work.
in this paper we focus on the f 1score as an accuracy indicator as we did for the tuning.
when two configurations yield the same f 1score we prefer the one with higher recall.
.
results and discussion we present and discuss the accuracy achieved by our three instances ofdemibud when detecting the absence of eb and s2r using different strategies and features see table .
table overall within project detection accuracy of the different instances of demibud .
approach strategy or featureseb s2r avg.
prec.
avg.
recall avg.
f avg.
prec.
avg.
recall avg.
f demibud r .
.
.
.
.
.
demibud h all patterns .
.
.
.
.
.
demibud h no ambiguous patterns .
.
.
.
.
.
demibud ml pos .
.
.
.
.
.
demibud ml n gram .
.
.
.
.
.
demibud ml pos n gram .
.
.
.
.
.
demibud ml patterns .
.
.
.
.
.
demibud ml patterns pos .
.
.
.
.
.
demibud ml patterns n gram .
.
.
.
.
.
demibud ml pos patterns n gram .
.
.
.
.
.
402esec fse september paderborn germanyo.
chaparro j. lu f. zampetti l. moreno m. di penta a. marcus g. bavota and v. ng .
.
demibud r s accuracy.
demibud r achieves .
avg.
recall and avg.
precision when detecting missing eb see table .
based on f this is the second best approach across all versions of demibud .
our analysis reveals that demibud r fails to detect missing eb in bug reports that do not describe eb i.e.
false negatives .
this is mainly due to the inherent imprecision of keyword matching via regular expressions.
for example we found usages of modal verbs to express ob instead of eb as in this problem could also be related to some sites not copying urls... from firefox and modal verbs appearing in error messages e.g.
... and the error could not load the item appeared on the screen from wordpress android .
we also observed that demibud r detects missing eb in bug reports that describe eb i.e.
false positives as they do not match the keywords used by demibud r .
for example we found eb sentences phrased with used to as in ...you used to be able to add new obs to an already existing encounter order... from openmrs trunk .
regarding s2r demibud r achieves the highest recall .
but also one of the lowest precision values i.e.
.
across the different versions of demibud .demibud r s recall suggests that bug reports missing s2r usually do not contain explicit s2r keywords and or itemizations enumerations.
yet in the few false negatives produced by demibud r i.e.
we found non s2r sentences using s2r keywords such as i tried to reproduce the issue without luck... from wordpress android or templates that contain s2r keywords but are filled in with non s2r content e.g.
steps to reproduce unsure how to reproduce... eclipse .
demibud r flagged missing s2r in bug reports describing s2r i.e.
false positives .
this is somehow expected as users describe s2r using alternative wordings to enumerations itemizations which are not keyword specific.
for example users can describe s2r in a narrative way open the history view on a file with interesting revisions.
click the date column to sort by date... from eclipse .
overall demibud r ranked as the second most accurate detector across all versions of demibud in terms of f 1score i.e.
.
.
the results indicate that demibud r is accurate in detecting missing s2r yet produces a rather large number of false alarms.
.
.
demibud h s accuracy.
when all the patterns are used demibud h is able to detect missing eb with .
recall and .
precision.
when we deactivate the ambiguous eb patterns i.e.
imperative and can demibud h s recall improves substantially i.e.
from .
to .
at almost the same precision i.e.
.
.
this large recall improvement is explained by the large number of bug reports missing eb that contain sentences matching the ambiguous patterns which lead to many false negatives i.e.
failing to detect missing eb .
we found and reports missing eb that contain imperative and can sentences i.e.
.
and .
of the bug reports that do not describe eb respectively.
we observe that imperative sentences are usually used to describe s2r e.g.
.
create a container with volumes in docker .
.
from docker or ask for information to the reporter via templates e.g.
describe the results you received from docker .
can sentences describe other non eb content e.g.
the user can only tell the difference when he recognizes... from firefox .
when the ambiguous eb patterns are deactivated we observe that the main reason for false negatives is sentences describingnon eb content yet following the should eb pattern.
we found conditional sentences expressing actions e.g.
if that s the case we should document this on the wiki... from openmrs trunk questions using the modal should e.g.
... should following tags be unavailable while signed out?
from wordpress android and sentences expressing other type of non eb content e.g.
openmrs shouldn t bomb in this situation from openmrs trunk .
our analysis of the false positives produced by demibud h revealed that our pattern implementation is unable to match some sentences.
in addition we found a handful of bug reports containing eb sentences that are not captured by any of our eb patterns e.g.
...works as expected as in the process is not killed from docker or with ff2 the user sees the tab transition smoothly to the new tab with no nasty white flash from firefox .
regarding s2r when all the patterns are used demibud h has the lowest recall i.e.
but the highest precision i.e.
.
.
we observe .
recall improvement and .
precision deterioration when demibud h relies on all the patterns except code ref i.e.
the ambiguous s2r pattern .
we found bug reports missing s2r but containing sentences matching the code ref pattern i.e.
.
.
the main reasons behind the false negatives are the imprecision of our implementation i.e.
regarding heuristics sentence parsing or code preprocessing and the presence of ambiguous sentences such as here are the definitions of the file systems from httpd .
when the code ref pattern is deactivated we observe two main reasons for false negatives namely the imprecision of our implementation and ambiguous content i.e.
sentences and paragraphs describing non s2r content yet following other s2r patterns .
regarding the latter we found non s2r paragraphs and sentences phrased imperatively that describe solutions e.g.
possible solutions ... .
disable tomcat s default ... from openmrs trunk or actions that do not intend to replicate the ob e.g.
see the user edit page for how the void patient... from openmrs trunk .
other ambiguous cases include non s2r conditional sentences describing high level tasks e.g.
i noticed that hsqldb is not enforcing... while trying to troubleshoot a particular... from openmrs trunk and sentences that convey actions expressed in present perfect tense present tense or past tense e.g.
i also asked about this in the hibernatate... and steve ebersole said that... from openmrs trunk .
these types of discourse are also used to describe s2r and are captured by our s2r patterns.
our analysis of false positives produced by demibud h revealed content ambiguity and unusual text structure as the main reasons for hindering precision.
we found labeled lists of s2r where each step was written as a separate paragraph as in libreoffice paragraphs containing different sentences that describe s2r and other types of information e.g.
i m using libreoffice .
.
.
... i downloaded .
.
and installed... and i knew .
.
requires... from libreoffice itemizations describing ob rather than s2r as in libreoffice sentences not related to ob replication e.g.
i am seeing junk characters and i have to change the encoding setting manually from httpd and ambiguous sentences describing actions i fixed the problem by using... from httpd .
overall we observed more content ambiguity related to s2r than to eb.
this is one of the reasons for the lower accuracy of demibud h and other demibud versions when detecting missing s2r.
403detecting missing information in bug descriptions esec fse september paderborn germany demibud h s high precision and low recall when detecting missing eb and s2r are explained by the focus of our pattern implementations on identifying all different ways to describe eb and s2r i.e.
identifying eb s2r in most bug reports without focusing on filtering non eb s2r content that is similar to eb s2r i.e.
it incorrectly predicts eb s2r in many bug reports .
compared to demibud r demibud h s overall accuracy is lower when detecting missing eb and s2r in bug descriptions in terms of f 1score .
the two main reasons for such in accuracy are imprecision of our heuristics and ambiguous content in bug descriptions.
while the former issue may be addressed by refining some of the patterns the latter one is more challenging.
in any case demibudh s main advantage over the other versions of demibud is its ability to produce very few false alarms.
.
.
demibud ml s accuracy.
when detecting missing eb demibud ml achieves the highest recall i.e.
between .
and .
at the expense of precision i.e.
between .
and .
.
the features used by demibud ml that lead to the highest i.e.
.
and lowest i.e.
.
recall are n grams and patterns pos tags respectively.
the features that lead to the highest i.e.
.
and lowest i.e.
.
precision are patterns andpos tags respectively.
we observe that n grams always increase recall when combined with other features and pos tags deteriorate recall when combined with patterns .pattern features always improve precision when combined with other features at the expense of recall unless they are combined with n grams .
the highest f 1score i.e.
.
.
precision and .
recall is achieved by demibud ml using pattern features.
we consider this version and configuration ofdemibud as the best for detecting missing eb.
demibud ml detects missing s2r with recall ranging between .
and .
.
these recall values are lower than that achieved by demibud r .demibud ml achieves lower precision than demibud h i.e.
between .
and .
.
however demibud ml represents the best compromise achieving the highest f 1score i.e.
.
.
precision and recall when using the patterns n gram features.
once again among the different features used bydemibud ml we observe that individual n grams are the features that lead to the highest recall i.e.
.
and always improve it when combined with other features.
conversely pos tags are the features that lead to the lowest recall i.e.
.
and always deteriorate it when combined with other features.
demibud ml based on pos tags achieves the lowest precision i.e.
.
while n grams lead to the highest i.e.
.
and always improve it when combined with other features.
although individual pattern features lead to lower precision i.e.
.
they always lead to precision improvement when combined with other features.
the highest f score i.e.
.
.
precision and recall is achieved by demibud ml using patterns n gram features.
we consider this configuration of demibud as the best for detecting missing s2r.
explaining the effect of individual features on the results is harder than with heuristics or regular expressions.
however we conjecture that the positive effect of n grams is its ability to capture the vocabulary and to some extent the structure of eb s2r and non eb s2r discourse.
our patterns also capture such characteristics however they further capture the discourse structure.
pos tags focus on capturing the type of vocabulary and to some extent the structure which has a negative impact on demibud ml s accuracy.
in any case all features are insufficient to resolve content ambiguity especially regarding s2r.
as part of our future work we plan to address this problem by capturing semantic properties of the text via semantic frames or rhetorical relations .
table overall cross project accuracy of demibud ml .
featureseb avg.
s2r avg.
prec.
recall f prec.
recall f pos .
.
.
.
.
.
n gram .
.
.
.
.
.
pos n gram .
.
.
.
.
.
patterns .
.
.
.
.
.
patt.
pos .
.
.
.
.
.
patt.
n gram .
.
.
.
.
.
all features .
.
.
.
.
.
.
.
demibud ml s cross project accuracy.
our machine learning based demibud achieves the best f 1score but relies on supervised training.
obtaining training data from a project often poses challenges so using training data from other projects is often desirable.
we analyze demibud ml s accuracy when bug reports from different projects are used to train its underlying learning model.
we compare demibud ml s accuracy when using cross project see table and within project training see table .
in the case of eb using cross project training we observe that demibud ml s precision improves for all type of features except forpos i.e.
avg.
improvement3.
conversely demibud ml s recall decreases .
on average except for posandpos n grams .
in the case of s2r we observe that demibud ml s precision improves for some features i.e.
n gram pos n gram patterns and all features combined and deteriorates for others i.e.
pos patterns pos and patterns n gram .
we observe little precision improvement on average i.e.
.
.
instead demibud ml s recall improves substantially for most features except for pos i.e.
.
avg.
improvement.
the patterns features improve precision i.e.
by .
and achieve the highest recall improvement among all features i.e.
.
.
overall demibud ml s accuracy is higher when using cross project training than when using within project training.
one likely explanation is that the larger training data used in the cross project training includes more patterns.
remarkably demibud ml based on patterns n gram has the best accuracy4for both eb and s2r see table .
its high crossproject accuracy indicates that demibud ml is extremely robust to the training strategy and can be highly useful in a practical setting where labeled data from a new project is unavailable.
this means that we can deploy demibud ml in different projects to the ones we used without retraining and expect similar accuracy levels.
threats to validity the main threat to construct validity is the subjectivity introduced in discourse patterns extraction and in the construction of the labeled bug reports section .
.
.
to minimize subjectivity we ensured that each bug report was coded by two coders independently.
we assessed coding reliability by measuring the inter rater agreement 3the avg.
improvement is computed by averaging the differences between the crossandwithin project precision recall values across the different types of features.
4while individual patterns andpatterns n gram features lead to the same f 1score the latter ones are preferred because they lead to a slightly higher recall.
404esec fse september paderborn germanyo.
chaparro j. lu f. zampetti l. moreno m. di penta a. marcus g. bavota and v. ng section .
.
.
regarding pattern extraction our coding procedure was based on open coding practices that aimed at minimizing subjectivity.
the five coders extracted the patterns in a strict iterative and open manner which led to continuous discussion of ambiguous cases refinement of our pattern catalog and coded data and assessment of our coding process.
we also defined coding criteria and trained the coders on them via interactive tutorials.
to strengthen the internal validity we mitigated the effect of different design and experimentation decisions e.g.
text preprocessing by tuning our three instances of demibud on data sets different from the ones used to measure demibud s accuracy.
to strengthen the external validity we collected bug reports from nine software projects that cover different types of systems e.g.
desktop web or mobile and domains e.g.
web browsing or development .
these projects are open source except for facebook and use different bug trackers.
the collected bug reports cover different types of bugs e.g.
crashes or functional the distribution of bug types can be found in our replication package .
related work our research relates to work on analysis of textual content characterization and classification of issues and issue quality assessment.
analysis of textual content.
our work is based on automated discourse analysis.
we followed the methodology proposed by polanyi for the analysis of the linguistic structure of discourse.
we built on this analysis to identify discourse patterns based on grounded theory practices e.g.
open coding.
this technique has been extensively used in se to e.g.
identify types of knowledge in api documents api privacy policy information or information relevant to development activity summaries .
characterization of issues.
issue or bug descriptions have been characterized from different angles and for different purposes.
previous work e.g.
focused on determining the structure of bug reports and its importance in bug triaging.
chilana et al.
investigated unwanted behavior types in bug reports.
tan et al.
identified defect types from bug reports.
breu et al.
determined stakeholders information needs from bug reports.
ko et al.
analyzed bug report discussions to reveal software design decisions.
based on bug descriptions guo et al.
investigated which bugs get fixed.
ko et al.
and rodeghero et al.
studied the role of different users in bug reporting.
other work has focused on the textual characteristics of bug reports.
ko et al.
performed a linguistic analysis of bug report titles to understand how users describe software problems.
sureka et al.
analyzed the part of speech and distribution of words in issue titles to find vocabulary patterns useful in predicting the bug severity specified in bug reports.
chaparro et al.
and moreno et al.
measured the vocabulary agreement between duplicate bug reports and between bug reports and source code respectively.
different from existing work our focus is on identifying the ob eb and s2r discourse used in bug descriptions.
classification of issues.
our research relates to work on issue classification which relies on machine learning and textual features to classify issues as for example features requests enhancements or bug reports.
similar approaches have been proposed to classify e mails app reviews forums explanations of apis in tutorials and outside se discourse elements in essays .
the essential difference between our svm based approach and existing software content classifiers is the use of discourse patterns from bug descriptions.
more related to our research is davies et al.
s work which proposed the explicit use of search terms e.g.
observed behavior to detect ob eb or s2r content in bug reports.
unfortunately this approach produces numerous undetected cases i.e.
false negatives .
assessment and improvement of issue quality.
our work also relates to research on issue quality assessment.
zimmerman et al.
proposed an approach to predict the quality level of bug reports.
dit et al.
and linstead et al.
measured the semantic coherence in bug report discussions.
hooimeijer et al.
measured quality properties of bug reports e.g.
readability to predict when a bug report would be triaged.
zanetti et al.
identified valid bug reports as opposed to duplicate invalid or incomplete reports by relying on reporters collaboration information.
to enhance bug reports moran et al.
focused on augmenting s2r in bug reports via screenshots and gui component images and on automatically reporting potential crashes in mobile applications .
in another direction some research has focused on summarizing bug reports and detecting duplicate issues .
similar to these approaches our final goal is to improve bug report quality.
our strategy however is to determine when essential information is absent from bug reports and to alert users about it.
conclusions and future work our analysis of bug reports from nine software systems revealed that while most of the reports i.e.
.
describe ob only .
and .
of them explicitly describe eb and s2r.
these findings motivate our effort in developing an automated technique to detect the absence of eb and s2r in bug descriptions.
in addition from our discourse analysis of a subset of bug report descriptions we found that reporters recurrently use discourse patterns to describe ob eb and s2r and few of them i.e.
or .
appear in most of the bug reports that contain such information i.e.
on average .
these results indicate that ob eb and s2r content can be automatically detected with high accuracy.
based on the discourse patterns we developed demibud an automated approach that detects missing eb and s2r in bug descriptions.
we implemented and evaluated three versions of demibud based on regular expressions heuristics and nlp and machine learning.
our ml based approach i.e.
demibud ml proved to be the most accurate in terms of f 1score i.e.
.
for eb and .
for s2r yet the other versions of demibud achieve comparable accuracy without the need for training.
demibud ml proved to be robust with respect to within andcross project training which means that we can deploy it in different projects to the ones we used without retraining and achieve high accuracy detection.
our future work will focus on i studying acceptable recall precision trade offs from the demibud users perspective and ii addressing bug content ambiguity to improve demibud s accuracy.