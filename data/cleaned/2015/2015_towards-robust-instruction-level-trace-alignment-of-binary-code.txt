towards robust instruction level trace alignment of binary code ulf karg en and nahid shahmehri department of computer and information science link oping university link oping sweden fulf.kargen nahid.shahmehrig liu.se abstract program trace alignment is the process of establishing a correspondence between dynamic instruction instances in executions of two semantically similar but syntactically different programs.
in this paper we present what is to the best of our knowledge the first method capable of aligning realistically long execution traces of real programs.
to maximize generality our method works entirely on the machine code level i.e.
it does not require access to source code.
moreover the method is based entirely on dynamic analysis which avoids the many challenges associated with static analysis of binary code and which additionally makes our approach inherently resilient to e.g.
static code obfuscation.
therefore we believe that our trace alignment method could prove to be a useful aid in many program analysis tasks such as debugging reverse engineering investigating plagiarism and malware analysis.
we empirically evaluate our method on popular linux programs and show that it is capable of producing meaningful alignments in the presence of various code transformations such as optimization or obfuscation and that it easily scales to traces with tens of millions of instructions.
i. i ntroduction recently matching binary code has received significant attention in the literature.
several works focus on the problem of searching a large corpus of binaries for known security bugs .
applications outside security include detection of plagiarism or code cloning .
several existing methods match binary code by means of approximate control flow graph isomorphism and can achieve good results in cases where small semantic changes have been applied to otherwise syntactically identical binaries.
however in cases where binaries exhibit significant syntactic differences e.g.
due to the use of different compilers or optimization settings such methods typically fail.
therefore semantic matching based on symbolic execution has been suggested as an alternative.
while such methods can handle syntactic differences better they are often prohibitively expensive for real life programs.
recently several works have tried to overcome these shortcoming by using various approximate code similarity measures .
however to provide sufficient semantic context for matching these methods often match code at a very course granularity typically on the level of entire functions.
moreover to the best of our knowledge all existing methods can only compute static code mappings i.e.
they create mappings between static code segments in two binary executables.
in this work we tackle the related problem ofaligning dynamic instruction traces.
that is we focus on creating fine grained mappings between dynamic instruction instances of two program executions a problem that to the best of our knowledge has not previously been treated in the literature.
being able to determine corresponding points of execution in two semantically similar but syntactically different programs can provide important information in many programcomprehension scenarios.
for example an analyst may be faced with the task of debugging or binary patching a legacy binary possibly compiled for another architecture or with a different compiler and for which the source code might have been lost.
in this case she can use a more recent version of the binary and align execution traces of the two binaries.
if source code and debug symbols are available for the modern version of the binary this information can be leveraged to understand the workings of the legacy binary.
we give a more detailed example of such a scenario in section v of this paper.
another potential application area is aiding in malware analysis.
criminals commonly distribute multiple binary versions of the same malicious software using different obfuscation methods to transform each sample in order to thwart signature based anti virus software.
a malware analyst can use trace alignment to identify that two malware samples behave the same and to better understand new obfuscation schemes used by malware authors.
moreover when analyzing two slightly semantically different executables e.g.
for studying the effects of security patches dynamic trace alignment can aid in the analysis by showing how concrete computations differ between the two executions not just which static instructions that are different.
in this paper we present a general approach to fine grained binary trace alignment.
given two semantically similar but not necessarily identical binaries we run both binaries with the same input and record runtime traces of their executions.
in particular we record all concrete input and output values of instructions which we use as our principal matching feature.
since concrete values capture information about a program s semantics rather than its syntax our method is highly resilient to syntactic differences stemming from e.g.
optimization.
we hope that this work will serve as a foundation for further research into trace alignment as an aid in various programanalysis tasks.
in summary the main contributions of this work are as follows we present a method for aligning binary code traces that are semantically similar but may exhibit significant syntac978 .
c ieeease urbana champaign il usa technical research342 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
mov 0xc rbp eax mov 0x10 rbp edx add edx eax mov eax 0x8 rbp mov 0x10 rbp eax mov eax 0xc rbp mov 0x8 rbp eax mov eax 0x10 rbp addl 0x1 0x14 rbp mov 0x4 rbp eax sub 0x2 eax cmp 0x14 rbp eax jg0x4005demov esi edi mov edx esi add 0x1 ecx lea rsi rdi edx cmp eax ecx jl0x4004e8fig.
.
unoptimized and optimized versions of x86 code for computing fibonacci numbers.
tic differences.
since our method is agnostic to the nature and origin of syntactic differences it is able to handle trace alignment across for example different optimization levels compilers or architectures.
furthermore since our approach is purely dynamic it is inherently resilient to static code obfuscation techniques.
we describe a novel approach to achieving scalable trace alignment and show that our method easily scales to traces with tens of millions of instructions.
we empirically evaluate our method on different linux programs and show that it can produce meaningful trace alignments for binaries compiled with different compilers optimization levels or target architectures as well as binaries subjected to obfuscation.
finally we demonstrate the utility of our method in a practical use case reverse engineering of legacy code for binary patching.
ii.
o verview aligning code traces compiled with e.g.
different optimization settings is a challenging problem since a perfect mapping between instructions is often not possible.
moreover complete sequential consistency between traces typically cannot be assumed.
consider the two code snippets in figure .
both blocks of code represent the main computation loop in a simple program for calculating fibonacci numbers.
both versions have been compiled with gcc for the x86 architecture but the code to the left has been compiled without optimization and the code to the right has been compiled with optimization level o3.
the arrows between the code blocks represent an approximate manual mapping between instructions.
due to arithmetic simplifications performed by the compiler the ordering of some of the roughly equivalent instructions is not preserved across the two versions.
also some instructions in the unoptimized code have no clear counterpart in the optimized version.
a. dynamic time warping while semantics preserving code transformations such as optimization will introduce small local discrepancies between traces semantic differences when aligning across different revisions of a program may introduce significant large scale discrepancies.
a generic trace alignment method must thereforeadopt a pragmatic approach performing a best effort global alignment while tolerating local unalignable sections of traces.
dynamic time warping dtw is a well known method for aligning time series of analogue signals used extensively in e.g.
speech recognition .
given two time series x yof respective lengths nandm and a distance measure dist x i yj of individual elements in the time series a cost matrix cis constructed.
each cell ci jrepresents the dissimilarity or cost dist x i yj between the corresponding elements in the two time series.
dtw constructs a warping matrix w where each element wi jcontains the accumulated cost of the optimal cost minimizing path in cfrom c0 0toci j. the final minimum accumulated cost can be retrieved from element wn min the warping matrix and is called the warping distance.
when w has been constructed finding the optimal alignment between x andysimply entails a greedy search through wfrom wn m tow0 to find the global minimum cost path or warp path.
the row and column of each point on the warp path give the actual element alignments.
for example if the warp path passes through cell wi j then elements xiandyjalign with each other.
a vertical or horizontal section of the warp path means that one element in one of the time series aligns with several elements in the other series.
figure shows a 3x5 matrix with a possible warp path thick dark line .
in this example elements and of the vertical series align respectively with elements and of the horizontal series.
the classical dynamic programming dtw algorithm has time complexity o nm which unfortunately limits its use to fairly short sequences.
the fastdtw algorithm is a popular approximate version of dtw with linear time complexity.
it works by aggregating adjacent elements in each time series to create a low resolution version of the cost matrix and then running standard dtw on the low resolution matrix.
the final warp path is created by iteratively repeating this process at higher and higher resolutions but restricting the construction of the warping matrix to a narrow band around the warp path from the previous iteration.
the width of the band is controlled by a radius parameter which determines how many additional elements on each side of the previous warp path to include.
b. aligning instruction traces the application of dtw for real valued signals is straightforward.
the euclidean distances between elements can be used to construct the cost matrix and averages over several adjacent elements in a time series can be used to construct lowerresolution versions of the problem for fastdtw.
however in our setting the elements of the time series are machine code instructions.
one option would be to treat individual opcodes as symbols in an alphabet and then compute the string edit distance essentially a variant of dtw for strings .
however such an approach would restrict us to align code generated by the same compiler using the exact same compilation settings since for example optimized binaries often use more efficient instructions to perform the same computations as their unoptimized counterparts.
instead we record the concrete input authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
fig.
.
constructing a cost matrix using a vector space model of instruction segments.
and output values of computations and use these to match instructions.
this approach also facilitates a natural way to aggregate several elements of a trace.
a simple approach is to represent a sequence of several instructions henceforth called asegment as the set of all unique values observed in that segment and use the jaccard similarity to compute distances between segments.
a limitation of the jaccard similarity however is that it does not account for the frequency distributions of values.
some values such as the value or small powers of two are extremely common in most program traces and may incorrectly inflate matching scores.
also since several segments may contain similar sets of values it is also beneficial to consider thenumber of times a certain value is observed within a segment.
to this end we adopt the approach proposed by karg en et al.
utilizing the vector space model with tf idf weighting from the field of information retrieval.
when the vector space model is used for information retrieval each document in a collection is represented by a point in a multidimensional space with one dimension per unique term in the vocabulary of the document collection.
the coordinates of a document are determined by the number of times each term occurs in that document denoted as the term frequency tf .
the tf components are also weighted by the inverse document frequency idf of the corresponding term to create the final tf idf vector components.
the idf of term tis calculated as log n n t where ndenotes the total number of documents and ntdenotes the number of documents in which term toccurs at least once.
the rationale for the idf factor is that common terms with low discriminative value receive lower weight in the document vectors.
finally the similarity of documents can be computed as a value between 0and1by calculating the cosine similarity between document vectors defined simply as the cosine of the vectors i.e.
similarity a b a b jajjbj the cost or distance is then given by similarity a b in our setting documents correspond to trace segments and terms correspond to values.
we apply the tf idf technique to construct vector space representations of trace segments henceforth referred to as value vectors.
figure shows a toy example of this approach where the aligned traces are and elements long respectively.
for simplicity we assume that each element has a single value and that idf weighting is not used.
a segment size of is used resulting in a 3x5 cost matrix.
the highlighted segments show how value vectors are constructed.
for example the vertical segment has two occurrences of the value one occurrence of and two occurrences of .
when computing the cosine of the two value vectors only the shared components corresponding to values and contribute to the dot product.
after dividing by the vector lengths and subtracting from to get a distance metric we arrive at a cosine distance of about .
.
in the following section we describe in more detail how values are recorded into traces and our adaption of the fastdtw algorithm for trace alignment.
iii.
d esign and implementation our system consists of four modular components that are executed in a pipeline.
the architecture specific trace recorder component collects detailed syntactic instruction traces of executions.
the trace distiller takes an instruction trace from the trace recorder and produces an architecture agnostic value trace consisting only of concrete values observed for each instruction.
a trace filterer component can optionally be applied to the value trace to remove irrelevant parts of the execution such as instructions in external libraries.
finally the trace aligner component takes two value traces produced by the preceding steps of the pipeline and performs trace alignment according to the procedure in section ii.
we implemented our method using about sloc c and a few hundred lines of python for parsing results calculating statistics etc.
below we describe each component of our system in more detail.
a. trace construction trace recording.
our trace recording is implemented using the pin dynamic binary instrumentation dbi framework .
all read and written register and memory locations including concrete addresses are recorded along with the values of all input and output operands.
trace distilling.
the trace distiller strips a trace from the trace recorder of all syntactic information and creates an architectureindependent value trace.
entries in the value trace consist only of a unique instruction id and a set of input and output values.
the trace distiller also outputs a mapping between instruction ids and actual instruction offsets in executables.
inputs and outputs are xor ed with their own respective magic constant so that e.g.
the value as an input is distinct from the value as an output.
this way we capture more information about the input output semantics of instructions.
to allow alignment across e.g.
bit architectures and to handle optimized code that performs calculations in parallel on several values using simd instructions we tokenize all values to a common bit width.
values greater than the token width are split into several values while values smaller than the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
token width are zero padded.
we currently use a token width of bits which appears to work well in practice and allows alignment of and bit traces.
we also attempt to filter out addresses from the value traces.
addresses are often used as data in binaries e.g.
when passing around pointers in the code.
since addresses are typically not comparable across different versions of a binary these unnecessarily inflate the size of traces and add noise to the segment matching.
since a handful of false positives or negatives during address filtering will not have a detrimental effect on segment matching we adopt a simple yet effective filtering approach for all addresses reported by the trace recorder we mask off the least significant bits corresponding to one 4kb page and store it in a set of address masks.
every recorded value is checked against these address masks before being stored in the value trace.
we found that address filtering reduced the number of stored values in traces by roughly .
trace filtering.
it is often desirable to be able to filter out irrelevant parts of an execution trace such as library code that is not interesting to the analysis.
such filtering will both reduce the time for trace alignment and can improve precision by e.g.
removing non deterministic parts of an execution which cannot be matched using value comparisons.
our system allows filtering based on individual instruction addresses ranges of addresses and names of entire executables e.g.
shared libraries .
b. trace alignment warping matrix construction.
the trace alignment component takes two value traces splits them into equally sized segments and constructs value vectors out of these segments.
the resulting arrays of value vectors are used to compute the cosine distances between segments and construct the warping matrix.
since some instructions such as branches or addressmanipulation instructions will not have any recorded values a value vector is allowed to be empty.
we treat empty vectors specially if two empty vectors are compared to each other they are considered equal and are assigned distance .
if one of the vectors is instead non empty they are considered maximally distant i.e.
with distance .
fastdtw implementation.
the initial segment size is chosen so that the smaller dimension of the warping matrix is always elements.
for each iteration of fastdtw we decrease the segment size with a factor two doubling the row and column count of the warping matrix.
fastdtw terminates when we have reached a segment size of for any of the traces.
we use a fastdtw radius parameter of elements.
this means that the maximum space required for the final warping matrix is always bounded to n where nis the size of the longer of the two traces.
we have implemented a simple multithreaded variant of fastdtw where matrix rows are processed partially in parallel in a pipelined fashion.
when using cores we observe roughly a 3x speedup with our parallel implementation.
a b c fig.
.
first iteration cost a and warp b matrices and third iteration cost matrix c of aligning optimized and unoptimized gzip traces.
value vector construction.
we use a simple sparse array implementation based on the c stl map to store value vectors so that values with a zero tf idf component need not be physically stored.
also only values that are present at least once in both traces get physical entries in the sparse arrays.
since values unique to one trace can never contribute to the dot product in the cosine distance computation they can be safely omitted.
they still however contribute to the length of the vector in the denominator of the cosine.
we observed that this simple optimization often massively reduced the size of value vectors.
while sparse arrays are necessary to reduce memory use they unfortunately result in an o nlogn time complexity in the size of value vectors when computing cosine distance.
this brings the overall time complexity of our fastdtw implementation up to o nlogn instead of o n .
to avoid this we set an upper bound on the size of value vectors by only keeping the largest components.
we found that this significantly reduces the time needed for the first coarse grained alignment iterations of fastdtw while not impacting the accuracy of the final fine grained alignments see section iv c .
subsequence alignment.
we have also implemented subsequence alignment .
in this variant of dtw the warp path is not constrained to start and end at opposite diagonal ends of the warping matrix but can run from any column of the top row to any column of the bottom row.
subsequence alignment is useful when a trace can only be aligned with a subsequence of the other trace.
for example given two versions aandb of a program an analyst might be interested in aligning only code within a certain function of awith a full trace of b in order to locate the corresponding function s instructions in b. subsequence alignment may be necessary if only parts of one trace are comparable to another trace.
visualization.
we have implemented a tool for rendering the cost and warp matrix at each fastdtw iteration to an image file.
figure 3a shows the cost matrix of aligning traces of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an optimized o3 and unoptimized o0 version of gzip .
each pixel of the 50x98 matrix represents instructions.
the aspect ratio of about is due to the optimized code using roughly half as many instructions to perform the same computations as the unoptimized code.
darker shades in the matrix indicate lower cost i.e.
lower dissimilarity .
we can see a clear diagonal line of dark pixels representing corresponding segments of instructions in both traces.
we can also discern several symmetrical square regions along the diagonal indicating different distinct stages of processing.
figure 3b shows the corresponding warping matrix with the optimal warp path highlighted in red.
we see that the warp path closely follows the dark diagonal line in the cost matrix.
figure 3c shows the third iteration cost matrix of fastdtw for the same alignment problem.
with times higher resolution finer details of the trace similarities are now visible.
since we use a radius parameter of most of the cost matrix is never constructed.
ignored regions are shown as black pixels here.
iv.
e mpirical evaluation in this section we empirically evaluate the effectiveness of our trace alignment method.
there are several challenges with such an evaluation.
first since we are as far as we know the first ones to consider the problem of dynamic trace alignment direct comparisons against other work is not possible.
a second problem is that ground truth is difficult to obtain for evaluating our method s effectiveness when aligning programs that are not semantically identical.
as a first step towards evaluating the feasibility of our approach we focus on measuring the robustness of our method against various semantics preserving code transformations.
in section v we then present a practical use case showing the utility of our method when comparing two revisions of the same program.
the programs used in our experiments and a brief description of the respective inputs used are shown in table i. except when explicitly stated otherwise the compiler used to generate all binaries was gcc .
.
.
all binaries were compiled with embedded debug info as ground truth.
experiments were performed on a linux mint .
workstation equipped with a quad core .
ghz intel xeon e3 cpu with hyperthreading and gb ram.
external dependencies such as the c library constitute an error source in our experiments.
within an instruction trace code from external shared libraries will be identical for all versions of the same program and would therefore be trivial to align.
this may aid the dtw algorithm in finding an accurate global alignment for the entire trace.
to accurately characterize our method s accuracy under maximally adversarial conditions we used the trace filterer section iii a to remove instructions from external libraries but kept instructions from libraries that programs ship with.
for example for the xmllint program we removed instructions in libc but kept instructions in libxml .
the last three columns of table i show some execution details of the unoptimized gcc o0 versions of binaries.
the columns show respectively the number of dynamic instructions the number of dynamicinstructions after trace filtering and the number of unique instructions in the filtered trace.
a. metrics used in a typical usage scenario an analyst may be interested in aligning a reference binary for example with embedded debug info against a target binary which may e.g.
be stripped to gain insights about the target binary.
in our experiments we used reference binaries compiled with gcc using optimization levelo0 and aligned against traces from binaries subjected to various code transformations.
to evaluate the accuracy of our alignment method we compute the alignment delta aof trace alignments using embedded debug info.
consider two program versions xand y. given an alignment between traces from the two versions let element xidenote the i th element of x s trace and yjthe j th element of y s trace.
if xialign with yj then the aof that particular instruction alignment is jj kj where kis the index closest to jsuch that ximaps to the same source code line as yk.
ifxialigns with several instructions in the other trace then ais0if any of those instructions map to the same source code line.
otherwise ais calculated asmin jj kj j j l kj where kis the matching index closest to either of yjoryj l. thus ameasures how close an individual instruction alignment is to the closest equivalent instruction in the target trace.
the main contribution of our method is the ability to produce dynamic instruction alignments.
however in order to allow an at least partial comparison with future and present codematching methods we also studied our method s ability to produce static instruction mappings.
for each static instruction in the reference binary we iterate over all its dynamic instances in the reference trace and record the instructions with which it aligns in the target trace.
from this we collect statistics on how many times each instruction in the reference binary aligns with each instruction in the target binary.
for a given instruction in the reference binary we can then create a list of most likely corresponding instructions in the target binary ranked by the number of alignments.
the accuracy of static mappings are evaluated using embedded debug info where instructions in the respective binaries that map to the same source code line are considered to match.
in the following experiments section iv b we report the fraction of static instruction mappings where the topmost candidate is the correct match.
however while our method finds exact matches for many instructions a perfect static instruction mapping between binaries is difficult to achieve due to e.g.
code layout differences as discussed in section ii.
therefore it is also interesting to investigate our method s ability to pinpoint regions of binaries where the same highlevel functionality is implemented or to identify corresponding execution points where the same high level operations are performed.
we therefore propose an additional metric that better capture our method s ability to perform fine grained but not necessarily exact trace alignment.
specifically we evaluate how well trace alignment performs in finding instruction that authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i programs used in the experiments .
program version input execution details dyn.
instr.
o0 filtered instr.
static instr.
bzip2 .
.
compress .
kb text file file to standard output date .
invoked without additional command line options df .
invoked without additional command line options djpeg 9b decode .
kb jpeg file to standard output gzip .
same as bzip2 lighttpd .
.
server startup http get request for b html file ls .
invoked with ls l mpg123 .
.
decode .
kb mp3 file to standard output sha256sum .
checksum text file same as bzip2 to standard output sqlite dump .
kb database to standard output as sql xmllint .
.
run on .
kb xml file with the noout option table ii instruction matching accuracy g c co0versus g c co3 program a a top close bzip2 .
.
.
.
date .
.
.
.
df .
.
.
.
djpeg .
.
.
.
gzip .
.
.
.
lighttpd .
.
.
.
ls .
.
.
.
mpg123 .
.
.
.
sha256sum .
.
.
.
sqlite .
.
.
.
xmllint .
.
.
.
arephysically close but not necessarily identical to the exact match.
to this end we measure the number of cases where the source code line of the topmost mapping is less than lines away from the correct match.
b. alignment accuracy here we evaluate how the alignment accuracy of our method is affected by a number of code transformations.
aligning optimized code.
in our first experiment we aligned optimized and unoptimized x86 binaries compiled with gcc.
in this experiment we used reference binaries compiled with optimization level o0 and o3 optimized target binaries.
table ii shows accuracy metrics for the programs in the experiment.
the first two columns show respectively the percentages of dynamic instruction alignments with a i.e.
exact matches and a i.e.
temporally close alignments.
with the exception of df more than of alignments are exact for all programs according to debug info and more than of alignments are close to an exact match.
the two rightmost columns show results for static instruction mappings.
the column topshows the fraction of instructions in the reference o0 binary where the topmost entry in the ranked list of static mappings is the correct match.
we see that for most programs the top ranking match is correct in about of cases.
the percentage of cases where the topmost mapping is close to the true match in the source code is shown in the column close.
these figures vary between and table iii instruction matching accuracy g c co2versus g c co3 program a a top close bzip2 .
.
.
.
date .
.
.
.
df .
.
.
.
djpeg .
.
.
.
gzip .
.
.
.
lighttpd .
.
.
.
ls .
.
.
.
mpg123 .
.
.
.
sha256sum .
.
.
.
sqlite .
.
.
.
xmllint .
.
.
.
showing that our method can be used to create approximate static mappings for large portions of executed instructions.
for reference we also performed the same experiment using o2 compiled binaries to generate reference traces.
the results are shown in table iii.
with the highly similar o2and o3optimization levels percentages are very high across all accuracy statistics.
aligning x86 and x86 code.
to study the performance of our method when aligning across and bit architectures we repeated the above experiment aligning o0ando3 gcc traces but this time compiling the target binaries for the bit x86 architecture.
results are shown in table iv.
precision figures are similar to the same architecture case but generally a few percentage points lower showing that our method can tolerate syntactic differences between x86 and x86 code.
aligning across different compilers.
in the next experiment we compiled the target binaries with clang .
and optimization level o3.
both sets of binaries were compiled for x86 .
as can be seen from table v results are generally comparable to those of the same compiler experiment with some programs even getting higher accuracy scores.
the exception is the date program where results are significantly lower for static mappings.
this is likely due to its very short trace length.
since most of the work in date is carried out in external libraries the filtered trace is only a few thousand instructions long.
moreover most instructions in the trace execute only once making static mappings very sensitive to small misalignments.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv instruction matching accuracy x86 g c co0versus x 86g c co3 program a a top close bzip2 .
.
.
.
date .
.
.
.
df .
.
.
.
djpeg .
.
.
.
gzip .
.
.
.
lighttpd .
.
.
.
ls .
.
.
.
mpg123 .
.
.
.
sha256sum .
.
.
.
sqlite .
.
.
.
xmllint .
.
.
.
table v instruction matching accuracy g c co0versus c l a n g o3 program a a top close bzip2 .
.
.
.
date .
.
.
.
df .
.
.
.
djpeg .
.
.
.
gzip .
.
.
.
lighttpd .
.
.
.
ls .
.
.
.
mpg123 .
.
.
.
sha256sum .
.
.
.
sqlite .
.
.
.
xmllint .
.
.
.
aligning obfuscated code to evaluate the accuracy of our method under adversarial conditions we also performed trace alignment of obfuscated binaries.
we used the open source obfuscator llvm ollvm which is implemented on top of the llvm framework and which applies obfuscation during compilation.
ollvm was chosen in part because its design allows embedding debug info as ground truth also in obfuscated binaries.
it should be noted however that the accuracy of debug info in obfuscated code is far from perfect due to the heavy code transformations.
as such the results in this section should be seen as indicative rather than providing exact figures of our method s performance on obfuscated code.
three types of obfuscations are supported by ollvm bogus control flow instruction substitution and control flow flattening.
bogus control flow inserts fake basic blocks guarded by opaque predicates to complicate static analysis of control flow.
instruction substitution transforms computations by replacing arithmetic operations with more convoluted but semantically equivalent sequences of computations.
the latter is a more significant challenge for trace alignment since traces of obfuscated code will be polluted with intermediate values not present in the non obfuscated traces.
control flow flattening hinders static control flow recovery by replacing all direct branches with indirect jumps routing all control flow through a dispatch loop.
the open source variant of ollvm applies this obfuscation very aggressively flattening control flow between every basic block in thetable vi instruction matching accuracy g c co0versus o l l v m program a a top close bzip2 .
.
.
.
date .
.
.
.
df .
.
.
.
djpeg .
.
.
.
gzip .
.
.
.
lighttpd .
.
.
.
ls .
.
.
.
mpg123 .
.
.
.
sha256sum .
.
.
.
sqlite .
.
.
.
xmllint .
.
.
.
program.
this results in both significant slowdowns and codeblowup.
unfortunately debug info for the inserted flatteningcode also appears to map instructions to source code lines in a somewhat arbitrary way leading to difficulties in evaluating the accuracy of trace alignment when this obfuscation is used.
we therefore only used bogus control flow and instruction substitution in our experiments.
all ollvm obfuscated binaries were built for the x86 architecture.
results for the different programs are shown in table vi.
while the accuracy is still reasonable for several programs it suffers considerably for others.
as in the previous experiment the short trace length of date becomes problematic.
the very low percentages for aindicate that dtw has found a degenerate alignment for date .
similarly other programs with relatively short trace lengths compared to the number of executed static instructions lighttpd ls sqlite xmllint suffer reduced static mapping accuracy.
again it should be noted that the reduced quality of debug information in obfuscated binaries may also contribute to the drop in accuracy.
sources of imprecision.
even under heavy syntactic transformations our method is generally able to produce global alignments where individual instruction instances in the reference trace are aligned with instructions that are close to their true counterpart in the target trace as indicated by the high a 10figures.
however in many cases instructions are not precisely aligned as indicated by the markedly lower a figures.
one reason for this is that many instructions do not perform computations useful for value comparisons.
for example on average of instructions in the o0traces had empty value sets.
also the frequency distribution of values in program traces is highly skewed as discussed in section ii b. during the first fastdtw iterations with large instruction segments tf idf weighting efficiently compensates for this skew but individual instruction alignments during the last fastdtw iteration still suffers from reduced accuracy.
finally we again note that precise instruction alignments are not always possible as shown in the example in figure .
c. scalability scalability is an important factor when applying trace alignment to real life problems.
in this section we have authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vii alignment times in seconds with value vector bounding enabled left columns and disabled right columns program o0 o3 o0 clang o0 x86 o0 ollvm bzip2 .
.
.
.
.
.
.
.
date .
.
.
.
.
.
.
.
df .
.
.
.
.
.
.
.
djpeg .
.
.
.
.
.
.
.
gzip .
.
.
.
.
.
.
.
lighttpd .
.
.
.
.
.
.
.
ls .
.
.
.
.
.
.
.
mpg123 .
.
.
.
.
.
.
.
sha256sum .
.
.
.
.
.
.
.
sqlite .
.
.
.
.
.
.
.
xmllint .
.
.
.
.
.
.
.
evaluated the efficiency of our method for different programs and trace sizes.
in the below experiments alignments were performed using parallel threads i.e.
one thread per virtual cpu core .
we first study the impact of our value vector bounding approximation section iii b .
table vii shows alignment times for the experiments in section iv b theo2versus o3case has been omitted to save space .
the left and right columns for each experiment show alignment times with and without value vector bounding respectively.
with the optimization active all alignments finish within minutes with the majority taking less than a minute.
while the value vector bounding has little impact for programs with short trace lengths it improves alignment times with a factor or more for the mpg123 program.
interestingly mpg123 also has significantly longer alignment times than bzip2 despite the two having traces of comparable size.
the difference is likely due to mpg123 more heavy numerical computations with more runtime values per instruction on average than bzip2.
to ascertain that value vector bounding does not negatively affect alignment accuracy we also compared the final warp paths from both sets of experiments and found that they were identical for all programs.
thus value vector bounding does not appear to negatively effect the accuracy of final alignments.
to evaluate the scalability when using longer traces with tens of millions of instructions we selected four of the more computationally heavy programs used earlier and repeated the gcc o0 versus gcc o3 alignment using similar but larger inputs.
the size of inputs and resulting filtered trace sizes in millions of instructions are shown in the first columns of table viii.
the two rightmost columns show respectively the time to prepare i.e.
record process and filter traces and the time to compute alignments.
the alignment times are roughly consistent with the theoretical linear time complexity of fastdtw.
v. c ase study in this section we demonstrate the utility of our method in a practical use case.
in our scenario we have a legacy binary for which it is unfeasible to recompile from source code e.g.
because the source code has been lost.
we also have a more modern version of the binary where source code is available.table viii alignment times for long traces .
program input kb trace sizes millions prep.
s align s bzip2 .
.
x .
.
.
djpeg .
.
x .
.
.
mpg123 .
.
x .
.
.
sqlite .
.
x .
.
.
if a bug is discovered and patched in the modern version we may need to binary patch the legacy version if it is still in use in some production environment and cannot be upgraded due to e.g.
backwards compatibility issues.
as an example of how trace alignment can be used in such a scenario we use two versions of the libtiff library and the bundled tiff2pdf tool.libtiff version .
.
has a memory corruption bug cve which could potentially be exploited by an attacker by using a specially crafted tiff file as input to tiff2pdf .
the bug is patched in subsequent versions of libtiff .
figure shows how the bug was patched.
after the patch the function tiffinitnext intif next.c sets a function pointer to an added input validation routine so that this routine is executed before further parsing of the tiff image.
in a binary patching scenario a straightforward approach to fix the vulnerable code would be to patch in an equivalent validation routine somewhere in the binary and also patch tiffinitnext to insert a pointer to this routine in thetiff struct.
we now show how trace alignment can be used to aid in locating the function tiffinitnext in the legacy binary using the at the time of writing most recent .
.
version of libtiff as reference.
to make the use case more realistic we compiled the modern version with gcc for x86 using optimization level o0 and the vulnerable version with clang for x86 using optimization level o3.
we filtered out external library code as in previous experiments and aligned executions of the respective binaries with an input that exercised the code intiffinitnext.
note that this is a challenging use case for our method since no actual computations are made in the code of interest and thus no runtime values are available for matching instructions.
our method therefore needs to rely on accurately aligning authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
int .
tiffinitnext tiff tif int scheme .
.
void scheme .
tif tif decoderow nextdecode .
tif tif decodestrip nextdecode .
tif tif decodetile nextdecode .
return .
.
int .
tiffinitnext tiff tif int scheme .
.
void scheme .
tif tif predecode nextpredecode .
tif tif decoderow nextdecode .
tif tif decodestrip nextdecode .
tif tif decodetile nextdecode .
return .
fig.
.
version .
.
top and .
.
bottom of tiffinitnext in libtiff.
the surrounding context.
the code in question would also be difficult to locate for existing static code matching approaches since they typically require larger sections of code for accurate matching.
for example state of the art tools like genius or discovre require functions to have at least five basic blocks to be searchable.
we used embedded debug info to study how well our method would perform in this scenario.
the identified instruction mappings translated to source code lines are shown below tif next.c !tif next.c tif next.c !tif next.c tif next.c !tif next.c tif next.c !tif next.c tif next.c !tif dir.c tif next.c !tif dir.c despite about four years worth of code revisions between the versions and despite the significant syntactic differences our method manages to pinpoint the location where the patch should be applied within a few instructions.
vi.
r elated work in this section we briefly survey related work.
due to the large corpus of work on binary analysis we limit our discussion to the works that are most similar to ours.
specifically we focus on methods capable of comparing binary code without the aid of source code and in the presence of syntactic differences between binaries.
dynamic methods.
the work most closely related to ours is the method by zhang and gupta which were later extended by nagarajan et al.
.
the purpose of these works are to compute static mappings between instructions in two syntactically different but semantically identical binaries.
like our method they also rely on dynamic analysis.
due to the similarities to our work we will discuss their respective approaches in more detail.
zhang and gupta compare sets of runtime values to match instructions.
if one instruction s value set is a subset of theother s the instructions are considered to match.
matches are further pruned by considering data flow relations using heuristic matching of the dynamic data dependence graphs of the binaries.
the method is very effective at finding true matches accuracy on average but also produces many false matches according to an experiment in the paper .
the primary purpose of zhang and gupta s work was to create mappings between optimized and unoptimized code.
nagarajan et al.
extended their method to cases where the mapping between functions in the two binaries is not known a priori.
first the dynamic call graphs of the respective executions were aligned using a heuristic method.
zhang and gupta s original instruction matching method was then applied to pairs of matched functions between the binaries.
finally the instruction mappings were used to match paths in the dynamic control flow graphs of the two binaries.
to deal with false matches matched paths were further prioritized using the structure of the dynamic cfgs.
while both works share similarities with our work they are also different in several important ways.
first even though they use dynamic analysis they both compute static code mappings while we focus on the alignment of dynamic instruction instances.
second both methods are explicitly targeted towards matching semantically identical binaries.
indeed zhang and gupta pointed out that a single differing data flow edge in one of the two compared binaries resulted in a sharp increase in the number of unmatchable instructions and proposed that this property could be used to detect compiler bugs.
lastly while both works use runtime values for matching just like our work we use the approximate temporal ordering as our second principal matching feature while they use approximate matching of the data flow and or control flow graphs.
furthermore since both their respective works rely on several stacked heuristics the risk of various unknown failure modes is intuitively higher.
we instead take a more fundamental approach using only the temporal ordering and concrete computations of instructions for our matching.
similar to the bias variance tradeoff in statistics the price for this generality is a somewhat lower accuracy.
since the purpose of this work was to investigate the accuracy of value based trace alignment in its own right we refrained from integrating any ad hoc heuristics into our method.
however combining our approach with methods based on data and control flow information could be an interesting direction for future work.
for example one alternative could be to combine our method s ability to find meaningful global alignments with a local dataflow matching approach similar to zhang and gupta s work.
other approaches to dynamic code matching include egele et al.
who proposed blanket execution for binary function matching.
like our method they also use dynamic analysis and collect runtime values for code matching.
however since their goal is to statically match queries against all functions in a binary they execute all functions of a given executable in a randomized memory environment and coerce some code paths to reach code coverage.
jhi et al.
also proposed using runtime observed values for plagiarism detection on binaries.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
their method however only performs matching on the level of entire binaries.
zhang et al.
later extended their approach for detection of algorithm plagiarism.
kirat et al.
proposed a method for finding the evasion point of evasive malware.
similar to our work they also use sequence alignment but only perform a coarse grained alignment of system calls.
static methods.
flake proposed comparing the control flow graph of functions in two binaries for identifying similar code.
this approach has been implemented in the industrystandard binary diffing tool bindiff which uses a heuristic graph isomorphism algorithm for comparing cfgs of functions.
bourquin et al.
proposed an improved graph isomorphism method for this problem.
since the structure of cfgs in a binary is often changed when using different compilers or optimization levels the accuracy of cfg matching typically drops dramatically in such cases .
binhunt and ibinhunt use symbolic execution to find semantically identical basic blocks in two binaries.
luo et al.
proposed using a theorem prover to find longestcommon subsequences of semantically equivalent basic blocks for matching obfuscated code.
scalability is unfortunately a significant problem for approaches based on symbolic execution.
chandramohan et al.
proposed to ameliorate this problem by pre filtering potential function matches before semantic comparison and suggested selective inlining to cope with inlined library code.
pewny et al.
proposed semantic hashes using randomly sampled input output pairs of basic blocks for finding known vulnerable code across different architectures.
david et al.
proposed a statistical framework for matching functions by aggregating semantic similarity scores of function code fragments.
eschweiler et al.
match functions using structural features of the cfg and improve scalability by using pre filtering based on simple numeric features.
feng et al.
pointed out that pre filtering can lead to many false negatives and instead proposed translating cfgs to numeric feature vectors to address scalability.
vii.
d iscussion and future work semantic differences.
we showed in section iv b that our method is resilient to various semantics preserving code transformations.
while our case study indicated that the method can also cope with semantic changes we intend to study this more rigorously in future work.
evaluation on other architectures.
since our prototype implementation is based on pin it is currently limited to the x86 and x86 architectures.
in future work we intend to also evaluate our method s performance on other popular architectures such as arm or mips.
since the method is entirely architecture agnostic and due to its resilience to other code transformations section iv b we are hopeful that it will also allow cross architecture alignments for other architectures.
due to our modular design supporting other architectures only requires implementing a new trace recorder component.
limitations of value based analysis.
we use runtimeobserved results of computations as a proxy for the actualsemantics of computations and use dtw to align value traces.
while this facilitates a scalable way to match long instruction traces it has three major limitations.
first it requires both traces to be produced using the exact same program input and runtime environment.
second it requires that all computations throughout both traces are carried out on the same concrete values.
finally while our method can tolerate small local discrepancies in sequential consistency global scale reordering of computations cannot be handled by dtw.
the first and second limitations can be problematic if parts of a program s computations are nondeterministic.
for example many cryptographic protocols use random nonces which makes recording two executions with identical inputs infeasible.
one potential way to address this challenge is to directly compute the semantic similarity of code sections rather than using observed runtime values.
while scalability still remains an issue with current approaches several recent works have proposed methods for semantic similarity comparisons of basic blocks .
if only parts of a trace can be aligned in a meaningful way e.g.
due to the second or third limitation above subsequence alignment can be used.
this however requires the analyst to identify alignable parts of traces a priori.
while visualization of the cost matrix can sometimes be used to identify such sections exploring more rigorous approaches to solving this problem would be an important topic of future work.
local sequence alignment using the smith waterman algorithm can be used to find locally optimal subsequence alignments between two sequences in cases where the full sequences cannot be globally aligned.
local sequence alignment is used extensively in bioinformatics for e.g.
aligning dna sequences.
since the smith waterman algorithm is designed to align strings its cost function is defined in terms of match or mismatch between letters.
adapting it to the problem of trace alignment would therefore require defining a similarity threshold for when two trace segments match .
determining such a threshold is unfortunately difficult in the general case but adapting local sequence alignment to our problem setting nonetheless remains an interesting direction for future work.
concurrency may also pose a problem for trace alignment.
assuming that the computations in each thread are deterministic our system can align traces of individual threads separately.
however in some cases e.g.
when using thread pools the set of computations performed in each thread may vary.
our method can currently not handle such programs.
viii.
c onclusion in this paper we proposed a novel approach for aligning binary code traces using dynamic time warping and techniques from information retrieval.
we showed that our method is resilient to a number of code transformations and can align code across and bit architectures.
we also demonstrated that the method scales to traces with tens of millions of instructions or more.
finally we also presented a practical usecase showing how our method can aid in reverse engineering legacy binaries.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.