isense completion aware crowdtesting management junjie wang1 y ey a n g4 rahul krishna5 tim menzies5 qing wang1 1laboratory for internet software technologie s2state key laboratory of computer science institute of software chinese academy of sciences beijing china 3university of chinese academy of sciences beijing china corresponding author .
4school of systems and enterprises stevens institute of technology hoboken nj usa 5department of computer science north carolina state university raleigh nc usa wangjunjie iscas.ac.cn ye.yang stevens.edu rkrish11 ncsu.edu tim menzies.us wq iscas.ac.cn abstract crowdtesting has become an effective alternative to traditional testing especially for mobile applications.
however crowdtesting is hard to manage in nature.
given the complexity of mobile applications and unpredictability of distributed crowdtesting processes it is difficult to estimate a remaining number of bugs yet to be detected or b required cost to find those bugs.
experience based decisions may result in ineffective crowdtesting processes e.g.
there is an average of wasteful spending in current crowdtesting practices.
this paper aims at exploring automated decision support to effectively manage crowdtesting processes.
it proposes an approach named isense which applies incremental sampling technique to process crowdtesting reports arriving in chronological order organizes them into fixed size groups as dynamic inputs and predicts two test completion indicators in an incremental manner.
the two indicators are total number of bugs predicted with capture recapture model and required test cost for achieving certain test objectives predicted with autoregressive integrated moving average model.
the evaluation of isense is conducted on reports of crowdtesting tasks from one of the largest crowdtesting platforms in china.
its effectiveness is demonstrated through two application studies for automating crowdtesting management and semi automation of task closing trade off analysis.
the results show that isense can pro vide managers with greater awareness of testing progress to achieve cost effectiveness gains of crowdtesting.
specifically a median of bugs can be detected with saved cost based on the automated close prediction.
keywords crowdtesting automated close prediction test completion crowdtesting management i. i ntroduction crowdtesting is an emerging paradigm which can improve the cost effectiveness of software testing and accelerate its process especially for mobile applications .
it entrusts testing tasks to online crowdworkers whose diverse testing environments background and skill sets could significantly contribute to more reliable cost effective and efficient testing results .
crowdtesting has been adopted by many software organizations including but not limited to google facebook amazon microsoft .
specifically google regularly deploys crowdtesting for of their major product lines .
a latest report by gartner inc. predicts that crowdtesting will constitute of allenterprise application testing initiatives by .
trade offs such as how much testing is enough are critical yet challenging project decisions in software engineering .
insufficient testing can lead to unsatisfying software quality while excessive testing can result in potential schedule delays and low cost effectiveness.
this is especially true for crowdtesting given the complexity of mobile applications and unpredictability of distributed crowdtesting processes.
in practice project managers typically plan for the close of crowdtesting tasks solely based on their personal experience.
for example they usually employ duration based or participant based condition to close crowdtesting tasks through either a fixed period e.g.
days or a fixed number of participant e.g.
recruiting crowdworkers .
if either of the criteria is met first the task will be automatically closed.
however our investigation on real world crowdtesting data section ii c reveals that there are large variations in bug arrival rate of crowdtesting tasks and in task s duration and consumed cost for achieving the same quality level.
it is very challenging for managers to come up with reasonable decisions.
these experience based decisions could result in ineffective crowdtesting process e.g.
an average of wasteful spending in our experimental crowdtesting platform section ii c .
furthermore crowdtesting is typically treated as a black box process and managers decisions remain insensitive to its actual progress.
this suggests the practical need and potential opportunity to improve current crowdtesting practices.
this paper aims at exploring automated decision support to raise completion awareness w.r.t.
crowdtesting processes and manage crowdtesting practices more effectively.
particularly we leverage dynamical bug arrival data associated with crowdtesting reports and investigate whether it is possible to determine that at certain point of time a task has obtained satisfactory bug detection level.
the proposed completion aware crowdtesting management approach isense1applies incremental sampling tech1isense is named considering it is like a sensor in crowdtesting processes to raise the awareness of the testing progress.
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
percentage of detected bugs050100150200250bug detection speed a bug detection speed10 percentage of detected bugs0100200300400500600700bug detection cost b bug detection cost1 group id of submitted reports0.
.
.
.
.
.0bug detection rate c bug detection rate figure observations on real world crowdtesting data nique to process crowdtesting reports arriving in chronological order organizes them into fixed size groups as dynamic inputs and integrates capture recapture crc model and autoregressive integrated moving average arima model to raise awareness of crowdtesting progress.
crc model is widely applied to estimate the total population based on the overlap generated by multiple captures .
arima model is commonly used to model time series data to forecast the future trend .
isense predicts two test completion indicators in an incremental manner including total number of bugs predicted with crc model and required test cost for achieving certain test objectives predicted with arima model.
isense is eva luated using tasks from one of the largest chinese crowdtesting platforms.
results show that the mre of prediction on total bugs and required cost are both below with about standard deviation.
we further demonstrate its effectiveness through two typical decision scenarios one for automating task closing decision and the other for semi automation of task closing trade off analysis.
the results show that decision automation using isense will provide managers with greater opportunities to achieve cost effectiveness gains of crowdtesting.
specifically a median of bugs can be detected with saved cost based on the automated close prediction.
the contributions of this paper are as follows empirical observations on crowdtesting bug arrival patterns based on industrial dataset which has motivated this study and can motivate future studies.
integration of incremental sampling technique to model crowdtesting bug arrival data.
development of crc based model for predicting total number of bugs and arima based model for predicting required cost for achieving certain test objectives.
isense approach for automated decision support in crowdtesting management including automating task closing decision and semi automation of task closing trade off analysis.
evaluation of isense on reports of crowdtesting tasks from one of the largest crowdtesting platforms in china and results are promising.ii.
b ackground and motiv a tion a. background in general crowdtesting practice managers prepare the crowtesting task including the software under test and test requirements and distribute it on certain online crowdtesting platform.
crowdworkers can sign in their interested tasks and submit crowdtesting reports typically summarizing test input test steps test results etc.
managers usually set up either a fixed period e.g.
days or a fixed number of participant e.g.
recruiting crowd workers for the close criteria of crowdtesting task.
if either of the criteria is met first then the task will be automatically closed.
there are different payout schema in crowdtesting e.g.
pay by report see section vi c for details .
generally the cost of a task is positively correlated with the number of received reports thus with the close time.
the crowdtesting platform receives and manages crowdtesting reports submitted by the crowdworkers.
project managers then inspect and verify each report for their tasks manually or using automatic tool support e.g.
for report labeling .
generally each report will be characterized using two attributes whether it contains a valid bug2 if yes whether it is a duplicate bug that has been previously reported in other reports.
in the following paper if not specified when we say bug o r unique bug we mean the corresponding report contains a bug and the bug is not the duplicate of previously submitted ones.
b. baidu crowdtest dataset our experimental dataset is collected from baidu3 crowdtesting platform which is one of the largest platforms in china.
the dataset contains all tasks completed between may.
1st and jul.
1st .
in total there are mobile application testing tasks from various domains4 with submitted reports.
the minimum average and maximum number of crowdtesting reports and unique bugs per task are and respectively.
2in our experimental platform a report corresponds to either or bug and there is no report containing more than bug.
3test.baidu.com 4details of dataset are in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
c. observations from a pilot study to understand the bug arrival patterns of crowdtesting we conduct a pilot study to analyze three metrics i.e.
bug detection speed bug detection cost and bug detection rate .
for each task we first identify the time when k bugs was detected where k is ranged from to .
then the bug detection speed for a task can be derived using the duration hours between its open time and the time it receives k bugs.
the bug detection cost can be derived using the number of submitted reports by reaching k bugs.
to examine bug detection rate we break the crowdtesting reports for each task into equal sized groups in chronological order.
the rate for each group is derived using the ratio between the number of unique bugs and the number of reports.
in addition for each crowdtesting task we also examine the percentage of accumulated bugs denoted as bug arrival curve for the previous x reports where x ranges from to the total number of reports.
the following bug arrival patterns are observed large v ariation in bug detection speed and cost figure 1a and 1b demonstrates the distribution of bug detection speed and cost for all tasks.
in general there is large variation in bug detection speed and cost.
specifically to achieve the same k bugs there is large variation in both metrics.
this is particularly true for a larger k .
for example when detecting bugs the bug detection cost ranges from to hours and from to reports.
decreasing bug detection rates over time figure 1c shows the bug detection rate of the break down groups across all tasks.
we can see that the bug detection rate decreases sharply during the crowdtesting process.
this signifies that the cost effectiveness of crowdtesting is dramatically decreasing for the later part of the process.
plateau effect of bug arrival curve figure shows typical bug arrival curves for four crowdtesting tasks.
while they differ somewhat note that they all exhibit the same plateau effect after which i.e.
red point in figure new reports find no new bugs.
figure observations on real world crowdtesting data bug arrival curvewe assume the cost spent on these reports after the red point are wasteful spending.
in the experimental tasks there is an average of wasteful spending.
the plateau effect together with the large amount of wasteful spending further suggest the potential opportunity and practical need for introducing early closing mechanism based on the recognition of that plateau to increase cost effectiveness.s figure overview of isense needs of automated decision support in addition an unstructured interview was conducted with the managers of baidu with findings shown below.
project managers commented the black box nature of crowdtesting process.
while receiving constantly arriving reports they are often clueless about the latent bugs or the required cost to find them.
due to lack of situation awareness the management of crowdtesting is conducted as a guesswork.
this frequently results in many blind decisions in task planning and management.
besides managers typically need to handle large number of crowdtesting tasks simultaneously which is very labor intensive and error prone in manual planning and management.
in summary because there is large variation in bug arrival speed and cost section ii c1 current decision making is largely done by guesswork.
this results in low cost effectiveness of crowdtesting section ii c2 and ii c3 .
a more effective alternative to manage crowdtesting would be to dynamically monitor the crowdtesting process and provide actionable decision support for task closing to save unnecessary cost wasting on later arriving reports.
besides current practice suggests a practical need to empower managers with greater visibility into the crowdtesting processes section ii c4 and ideally raise their awareness about task progress thus facilitate their decision making.
iii.
a pproach figure presents an overview of isense.
it consists of three main steps.
first isense adopts an incremental sampling process to model crowdtesting reports.
during the process isense conv erts the raw crowdtesting reports arrived chronologically into groups and generates a bug arrival lookup table to characterize the bug arrival information.
then isense integrates two models i.e.
crc and arima to predict the total number of bugs contained in the software and the required cost for achieving certain test objectives respectively.
finally isense applies such estimates to support two typical crowdtesting decision scenarios i.e.
automating task closing decision and semi automation of task closing trade off analysis.
we will present each of the above steps in more details.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a. preprocess data with incremental sampling technique incremental sampling technique is a composite sampling and processing protocol.
its objective is to obtain a single sample for analysis that has an analytic concentration representative of the decision unit.
it improves the reliability and defensibility of sampling data by reducing variability when compared to conventional discrete sampling strategies.
considering the submitted crowdtesting reports of chronological order section ii a when smpsize smpsize is an input parameter reports are received isense treats it as a representative group to reflect the multiple parallel crowdtesting sessions.
remember in section ii a we mentioned that each report is characterized as whether it contains a bug whether it is duplicate of previously submitted reports if no it is marked with a new tag if yes it is marked with the same tag as the duplicates.
during the crowdtesting process we dynamically maintain a two dimensional bug arrival lookup table to record these information.
table i example of bug arrival lookup table ... sample sample sample sample sample sample sample ... table i provides an illustrative example.
after each sample is received we first add a new row suppose it is row i i n the lookup table.
we then go through each report contained in this sample.
for the reports not containing a bug we ignore it.
otherwise if it is marked with the same tag as existing unique bugs suppose it is column j record 1in row icolumn j. if it is marked with a new tag add a new column in the lookup table suppose it is column k and record 1in row icolumn k. for the empty cells in row i fill it with .
b. predict total bugs using crc background about crc the crc capturerecapture model was firstly used to estimate the size of an animal population in biology .
in so animals are captured marked and released on several trapping occasions.
the number of marked animals that are recaptured allows one to estimate the total population size based on the samples overlap.
it has also been applied in software inspections to estimate the total number of bugs .
existing crc models can be categorized into four types according to bug detection probability i.e.
identical vs. different and crowdworker s detection capability i.e.
identical vs. different as shown in table ii.
model m0 supposes all different bugs and crowdworkers have the same detection probability.
model mh supposes that the bugs have different probabilities of being detected.model mtsupposes that the crowdworkers have different detection capabilities.
model mth supposes different detection probabilities for different bugs and crowdworkers.
table ii capture recapture models crowdworker s detection capability identical different bug detection identical m0 m0 m t mtch probability different mh mhjk mhch mth mth based on the four basic crc models various estimators were developed.
according to a recent systematic review mhjk mhch mtch are the three most frequently investigated and most effective estimators in software engineering.
apart from that we investigate another two estimators i.e.
m0 and mth to ensure all four basic models are investigated.
how to use in isense isense treats each sample as a capture or recapture .
at the end of each capture after updating the bug arrival lookup table isense predicts the total number of bugs in the software5based on current lookup table.
we only demonstrate how it works with mth estimator due to space limit.
for other four estimators one can obtain the estimated bugs in a similar way6.
mth estimator predicts the total number of bugs based on equation .
table iii shows the meaning of each variable how to compute its value based on the bug arrival lookup table in table i. n d c f1 c 2 c f1 summationtextt k 1kfk 2 max d c summationtext kk k fk summationtext summationtext j knjnk table iii v ariables meaning and computation var.
meaning computation based on bug arrival lookup tableexample value n predicted total number of bugspredicted value d actual number of bugs captured so farnumber of columns t number of captures number of rows nj number of bugs detected in each capturenumber of cells with 1in row j3 fk number of bugs captured exactly ktimes in all captures i.e.
summationtextfi dcount the number of cells with 1in each column and denote as ri fkis the number of riwith value k1 c. predict required cost using arima background about arima arima autoregressive integrated moving average model is commonly used to model time series data to forecast the future values .
it extends arma autoregressive moving average model by allowing for non stationary time series to be modeled i.e.
a time series whose statistical properties such as mean variance etc.
are not constant over time.
5to be precise what we predict is the total number of potential bugs that are uncovered by crowdtesting.
6refer to for more details.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a time series is said to be autoregressive moving average arma in nature with parameters p q if it takes the following form yt p summationdisplay i 1 iyt i q summationdisplay i 1 i epsilon1t i epsilon1t whereytis the current stationary observation yt ifor i ... p are the past stationary observations epsilon1tis the current error and epsilon1t ifori ... q are the past errors.
if this original time series zt is non stationary then d differences can be done to transform it into a stationary one yt .
these differences can be viewed as a transformation denoted by yt triangleinvdzt where triangleinvd b dwhereb is known as a backshift operator.
when this differencing operation is performed it converts an arma model into an arima model with parameters p q d .
how to use in isense figure demonstrates how arima is applied in predicting future trend of bug arrival.
we treat the reports of each sample as a window and obtain the number of unique bugs submitted in each sample from the bug arrival lookup table i.e.
number of cells in the row whose value is 1and the corresponding column does not have other .
then we use the former trainsize windows to fit the arima model and predict the number of bugs for the later predictsize windows.
when new window is formed with the newly arrived reports we move the window by and obtain the newly predicted results.
figure illustrative example of arima suppose one want to know how much extra cost is required for achieving certain test objective i.e.
x bugs .
as we already know the predicted total number of bugs section iii b we can figure out how many bugs should be detected in order to meet the test objective suppose it is y bugs.
based on the prediction of arima we then obtain when the number of ybugs can be received suppose it needs extrakireports.
in this way we assume kiis the required cost for meeting the test objective.
d. apply isense to two decision scenarios to demonstrate the usefulness of isense we generalize two typical decision scenarios in crowdtesting management and illustrate its application to each scenario.
automating task closing decision the first scenario that can benefit from the prediction of total bugs of isense section iii b is decision automation of dynamic task closing.
as soon as a crowdtesting task begins isense can be applied to monitor the actual bug arrival constantly updatethe bug arrival lookup table as well as keep tracking of the percentage of bugs detected i.e.
the ratio of the number of submitted bugs so far over the predicted total bugs .
in such scenario different task close criteria can be customized in isense so that it automatically closes the task when the specified criterion is met.
for instance a simple criterion would be to close the task when bugs have been detected in submitted reports.
under this criterion when isense monitors bugs have received and the prediction remains unchanged for successive two captures it determines the time when the last report was received as the close time and would automatically close the crowdtesting task at run time.
note that the restriction of two successive captures is to ensure the stability of the prediction.
isense supports flexible customization of the close criteria.
as an example a task manager can set to close his her tasks when bugs have been detected.
consequently isense will help to monitor and close the task by reacting to these customized close criteria.
semi automation of task closing trade off analysis the second scenario that benefits from the prediction of required cost of isense section iii c is decision support of task closing trade off analysis.
for example suppose bugs have been reported at certain time isense can simultaneously reveal the estimated required cost for detecting an additional x bugs i.e.
in order to achieve a higher bug detection level.
such cost benefit related insights can provide managers with more confidence in making informed actionable decision on whether to close immediately if the required cost is too high to be worthwhile for additional x detected bugs or wait a little longer if the required cost is acceptable and additional x detected bugs is desired.
iv .
e xperiment design a. research questions four research questions are formulated to investigate the performance of the proposed isense.
the first two research questions are centered around accuracy evaluation of the prediction of total bugs and required cost.
presumably to support practical decision making these underlying predictions should achieve high accuracy.
rq1 to what degree can isense accurately predict total bugs?
rq2 to what degree can isense accurately predict required cost to achieve certain test objectives?
the next two research questions are focused on investigating the effectiveness of applying isense in the two typical scenarios section iii d in which isense is e xpected to facilitate current practices through automated and semiautomated decision support in managing crowdtesting tasks.
rq3 to what extent can isense help increase the effectiveness of crowdtesting through decision automation?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
rq4 how isense can be applied to facilitate the trade off decisions about cost effectiveness?
b. evaluation metrics we measure the accuracy of prediction based on magnitude of relative error a.k.a.
mre which is the most commonly used measure for accuracy .
it measures the relative error ratio between the actual value and predicted value expressed as follows mre actual value predicted value actual value it is applied in the prediction of total number of bugs section v a and required cost section v b .
note that although mre has been argued as not the best metric for some applications as effort estimation it is the most popular and widely used metric so we use it in this work.
we measure the cost effectiveness of close prediction section v c based on two metrics i.e.
bug detection level i.e.
bug and cost reduction i.e.
reducedcost .
bug is the percentage of bugs detected by the predicted close time.
we treat the number of historical detected bugs as the total number.
the larger bug the better.
reducedcost is the percentage of saved cost by the predicted close time.
to derive this metric we first obtain the percentage of reports submitted at the close time in which we treat the number of historical submitted reports as the total number.
we suppose this is the percentage of consumed cost and reducedcost is derived using minus the percentage of consumed cost.
the larger reducedcost the better.
intuitively an increase in bug would be accompanied with a decrease in reducedcost.
motivated by the f1 or f measure in prediction approaches of software engineering we further derive an analogous metric f1 to measure the harmonic mean of bug and reducedcost as follows f1 bug reducedcost bug reducedcost to further demonstrate the superiority of our proposed approach we perform one tailed mann whitney u test between our proposed isense and baselines section iv d .
we include the bonferroni correction to counteract the impact of multiple hypothesis tests.
besides the p value for signifying the significance of the test we also present the cliff s delta to demonstrate the effect size of the test.
we use the commonly used criteria to interpret the effectiveness levels i.e.
large .
.
median .
.
small .
.
negligible .
see details in .
c. experimental setup for rq1 we set up checkpoints in the range of receiving to reports with an increment interval of5 in between.
at each checkpoint we obtain the estimated total number of bugs at that time see section iii b .
based on the ground truth of actual total bugs we then figure out mre section iv b in predicting total bugs for each task.
for rq2 we also set checkpoints.
different from rq1 the checkpoints of rq2 is based on the percentage of detected bugs i.e.
from bugs to bugs with an increment of in between.
at each checkpoint we predict the required test cost section iii c to achieve an additional bugs i.e.
target corresponding to the next checkpoint.
for example at checkpoint when bugs have detected we predict the required cost for achieving bugs.
based on the ground truth of actual required cost we then figure out mre section iv b in predicting required cost.
for rq3 we analyze the effectiveness of task closing automation with respect to five sample close criteria i.e.
close the task when or bugs have detected respectively.
the reason why we choose these criteria is that we assume it is almost meaningless to close a task when less than bugs being detected.
for rq4 we use several illustrative cases from experimental projects to show how isense can help trade off decisions.
for all these experiments we employ a commonly used longitudinal data setup .
in detail all the tasks are sorted in the chronological order and we use the former n1 tasks as training set to tune the parameter see details in section iv e and use the nthtask as testing set to evaluate the performance of isense.
w e e xperiment nfrom to to ensure a relative stable performance because a too small training set would bring in bias.
in this way we obtain the performance of test tasks.
d. baselines we compare isense with two baselines.
rayleigh this baseline is adopted from one of the most classical models for predicting the dynamic defect arrival in software measurement.
generally it supposes the defect arrival data following the rayleigh probability distribution .
in this experiment we implement code to fit specific rayleigh curve i.e.
the derived rayleigh model based on each task s bug arrival data then predict the total bugs and required cost for certain test objectives with the future bug trend using the derived rayleigh model.
naive this baseline is designed to employ naive empirical results i.e.
the median value of the dataset.
specifically for the prediction of total bugs it uses the median total bugs calculated based on the tasks of training set.
for required cost it uses the median required cost from training set in terms of the corresponding checkpoint section iv c .
e. parameter tuning for each crc estimator the input parameter is smpsize which represents how many reports are considered in each authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1mre percenta ge of received re portsall results first quarter median third quarter figure mre of predicted total bugs of mth rq1 capture.
we tune the optimal parameter value based on the training set see section iv c and apply it in the testing set to evaluate the performance.
in detail for every candidate parameter value we experiment from to and for each checkpoint we obtain the mre for the prediction of total bugs for each task in the training set.
we then calculate the median of these mre values and sum the median across all checkpoints for each candidate parameter value.
we treat the parameter value under which the smallest sum is obtained as the best one.
for arima model we use the same method for deciding the best parameter value.
v. r esults and analysis a. answers to rq1 accuracy of total bugs prediction table iv demonstrates the median and standard deviation for the mre of predicted total bugs for all five crc estimators.
the columns correspond to different checkpoints and the best two performer under each checkpoint are highlighted in italic font and red color .
we additionally present the detailed performance for mth the best estimator in figure .
from table iv and figure we can see that the predicted total number of bugs becomes more close to the actual total number of bugs i.e.
mre decreases towards the end of the tasks.
among the five estimators m0 and mth have the smallest median mre for most checkpoints.
but the variance ofm0 is much larger than that of mth .
hence estimator mth is more preferred because of its relatively higher stability and accurate prediction in total number of bugs.
in the following experiments if not specially mentioned the results are referring to those generated from isense with mth estimator.
comparison with baselines table v compares the prediction accuracy of isense and the two baselines in terms of the median and standard deviation of mre .
table vi summarizes the results of mann whitney u test for the mre of predicted total bugs between each two methods.
7detailed performance of other four estimators are in wangjunjieiscas cmit shows that isense significantly p value .
and substantially cliff s delta is large outperforms the two baselines especially during the later stage i.e.
after the checkpoint of the crowdtesting tasks.
answers to rq1 isense with the best estimator mth is accurate in predicting the total bugs in crowdtesting and significantly outperforms the two baselines.
specifically the median of predicted total bugs is nearly equal with the ground truth i.e.
mre .
with a standard deviation of less than during the latter half of the process.
b. answers to rq2 accuracy of required cost prediction table vii summarizes the comparison of median and standard deviation of the mre of predicted required cost across isense and the two baselines with columns corresponding to different checkpoints.
we highlight the methods with the best performance under each checkpoint.
table viii presents the results of mann whitney u test between each pair.
as indicated by the decreasing median mre in table vii the prediction of required cost becomes increasingly accurate for later checkpoints.
for example after checkpoint the median mre of predicted cost is lower than with about standard deviation.
this implies that isense can effectively predict the required cost to target test objectives.
comparison with baselines we can see that the median and standard deviation of mre for two baselines are worse than isense during the second half of the task process.
observed from table viii the difference between the proposed isense and two baselines is significant p value .
and substantial cliff s delta is not negligible during the second half of crowdtesting process.
this further signifies the advantages of the proposed isense.
answers to rq2 isense can predict the required test cost within averagely mre for later stage of crowdtesting.
c. answers to rq3 task closing automation figure shows the distribution of bug reducedcost and f1for five customized close criteria.
let us first look at the last series of three boxes in figure which reflects a close criterion of bugs being detected i.e.
most commonly used setup .
the results indicate that a median of bugs can be detected with median cost reduction.
this suggests an additional more cost effectiveness for managers if equipped with such a decision automation tool as isense to monitor and close tasks automatically at run time.
the reduced cost is a tremendous figure when considering the large number of tasks delivered in a crowdtesting platform.
in addition the standard deviation is relatively low further signifying the stability of isense.
we then shift our focus on other four customized close criteria i.e.
and in terms of percentage of detected bugs .
we can observe that for each authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv statistics for mre of predicted total bugs rq1 median m0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mtch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mhch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mhjk .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mth .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
standard deviation m0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mtch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mhch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mhjk .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mth .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table v comparison with baselines in mre of predicted total bugs rq1 median isense .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rayleigh .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
naive .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
standard deviation isense .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rayleigh .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
naive .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table vi results of mann whitney u test for mre of predicted total bugs rq1 isense vs. rayleigh0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
isense vs. naive0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rayleigh vs. naive1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
note that the upper figure within a cell is p value and the lower figure is cliff s delta.
background denotes the effect size of large median small and negligible .
table vii statistics and comparison with baselines for mre of predicted required cost rq2 median isense .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rayleigh .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
empirical .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
standard deviation isense .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rayleigh .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
naive .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table viii results of mann whitney u test for mre of predicted required cost rq2 isense vs. rayleigh0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
isense vs. naive0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rayleigh vs. naive0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
note that the upper figure within a cell is p value and the lower figure is cliff s delta.
background denotes the effect size of large median small and negligible .
close criterion the median bug generated from isense is very close to the targeted close criterion with small standard deviation.
among these close criteria to cost can be saved which further signify the effectiveness of isense.
we also notice that the median bug is a little larger than the customized close criterion.
for example if the project manager hopes to close the task when bugs detected a median of bugs have submitted at the predicted close time.
this implies in most cases the close prediction produced by isense does not have the risk of insufficienttesting.
furthermore we have talked with the project managers and they thought detecting slightly more bugs even with less reduced cost is always better than detecting fewer bugs with more reduced cost .
this is because bug is more like the constraint while reducedcost is only the bonus.
we also analyze the reason for this phenomenon.
it is mainly because before suggesting close our approach requires the predicted total bugs remain unchanged for two successive captures section iii d1 .
this restriction is to alleviate the risk of insufficient testing.
this is also because authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
customized close criteria .
.
.
.
.
.
.0performance bug reducedcost f1 figure task closing automation performance rq3 we treat a sample of reports as the unit during the prediction which can also potentially result in the close time being a little later than the customized close time.
answers to rq3 the automation of task closing by isense can make cro wdtesting more cost effective i.e.
a median of bugs can be detected with saved cost.
d. answers to rq4 trade off decision support trade off between investment and outcomes is important for optimizing the resource allocation.
to reflect such tradeoff context we randomly pick a time and slice the experimental dataset to retrieve all tasks under testing at that time then examine the cost effectiveness of more testing.
figure demonstrates trade off analysis examples across tasks i.e.
p1 p6 generated from repeating the above analysis at four different time points i.e.
corresponding to time1 to time4 in a sequential order .
the y axis denotes the next test objective to achieve while x axis shows the predicted required cost to achieve that objective.
generally speaking in each of the four boxes the crowdtesting tasks to the right are less cost effective than the tasks to the left.
for example at time3 p6is estimated to require additional cost to achieve test objective.
if the manager is facing budget constraints or trying to improve cost effectiveness he she could choose to close p6attime3 because it is the least effective one among all tasks.
to facilitate such kind of trade off analysis on which task to close and when to close we design two decision parameters as inputs from decision maker quality benchmark which sets the minimal threshold for bug detection level e.g.
the horizontal red lines in figure cost benchmark which sets the maximal threshold for required cost to achieve the next objective e.g.
the vertical blue lines in figure .
these two benchmarks split the tasks into four regions at each slicing time as indicated by the four boxes in each subfigure of figure .
each region suggests different insights on the test sufficiency as well as cost effectiveness for more testing which can be used as heuristics to guide actionable decision making at run time.
more specifically cost time1 .
.
.
.
.0test objectives cost time2 .
.
.
.
.
cost time3 .
.
.
.
.
cost time4 .
.
.
.
.
p1 p2 p3 p4 p5 p6 figure decision support for trade off analysis rq4 lower left close later tasks in this quadrant are recommended to be closed later and continuing testing produces low hanging fruits.
for these tasks their quality levels are not acceptable yet and it only requires relatively less cost to improve quality i.e.
to achieve next test objective .
this indicates the most cost effective options and testing should definitely continue.
lower right continue manage tasks in this quadrant have not met the quality benchmark so continue testing is preferred even though they require significant more cost to achieve quality objective.
it also suggests that the task is either difficult to test or the current crowdworker participation is not sufficient.
therefore managers are recommended to take actions to drill down in particular tasks and see if more testing guidelines or worker incentives are needed.
upper left plan to close tasks in this quadrant already meet their quality benchmark however it is very costeffectiveness for reaching next higher quality level i.e with little additional cost.
managers may plan to close all these tasks or identify some high priority ones for further testing and quality improvement.
upper right close now tasks in this quadrant are candidates for closing immediately since they have meet the pre specified quality objectives and will require relatively greater cost to reach next quality level.
it is not practical to continue testing considering the cost effectiveness.
note that the two benchmarks in figure can be customized according to practical needs.
answers to rq4 isense pro vides practical insights to help managers make trade off analysis on which task to close or when to close based on two benchmark parameters and a set of decision heuristics.
vi.
d iscussion a. best crc estimator for crowdtesting in traditional software inspection or testing activities mhjk mhch and mtch have been recognized as the most effective estimators for total bugs .
however in crowdtesting the most comprehensive estimator mth see section iii b1 outperforms the other crc estimators.
this is reasonable because crowdtesting authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is conducted by a diversified pool of crowdworkers with different levels of capability experience testing devices and the nature of bugs in the software under test also vary greatly in terms of types causes and detection difficulty etc.
in such cases mth which assumes different detection probability for both bugs and crowdworkers supposes to be the most suitable estimator for crowdtesting.
b. necessity for more time sensitive analytics in crowdtesting decision support as discussed earlier in the background and motivational pilot study section ii c challenges associated with crowdtesting management mainly lie in two aspects uncertainty in crowdworker s performance and lack of visibility into crowdtesting progress.
we believe there is an increasing need for introducing more time sensitive analytics during crowdtesting process to support better decision making.
isense can generate time based information revealing dynamic crowdtesting progress and provide practical guidelines of trade off analysis.
besides isense pro vides additional visibility into the testing progress and insights for effective task management along with the crowdtesting process.
the experimental results have proven that a significant portion of crowdtesting cost can be saved.
this is extremely encouraging and we look forward to more discussion and innovative decision support techniques in this direction.
c. threats to v alidity first this paper treats the number of crowdtesting reports as the amount of cost when measuring the reduced cost section iv b .
this is applicable for the paid by report schema i.e.
crowdworkers who submit report can get paid which is a commonly used payout schema.
for a second popular schema paid by bug i.e.
crowdworkers who report bug can get paid isense can also reduce the cost by closing the task properly i.e.
fewer bug reports means less cost .
and we believe isense can obtain a comparable performance because the proportion of bugs in the reports almost remain unchanged across the crowdtesting process.
for a third popular schema paid by first bug i.e.
crowdworkers who report the first bug can get paid .
this schema is less sensitive to the automated task closing decision because under this schema the payment to crowdworkers is constant i.e.
the submitted number of unique bugs .
however by closing the task at proper time the platform can potentially reduce the cost for managing the duplicate reports as well as shorten the duration of crowdtesting tasks.
second our designed methods are based on the report s attributes i.e.
whether it contains a bug and whether it is the duplicates of previous ones .
in crowdtesting process each report would be inspected and triaged with these two attributes so as to better manage the reported bugs and facilitate bug fixing .
this can be done manuallyor using automatic tool support e.g.
.
therefore we assume our designed methods can be easily adopted in the crowdtesting platform.
vii.
r ela ted work crowdtesting has been applied to facilitate many testing tasks e.g.
test case generation usability testing software performance analysis software bug detection and reproduction .
these studies leverage crowdtesting to solve the problems in traditional testing activities while some other approaches focus on solving the new encountered problems in crowdtesting.
feng et al.
and jiang et al.
proposed approaches to prioritize test reports in crowdtesting.
wang et al.
proposed approaches to automatically classify crowdtesting reports.
cui et al.
and xie et al.
proposed crowdworker selection approaches for crowdtesting tasks.
this work focuses on the automated decision support for crowdtesting management which is valuable to improve the cost effectiveness of crowdtesting and not explored before.
many existing approaches proposed risk driven or valuebased analysis to prioritize or select test cases so as to improve the cost effectiveness of testing.
however none of them is applicable to the emerging crowdtesting paradigm where managers typically have no control over online crowdworkers dynamic behavior and uncertain performance.
there are also existing researches focusing on defect prediction and effort estimation .
the core part of these approaches is the extraction of features from the source code or software repositories.
however in crowdtesting the platform can neither obtain the source code of these apps nor involve in the development process of these apps.
several researches focused on studying the time series models for measuring software reliability .
among these arima is the most promising one for modeling software failures over time .
this paper used arima in modeling the bug arrival dynamics in crowdtesting and estimating future trend.
viii.
c onclusion motivated by the empirical observations from an industry crowdtesting platform we propose completion aware crowdtesting management approach isense which can raise the awareness of testing progress through two completion indicators and be used to automate the task closing and semi automate trade off decisions.