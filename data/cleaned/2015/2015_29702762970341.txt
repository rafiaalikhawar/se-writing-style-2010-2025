bugram bug detection with n gram language models song wang devin chollak dana movshovitz attiasy lin tan electrical and computer engineering university of waterloo canada ycomputer science department carnegie mellon university usa song.wang dchollak lintan uwaterloo.ca ydma cs.cmu.edu abstract to improve software reliability many rule based techniques have been proposed to infer programming rules and detect violations of these rules as bugs.
these rule based approaches often rely on the highly frequent appearances of certain patterns in a project to infer rules.
it is known that if a pattern does not appear frequently enough rules are not learned thus missing many bugs.
in this paper we propose a new approach bugram that leverages n gram language models instead of rules to detect bugs.
bugram models program tokens sequentially using the n gram language model.
token sequences from the program are then assessed according to their probability in the learned model and low probability sequences are marked as potential bugs.
the assumption is that low probability token sequences in a program are unusual which may indicate bugs bad practices or unusual special uses of code of which developers may want to be aware.
we evaluate bugram in two ways.
first we apply bugram on the latest versions of open source java projects.
results show that bugram detects bugs of which are manually verified as correct of which are true bugs and are code snippets that should be refactored.
among the true bugs cannot be detected by pr miner.
we have reported these bugs to developers of which have already been confirmed by developers of them have already been fixed while the rest await confirmation.
second we further compare bugram with three additional graph and rule based bug detection tools i.e.
jadet tikanga and grouminer.
we apply bugram on java projects evaluated in these three studies.
bugram detects true bugs at least of which cannot be detected by these three tools.
our results suggest that bugram is complementary to existing rule based bug detection approaches.
ccs concepts software and its engineering !automated static analysis software testing and debugging keywords bug detection static code analysis n gram language model1.
introduction software bug detection techniques have been shown to improve software reliability by finding previously unknown bugs in mature software projects .
rule based bug detection approaches infer likely programming rules from source code version histories and source code comments .
these approaches detect violations of these rules as potential bugs.
frequent itemset mining techniques were used to mine rules that capture the co occurrence of methods and variables.
violations of these rules are reported as bugs .
along this line more complex graph models are combined with frequent itemset mining techniques which focus on mining programming rules that capture both method order and control flow information to detect violations of these complex rules .
letabc denote a sequence of calls to the methods a b and c. imagine a contrived program that includes occurrences of the sequence abc two occurrences of abd and a single occurrence ofefg .
existing rule based bug detection approaches such as pr miner jadet tikanga and grouminer infer rules based on the conditional probabilities of method calls for example the conditional probability p cjab which denotes the likelihood of seeing a call to method cafter the sequence of callsab.
in our example p cjab .
this probability is higher than the threshold used by pr miner which is and therefore the potential rule that cshould appear after ab denoted asfab cg is selected as a high probability rule.
the confidence of this rule is its conditional probability which is .
given this rule the sequence abd is flagged as a bug because cinstead of dis expected to follow ab.
it has recently been demonstrated that n gram language models can capture the regularities of software source code .
to take advantage of the n gram language model which provides us with a markov model for tokens we propose an n gram language model based bug detection technique called bugram .
the assumption is that low probability token sequences in a program are unusual which may indicate bugs bad practices or unusual special uses of code of which developers may want to be aware .
while existing studies leverage n grams for detecting clone bugs localizing faults and code search including some that use the term n gram models these studies do not leverage n gram models.
instead they use n grams which are token sequences while n gram models are markov models built on n grams.
on the other hand n gram models have been used for code completion and suggestion fault localization and coding style checking .
the focus of this paper is leveraging n gram models for bug detection which has its own challenges and requires a different design as detailed in section .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
figure bugs detected by bugram versus bugs detected by rule based techniques in the latest version of hadoop instead of using conditional probabilities bugram highlights suspicious call sequences based on their absolute probabilities in the program.
so in the example above our approach evaluates the absolute probability p abc of the full sequence abc which is in contrast to pr miner which evaluates the conditional probability ofp cjab .
the two sequences with the lowest probabilities in the program are efg andabd which have a probability that is markedly lower than that of abc .
by selecting sequences with low absolute probabilities bugram is able to recognize that abd andefg are both suspicious sequences.
notably efg is not recognized as a suspicious sequence by pr miner despite having only a single occurrence in the program because there are no rules with high confidence related to efg .
in addition it is known that even if a rule exists but the rule pattern does not appear frequently enough the rule cannot be learned thus missing many bugs .
more broadly rule based approaches detect bugs from common program patterns while bugram detects sequences which are overall uncommon in the program.
these two approaches target different types of program abnormalities and will ultimately detect different types of bugs as illustrated in figure .
the curve shows the probabilities of sequences in the java project hadoop sorted ascendingly.
the bars depict examples of bugs that can be detected by our n gram based approaches while circles represent examples of bugs that can be detected by rule based approaches.
in this paper we study whether our n gram based approach can detect bugs in real world software that rule based approaches cannot find.
in addition we study whether bugram is more precise than rule based approaches i.e.
whether bugram reports a smaller portion of false bugs than rule based approaches.
.
a motivating example existing rule based techniques detect potential bugs by using mined rules with enough confidence and support the number of occurrences to avoid generating a large number of false bugs.
for example pr miner requires the confidence of a method call sequence to be over and the support larger than to be identified as a rule missing opportunities to detect many bugs.
for example figure shows a real bug detected by our tool in the latest version of pig which has already been confirmed by pig developers.
the code snippet in figure a contains a bug for the purpose of logging the code snippet should convert the object value to a string by calling the tostring method but it does not.
the method call sequence of the buggy code snippet is which appears only once in the program.
a similar but correct code snippet with a method call sequence isdebugenabled a method call sequence from a buggy code snippet appears once isdebugenabled debug indent stringify 1if log.
isdebugenabled log.
debug indent depth converting from pig pigtype value using stringify schema b a similar but correct method call sequence appears three times isdebugenabled debug indent tostring 1if log.
isdebugenabled log.
debug indent depth converting from pig pigtype tostring value using stringify schema figure a motivating example from the latest version .
.
of the project pig.
bugram automatically detected a real bug in a which has been confirmed and fixed by pig developers after we reported it.
debug indent tostring appears three times.
one of the appearances is shown in figure b .
using rule based bug detection approaches such as pr miner a potential rule that may detect this bug is isdebugenabled debug indent tostring .
since pr miner groups method calls into a set which means it ignores the order of method calls for example all other rules that can potentially detect this bug are debug indent tostring isdebugenabled indent tostring debug tostring and .
the confidence of each potential rule is only considering only these four code snippets.
if we consider the entire pig project the confidences of these rules are even lower ranging from .
to .
.
since pr miner requires rules to have confidences at least to avoid detecting too many false bugs it filters out all these potential rules thus missing this real bug.
while it is possible to reduce the confidence requirement the bug detection precision will likely be too low given that already of bugs reported by pr miner are false bugs with the confidence .
different from these rule based bug detection approaches bugram does not use programming rules.
instead it detects potential bugs by reporting method call sequences of low probabilities in a project.
we detect the real bug shown in figure because the method call sequence isdebugenabled debug indent stringify has a low probability of which is the 13th lowest probability of all sequences of length five using a gram model.
.
contributions in this paper we propose bugram to leverage n gram models to detect real world software bugs.
experimental results show that bugram complements existing rule based bug detection approaches.
the contributions of this work are we propose a new bug detection approach called bugram that leverages n gram models.
bugram learns a probability distribution of method call sequences with control flow and uses the probability distribution to detect bugs.
we evaluate bugram on the latest versions of open source projects.
the results show that bugram detects bugs of which are manually verified as correct of which are 709source files tokenizationn gram model buildingbug detectionpotential bugstoken sequencesranked token sequences figure overview of bugram true bugs and are code snippets that should be refactored.
among the true bugs cannot be detected by pr miner.
the detection precision is .
higher than pr miner and other related work.
we have reported these bugs to developers have been confirmed by developers of them have already been fixed while the rest await confirmation.
we compare bugram with three additional graph and rulebased techniques i.e.
jadet tikanga and grouminer.
specifically we apply bugram on the projects evaluated by the three studies.
bugram detects true bugs at least of which cannot be detected by these three tools.
since the projects are not the latest versions we did not report the bugs to developers.
instead we have checked that of the true bugs have already been fixed in a later version.
since jadet is publicly available we have also applied jadet on the projects evaluated above.
jadet reported true bugs while bugram detected true bugs.
our results suggest that bugram is complementary to existing rule based bug detection approaches.
our results show that gram models are the most effective for bug detection among all n gram models with gram sizes from two to ten.
.
background the n gram language model has been widely used in modelling natural language and solving problems such as speech recognition statistical machine translation and other related language problems .
the n gram language model typically has two components words and sentences where each sentence is an ordered sequence of words.
a dictionary dcontains all possible words of a language and each word is represented as w. the language model can build a probabilistic distribution over all possible sentences in a language using markov chains.
the probability of a sentence in a language is estimated by generating the sequence word by word.
the probability of each word in a sentence is only determined by the conditional probabilities of the previous n 1tokens.
given a sentence s w1w2w3 wm its probability is estimated as p s my i 1p wijhi where the sequence hi wi n wiis the history.
in the ngram model the probability of the next word widepends only on the previous n 1words.
for example if the sequence length m is four the probability of the sequence s w1w2w3w4using a gram model is p s p w1 p w2jw1 p w3jw1w2 p w4jw1w2w3 if we use a gram model the probability of token w4depends only on the previous two tokens and the probability of sis p s p w1 p w2jw1 p w3jw1w2 p w4jw2w3 in this work we build n gram models to learn probabilities of using a method given different contexts.
with the learned probability distribution we further calculate the possibility of each token sequence and flag low probability token sequences as potential bugs.
.
approach figure shows the overview of bugram.
in this section we first describe how to parse a project to convert it into tokens section .
and then use the tokens to build n gram models for the project section .
.
finally we present how to leverage the ngram models to detect bugs in the project section .
.
.
tokenization to build n gram models we need to tokenize the source code of a given project.
a main challenge is selecting a suitable level ofgranularity for tokens when building the n gram models.
existing work builds n gram models at the syntactic level using lowlevel tokens to suggest the next tokens for code completion and suggestion .
for example after seeing for int i i n it suggests the tokens i .
building n gram models at this level is likely to only detect syntactic errors e.g.
missing or i in a for loop which will be caught by a compiler.
to detect bugs at the semantic level we need to build n gram models at a semantic level.
bugram selects high level tokens that represent the structure and context of the code using a succinct semantic representation.
take the loop for int i i n i foo i as an example.
bugram will represent it with the following high level tokens for foo end for .
as reported in existing work control flow information is important for the accuracy of bug detection.
inspired by the above work during the tokenization process bugram also considers the control flow information of source code by adding the control flow elements into the token sequences.
therefore we focus on method calls and control flow which are method calls constructors and initializers if else branches for do while foreach loops break continue statements try catch finally blocks return statements synchronized blocks switch statements and case default branches and assert statements.
a method call methoda is resolved to its fully qualified name org.example.foo.methoda to prevent unrelated methods with an identical name from being grouped together.
in addition the type of exception in the catch clauses are considered as they provide important context information to help us infer more accurate contextual information of method sequences.
bugram uses the eclipse jdt core1to tokenize the source files construct the abstract syntax trees asts and resolve the type information for the tokens.
in this work we consider both method andcontrol flow as tokens.
.
n gram model building in this work we use n gram models to learn a probability distribution over token sequences using all extracted sequences.
for every sequence extracted from a method we add all its subsequences to the model.
for example given a token sequence abc extracted from a method we add all of its ordered subsequences i.e.
a b c ab bc andabc to the model.
note that we ignore incontinuous subsequences such as acin this example.
710smoothing is a common process for n gram models to help with handling unknown sequences.
however since the entire source code of a project is being used we have the complete language of all possible sequences.
this means smoothing is unnecessary since there are no unknown sequences.
when building n gram models one important parameter is gram size details are in section .
.
which defines the length of considered token sequences.
n gram models assume that each token depends only on the previous n 1tokens.
to leverage n gram models to generate probabilities of token sequences given a specific gram size n we build a set of internal probabilities .
for example the probability of a token sequence abc calculated by a gram model is p abc p a p bja p cjab .
we refer to p a p bja and p cjab as internal probabilities.
these internal probabilities can be reused to calculate the probabilities of sequences that shared common subsequences.
thus we store internal probabilities to cut down the probability calculation time for building n gram models of different gram sizes.
after obtaining all the internal probabilities for an n gram model we use them to calculate the probabilities of token sequences.
previous studies that leverage n gram models for code completion found that gram gram gram and gram models generated reasonable results.
however the appropriate ngram size for detecting bugs is unknown.
to answer this question we build n gram models with gram size from two to ten to study the impact of gram size on the effectiveness of bug detection.
the algorithm that we use to build the n gram model is standard which is described in section .
.
bug detection bugram detects potential bugs by calculating and ranking the probabilities of all sequences.
after obtaining the probabilities of all sequences bugram ranks them based on their probabilities in descending order then reports sequences with the lowest probabilities as potential bugs.
.
.
configurations a few important factors affect the effectiveness of bugram i.e.
the number of bugs bugram can find.
the four main factors are as follows.
section .
describes the setup tuning and impact of these parameters.
gram size the size of an n gram model.
sequence length the length of token sequences to be considered when building n gram models and detecting bugs.
reporting size the number of sequences in the bottom of the ranked list which will be reported as bugs.
minimum token occurrence the minimum number of times a token must occur in the software to be included in an n gram model.
gram size n. as described in section the gram size is the sizenin an n gram model.
the probability of a sequence is estimated by generating the sequence token by token and the probability of each token is determined by the conditional probabilities using a history of up to n 1tokens.
for example given a token sequence s abcd a gram model considers the probabilities of each two sequential tokens and will calculate its probability with p s p a p bja p cjb p djc .
while a gram model considers the probabilities of each four sequential tokens and calculates its probability as p s p a p bja p cjab p djabc .
in this work we build n gram models with gram size from two to ten to find an appropriate gram size for detecting bugs.
sequence length l. for building n gram models token sequences are extracted from all methods of a project.
the length1string q qqf.
bestqueries body 2for int i i q.length i system.out.
println newline formatqueryastrectopic i q null null figure the filtering based on minimum token occurrence can help bugram avoid reporting this false bug from the latest version of lucene.
of token sequences extracted from different methods varies which can be as small as one and as large as .
breaking these long token sequences into many small sequences may help us obtain fine grained method usage scenarios and detect more bugs.
in this work we evaluate the impact of different sequence lengths on the performance of bugram.
reporting size s. different from rule based bug detection techniques bugram detects bugs by identifying token sequences of low absolute probabilities.
thus an important question is how to set an appropriate threshold to separate sequences that indicate bugs from common sequences that are not bugs.
we use this parameter to determine the bottom ssequences in the ranked list and report them as bugs.
in general a larger sallows bugram to find more bugs at the cost of examining more sequences that potentially indicate bugs.
we expect that as the probabilities increase in the ranked list the percentage of true bugs decreases.
an appropriate sshould help bugram find as many bugs without losing much precision of bug detection.
the task of selecting the parameter sin bugram is the counterpart of selecting a rule probability threshold in rule based bug detection approaches such as pr miner .
as described in section rule based approaches select a high probability rule based on the conditional probability of tokens.
for example if the probability p cjab is higher than the threshold fab cgwill be selected as a rule and occurrences of the sequence ab followed by a call other than cis reported as a bug.
according to the definition of conditional probability p cjab p abc p ab meaning that rule based techniques consider the probability of the sequence abc and compare it to the background probability ofab.
however when the rule fef ggis evaluated the background probability is now that of the sequence ef since the conditional probability is p gjef .
to achieve optimal performance rule based methods should ideally find the individual correct threshold for each background probability.
this is of course not practically feasible which is the reason a single threshold is used in practice.
in bugram we avoid this problem by directly evaluating the probability of the entire sequence.
in this case it is more theoretically sound to select a single threshold.
minimum token occurrence y. after performing the tokenization process described in section .
bugram keeps only tokens with occurrences greater than yin the project.
filtering out uncommon tokens is a standard technique for natural language processing nlp techniques .
in this paper the rationale is that some methods are generally not well used or too unique making them corner cases that are harder to evaluate on a statistical basis.
as such their inclusion leads to generating false bugs.
take the code snippet in figure as an example.
using a gram model the sequence bestqueries println formatqueryastrectopic is ranked at the bottom of all token sequences by their probabilities.
however this token sequence is not a bug.
it has a low probability because it uses two infrequent private methods bestqueries and formatqueryastrectopic each of which is used only once in the whole project.
711to avoid reporting the above false bug bugram performs a token level filtering.
it filters out all tokens that appear fewer than a given y. such process can help bugram avoid reporting many token sequences with low probabilities that are not bugs.
.
.
pruning false bugs bugram identifies token sequences with low probabilities as potential bugs.
however some low probability token sequences are unusual special uses of code and are not bugs they are false bugs for the purpose of bug detection.
to filter out false bugs we reduce the number of reported bugs also called candidate bug set by keeping only token sequences at the bottom of at least two ranked lists generated by different n gram models with different sequence lengths.
the rationale is that if a bug can be detected by multiple ranked lists there is a higher chance that it is a true bug.
remember that given a specific gram size we generate multiple n gram models of different sequence lengths ranging from two to ten.
therefore we only report token sequences that are at the bottom of at least two different n gram models with the same gram size but different sequence lengths.
for example if both sequences abcde andbcd are ranked at the bottom of the list of token sequences and tokensequences respectively bugram identifies them as an overlap two sequences contain a common substring and reports abcde andbcd as one bug.
more formally we obtain a new candidate bug set using the following formula c n t 8i j2m i6 j bottom n t i bottom n t j where c n t is the candidate bug set generated by an n gram model with the reporting size of t.mis the set of sequence length andiandjare two different sequence lengths.
nis the gram size.
bottom n t i is the bottom ttoken sequences generated by an n gram model with sequence length of i. note that denotes the overlaps that are at the bottom of the two different n gram models andsdenotes the union of the overlaps .
.
experimental setup we evaluate bugram in terms of the number of detected bugs and detection precision and explore appropriate parameters for bugram.
all our experiments are conducted on a .0ghz i7 3930k desktop with 64gb of memory.
.
evaluated software we evaluate bugram on widely used open source java projects ranging from thousand lines of code kloc to almost one million lines of code mloc .
table lists their versions numbers of files lines of code loc and numbers of methods.
we used the latest version of each project.
.
parameter setting and sensitivity to build n gram models and detect bugs effectively we need to tune these parameters proposed in section .
.
.
we use three widely used and representative projects from table i.e.
pig hadoop and solr to study the impact of different parameters on the performance of bugram.
specifically we tune three of the four parameters i.e.
different gram sizes different sequence lengths and different reporting sizes.
for minimum token occurrence we remove any token that appears fewer than three times .
in total there are possible combinations of the four parameters.
note that for each combination we need to manually examine the reported bugs for each project which is prohibitively expensive.
to save efforts we only pick the threetable projects evaluated in our experiments project version files loc methods elasticsearch .
geotools rci jedit .
.
proguard .
vuze xalan .
.
hadoop .
.
hbase .
.
pig .
.
solr core .
.
lucene .
.
opennlp .
.
struts .
.
zookeeper .
.
nutch .
.
cassandra .
.
figure impact of the gram size on the number of true bugs detected representative projects to tune the four parameters in total we need to manually examine the reported bugs of combinations in this work.
in practice if users can afford more time they can tune on more projects to obtain an optimal parameter combination of bugram to detect bugs more effectively.
setting gram size .
different gram sizes enable bugram to use different internal probabilities to calculate the probabilities of token sequences.
we build n gram models for each project and the gram size ranges from two to ten.
to evaluate the performance of ngram models of different gram sizes we calculate the probabilities of all token sequences and rank them based on their probabilities in descending order then we examine the bottom and sequences from each n gram model respectively and manually verify whether a token sequence contains a bug or not.
for each n gram model we count the number of real bugs detected in the three projects.
figure shows the results of the three projects combined.
the results show that bugram finds the most number of true bugs with a gram model.
thus in this paper we build gram models for bugram to detect bugs.
setting sequence length .
as described in section .
we break long sequences extracted from a function into small subsequences.
different sequence lengths enable bugram to capture different program scenarios and further affect the performance of bugram.
to evaluate the impact of different sequence lengths we perform bugram with sequence length ranges from two to ten.
for each sequence length we build a gram model then calculate probabilities of all sequences.
based on generated probabilities we rank all sequences.
we examine the bottom sequences with low probabilities to check how many true bugs are detected.
table shows the results of detected true bugs in the bottom sequences with different sequence lengths.
as we can see sequence length can significantly affect the performance of bugram.
712table detected true bugs in the bottom token sequences with different sequence lengths sequence length project pig hadoop solr figure probabilities distribution of all token sequences in hadoop solr and pig n gram models with sequence length ranges from three to eight enable bugram to detect bugs effectively.
for example when the sequence length is equal to five we find seven true bugs on hadoop three on pig and one on solr.
when the sequence length is quite low e.g.
two or quite big e.g.
nine and ten bugram detects no bugs in two of the three examined projects.
thus in this paper sequence length ranges from three to eight.
setting reporting size .
in this work we use this parameter to limit the number of sequences in the bottom of ranked sequence list to be reported as bugs.
an appropriate reporting size might help us identify many true bugs with a small number of false positives.
for each examined project we build gram models with sequence length ranges from three to eight.
we first examine the probabilities of all sequences to explore whether there has a clear cutoff between low probability sequences and high probability sequences.
we normalized the probabilities of all sequences in a project since the range of sequence probability distribution varies in different projects e.g.
in hadoop the sequence probability range is while this range for pig is .
figure shows the normalized probabilities of all sequences ranked by probability the x axis is the number of sequences in different projects.
as we can see the probability curves are quite smooth at the bottom ten thousand sequences.
however it is prohibitively expensive to examine all these sequences.
next we narrow down the reporting size by only looking at the bottom sequences.
specifically for each project we examine how many true bugs are detected when the reporting size is equal to and which means we only examine the bottom and sequences in the ranked list.
in practice if developers can afford more time they can examine more sequences to find more bugs.
the number of detected true bugs and detection precisions of the three projects are shown in figure 7a figure 7b and figure 7c.
as we can see with the increasing of reporting size the number of detected bugs increases while corresponding detection precision declines sharply.
when the reporting size is equal to the corresponding detection precision is smaller than .
in this study we set reporting size equal to which could enable us to detect true bugs with an average detection precision of on the three examined projects.setting minimum token occurrence .
this parameter is the minimum number of times a token is required to appear in a program to be included in sequences.
an appropriate value of this parameter helps filter out token sequences that use unusual special methods thus have low probabilities but are not bugs.
in this study we remove any token that appears fewer than three times.
this is a common practice in nlp research aimed at improving system performance .
.
comparison with existing techniques we compare bugram with five existing graph and rule based approaches.
first we choose the most closely related work prminer .
while comparing with pr miner allows us to compare bugram with an existing approach as is we also want to study the sole impact of using n gram models.
in addition to using n gram models the differences between bugram and pr miner include bugram uses control flow information but pr miner does not and bugram preserves the token order while pr miner ignores the token order.
as discussed in section we use control flow information because it has been shown to be beneficial for bug detection techniques .
therefore our second approach for comparison is identical to pr miner except that it considers both the order of tokens and control flow information.
since pr miner is not publicly available we have reimplemented our own version of it.
we refer to our implementation of pr miner as fim which stands for frequent itemset mining.
we call our implementation of the second approach described above fsm which stands for frequent sequence mining because fsm mines rules with order preserved which is what frequent sequence mining does.
to implement pr miner we follow each step described in we first parse the source code and extract variables method calls classes in a function.
after that we hash selected elements into numbers.
next each function is mapped to an itemset.
then using these itemsets we perform frequent itemset mining as provided by weka to mine frequent itemsets with a specific support and confidence.
finally these frequent itemsets are treated as rules and violations of these rules are identified as bugs.
note that fim does not consider the order of tokens.
therefore given a frequent itemset abc fim may generate many rules e.g fa bcg fb acg fab cg fac bg fc abg andfbc ag.
any violations of these rules will be reported as potential bugs.
while fsm considers the order thus given the same frequent itemset abc fsm generates at most two rules i.e.
fa bcgandfab cg.
the more rules are inferred the more potential bugs are likely to be reported.
thus in practice both the number of generated rules and the number of reported bugs of fim are significantly larger than those of fsm.
to reduce false positives pr miner set the support threshold to and the confidence threshold to .
with these thresholds pr miner reports many potential bugs e.g.
pr miner reported potential bugs in the linux kernel .
to save effort they examined only the top potential bugs ranked by confidence.
these thresholds are only evaluated on three c projects.
we find that these thresholds produce poor results on the java projects used in this paper e.g.
we find true bugs in the top ranked bugs in the three java projects i.e.
hadoop solr and pig.
therefore to set appropriate support and confidence thresholds for fim and fsm we have explored fim and fsm with different combinations of support and confidence on the three projects.
for fim we find that when the support is equal to seven and the confidence is larger than it performs the best on the three projects when examining the top sequences ranked by confidence we have examined up to the top and found the top gives the highest precision and recall .
for fsm when the support is equal to five and the a results of hadoop b results of pig c results of solr figure detection precision and number of detected bugs in the overlaps of bottom stoken sequences with low probability confidence is larger than it performs the best on the three projects.
for a fair comparison we tune all four parameters of bugram on the same three projects and apply the best parameters on the rest of the evaluated projects.
second we compare bugram with three graph and rule based bug detection approaches i.e.
jadet tikanga and grouminer .
these three approaches leverage graph models and frequent itemset mining techniques to mine rules that capture both method order and control flow information to detect bugs.
different from pr miner that was evaluated on c projects all these three approaches were evaluated on open source java projects.
thus bugram can be applied to these projects directly.
since two of these three tools i.e.
tikanga and grouminer are not publicly available to compare with these three approaches instead of implementing our own versions we perform bugram on the projects evaluated by these three approaches and compare our detection results with the results from these three studies.
in addition since jadet is an open source tool we also apply jadet on projects listed in table and compare its detection results with bugram.
.
evaluation measures we manually examine the reported potential bugs and categorize the bugs into three types true bugs refactoring opportunities and false positives .
true bugs are faults and can be fixed by altering the code and correcting its behaviour.
refactoring opportunities are bad practices and can be fixed by refactoring the infrequent code snippets to make them more regular.
any reported bugs that do not fit into the above two groups are considered to be false positives.
we refer to the number of true bugs and refactoring opportunities as true positives .
to evaluate the performance of a bug detection approach we use three measures standard precision relative recall and f1.
note that we use relative recall not standard recall because it is not practical to know all bugs in a project.
the precision is true positive reported potential bugs to calculate the relative recall we first define the relative ground truth as all the unique true positives reported by bugram fim and fsm.
for each of the three approaches we calculate its relative recall as true positive relative ground truth f1 is the harmonic mean of the precision and relative recall.
.
manual examination of reported results following prior work we manually check whether the bugs reported by bugram are true positives.
for manual evaluation a token sequence was only considered buggy if both the first author and a non author graduate student agreed.
given a reported buggy sequence we consider it a bug if it meets one of the following conditions it has obviously incorrect project specific function calls.
for example the true bug in figure for the purpose of log ging developers should convert the object value to a string by calling a local method tostring without such conversion the logged information is the memory address of the object value not its textual information.
this bug has already been confirmed and fixed by pig developers.
it violates common api usages e.g.
exception handling api usages2and log related api usages3.
for exception handling apis we detect several true bugs that violate their usages e.g.
in the true bug in figure b developers did not handle all potential exceptions that might be thrown by method waitforcompletion which may crash the system if any of these exceptions are thrown.
for log related apis one common usage is that the checked log level and the used log level should be the same.
several of our detected true bugs violate this usage e.g.
in the true bug in figure c before calling the method info to log messages instead of checking whether the log level info is available developers checked whether debug is available which is incorrect.
we consider a reported buggy sequence a refactoring issue based on principles of code smells proposed in e.g.
duplicated code when two code fragments look almost identical.
duplicated code could hinder maintenance because developers need to track and modify each repeating fragment.
developers could refactor the duplicated code by extracting it into a function.
.
experimental results this section presents the results of detected bugs section .
and section .
including the comparison with existing graph and rule based techniques detected bug examples section .
and the execution time of bugram section .
.
.
comparison with fsm and fim table shows the number of bugs detected by bugram fsm and fim on each evaluated project.
in total bugram reported potential bugs of which are correct and useful true bugs and refactoring opportunities.
we have reported these true bugs to developers of which have already been confirmed by developers while the rest await confirmation.
the results suggest that bugram is effective in finding real bugs in widely used mature software projects to improve software reliability.
as described in section .
the relative recall shows the ability of a technique in finding new bugs while the precision indicates the ability of a technique in avoiding reporting false bugs.
f1 is the harmonic mean of the precision and recall.
the relative recall of bugram is .
which is higher than fim s relative recall of .
and fsm s relative recall of .
.
the detection precision of bugram is .
which is higher than fim s precision of .
714table bug detection results.
reported is the number of reported bugs tbbugs is the number of true bugs and refs is the number of refactoring opportunities.
we manually inspect all reported bugs except for fim whose inspected column shows the number of bugs inspected.
numbers in brackets are the numbers of true bugs detected by bugram that are detected by neither fim nor fsm.
bugram fsm fim project reported tbugs refs reported tbugs refs reported inspected tbugs refs elasticsearch geotools jedit proguard vuze xalan hadoop hbase pig solr core lucene opennlp struts zookeeper nutch cassandra total relative recall .
.
.
precision .
.
.
f1 .
.
.
and fsm s precision of .
.
the f1 of bugram is .
again higher than fim s f1 of .
and fsm s f1 of .
.
the results suggest that bugram can find more bugs than examined rulebased approaches and is more precise than them suggesting bugram complements existing rule based bug detection techniques.
in addition as shown in table among the true bugs detected by bugram only two can also be detected by fim and fsm.
the majority of the true bugs can only be detected by bugram showing that bugram can find real bugs in real world software that examined rule based approaches cannot find.
in comparison fim detected eight true bugs and refactoring opportunities.
fsm reported potential bugs and nine of which are true bugs and are refactoring opportunities.
since six true bugs are detected by both fim and fsm a total of unique true bugs are detected by these two approaches nine of which cannot be detected by bugram.
in total there are unique true positives generated by the three approaches.
since fsm considers both the control flow and order of tokens both the numbers of rules and bugs discovered by fsm are much smaller than those of fim.
table shows that fim reported a total of potential bugs while fsm only reported potential bugs.
as we described in section bugram and fim detected bugs based on different probability distributions of token sequences.
bugram identifies token sequences with absolute low probability as bugs while fim and fsm identify token sequences with relatively low probability as bugs.
a relatively low probability token sequence might have a high absolute probability with a high rank in all token sequences extracted from a project.
thus our results suggest that bugram and rule based bug detection techniques complement each other to detect more bugs.
.
comparison with jadet tikanga and grouminer as described in section .
we also compare bugram with three graph and rule based bug detection approaches i.e.
jadet tikanga and grouminer .
these approaches have been evaluated on java projects and their authors have presented the number of detected potential bugs and manually identified true bugs.
since jadet tikanga and grouminer each reported many potential bugs for the evaluated projects to save effort thetable comparison with jadet tikanga and grouminer.
fixed denotes the number of true bugs detected by bugram that have already been fixed in later versions.
denotes the number of unique true bugs detected by bugram that the tools in comparison failed to detect.
graph based tools bugram project jadet reported tbugs fixed azureus .
.
columba .
aspectj .
.
project tikanga bugram aspectj .
.
tomcat .
.
argouml .
vuze 3. .
.
columba .
project grouminer bugram columba .
ant .
.
log4j .
.
aspectjrt .
.
axis .
jedit .
jigsaw .
.
struts .
.
authors of the three tools manually verified a subset of reported potential bugs i.e.
top in jadet top in tikanga and top in grouminer.
for a fair comparison we apply bugram on the projects that are evaluated by these approaches and use the same bugram parameters that are used in the comparison with pr miner meaning that bugram parameters are not tuned for these projects.
jadet was evaluated on five projects tikanga was evaluated on six projects and grouminer was evaluated on nine projects.
we exclude four projects which are not publicly available anymore.
in total projects unique are available .
table shows the detection results of bugram and these three bug detection approaches.
jadet detected three true bugs on the three projects.
bugram detected five true bugs on the same projects at least one of which cannot be detected by jadet.
since these papers did not report the full list of detected bugs we do not know if the bugs detected by jadet and bugram overlap.
however since jadet detected true bugs in columba while bugram detected one bug we know that bugram detected one bug that jadet can715not detect.
we also found that all five true bugs detected by bugram are fixed in a later version by developers.
for the same reason as above we do not know if the bugs detected by jadet have been fixed in a later version.
tikanga detected true bugs on the five projects while bugram detected six true bugs on the same projects four of which are unique to bugram detected in tomcat .
four of the six true bugs have already been fixed.
grouminer detected five true bugs on the eight projects while bugram detected true bugs on the same projects five of which are unique to bugram detected in log4j andaxis .
eight of the true bugs have already been fixed.
in total the three approaches detected true bugs in the projects while bugram detected true bugs and we have manually checked that of them have already been fixed in a later version.
in addition at least of the true bugs cannot be detected by these three tools.
since jadet is an open source tool we further apply jadet with recommended parameters on the projects listed in table .
results shown that jadet did not detect any true bugs the top potential bugs detected by jadet are missing method calls of ja v a library classes e.g.
map list iterator etc.
jadet tikanga and grouminer are based on object usage graph models so rules generated by them are method usages of classes both library classes and project specified classes .
for example one of jadet s representative rules is the method iterator.next should always follow the method iterator.hasnext .
violations of this rule will be flagged as potential bugs.
however it is not necessary to use both the two methods in every scenario.
thus it is possible for jadet to report large numbers of false positives related to the java library classes listed above.
the comparison results show that bugram is complementary to these graph and rule based approaches.
in addition the detection precision of bugram is better than these three techniques.
jadet reported that three of the potential bugs detected in the three projects listed in table are true bugs.
thus jadet has a detection precision of .
while bugram achieves a precision of .
on the same projects.
tikanga s authors manually examined potential bugs on the five projects of which are true bugs indicating a detection precision of .
.
while bugram achieves a precision of .
on the same five projects.
similarly grouminer has a detection precision of .
on the eight projects while bugram achieves a precision of .
on the same projects.
.
examples example bugs .
we show some of the detected true bugs in figure .
specifically figure shows three examples of detected true bugs in proguard and nutch that are detected by our tool.
fim and fsm fail to detect them.
we reported the three bugs to the proguard and nutch developers and all of them have been confirmed as true bugs.
in addition the bugs in figure a and figure c have already been fixed by developers.
the bug in figure a is caused by an incorrect api usage.
specifically the instantiation of configurationwriter writer should be closed in a finally block instead of a try block.
to fix this bug instead of closing the writer in the try block developers added a finally block and closed thewriter in it.
figure b also shows a true bug.
the method waitforcompletion might throw several exceptions e.g.
classnotfoundexception andioexception while in this case developers used this function without handling these potential exceptions.
to fix this bug developers should either use a catch block to handle the exceptions or raise the exceptions to be handled by the calling functions.
figure c shows another a a confirmed and fixed true bug from proguard bugid .
1try 2configurationwriter writer new configurationwriter file 3writer .write getproguardconfiguration 4writer .close 5catch exception ex ... b a confirmed true bug from nutch bugid nutch .
1try currentjob.
waitforcompletion true finally ... ... c a confirmed and fixed true bug from nutch bugid nutch2256 .
1if log.
isdebugenabled 2log.
info crawl delay for queue ... figure true bug examples from version .
of proguard a and version .
.
of nutch b and c 1byte dt1 bb1.get 2byte dt2 bb2.get 3switch dt1 4case binintersedes.biginteger 5if 6int sz1 readsize bb1 bb1.get 7int sz2 readsize bb2 bb2.get ... figure a refactoring bug example from version .
.
of pig true bug.
this bug is caused by the inconsistency between the checked log level debug and the used log level info .
to fix this bug developers replaced the method info with the method debug to make it consistent with the checked log level.
example refactoring opportunities .
figure shows an example of refactoring opportunity detected by our tool from pig project.
in this example developers first define two variables dt1 anddt2 to keep data via method call get of objects bb1 and bb2.
next in the switch block one of the case branch needs data from objects bb1 andbb2 while such data already kept in variables dt1 anddt2.
without reusing these two variables developers call get of objects bb1 andbb2 to obtain data again.
this costs extra memory and time and should be refactored by using variables dt1 anddt2 directly in lines and .
some of our detected bugs may appear to be simple but many of them have been confirmed or fixed confirmed and fixed confirmed with patches proposed confirmed by the developers of these projects suggesting the value of our approach.
.
execution time and space we collect the time and space costs for all the projects listed in table and details are presented in table .
we can see that the total execution time for tokenization model building and bug detection varies from to seconds.
our largest evaluated project geotools uses 2gb of memory.
as shown in the table most of the time is spent building asts with type information with a fraction of the time on building n gram models and detecting bugs.
the results demonstrate bugram s practical value.
.
threats to v alidity implementation of pr miner .
to compare bugram with rule based bug detection approaches we have reimplemented 716table execution time in seconds project total tokenization model building and bug detection elasticsearch geotools jedit proguard vuze xalan hadoop hbase pig solr core lucene opennlp struts zookeeper nutch cassandra a rule based approach pr miner since pr miner is not publicly available.
the pr miner paper reported higher precision than what we reported with our implementation of pr miner in this paper.
one possible reason is that pr miner has only been evaluated on c projects.
its false positive pruning approach recommended threshold values of support and confidence may only be effective on c projects while in this study we evaluate on java projects.
however for our implementation of pr miner we have tried our best to tune parameters e.g.
threshold values of support and confidence to obtain the best results.
this is our best effort given that pr miner is not publicly available.
our comparison is fair since both bugram and pr miner are tuned and evaluated on the same projects.
bugs are verified by the authors .
following prior work we manually check whether the potential bugs reported by the tools are true positives.
although this approach is a common practice this process contains bias since the authors of this paper are not the developers of these projects.
we mitigate this threat by sending the bugs to developers for further confirmation which can take a long time.
so far have already been confirmed as true bugs by developers.
.
related work statistical language models .
statistical language models have been successfully used for tasks including code completion fault localization and coding style consistency checking .
hindle et al.
leveraged n gram language models to show that source code has high repetitiveness.
han et al.
presented an algorithm to infer the next token by using a hidden markov model.
pradel et al.
proposed an approach to generate object usage specifications based on a markov model.
yusuke et al.
leveraged n gram models to generate pseudo code from software source code.
ray et al.
used n gram models to study language statistics of buggy code which showed that software buggy lines are more unnatural than non buggy lines.
they also proposed a defect prediction model based on n gram models.
specifically they built n gram models on an old version of a software project and then used entropy from the n gram models to estimate the naturalness of the source code lines in a later version.
source code lines with higher entropy values are flagged as buggy lines.
there are three main differences between their approach and bugram.
first given a software project bugram directly builds n gram models on it and detect bugs in this project while their tool requires an old version of this project as training data.
second they build n gram models at the token level while we build n gram models at a higher level e.g.
statements method calls and control flows section .
aiming to detect semantic bugs more effectively.
third their approach leverages entropy while bugram uses probability to detect bugs.
using probability and entropy to rank token sequences aretwo different approaches .
the entropy used in combines probability and sequence length.
it may worth comparing using entropy versus probability for detecting bugs in the future.
white et al.
and raychev et al.
investigated the effectiveness of language models i.e.
n gram and deep learning models for code completion.
movshovitz attias et al.
leveraged n gram models to predict class comments for program source file documents.
campbell et al.
built n gram models with historical correct source code to locate the cause of syntax errors.
some studies used n gram token sequences instead of n gram models to solve software engineering tasks.
nessa et al.
and yu et al.
leveraged n gram token sequences to help software fault localization.
hsiao et al.
proposed to use n gram token sequences and tf idf style measures to detect code clone and related bugs.
in contrast bugram is not limited to detecting clone bugs.
rule mining and defect detection .
many techniques have been developed for programming rule mining and bug detection .
engler et al.
shown that checking the inconsistent programmers beliefs can be an effective approach to detect real bugs.
li et al.
developed pr miner to mine programming rules from c code and detect violations of these rules.
chang et al.
proposed an approach to mine rules for detecting neglected conditions.
wasylkowski et al.
proposed jadet which combined frequent itemset mining and object usage graph models to detect object usage anomalies.
gruska et al.
extended jadet by mining object usages from over projects.
wasylkowski et al.
proposed tikanga which combined jadet with model checking and concept analysis to learn and check operational preconditions.
nguyen et al.
extended jadet by mining the usage patterns of multiple objects.
different from existing rule based bug detection tools bugram detects bugs by calculating and ranking the probabilities of token sequences based on the probability distribution of tokens in a project.
.
conclusions and future work this paper introduces bugram that leverages n gram models to detect bugs.
bugram detects potential bugs via calculating and ranking the probabilities of program tokens based on the probability distribution of program tokens in a project.
low probability token sequences are flagged as potential bugs.
we evaluate bugram in two ways.
first we compare it with two rule based bug detection approaches on projects.
results show that bugram detects true bugs and of which cannot be detected by pr miner.
second we further apply bugram on projects evaluated in three graph and rule based tools i.e.
jadet tikanga and grouminer.
bugram detects true bugs at least of which cannot be detected by these three tools.
our results suggest that bugram is complementary to existing rule based bug detection approaches.
in the future we plan to build n gram models from multiple projects to perform cross project bug detection which may help us find more bugs more accurately.
we also plan to explore different approaches to segment sequences when building n gram models.
currently bugram breaks a token sequence from a method into sequences of fixed lengths.
it would be promising to break sequences at the boundaries of code s semantic blocks.
in addition we plan to extend bugram to c c projects and combine bugram with rule based approaches to detect more bugs.
.