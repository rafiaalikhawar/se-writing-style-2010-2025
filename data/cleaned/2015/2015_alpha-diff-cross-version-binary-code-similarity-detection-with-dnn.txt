diff cross versionbinarycodesimilaritydetectionwithdnn bingchang liu liubingchang iie.ac.cn institute of information engineering chinese academy of scienceswei huo huowei iie.ac.cn institute of information engineering chinese academy of scienceschao zhang chaoz tsinghua.edu.cn institute for network science and cyberspace tsinghua university wenchao li institute of information engineering chinese academy of sciencesfeng li institute of information engineering chinese academy of sciencesaihua piao institute of information engineering chinese academy of sciences wei zou zouwei iie.ac.cn institute of information engineering chinese academy of sciences abstract binary code similarity detection bcsd has many applications including patch analysis plagiarism detection malware detection and vulnerability search etc.
existing solutions usually perform comparisons over specific syntactic features extracted from binary code based on expert knowledge.
they have either high performance overheads or low detection accuracy.
moreover few solu tions are suitable for detecting similarities between cross version binaries which may not only diverge in syntactic structures but also diverge slightly in semantics.
in this paper we propose a solution diff employing three semantic features to address the cross version bcsd challenge.it first extracts the intra function feature of each binary function usingadeepneuralnetwork dnn .thednnworksdirectlyon raw bytes of each function rather than features e.g.
syntactic structures providedbyexperts.
difffurtheranalyzesthefunction callgraphofeachbinary whicharerelativelystableincross version binaries and extracts the inter function and inter module features.
then a distance is computed based on these three features and used for bcsd.
we have implemented a prototype of diff and evaluated it on a dataset with about .
million samples.
the result shows that diff outperforms state of the art static solutions by over percentages on average in different bcsd settings.
ccs concepts securityandprivacy softwarereverseengineering computing methodologies machine learning also with school of cyber security university of chinese academy of sciences.
corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september montpellier france copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
code similarity detection dnn acm reference format bingchang liu wei huo chao zhang wenchao li feng li aihua piao and wei zou.
.
diff cross version binary code similarity detection with dnn .
in proceedings of the 33rd acm ieee international conference on automated software engineering ase september montpellier france.
acm new york ny usa 12pages.https introduction given two binary functions the problem of evaluating whether theyaresimilariscalledbinarycodesimilaritydetection bcsd .
it plays an important role in many applications including code plagiarismdetection andmalwarefamilyandlineage analysis .
it could also be used to analyze day i.e.
patched vulnerabilities or summarize vulnerability patterns when applying bcsd on pre patch and post patch binaries.
moreover itcouldevenbeusedincross architecturebugsearching whenapplyingbcsdonaknownbugandtarget applications.
however bcsdfacesseveralchallenges.first differentcompiler optimizations yield cross optimization binaries.
second compilers withdifferentalgorithms e.g.
registerallocation generate crosscompilerbinaries.third thesourcecodecompiledondifferentplatforms e.g.
with different instruction sets yields cross architecture binaries.thesebinariesaresemantic equivalent buthavedifferent syntactic structures.
on the other hand the source code itself may evolveovertime e.g.
beingpatched yielding cross version binaries.
thesebinariesbynaturearesimilar becausetheyhaveasameroot.
buttheyhavedifferentsyntacticstructuresandslightlydifferent semantics.
existing solutions could address these bcsd challenges to some extent but perform poorly in cross version binaries.
state of the artbcsd solutionsheavilyrely onaspecificsyntactic feature of binary code i.e.
control flow graphs cfgs offunctions.
the most widely used tool bindiff utilizes graphisomorphism gi theory tocomparefunctions cfgs.however gi algorithms are time consuming and lack polynomial time authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france b. liu w. huo c. zhang w. li f. li a. piao w. zou solutions.
moreover gi is vulnerable to even minor cfg changes andthushasalowaccuracy.binhunt andibinhunt e xtendgiwithsymbolicexecutionandtaintanalysistoaddressthese challenges but still have low accuracy and high overheads.
bingo esh and cabs provide more resilience to cfg changes by computing the similarities of cfg fragments and composingtheoverallcfgsimilarity.discovre providesbetter performance by employing a filter on cfgs to reduce the number ofgicomparisons.itextractssomenumericfeaturesfromcfgs e.g.
countsofinstructionsorbasic blocks bbs andusetheknn algorithm to pre filter similar cfgs.
genius extracts similar numeric attributes from bbs and use them to augment cfg nodes and get attributed cfgs acfgs to support cross architecture bcsd.gemini usesanend to endneuralnetworktoembed acfgs providing better performance and accuracy.
thesesolutionsallrelyonthesyntacticfeature i.e.
cfgs.these featuresarederivedfromexpertknowledge whichcouldintroducebiassometimes.forexample cfgscouldchangedramaticallyeven if there is none or minor code changes and cause noticeable deviations in the bcsd results.
the first research question addressed in this paper is rq1 how to extract features from binary code with as little human bias as possible?
few solutions consider the semantics of the binary code except bingo andesh .thesetwousetheoremprovingtocheck semanticequivalence ofcfgfragments andthusarecomputationallyexpensive.ontheotherhand thesemanticsofcross version binariesmaychangeslightly e.g.
duetopatching.so strictsemanticequivalencecomparisonsarenotsuitableneither.thesecond research question addressed in this paper is rq2 how to efficiently utilize semantic features to improve the accuracy of bcsd?
cross version bcsd is demanded for two decades e.g.
in patch analysis and knowledge transfer .
it is also one of the most attractive functionalities provided by the popular tool bindiff .
however thisproblemisfarfrombeingsolved.forexample the average accuracy of bindiff is less than .
when comparing coreutils .
with coreutils .
that consist of hundreds of binaries.
but researchers have paid few attentions to this specific topic.
the thirdresearchquestionaddressedinthispaperis rq3 howtobuild a solution fit for cross version bcsd?
inthispaper weproposeasolution difftoaddresstheaforementioned questions.
in short it extracts proper semantic features frombinaries andusesthemtocomputesimilarityscorestoperform bcsd.
to fit for cross version bcsd each binary function is characterizedasthreesemanticfeatures i.e.
thefunctioncode s i.e.
intra function features function invocation i.e.
inter function features and module interactions i.e.
inter module features.
first tocharacterizeafunction sintra functionfeature wedo not use its cfg or other attributes derived from expert knowledge.
we notice that the raw bytes contain all semantic information ofthefunction andneuralnetworks couldautomaticallyretrieve unbiasedfeaturesfromthem.thus weproposeaneuralnetworktoextractfeaturesfromthefunction srawbytes inspiredbyprevious works .morespecifically werepresenttherawbytesasamatrix and use convolutional neural network cnn to convert it intoan embedding i.e.
a vector .
further in order to ensure similar functions embeddings are close to each other we embed this cnnintoasiamesenetwork i.e.
apopularsolutionusedinfine grained visual similarity recognition .
second wenoticethatsimilarfunctionshavesimilarcallgraphs but not the opposite.
so we analyze each function s call graph to extract its inter function feature.
ideally the whole call graph shouldbeconsidered.butinoursolution weextractonlytheindegreeandout degreeofafunctionnodeinthecallgraphasthe function s feature for performance reason.
third wealsonoticethatsimilarfunctionshavesimilarimported functions even in different architectures but not the opposite.
so weanalyzeeachfunction simportedfunctionsetanduseitasintermodule feature.
a specific algorithm section .
i sp r o p o s e dt o embed this set into a vector to support distance computation.
so given any two binary functions we could extract their intrafunction inter function and inter module features.
then we could compute their distances in terms of each feature respectively.
finally wewillmergethesethreedistancestomeasuretheoverall similarity of these two functions.
we have implemented a prototype of our solution diff and evaluated it on a custom dataset consisting of about .
millions pairs of cross version functions which are collected from public repositories.theresultsshowedthat diffoutperformsbindiffby 11percentageonaverage upto52 forsomebinarypairs.with only the intra function feature diff outperforms bindiff by percentage on average up to for some binary pairs.
more importantly although our training data is composed of cross versionbinaries ourmodelisalsogoodforcross compiler andcross architecturebinarycodesimilaritydetection aswellasin a specific application i.e.
vulnerability search.
the results showed that diff in general outperforms state of the art solutions.
overall we made the following contributions we proposed a neural network solution to extract intra function semantic features from raw bytes of binary functions withoutinterferenceofexpertknowledge.together withtwootherproposedsemanticfeatures i.e.
inter function and inter module features we built an end to end system diff able to perform cross version bcsd.
we built a labelled dataset for deep learning which contains66 823pairsofbinariesandabout2.5millionpairsof functions.researcherscanfreelyusethisdataset1 todesign other neural network models and solve other problems.
we developed a prototype diff and evaluated it on this dataset.
the results showed that it outperforms state of theart solutions in all of cross compiler cross architecture and cross version bcsd settings.
problem definition in this section we will introduce the definition of the crossversion bcsd binary code similarity detection problem.
.
notation and assumption weassumeallbinariesarecompiledfromsourcecodewrittenin high levellanguages notassembledfromhand writtenassembly or generated by packers which aim at obfuscating the binaries.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diff cross version binary code similarity detection with dnn ase september montpellier france tobepractical wealsoassumethedebugsymbolsinbinariesare stripped which makes binary analysis more challenging.
a binary biconsists of a set of functions fi1 fi2 ... fin.
binary function identification which is out of the scope of this paper couldbehandledwellbyexistingbinarydisassemblysolutions.we herebyassumeeachbinary sfunctionscouldbeidentifiedcorrectly i.e.
all bytes of each function fijin a binary bican be determined.
acoretaskofbcsdistofindeachfunction smatchingcounterpart.
two binary functions are considered as matching if they are compiledfromfunctionswiththesamename includingnamespace and class etc.
and used in similar contexts.
it is worth noting that identical functions i.e.
with same raw bytes are matching but matching functions could be non identical.
.
cross version bcsd problem thecross versionbcsdproblem focusesonanalyzingtwobinariesb1andb2compiled from a same source code project which could evolve over time.
it is related to the following tasks function matching for each function f1iin a binary b1 i f exist find its match f2jin the other binary b2.
similarityscore foreachpairoffunctions f1iandf2j computeasemanticsimilarityscorerangingfrom0to1between them indicating how likely they are similar to each other.
difference identification for each matching pair of functions f1iandf2j identifytheexactdifferencesintheircodebytes if their similarity score is less than i.e.
non identical .
in this paper we only focus on the first tasks.
.
variant bcsd problems inthispaper weaimtosolvethechallengesinthecross version bcsd problem which is more challenging than other bcsd problems.
as the evaluation results showed our solution could be used directlyforthefollowingvariantsettingsandreceivedgoodresults.
cross optimizationbcsd itaimsatanalyzingtwobinaries compiledfromasamecopyofcode usingasamecompiler but with different compilation optimizations.
cross compiler bcsd it aims at analyzing two binaries compiled from a same copy of code using different compilers e.g.
different vendors.
.
cross architecture bcsd it aims at analyzing two binariescompiled from a same copy of code targeting different architectures e.g.
with different instruction sets .
.
evaluation metric the goal of bcsd solutions is identifying matching functions accurately.wethusevaluatewhetherthematchingfunctionisin the topkmatching candidates reported by a given bcsd namely recall k similar to related works giventwobinaries b1 f11 f12 ... f1nandb2 f21 f22 ... f2m for simplicity we assume they have tpairs of matching functions i.e.
f11 f21 f12 f22 ... and f1t f2t respectively.
the rest of functions do not match.
for any function f1iinb1 the bcsd solution could sort functions in the other binary b2 based on their similarities with f1i.
we denote the top k similar functions as topk f1i and denotehit k f1i as whether f1i s matching function exists in topk f1i .
hit k f1i braceleftbigg f2i topk f1i andi t otherwise the evaluation metric of bcsd is thus defined as follows.
recall k b1 b2 t summationtext.
i 1hit k f1i t approach in this section we present the key idea of our solution to the problem of cross version binary code similarity detection.
.
overview traditionalsolutionsbasedonsyntacticattributesareinadequate for cross version bcsd.
the similarity of two cross version binary functions should be estimated by their semantics i.e.
their raw bytes their relationships with other functions defined in the same binaries andtheirrelationshipstoimportedfunctionsdefinedin externalmodules.forsimplicity wenamethesefeaturesasintrafunction inter function and inter module features respectively.
as shown in figure our solution diff first extracts these featuresfromtwobinaryfunctions thencalculatesthedistances between each pair of features and finally evaluates an overall simi larityscorebasedonthesethreedistances.thefinalscoreindicates the similarity between the two functions.
unlike traditional solutionswhich use cfg and othersyntactic attributesasfeatures weapplyadeepneuralnetworktodirectly extract intra function features from each function s raw bytes.
an embedding is generated by this neural network to represent the binary function s semantic feature.
furthermore we use the call graph cg to characterize the inter function semantic feature and use the imported function invocation relationship to characterize the inter module semantic feature.thesetwofeaturescouldbeextractedfrombinarieswith traditional lightweight program analysis.
.
intra function semantic feature inspired by previous binary analysis solutions we also utilize a neural network to extract intra function semantic featuresfrom the raw bytes of binary functions.
after many trials e.g.
conv1d lstm and convolutional lstm we find out the convolutional neural network cnn is the best fit.
in our solution the cnn takes the raw bytes of a function iq as input and maps it to an embedding f iq i.e.
a vector in a d dimensional euclidean space rd.
then we could calculate the distance between any two functions using their embeddings.
inordertodetectsimilarity wehavetotrainthemodeltosatisfy thefollowingrequirement.
rq thedistancebetween embeddings of two similar functions should be small while the distance between embeddings of two dissimilar functions should be large.
inspired by deep metric learning we also embed two identical cnns into a siamese architecture to comply with the requirement rq and train the cnn s parameters.
unlike the recent work gemini which generates embeddings with dnn authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france b. liu w. huo c. zhang w. li f. li a. piao w. zou pre processcnn tuned with siameseraw bytes of input funcin out degree in call graph of input funcimported functions of input func and binaryfunc bin1 bin2 euclid distanceeuclid distance binary func1binary func2intra func distanceinter func distanceinter mod distanceintra func feature intra func feature 2inter func feature inter func feature 2inter mod feature inter mod feature function similarity score figure overview of the cross version binary code similarity detection solution diff.
based onsome engineered syntacticfeatures our solutiondoes not require expert knowledge and is suitable for cross version bcsd.
.
.
embeddingfunctionswithcnn.
theconvolutionalneural network cnn is a specific kind of neural network for processing data that hasa known grid like topology .
it hasachieved great success in many applications e.g.
alexnet .
however classical cnns are specifically designed for image classification requiring inputs similar to rgb images which have atleast3channels.thisisnotsuitableforourproblemscopeandour input format.
after many trials we design a new cnn as follows.
networkstructure.
thecnnthatweproposeconsistsof8convolutional layers batch normalization layers max pooling layers and2full connectedlayers.thewholemodelusesrectifiedlinear units i.e.
relu as the non linear activation function.
in total there are more than .
million parameters in this network2.
network i o. this cnn takes a tensor tas input andoutputsa64 dimensionalvector i.e.
embedding .wefillthe raw bytes of a function into tbyte by byte.
if the function has less than bytes we will fill the tensor twith zero byte paddings.
otherwise we will discard the redundant bytes of this function.
itisworthnotingthat fewfunctions e.g.
lessthan0.
inour dataset havemorethan10 000bytes.moreover iftwofunctions first bytes are similar they are likely similar too.
so it is reasonable to simply discard redundant bytes of the function.
data augmentation.
in image classification applications data augmentationisapopularmeasuretoimprovedatasetsforcnn training.unlikeimagespixelsthataretoleranttominormodifications functionbytesarevulnerabletochanges sincetheywillalter the function semantics.
so during the training of our model we do not apply any data augmentation measures.
overfitting issue.
we also investigated the measures used in alexnet etc.
i.e.
layer stacking style and solutions to avoid model overfitting.inparticular weadopt batchnormalization toaddress the overfitting issue.
.
.
learningparametersusingsiamesenetwork.
inordertotrain theparametersofthiscnnembeddingnetwork weusethesiamese architecture .
as shown in figure the siamese architecture uses two identical cnn embedding networks.
each cnn takes one function as input namely iqandit and outputs the corresponding 2detailscouldbefoundat cnniq it yf iq f it l figure siamese network illustration.
embeddings namely f iq andf it respectively where f represents the network structure and represents the network parameters.
in addition to the input pair iq it the siamese architecture also accepts an indicator input y. this input yindicates whether the two functions iqanditare similar or not.
if they are similar y otherwise y .
thegoalofthetrainingistofindthebestparameter tosatisfy the aforementioned requirement rq i.e.
the distance betweenfunctions iqanditis small if they are similar otherwise large.
formally thedistanceoftwofunctions intra functionfeaturesis defined as follows.
d1 iq it bardblex bardblexf iq f it bardblex bardblex to achieve this goal we evaluate a contrastive loss function of this siamese network as follows.
l avera e iq it y d1 iq it y max m d1 iq it wheremisapre definedhyper parameter i.e.
theminimalmargin distance that dissimilar functions are expected to have.
we can infer that if this loss function gets a minimal value d1 iq it is close to when y andmax m d1 iq it is closeto0when y .inotherwords eachfunctionwillbecloseto similar onesand farfrom dissimilar ones in the embeddingspace.
so the aforementioned requirement rq is satisfied.
theobjectiveofthetrainingthusbecomestofindtheparameter to minimize the siamese network s loss function i.e.
ar min l this objective function can be solved using stochastic gradient descent sgd with standard back propagation algorithms .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diff cross version binary code similarity detection with dnn ase september montpellier france .
.
negative training samples.
it is crucial to build a set of positivesamples i.e.
pairsofsimilarfunctions andnegativesamples i.e.
pairs of dissimilar functions in order to get desirable convergence in the aforementioned cnn and siamese network.
we havecollected about2.
million positivesamples frompublic repositories.
we thus need a way to either collect or generate sufficientnegativesamplesfortraining.similarto we also generate negative samples in each mini batch during training based on the positive samples.
morespecifically foreachpositivesample iq ip inamini batch we will generate two semi hard negative samples namely iq in1 and ip in2 .takethefunction iqasanexample wewill look for function inthat satisfies the following equation3.
d1 iq in m werandomlyselectonefunction in1thatsatisfiesthisconstraint as the negative function.
but we will skip the hardest negative function i.e.
ar mind1 iq in becausesuchsamplescaneasily lead the model to bad local minima during training.
in order to get sufficient different negative samples we will shuffle the mini batch of positive samples in each epoch during training.morespecifically ineachepoch wefirstrandomlysort the binary file pairs then randomly sort the positive function pairs betweeneachbinaryfilepair.therandomlysortedpositivesamples function pairs will then be divided into mini batches and new negative samples could be generated from these new mini batches.
.
inter function semantic feature functions do not work solely i.e.
they will call other functions or be called by others.
the interactive relationship with other functions in the same binary including themselves is an important semanticfeature i.e.inter functionfeature.thisfeaturecanberepresentedasthefunctioncallgraph.wenoticethatsimilarfunctions have similar call graphs.
ideally the whole call graph should be considered.
for example smit usesthecallgraphmatchingtodetectsimilaritybetween malwaresamples.althoughtheyproposeanefficientgraphedit distance algorithm the computation cost is still too high to deploy.
in our solution we extract only the in degree and out degree of a node i.e.
function in the call graph as its inter function feature.
more specifically for each function iq we embed its inter function feature as a dimensional vector as follows.
iq in iq out iq wherein iq andout iq are the in degree and out degree of the functioniqin the call graph respectively.
formally the euclidean distance of two functions inter function features is defined as d2 iq it bardblex bardblex iq it bardblex bardblex .
inter module semantic feature a function iqalso invokes a set of imported functions denoted asimp iq which are defined in external modules libraries .
we notice that similar functions invoke similar imported functions butnottheopposite.moreover theset imp iq isrelativelystable evenifiqchangesacrossversions duetothemodulardevelopment 3this is different from facenet and vggface .process.
as a result the imported function set is also an important semantic feature i.e.
inter module feature.
for consistency we also convert the inter module feature i.e.
theimportedfunctionset intoavectorfordistancecomputation.
therefore we use the following element testing formula to embed a set into the superset s space.
h set superset x1 x2 ... xn wherenisthesizeofthesuperset and xi 1ifthei thelement ofsupersetis inset otherwise .
for two functions iqandit assuming their binaries are bqand bt we will get their imported function set imp bq andimp bt too.thenwetake imp bq imp bt asthesuperset andusethe aforementioned formula to encode each inter module feature then compute their distance as follows.
d3 iq it bardblh imp iq imp bq imp bt h imp it imp bq imp bt bardbl it is worth noting that imp bq is a superset of imp iq and imp bt is a superset of imp it .
moreover although symbols e.g.
functionnames maybestrippedfrombinaries thenamesofimportedfunctionswillalwaysbekeptsothatthelinkercouldlink modulestogether.asaresult itiseasytoextractthesetofimported functions for a binary or a function.
.
overall similarity computation given any two functions iqandit we could thus compute their intra functiondistance d1 inter functiondistance d2 andintermoduledistance d3 followingequation equation 8andequation10respectively.
as aforementioned the imported function set of a function is usually stable so the inter module distance d3 between similar functions in general is small.
moreover the intra function distance d1 between similar functions is also small usually smaller than theminimalmarginofdissimilarfunctions i.e.
theparameter min equation .but similarfunctionsincross versionbinariescould have different call graphs especially different in degree and outdegree resulting a relatively large inter function distance d2.
so weeventuallycomputeanoveralldistancetorepresentthe overall similarity of these two functions as follows.
d iq it d1 iq it d2 iq it d3 iq it where is a pre defined hyper parameter in the range used for suppressing the effect of d2.
for any function iqin question we will compute the overall distancedwitheachtargetfunction andthensortalltargetfunctions bythedistance.theclosesttargetfunctions i.e.
topkdefinedin section2.
are more likely similar to iq.
evaluation .
implementation wehaveimplementedaprototypeof diff.itconsistsofthree majorcomponents preprocessor featuregenerator andneuralnetworkmodel.thepreprocessorisimplementedasaplug inofthe binaryanalysistool ida pro6.
.fromeachfunction inabinary three types of information are extracted i.e.
its raw bytes its authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france b. liu w. huo c. zhang w. li f. li a. piao w. zou table sources of the dataset data src projects versioned cross version cross version packages proj pkg binary pairs function pairs github repo debian repo total for training for validation for testing in out degreeinthecallgraphanditsimportedfunctionset.these rawinformationarethenencodedintoembeddings asdiscussedinsection3.specifically therawbytesareconvertedintoembeddings usingaspecialneuralnetwork.thisnetworkmodelisimplemented in tensorflow .
and keras .
.
.
evaluation setup ourexperimentsareconductedonaserverequippedwithtwo intel xeon e5 2650v4 cpus cores in total running at .
ghz gb memory 12tb hard drives and nvidia tesla p100 pcie16ggpucards.duringbothtrainingandevaluation only1gpu card was used.
.
.
dataset.
a dataset is needed to train neural network model andevaluateitseffectiveness.wecollectedasetof2 793positivesamples i.e.
pairsofmatchingfunctions from66 823pairsof crossversionbinaries in x86 linux platform.
as shown in table the dataset has two sources.
the first source is the github repository where we collected sourcecodefrom31projectswith9 419releases.eachreleaseisthen compiled with the compiler gcc .
with the default optimization options.weplacedeachproject stwosuccessivereleasesofbinaries into one pair and got pairs in total.
thesecondsourceisdebianpackagerepository wherewedirectly collected binaries from .debpackages.
we have collected 895packageswith1 842versions fromtheubuntu12.
.04and .
platform.
each package may contain more than one binaries.
wegroupedeachversionofbinarywithitsclosestversionasapair and got pairs in total.
foreachpairofcross versionbinaries wethenretrievedpairsof matching functions which have a same name but are not identical.
to increase the diversity we also extracted some pairs of functions that are identicalin cross version binaries.
finally intotal wehave pairs of cross version matching functions from pairsofcross versionbinaries.amongthem about1.52percents of pairs of cross version functions are identical.
it is worth noting that bindiff reports that .
percent of pairs are identical due to the inaccuracy introduced in its gi based algorithm.
groundtruth.
asaforementioned togetthegroundtruthof matching functions we utilized function names and thus relied on debug symbols optionally shipped with binaries.
for binaries that arecompiledfromgithubcode thecompilationoption gisadded when building.
for binaries from debina package repository we onlycollectedpackageswithsymbolicfiles e.g.
.ddebpackages .
after collecting the ground truth we stripped all debug symbolsfrom binaries and evaluate our tool diff and other tools on the stripped binaries only.
.
.
datasetsplit.
similartootherworks wealsosplitthedataset into three disjoint subsets for training validation and testing in order to evaluate the generalization capability of the trained modelonunseenbinaries.roughly wesetthenumberofpositivesamples i.e.
pairs of matching functions in these three subsets proportion to .
moreover we ensure that matching pairs from one pair of binarieswillbeplacedinonesubset.table 1showsthesizeofeach subset at the bottom.
.
.
neural network training.
this dataset is used to train the neural network model for intra function feature extraction.
in the cnnmodel weusethermspropoptimizer setthelearning rate to .
and set the forgetting factor to .
.
in the siamesenetwork eq.
we set the margin m i.e.
the minimal distance betweendissimilarfunctions to1.
.furthermore wesetthe in theoverallsimilarityscoreformula e.g.
equation to0.
.for each mini batch positive samples are selected and semihard negative samples are generated online.
the siamese network is trained for epochs .
h epoch to tune the parameters in the cnn embedding network.
.
hyper parameters in the siamese network inaddition thednninvolvesseveralotherhyper parameters anddesigndecisions e.g.
theshapeofinputtensor theembedding size the negative sample mining method and the network architectureetc.thechoicesoftheseparametersanddesigndecisions could also affect the effectiveness of the model.
we have conducted a set of experiments to select proper parameters and design decisions.
due to the time and resource limitation we train each model setting with samples of the training set for epochs.
we evaluate each model s performance on a subset of the testing set in which the number of positive samples of each binary pair is no less than .
.
.
inputshapeandconvolutionallayertype.
wehaveevaluatedtheperformanceofthenetworkindifferentinputshapeand convolutional layer as shown in figure 3a.
we can see that the model sperformanceisaffectedbytheshapeofinputtensor.be sides we also evaluate the performance of 1d cnn and find it doesn t perform better than 2d cnn with 100x100x1 input tensor.
section5will discuss more about it.
.
.
embedding size.
we have evaluated the performance of the network in different embedding size i.e.
the dimension of thecnn s output vector as shown in figure 3b.
it shows that if the embeddingsizeissetto64 themodelingeneralperformsbestand get the highest average recall accuracy.
thus in our model we set the embedding size to .
.
.
hard negative sample mining method.
we evaluate the performanceofourhardnegativesamplesminingmethodandcompare it with another two typical mining methods i.e.
facenet and vggnet .
further we evaluate the performance of network in different countof hard negative samplescorresponding to each positive sample.
in figure 3c facenet 3tuple means one semi hard negativesampleisminedbyfacenetmethod foreachpositive authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diff cross version binary code similarity detection with dnn ase september montpellier france epoch0.
.
.
.85average recall 1conv2d 100x100x1 conv2d 400x25x1 conv1d 10000x1 a average recall in different input shape and convolutional layer.
epoch0.
.
.
.85average recall b average recall accuracy in different embedding sizes.
epoch0.
.
.
.
.
.
.90average recall diff 4tuple diff2 4tuple vggnet 4tuple facenet 4tuple vggnet 3tuple facenet 3tuple c average recall in different negative sample mining methods.
epoch0.
.
.
.
.
.85average recall 1triplet loss tetrad loss siamese 3tuple siamese 4tuple d average recall in different network architecture.
figure evaluation of hyper parameters and design decisions.
table the recall accuracy of diff 1f and diff 3f on the testing set of pairs of binaries.
whole set big subset diff 1f diff 3f diff 1f diff 3f avg.
recall .
.
.
.
avg.
recall .
.
.
.
avg.
mrr .
.
.
.
sample while facenet 4tuple meanstwosemi hardnegativesamples correspondingtotheleftandtherightofeachpositivesample i.e.
a tuple .
besides we have also tried a more gentle mining criterion4 as described by the line diff2 4tuple in figure 3c.
however it doesn t performs better than ours.
we can see that our method performs better than another two methods.
and tuple tetrad mining method performs better than tuple triplet mining.
.
.
network architecture.
we have also evaluated the performance of different network architectures as shown in figure 3d.
we evaluated the triplet architecure of facenet and tetrad architectureof .wealsoevaluatedsiamesearchitecturewith triplet mining and tetrad mining method.
we can see that siamese architecture with tetrad mining method performs best.
4d1 iq ip d1 iq in1 m4.
accuracy in cross version bcsd in this section we evaluated the accuracy of diff with only theintra functionfeatureenabled denotedas diff 1f andwith allthree featuresenabled denotedas diff 3for diff using the metricrecall k .
the task is essentially a ranking task and every query has only one correct answer matched function .
so we thus also evaluated the mrr mean reciprocal rank .
.
.
evaluation on testing set.
we first evaluated diff 1f and diff 3f on the testing dataset consisting of pairs of crossversion binaries and calculated the metrics of recall and recall for each pair of binaries.
in order to evaluate diff s performanceonbigbinarypair morefunctionpairs wesplitthetesting set into big subset and small subset.
the big subset is consistedof binary pairs and each binary pair contains more than function pairs.
table 2shows the average recall and mrr results.
.
.
evaluation on coreutils.
we further evaluated the accuracy of diffonunseenbinaries e.g.
coreutils thatiscommonlyused targetinotherbcsdsolutions .wecollected7versions ofcoreutils including the latest version i.e.
v8.
at the time of writing and got pairs of cross version binaries.
table4shows the results of accuracy when matching the old version of coreutils to its latest version compared with state ofthe art cross version bcsd tool bindiff.
first bindiffbecomeslessaccuratewhentheversiongapgets larger.
for example the accuracy of matching v5.
to v8.
is only .
lessthanhalfoftheaccuracyofmatchingv8.28tov8.
.
diff authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france b. liu w. huo c. zhang w. li f. li a. piao w. zou table accuracy of diff and bindiff in cross compiler vendor cross version bcsd.
vulnerability aliasbinaries bindiff diff pre post vul func found?
recall vul func found in top ?
recall mrr cve heartbleed openssl .
.1f openssl .
.1g .
.
.
cve shellshock bash .
bash .
.
.
.
.
cve wget .
wget .
.
.
.
cve shellshock2 bash .
bash .
.
.
.
.
cve clobberin time ntpd .27p10 ntpd .
.
.
.
cve venom qemu .
qemu .
.
.
.
table comparison between diff and bindiff on unseen binariesfrom coreutils.eachversionofbinaryisevaluated against its latest version i.e.
v8.
released on .
the average accuracy is shown here.
means recall and means recall .
ver date bindiff diff 1f diff 3f 5mrr 5mrr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.997basename cat chgrp chmod chown chroot cksum comm cp csplit cut date dd df dir dircolors dirname du echo env expand expr factor false fmt fold ginstall head hostid id join kill link ln logname ls md5sum mkdir mkfifo mknod mv nice nl od paste pathchk pinky pr printenv printf ptx pwd readlink rm rmdir seq sha1sum shred sleep sort split stat stty sum sync tac tail tee test touch tr true tsort tty uname unexpand uniq unlink uptime users vdir wc who whoami yes0.
.
.
.
.
.8recall diff 1f diff 3f bindiff figure comparison of the exact matching accuracy i.e.
recall between diffandbindiff whencomparingeach binary in coreutils of version v5.
to version v8.
.
has amuch better performancein detectsimilarities between versionsspanningalongtimeperiod.forexample asshowninfigure diff outperforms bindiff by more than when comparing some binaries from v5.
to v8.
.
second diff 1f is also better than bindiff when the version gap is large.
for example it outperforms bindiff by over on average as shown in figure .
it shows that the sole intra function feature whichisidentifiedbythesiamesenetwork isaverystrong feature for cross version bcsd.
third bindiff performs slightly better than diff when the versiongapissmall.forexample therecall 1of diffis0.
smaller than the accuracy of bindiff i.e.
.
when comparing binaries of version v8.
to v8.
.
however the recall of diff is better thanbindiff eveniftheversiongapissmall.itshowsthat diff could get better match in the top candidates than bindiff.
.
performance in cross compiler bcsd cross compilerbcsdhasthreesub types cross compiler vendor cross compiler version and cross optimization level.
several approacheshavebeenproposedtosolveoneortwoofthem however neither one can solve all of them well.
our solution diff employs semantic features to solve bcsd and brings a chance to solve cross compiler bcsd too.
.
.
cross compiler vendor cross compiler version.
esh i s one of the representative solutions for this problem.
however it is too slow taking about minutes to compare a pair of functions.
butwehaveabunchofpairstoanalyze.sohereweonlycompared diff with the state of the art industrial tool bindiff not esh.
asshownintable wecollectedsixprojectswithknownvulnerabilities which are also evaluated in .
to construct crosscompiler vendorand cross compiler versionbinary pairs we first selected both vulnerable version and patched version for eachproject.
then we compiled the pre patch version with gcc .
.
andcompiledthepost patchversionwithclang .
.toevaluatetheaccuracyofsimilaritydetection wethendesignedtwoexperiments.
wefirstqueriedthevulnerablefunction s inthepatchedbinary.
ifthetoolreturnsthematchingvulnerablefunctionorputsitinthe top candidate we mark a otherwise in the table.
the results showed that diff succeeded in four out of six cases whereas bindiff failed in all cases.
thenwecomputedtheoverallaccuracyoffunctionmatching i.e.
recall between these two binaries.
in all cases except cve2014 diffoutperformedbindiffby10percentageonaverage.
.
.
cross compiler vendor cross optimization level.
bingo is specialized on cross compiler and cross architecture bcsd problems.here wecompared diffwithbingoandbindiff usingsame experiment configurations as bingo.
morespecifically wecompiledcoreutilsforx8632 bitandx86 bitarchitectures usinggcc v4.
.
andclang v3.
withvarious optimization levels o0 too3 .
tomakeahead to headcomparisonwithbingo wealsousethe samemetricasbingo.morespecifically itevaluatesthepercentage of functions in the first binary whose matching function in the secondbinaryisrankedone i.e.
bestmatch bythetool.wecan infer that this percentage is similar to recall except with a different denominator in equation .
we evaluated six settings and listed the results in table .i nt h e x86 architecture we can see that diff outperforms bingo in all casesby20 onaverage andoutperformsbindiffinallcasesexcept the two clang o2 vs. clang o3 settings.
in the x64 architecture authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diff cross version binary code similarity detection with dnn ase september montpellier france table comparing percentage of matching functions ranked 1bydifferenttools i.e.
diff bingoandbindiff in cross compiler vendor cross optimization level settings.
c is short for clang and g is short for gcc.
x86 64 x86 diffbingobindiff diffbingobindiff c o0 vs. g o3 .
.
.
.
.
.
c o0 vs. c o3 .
.
.
.
.
.
c o2 vs. c o3 .
.
.
.
.
.
g o0 vs. c o3 .
.
.
.
.
.
g o0 vs. g o3 .
.
.
.
.
.
g o2 vs. g o3 .
.
.
.
.
.
c32 c64c32 g64c32 arm c64 c32c64 g32c64 armg32 c64g32 g64g32 arm g64 c32g64 g32g64 armarm c32arm c64arm g32arm g640.
.
.
.
.
.
.
of fucntions ranked 1bingo diff figure comparing diff with bingo in cross compilervendorandcross architecturesettings.c32isshortforclang x86 32bit and g64 for gcc x86 64bit.
wecouldalsodrawsimilarresults.ingeneral diffoutperforms bingo and bindiff in this setting of bcsd problems.
.
performance in cross architecture bcsd since bingo is also specialized on cross architecture bcsd we still made a comparison with it here using a same experiment configuration.
more specifically we matched all functions in coreutils binaries compiled for one architecture i.e.
x86 bit x8664 bit and arm to namesakes in binaries compiled for anotherarchitecture.
we also used the percentage of matching functions ranked one as metric.
the evaluationresult is shown infigure .
for bingo we took its best result i.e.
the one with selective inlining .
the data in the plot can be read in the same way as bingo.
for example the bar c32 g64 means that when querying functions compiled using clang for x86 architecture around of their matching functions compiled using gcc for x64 architecture are ranked by the tool bingo while46 areranked1by diff.overall diffoutperforms bingo in all settings except g64 c32.
.
application in vulnerability search manybcsdsolutionsareproposedtosolvevulnerabilitysearch problem.herewealsoevaluatedtheperformanceof diffinvulnerabilitysearch inthecross architecturesetting asdonebydiscovre multi k mh and genius .wefirstcompiledthevulnerableopenssllibrary whichhave twovirtuallyidenticalvulnerabilities tlsanddtls onplatforms arm mipsandx86.thenwequeriedonevulnerablefunctionin one binary and examined the ranks of the two matching functions inanotherbinary reportedbyeachtool.theresultsarelistedin table6.
it shows that when searching one function e.g.
tls diff could always rank the two matching functions at place and .
in general it outperforms other tools.
for example when querying the x86 tls function in the mips binary discovre ranks the dtls function at place while diff ranks it at place .
it shows that the semantic feature automatically extracted by neural network used in diff is effective even better than the cfgandotherattributes usedindiscovre providedbyhuman experts.
itisworthnotingthat gemini alsousesanetworktosearch bugs in cross architecture.
however it uses features e.g.
cfg and nodeattributes providedbyhumanexpertstopre processinput binaries which we believe would introduce bias.
but we do not have the benchmark they used to evaluate the effectiveness.
so we omit the comparison between diff and gemini.
discussion in our work we use cnn to encode the raw bytes of a function intoanembedding.morespecifically wetakeafunctionasa2dgrid of bytes through dimensional convolutional network 2d cnn .2d cnnisusuallyusedforimageprocessing becauseanimage presents strong spatial locality on its the two dimensions i.e.widthandheight .afunctionisdifferentfromthisandmore similar to text meaning 1d cnn seems to be more appropriate.
in our evaluations 1d cnn performs not bad however 2d conv with the specific configuration performs better.
we can t explain the reason behind this and plan to explore itbasedontheadvancesonneuralnetworkvisualization related researches inthefuturework.althoughwedon tthinkwe have found the best configuration for both 2d cnn and 1d cnn weshowthefeasibilityofextractingsimilarityfeaturesfromraw bytes with 2d cnn.
other researchers can also continue to find better models and configurations with our dataset.
although diff outperforms bingo in cross architecture evaluations in fact the inter function feature and inter module featureplayimportantrolesinourevaluations.inthefuture we plan to transfer our approach to the cross architecture settings i.e.
training a model with cross architecture dataset.
related work in this section we briefly survey closely related work.
.
binary code similarity analysis staitic analysis.
bindiff discovre andgenius are based on cfg cg graph isomorphism gi theory .
discovre identifies a set of lightweight numeric features and builds a pre filter based on the features to quickly identify a small setofcandidatefunctions.genius encodesthecfgsintohighlevel numeric vectors to achieve realtime vulnerability search in a largesetoffirmwareimages.theseapproachesdependongraph authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france b. liu w. huo c. zhang w. li f. li a. piao w. zou table accuracy of searching the two vulnerable functions of heartbleed namely tls1 process heartbeat denoted as tls and dtls1 process heartbeat denotedasdtls inopensslbinariescompiledforarm mipsandx86.whensearchingone function bothtwoshouldappearinthetopcandidates sincetheyarevirtuallyidentical.whensearchingtls ordtls the value in the cell means the ranks of the matching tls dtls functions or dtls tls reported by the tool.
multi mh multi k mh discovre genius centroid diff from to tls dtls tls dtls tls dtls tls dtls tls dtls tls dtls arm x86 arm mips arm readynas arm dd wrt mips arm mips x86 mips readynas mips dd wrt x86 arm x86 mips x86 readynas x86 dd wrt matching which has no known polynomial time algorithm and ignore the semantics of concrete assembly level instructions.
inspiredbydiscovreandgenius gemini assumesafunctioncanberepresentedasanacfg acfgwithnumericattributes.
it converts each acfg to an embedding through siamese architecture and structure2vec network which is similar with ours.
however gemini relies on hand tuned features such as cfg structuresandnumericfeatures.
diffextractstheintra functionfeature from the raw bytes of functions without human interference.
binhunt and ibinhunt extend gi with symbolic execution and taint analysis to find semantic differences.
bingo captures the complete function semantics by a selective inlining techniqueandthenutilizeslengthvariantpartialtracestomodel binary functions in a program structure agnostic fashion.
esh statisticallyreasonssimilarityoffunctionsbasedonsmallerfragments semanticsimilaritiescomputedbyaprogramverifier.these approaches are computationally expensive.
for example esh takes minutes on average to compare a pair of functions.
dynamic analysis.
under the assumption that similar code hassimilarruntimebehaviors belx executeseachfunction for several calling contexts and collects runtime behaviors of functionsunderacontrolledrandomizedenvironment.imf sim introduces in memory fuzzing to solve the coverage issue of dynamic approaches.
these approaches rely on architecture specific tools to execute or emulate binaries and are inconvenient to apply.
.
deep metric learning bromley et al.
paves the way on deep metric learning and trainedsiamesenetworksforsignatureverification.chopraetal.
presents a method for training a similarity metric from data andappliedittofaceverification.seanetal.
learnsfine grained visualsimilarity forproduct designwith deepconvolutionalneural networkandsiamesenetwork.facenet usesadeepconvolutional network and triplet embedding to learn unified embedding on faces for face verification and identification.
in contrast to contrastive embedding and triplet embedding song et al.
proposesanewdeepfeatureembeddingalgorithmbytaking full advantage of the training batches.
.
convolutional neural network convolutional networks are a specialized kind of neural networkforprocessingdata thathasaknownandgrid liketopology .
cnns typically consist of multiple interleaved layers of convolutions non linearactivations localresponsenormalizations pooling layers and one or more full connected layers.
since the notablesuccessofalexnet inilsvrc2012 therehasbeen an explosion of interest in cnns and many successful variants suchasvggnet inception v3 andresnet havebeen presented.
a full review of cnns is beyond the scope of this paper and more information can be found in .
conclusion in this paper we propose a dnn augmented solution diff to solvethe cross versionbcsdproblem.
itemploysthreesemantic features i.e.
intra function inter function and inter module features whichareexactedfrombinarycodewithlightweightsolution.
wehaveimplementedaprototypeof diff andevaluateditona datasetwithabout2.
millionsamples.theresultshowsthat diff outperforms state of the art static solutions by over percentages on average in detecting similarities between cross version cross compiler and cross architecture binaries.