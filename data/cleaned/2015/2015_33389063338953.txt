white box testing of big data analytics with complex user defined functions muhammad ali gulzar shaghayegh mardani university of california los angeles usamadanlal musuvathi microsoft research usamiryung kim university of california los angeles usa abstract data intensive scalable computing disc systems such as google s mapreduce apache hadoop and apache spark are being leveraged to process massive quantities of data in the cloud.
modern disc applications pose new challenges in exhaustive automatic testing because they consist of dataflow operators and complex user defined functions udf are prevalent unlike sql queries.
we design a new white box testing approach called bigtest to reason about the internal semantics of udfs in tandem with the equivalence classes created by each dataflow and relational operator.
our evaluation shows that despite ultra large scale input data size real world disc applications are often significantly skewed and inadequate in terms of test coverage leaving of joint dataflow and udf jdu paths untested.
bigtest shows the potential to minimize data size for local testing by 105to 108orders of magnitude while revealing 2x more manually injected faults than the previous approach.
our experiment shows that only few of the data records order of tens are actually required to achieve the same jdu coverage as the entire production data.
the reduction in test data also provides cpu time saving of 194x on average demonstrating that interactive andfastlocal testing is feasible for big data analytics obviating the need to test applications on huge production data.
ccs concepts software and its engineering cloud computing software testing and debugging information systems mapreduce based systems .
keywords symbolic execution dataflow programs data intensive scalable computing map reduce test generation acm reference format muhammad ali gulzar shaghayegh mardani madanlal musuvathi and miryung kim.
.
white box testing of big data analytics with complex user defined functions.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
introduction data intensive scalable computing disc systems such as mapreduce apache hadoop apache spark are commonly used today to process terabytes and petabytes of data.
at this scale rare and buggy corner cases frequently show up in production .
thus it is common for these applications to either crash after running for days or worse silently produce corrupted output.
unfortunately the common industry practice for testing these applications remains running them locally on randomly sampled inputs which obviously does not flush out bugs hiding in corner cases.
this paper presents a systematic input generation tool called bigtest that embodies a new white box testing technique for disc applications.
bigtest is motivated by the recent successes of systematic test generation tools .
however the nature of disc applications requires extending these in important ways to be effective.
unlike general purpose programs addressed by existing testing tools disc applications use a combination of relational operators such as join andgroup by and dataflow operators such asmap flatmap along with user defined functions udfs written in general purpose languages such as c c java or scala.
in order to comprehensively test disc applications bigtest reasons about the combined behavior of udfs with relational and dataflow operations.
a trivial way is to replace these operations with their implementations and symbolically execute the resulting program.
however existing tools are unlikely to scale to such large programs because dataflow implementation consists of almost kloc in apache spark.
instead bigtest includes a logical abstraction for dataflow and relational operators when symbolically executing udfs in the disc application.
the set of combined path constraints are transformed into smt satisfiability modulo theories queries and solved by leveraging an off the shelf theorem prover z3 or cvc4 to produce a set of concrete input records .
by using such a combined approach bigtest is more effective than prior disc testing techniques that either do not reason about udfs or treat them as uninterpreted functions.
to realize this approach bigtest tackles three important challenges that our evaluation shows are crucial for the effectiveness of the tool.
first bigtest models terminating cases in addition to the usual non terminating cases for each dataflow operator.
for example the output of a join of two tables only includes rows with keys that match both the input tables.
to handle corner cases bigtest carefully considers terminating cases where a key is only present in the left table the right table and neither.
so is crucial as based on the actual semantics of the join operator the output can contain rows with null entries which are an important source of bugs.
second bigtest models collections explicitly which are created by flatmap and used by reduce .
prior approaches esec fse august tallinn estonia muhammad ali gulzar shaghayegh mardani madanlal musuvathi and miryung kim 1val x y z 2if x y z y x pc1 x y true effect z y x 4else z x y pc2 x y true effect z x y figure symbolic pathfinder produces a set of path constraints and their corresponding effects do not support such operators and thus are unable to detect bugs if code accesses an arbitrary element in a collection of objects or if the aggregation result is used within the control predicate of the subsequent udf.
third bigtest analyzes string constraints because string manipulation is common in disc applications and frequent errors are arrayindexoutofboundexception and stringindexoutofboundsexception during segmentation and parsing.
to evaluate bigtest we use a benchmark set of real world apache spark applications selected from previous work such as pigmix titian and bigsift .
while these programs are representative of disc applications they do not adequately represent failures that happen in this domain.
to rectify this problem we perform a survey of disc application bugs reported in stack overflow and mailing lists and identify seven categories of bugs.
we extend the existing benchmarks by manually introducing these categories of faults into a total of faulty disc applications.
to the best of our knowledge this is the first set of disc application benchmarks with representative real world faults.
such benchmarks are crucial for further research in this area.
we assess jdu joint dataflow and udf path coverage symbolic execution performance and smt query time.
our evaluation shows that real world datasets are often significantly skewed and inadequate in terms of test coverage of disc applications still leaving of jdu paths untested.
compared to sedge bigtest significantly enhances its capability to model disc applications in out of applications sedge is unable to handle these applications at all due to limited dataflow operator support and in the rest applications sedge covers only of paths modeled by bigtest .
we show that jdu path coverage is directly related to improvement in fault detection bigtest reveals 2x more manually injected faults than sedge on average.
bigtest can minimize data size for local testing by 105to 108orders of magnitude achieving the cpu time savings of 194x on average compared to testing code on the entire production data.
bigtest synthesizes concrete input records in seconds on average for all remaining untested paths.
below we highlight the summary of contributions.
bigtest is the first piece of disc white box testing that comprehensively models dataflow operators and the internal paths of user defined functions in tandem.
bigtest makes three important enhancements to improve fault detection capability for disc applications it considers both terminating andnon terminating cases of each dataflow operator it explicitly models collections created byflatmap and translates aggregation logic into an iterative aggregator and it models string constraints explicitly.
it puts forward a benchmark of manually injected disc application faults along with generated test data inspired by the characteristics of real world disc application faults evidenced by stack overflow and mailing lists.1val trips sc.textfile trips table.csv .map s val cols s.split cols cols .toint cols .toint returns location and speed 6val zip sc.textfile zipcode table.csv .map s val cols s.split cols cols returns location and its name .filter s s. palms val joined trips.
join zip joined .map s if s. 2.
1 car else if s. 2.
1 bus else walk .reducebykey .saveastextfile hdfs ... figure alice s program estimates the total number of trips originated from palms.
bigtest finds 2x more faults than sedge minimizes test data by orders of magnitude and is fast and interactive.
our results demonstrate that interactive local testing of big data analytics is feasible and that developers should not need to test their program on the entire production data.
for example a user may monitor path coverage with respect to the equivalent classes of paths generated from bigtest and skip records if they belong to the already covered path constructing a minimized sample of the production data for local development and testing.
the rest of the paper is organized as follows.
section provides a brief introduction to apache spark and symbolic execution.
section describes a motivating example.
section describes the design of bigtest .
section describes evaluation settings and results.
section discusses related work.
section concludes the paper.
background apache spark.
bigtest targets apache spark a widely used data intensive scalable computing system.
spark extends the mapreduce programming model with direct support for dataflow and traditional relational algebra operators e.g.
group by join and filter .
datasets can be loaded in spark runtime using several apis that create resilient distributed datasets rdds an abstraction of distributed collection .
rdds can be transformed by invoking dataflow operations on them e.g.
val filterrdd rdd.filter .
dataflow operators such as map reduce and flatmap are implemented as higher order functions that take a user defined function udf as an input parameter.
the actual evaluation of an rdd occurs when an action such as count orcollect is called.
internally spark translates a series of rdd transformations into a directed acyclic graph dag where each vertex represents a transformation applied to the incoming rdd.
the spark scheduler executes each stage in a topological order.
symbolic execution using java path finder.
bigtest builds on symbolic java pathfinder spf .
internally spf relies on the 291white box testing of big data analytics with complex udfs esec fse august tallinn estonia trips zipcode map fmap1 map fmap2 filter ffilter join map fmap3 reducebykey fagg ffilter k2 v2 t1 t4falsetrue ffilter k2 v2 k1 k2 k1 v1 k2 v2 k1 v1 v2 s s n ffilter k2 v2 k1 zipcode k1 zipcodek2 trips ffilter k2 v2 k2 trips t2 t3tz a dataflow operators paths by bigtest map fmap1string t t.split .length isint t.split isint t.split t.split .toint!
k1 t.split v1 t.split .toint t.split .toint string z z.split .length map fmap2k2 z.split v2 z.split string k2 string v2 v2 palms true filter ffilterstring k1 int v1 string v2 v1 s car map fmap315 v1 v1 s public s walk string s int reducebykey faggk n a1 k k n n fagg n fagg a1 fagg a2 ... fagg an an ... .
.
.
b non terminating path conditions of individual udfs map fmap1string t t.split .length 5t.split .length notint t.split t.split .length isint t.split notint t.split t.split .length isint t.split isint t.split t.split .toint z.split .length x x x x map fmap2string z x c path constraints for terminating paths in udfs figure solid and dotted boxes represent transformations and path constraints respectively.
bigtest identifies path constraints for both non terminating and terminating program paths while symbolically executing the program.
analysis engine of java pathfinder jpf model checking .
it interprets java bytecode on symbolic inputs and produces a set of symbolic constraints.
each constraint represents a unique path in the program and can be ingested by a theorem solver to generate test inputs.
figure illustrates an example symbolic execution result.
by attaching listeners to spf the path conditions and the effects of each path can be captured.
for this program spf produces two path conditions the first path produces the effect of z y x when the path condition x yholds true and the second path produces z x y as an effect when the path condition x yis satisfied.
motivating example this section presents a running example to motivate bigtest .
suppose that alice writes a disc application in spark to analyze the los angeles commuting dataset.
she wants to find the total number of trips originating from the palms neighborhood using a public transport whose speed is assumed to be faster than but slower than mph a personal vehicle which is estimated to be faster than mph and on foot which is estimated as slower than mph.
each row in the trips dataset represents a unique identifier for the trip the start and end location in terms of a zip code the trip distance in miles and the trip duration in hours for example .
to map an area zip code to its corresponding area name alice uses another dataset that assigns a name to each zip code in the following manner culver city to perform this analysis alice writes a spark application in figure .
she loads both datasets lines and parses each dataset selects the start location of a trip as a key and computes the average speed as a value by dividing the distance by duration lines .
alice outputs a zip code as a key and an area name as a value lines and filters the area name with palms at line .
she joins the two data sets line .
in the subsequent mapoperation line she categorizes the trips based on the average speed into three categories.
she finally counts the frequency of each trip kindand stores them lines and .
though this program is only lines long it poses several challenges for modeling test paths.
equivalence classes of dataflow operators.
consider filter at line .
to exhaustively test this operator we must consider two equivalence classes the first where a data record satisfies the filter and moves onto the next operator and the second where the filter does not satisfy and its data flow terminates.
if we only model non terminating case then test data would contain passing data records only and hence would not detect a fault in which filter is removed from the disc application.
to model join at line we must have three equivalence classes two terminating cases and one non terminating case an input record in the left table trip does not have a matching key on the right table zipcode terminating its data flow an input in the right table does not have a matching key on the left terminating its data flow and there exists a key that appears in both tables passing the joined result to the next operator.
modeling such terminating cases is crucial otherwise test data generated produce the same output for both join andleftouterjoin and do not reveal faults that are based on incorrect join type usage.
udf paths.
consider the map at lines .
there are three internal path conditions speed mph mph speed mph and speed mph.
the sub figure in figure 3b shows corresponding path conditions and effects.
string constraints.
to analyze the second map at lines to we must reason about the entailed string constraints.
given a string zin sub figure in figure 3b and 3a to split the data into two columns it must satisfy a string constraint z.split .length 2to produce the effect where the key k2is z.split and the value v2is z.split .
string manipulation is critical to many disc applications.
in the above example at least one test must contain a string zwithout delimiter so thatz.split leads to arrayindexoutofboundsexception which will then expose the inability of the udf to handle exceptions.
292esec fse august tallinn estonia muhammad ali gulzar shaghayegh mardani madanlal musuvathi and miryung kim table generated input data where each row represents a unique path.
variables t z v and kare defined in figure 3a.
constraint trips zipcode c1 t.split .length c2 t.split .length notint t.split c3 t.split .length isint t.split notint t.split c4 t.split .length isint t.split isint t.split t.split .toint c5 z.split .length c6 z.split .length v2!
palms x00 c7t.split .length isint t.split isint t.split t.split .toint !
z.split .length v2 palms k1 zipcode !
!
x00 palms c8 .
.
.
v2 palms k2 trips !
!
x00 palms c9 .
.
.
v2 palms k1 k v1 x00 x00 palms c10 .
.
.
v2 palms k1 k v x00 x00 palms c11 .
.
.
v2 palms k1 k v1 x00 x00 palms otherwise this application may crash in production when the input record does not have an expected delimiter.
arrays.
to analyze reducebykey at line also in figure 3b we must model how the udf operates on the input array of size k and produces the corresponding output fa a1 fa a2.
.
.fa ak ak .
.
.
.
for example the udf returns the sum of two input arguments.
when the array size kis given by a user the final output nisa1 a .
.
.
ak ak .
summary.
due to the internal path conditions entailed by individual udfs instead of four high level dataflow paths shown in figure 3a alice must consider eleven paths in total which are enumerated in table .
figure shows the symbolic execution tree at the level of dataflow operators on the left and the internal symbolic execution trees for individual udfs on the right.
lastly example data generated by bigtest for each jdu path using z3 is shown in table .
while these example data records may not look realistic such data is necessary to exercise the downstream udfs that are otherwise unreachable with the original dataset.
for instance filtering a dataset without any passing data record will result in an empty set and consequently the udfs after the filter will never get tested with the original data.
therefore synthetic data is necessary and crucial to expose downstream program behavior.
approach bigtest takes in an apache spark application in scala as an input and generates test inputs to cover all paths of the program up to a given bound by leveraging theorem provers z3 and cvc4 .
.
dataflow program decomposition a disc application is comprised of a direct acyclic graph where each node represents a dataflow operator such as reduce and corresponding udfs.
as the implementation of dataflow operators in apache spark spans several hundred thousand lines of code it is not feasible to perform symbolic execution of a disc application along with the spark framework code.
instead we abstract the internal implementation of a dataflow operator in terms of logical specifications.
we decompose a disc application into a dataflow graph where a node calls each udf and combine the symbolic execution of the udfs using the logical specification of dataflow operators.
udf extraction.
bigtest compiles the disc application into java bytecode and traverses each abstract syntax tree ast to search for a method invocation corresponding to each dataflow operator.
the input parameters of such method invocation are udfs represented as anonymous functions as illustrated in figure 4b.
bigtest stores the udf as a separate java class shown in figure 4c and1sc.textfile zipcode.csv .
map ... .filter .
2 palms a disc application methoddeclaration name main body methodinvocation name fitler parameter classinstancecreation udffilter anonymousclassdeclaration .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
b generated ast1class filter static void main string args apply null static boolean apply tuple2 s return s. 2 .equals palms c extracted filter udf figure bigtest extracts udfs corresponding to dataflow operators through ast traversal.
generates a configuration file required by jpf for symbolic execution.
bigtest also performs dependency analysis to include external classes and methods referenced in the udf.
1def f a int b int return a b usage in reduce ...reduce f a 1def f reduce arr array var sum for a to k k is bound sum udf sum arr a return sum b figure a a normal invocation of reduce with a corresponding udf.
b an equivalent iterative version with a bound k handling aggregator logic.
for aggregation operators the attached udf must be transformed.
for example the udf for reduce is an associative binary function which performs incremental aggregation over a collection shown in figure 5a.
we translate it into an iterative version with a loop shown in figure 5b.
to bound the search space of constraints we bound the number of iterations to a user provided bound k default is .
.
logical specifications of dataflow operators this section describes the equivalence classes generated by each dataflow operator s semantics.
we use cito represent a set of path constraints on the input data i for a particular operator.
a single element cincicontains path constraints that must be satisfied to exercise a corresponding unique path.
we define fas the set 293white box testing of big data analytics with complex udfs esec fse august tallinn estonia table cirepresents a set of incoming constraints from the input table i where each constraint c cirepresents a nonterminating path.
c t represents that record t imust satisfy constraint c.fdefines the set of path constraints generated by symbolically executing ud fand f t represents the path constraint of a unique path exercised by input tuple t. operator inputs logical specification filter ud f i input table ud f t boolnon terminating t t i c ci c t f t terminating t t i c ci c t f t map ud f i input table o output table ud f t t where t onon terminating t t i c ci c t f t flatmap ud f i input table o output table ud f t collection of t where t onon terminating t ti i c ci c t f t joinr right table tr r l left table tl lnon terminating tr tl cr cr cl cl cr tr tr key tl key cl tl terminating key tr cr cr cl cl cr tr tr key key tl l cl tl tl key key terminating key tl cr cr cl cl cl tl tl key key tr r cr tr tr key key groupbykeyi input table t iandt tkey tvalue non terminating t t i c ci c t x x i xkey tkey reduce ud f reducebykey ud f i input table o output ud f t t t where t onon terminatingud f is an iterative version of the original udf ud f given as an input to reduce reducebykey.
f represents the set of path constraints generated from symbolic execution of ud f .
t1 t2 t3 .
.
.
tn i c1 c2 .
.
.
cn ci c1 t1 c2 t2 .
.
.
.. cn tn f i of symbolic path constraints of a udf where f t represents constraints of a unique path exercised by input t. by abstracting the implementation of dataflow operators into logical specifications bigtest does not have to symbolically execute the spark framework code about 700kloc as it focuses on application level faults only as opposed to framework implementation faults which is out of the scope of this paper.
bigtest supports all popular dataflow operators with the exception of deprecated operators such as co join .
filter.
filter takes a boolean function ud f deciding if an input record should be passed to downstream operators or not.
therefore we model two equivalence classes there exists a record tthat satisfies ud f and one of the incoming constraints cifrom input table i i.e.
the table produced by its upstream satisfying operator shown in table there exists a record tthat satisfies one of the incoming constraints but not ud f shown in table .
map and flatmap.
maptakes a udf ud f as an input and applies it to each input record to produce an output record.
it has one equivalence class where there exists tuple tfrom the input table isatisfying one of the incoming constraints c ciand also one of the path constraints in fi.e.
path constraints generated by symbolically executing ud f shown in table .
mapis supported by the previous work sedge butsedge considers the udf ud f as a black box uninterpreted function.
flatmap splits an input record using a ud f to generate a set of records and thus the equivalence class of flatmap is similar to that of map as shown in .bigtest handles flatmap by explicitly modeling a collection described in section .
.
join.
join performs an inner join of two tables tron the right and table tlon the left based on the equality of keys assuming that records from both tables are of the type tuple key value .
we model the output records of join into three equivalence classes the key of tuple trin the right table matches with a key of tuple tl on the left the key of tuple trin the right table does not match with any key of tuple tlon the left and the key of tuple tlin the left table does not match with any key of tuple tron the right.
and in table represent the three equivalence classes.
reduce and reducebykey.
reduce takes a ud f and a collection as inputs and outputs an aggregated value while reducebykey performs a similar operation per key.
as discussed in section .
bigtest generates an equivalent iterative version of the ud f with a loop.
by this refactoring of ud f toud f the equivalence classes could be modeled similar to that of map where there exist inputrecords t1 t2 .
.
.
tn ion which each of the corresponding nonterminating constraint c1 c2 .
.
.
cn cifrom the input table i holds true.
in addition each record must satisfy the constraints of ud f satisfying f as shown in of table .
.
path constraint generation this section describes several enhancements in symbolic path finder spf to tailor symbolic execution for disc applications.
disc applications extensively use string manipulation operations and rely on a tuple data structure to enable key value based operations.
using an off the shelf spf na vely on a udf would not produce meaningful path conditions thus overlooking faults during testing.
1def parse s string val cols s.split cols cols figure a udf with string manipulation strings.
operations such as split for converting an input record into a key value pair are common in disc applications but are not supported by spf.
bigtest extends spf by capturing calls to split recording the delimiter and returning an array of symbolic strings.
when an n th element of this symbolic array is requested spf returns a symbolic string encoded as splitn with a corresponding index.
by representing the effect of figure as splitn s splitn s bigtest generates one terminating constraint where scan only split into fewer than two segments and one nonterminating constraint where scan split into at least two segments.
due to no split support na ve spf generates a string without any delimiter as a test input e.g.
x00 instead of x00 x00 .
this input would lead to arrayindexoutofboundsexception while accessing a string using split .
1def agg arr array val sum arr bound k for a to min arr.size sum arr a ?
arr a sum figure an iterative version of aggregator udf collections.
constructing and processing collections through operators such as flatmap are essential in disc applications.
therefore bigtest explicitly models the effect of applying a udf on a collection.
in figure an iterative version of aggregator logic 294esec fse august tallinn estonia muhammad ali gulzar shaghayegh mardani madanlal musuvathi and miryung kim produced by bigtest takes a collection as input and sums up each element if the element is greater than or equal to zero.
given a user provided bound k bigtest unrolls the loop three time and generates four pairs of a path condition p and the corresponding effect e p a a e a p a a e a a p a a e a a p a a e a a a a na ve spf does not handle collections well and thus may generate an array of length only not exercising line in figure .
for example agg outputs the same sum of when arr a is mutated to arr a because the loop starts from instead of andsumis initialize to the first element of the array.
thus it is not possible to defect the fault using an array of length .
exceptions.
bigtest extends spf to explicitly model exceptions.
for example when an expression involves a division operator division by zero is possible which can lead to program termination.
in figure bigtest creates two additional terminating path conditions due to division by zero i.e.
x y x andy x y .
combining udf symbolic execution with equivalence classes.
bigtest combines the path conditions of each udf with the incoming constraints from its upstream operator.
for example the udf offilter in section produces a path condition of s. 2 palms .
suppose that the upstream operator mapproduces one non terminating path condition s.split .length 2with the effect s. 2 splitn s .
inside the equivalence classes offilter rows and in table bigtest plugs in the incoming path conditions effects of an upstream operator maptociand the path conditions effects of the filter s udf to f producing the following path conditions.
c t f t s.split .length splitn s palms c t f t s.split .length splitn s palms joint dataflow and udf path.
bigtest defines the final set of paths of a disc application as joint dataflow and udf jdu paths.
we define a jdu path as follows let g d e represent a directed acyclic graph of a disc application where dis a set of vertices representing dataflow operators and erepresents directed edges connecting dataflow operators.
imagine a disc application constructed with a mapfollowed by filter andreduce .
we represent this dataflow graph as g d e such that d d1 d2 d3 t1 ande d1 d2 d2 d3 d2 t1 where d1 d2 and d3aremap filter and reduce respectively.
filter introduces a terminating edge d2 t1 where a terminating vertex is t1.
since each dataflow operator takes a user defined function f for a vertex di we define a subgraph gi vi ei which represents the control flow graph of f. in this subgraph a vertex v vi represents a program point and an edge va vb eirepresents the flow of control from vatovb.gihasv1 start andvn stop corresponding to the first and last statements.
then from each dataflow operator node di we add a call edge from dito the start node of giand from the stop node of gito the di .
since some udfs include a loop and thus have a cycle in the control flow graph assert line2 str.
str.
line20 line21 assert line1 str.
str.
str.
str.
line11 str.
str.
str.
str.
line13 line14 assert and not str.to.int line14 and isinteger line14 and isinteger line13 and palms line21 and x11 line20 and s21 and s21 and s21 x621 and s1 x61 s22 x622 assert and x11 line11 and x12 str.to.int line13 str.to.int line14 and x61 x11 and x621 x12 and x622 x42 and x71 walk x72 figure output smt query constructed by bigtest to reflect jdu path constraint c11of table from motivating example.
we finitize the loop using a user provided bound kand unroll the loop ktimes.
we enumerate a set of all unique paths pkfor the graph gwith expanded subgraphs and call each unique path a joint dataflow and udf jdu path .
for an arbitrary test suite t the jdu path coverage is measured as a set of covered paths pk t p p pk t t and t cp where a test input tsatisfies the path condition cp of path p. given a user provided bound kfor unrolling a loop jdu path coverage is pk t pk .
.
test data generation bigtest rewrites path constraints into an smt query.
for constraints on integer variables bigtest uses analogous arithmetic and logical operators available in smt.
for string constraints bigtest uses operations such as str.
str.to.int and str.at .bigtest introduces a new splitn symbolic operation.
if a path constraint contains a clause v splitn s bigtest generates assert s str.
str.
v that is equivalent to s v where vis a symbolic string.
the path conditions produced bybigtest do not contain arrays and instead model individual elements of an array up to a given bound k. bigtest generates interpreted functions for java native methods not supported by z3.
for example bigtest replaces isinteger with an analogous z3 function.
bigtest executes each smt query separately and finds satisfying assignments i.e.
test inputs to exercise a particular path.
while executing each smt query independently may lead to redundant solving of overlapping constraints in our experiments we do not find it as a performance bottleneck.
theoretically the number of path constraints increases exponentially due to branches and loops however empirically our approach scales well to disc applications because udfs tend to be much smaller in order of hundred lines than disc frameworks and we abstract the framework implementation using logical specifications.
figure shows an smt query produced by bigtest for figure .
lines to constrict the first table to have four segments and the second table to have two segments separated by a comma.
lines to restrict a string to be a valid integer.
to enforce such constraint 295white box testing of big data analytics with complex udfs esec fse august tallinn estonia table subject programs subject of program characteristics jdu paths programoutputoperatorsoperatorsstring parsing branches udfs k p1 incomeaggregate total income of individuals earning weekly map filter reduce p2 movieratings total number of movies with rating map filter reducebykey p3 airportlayover total layer time of passengers per airport map filter reducebykey p4 commutetypetotal number of people using each form of transport for daily commute6map fitler join reducebykey p5 pigmix l2 pigmix performance benchmark map join p6 grade analysis list of classes with more than failing students 5flatmap filter reducebykey map p7 wordcount finds the frequency of words flatmap map reducebykey p1 p2 p3 p4 p5 p6 p7020406080100 .
.
.
.
.
.
.
.
100jdu path co vera e normalized bigtest sedge original figure jdu path coverage of bigtest sedge and the original input dataset that crosses the boundary of strings and integers bigtest uses a custom function isinteger and z3 function str.to.int .
lines to enforce a record to contain palms and the speed to be less than or equal to .
lines to join these constraints generated from a udf to the subsequent dataflow operator.
evaluation we evaluate the effectiveness and efficiency of bigtest using a diverse set of benchmark disc applications.
we compare bigtest against sedge in terms of path coverage fault detection capability and testing time.
we compare test adequacy input data size and potential time saving against three alternative testing methods random sampling of k records and using a subset of the first k records and testing on the entire original data.
to what extent bigtest is applicable to disc applications?
how much test coverage improvement can bigtest achieve?
how many faults can bigtest detect?
how much test data reduction does bigtest provide?
how long does bigtest take to generate test data?
subject programs.
in terms of benchmark programs we use seven subject programs from earlier works on testing and debugging disc applications listed in table .
the pigmix benchmark package contains a data generator script that generates large scale datasets.
we utilize mapandflatmap with udfs in apache spark to translate unsupported pig operators like load as and split .
three programs movierating p2 airportlayover p3 andwordcount p7 are adapted from bigsift .
each program is paired with a large scale dataset.
the rest are self created custom apache spark applications to add heterogeneity in dataflow operators and udfs.
table shows detailed descriptions of subject programs.
all applications involve complex string operations including split substring and toint perform complex arithmetics use type tuple for key value pairs and generate and process a collection with custom logic using flatmap .
experimental environment.
we run all large scale data processing on a node cluster.
each node is running at .40ghz and equipped with cores 32gb of ram and 1tb of storage allowingp1 p2 p3 p4 p5 p6 p7020406080100 .
.
.
.
.
.
.
.
100jdu path co vera e normalized bigtest random sample first data figure jdu path coverage of bigtest in comparison to alternative sampling methods us to run up to tasks simultaneously.
for storage we use hdfs version .
.
with a replication factor of .
due to a very small size of test data generated by bigtest we leverage apache spark s local running mode to perform experiments on a single machine.
.
dataflow program support bigtest supports a variety of dataflow operators prevalent in disc applications.
for instance apache spark provides flatmap and reducebykey for constructing and processing collections.
the previous approach sedge is designed for pig latin with only a limited set of operators support .sedge is neither open source nor have any implementation available for apache spark for direct comparison.
therefore we faithfully implement sedge precisely based on the technical details provided elsewhere .
we manually downgrade bigtest by removing symbolic execution for udfs and equivalence classes for certain operators to emulate sedge .
the implementations of both sedge and bigtest are publicly available1.
out of seven benchmark applications written in apache spark five applications contain flatmap andreducebykey therefore sedge is not able to generate testing data for these applications.
.
joint dataflow and udf path coverage we evaluate code coverage of bigtest sedge and the original input dataset based on jdu path coverage defined in section .
.
jdu path coverage evaluation.
we compare bigtest with three alternative sampling techniques random sampling of k of the original dataset selection of the first k of the original dataset as developers often test disc applications using head n and a prior approach sedge .
to keep consistency in our experiment setting we enumerate jdu paths for a given user provided bound k and measure how many of these paths are covered by each approach.
figure compares the test coverage from bigtest sedge and the original dataset.
y axis represents the normalized jdu path coverage ranging from to .
across seven subject programs we observe that sedge covers significantly fewer jdu paths of what 296esec fse august tallinn estonia muhammad ali gulzar shaghayegh mardani madanlal musuvathi and miryung kim lo scale k jdu path co vera e normalized k random sample first k of data a jdu path coverage10 lo scale k test runnin time s k random sample first k of data b test execution time figure the number of jdu paths covered and the test execution time when k of the data is randomly selected and the f irst k of data is selected for subject program commutetype .
is covered by bigtest .
by not modelling the internal paths of udfs sedge fails to explore many jdu paths.
even when the complete dataset is used the jdu path coverage reaches only of what bigtest could achieve.
the entire dataset achieves better coverage than sedge but it still lacks coverage compared to bigtest .
in other words using the entire bigdata for testing does not necessarily provide high test adequacy.
in figure both random sample andfirst sample provide of what is covered by bigtest .
we perform another experiment to measure the impact of different sample sizes on jdu path coverage and test execution time.
figure 11a and figure 11b present the results on commutetype .
incommutetype the covered jdu paths increases from two to six when the percentage of the selected data increases from .
to .
for those small samples input tables do not have matching keys to exercise downstream operators and the time and distance columns may not have specific values to exercise all internal paths of the udf.
in terms of running time as the sample size k increases the test execution time also increases linearly see figure 11b in which x axis is in log scale .
.
fault detection capability we evaluate bigtest s ability to detect faults by manually injecting commonly occurring faults.
because disc applications are rarely open sourced for data privacy reasons and there is no existing benchmark of faulty disc applications we create a set of faulty disc applications by studying the characteristics of real world disc application bugs and injecting faults based on this study.
we carefully investigate stack overflow and apache spark mailing lists with keywords apache spark exceptions task errors failures and wrong outputs and inspect top posts.
many errors are related to performance and configuration errors thus we filter out those and analyze posts related to coding errors.
for each post we investigate the type of fault by reading the question posted code error logs answers and accepted solutions.
we categorize our findings into seven common fault types incorrect string offset e.g.
a user uses instead of as the starting index in method substring and encounters stringindexoutofboundsexception .
incorrect column selection e.g.
a user accesses a wrong column in a csv file and thus receives arrayindexoutofboundsexception .
use of wrong delimiters e.g.
while splitting a string a user uses instead of leading to a wrong output .table fault detection capabilities of bigtest and sedge subject program p1 p2 p3 p4 p5 p6 p7 seeded faults detected by bigtest detected by sedge incorrect branch conditions e.g.
a user places a wrong order of control predicates executing only one branch s side .
wrong join types e.g.
a user uses a wrong relational operator such as cartesian join instead of inner join .
swapping a key with a value e.g.
a user tries to join two tables while the keys and values are interleaved .
other common mutations such as incorrect arithmetic or boolean operator in udfs.
when applicable we inject one of each fault type in every application.
for example fault types and could only be inserted when substr orsplit method is used.
when a fault type is applicable to multiple locations we select a location which is inspired by and similar to the fault location in the corresponding stackoverflow mailing list post.
for instance for fault type above we manually modify code to extract the first column instead of the second as a key in line of figure .
similarly for fault type we introduce fault by replacing the delimiter with .
in total our benchmark comprises of faulty disc applications.
while sedge is not designed to handle string constraints the main goal of this exercise is to justify the need to model udfs and string constraints.
sedge represents the internal udfs as uninterpreted functions and therefore is unable to model all internal udf paths.
conversely bigtest treats udfs as interpreted functions by representing them symbolically and models all internal udf paths up to bound k which is crucial for high coverage testing of udf s internal.
table shows a comparison of fault detection by bigtest and sedge .bigtest detects 2x more injected faults than sedge .
for instance in application p4 bigtest detects faults whereas sedge detects faults.
sedge uses concrete execution to model the udf exercising line of figure only.
therefore it is unable to find an input for detecting fault at line when the binary operator is replaced with i.e.
s. 2.
1 tos.
2.
1 .
similarly when join in line is changed to rightouterjoin sedge cannot detect any difference in the output because the equivalence classes do not model the terminating cases of join.
table modelling terminating and non terminating cases output from programapproach test input dataoriginal faulty bigtestterminating cs100 01l non terminating cs200 0cs200cs100 cs200 alternative non terminating cs200 cs200 cs200 as another example application p6 identifies courses with more than failing students.
a faulty version of p6 replaces the filter predicate count tocount to output courses with at least one failing student.
the original version of p6 uses mapandfilter to parse each row and identify failing students reducebykey to count the number of failing students and uses filter to find courses with more than failing students.
bigtest generates at least two records to exercise both terminating and non terminating cases of the last filter thus the original and faulty versions produce different 297white box testing of big data analytics with complex udfs esec fse august tallinn estonia p1 p2 p3 p4 p5 p6 p71011071013 .
.
.
.
.
of input records lo scale minimal input data selected for maximal jdu coverage entire data figure reduction in the size of the testing data by bigtest p1 p2 p3 p4 p5 p6 p7020406080100 .
.
.
.
.
.
.
.
.
.
.
.
.8test runnin time s entire data test data generated by bigtest figure test running time of entire data on large scale cluster vs. testing on local machine with bigtest outcomes on this data.
on the other hand a record is generated to exercise a non terminating case only.
such data would produce the same outcome for both the original and the faulty versions unable to detect the injected fault as shown in table .
.
testing data reduction testing disc applications on the entire dataset is expensive and time consuming.
bigtest minimizes the size of the dataset while maintaining the same test coverage.
it generates only a few data records in order of tens to achieve the same jdu path coverage as the entire production data.
four out of seven benchmarks have an accompanied dataset whereas the rest relies on a synthetic dataset of around 20gb each.
figure shows the comparison result.
in application p6 bigtest generates rows of data to achieve more jdu path coverage than the entire dataset of million records.
in other words bigtest produces testing data 106times smaller than the original dataset.
across all benchmark applications bigtest generates data ranging from to rows.
this is 105to times smaller than the original dataset showing the potential to significantly reduce dataset size for local testing.
.
time and resource saving by minimizing test data without compromising jdu path coverage bigtest consequently reduces the test running time.
the benefit of a smaller test data is twofolds the amount of time required to run a test case decreases and the amount of resources worker nodes memory disk space etc.
for running tests also decreases.
we measure on a single machine the total running time by bigtest and compare it with the testing time on a node cluster with the entire input dataset.
we present a breakdown of the total running time into test data generation vs. executing an application on the generated data.
figure represents the evaluation results.
in application p6 it takes .
seconds on a single machine to test with data from bigtest otherwise testing takes .
cpu seconds .
seconds x machines on the entire dataset which still lacks complete jdu path coverage.
across the seven subject programs p1 p2 p3 p4 p5 p6 p7020406080 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.6runnin time s constraint generation constraint solver test execution figure breakdown of bigtest s running time de ree of bound k ofjdu pathsw ord count grades anal ysis income a re ate de ree of bound k test generation time s figure bigtest s performance when the degree of upper bound k on loop iteration and collection size changes bigtest improves the testing time by 194x on average compared to testing with the entire dataset.
figure reports the complete breakdown of the total running time of bigtest .
the maximum test generation time observed is seconds for airport layover p3 in which seconds are consumed by constraint solving.
this is because the resulting jdu paths include integer arithmetics and complex string constraints together.
solving such constraints that cross the boundaries of different dimensions integer arithmetics vs. string constraints is time consuming even after bigtest s optimizations.
if we combine both the test running time and test generation time and compare bigtest with the testing time with the entire dataset bigtest still outperforms.
in fact bigtest still is 59x faster than testing on the entire dataset.
.
bounded depth exploration bigtest takes a user provided bound kto bound the number of times a loop is unrolled.
we assess the impact of varying kfrom to and present the results in figure .
at k the number of jdu paths for gradeanalysis is .
when kis bigtest generates jdu paths.
an exponential like increase in the test generation time can be seen across the subject program as we increase k. when k in gradeanalysis bigtest takes seconds and with k bigtest takes seconds.
we empirically find k to be a reasonable upper bound for loop iteration to avoid path explosion.
.
threats to validity as we manually seed faults in the benchmark applications the location of faults may introduce a bias in fault detection rate of bigtest posing a threat to internal validity.
however as mentioned before most type of faults are only applicable to a single code location.
if a fault type is applicable to multiple locations we then select the fault location inspired by the corresponding stackoverflow mailing list post.
in case of external validity our classification of disc faults may not be representative of all possible disc application faults out there as the survey is based on stackoverflow mailing lists 298esec fse august tallinn estonia muhammad ali gulzar shaghayegh mardani madanlal musuvathi and miryung kim posts.
additionally the selection of fault types in our evaluation may be unfair to prior approaches.
we attempt to mitigate this bias by restricting the evaluation to top seven most commonly occurring faults in disc applications.
to eliminate this threat in the future we plan to perform a large scale study on disc application faults.
related work testing map reduce programs.
csallner et al.
propose the idea of testing commutative and associative properties of map reduce programs by generating symbolic constraints .
their goal is to identify non determinism in a map reduce program arising from a non associative or non commutative user defined function in thereduce operator.
they produce counter examples as evidence by running a constraint solver over symbolic path constraints.
xu et al.
add few more map reduce program properties such as operator selectivity operator statefulness and partition interference .
both of these techniques test only high level properties of individual dataflow operators and they do not model the internal program paths of user defined functions.
olsten et al.
generate data for pig latin programs .
their approach considers each operator in isolation and does not model internal program paths of udfs treated as black box.
furthermore olsten et al.
require knowing the inverse function of a udf given to transform .
li et al.
sedge is the most relevant approach to bigtest .
sedge has three main limitations.
first its symbolic execution does not analyze the internal paths of individual udfs.
it considers udfs as black box procedures and encodes them into uninterpreted functions .
second it does not support operators such as flatmap reduce and reducebykey which are essential for constructing a collection and aggregating results from a collection in big data analytics.
third the equivalence class modeling for each dataflow operator is not comprehensive as it does not consider early terminating cases for some operators where a data record does not flow to the next dataflow operator.
our empirical evaluation in section finds that these limitations lead to low defect detection in sedge .
table compares dataflow operator support for related approaches and shows that bigtest has the most comprehensive and advanced support for modern disc applications.
test generation in databases.
jdbc or odbc enable software developers to write applications that construct and execute database queries at runtime.
testing such programs requires test inputs and database states from a user.
emmi et al.
perform concolic execution of a program embedded with an sql query by symbolically executing the program till the point where a query is executed.
their approach is only applicable to basic sql operations such as projection selection etc.
e.g.
select where .
braberman et al.
select input data to test the logic of computing additional fields from existing columns in the database .
they do not handle arbitrary udfs which are prevalent in disc applications.
symbolic execution.
symbolic execution is a widely used technique in software engineering and is used to generate test data using constraint solvers .
for example visser et al.
use jpf java pathfinder to generate test input data .
however the same approach cannot be applied to disc applications directly because it would symbolically execute the application as well as the underlying disc framework.
such practice will produce an unnecessarily large number of complextable support of dataflow operators in related work dataflow operators olston et al .
li et al .
emmi et al .pan et al .bigtest load map select map transform incomplete incomplete filter where group join incomplete incomplete incomplete union flatmap split incomplete intersection reduce path constraints facing scalability issues.
this justifies and motivates our approach that abstracts dataflow operators as a logical specifications while performing symbolic execution for the udfs.
rosette is a framework for designing a solver aided language to ease the process of translating each language construct into symbolic constraints.
bigtest and rosette both translate higher order types such as arrays into lower level constraints.
bang et al.
address the problem of solving constraints crossing boundaries between different theories numerics integer and string constraints .
such cross theory constraints are known to be difficult to solve with z3 or cvc4.
they extend spf by modeling strings into bit vectors and by integrating numeric model counting in abc which could be used for bigtest in the future.
regression testing.
regression testing has been extensively studied in software testing.
safe regression testing selects only those test cases that exercise the updated regions of a program .
rothermel et al.
summarize several regression testing techniques and evaluate them under a controlled environment .
test augmentation techniques help developers generate new test data to cover code not exercised by the available test cases using symbolic execution .
xu et al.
evaluate concolic and genetic test generation approaches and report trade offs .
the aforementioned approaches are not directly applicable to disc applications as they do not explicitly model the combined behavior of dataflow relational operators and the internal semantics of udfs.
conclusion big data analytics are now prevalent in many domains.
however software engineering methods for disc applications are relatively under developed.
to enable efficient and effective testing of big data analytics in real world settings we present a novel white box testing technique that systematically explores the combined behavior of dataflow operators and corresponding udfs.
this technique generates joint dataflow and udf path constraints and leverages theorem solvers to generate concrete test inputs.
bigtest can detect 2x more faults than the previous approach and can consume 194x less cpu time on average than using the entire dataset.
with bigtest fastlocal testing is feasible and testing disc applications on the entire dataset may not be necessary.