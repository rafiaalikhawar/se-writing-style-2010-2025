lightweight collection and storage of software repository data with datarover thomas kowark christoph matthies matthias uflacker and hasso plattner hasso plattner institute university of potsdam august bebel str.
potsdam germany firstname.lastname hpi.de abstract the ease of setting up collaboration infrastructures for software engineering projects creates a challenge for researchers that aim to analyze the resulting data.
as teams can choose from various available software as a service solutions and can con gure them with a few clicks researchers have to create and maintain multiple implementations for collecting and aggregating the collaboration data in order to perform their analyses across di erent setups.
the datarover system presented in this paper simpli es this task by only requiring custom source code for api authentication and querying.
data transformation and linkage is performed based on mappings which users can de ne based on sample responses through a graphical front end.
this allows storing the same input data in formats and databases most suitable for the intended analysis without requiring additional coding.
furthermore api responses are continuously monitored to detect changes and allow users to update their mappings and data collectors accordingly.
a screencast of the described use cases is available at https youtu.be mt4zt 4sfu.
ccs concepts information systems !extraction transformation and loading software and its engineering !software evolution keywords data collection data mapping de nition link discovery api monitoring data storage .
introduction software development teams are nowadays not only supported by issue trackers and source code management systems.
a variety of software as a service solutions for tasks like project management continuous integration or codeanalysis can be set up with only a few clicks.
the information stored in these systems is vital for researchers to reason about the e ects of software engineering practices and their e ects on product and process metrics.
in order to perform such analyses the data rst needs to be extracted from the di erent source system transformed into a single holistic representation and loaded into a suitable target database in a so called etl process.
in data collection it is challenging to keep up with the development of new systems and the evolution of existing tools.
connection plugins or scripts need to be created and maintained either on demand or by maintainers of plugins for systems like sonarqube1 in order to be able to collect the required data.
with regards to data storage the suitability of database systems and data formats is in uenced by the intended analysis.
if social graphs formed by developers are of interest a graph database that is shipped with built in graph analysis algorithms is superior to a document store lacking those features.
the document store on the other hand allows for simpler text analysis.
thus in addition to providing source code for data collection the same input data needs to be transformable to di erent output formats depending on the analysis use case.
in this paper we introduce datarover .
the system reduces the implementation e ort for etl work ows by separating api connection and querying from data storage.
minimalistic connector implementations solely perform api connection and querying.
the returned results are transformed into property graphs according to user de ned mappings which are created through a graphical front end based on sample javascript object notation json2 responses.
by that users can link ground data from di erent sources and store it in a schema and database most suitable for their use case without programming knowledge.
datarover further monitors api responses to detect changes in document structure or endpoints and thus allows to change connector implementations or data mappings in a timely manner.
the paper rst presents datarover s target users and their desired use cases section .
subsequently we walk through a sample parsing procedure section and discuss the system s architecture along with its implications for the development of connector implementations and storage components in section .
initial evaluation ideas and results are shown in section .
finally we discuss related work the current state of implementation and future work.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
.
users our main user group are researchers in the area of software repository mining msr .
in their work they analyze the in uence of software development practices on the outcome and progress of projects.
to this end they need to collect artefacts created by software development teams process them cleansing link detection etc.
and then test their hypotheses by executing queries on the created dataset.
for validation purposes they also have to perform this importprocess analyze work ow for data originating from projects which employed a di erent collaboration infrastructure.
beyond researchers the people involved in the software development processes developers project managers etc.
have a valid interest in such analyses to introspect and improve their development practices in a data driven manner.
while their infrastructures rarely change completely updates or replacement of existing and addition of new collaboration tools also require at least a review of the data extraction scripts and systems in the worst case new tools have to be developed.
for both groups their primary task is not programming or maintaining these systems but analyzing the gathered data.
hence e ort in terms of programming setup overhead and general amount of interactions with the system should be minimal.
this requires the tool to provide a simple front end for selecting and con guring parsing implementations as well as de ning how returned data is stored in the desired target database.
furthermore the implementation should reduce the amount of programming required to update an existing or create a new parsing implementation or connection to a new database management system.
we discuss both aspects of our system in the following chapters.
.
data collection linkage we now outline how researchers and and practitioners can setup datarover.
as a use case we recreate the work done for and collect issue tracker and repository data from the ruby on rails github project and combine it with travis ci3build information.
as an extension of the original study we further integrate question and answer data from stackover ow and link it to the developers and contributors of the rails project.
a screencast of this use case is provided at 4sfu.
the created datasets are available at .
job definition data collection missions are de ned by specifying the database type that results will be written to as well as the database url and access credentials.
currently users can select between neo4j graphs mongodb document stores and di erent schemas for an sap hana database.
support for further relational databases is planned.
afterwards users can select explorers from the list of available implementations.
each explorer internally de nes which input data is required from the user and a simple input form is rendered accordingly.
for the github and travis ci explorers the repository name and owner su ces as information.
if the to be parsed project is private access can be granted using oauth authorisation.
finally users can de ne how often parsing tasks are performed ranging from continuous parsing to only daily explorer runs.
the mission control shows all selected explorers their parameters and status messages created throughout data collection and storage runs on a single screen see figure .
.
mapping selection once an explorer is chosen and con gured with the required input data users are asked to de ne the data mapping.
each mapping can be named and stored in order to be reused as a template.
the interface for mapping de nition is displayed in figure .
based on a json sample that is created from previous api calls see section for details users de ne which data schema they want to achieve.
to this end they de ne classes for elements choose to ignore keys move elements between json objects or de ne links between nodes.
to better understand the resulting data structures a graphical representation is available on demand see figure .
mapping templates can be loaded through a dropdown list.
the opt in ag speci es whether elements for which no mapping is de ned are ignored or stored automatically see section .
in the example use case one possible option is to create a at object hierarchy that contains commits as nodes.
committer and author information is added as attributes of these nodes instead of links to nodes of type githubuser .
thus the memory footprint of the resulting data set can be reduced which might be necessary for larger projects containing thousands of users.
build information from travisci is in this case also be added as an attribute e.g.
as a boolean ag stating that the build was successful.
to achieve this the mapping for builds does not create a separate node of type build but identi es the corresponding githubcommit node using the commit url and adds the desired attribute.
for other analyses graph structures are a better choice as they for example allow to detect developer groups around certain les and folders through network analysis.
in this case users de ne a mapping that creates nodes for each element and establishes links between them instead of storing the same information twice.
builds could thereby also become nodes of their own and links to the respective commits are established.
.
mission extension as an extension point of the original study stackover ow data can be used to provide a proxy for someone s expertise with a given language or framework.
to add this data users rst need to select the respective explorer and de ne keywords of interest e.g.
ruby on rails .
secondly they have to create a mapping that speci es a link between the stackover ow data and the github travis ci data.
this can be achieved by creating a foreign key eld on the display name property of the stackover ow user object.
if a user registered their github and stackover ow accounts with the same username the system creates the respective link between the two user objects.
similar to the travisci github linkage the memory footprint can be reduced by merging the stackover ow user information into an existing github user node.
beyond explicit links users can instruct datarover to scan strings like the body of stackover ow questions for urls.
if hyperlinks are found that reference for example a github issue or a github commit they are also created within the resulting dataset.
811figure screenshot of the mission control overview page of datarover.
users can edit mission parameters add explorers from the catalogue jump to con guration pages for each explorer and get latest status information from each explorer and the data storage system on the right hand side.
explorers can also be started individually or all at once through the explore!
and run all explorers!
buttons.
figure graphical representation of the linked mapping for github repository data.
.
api changes once a data collection job is set up it will continuously run until users decide to stop it.
if during this time the api of a tool changes explorers will produce errors of which the users are noti ed via email.
they can then either select a new explorer that is suitable for the new api version or need to implement one themselves.
if changes only manifest in the returned data e.g.
if a eld is added to the result users will also get noti ed but only have to change the mapping de nition instead of altering the parser de nition.
once the mapping is updated the explorers are restarted and parsing continues as expected.
in summary the front end limits user interaction to providing input parameters for the explorers and adapting data mappings to their needs.
by using template mappings and the opt in ag the amount of mapping entries that haveto be provided manually is limited to only elds of direct interest to the user.
.
implementation datarover is a web application written in ruby on rails.
its architecture is presented in figure .
in the following we describe implementation details of the main classes explorers mappings and data stores.
data sourcesdatarover concrete data storemissionexplorer 1explorer n base data store... neo4jmongodbsap hanadev support toolsapijson document link url example.org mapping default keys opt in true type name a link type name b... constrained property graphconcrete data storeanalysis toolsclitarget dbs ablinktriggertriggerstoreinvoke readcreatetraversestore query figure overview of the internal classes angled corners and data round corners involved in storing software repository information with datarover.
812figure screenshot of the mapping de nition front end of datarover.
users can select json element on the left side and either edit an existing or create a new mapping.
for the overall collection of mappings it is possible to de ne whether unmapped elements are by default ignored or stored as is.
the graph view button triggers the creation of a graphical representation of the resulting dataset see figure .
.
explorers each explorer is a subclass of an abstract explorer class.
to implement a new explorer only a single method that performs the respective api call using the parameters speci ed by the user has to be implemented.
for example the explorer for git repository data on github queries the api for all commits in a given repository and continues to collect details i.e.
le changes and patches about the single commits which are not included in the initial response.
each returned element i.e.
json object containing information about commits is stored in a datastore instance.
these instances are initialized with a mapping and later perform data transformation linking and storage.
the explorer implementation itself is oblivious to this process.
hence for example the whole parser for github commits only contains code for client creation a call to retrieve all branches retrieving a list of commits and the details for each commit.
overall lines of code loc su ce when using the octokit4client gem for ruby.
creating similar implementations for other apis are straightforward as ruby even provides internal mechanisms for xml to json transformation hash.from xml xml doc .to json .
other potential data sources are third party tools and libraries e.g.
for source code analysis.
for them the implementation includes encapsulating preparations of the source code repository con guring the tool calling it through the command line interface cli parsing the output and transforming it into json.
for the metric fu tool suite available for ruby rails5 this amounts to roughly loc albeit most of this performs repository setup and cleaning and is therefore independent from changes to the tool.
.
data mappings for the json documents retrieved by an explorer a mapping needs to be present which in turn comprises a set of mapping entries.
each mapping entry either directly refers to a json key or speci es a regular expression.
in the latter case the mapping is applied for all matching elements.
this for example is useful to avoid duplicate mapping de nitions for all elements ending with at within github api responses.
within mapping entries any of the options presented in table can be speci ed.
datarover checks wether combinations of options are invalid or contradictory e.g.
using type name and datatype for the same element and warns the user accordingly.
unmapped json elements are ignored if the opt in ag is set for the entire mapping.
otherwise the elements will be stored using the corresponding json keys as relation or attribute names.
type names have to be provided by users for all objects nested within the json document.
finally a default key can be de ned to avoid having to specify url as a uniquely identifying attribute for all objects.
.
data stores based on the provided mapping the datastore base class performs transformation of json objects into constrained fu 813table reference of mapping entry options.
mapping key meaning type name type of a resulting node.
if the foreign key option is also set type of the linked node.
primary key json key that uniquely identi es a node e.g.
a github url or an email address datatype allows to denote special types like long strings or dates which require di erent column types or preprocessing for certain target databases.
ignore skips this eld in graph creation move to target element to which a json element and its sub elements are moved rename to allows renaming an attribute or relation foreign key uniquely identifying attribute of a node class which this element refers to contains links strings will be parsed for urls that identify other nodes noupdates only a relation is created but the linked object is not updated.
noinserts relations are only inserted if the linked object is already present in the database.
directed property graphs in which nodes are labeled with a class have attributes and are linked by relations which itself do not contain attributes.
once the graphs are built they are stored into concrete database systems according to rules de ned within database speci c data store implementations.
for graph databases the resulting property graph is stored as is.
for relational databases a table for each object class is created and relations are persisted in separate join tables.
to avoid creation of duplicates upon multiple parsing runs the primary key property is used by the data stores to query for existing elements which in turn are upserted with the newly retrieved data.
thus each concrete data store has to also implement a query interface allowing for node lookups based on primary key and attribute values.
.
api monitoring datarover monitors the queried apis.
contrary to the approach presented in our system does not use a proxy server but directly compares each returned json object against previously received versions.
this allows to detect whether new elements have been added to the responses or if data types of any elements changed.
null values empty strings or empty arrays do not overwrite existing samples.
if elements are no longer detected in the json responses they are not removed from the sample as it is possible that this behavior is speci c to a certain project and does not apply to all explorers.
this method can also be used to bootstrap mappings when creating new explorers.
upon their rst invocation without a valid mapping responses are collected to form a sample json le.
from this le a sample mapping is created containing an entry for each detected json key.
users can then modify these mappings as needed.
.
ev aluation evaluation of our system is ongoing.
to show that it simpli es etl processes for msr researchers we plan to perform an evaluation according to kitchenham i.e.
de table query times and memory consumption for at and linked mappings on the ruby on rails repository data.
system ghz macbook pro 16gb ram mac os x .
.
.
queries average of runs fastest and slowest removed.
databases neo4j community .
.
and postgresql .
.
.
schema memory query runtime linked no opt in neo4j .
mib 492ms linked neo4j .
mib 280ms flat neo4j .
mib 221ms flat postgresql .
mib 30ms linked postgresql .
mib 54ms ne requirements for an etl tool identify shortcomings of existing solutions and discuss how our tool overcomes them based on exemplary use cases.
beyond ease of use the following aspects are also relevant for assessing data rover.
.
data footprint for a rst estimate of di erent data schemas on memory consumption and query performance we extracted commit data from the ruby on rails github project6and stored it once using a at mapping and using a linked version with and without the opt in ag being set.
as of may the repository contains around .
commits created by close to collaborators.
using the at mapping and omitting any unnecessary elements only one node per commit is created containing the email addresses of the committer and author as well as the commit timestamps.
using the graph like mapping separate nodes for users and commits are created.
as an example query the amount of commits contributed by each author is determined.
the results for a neo4j database are shown in table .
without the opt in ag the resulting data set is about twice as big as the minimal one.
in terms of total data size this di erence seems marginal at rst but scaling up to multiple repositories of similar or even larger size it can decide whether an analysis is performable on a personal laptop or a larger system is required.
the at mapping does not further reduce memory footprint as each commit node needs to store duplicate information which outweighs the overhead for node relationship and index creation for users.
query times for the at mapping on the other hand are faster as no relationships need to be considered.
storing the same data in a postgresql database the at mapping not only provides faster query times but also reduces memory footprint signi cantly.
we plan to extend these analyses to data sets created from multiple sources and more complex queries in the future.
.
data import the exibility in data storage comes at the cost of additional computing that is necessary to transform each returned json object lookup objects in the target database by their primary keys and perform the upsert operations.
to compare our solution to state of the art implementations we plan to perform identical data import tasks with the systems mentioned in section and datarover and measure the overall completion time.
through data mappings identical data schemas can be created thus allowing to compare 814data quality as well.
for the presented use case we can already conclude that in comparison to the response times of the apis the data import overhead is negligible.
hence future evaluation will mainly focus on cases where data import costs slow down the overall process signi cantly such as the import of large local git repositories.
.
link discovery to assess the quality of link discovery a gold standard dataset of stackover ow members and their corresponding github accounts needs to be created and compared against the links datarover detected through foreign key mapping and link detection in texts.
from this data precision and recall allow to quantify link quality and compare it to competing systems.
for the sample use case links for of the total github user accounts could be detected using the foreign key approach presented in section .
through these di erent evaluations we aim to not only assess whether datarover actually allows msr researchers and practitioners to collect link and store software repository data in a more convenient manner with less programming overhead but also determine if the resulting datasets allow for faster analysis by reducing query times or memory consumption.
.
related work import of ground data is a necessity for all software repository mining msr tools.
mainly this task is achieved through plugin architectures which allow for distributed development of data collectors and on demand addition of new features.
a downside of this approach is the strong connection to the target platform.
an alitheia core plugin for parsing bugzilla data for example requires approximately lines of code most of which implements integration of the data into the platform and therefore is not reusable for other tools.
a di erent approach was taken by ghezzi et al.
who developed sofas a service oriented platform for software repository mining .
here data collectors are standalone services who return data in rdf format according to the software evolution ontology family.
while rdf data can be transformed into other formats to enable di erent use cases e.g.
by using rdf to relational mapping systems like ontop the additional data processing step is time consuming especially for large software repositories.
to process such large repositories boa was developed .
it frees users entirely from data importing tasks and lets them focus on large scale analysis.
unfortunately this setup also prevents single users from directly extending and using the platform for their own purposes e.g.
importing data from industry projects.
our system therefore extends the existing landscape of msr tools with lightweight data collection and transformation.
the con gurable mappings further enable reuse of existing analyses by creating data schemas similar to the ones used by the aforementioned tools.
.
summary in this paper we presented how datarover stores software repository data by mapping input json objects to constrained property graphs.
using this approach minimalistic data sets tailored to speci c use cases can be created which reduces memory consumption and increases query performance.
the system s linking mechanisms further allow toadd new data sources on demand in order to extend existing analyses.
with the built in api monitoring mechanism necessary adaptations to explorer implementations or mapping de nitions are detected in a timely manner.
.
availability the system is publicly available at tkowark data rover and distributed under mit license.
the presented use cases can already be performed and additional explorers for jira issue trackers organisation directories of large companies and plain websites are available.
deployment is automated through capistrano7tasks which create a running datarover instance on a selected target system.
to create such a system a vagrant8 le can be used to install and setup all necessary operating system packages within a virtual machine.
.
future work besides cleaning up the implementation and providing bug xes future development will focus on creating explorers for additional source systems and supporting further target databases.
with this foundation we aim to deploy datarover within real companies to observes how the instant availability of holistic interlinked software development data is used by development teams in practice.
.