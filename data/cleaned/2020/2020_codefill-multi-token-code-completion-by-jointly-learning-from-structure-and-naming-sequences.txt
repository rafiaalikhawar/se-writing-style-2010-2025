codefill multi token code completion by jointly learning from structure and naming sequences maliheh izadi m.izadi tudelft.nl delft university of technology delft netherlandsroberta gismondi r.gismondi student.tudelft.nl delft university of technology delft netherlandsgeorgios gousios g.gousios tudelft.nl delft university of technology delft netherlands abstract codecompletionisanessentialfeatureofides yetcurrentautocompleters are restricted to either grammar based or nlp based single token completions.
both approaches have significant drawbacks grammar based autocompletion is restricted in dynamicallytyped language environments whereas nlp based autocompleters struggletounderstandthesemanticsoftheprogramminglanguage and the developer s code context.
inthiswork wepresent codefill alanguagemodelforautocompletionthatcombineslearnedstructureandnaminginformation.
usingaparalleltransformerarchitectureandmulti tasklearning codefillconsumessequencesofsourcecodetokennamesandtheir equivalent ast token types.
uniquely codefill is trained both for single tokenandmulti token statement prediction whichenables it to learn long range dependencies among grammatical and naming elements.
we train codefill on two datasets consisting of 29m and 425m lines of code respectively.
to make the evaluation more realistic we develop a method to automatically infer points in the source code at which completion matters.
we compare codefill against four baselines and two state of the art models gpt cand travtrans .
codefill surpasses all baselines in single token prediction mrr .
vs. .
and .
and outperforms the state of theartformulti tokenprediction rouge l .
vs. .
and .
forn 4tokens .wepubliclyreleaseoursourcecodeand datasets.
ccs concepts software and its engineering software notations and tools.
keywords automaticcodecompletion transformers multi tasklearning types dynamically typed languages acm reference format maliheh izadi roberta gismondi and georgios gousios.
.
codefill multi tokencodecompletionbyjointlylearningfromstructureandnaming sequences.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
this work is licensed under a creative commons attribution noncommercial international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
introduction automatic code completion also called autocompletion is the task ofcompleting sourcecodestatementsby predictingwhatthedeveloper would write given the current context.
it helps developers finish theirprogramming tasksfaster bydecreasingthe typingeffort and saving keystrokes correcting typographical errors and enablingthemtoexploreapisinacontext sensitivemanner .autocompletion has therefore emerged as one of the most prominent features in integrated development environments ides .
to support autocompletion current ides exploit the regular structureofprogramminglanguages.forexample anideknows thatanopeningparenthesischaracter atafunction callpositionmustbefollowedbyenoughargumentstomatchthefunction s arity.itcanthereforeproposeargumentnamesforvariablesthat are in scope.
the availability of types in the host programming languagehelpsincreasetheprecisionof suggestions continuing with the example above the ide will only propose variable names for variables whose types match the function argument.
recent autocompletion systems also take into account past completions andanalyzelargecodebases toranksuggestionsaccordingto their past popularity.
despite the best efforts of researchers and idedevelopers developersfindrule basedcodecompletionmechanisms lacking.
ranking suggestions based on alphabetical or usage frequency or even the suggestion list length neglects the current context leading to unrelated recommendations .
these problems are exacerbated in dynamicallytyped language settings as the ide is lacking significant information to provide accurate suggestions.
to mitigate rule based autocompletion issues researchers have proposedstatistical andlearning based autocompletion models.
motivated by the naturalness hypothesis learning based models treat source code as natural language text hence code completion becomes an instance of the well studied text completion problem.
however treating source code as text deprives learning based models of important code structureandsemanticinformation .moreover theopen ended nature of code leads to extremely large prediction spaces due to developers constantly inventing identifier names .
in an illuminating study hellendoorn et al.
identified a set of issues with current research in code completion.
initially the currentapproachofevaluatingaccuracyasmaskedtokenpredictiondoesnotreflecthowautocompletionisused developersonlytrigger autocompletion after specific and certainly not arbitrary pointsin a program s syntax e.g.
after an opening parenthesis .
thus treating all tokens equally masks the fact that some tokens e.g.
punctuation aremucheasiertopredictthanothers e.g.
identifiers .moreover mostapproaches especiallylearning basedones donot ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa izadi et al.
accountfornamescomingfromdependencies whichdeprivesthem of important context.
in this work we propose codefill a novel learning based approachthataimstoaddresstheproblemsidentifiedabove.codefill borrows from the bimodality hypothesis to model source code inputs.
specifically codefill exploits that information is conveyed bysourcecodethroughtwochannels the naturallanguagechannel variable names functions etc.
and the code structure channel inheritance containment etc.
.
inputs are fed into the model simultaneouslyasbothsequencesoftokenvalues whichenableitto learnrelationshipsamongtokenvalues and uniquely sequencesoftokentypes whichenableittolearnassociationsbetweensyntactic elements.codefillisthenaskedtopredicteitherthevalueorthe type of the next ntokens.
to enable codefill to learn name dependencies across longer ranges we also train it with an additional task multi tokenstatementcompletionatthevaluelevel.theinputtokennamestocodefillisencodedwithbyte pairencoding bpe whichenablescodefilltobothcompresstheinputnamespaceand generate names that are not in the input vocabulary.
to present suggestions relevant to the developer s context codefill includes apost processingstepthatre ranksthepredictionsbasedonthe context visible to the model at the completion point.
codefill is instantiatedasasetofthreetransformers gpt2 based trainedwith soft parameter sharing multi task learning mtl setting.
eachtransformer models one of the three tasks namely token value token type and multi token prediction a joint loss function across all three tasks updates the weights of all three model components.
during eachepoch themodelis trainedon onetask accordingto a configurable task picking policy.
our target language is python to bothdemonstratetheefficiencyofthemodelwhentypeinformation is missing and also make our work comparable with the state of the art.
we pit codefill against four baseline models and two the stateof the art models namely gpt c and travtrans .
we use two deduplicated datasets the eth150k dataset deduplicated py117k and a manually collected dataset consisting of practically all non forked python repositories on github py1690k .
we evaluate all models on two tasks token level andstatement level predictions tlp and slp .
for tlp we evaluate for i next token prediction tlp a ii next token type prediction tlp b iii next tokenvalueprediction tlp c .toensurethattheevaluationsetting reflects real world use of autocompletion we also evaluatecompletions after specific syntactic elements e.g.
a dot .or an awaitkeyword tlp d .
we devise an algorithm to identify those syntacticelements cardinalpoints automaticallygivenacorpus.
we use top accuracy and the mean reciprocal rank mrr as evaluationmetrics.fortheslptask weassessthemodelsonstatementcompletionwith ntokensandwecomparethemusingmeteorandrouge lmeasures.toshowthateachcomponentin the codefill model is necessary we perform an ablation study.
the results demonstrate that codefill outperforms all the competing approaches in alltasks.
indicatively for each of the tpl a tpl b tpl c and tpl d evaluation tasks codefill achieves a stateoftheartmrrof81 .
.
.
.
whiletravtrans a current state of the art scores .
.
.
and .
respectively.
in the slp evaluation task for completing statements with four tokens the average completion length in our datasets codefillobtains70 .
and63.
forthemeteorandrouge l metricsrespectively andthussignificantlysurpassestravtrans .
and .
.
the main contributions of this work are codefill a model that unifies learning of structural and name based information for the autocompletion task.
an implementation of codefill including training procedures for the python programming language.
we make our code and datasets available.
an extensive evaluation of codefill against four baseline models and two state of the artapproaches demonstrating its superior performance.
background and related work in this section we briefly review the background work relating to ourapproach.then wepresentthemainapproachestoautocompletion including the baselines we used for comparison.
.
language models and transformers statistical language modeling lm is the task of developing aprobabilistic model for predicting the next tokens in a sequencegiven its preceding tokens i.e.
the context .
this context for simplerlmsisashortsequenceofwords whileitcanbesentences or paragraphs for larger models .
lms are either used without modification e.g.
in a text generation task or used inside a downstream task which requires language understanding.
programming languages also contain predictable statistical properties which can be learned using lms .
recently neurallms havegained popularity dueto their superior performance and generalization capabilities .
neural lms address the n gram data sparsity problem through param eterization of words as vectors .
a real valued vector word embedding is usedtorepresent eachword inavector space.this representationofwordsislearnedbasedontheirusage.thisallows wordswithasimilarmeaningtohaveasimilarrepresentation.note thattraditionalstatisticallmswerenotabletoachievethislevel of generalization .
moreover the distributed representation approachmakesiteasierfortheembeddingrepresentationtoscale with the vocabulary size.
this is specifically useful with source code where the vocabulary size can be unlimited due to the use of arbitrary identifiers.
initially feed forward neural network mod els then recurrent neural networks rnns and next networks withlong termmemory suchaslongshorttermmemory lstm networks were used.
mostrecently therehavebeensignificantimprovementswith the introduction of self attention architectures in the transformer which is a sequence to sequence architecture for transforming a given sequence of elements to another form .
attention enable transformerstofocusonselectivepartsofaninput thusgenerating morerelevantoutputs .transformersoutperformpreviousdeep models such as rnns and lstms on multiple nlp tasks .
a transformer consists of two main components an encoder and a decoder.
gpt introduced by openai2 is a large generative transformer based lm trained on a dataset of 8m web pages .
402codefill multi token code completion by jointly learning from structure and naming sequences icse may pittsburgh pa usa gpt has been successfully exploited for various nlp and source code analysis tasks .
.
multi task learning multi task learning mtl is a model training technique that combines multiple tasks and a joint loss function with the goal of maximizingperformanceononeorallofthetasks.mtlenables knowledgetransferacrossrelatedtasksandimprovesgeneralizationbyleveragingthedomain specificinformationcontainedinthe trainingsignalsofrelatedtasks .anmtlmodelcapturesthe commonfeaturesamongallthetasksthroughsharinghiddenlayers among them.
mtl has been applied successfully in both nlp and source code analysis .
there are two approaches to jointlytrainmodelsusingmtl hard parameter andsoft parameter sharing.
in the former the hidden layers are shared between alltasks while keeping several task specific output layers.
for the latter each task has its own model with its own parameters.
however the distance between them is regularized to encourage the parameters to be similar.
in the soft parameter sharing case training can happen either sequentially one task per training round or alternatively one task per epoch .
.
related work autocompletion is an active research area for both practitioners and researchers.
below we review the latest approaches to autocompletion.
.
.
conventional autocompletion.
traditionally autocompleters usedheuristicrulesstatictypeinformation similarcodeexamples and program history data for suggesting completions.
for instance ides conventionally return a list of type checked names either based on the order of alphabet or usage frequency.
.
.
statistical lms and grammar based models.
several studies use statistical lms for modeling source code .
tu et al.
built upon an n gram model using a cache mechanism tocapturelocalityinsourcecode.hellendoornanddevanbu improved the n gram model by exploiting various techniques including nested scopes locality and unlimited vocabulary.
raychev etal.
proposedaprobabilisticmodelbasedondecisiontrees and domain specific grammars.
researchers also studied the useof syntactic structure through exploiting probabilistic graphical models.allamanisetal.
employprobabilisticcontext freegrammars while raychev et al.
use probabilistic higher order grammars to this end.
.
.
deep learning for autocompletion.
recently deep neural networks suchas rnns lstms and transformers are beingeffectivelyusedformodelingsourcecode .in2018 li et al.
proposed a pointer mixture model to mitigate the outof vocabulary oov problem.
they trained two lstm models on types and tokens.
karampatsis et al.
presented a large scale open vocabulary neural lm.
they incorporated bpe beam search and cache mechanism to address the oov problem.
most recently kim et al.
incorporated the syntactic structure of trees into their transformer based model to better learn from source code.
.
.
multi token autocompletion.
although most research on codecompletionisfocusedonsingle tokenprediction severalstud iesaimedtocompleteentirestatementsorblocksofcode .
yang et al.
proposed pccand introduced an intermediate representation for source code to put tokens into groups usinglexeme and variable relative order.
nguyen et al.
proposed autosc to combine program analysis and software naturalness and fill in a partially completed statement with frequent and valid recommendations.
svyatkovskiy et al.
recently proposed a gpt 2basedmulti lingualmodel gpt c forcompletinglines.wen etal.
introduced fearswhichrecommendsthenextmethod given the current code in an ide using implementation patterns learned through mining open source projects.
.
.
mtl for autocompletion.
mtlhasbeenusedinvariousnlprelated tasks .
recently it has also been employed for programminglanguageprocessingtasks.liuetal.
proposed twoapproachesbasedonmtlforautocompletion.inthefirststudy theauthorsusedatransformer xlandanrnnforpredictingnext token type and value .
they develop a partial ast encoder and a path2root encoder and use them in their mtl framework.
in their second study liu et al.
pre train their model with hybrid objective functions for code understanding and code generation tasks.
next they fine tune it on code completion.
the pre training tasksaremaskedbidirectionallm nextcodesegmentprediction and unidirectional lm.
the fine tuning tasks are unidirectional masked lm and unidirectional lm.
.
.
practical aspects of autocompletion.
hellendoorn et al.
claimtheaccuracyofautocompletersevaluatedonsyntheticdata can drop on real world data.
aye et al.
trained models on realworld code completion examples of an internal dataset facebook .
they showed that models trained on data distributions that are closer to those of where the model will be deployed can outperform models trained on committed source code in public repositories.svyatkovskiyetal.
integratedpythia anlstmmodel tointellicode an extension to microsoft vscode ide.
in afollowup study they introduced intellicode compose as a generalpurpose multilingual autocompletion using transformers.
the improved modelpredicts sequences ofcode tokens generating up to entirestatements.intellicodecomposeisintegratedintothemicrosoftvscodeide.finally svyatkovskoyetal.
implemented and evaluated several neural code completion models which offer varying trade offs in terms of memory speed and accuracy.
commercial autocompletion tools such as tabnineandgithub copilot also exist but very little technical information has been shared about them.
.
baselines we include six recent models as baselines to provide a comprehensiveevaluation.forallbaselines weusethereplicationpackages provided bythe authorsand set theparameters as definedin each respective study.
for the statement level prediction task we modified the output layer of the baselines to predict up until the end of a statement.
n gram lstm fse hellendoornetal.
claimthat a well engineered and simple approach n gram based language 403icse may pittsburgh pa usa izadi et al.
models canprovidebetterperformancethanmorecomplexmodels deep neural networks .
the authors show that the combination of an n gram and lstm based model outperforms the rest of their models.
pointermixture ijcai lietal.
proposeapointer mixture model to address the oov problem.
they also try to incorporatestructuralinformationintheirmodelsbytrainingtwo models token types and values separately.
t xl bi lstm icpc liu et al.
propose two models based on the mtl technique.
the first study uses transformer xl and a bi lstm to train two models for tokens and ast paths for dynamically typed languages such as python.
the second study by the same group presents a pre trained language model which is fine tuned for code completion.
the authors usestaticanalysisandtypeannotationsfortheirtypeprediction task forjava.wecompareagainstthefirstmodelonly asitmost closely matches our setup.
openvocab icse toaddresstheoovproblem karampatsis et al.
present a bpe based language model.
we include it here for completeness even though their model is not tuned for autocompletion.
intellicode compose fse svyatkovskiy et al.
propose a general purpose multi lingual autocompletion supportingmulti tokenstatementcompletion.theytrainagpt 2model on1.2blocwritteninpython c typescript andjavascript.this toolisdeployedasacloud basedwebserviceandusesclient side cachingandparallelimplementationtospeedupthe predictions.
as the source code is not publicly available we trained a gpt model for source code and did our best to adhere to the settingsreported in the study.
as the focus of our study is mono lingual we only train this model on python code.
travtrans icse kimetal.
proposeatransformerbasedapproachwhichexploitsastpaths.weusetheirbestmodel travtrans as the state of the art in our evaluation.
approach thecodefillpipelinecomprisestwomainphases pre processing model training.
figure presents the overall workflow.
initially codefill pre processes tokenizes and converts the input sourcecode to equivalent syntax token sequences.
training consists of two main phases pre training with tasks token sequence type and name completion statement completion and fine tuning on tasks nameandstatementcompletion .forbothstages codefill uses soft parameter sharing mtl to learn from different representationsofsourcecode.atevaluationtime codefillalsore orders recommendations based on their type and the visible context.
inthefollowingsection wepresenthowtheproposedapproach works in detail.
.
pre processing duringpre processing codefillconvertstheinputprogramfilesto an equivalent format where keywords and identifiers are swapped with their ast equivalents.
the algorithm starts by removing commentsections blankspaces andblanklines.itthenextractsthelist ofmodules libraries andtheir aliasesusingthepython astlibrary.
those are stored in a dictionary and using it codefill replaces all 3uh surfhvvlqj 0rgho 7udlqlqj 3rvw surfhvvlqj 5h udqn wkh ilqdo uhfrpphqghg olvw 3uh wudlq d edvhg prgho zlwk wdvnv rq vrxufh frgh iurp vfudwfk2qh wr rqh frqyhuvlrq ri surjudp wrnhqv wr wkhlu fruuhvsrqglqj kljk ohyho w shv5hpryh gxsolfdwh surjudp ilohv roohfwlqj gdwd iurp lw xe lqh wxqh wkh edvhg prgho zlwk wdvnv 7rs q uhfrpphqghg frpsohwlrqv figure codefill workflow 1deftransform node ctx 2node qual names.resolve node 3node calltreetransformer ctx .visit node returnnode type value line position return return name node figure2 samplecodesnippetandtheextractedinformation theiroccurrences incodewith theirrespectivetypes i.e.
module library and alias .
codefill also pre processes and tokenizes the input source code.
for each line it reads the tokenized information and stores four typesofinformationabouteachtokennamely itsvalue its type itslinenumber and itspositionintheline.forinstance for the statement return node in figure it stores two tokens as shown in the table following the code example.
moreover variable visibility information e.g.
global vs. localvariables is maintained to differentiate between different name usages in the same context.
to address the oov problem codefill uses a bpe encoded name representation.
exploiting word segmentation bpe iterativelymergesthemostfrequentlyoccurringcharactersequences.
prior to applying bpe encoding and similarly to other studies codefill normalizes the input strings by replacing string andnumericliteralswithrespectivespecialtokens i.e.
stringand number.
auniquecharacteristicofthepythonlanguageisthatindentationdefinescodeblocks itisthereforeimportantforsourcecode models to learn to encode indentation as part of their learned representation.todoso codefillstoresthepositioningofindentation markers.
for the first line with an indentation it adds a special token angbracketleftindent angbracketrightatthebeginningofthegivenline.itpassesthrough the following lines with the same indentation to reach the next indentation or a dedentation position at which point it adds a respective angbracketleftindent angbracketrightor angbracketleftdedent angbracketrighttoken.
thepre processingstepresultsintwofilesforeachinputsource codefile onecontainingsequencesoftokennamesminusthe comments and extra blank lines and one containing sequences of token types.
both are fed into codefill as two different but corresponding representations of source code.
figure shows a 404codefill multi token code completion by jointly learning from structure and naming sequences icse may pittsburgh pa usa raises an error when the required variable is missing 2defrequired env var 4value os.environ.get var ifvalue isnone raise runtimeerror var is required to start the service.
returnvalue 1defrequired env var 2value os.environ.get var ifvalue isnone raise runtimeerror string returnvalue 1def function name local variable eos 2indent local variable lib.module.function name local variable eos 3if local variable is none eos 4indent raise errortoken string eos 5dedent return local variable eos figure3 anexamplecodesnippetanditsconvertedversion samplefunctionanditscorrespondingtypeinformationwiththe correct indention.
.
model training in this phase codefill learns from two granularity levels tokenand statement level completions with three simultaneous tasks namely next token valueprediction tvp next token type prediction ttp and statement completion sc .
model training follows a two stage process first a generic language modeling objective is used onthe unlabeled data to learnthe initial parameters.then theseparametersareadaptedtothetargettasksusing the corresponding objective.
thus while pre training codefilllearns from all three tasks while fine tuning is restricted to the tvp and sc tasks.
the reason for excluding the ttp task is that the number of types for all the program files is limited.
hence the model quickly learns how to properly predict these type sequences i.e.
learns an effective representation of the python grammar eliminating the need for further fine tuning.
themainneuralnetworkarchitectureforalltasksisbasedon thegpt 2transformerwith llayers.codefillusesthreedistinct gpt transformers each with its own input and training objective.
the models are initialized with random weights.
transformer blocksincludeself attentionlayer feed forwardneuralnets and anormalizationlayer.self attentionblocksidentifywhichtokens tofocuson.feed forwardneuralnetsconsistofaninputlayerto acceptinformation hiddenlayerstocapturethehiddencorrelations between each data point and finally an output layer to transmit information.
the parameters are transferred to the next decoder in the stack after being regularised with l2 norm to be similar to the respective decoder sparameters.
codefill uses softmaxactivation functionintheoutputlayertogenerateprobabilitydistributions over the vocabulary.
totrainthemodeltopredictasequenceoftokens vt d t withdas the vocabulary and cas the existing code context codefill estimates the following conditional probabilitydistribution p p v0 ... v n c0 ... c t n productdisplay.
i 1p vi c0 ... c t v0 ... v i .
weuseastandardlanguagemodelingobjective predictingthe next token given a context and maximize the following likelihood based on our unsupervised corpus of tokens.
in equation mis the length of the predicted sequence of code token values and is the set of parameters that is learned through stochastic gradient descent optimization to model p .
l v summationdisplay.
ilogp vi c0 ... c t vi m ... v i .
ineachlayer multi attentionheadsareusedtoaggregatetheoutput of the previous layer for each transformer block.
multi headed self attention is applied over the input context tokens followed by position wisefeed forwardlayerstoproducetheoutputdistribution.
h0 cwe wp hl transformer block hl l p vt softmax hlwt e t wherecisthecontextvectoroftokens listhenumberoflayers we is the token embedding matrix and wpis the position embedding matrix.
for training with mtl codefill uses the alternative training strategy whichaimstopreventcatastrophic forgetting asopposed to the sequential strategy .
with a probability of and for each of the ttp tvp and sc tasks respectively codefill picks a random task for each epoch.
ttp requires fewer epochs as itsvocabularyisfairlylimited.furtheron fortvpandsctasks codefillusesbeamsearchtoidentifythemostlikely sub token sequences.
lossis sharedamong alltasks.
duringpre training theparameters are tuned to minimize the absolute minimum of the crossentropy losses among the three pre training tasks namely tvp ttp and sc equation .
when fine tuning only tvp and sc losses are used.
loss fi n a l min loss tvp loss ttp loss sc .
.
token value prediction task tvp .
codefill uses different representationsofprogramsforeachtaskwithinthesoft parameter sharingmtlframework.codefilltreatsthetvptaskasmasked unidirectional prediction left side context is used to predict the next token.
the inputs to the task are sequences of token values represented as real valued vectors of .
.
.
token type prediction task ttp .
similarly to tvp ttp is also treated as left to right masked unidirectional prediction.
the input are corresponding token type representations as realvalued vector of as both the ttp and tvp models are trained jointly codefill is capable of exploiting token types when the ultimate goal is to predicting token values.
405icse may pittsburgh pa usa izadi et al.
c nj dz dzdz dz wrnhq w sh suhglfwlrq ckhdg od hu wrnhq qdph suhglfwlrq ru vwdwhphqw frpsohwlrqehdp vhdufk 0dvnhg vhoi dwwhqwlrq dwwhqg rqo wr ohiw frqwh w 3rvlwlrqdo hqfrglqj 7rnhq ydoxh hpehgglqj 3rvlwlrqdo hqfrglqj 7rnhq w sh hpehgglqj dzdz dz nj dz dzdz dz dzdz dz dz nj c nj dz dzdz dz nj dz dz nj 7rnhq ydoxhv ruuhvsrqglqj wrnhq w shv5hfrpphqghg frpsohwlrqv rqvwudlqhg od huv 7dvn 7rnhq ohyho suhglfwlrq7dvn 6wdwhphqw ohyho suhglfwlrq7dvn 7rnhq w sh suhglfwlrq figure model training .
.
statement completion task sc .
as useful as next token prediction may be developers can also benefit from getting longer suggestions to complete code statements .
correspondingly codefill can also benefit from training to predict longer sequences as training will enable it to better prioritize context use.
thus weaddathirdtasktotraincodefilltoprovidecompletion suggestionsupanduntiltheendofastatement.topredictawhole statement given the existing code context c and the vocabulary d codefill attempts to generate token values vt d conditioned on the sequence of preceding token values ct d for thistask thepre processingstepsintroduceaspecialtoken angbracketlefteos angbracketright to demarcate the end of a statement.
codefill is trained to keep predicting sequences of token names until it produces an angbracketlefteos angbracketright token.
.
.
beam search.
codefill uses greedy beam search to identify the most probable sequences given a sequence of probabilistic predictions.
specifically b width of the beam top probabilities are recorded partially for every step.
this heuristic algorithm does not necessarilyoptimizeresults however itscomputationalcomplexityequalsto o b v whichismuchfasterthancomputingallcases.
as b increases thequalityofgeneratedsummariesimproves however the learning time increases as well.
we experimented with several beam values and and settled to as it provided a goodbalanceofaccuracy andspeed.
.
post processing re ranking recommendations.
for a recommendation system to be useful predictions should be ranked similarly to user expectations.tooptimizeranking codefillincludesapost processinglayer to re rank the leaf nodes in the final recommendation listbased on the visible scope i.e.
the current file .
this is based on theobservationthatmostcompletionsshouldbelocaltotheedited file as naming visibility rules should force names to cluster.
tore rankthesuggestions codefillhierarchicallydividesthe visiblescopetofile class andclosestfunction.theintuitionhere is whenthemodelispredictingthenexttokenanditstypeisexpectedtobeavariablename candidatesintheclosestscopehavea higher probability of being correct.
however when the next token is predicted to be a function name candidates from the same class functions defined in the same class should be probably at the top ofthelist.there rankingprocessconsistsofmultiplyingthepredictionprobabilitiesofthetop 10predictionswithacorresponding weightcoefficient.theweightsareselectedbasedonthetypeof thepredictedtokenandthescopeofthedeclarationoftheidentifier.eachpredictionconsistsofa token type probability triplet with respect to the prediction point that it is made available wegeneratethelistofallvisiblenamesandtheirhierarchicalscope function class file .
each prediction is then cross checked with this list in the case where the predicted identifier is indeed alreadydeclaredinthefile andthusinthelist itspredictionprobabilityismultipliedbyaweightdependingonthetypeofthepredictedtoken andthescopeassociatedwiththeiteminthelist.astheweights impact the quality of predictions we first defined a range ratio for different categories based on our programming intuition.
then we experimented with this range and selected the best performing weights.
table presents the weights used in this process.
406codefill multi token code completion by jointly learning from structure and naming sequences icse may pittsburgh pa usa table1 weightsinthepost processinglayerforre ranking leaf node type function class file attribute access .
.
.
variable names .
.
.
function names .
.
.
algorithm re ranking final recommendations inputpredictions weightslist outputpredictions list of updated predictions names getsignificantnames get the list of important names in left context from the file forpredinpredictions do whiletruedo names getsignificantname.pop ifsignificantname.token prediction.token then typecategory gettypecategory weight weights pred.probability pred.probability weight break end if end while end for although the current weights improve the predictions this only sets the minimum bar.
future work can exploit automatic learning of these weights.
experimental setup totrainandevaluatecodefill weusetwopythondatasets.weevaluate the models based on different evaluation scenarios to achieve a more realistic and comprehensive outlook on the performance of code completion models to benefit developers in real world cases.
.
evaluation tasks weevaluatecodefillontwotasks namely token level andstatementlevelpredictions tlp and slp .
.
.
token level prediction.
weusetlptoassesstheabilityof themodeltopredictasinglenexttoken.wesplitthispartofthe evaluation into four subtasks presented below.
any token prediction.
our first sub task is to evaluate the predictions of any token irrespective of its type tlp a .
this is the baseline evaluation task employed in the literature but as research has shown itis notrepresentativeof real world autocompletion use.
for this reason we resort to more detailed evaluations as presented below.
token type prediction.
to assess the model s ability to learn grammaticalsequences weevaluatehowwellamodelcanpredicta correctasttokengivenacontext tlp b .wegrouptogetherast tokens in the following categories identifiers keywords operators punctuation and finally numerals and string literals.figure length of statements in the py117k dataset leaf node prediction.
inspired by the evaluation setup of the state of the artstudybykimetal.
weinvestigatetheabilityof modelswhenpredictingastleafnodes tlp c including attribute access names function parameters and constants.
cardinalpointprediction.
thethreetaskspresenteduptonow give a comprehensive view of the prediction ability of a model.however in practical settings autocompletion is only triggered atspecificpoints e.g.
afteradot orafterspecifickeywordssuchas for whilethedeveloperiseditingsourcecode.toensurethat predictions translate to practical benefits for the developers we evaluatecompletionsoncardinalpoints tlp d .toobtainalist ofkeywordsafterwhichautocompletionislikelytobetriggered wefirstselectthelistofpunctuationandkeywordstokensthatcan be completed.
we then compute the frequency of all bi grams with any of these tokens as their first token in our dataset.
then weremove three sets of bi grams those that are mostly written togetherwithoccurrencefrequencyabove95 e.g.
async def thosethatarenormallynotpredictable e.g.
class name ordef function name andfinally thosethatareusuallynotpractical completions e.g.
true .
the resulting list of tokens after which itismostbeneficialforautocompletiontobetriggeredisasfollows.
dot await assert raise del lambda yield return except while for if elif else global in and not or is binop with evaluation metrics.
as the model only predicts a single token in the tlp task we include two evaluation metrics namely the accuracy ofthetoppredictionandthemeanreciprocalrank mrr for the top recommendations.
accuracy measures the proportion of samples for which the suggested completion token exactly matches the single target label.
mrrassesses the whole top nrecommended completions and takesintoaccountthefirstpositionthetargetismatched .fora singlequery thereciprocalrankis1 rankwhererankistheposition of the highest ranked answer ... nfornanswers .
if no correct answer exists in top n then the reciprocal rank is .
for multiple queries q the mrr is the mean of the qreciprocal ranks.
.
.
statement level prediction slp .
the slp task assesses a model s ability to complete statements with up to ntokens.
the boxplot in figure shows the distribution of number of tokens for completions in the evaluation dataset py117k .
in our datasets statementsare4 .2tokenslongonaverage median maximum .toprovideacomprehensiveview weevaluatetheperformance ofthemodelswhenpredictingnext ntokenswithn .
evaluation metrics on absence of code specific metrics we use twometricscommonly usedforautomaticevaluationoftextgeneration namely metric for evaluation of translation with explicit 407icse may pittsburgh pa usa izadi et al.
table datasets used for training and evaluation py1690k py117k repositories .7k .9k files .7m 117k loc 425m 29m tokens unique .7m 766k types unique ordering meteor andrecall orientedunderstudyforgisting evaluation rouge l .
rouge rouge nreferstooverlappingn grams.rouge l oneofthevariationsoftherougemetric countslongestmatching sequence of words using the longest common subsequence algorithm.
it considers sentence level structure similarity and automaticallyidentifiesthelongestco occurringchainofinsequence n grams.
thus it does not require consecutive matches but insequence matches that reflect sentence level word order.
meteor isbasedontheterm to termmappingofthegenerated code with its corresponding reference code.
it focuses mainly on recall.
lavie et al.
showed metrics based on recall consistently achieve higher correlation with user preferences than those based on precision alone.
.
datasets we use two python datasets for training and evaluation the eth 150k python dataset for compatibility with previouswork.theauthorscollectedpythonprogramsfrom github repositories and removed duplicate files project forks files that do not parse and have more than 30k nodes intheirasts.theyalsoremovedobfuscatedfilesandonly used repositories with permissive licenses including mit bsd and apache.
thecodefilldataset whichwascollectedbyqueryingghtorrent forallnon forkedpythonrepositorieswithmore than stars 58k repositories .
afterdeduplication usingthemethodproposedbyallamanis we ended up with two versions of the original datasets py117k andpy1690kfortheethandcodefilldatasets respectively.note thatpy1690kandpy117kdo not have any common files.
table presents an overview of the contents of the datasets.
weusepy1690kexclusivelyforpre trainingourlm.wethen use90 ofpy117kforfine tuningthemodelonthetaskspresented insection4.
andfinallythelast10 of py117kforevaluation.for the baselines we concatenate py1690kwith the same portion ofpy117kas above for training and evaluate on the remaining ofpy117k.table tpl a results any token prediction approach venue accuracy mrr n gram lstm fse .
.
pointer mixture ijcai .
.
openvocab icse .
.
t xl bi lstm icpc .
.
gpt c fse .
.
travtrans icse .
.
codefill proposed .
.
.
implementation and configuration weusepython sast3 tokenize4 andthedis5librariesinour conversiontool.moreover weusethe huggingface6libraryforthe implementationofourgpt 2andmtlmodels.wesetthelearning rateto0.
maximumsequencelengthto2048 andtrainedour model for epochs.
we set the remaining parameters to default values.ourexperimentsareconductedonamachineequippedwith two geforce gtx ti gpus an intel r xeon r cpu e5 v4 .60ghz cpu with core processors and 128g ram.
results and discussion in this section we present the results for each evaluation task along with an ablation study and a characterization of the models performance.
.
token level prediction tlp .
.
any token prediction.
the most basic form of evaluation for an autocompletion model is to gauge its ability to predict the next tokengivensomecontextasinput.tlp acanprovideanoverview on the ability of an autocompleter to predict however it does not account for the prior probabilities of different types of tokens.
we present this taskfor compatibility with existingwork and further elaboratecodefill sperformanceinthefollowingtasks.theresults can be seen in table our model outperforms all the baselines across all metrics.
.
.
token type prediction.
we investigate the performance of the models when predicting different types of tokens i.e.
their abilitytoassimilatehowdevelopersusegrammartoexpressconcepts.
models generally struggle more with specific token types.
forinstance itisknownthat predictingidentifiersisharderthan predictingkeywords .table4presenttheaccuracyandmrr results based on all token types.
as demonstrated codefill out performs the baselines for all token types based on both metrics except formrronkeywordsandpunctuation whereitsperformanceisonpar .transformer basedapproachesarehighlycapableofpredictingspecifictypesoftokens namelykeywordsandpunctuation effectively this means that given enough training examples theycanefficientlylearnsyntacticalpatterns.predictingidentifiers andliteralsacrossallmodelsismorechallenging.foridentifiers 408codefill multi token code completion by jointly learning from structure and naming sequences icse may pittsburgh pa usa table tpl b results token type predictionsmetricapproach identifier keyword punctuation literals operators all token percentage accuracyn gram lstm .
.
.
.
.
.
pointer mixture .
.
.
.
.
.
openvocab .
.
.
.
.
.
t xl bi lstm .
.
.
.
.
.
gpt c .
.
.
.
.
.
travtrans .
.
.
.
.
.
codefill .
.
.
.
.
.8mrrn gram lstm .
.
.
.
.
.
pointer mixture .
.
.
.
.
.
openvocab .
.
.
.
.
.
t xl bi lstm .
.
.
.
.
.
gpt c .
.
.
.
.
.
travtrans .
.
.
.
.
.
codefill .
.
.
.
.
.
all models result range from to accuracy.
in both cases codefill maintains a non trivial edge over the baselines which we attribute to the statement completion task.
we believe it helps codefill to learn syntactical patterns over longer ranges.
.
.
leaf node prediction.
wecompareeachmodel sperformance inpredictingdifferenttypesof leafnodes inanast e.g.
function calls variables and attribute names.
tables present the accuracy and mrr results for this task.
codefill is the best model in both accuracy and especially mrr.
this means that its name predictions whichisarguablythemostimportantfeatureforanautocompleter are out of times correct and have a high probability of being included in the top suggestions.
.
.
cardinal point completion.
in table we report the performance of modelswhen predicting at cardinalpoints described in section .
.
as indicated codefill outperforms all the baselines.
consequently itismorecapableofpresentingcorrectrecommenda tionsatpointswhereautocompletionismorelikelytobetriggered.
.
statement level prediction slp we reportthe resultsfor autocompletingcode statements by predicting the remaining ntokens at a given statement position with nranging between and .
figure presents the results of this experiment based on the achieved meteor and rouge l scores.
alltransformer basedmodels areconsistentlymore capablethanthethreebaselineapproaches.codefillimprovesoverallcompetitors.themargingrowswiderasthenumberoftokensre quiredtocompletestatementsincrease especiallyintherouge lcase .thisresulthighlightsthemeritsofourstatementcompletion task.inturn thiscanhelpdeveloperscodefasterbyreducingthe number ofrequiredkeystrokes theexperience ofusing statement completionshouldbereminiscentoftextlinecompletioninpopulartable tlp c results leaf node predictionmetricapproach attribute access names function names numeric constant all token percentage accuracyn gram lstm .
.
.
.
.
pointer mixture .
.
.
.
.
openvocab .
.
.
.
.
t xl bi lstm .
.
.
.
.
gpt c .
.
.
.
.
travtrans .
.
.
.
.
codefill .
.
.
.
.3mrrn gram lstm .
.
.
.
.
pointer mixture .
.
.
.
.
openvocab .
.
.
.
.
t xl bi lstm .
.
.
.
.
gpt c .
.
.
.
.
travtrans .
.
.
.
.
codefill .
.
.
.
.
table tpl d results cardinal points completion approach accuracy mrr n gram lstm .
.
pointer mixture .
.
openvocab .
.
t xl bi lstm .
.
gpt c .
.
travtrans .
.
codefill .
.
online email or document editors.
statistically more than out of statement completions of or fewer tokens will be correct.
.
ablation study weperformanablationstudytoexaminetheimpactofdifferent components ofcodefill .table 7presents the resultsof thisstudy.
we include the performance of a vanilla gpt model to show the importance of employing the mtl approach to jointly train modelsondifferentrepresentationsofsourcecode.theresultsshow thatemployingthemtltechniquetotrainthemodelsjointlyon multipletasksindeedhelpsthemodellearnbetter.next weconduct experimentstocomparehard parameterandsoft parametermodels with the two task mtl model.
it is worth mentioning that for thehard parametersharingvariation weneedtoinputaunified representationtothemodels.thus weconcatenatethetypeand value of each token as xi and then feed the vectors of this concatenatedrepresentationtothemtlmodel.theresultsindicate that the soft parameter sharing works better in our case.
this is probablybecausethissettingallowseachtasktohaveitsownmodel and parameters and then regularizes the distance between them to 409icse may pittsburgh pa usa izadi et al.
figure results for the slp task table7 effectivenessofdifferentcomponentsofthemodel approach tasks train time accuracy mrr gpt value 12h .
.
mtl hp value type 17h .
.
mtl sp value type 19h .
.
mtl sp value type statement 24h .
.
encourage the parameters to be similar.
finally to verify whether adding information regarding statements helps we investigate the effect of adding the third task statement completion.
the results demonstratethattrainingontwodifferentgranularity single token and statement also helps them learn better.
to conclude each componentof theproposedmodeladds toitsvalue.although the training time increases it can be argued that training time is a onetimecost andcanbesignificantlyreducedwithparalleltraining on multiple gpus.
.
runtime characteristics an important aspect of ml based autocompletion tools is their predictionlatency.averyaccuratemodelthattakes1secondper prediction will not be very useful in practice as it will interfere with the developer s workflow.
as table all models feature an averagelatencyoflessthan100milliseconds whichisconsidered the golden standard in the industry.
moreover the model size and number of parameters are importantpracticalaspectsthataffectamodel sdeployment ifthemodel istoobig itwillneedtobedeployedcentrallyandclientsshould connecttothemodelserveroveranetworkconnection whichmay affectlatencynegatively otherwise itcouldbedistributedtothe clients.
as table shows codefill s number of parameters is moretable runtime characteristics approach train time hr latency ms params n gram lstm 168m pointer mixture 177m openvocab 145m t xl bi lstm 173m gpt c 125m travtrans 119m codefill 258m thanotherbaselinesduetoourarchitecturespecification.however thesizeofalltransformer basedmodelsmakesthemimpractical for distribution to clients necessitating centralized deployments.
contributions and implications autocompletionisapopularresearcharea however theexisting challenges leave substantial margin for improvement particularly for recommending identifiers or completing longer sequences .
inthisstudy codefilllearnsfromsequencesofbothtokentypes andtokennamessimultaneouslyusingmtl.thecontributionof this work is twofold technical novelty similar to the state of the art w e use transformers for learning a name based sequencing model and similar to the studies by liu et al.
we use the mtl technique to condition our models under different tasks.
however intellicodecompose treats code as natural text neglecting the rich structure inherent in programs.
moreover they focus on multilingual lms.
travtrans uses serialized asts in an attempt to learn from structure however we show that our novel transformation whichwedesignedsothatitisclosertohowdevelopers 410codefill multi token code completion by jointly learning from structure and naming sequences icse may pittsburgh pa usa treat source code structure outperforms travtrans .
codefill also learnsfromournovelstatementcompletiontasktoconsiderlonger contexts.
both figure 6andtable 7show that this technique improves the model probably by helping it better utilize completion context.
the combination of the above demonstrably results in higher evaluation scores and better recommendations.
evaluation we propose two novel evaluation tasks cardinal point and statementcompletion toaddressdeficienciesincurrent autocompletion evaluation setups.
we also collect pre process deduplicate and sharean large python dataset consistingof practically all python code on github.
threats to the validity threatstointernalvalidity theseincludethethreatspertaining to the parameters affecting the performance of the model.
another threat in this section relates to the errors in the implementation of the baselines.
for all of these approaches we have used the replication packages provided by these studies.
threats toexternal validity these threatsrelate to the qualityofthedatasetsweusedandthegeneralizabilityoftheresults.
we used two python datasets py117k is a benchmark dataset frequently used in the literature .
py1690k our second dataset is ten times larger with approximately .7mprogram files.moredatacanleadtomoregeneralizableresults.furthermore as allamanis.
suggests we have de duplicated both datasets to avoid biasing the models.
all of the programs in both datasets are collected from open source github repositories.
however further studies are needed to validate and generalize our findings to other programming languages.
threats to construct validity these relate to the suitability oftheevaluationsettingandmetrics.inthiswork wehavetried toincorporatediverseevaluationmeasures.forthetlptask we have used standard evaluation metrics namely accuracy and mrr in the top oneand top ten recommendations which are both frequently used in the literature .
furthermore we use rouge l and meteor scores for evaluation in the slp task asused in previous studies on source sequence of code generation summarization and translation .
conclusion and future work unlike natural language text source code is more structured its grammarismorewelldefinedbutitsvocabularyisordersofmagnitudebigger.consequently nlp basedmodelsandcorresponding evaluation methods need to be adapted to the particular case of source code.
in this work we proposed codefill a transformer based generativelmforsourcecodepre trainedonthreetaskscloselyrelevanttoprogramming.givenacontextoftokens andtheirtypes codefillistrainedtopredict thetypeofthenexttoken its value and the values of up to nnext tokens.
we employ the mtl approach to jointly train codefill on the above tasks.
we also propose novel evaluation tasks cardinal point prediction and statement level multi tokenprediction whichwe arguethat they better represent how autocompletion systems are used in practice.
weextensivelyevaluatecodefillagainstsixbaselinesonbothtasks.
ourresultsindicatethatcodefilloutperformsallthebaselinesinallscenarios achievingstateoftheartscoresonbothaccuracy .
andmrr .
inthebasictoken levelpredictiontask.moreover weshowthatcodefillalsolearnstoautocompletestatementsofup to4tokenswithover70 accuracy asignificantimprovementover the baselines making it practical to offer statement completions as an ide feature.
inthefuture weplantoincorporatemoredomainspecificknowledgeonaspectsoftrainingandevaluatingatrainingmlmodels.
forinstance onecanlimitthecontextfedtothemodelbasedonthe programminglanguagetobetterincorporaterelatedinformation offunctionsandnestedscopesinapieceofcode.wealsoplanto further investigate statement completion including better metrics for its evaluation.