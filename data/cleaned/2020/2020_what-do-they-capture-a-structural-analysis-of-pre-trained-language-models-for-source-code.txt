what do they capture?
a structural analysis of pre trainedlanguage models for source codeyao wan school of computer science andtechnology huazhong university ofscience and technology chinawanyao hust.edu.cnwei zhao school of computer science andtechnology huazhong university ofscience and technology chinamzhaowei hust.edu.cnhongyu zhanguniversity of newcastleaustraliahongyu.zhang newcastle.edu.auyulei suischool of computer science university of technology sydneyaustraliayulei.sui uts.edu.auguandong xuschool of computer science university of technology sydneyaustraliaguandong.xu uts.edu.auhai jin school of computer science andtechnology huazhong university ofscience and technology chinahjin hust.edu.cnabstractrecently many pre trained language models for source code havebeen proposed to model the context of code and serve as a basis fordownstream code intelligence tasks such as code completion codesearch and code summarization.
these models leverage maskedpre training and transformer and have achieved promising results.however currently there is still little progress regarding inter pretability of existing pre trained code models.
it is not clearwhythese models work andwhatfeature correlations they can capture.in this paper we conduct a thorough structural analysis aimingto provide an interpretation of pre trained language models forsource code e.g.
codebert and graphcodebert from threedistinctive perspectives attention analysis probing on theword embedding and syntax tree induction.
through compre hensive analysis this paper reveals several insightful f indings thatmay inspire future studies attention aligns strongly with thesyntax structure of code.
pre training language models of codecan preserve the syntax structure of code in the intermediate rep resentations of each transformer layer.
the pre trained modelsof code have the ability of inducing syntax trees of code.
theses f indings suggest that it may be helpful to incorporate the syntaxstructure of code into the process of pre training for better coderepresentations.ccs concepts software and its engineering reusability.
also with national engineering research center for big data technology and system services computing technology and system lab cluster and grid computing lab hust wuhan china.permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro f it or commercial advantage and that copies bear this notice and the full citationon the f irst page.
copyrights for components of this work owned by others than acmmust be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci f ic permission and or afee.
request permissions from permissions acm.org.icse may pittsburgh pa usa association for computing machinery.acm isbn .
.
.
.
representation deep learning pre trained language model probing attention analysis syntax tree induction.acm reference format yao wan wei zhao hongyu zhang yulei sui guandong xu and hai jin.
.
what do they capture?
a structural analysis of pre trained lan guage models for source code.
in44th international conference on softwareengineering icse may pittsburgh pa usa.acm newyork ny usa pages.
introductioncode representation learning also known as code embedding aimsto encode the code semantics into distributed vector representations and plays an important role in recent deep learning based modelsfor code intelligence.
code embedding can be used to support avariety of downstream tasks such as code completion codesearch and code summarization .current approaches to code embedding mainly fall into two cat egories from the perspectives of supervised and unsupervised orself supervised learning paradigms.
the supervised approachesare typically developed for speci f ic tasks following theencoder decoderarchitecture .
in this architecture anencodernetwork e.g.
lstm cnn and transformer is used to produce a vectorrepresentation of a program.
the resulting vector is then fed as aninput into adecodernetwork to perform some prediction tasks e.g.
summary generation or token sequence prediction .recently there has been signi f icant improvement in the expres siveness of models that can learn the semantics of code such asself attention based architectures like transformer .
anotherline of code embedding research is based on unsupervised learning.some approaches utilize word embedding techniques to representsource code which aim to learn a global word embeddingmatrixe r u1d449 u1d437 where is the vocabulary size andais the num ber of dimensions.
code2vec is such kind of approach whichlearns a distributed representation of code based on the sampledpaths fromabstract syntax trees asts .recently self supervised models which are pre trained throughmasked language modeling have attracted much attention.
pre trained models such as bert and elmo are representativeapproaches and have been successfully used in a variety of tasks23772022 ieee acm 44th international conference on software engineering icse icse may pi t tsburgh pa usayao wan wei zhao hongyu zhang yulei sui guandong xu and hai jinin nlp.
inspired by the self supervised pre training in nlp therehave been some recent efforts in developing pre trained models onlarge scale code corpus for software engineering tasks.
for example cubert is a pre trained bert model using .4m python f ilesfrom github.
codebert is a bimodal pre trained model onsource code and natural language descriptions.
to incorporate thesyntax structure of code guo et al.
further propose graphcode bert to preserve the syntax structure of source code by introducingan edge masking technique over data ow graphs.
with much effortbeing devoted to pre trained code embedding there is a pressingdemand to understandwhythey work andwhatfeature correctionsthey are capturing.in the nlp community several recent studies have been madetowards interpreting the pre trained language models e.g.
bert from the perspective of attention analysis and task probing.
thiskind of research has become a subspecialty of bertology which focuses on studying the inner mechanism of bert model .however in software engineering such an understanding is yet tobe achieved.
often we see pre trained language models that achievesuperior performance in various software engineering tasks but donot understand why they work.
currently there have been severalempirical studies that aim to investigate the effectiveness of codeembedding.
for example kang et al.
empirically assessed theimpact of code embedding for different downstream tasks.
chirkovaand troshin conducted another study to investigate the capabil ities of transformers to utilize syntactic information in differenttasks.
however these studies only show in which scenarios a codeembedding technique works better without explaining the inner mechanism of why the embedding achieves good results.
therefore it is still not clear why the pre trained language models work andwhat they indeed capture in the context of software engineeringtasks.in this work we explore the interpretability of pre trained codemodels.
more speci f ically we try to answer the following question can the existing pre trained language models learn the syntacticalstructure of source code written in programming languages?address ing this question plays an important role in understanding thelearned structure of deep neural networks.
we conduct a thoroughstructural analysis from the following three aspects aiming to pro vide an interpretation of pre trained code models e.g.
codebert graphcodebert .
as the f irst contribution we analyze the self attention weightsand align the weights with the syntax structure see sec.
.
.given a code snippet our assumption is that if two tokensare close to each other in the ast i.e.
have a neighbourhoodrelationship the attention weights assigned to them shouldbe high.
our analysis reveals that the attention can capturehigh level structural properties of source code i.e.
themotifstructurein asts.
as the second contribution we design a structural probingapproach to investigate whether the syntax structureis embedded in the linear transformed contextual word em bedding of pre trained code models see sec.
.
.
using ourprobe we show that such transformations also exist in thepre trained language models for source code showing ev idence that the syntax structure of code is also embeddedimplicitly in the vectors learned by the model.
!
figure a general framework for transformer based lan guage model pre training .
as the third contribution we investigate whether the pre trained language models for source code provide the abilityof inducing the syntax tree without training see sec.
.
.we f ind that the pre trained models can indeed learn thesyntax structure of source code to a certain extend.our work is complementary to other works that aim to designbetter neural networks for source code representation.
we believethat the f indings revealed in this paper may shed light on the innermechanism of pre training models for programming languages aswell as inspire further studies.
backgroundin this section we introduce some background knowledge of ourwork including transformer and pre training language model.
fig ure shows a general framework for transformer based languagemodel pre training.
.
self attention based transformertransformer which is solely based on self attention has be come a popular component for code representation learning.
letg p1 p2 ... p u1d45b denote a code snippet of a sequence of to kens with length ofq.
a transformer model is composed ofjlayers of transformer blocks to represent a code snippet into con textual representation at different levelsh u1d459 whereddenotes thed th layer.
for each layer the layer repre sentationh u1d459is computed by thed th layer transformer blockh u1d459 transformer u1d459 h u1d459 d ... j .in each transformer block multiple self attention heads areused to aggregate the output vectors of the previous layer.
givenan inputg the self attention mechanism assigns each tokenp u1d456aset of attention weights over the token in the input a n p u1d456 m u1d456 g m u1d456 g ... m u1d456 u1d45b g 2378what do they capture?
a structural analysis of pre trained language models for source codeicse may pi t tsburgh pa usa !
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
a a python code snippet with its ast b attention heatmap in layer c attention distribution in layer head 12figure visualization of self attention distribution for a code snippet in codebert.
a a python code snippet with itscorresponding ast.
b heatmap of the averaged attention weights in layer .
c self attention distribution in layer head .the brightness of lines indicates the attention weights in a speci f ic head.
if the connected nodes appear in themotif structureof the corresponding ast we mark the lines in red.wherem u1d456 u1d457 g is the attention thatp u1d456pays top u1d457.
the attentionweights are computed from the scaled dot product of thequeryvectorofp u1d456 and thekey vectorofp u1d457 followed by a softmax.
inthe vectorized computing a general attention mechanism can beformulated as the weighted sum of the value vectorv using thequery vectorqand the key vectork a q k v so max parenleftbiggqk u1d447 radicalbigamodel parenrightbigg v whereamodelrepresents the dimension of each hidden representa tion.
for self attention q k andvare mappings of the previous hid den representation by different linear functions i.e.
q h u1d459 1w u1d459 u1d444 k h u1d459 1w u1d459 u1d43e andv h u1d459 1w u1d459 u1d449 respectively.
at last the encoderproduces the f inal contextual representationh u1d43f which is obtained from the last transformer block.in order to utilize the order of the sequential tokens the posi tional encodings are injected to the input embedding.w u1d456 h p u1d456 eq p u1d456 wherehdenotes the word embedding layer andeq denotes the po sitional embedding layer.
typically the positional encoding impliesthe position of code token based on sine and cosine functions.
.
pre training language modelgiven a corpus each sentence or code snippet is f irst tokenizedinto a series of tokens e.g.
byte pair encoding bpe .
be fore bert s pre training it takes the concatenation of two seg ments as the input de f ined asg1 p1 p2 ... p u1d45b andg2 f1 f2 ... f u1d45a whereqand.denote the lengths of two seg ments respectively.
the two segments are always connected by aspecial separator token .
the f irst and last tokens of each se quence are always padded with a special classi f ication token and an ending token respectively.
finally the input of eachtraining sample will be represented as follows p1 p2 ... p u1d45b bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipupright f1 f2 ... f u1d45a bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipupright .the input is then fed into a transformer encoder.
during bert spre training two objectives are designed for self supervised learn ing i.e.
masked language modeling mlm andnext sentence pre diction nsp .
in the masked language modeling the tokens of aninput sentence are randomly sampled and replaced with the specialtoken .
in practice bert uniformly selects of the inputtokens for possible replacement.
among the selected tokens are replaced with are unchanged and the left arerandomly replaced with the selected tokens from vocabulary .for next sentence prediction it is modeled as a binary classi f icationto predict whether two segments are consecutive.
training positiveand negative examples are conducted based on the following rules if two sentences are consecutive it will be considered as a posi tive example otherwise those paired segments from differentdocuments are considered as negative examples.recently self supervised learning using masked language mod eling has become a popular technique for natural language un derstanding and generation .
in the context ofsoftware engineering several pre trained code models have alsobeen proposed for program understanding.
in this paper we selecttwo representative pre trained models for code representations codebert which takes the concatenation of source code andnatural language description as inputs and pre trains a languagemodel by masking the inputs and graphcodebert whichimproves codebert by incorporating the data ow informationamong variables into model pre training.
motivationprior work in nlp has pointed out that the self attention mecha nism in transformer has the capability of capturing certain syntaxinformation in natural languages.
inspired by this we visualize and2379icse may pi t tsburgh pa usayao wan wei zhao hongyu zhang yulei sui guandong xu and hai jin figure an illustration of attention analysis probing on word embedding and syntax tree induction with a speci f ic pythoncode snippet.investigate the attention distribution of the pre trained model i.e.
codebert for a code snippet as shown in figure .
figure a shows a python code snippet with its ast.
in this paper we de f inethe syntax structure of the ast consisting of a non leaf node withits children e.g.
if statementandblockin figure a as amotifstructure.
we believe that the syntax information of code can berepresented by a series ofmotif structures.given a code snippet and its corresponding ast figure b visualizes the self attention heatmap for a speci f ic layer i.e.
layer5 which is an average of attention weights over multiple heads.from this f igure we can observe that several patterns indeed existin the self attention heatmap depicted as groups of rectangles marked in red .
these rectangles indicate that the code tokensform groups.
interestingly we can also f ind that each group oftokens is close to each other in the ast.
taking if exit codeis not none as an example which is anifstatement we cansee that in the ast all of these tokens are in the same branch ofif statement.
in addition we can see that these code tokens arealso closely connected in the self attention heatmap.moreover we also visualize the self attention distribution in aspeci f ic head layer head to analyze the connections betweentwo tokens as shown in figure c .
in this f igure the brightnessof lines indicates the attention weights in a speci f ic head.
if theconnected nodes appear in themotif structureof the correspondingast we mark the lines in red.
from this f igure we can observethat those code tokens i.e.
if exit code not and none that are in amotif structureindeed have been highlighted as closelyconnected by self attention.as we have identi f ied several patterns from the attention distri bution which provide some hints to the syntax structure of code itis necessary for us to further explore this phenomenon with quan titative analysis and systematic assessment.
motivated by the afore mentioned observations this paper investigateswhypre trainedlanguage models for source code work andwhatfeature correlationsthey are capturing by analyzing the self attention mechanism.
inparticular we analyze two outputs of the self attention mechanism i.e.
the attention distribution and the generated hidden vectors under the framework of transformer.
structural analysis of pre trainedlanguage models for source codein this section we propose three distinct structural analysis ap proaches i.e.
attention analysis structural probing on word em bedding and syntax tree induction to interpret pre trained codemodels i.e.
codebert and graphcodebert .
figure gives anillustration of the three structural analysis approaches.
before intro ducing each approach we f irst introduce several common notationsthat will be used later.
let p1 p2 ... p u1d45b denote the code tokensequence of code snippetg with lengthq.
on thed th layer of trans former we use h u1d4591 h u1d4592 ... h u1d459 u1d45b to denote the sequence of contextualrepresentation of each code token.
.
attention analysiswe start by analyzing the self attention weights which are the coremechanism for pre training transformer based models.
intuitively attention de f ines the closeness of each pair of code tokens.
fromthe lens of attention analysis we aim to analyze how attentionaligns with the syntactical relations in source code.
in particular we consider the syntactical relations such that the attention weightis high between two ast tokens sharing the same parent node.figures a and b illustrate the attention analysis.
given a codesnippet with its ast we can see that the leaf nodesforandinshare the same parent.
as expected this structure is aligned withthe attention weightmfor inbetween these two nodes.speci f ically on each transformer layer we can obtain a set ofattention weightsmover the input code wherem u1d456 u1d457 is the atten tion fromk th code token toku1d457 th token.
here we de f ine an indicatorfunctionku1d453 p u1d456 p u1d457 that returns ifp u1d456andp u1d457are in a syntacticrelation p u1d456andp u1d457have the same parent node in the ast and0 otherwise.
we de f ine the attention weight betweenp u1d456andp u1d457asm u1d456 u1d457 g and ifp u1d456andp u1d457are very close the attention weightshould be larger than a threshold i.e.
m u1d456 u1d457 g ku1d703.
therefore theproportion of high attention token pairs m u1d456 u1d457 g ku1d703 aggregated1note that in this paper we exclude the condition that u1d464 u1d456and u1d464 u1d457are adjacent intextual appearance.2380what do they capture?
a structural analysis of pre trained language models for source codeicse may pi t tsburgh pa usaover a datasetccan be formulated as follows e u1d6fc ku1d453 summationtext.
c summationtext.
u1d456 summationtext.
u1d457 u1d7d9 u1d6fc u1d456 u1d457 u1d703 ku1d453 p u1d456 p u1d457 summationtext.
c summationtext.
u1d456 summationtext.
u1d457 u1d7d9 u1d6fc u1d456 u1d457 u1d703 whereku1d703is a threshold selected for high con f idence attention weights.variability.equation shows that the portion of aligned atten tion is only dependent on the absolute value of attention weightm u1d456 u1d457 g .
we hypothesize that those heads that are attending to theposition i.e.
those heads that focus on the previous or next codetoken would not align well with the syntax structure of code sincethey do not consider the content of the code token.
to distinguishwhether the heads are attending to content or position of the codetoken we further investigate the attention variability which mea sures how attention varies over different inputs.
the attentionvariability is formally de f ined as follows ku1d44e ku1d45fkku1d44e ku1d44fkdkku1d461ku1d466.alt u1d6fc summationtext.
u1d436 summationtext.
u1d456 u1d456 summationtext.
u1d457 barex barexm u1d456 u1d457 g m u1d456 u1d457 barex barex2 summationtext.
u1d436 summationtext.
u1d456 u1d456 summationtext.
u1d457 1m u1d456 u1d457 g where m u1d456 u1d457is the mean of m u1d456 u1d457 g over allg c. we only includethe f irstku1d441tokens ku1d441 of eachg cto ensure a sufficientamount of data at each positionk.
the positional patterns appear tobe consistent across the entire sequence.
the high variability wouldsuggest a content dependent head while low variability wouldindicate a content independent head.
figure an illustration of the connection between distanceand the syntax structure.
.
structural probing on word embeddingin this approach we propose a structural probing analysis approachto investigate whether a pre trained model embeds the syntacticalstructure in its contextual word embedding.
the key idea of ourapproach is that tree structure is embedded if the transformed spacehas the property that the euclidean distance between two words vectors corresponds to the number of edges between the wordsin the syntax tree.
one question may arise why does the distancebetween nodes in the syntax tree matter for syntax information.thisis because the distance metric i.e.
the path length between eachpair of words can recover the syntax tree simply by identifying thatnodesfandku1d463are neighbors if the distance between them equalsto .
this has also been shown in the code2vec which utilizesthe contextual information among a set of paths sampled fromthe ast to represent the structure information of code.
figure 4gives a toy example to illustrate the connection between distanceand syntax structure.
let p1 ... p u1d456 ... p u1d457 ... p u1d45b denote thesequence of code tokens for code snippetg if we know the distancebetween every pair of nodes we can induce the syntax structure ofcode.
note that the distance metric which measures the distanceamong any two code tokens can learn the global syntax structureof code to some extent.figures a and c illustrate the structural probing on wordembedding.
taking the leaf nodesforandinwhich share the sameparent as an example the euclidean square of distance betweenthese two nodes is .
we f irst map the representations of these twotokens into a hidden space via a linear transformationku1d435 obtainingthe hgku1d461qku1d45ffor and hgku1d461qku1d45fin respectively.
we believe that if the eu clidean square of distance between hgku1d461qku1d45ffor and hgku1d461qku1d45finis closeto the syntax structure betweenforandinis well preserved.in particular we learn the mapping functionku1d435in a supervisedway.
formally given a code sequence p1 p2 ... p u1d45b as the input and each model layer generates word vectors h1 h2 ... h u1d45b .w ecompute the square of distance between the two word vectorsh u1d456andh u1d457in high dimensional hidden space as follows a u1d435 h u1d456 h u1d457 ku1d435 h u1d456 h u1d457 u1d447 ku1d435 h u1d456 h u1d457 wherekandku1d457are indices of the words in the code sequence.
theparameters of the structural probe we use are exactly the matrixku1d435 liner mapping which is trained to reconstruct the tree distancebetween all word pairs p u1d456 p u1d457 in the code sequenceku1d447in the train ing set of source code.
we de f ine the loss function for parametertraining as follows min u1d435 summationdisplay.
c1 g summationdisplay.
u1d456 u1d457 barex barex barexa u1d447 u1d450 p u1d456 p u1d457 a u1d435 h u1d456 h u1d457 barex barex barex where g is the length of code sequenceg a u1d447 u1d450 p u1d456 p u1d457 denotes thedistance between code tokens in ast anda u1d435 h u1d456 h u1d457 2denotes thedistance between the embedding vectors of code tokens for codesequenceg.
the f irst summation calculates the average distancefor all training sequences while the second one sums all possiblecombinations of any two words in the code sequences.
the goal ofthis supervised training is to propagate the error backwards andupdate the parameters of the linear mapping matrixku1d435.
.
syntax tree inductionin this approach we propose to investigate the capability of pre trained code model in inducing syntax structure withouttraining.the key insight of our approach is that if the distance betweentwo tokens is close e.g.
with a similar attention distribution orwith a similar representation they are expected to be close in thesyntax tree i.e.
sharing the same parent.
based on this insight wepropose to induce the syntax tree from the distances between twotokens.
our assumption is that if the induced tree derived fromthe pre training model is similar to the gold standard syntax tree we can reasonably infer that the syntactic structures have beenpreserved during the model pre training.we propose to induce the syntax tree based on the syntactic dis tance among code tokens which was f irst introduced for grammarinduction for natural languages .
formally given a code se quence p1 p2 ... p u1d45b we computed a1 a2 ... a u1d45b wherea u1d456corresponds to the syntactic distance between tokensp u1d456andp u1d456 .
eacha u1d456is de f ined as follows a u1d456 ku1d453 ku1d454 p u1d456 ku1d454 p u1d456 2381icse may pi t tsburgh pa usayao wan wei zhao hongyu zhang yulei sui guandong xu and hai jinalgorithm greedy top down binary syntax tree induc tion based on syntax distances.1ku1d446 p1 p2 ... p u1d45b a sequence of code tokens withlengthq 2d a1 a2 ... a u1d45b a vector whose elements are thedistance between two adjacent code tokens 3functiontree ku1d446 d 4ifd then5qqah jhku1d44eku1d453 ku1d4460 6else7k argmax d 8gkuni210ekda u1d459 tree ku1d446 u1d456 d u1d456 9gkuni210ekda u1d45f tree ku1d446 u1d456 d u1d456 10qqah ku1d441qah gkuni210ekda u1d459 gkuni210ekda u1d45f 11end12returnqqah 13end functionwhereku1d453 andku1d454 denote the function of distance measurementand code representation learning respectively.
here we measurethe syntactic distance between two tokens from their intermediaterepresentation vector as well as the self attention distribution withvarious distance measurement functions.
speci f ically letku1d454 u1d463 u1d459andku1d454 u1d451 u1d459 u1d458denote the functions to generate the intermediate representa tion and self attention ind th layer andku1d458 th head.
to calculate thesimilarity between vectors we have many options in terms of theintermediate representation and attention distributions.
for exam ple we can usej1 andj2 to calculate the similarity between twointermediate representation vectors.
we can use jensen shannondivergence and hellinger distance to calculate the similar ity between two attention distributions.
table summarizes all theavailable distance measurement functions.once the distance vectordis computed we can easily convertit to the target syntax tree through a simple greedy top downinference algorithm based on recursive partitioning of the input as shown in algorithm .
alternatively this tree reconstructionprocess can also be done in a bottom up manner which is left forfurther exploration .injecting bias into syntactic distance.from our observation theast of source code tends to be right skewed.
this has also been awell known bias in the constituency trees for english.
therefore itmotivates us to in uence the induced tree such that they are mod erately right skewed following the nature of gold standard asts.to achieve this goal we propose to inject the inductive deviationinto the framework by simply modifying the value of the syntacticdistance.
in particular we introduce the right skewness bias to in uence the spanning tree to make it right biased appropriately .formally we compute a u1d456by appending the following linear biasterm to everya u1d456 a u1d456 a u1d456 ku1d706 ku1d434 ku1d43a d parenleftbigg1 .
k parenrightbigg where avg outputs an average of all elements in a vector ku1d706is ahyperparameter andkranges from to.
where.
q .table the de f inition of different functions of distancemeasurement to compute the syntactic distance betweentwo adjacent words in a code sequence.
note thatku1d45f ku1d454 u1d463 p u1d456 ku1d454 u1d463 p u1d456 ku1d443 ku1d454 u1d451 p u1d456 ku1d444 ku1d454 u1d451 p u1d456 kuni210edenotes the hiddenembedding size andqdenotes the length of code sequence.functionde f initiondistance functions for intermediate representationsj1 ku1d45f summationtext.
uni210e u1d456 ku1d45f u1d456 u1d456 j2 ku1d45f radicalbig summationtext.
uni210e u1d456 ku1d45f u1d456 u1d456 2distance functions for attention distributionsku1d43dku1d446ku1d437 ku1d443 ku1d444 radicalbig ku1d437 u1d43e u1d43f ku1d443 ku1d440 ku1d437 u1d43e u1d43f ku1d444 ku1d440 2whereku1d440 ku1d443 ku1d444 2andku1d437 u1d43e u1d43f ku1d434 ku1d435 summationtext.
u1d464 ku1d434 p log u1d434 u1d464 u1d435 u1d464 ku1d43bku1d438j ku1d443 ku1d444 radicalbig summationtext.
u1d45b u1d456 parenleftbig e u1d456 ku1d45e u1d456 parenrightbig2note that introducing such a bias can also examine what changesare made to the resulting tree structure.
our assumption is that ifinjecting the bias does not affect the performance of the pre trainedmodel for unsupervised analysis we can infer that they capture thebias to some extent.similarity between two trees.here we introduce the way we mea sure the similarity of the induced tree and the gold standard ast.speci f ically we f irst transform the tree structure into a collectionof intermediate nodes where each intermediate node is composedof two leaf nodes.
then we measure the similarity between the twocollections.
figure shows a toy example to illustrate the calcu lation of similarity between two trees i.e.
the gold standard ast figure a and induced tree figure b .
as shown in figure a the gold standard ast consists of four intermediate nodes i.e.
ku1d4471 ku1d4472 ku1d4473 andku1d4474 .
for each intermediate we further expand it usingtwo leaf nodes.
for example theku1d4471is expanded into p1 p6 wherep6is randomly selected from thep4 p5 andp6.
similarly we alsotransform the induced tree into a collection of leaf nodes.
figure a toy example to illustrate the calculation of simi larity between the gold standard ast and induced tree.given two sets we use theku1d4391 score to measure their similar ity.
letku1d446denote the set of gold standard tree andku1d446 denote theset of predicted tree we can calculate the precision and recall byeku1d45fhgk kqq u1d446 u1d446 u1d446 andku1d45fhgku1d44edd u1d446 u1d446 u1d446 respectively.
the f1 score2382what do they capture?
a structural analysis of pre trained language models for source codeicse may pi t tsburgh pa usa a codebert python b graphcodebert python c codebert java d graphcodebert java e codebert php f graphcodebert php figure consistency between the attention and ast for the codebert and graphcodebert on different programminglanguages i.e.
python java and php .
these heatmaps show the proportion of high con f idence attention weights m u1d456 u1d457 ku1d703 from each head which connect those code tokens that in themotif structureof ast.
the bars show the maximum value of eachlayer.is the the harmonic mean of precision and recall as follows ku1d4391 eku1d45fhgk kqq ku1d45fhgku1d44eddeku1d45fhgk kqq ku1d45fhgku1d44edd.
experimental design and resultsin this section we conduct experiments to explore what the pre trained code models capture from three distinct aspects i.e.
atten tion analysis structural probing on word embedding and syntaxtree induction.
.
experimental designwe investigate two transformer based pre trained models i.e.
codebert and graphcodebert both of which are com posed of layers of transformer with attention heads.
thesemodels are both pre trained on codesearchnet a large scaleof code corpora collected from github across six programminglanguages.
the size of representation in each transformer layer isset to .
without loss of generability we select python java andphp as our target programming languages and use the correspond ing dataset from codesearchnet.
for all experiments we excludethe attention to the delimiter as it has been proven to be a no op attention mark as well as the attention to the mark which is not explicitly used for language modeling.
notethat in the pre training phase the input code snippets have beentokenized into subwords viabyte pair encoding bpe beforebeing passed to the pre trained model.
however our analyses areall based on the word level code tokens.
therefore we representeach word by averaging the representations of its subwords.
allthe experiments were conducted on a linux server with 128gbmemory and a single 32gb tesla v100 gpu.through comprehensive analysis we aim to answer the follow ing research questions rq1 attention analysis does attention align with thesyntax structure in source code?
rq2 probing on word embedding are the syntax struc ture encoded in the contextual code embedding?
rq3 syntax tree induction are the pre trained codemodel able to induce the syntax structure of code?
.
rq1 attention analysisin attention analysis we aim to investigate whether the attentionaligns with the syntax structure of source code.experimental settings.following we set the attention thresh oldku1d703in equation as .
so as to ensure selecting high con f idenceattention and retaining enough data for our analysis.
we leavethe analysis of the impact ofku1d703in future work.
furthermore ouranalysis is based on a subset of code snippets randomly sam pled from the training dataset.
in order to reduce the memory andaccelerate the computation process we truncate all the long codesequences within the length of .
we only include the results ofattention heads where at least high con f idence attention scoresare available in our analysis.2383icse may pi t tsburgh pa usayao wan wei zhao hongyu zhang yulei sui guandong xu and hai jin variability .
content dependent p f none consistency variability .
position based p f .
low consistency p f .
high consistency variability .
position based layer head 3layer head 11layer head figure visualization of attention heads in codebert along with the value of attention analysis e u1d6fc ku1d453 and attentionvariability given a python code snippet.
left attention visualised in layer head which focuses attention primarily on theposition of next token.
center attention visualized in layer head which disperses attention roughly evenly across alltokens.
right attention visualized in layer head which focuses on the content and is highly aligned with the ast.
figure the variability of attention distribution by layer head in python.
high values indicate content dependentheads and low values indicate position based heads.experimental results.figure shows how attention aligns withthe ast structure for codebert and graphcodebert on differentprogramming languages i.e.
python java and php according tothe indicators de f ined in equation .
the f igure shows the propor tion of high con f idence attention weights m u1d456 u1d457 ku1d703 from each headwhich connect those code tokens that in themotif structureof ast.the bar plots show the maximum score of each layer.
from this f igure we can observe that the most aligned heads are located inthe deeper layers and the concentration is as high as .
layer11 head in codebert and layer head in graphcode bert .
these high scores indicate thatattention aligns stronglywith themotif structurein ast especially in the deeperlayers.this is because the heads in deeper layers have strongercapabilities in capturing longer distances.although there is a strong alignment between the attention andthe syntax structure of code it is still necessary to distinguishwhether the attention is based on the position or content of codetoken as mentioned in sec.
.
.
in figure we show the atten tion variability of attention heads in codebert for a python codesnippet.
figure left and figure center show two examplesof heads that put more focus on the position respectively fromlayer head and layer head .
based on the variabilityde f ined in equation we can see that the attention in layer head is evenly dispersed with the variability of .
.
more over in layer head it is apparent to see that the attentionis focusing on the next token position.
figure right shows thecontent dependent head from layer head which has the high est alignment relationship with the abstract syntax tree structureamong all heads.
in figure we also visualize the variability ofattention distribution by layer head in python.
the high valuesindicate the content dependent heads and the low values indicatethe position based or content independent heads.summary.through attention analysis we f ind that the learnedattention weights are strongly aligned with themotif structurein an ast.
additionally each attention across different headsand layers put different focus on the position and content of thetokens of source code.2384what do they capture?
a structural analysis of pre trained language models for source codeicse may pi t tsburgh pa usatable the average spearman correlation of probing inpython.methodspearman correlationcodebert .60codebert .69codebert .85graphcodebert .
.
rq2 probing on word embeddingwe conduct structural probing on the word embedding of sourcecode to investigate whether the word embedding in the transformer based pre trained model embeds the syntax structure of code.experimental settings.given a pair of code tokens leaf nodes in ast we measure the correlation between the predicted dis tance using word embedding and the gold standard distance in theast.
specially we use the spearman correlation to measurethe predicted distance vector and the gold standard distance vec tor among all samples of code snippets.
when training the lineartransformation matrixku1d435in equation we limit the code lengthto .
we probe the contextual representations in each layer oftransformer and denote the investigated pre trained code modelsas codebert ku1d43eand graphcodebert ku1d43e whereku1d43eindexes the layerof transformer in corresponding model.
to serve as a comparisonagainst the pre trained code models we also design a baseline model codebert which denotes the simple word embedding beforebeing fed into the transformer layers.
in evaluation we averagethe spearman correlation between all f ixed length code sequences.we report the average value of the entire sequence length of 50as the spearman metric as in .experimental results.table shows the results of probing inpython.
from this table we f ind that the codebert withouttransformer layers achieves inferior performance than that withmultiple layers of transformer.
this con f irms our assumption thattransformer has the ability of capturing the syntax information ofsource code.
in addition we can also f ind that graphcodebert per forms better than codebert indicting that it is helpful to explicitlyincorporate the syntax structure into model pre training.figure shows the spearman correlation of probing on the rep resentation in each layer of the models.
we can observe thatthecapability of capturing syntax structure differs across dif ferent layers of transformer.
the best performance is obtainedin the th layer.
for example in python codebert and graph codebert achieve the highest spearman correlation and respectively in the th layer.
furthermore in each layer of trans former graphcodebert still performs better than codebert incapturing the syntax structure of programs written python java and php con f irming the observation from table .figure shows the heatmaps of gold standard and predicteddistances based on pre trained codebert and graphcodebert fora given input python code snippet.
we can see that figures 10c and10d look more similar to the gold standard one figure 10a than tofigure 10b.
in these f igures some matching parts are marked in red.the result con f irms that codebert and graphcodebert withmultiple layers of transformer perform better than codebert without passing through the transformer layers .figure the average spearman correlation for codebertand graphcodebert in multiple programming languages.summary.through embedding analysis we can observe thatthe syntax structure of code has been well preserved in dif ferent hidden layers of the pre trained language models i.e.
codebert and graphcodebert .
.
rq3 syntax tree inductionwe investigate the extent to which pre trained code models capturethe syntax structure of code by inducing a tree.experimental settings.for comparison we introduce four tradi tional greedy top down tree induction baselines for comparison e.g.
random balanced left branching and right branching binary trees.take the random based approach as an example we recursivelypartition the code snippet based on a randomly selected position.in addition we also derive another baseline codebert which isbased on the word embedding before being fed into transformerlayers.
when injecting bias into the syntactic distance we set thehyperparameter of biasku1d706to .
due to the space limitation we onlyreport the f1 scores for six common intermediate nodes in pythonast i.e.
parameters attribute argument list assignment andstatement.experimental results.table presents the results of various mod els for syntax tree induction on the test dataset.
from this table we can observe thatthe right branching tree induction ap proach achieves the best performance among all the base lines con f irming our assumption that the ast tends to beright skewed.when comparing the pre trained code models i.e.
codebert and graphcodebert with other baselines it is clearto see the pre trained code models signi f icantly outperform otherbaselines even without bias injection.
these results show that thetransformer based pre training models are more capable ofcapturing the syntax structure of code to a certain extentthrough pre training on a large scale code corpus.when com paring the pre trained models w and w o bias injection we canobserve that injecting bias can increase the performance of syntaxtree induction up to .
this improvement indirectly shows thatthe current pre trained code models do not capture well the prop erty of the right skewness of ast.
it is worthy mentioning thatthe performance ofassignmenthas been reduced after injection2385icse may pi t tsburgh pa usayao wan wei zhao hongyu zhang yulei sui guandong xu and hai jin a gold standard b codebert prediction c codebert prediction d graphcodebert predictionfigure the heatmaps of gold standard distance and predicted distance based on pre trained codebert and graphcodebertmodels in python.
a gold standard tree distance between all pairs of code tokens in ast.
b d the predicted distance basedon the probing of codebert and graphcodebert.
the darkness of color indicates the closeness of the paired words.table results of syntax tree induction in python.
f function of distance measurement l layer number a attention headnumber avg the average of all attentions.modelf l a f1parameters attribute argument list assignment statementbaselinesrandom trees .
.
.
.
.
.
.
balanced trees .
.
.
.
.
.
.
left branching trees .
.
.
.
.
.
.
right branching trees .
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
pre trained models w o bias codebertjsd avg .
.
.
.
.
.
.
graphcodebert hel .
.
.
.
.
.
.
pre trained models w biasku1d706 codeberthel avg .
.
.
.
.
.
.
graphcodebert hel avg .
.
.
.
.
.
.
the bias.
one possible hypothesis is that although the ast shows aright skewness trend as whole several subtrees e.g.
the subtree ofassignment are not right skewed.note that in the experiments we have tried different distancefunctions as shown in table to measure distance based on at tention and contextual representation in each transformer layer.due to the space limitation in table we only present the bestperformance when using different distance functions for each trans former layer.
we can f ind that the jsd and hel distance functionsthat are performed over attention distributions perform better thanthose over contextual word representations.
it shows that parsingtrees from attention information is more effective than extractingfrom the contextual representation of pre trained code models.in figure we also show a case study of code snippet withthe induced trees based on codebert with and without bias in jected.
we can see that several motif structures have been capturedby codebert e.g.
return images andself postprocessor.
itveri f ies the effectiveness of the syntax tree induction.summary.the syntax tree of code can be induced by the pre trained language models for code to some extent.
in addition extracting parse trees from attention information is more ef fective than extracting from the contextual representations ofpre trained code models.
discussion6.
observed findingsthrough comprehensive analysis from three perspectives of pre trained code models we observe several insightful f indings whichmay inspire future study.
from attention analysis we f ind that aword s attention distribution can align with the ast.
the attentionsalign better with syntax connection in deeper layers than lowerlayers in the self attention network.
moreover we f ind that thereexit position based heads which do not consider the context oftext.
it could suggest that if we remove these heads it will notaffect the f inal results and we can reduce the number of parametersof the pre trained models.
then we f ind the pre trained modelscan embed syntactic information in the hidden layers.
all pairsof words know their syntactic distance and this information is aglobal structural property of the vector space.
finally we use asimple tree construction algorithm to induce a syntax tree from pre trained models.
the results indicate that the pre trained model suchas codebert is capable of perceiving the syntactic informationto a certain extent when training on a large corpus.
our f indingssuggest that grammatical information can be learned by the pre trained model which could explain why a pre trained model suchas codebert can achieve promising results in a variety of sourcecode related downstream tasks such as code summarization codesearch and clone detection.2386what do they capture?
a structural analysis of pre trained language models for source codeicse may pi t tsburgh pa usa figure a case study of syntax tree induction based on codebert for a given python code snippet.
.
limitations and future workone limitation of our work is that the adopted structural analysisapproaches are based on the ast structure of code which couldbe just one aspect where the pre trained models can achieve goodresults for source code.
in our future work we will investigate howthe pre trained models learn other aspects of source code suchas code tokens control ow graphs cfgs and data ow graphs dfgs .
besides in this paper we only investigate two representa tive self supervised pre trained language models for source code i.e.
codebert and graphcodebert.
it will be interesting to extentthe analysis to other supervised learning models as well as otherdeep neural networks e.g.
lstm and cnn .with regard to the design of the structural analysis approachesadopted in this paper one limitation is that the structural probingon word embedding we currently use is relative simple.
it wouldbe interesting to develop a deep neural network to learn bettermapping functions.
meanwhile the tree construction algorithmwe use is a relatively simple top down recursive binary tree algo rithm.
theright skewnessbias we use was originally designed forthe constituency trees in natural languages e.g.
english whichcan be improved for asts.
lastly the ast structure is more com plex than the induced tree therefore there is still ampler room forimprovement in the grammar induction algorithm.
related workrecently there has been much effort in interpreting the bert mod els in the nlp community.
at a high level these interpretationapproaches are developed from two perspectives interpretingthe learned embedding and investigate whether bert can learnsyntax and semantic information of natural languages.
to interpretthe learned embedding ethayarajh studies whether the contex tual information are preserved in the word embedding learned frompre training models including bert elmo and gpt .
mickuset al.
systematically evaluate the pre trained bert using adistributed semantics models.
conneau et al.
and liu et al.
design several probing tasks to investigate whether the sentenceembedding can capture the linguistic properties.to investigate the syntax and semantic knowledge in bert ten ney et al.
develop a series of edge probing tasks to explorehow the syntactic and semantic structure can be extracted fromdifferent layers of pre trained bert.
htut et al.
propose toextract implicit dependency relations from the attention weightsof each layer head through two approaches taking the maximumattention weight and computing the maximum spanning tree.
he witt and manning propose a structural probing approach toinvestigate whether the syntax information are preserved in wordrepresentations.specially there also exists another line of work on visualizing at tentions to investigate which part of the feature space the model putmore focus.
kovaleva et al.
study self attention and conduct aqualitative and quantitative analysis of the information encoded byindividual bert s heads.
hoover et al.
introduce a tool calledexbert to help humans conduct exible interactive investigationsand formulate hypotheses during the model internal reasoning pro cess.
following this line of research this paper proposes to extendand adapt the interpretation techniques from the nlp communityto understand and explain what feature correlations can be capturedby a pre trained code model in the embedding space.
conclusionin this paper we have explored the interpretability of pre trainedlanguage models for source code e.g.
codebert graphcode bert .
we conduct a thorough structural analysis from the follow ing three aspects aiming to give an interpretation of pre trainedcode models.
first we analyze the self attention weights and alignthe weights with the syntax structure.
second we propose a struc tural probing approach to investigate whether the contextual rep resentations in transformer capture the syntax structure of code.third we investigate whether the pre trained code models havethe capability of inducing the syntax tree without training.
theanalysis in this paper has revealed several interesting f indings thatcan inspire future studies on code representation learning.artifacts.all the experimental data and source code used inthis work will be integrated into the open source toolkit n u.sc hyphen.sc cc which is available at work is supported by national natural science foundationof china under grand no.
.
this work is also partiallysponsored by tencent rhino bird focus research program of basicplatform technology.
we would like to thank all the anonymousreviewers for their constructive comments on improving this paper.2387icse may pi t tsburgh pa usayao wan wei zhao hongyu zhang yulei sui guandong xu and hai jinreferences uri alon shaked brody omer levy and eran yahav.
.
code2seq gen erating sequences from structured representations of code.
inproceedings ofinternational conference on learning representations.
uri alon meital zilberstein omer levy and eran yahav.
.
code2vec learn ing distributed representations of code.proceedings of the acm on programminglanguages3 popl .
nadezhda chirkova and sergey troshin.
.
empirical study of transformersfor source code.
inproceedings of 29th acm joint european software engineeringconference and symposium on the foundations of software engineering.
.
kevin clark urvashi khandelwal omer levy and christopher d. manning.
.what does bert look at?
an analysis of bert s attention.
inproceedings ofthe acl workshop blackboxnlp analyzing and interpreting neural networksfor nlp.
.
kevin clark minh thang luong quoc v. le and christopher d. manning.
.electra pre training text encoders as discriminators rather than generators.inproceedings of international conference on learning representations.
alexis conneau germ n kruszewski guillaume lample lo c barrault andmarco baroni.
.
what you can cram into a single vector probing sentenceembeddings for linguistic properties.
inproceedings of the 56th annual meetingof the association for computational linguistics.
.
gregory w. corder and dale i. foreman.
.nonparametric statistics a step by step approach.
john wiley sons.
jacob devlin ming wei chang kenton lee and kristina toutanova.
.
bert pre training of deep bidirectional transformers for language understanding.
inproceedings of the conference of the north american chapter of the associationfor computational linguistics human language technologies.
.
li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfenggao ming zhou and hsiao wuen hon.
.
uni f ied language model pre training for natural language understanding and generation.
inproceedings ofadvances in neural information processing systems.
.
kawin ethayarajh.
.
how contextual are contextualized word represen tations?
comparing the geometry of bert elmo and gpt embeddings.
inproceedings of the conference on empirical methods in natural language pro cessing and the 9th international joint conference on natural language processing.
.
zhangyin feng daya guo duyu tang nan duan xiaocheng feng ming gong linjun shou bing qin ting liu daxin jiang and ming zhou.
.
codebert a pre trained model for programming and natural languages.
inproceedings offindings of the association for computational linguistics emnlp .
.
xiaodong gu hongyu zhang and sunghun kim.
.
deep code search.
inproceedings of 40th international conference on software engineering.
.
daya guo shuo ren shuai lu zhangyin feng duyu tang shujie liu longzhou nan duan alexey svyatkovskiy shengyu fu michele tufano shao kundeng colin b. clement dawn drain neel sundaresan jian yin daxin jiang andming zhou.
.
graphcodebert pre training code representations with dataflow.
inproceedings of 9th international conference on learning representations.
john hewitt and christopher d. manning.
.
a structural probe for findingsyntax in word representations.
inproceedings of the conference of thenorth american chapter of the association for computational linguistics humanlanguage technologies.
.
benjamin hoover hendrik strobelt and sebastian gehrmann.
.
exbert avisual analysis tool to explore learned representations in transformer models.inproceedings of the 58th annual meeting of the association for computationallinguistics system demonstrations.
.
phu mon htut jason phang shikha bordia and samuel r. bowman.
.
doattention heads in bert track syntactic dependencies?corrabs .
.
hamel husain ho hsiang wu tiferet gazit miltiadis allamanis and marcbrockschmidt.
.
codesearchnet challenge evaluating the state of semanticcode search.corrabs .
.
aditya kanade petros maniatis gogul balakrishnan and kensen shi.
.learning and evaluating contextual embedding of source code.
inproceedingsof the 37th international conference on machine learning vol.
.
.
hong jin kang tegawend f. bissyand and david lo.
.
assessing thegeneralizability of code2vec token embeddings.
inproceedings of 34th ieee acminternational conference on automated software engineering.
ieee .
taeuk kim jihun choi daniel edmiston and sang goo lee.
.
are pre trained language models aware of phrases?
simple but strong baselines forgrammar induction.
inproceedings of 8th international conference on learningrepresentations.
olga kovaleva alexey romanov anna rogers and anna rumshisky.
.revealing the dark secrets of bert.
inproceedings of the conference onempirical methods in natural language processing and the 9th international jointconference on natural language processing.
.
lucien le cam and grace lo yang.
.asymptotics in statistics some basicconcepts.
springer science business media.
nelson f. liu matt gardner yonatan belinkov matthew e. peters and noah a.smith.
.
linguistic knowledge and transferability of contextual repre sentations.
inproceedings of the conference of the north american chapterof the association for computational linguistics human language technologies naacl hlt.
.
yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omerlevy mike lewis luke zettlemoyer and veselin stoyanov.
.
roberta arobustly optimized bert pretraining approach.corrabs .
.
christopher manning and hinrich schutze.
.foundations of statistical naturallanguage processing.
mit press.
timothee mickus denis paperno mathieu constant and kees van deemter.
.
what do you mean bert?
assessing bert as a distributional semanticsmodel.corrabs .
.
tom s mikolov kai chen greg corrado and jeffrey dean.
.
efficient esti mation of word representations in vector space.
in1st international conferenceon learning representations workshop track proceedings.
tom s mikolov ilya sutskever kai chen gregory s. corrado and jeffrey dean.
.
distributed representations of words and phrases and their composi tionality.
inproceedings of advances in neural information processing systems.
.
matthew e. peters mark neumann mohit iyyer matt gardner christopherclark kenton lee and luke zettlemoyer.
.
deep contextualized word rep resentations.
inproceedings of the conference of the north american chapterof the association for computational linguistics human language technologies.
.
veselin raychev martin vechev and eran yahav.
.
code completion withstatistical language models.
inproceedings of the 35th acm sigplan conferenceon programming language design and implementation.
.
anna rogers olga kovaleva and anna rumshisky.
.
a primer in bertol ogy what we know about how bert works.transactions of the association forcomputational linguistics8 .
rico sennrich barry haddow and alexandra birch.
.
neural machinetranslation of rare words with subword units.
inproceedings of the 54th annualmeeting of the association for computational linguistics.
yikang shen zhouhan lin athul paul jacob alessandro sordoni aaron c.courville and yoshua bengio.
.
straight to the tree constituency parsingwith neural syntactic distance.
inproceedings of the 56th annual meeting of theassociation for computational linguistics.
.
kaitao song xu tan tao qin jianfeng lu and tie yan liu.
.
mass maskedsequence to sequence pre training for language generation.
inproceedings ofthe 36th international conference on machine learning vol.
.
pmlr .
yulei sui xiao cheng guanqin zhang and haoyu wang.
.
flow2vec value ow based precise code embedding.proceedings of the acm on programminglanguages4 oopsla .
yu sun shuohuan wang yu kun li shikun feng xuyi chen han zhang xintian danxiang zhu hao tian and hua wu.
.
ernie enhanced represen tation through knowledge integration.corrabs .
.
ilya sutskever oriol vinyals and quoc v. le.
.
sequence to sequence learningwith neural networks.
inproceedings of advances in neural information processingsystems.
.
ian tenney dipanjan das and ellie pavlick.
.
bert rediscovers the clas sical nlp pipeline.
inproceedings of the 57th conference of the association forcomputational linguistics.
.
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n. gomez .ukasz kaiser and illia polosukhin.
.
attention is allyou need.
inproceedings of advances in neural information processing systems.
.
jesse vig and yonatan belinkov.
.
analyzing the structure of attentionin a transformer language model.
inproceedings of the acl workshopblackboxnlp analyzing and interpreting neural networks for nlp.
.
jesse vig ali madani lav r. varshney caiming xiong richard socher andnazneen fatema rajani.
.
bertology meets biology interpreting attentionin protein language models.
inproceedings of 9th international conference onlearning representations.
yao wan yang he zhangqian bi jianguo zhang yulei sui hongyu zhang kazuma hashimoto hai jin guandong xu caiming xiong and philip s. yu.
.
naturalcc an open source toolkit for code intelligence.
inproceedings of44th international conference on software engineering companion volume.
acm.
yao wan jingdong shu yulei sui guandong xu zhou zhao jian wu andphilip s. yu.
.
multi modal attention network learning for semantic sourcecode retrieval.
inproceedings of 34th ieee acm international conference onautomated software engineering.
ieee .
yao wan zhou zhao min yang guandong xu haochao ying jian wu andphilip s. yu.
.
improving automatic source code summarization via deep rein forcement learning.
inproceedings of the 33rd acm ieee international conferenceon automated software engineering.
acm .
jian zhang xu wang hongyu zhang hailong sun and xudong liu.
.retrieval based neural source code summarization.
inproceedings of 42nd inter national conference on software engineering.
acm .