vet identifying and avoiding ui exploration tarpits wenyu wang university of illinois at urbana champaign usa wenyu2 illinois.eduwei yang university of texas at dallas usa wei.yang utdallas.edu tianyin xu university of illinois at urbana champaign usa tyxu illinois.edutao xie peking university china taoxie pku.edu.cn abstract despite over a decade of research it is still challenging for mobile ui testing tools to achieve satisfactory effectiveness especially on industrial apps with rich features and large code bases.
our experiences suggest that existing mobile ui testing tools are prone to exploration tarpits where the tools get stuck with a small fraction of app functionalities for an extensive amount of time.
for example a tool logs out an app at early stages without being able to log back in and since then the tool gets stuck with exploring the app s pre login functionalities i.e.
exploration tarpits instead of its main functionalities.
while tool vendors users can manually hardcode rules for the tools to avoid specific exploration tarpits these rules can hardly generalize being fragile in face of diverted testing environments fast app iterations and the demand of batch testing product lines.
to identify and resolve exploration tarpits we propose vet a general approach including a supporting system for the given specific android ui testing tool on the given specific app under test aut .
vet runs the tool on the aut for some time and records ui traces based on which vet identifies exploration tarpits by recognizing their patterns in the ui traces.
vet then pinpoints the actions e.g.
clicking logout or the screens that lead to or exhibit exploration tarpits.
in subsequent test runs vet guides the testing tool to prevent or recover from exploration tarpits.
from our evaluation with state of the art android ui testing tools on popular industrial apps vet identifies exploration tarpits that cost up to .
testing time budget.
these exploration tarpits reveal not only limitations in ui exploration strategies but also defects in tool implementations.
vet automatically addresses the identified exploration tarpits enabling each evaluated tool to achieve higher code coverage and improve crash triggering capabilities.
ccs concepts software and its engineering software testing and debugging .
tao xie is with the key laboratory of high confidence software technologies peking university ministry of education china and is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
ui testing trace analysis mobile testing mobile app android acm reference format wenyu wang wei yang tianyin xu and tao xie.
.
vet identifying and avoiding ui exploration tarpits.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction with the prosperity of mobile apps especially their roles in people s daily life during pandemic e.g.
food ordering grocery delivery and social networking quality assurance of mobile apps becomes crucially important.
user interfaces uis as the primary interface of user app interactions are natural entry points for app testing.
while manual ui testing is still often used in practice automated ui testing is becoming popular .
automated ui testing mimics how human users interact with apps through the uis and detects reliability and usability issues.
automated ui testing complements manual testing with greater timing flexibility and better code coverage requiring little human intervention.
however existing mobile ui testing tools are found to be ineffective in exploring app functionalities despite their sophisticated strategies for ui exploration.
while recent proposals have reported promising results measurement studies on comprehensive app benchmarks have drawn different conclusions.
for example a recent study shows that state of the art mobile ui testing tools yield low code coverage about in method coverage after hours of testing on popular industrial apps.
note that industrial apps typically have richer functionalities and larger code bases compared with open source apps.
the findings suggest a significant effectiveness gap that needs to be filled for automated mobile ui testing.
according to our experience the ineffectiveness of existing mobile ui testing tools often stems from their proneness to exploration tarpits1 where tools get stuck with a small fraction of app functionalities for an extensive amount of time.
we show a real world example in where a state of the art android ui testing tool named ape decides to log itself out one minute after testing an app starts without being able to log back in and since then gets stuck with exploring the app s pre login functionalities i.e.
exploration tarpits instead of its main functionalities.
it is possible that tool vendors users manually hardcode rules for the tools to avoid specific exploration tarpits such as instructing ape to avoid 1the name of exploration tarpits is inspired by the mythical man month book .
esec fse august athens greece wenyu wang wei yang tianyin xu and tao xie tapping the logout button or writing a script to support automatic login.
however these rules can hardly generalize being fragile in face of diverted testing environments e.g.
unreliable network to process login requests fast app iterations and the demand of batch testing product lines.
our findings in .
show various cases as such where exploration tarpits can be caused by unexpected flaws in a tool s exploration strategies and or implementation defects.
to automatically identify and resolve exploration tarpits in this paper we propose a general approach and its supporting system named vet for the given specific android ui testing tool on the given specific app under test aut .
vet works in three stages.
vet runs the tool on the aut for some time and records the interactions between the tool and aut in the form of ui traces .
a ui trace consists of app uis interleaving with the actions taken by the tool.
vet then analyzes the collected traces to identify trace subsequences termed regions that manifest exploration tarpits.
vet guides the tool in subsequent test runs to prevent or recover from an exploration tarpit by monitoring the testing progress and taking actions based on findings from the identified regions.
vet includes two specialized algorithms targeting two corresponding patterns of exploration tarpits exploration space partition andexcessive local exploration see and .
exploration space partition corresponding to figure 1a indicates that the fraction of app functionalities explored by the tool is disconnected from most of the app functionalities after some specific action e.g.
tapping ok in screen c .
such situations can be prevented by disabling the aforementioned action.
excessive local exploration indicates that the tool enters a hard to escape fraction of the app uis and needs a significant amount of time to reach other functionalities as demonstrated in figure 1b.
this issue can be addressed by either preventing the tool from entering e.g.
disabling start in screen e in figure 1b or assisting the tool to escape e.g.
restart the app upon observation of screen f .
to design the two algorithms we first construct fitness value formulas that quantify how well a region on the given trace matches a targeted pattern.
we then apply fitness value optimization on the entire trace to determine the region that best fits our targeted patterns.
we evaluate vetusing three state of the art practice android ui testing tools monkey ape and wctester with widely used industrial apps.
we collect traces by running each tool on each app three times for one hour each original runs .
vetreports at least one exploration tarpit region in each tool app pair with regions in total each spanning about minutes on average.
the longest regions span over minutes about .
of the one hour testing time budget.
after inspecting the reported regions we confirm the root causes of regions including both limitations of ui exploration strategies e.g.
early logouts and defects in tool implementation e.g.
hanging as shown in .
.
we then perform six other one hour runs for each tool app pair three guided runs using vet to automatically avoid all the exploration tarpit regions identified in the original runs during testing on three runs and three comparison runs not using vet.
based on the preceding evaluation setup we compare the code coverage of the given app achieved by applying each tool with and without the assistance of vet given the same time budget.
specifically we compare the combined code coverage and the numbers ofdistinct crashes for original runs and guided runs and original runs and comparison runs.
the evaluation results show that on average a tool assisted by vet achieves up to a .
relative code coverage increment and triggers up to .1x distinct crashes than the tool without the assistance of vet.
in summary this paper makes the following main contributions a new perspective of improving the given automated ui testing tool by automatically identifying and addressing exploration tarpits for the given target aut algorithms for effective identification of two manifestation patterns of exploration tarpits a practical system that can be automatically applied to enhance any android ui testing tool such as monkey ape and wctester on any aut comprehensive evaluation of vet demonstrating that vet reveals various issues related to tools or app usability and that vet automatically resolves those issues helping the tools achieve up to a .
relative code coverage increment and .1x distinct crashes on popular industrial apps.
motivating examples we present two concrete examples from our experiments covering exploration space partition and excessive local exploration see .
these examples provide contexts for further discussion and help illustrate the motivations that drive the design of vet.
.
exploration space partition we run ape a state of the art android ui testing tool to test a popular app microsoft onenote .
the result is illustrated in figure 1a.
we manually set up the account to log in to the app s main functionalities and then start ape.
we run ape without interruptions for one hour and check the test results afterward.
in the one hour testing period ape explores only out of activities.
to understand the low testing effectiveness we investigate the ui trace captured during testing and find the root cause to be exploration tarpits ape performs exploration around onenote s main functionalities for about two minutes covering out of of all the activities covered in the entire one hour test run.
we omit this phase in figure 1a.
about two minutes after testing starts ape arrives at the settings screen screen a and decides to click account the red boxed ui element for further exploration.
ape arrives at the account screen screen b and clicks the sign out button.
the click pops up a window screen c asking for confirmation of getting logged out.
ape clicks cancel first and then goes back to the account screen.
however ape clicks the sign out button again knowing that there is one action not triggered yet in the confirmation dialog.
subsequently ape clicks the ok button screen c and logs itself out.
the logout leads to the entry screen screen d .
from this point ape has access to only a small number of functionalities e.g.
logging in .
ape cannot log in due to the difficulty of autogenerating the username password of the test account.
in the remaining minutes ape explores two new activities in total.
84vet identifying and avoiding ui exploration tarpits esec fse august athens greece a b c d2 mins58 mins efg22 mins a exploration space partition b excessive local exploration note colored bars on the top represent the progress of two hour tests where green bars refer to normal exploration and red bars refer to exploration tarpits.
dashed straight arrows indicate visiting the screen from some other screen and solid arrows show transitions between two screens after clicking the red boxed ui elements.
the dashed curve arrow on screen d depicts that ape cycles around d until the end of testing.
the dashed curve arrow on screen f shows that monkey stays on f within the minute exploration tarpit window.
figure motivating examples of exploration tarpits described in .
this example represents exploration space partition described in .
the essential problem is that ape does not understand ui semantics it does not know that the majority of onenote s functionalities will be unreachable by clicking the ok button at the time of action.
.
excessive local exploration figure 1b presents another example in which we run monkey a widely adopted tool to test another popular industry quality app nike run club .
in this example monkey spends about minutes trying to saturate one of the app s functionalities.
after investigating into the collected ui trace we find the following behavior when monkey interacts with the app monkey explores other functionalities normally before entering screen e that allows the tool to enter the functionality where the tool later gets trapped.
we name the functionality the trapping functionality .
monkey clicks the start button and enters the trapping functionality screen f .
monkey keeps clicking around in the trapping functionality.
to escape from the trapping functionality monkey first needs to press the back button and a confirmation dialog screen g will pop up.
monkey then has to click the ok button to finish escaping.
however due to being widget oblivion monkey clicks only randomly on the screen resulting in constant failures to click ok when the confirmation dialog is shown.
furthermore the dialog disappears when monkey clicks outside of its boundary and monkey needs to press the back button again to make the confirmation show up one more time.
it takes minutes for monkey to find and execute an effective escaping ui event sequence and finally leave the trapping functionality.
the aforementioned behaviors are repeatedly observed in the trace with different amounts of time used for escaping .
this example represents excessive local exploration behavior described in .
the essential problem is that monkey is both widgetand state oblivion i.e.
the tool is unable to locate actionable ui elements efficiently or sense whether it has been trapped and react accordingly e.g.
by restarting the target app .
.
implications to prevent such undesirable exploration behaviors a conceptually simple idea is to de prioritize exploring the entries to aforementioned trapping states i.e.
the ok button in screen c and start button in screen f .
one potential solution is to develop natural language processing nlp or image processing based approaches that can infer the semantics of ui elements .
while solutions based on understanding ui semantics are revolutionary they are challenging due to fundamental difficulties rooting in nlp and image processing.
in this paper we explore a more practical and evolutionary solution based on understanding exploration tarpits by mining ui traces.
we show that it is feasible to identify the existence and location of such behavior through pattern analysis on interaction history.
given the location of exploration tarpits we can further identify which ui actions might have led to such behavior.
taking the example of figure ape starts to visit a very different set of screens e.g.
the welcome screens in screen d in figure 1a after clicking ok and the number of explored screens dramatically decreases.
therefore we can look at the screen history and find the time point where the symptom starts to appear.
the ui action located at the aforementioned time point is then likely the cause of the symptom.
our vet system uses a specialized algorithm .
to effectively locate the starting time point of exploration tarpits similar to the aforementioned instance.
background this section presents background knowledge about ui hierarchy to help readers understand our algorithm design and implementations in the scope of android ui testing.
aui hierarchy structurally represents the contents of app ui shown at a time.
each ui hierarchy consists of ui properties e.g.
location size for individual ui elements e.g.
buttons textboxes and hierarchical relations among ui elements.
on android each activity internally maintains the data structure for its current ui hierarchy.
typically ui elements are represented by view subclass instances and hierarchical relations are represented by child view s ofviewgroup subclass instances.
85esec fse august athens greece wenyu wang wei yang tianyin xu and tao xie a key component of ui testing is to identify the current app functionality.
the functionality is identified by equivalence check for ui hierarchies because ui hierarchies are usually used as indicators of apps functionality scenarios.
thus checking the equivalence between the current and past ui hierarchies allows tools to identify whether a new functionality is being exercised.
if the current functionality has been covered the tool can additionally leverage the knowledge associated with the functionality to decide on the next actions.
there are different ways to check ui hierarchy equivalence strict comparison.
a simple way to check the equivalence of two ui hierarchies is to compare their ui element trees and see whether they have identical structure and ui properties at each node.
in practice such simple equivalence checking is too strict.
for example on an app accepting text inputs a tool checking exact equivalence can count a new functionality every time one character is typed.
checking similarity.
a workaround to the aforementioned issue of strict comparison is to check similarities of two ui hierarchies against a threshold.
however ambiguity can become the new issue given that the similarity relation is not transitive suppose that a is similar to both b and c it is still possible that b is not similar to c. then if both b and c are in the history regarded as different functionalities and a comes as a new ui hierarchy the tool is unable to decide on which functionality to use the associated knowledge from.
to fix the ambiguity issue we can perform screen clustering essentially putting mutually similar screens into individual groups and regarding each group as representing one single functionality.
then the downside is that screen clustering can be a computationally expensive operation especially for traces with many screens.
comparing abstractions.
a more advanced solution is to check the equivalence at an abstraction level employed by many modelbased ui testing tools .
in the previous example one can leave out all user controlled textual ui properties from the hierarchy and the equivalence check can tell that the tool is staying on the same screen regardless of what has been entered.
while abstracting ui hierarchies is conceptually effective it is challenging to design effective ui abstraction functions.
the difficulty lies in identifying ui properties or structural information to distinguish different app functionalities especially when screens have variants with relatively subtle differences.
ape includes adaptive abstractions to address the challenge of automatically finding proper ui abstraction functions in different scenarios.
ape dynamically adjusts its abstraction strategy e.g.
which ui property values should be preserved during testing based on feedback from strategy execution e.g.
whether invoking actions on ui hierarchies with the same abstraction yields the same results .
unfortunately the adaptive abstraction idea assumes the availability of sufficiently diversified execution history for feedback and such history is not always available when analyzing given traces as in our situation.
given the pros and cons of the aforementioned ways we empirically adopt a hybrid approach for ui hierarchy equivalence check.
first we always abstract ui hierarchies we consider only visible ui elements i.e.
view.getvisibility visible and the element s bounding box intersects with its parent s screen region stage i trace collection app under test ui testingtool traces vetdetectionalgorithms toller ui action recording explorationtarpitregionsstage ii analysisstage iii enhanced exploration avoidableactions screensandroid framework ui monitoring manipulationandroid frameworktollerfigure overview of vet.
we keep the activity id and the original hierarchical relations among ui elements and we retain only ui element types and ids from ui properties.
second we check the similarities of abstract ui hierarchies and cluster them into groups only when the analysis is sensitive to the absolute number of distinct screens.
more details on achieving clustering efficiently are elaborated in .
.
the vet approach .
overview we propose vet a general approach and its supporting system that automatically identifies and addresses exploration tarpits for any given android ui testing tool on anygiven aut.
our implementation of vet is publicly available at .
as illustrated in figure for a given tool and aut vet works in three stages.
first vet runs the target tool on the aut for a certain amount of time and records the interactions between the tool and aut.
with help from our android framework extension toller vet collects trace s that consist of aut uis interleaving with the tool s actions.
then vet analyzes each individual trace with specialized algorithms to identify trace subsequences termed regions that manifest the tool s exploration tarpits.
optionally one can rank the identified regions based on their time lengths where longer regions receive higher ranks to prioritize regions that are likely to exhibit exploration tarpits with higher impacts see .
.
finally vet learns from the identified regions and guides the tool in subsequent runs to avoid exploration tarpits by monitoring the testing progress and taking actions based on findings from the identified regions.
with the support from toller vet is currently capable of preventing specified actions by disabling the corresponding ui elements at runtime and assisting the aut to escape from the specified screens by restarting the aut.
the identified regions additionally support manual investigations of testing efficacy by providing localization help.
we equip vetwith two specialized algorithms targeting two patterns of exploration tarpits exploration space partition andexcessive 86vet identifying and avoiding ui exploration tarpits esec fse august athens greece a b note that each subgraph corresponds to an example trace where each circle represents a distinct screen in the trace e.g.
each subfigure in figure each arrow indicates that action s is observed between two screens in the trace and each curved rectangle depicts a ui subspace.
red arrows denote destructive actions e.g.
clicking ok in screen c of figure 1a start in screen e of figure 1b while dashed arrows show where traces begin.
figure two patterns of exploration tarpits.
local exploration .
characteristics of the two algorithms targeted patterns are illustrated in figure and discussed as follows exploration space partition .
as shown in figure 3a the ui testing tool traverses through a ui subspace subspace for a long time after the execution of some action the red arrow and the tool is unable to return to the previously visited ui subspace subspace .
furthermore the tool visits much fewer distinct screens after the action.
the presence of the symptom suggests that the tool has triggered a destructive action effectively the partition boundary of the entire trace and beginning of the exploration tarpit that prevents the tool from further exploring the app s major functionalities.
the first motivating example from corresponds to this symptom where clicking the ok button is the destructive action that gets ape trapped in multiple screens related to logging in subspace and prevents ape from further accessing onenote s main functionalities subspace .
excessive local exploration .
as shown in figure 3b the ui testing tool is trapped in a small ui subspace subspace for an extended amount of time after the execution of the corresponding destructive action the red arrow .
however the tool is capableof returning to the previously visited ui subspace subspace despite the difficulties.
it is also likely that the tool will get trapped again within subspace after returning to subspace .
consequently the tool spends an excessive amount of time repetitively testing limited functionalities in this hard to escape subspace.
the second motivating example from corresponds to this symptom where clicking the start button gets monkey trapped in screens f and g subspace .
clicking ok helps monkey go back to screen e within subspace and other functionalities but it does not take a long time before the tool gets trapped again within subspace .
as can be seen exploration space partition targets higher level irreversible transition of ui exploration space while excessive local exploration focuses on lower level difficulties of exercising a specific functionality.
note that it is possible for the regions reported by the two algorithms on the same trace to overlap.
for example excessive local exploration might also capture exploration tarpits within exploration space partition s trapped ui subspace corresponding to subspace in figure .
such overlaps do not prevent ustable notations and descriptions used in the algorithms notation description si screen iin the trace represented by the ui hierarchy.
ti the timestamp of screen sibeing observed.
sl r a region of screens starting at screen land ending at screen r with both ends included .
sl r the set of distinct screens from sl rby de duplicating their ui hierarchies.
ss l r the number of occurrences of sinsl r. tmin a predefined threshold that decides the minimum time length of any sl r i.e.
tr tl tmin that may be included in algorithm outputs.
from finding meaningful targeted exploration tarpits different exploration tarpits revealed by regions identified by both algorithms suggest the existence of different exploration difficulties.
in the remaining of this section we describe the two algorithms for capturing exploration space partition and excessive local exploration in .
and .
respectively.
we show that pattern capturing can be expressed as optimization problems.
table describes the notations used to describe vet s algorithms.
.
capturing exploration space partition according to our introductions of exploration space partition we need to find a destructive action exerted on screen snas the partition boundary such that the aforementioned characteristics from .
can be best reflected.
for instance considering our first motivating example in we hope to pick up the screen shown in figure 1c as sn.
we optimize the following formula to find the most desirable snfrom a trace with nscreens arg min n ep s s1 n ss n n n n sn n sep n in the formula epis a pre calculated limit indicating the upper bound ofnduring optimization and denotes the sigmoid function.
note thatn ncan be pulled out of the sum subformula.
the intuition of the formula design is as follows as part of the characteristics the tool should ideally be able to visit few to no screens that have appeared no later than sn after the tool passes sn.
correspondingly in our motivating example screens shown before figure 1c depicting the app s main functionalities are dramatically different from the screens afterward logging in tos etc.
.
in the formula the nominator of the first term intended to be minimized quantifies the proportion of screens seen before snwithinsn n. as the denominator of the first term n nessentially calculates how many non distinct screens the tool visits after sn.
there are two purposes of this design.
first we hope to normalize the first term in the formula so that two terms can weigh the same .
given that s s1 n ss n n s s1 n sn n ss n n s sn n ss n n n n the first term is guaranteed to fall within .
second we want to push snbackward note that smaller nmakes the first term smaller because we assume 87esec fse august athens greece wenyu wang wei yang tianyin xu and tao xie that the design makes sncloser to the exploration tarpit s root cause which should appear earlier than other causes.
as another part of the characteristics the tool stays within a certain ui subspace for a long time thus the tool will go through screens within the subspace very often.
if the tool generally uniformly visits most or all distinct screens within the subspace by observing a small period of exploration corresponding to sep nin the formula we should have a fairly precise estimation i.e.
sep n of the subspace boundary which is characterized by sn n .
the second term in the formula corresponds to this intuition where the closer sep n is to sn n note that sep n sn n the more favorable it becomes during optimization.
by setting an upper bound eponnand regularizing the ratio with a sigmoid function and applying appropriate linear transformations we can guarantee that the second term in the formula always ranges from to being the same as the first term.
in the end two terms in the formula contribute equally to optimization choices.
to determine epon each trace because our optimization scope does not include any interval shorter than we choose a value such that tn tepis closest to tmin.
after obtaining a potentially suitable snthrough optimizing the aforementioned formula we additionally check whether s1 n sn n is satisfied essentially enforcing the property that the exploration space should be smaller after the partition.
finally the reported region is sn n. .
capturing excessive local exploration based on the characteristics of excessive local exploration from .
we should track the presence of a region showing that the tool is trapped within a small ui subspace for an extended amount of time.
for our second motivating example in one valid choice is the minute region starting from the button click in screen e of figure 1b.
we accordingly optimize the following formula to find the boundaries slandsrof the most suitable region on a trace arg min l r n merge sl r r l in the formula merge denotes the operation of merging similar screens and returning the groups of merged screens.
as the optimization formula suggests we hope to find a suitable region such that it covers few distinct screen groups despite that the tool tries to explore diligently by injecting numerous actions quantified by r l .
then iftr tl tmin we regard that the exploration tarpit regionsl rcan be reported.
accordingly in our motivating example the choice of slis screen f of figure 1b and sris the last instance of screen g of figure 1b in the minute region.
sl 1corresponds to screen e of figure 1b and the destructive action is reported.
note that there can be more than one region exhibiting excessive local exploration behavior within a single trace given the possibility for the tool to escape the ui subspaces where excessive local exploration behavior is observed.
in order to find all potential regions we iterate the aforementioned optimization process on remaining region s each time after one region is chosen until no more region can be divided.input a set of abstract ui hierarchies h output a mappingr h r wherer h sorth hby h in ascending order r foreachh hdo r nil end foreachh hdo ifr nilthen r h foreachh hdo ifr nil simcheck h h then r h end end end end returnr algorithm merge merging similar screens into groups each group is represented by its root screen in r design of merge .as mentioned in involving screen merging is especially useful for handling excessive local exploration given that the aforementioned optimization formula is very sensitive to the absolute numbers of distinct screens.
being part of the challenge an efficient and mostly effective screen merging algorithm requires careful design.
given a set of distinct abstract screens represented by ui hierarchies to merge a relatively straightforward and precise approach is to first calculate the tree editing distance for each pair of abstract ui hierarchies for similarity check and then use combinatorial optimization to decide the optimal grouping strategy e.g.
by converting to an integer linear programming problem such that all screens within the same group are mutually similar and the total number of groups is minimal.
unfortunately such an algorithm requires exponential time in regards to the number of distinct abstract screens to merge.
the design will likely fall short on traces collected using industrial quality apps from which we can easily capture hundreds to thousands of distinct abstract screens.
aiming to make the algorithm practically efficient we relax the definition of similarity and the goal of optimization from the aforementioned merging algorithm based on insights from our observations.
specifically we find that in many cases similar screens can be seen as screen variants derived from base screens by inserting a small number of leaf nodes or subtrees into the abstract ui hierarchy.
based on this assumption with some tolerance for inaccuracy we can design a more efficient tree similarity checker algorithm which considers only node insertion distances and has h1 h2 time complexity compared with o h1 h2 height h1 height h2 for full tree edit distance and replace the inefficient combinatorial optimization with a highly efficient greedy algorithm algorithm which tries to find all the base screens with o h maxh h h time complexity.
in practice with multiple other optimizations not affecting the level of time complexity the algorithm needs only several seconds on average to process a trace.
even for a very long trace with distinct abstract screens and tens of thousands of concrete screens the algorithm runs for only several minutes.
88vet identifying and avoiding ui exploration tarpits esec fse august athens greece input abstract ui hierarchies h1 h2 output whetherh1 h2are similar enough const max allowed distance dmax empirically set to seq foreachnode depthfirsttraverse h1 do seq seq props node depth node props obtains the node s ui properties that are preserved during abstraction.
see more details in .
end seq foreachnode depthfirsttraverse h2 do seq seq props node depth node end lcs longestcommonseqence seq seq if lcs min h1 h2 then return false else return max h1 h2 lcs dmax end algorithm simcheck ui hierarchy similarity checker evaluation our evaluation answers the following research questions rq1 how effectively can vet help reveal android ui testing tool issues with the identified exploration tarpit regions?
rq2 what is the extent of effectiveness improvement of android ui testing tools through automatic enhancement by vet?
rq3 how likely do vet algorithms miss tool issues in their identified exploration tarpit regions?
.
evaluation setup android ui testing tools and android apps.
we use three stateof the art practice android ui testing tools monkey ape and wctester .
we use popular industry android apps from the google play store as shown in table .
these apps are from a previous study which picks the most popular apps from each of the categories on google play and compares multiple testing tools applied on these apps.
the apps that we choose need to work properly on our testing infrastructure they need to provide x86 x64 variants of native libraries if they have any they do not constantly crash on our emulators and toller is able to obtain ui hierarchies on most of the functionalities.
we additionally skip apps that have relatively limited sets of functionalities or require logging in for access to most features and we are unable to obtain a consistently usable test account e.g.
some apps have disabled our test accounts after some experiments .
trace collection.
we run each tool on every app for three times to alleviate the potential impacts of non determinism in testing.
each run takes one hour without interruption and we restart the tool if it exits before using up the allocated run time.
toller records one ui trace for each test run.
while vet runs separately on each ui trace results are grouped for each tool app pair.
in total we collect one hour ui traces from tool app pairs.
testing platform.
all experiments are conducted on the official android x64 emulators running android .
on a server with xeon e5 v4 processors.
each emulator is allocated with dedicated cpu cores gib of ram and gib of internal storage space.
emulators are stored on a ram disk and backed by discrete graphicstable overview of industrial apps used for evaluation app name version category inst login accuweather .
.
free weather 50m autoscout24 .
.
auto vehicles 10m duolingo .
.
education 100m flipboard .
.
news magazines 500m merriam webster .
.
books reference 10m nike run club .
.
health fitness 10m onenote .
.
business 100m quizlet .
.
education 10m spotify .
.
music audio 100m tripadvisor .
.
food drink 100m trivago .
.
travel local 10m wattpad .
.
books reference 100m webtoon .
.
comics 10m wish .
.
shopping 100m youtube .
.
video player editor 1b zedge .
.
personalization 100m notes inst denotes the approximate number of downloads.
login indicates whether the app requires logging in to access most features.
cards for minimal mutual influences caused by disk i o bottlenecks and cpu intensive graphical rendering.
we manually write autologin scripts for apps with login ticked in table and each of these scripts is executed only once before the corresponding app starts to be tested in each run.
to alleviate the flakiness of these auto login scripts we manually check the collected traces afterward and rerun the experiments with failed login attempts.
overall statistics.
vet reports regions to exhibit exploration tarpits averaging .
on each tool app pair.
based on vet s reports the average amount of time involved in exploration tarpits is about minutes per region with the maximum being minutes and minimum being slightly more than minutes given that we empirically set tmin 10minutes for all the experiments .
.
rq1.
detected tool issues .
.
methodology.
we evaluate the effectiveness of vet algorithms in capturing exploration tarpits that reveal issues of testing tools upon auts.
specifically we first group exploration tarpit regions by the tool app pairs that these regions are observed on.
then we rank the regions within each tool app pair by their time lengths as mentioned in .
.
finally we manually investigate each of regions from all tool app pairs.
we report any issue for each of these regions with manual judgment.
note that we count only the issue that we consider most specific to the exploration tarpit revealed by each region if both issues a and b contribute to the exploration tarpit on some region and a also contributes to other regions on the same trace we count only b in the statistics.
.
.
results.
we are able to determine tool issues on of manually investigated regions.
table shows the distribution of issue types w.r.t the tool and region ranking.
note that we find each tool app pair to have up to three regions reported by vet thus rank regions cover all regions with regions each .
the tool issues can be traced to two root causes apps under test require extra knowledge for effective testing and tool defects prevent themselves from progressing.
we discuss specific issues w.r.t.
these two root causes 89esec fse august athens greece wenyu wang wei yang tianyin xu and tao xie table distribution of confirmed issue types issuerank rank rank 3totalape mk wt sum ape mk wt sum ape mk wt sum lout ui ntwk loop esc abs misc total notes mk and wt refer to monkey and wctester respectively.
issue type details are discussed in .
.
app logout or equivalent abbreviated as lout in table accounts for exploration tarpits on out of of investigated regions with identified issues.
some apps essentially require login states for the majority of their functionalities to be accessible.
however the tools used in our experiments have no knowledge about the consequences of clicking the logout buttons in different apps before the tools actually try clicking these buttons.
unfortunately after the tools try out such actions driven by their exploration strategies the apps login states both on device and on server have been destroyed.
the tools then have to spend all remaining time on a limited number of functionalities leading to exploration tarpits.
this case reveals a common weakness of existing ui testing tools they have a limited understanding of action semantics.
as can be seen from table ape is more likely to be affected by this type of issues.
unresponsive uis ui are found in out of of regions.
we find that some apps stop responding to ui actions after the advertisement banner is clicked even though the apps ui threads are not blocked.
the issue is likely caused by the ui design defects in the google admob sdk.
the auts should be restarted as soon as possible to resume access to their functionalities.
according to table monkey is most vulnerable to such issues.
one interesting finding is that ape is actually also vulnerable to unresponsive uis although the tool s implementation is capable of identifying such situations.
however while ape proceeds to restart the app most of the times when ape finds the app unresponsive ape fails to do so occasionally.
network disconnections ntwk are found in out of of regions.
we find that turning networking off is undesirable for some apps especially when the disconnection lasts for a long time.
consequently these apps may show only messages prompting users to check their networks leaving nothing for exploration.
tools such as monkey can control network connections through android system ui e.g.
by clicking the airplane mode icon .
while the capability helps test app logic in edge conditions in general it might hurt the tool s effectiveness on apps heavily relying on network access.
all regions with the aforementioned issue come from monkey s traces.
restart action loops loop are found in out of of regions.
the tool essentially keeps restarting or performing the same actions on the target app after some point.
one potential cause for such issues is that the tool thinks that allui elements in the target app s main screen have been explored.
the toolmight need to revise its exploration strategy for discovering more explorable functionalities.
obscure escapes esc are found in out of of regions.
defects in a tool s design or implementation can make it difficult for the tool to escape from certain app functionalities and consequently the tool loses opportunities to explore other functionalities.
for instance monkey finds it challenging to escape a screen where the only exit is a tiny button on the screen see the second motivating example in due to the tool s lack of understanding of ui hierarchies.
ui abstraction defects abs are found in out of of regions.
defects in a tool s ui abstraction strategies can trick the tool into incorrectly understanding the testing progress.
seen from collected traces wctester considers all texts as part of abstract ui hierarchies.
while the strategy works well in a wide range of testing scenarios it keeps the tool repetitively triggering actions on ui elements with changing texts such as counting down given that the tool incorrectly thinks that new functionalities are being covered.
miscellaneous tool implementation defects misc are in out of of regions.
in our case we find potential implementation defects in three tools ape and monkey fail to handle unresponsive apps injection failed or being unable to obtain ui hierarchies and wctester is found to explore only a certain fraction of app functionalities after some point.
another finding is that the ratio of confirmed issues decreases when the rank goes lower for rank for rank and for rank suggesting the usefulness of prioritizing regions based on their lengths.
.
rq2.
improvement of testing performance this section shows that the identified exploration tarpit regions by vet can be used to automatically address tool issues.
.
.
automatic fix application.
the essential idea is to prevent some tool issues from happening again or getting rid of tool issues quickly by controlling the interactions between tools and apps.
specifically given an exploration tarpit region we identify the ui element that the tool acts on right before the region begins and then we use toller to disable the ui element for vet guided runs.
many tool issues can be targeted by this simple approach.
for example if we disable the advertisement banners that lead to unresponsive uis in .
tools will simply not run into the undesirable situation and they can focus on testing other more valuable app functionalities.
in some cases when there are multiple entries to the region and existing traces do not reveal all the entries the aforementioned approach might fail.
we mitigate this limitation by monitoring and controlling the testing progress currently if we observe any of the most frequently appearing screens from excessive local exploration regions we restart the aut in vet guided runs.
we implement ui element disablement by relying on toller to monitor screen changes during testing and dynamically modify ui element properties.
when a target screen i.e.
a ui screen containing any target ui element as determined by ui hierarchy equivalence check shows up we pinpoint the target ui element by matching with the path to each ui element from the root ui element.
once we confirm that the target ui element exists on the current 90vet identifying and avoiding ui exploration tarpits esec fse august athens greece screen we instruct toller to disable the ui element which will not respond to any further action on itself.
for the edge case where the action is not on any ui element e.g.
pressing the back button we restart the target app once we see the corresponding target screen.
note that there is no need to modify the app installation packages given that we manipulate app uis dynamically.
.
.
methodology of experiments.
we aim to measure the testing effectiveness throughout the entire process of applying vet.
for each tool app pair in addition to the three initial runs for trace collection we perform the experiments for three runs in each of these three settings disabling ui elements based on rank regions rank guided runs see .
for our ranking strategy disabling ui element based on rank rank and rank regions rank guided runs and keep the same settings as initial runs comparison runs .
note that each run also lasts for one hour and all experiments are conducted in the same hardware and software environment regardless of different settings.
we measure method coverage numbers of uniquely covered methods in app bytecode as one testing effectiveness metric in our experiments.
note that methods involved by app initialization i.e.
before tools start to test are excluded for a more precise comparison of code coverage gain.
upon each tool app pair we use the following test groups tgs for effectiveness comparison tg three initial runs and three comparison runs.
tg three initial runs and three rank guided runs.
tg three initial runs and three rank guided runs.
each group consists of six one hour runs intended for reducing random biases.
we accumulate the method coverage of all runs within a group for the group s method coverage.
test group serves as the baseline while the other two test groups aim to measure how much testing effectiveness gain can vet users expect.
the main reason for experimenting with exploration tarpit regions of different ranks is that there can be multiple tool issues on an aut and addressing only one of the issues might not suffice.
we also measure the crash triggering capabilities with cumulative numbers of distinct crashes.
we consider only crashes from bytecode given that android apps are predominantly written in jvm languages java and kotlin .
crashes are identified by hashing the code locations in stack traces.
we additionally leverage toller to disable each app s uncaughtexceptionhandler which is widely used by industrial apps to collect crash reports and might prevent crash information from being exposed to the android log system i.e.
logcat and captured by our scripts.
it should also be noted that toller monitors captures and manipulates uis with negligible overheads thus the testing effectiveness of original runs should remain comparable with and without toller in use.
in addition vetanalyzes traces very efficiently usually requiring only a few seconds on a single trace while analysis of multiple traces can be trivially parallelized.
.
.
results.
tables and show the effectiveness improvements by comparing three test groups.
as can be seen from the results automatically applying fixes based on vet s identified exploration tarpit regions helps ape monkey and wctester achieve up to .
.
and .
cumulative code coverage improvements relatively on apps using the same amount of time.
additionally vethelps ape monkey and wctester achieve up to .1x .1x and .9x overall distinct crashes respectively.
it should be noted that vet s automatic approach does not address all the tool issues some issues especially those rooting in tool implementations are likely addressable by only humans.
for most tool app pairs with improvements considering only rank exploration tarpit regions is sufficient for code coverage gain.
however there are cases where code coverage increases considerably when we consider rank top regions instead.
one explanation is that there are multiple applicability issues or multiple instances of exploration tarpit corresponding to the same applicability issue.
for example when applying monkey on the app nike run club there are multiple ways to enter a hard to escape functionality as depicted by the motivating example in .
if we block only one entry monkey can still find other ways to enter the functionality despite being more difficult and waste time there.
there are also cases where code coverage decreases when we consider rank exploration tarpit regions.
one reason is that lowerranked regions might not capture real issues but vet tries to fix them anyway indeliberately interfering with normal functionalities.
in the case of applying wctester on spotify the rank region does not reveal any tool issue according to our observation.
fixing this region can cause vet to restart the app when one major functionality shows up.
consequently wctester is unable to explore that functionality to achieve more coverage in guided runs.
.
rq3.
missed tool issues we show our analysis of tool issues that are not revealed by any exploration tarpit regions i.e.
false negatives reported by vet.
.
.
methodology.
we propose using issue specific detection tools to discover hidden tool issues in all the collected traces which can provide an estimation of how likely any issue is missed.
specifically we summarize the characteristics of two issues from .
app logout andunresponsive uis to design two approaches specifically targeting these two issues.
the reason for choosing the aforementioned issues is that they have a substantial appearance among all issues that we have identified and their existence is relatively more straightforward to be determined using our infrastructure.
we do not adopt manual inspection due to the subjectivity and the error prone nature of manual judgments especially given that we need to look at all the collected traces entirely.
our issue specific detection approaches work as follows app logout.
we first manually look into the activity list of each app with login ticked in table and identify the subset of activities that are used for logging in to the app.
then when we analyze a given trace we find the first and the last occurrence of any activity that belongs to the aforementioned list.
if there is any occurrence and the time distance between the first and the last occurrence is at least tmin we regard that an issue of app logout is found in the trace.
unresponsive uis.
in our investigation we find only one case that leads to unresponsive uis when an advertisement banner is clicked.
consequently a new activity can be observed where the activity belongs to the google admob sdk and has the same activity id across different apps.
thus we simply look for continuous appearance i.e.
there is no other activity in between 91esec fse august athens greece wenyu wang wei yang tianyin xu and tao xie table cumulative code coverage statistics.
app nameape monkey wctester m1rank rank m1rank rank m1rank rank m2 m2 m3 m3 m2 m2 m3 m3 m2 m2 m3 m3 accuweather .
.
.
.
.
.
autoscout24 .
.
.
.
.
.
duolingo .
.
.
.
.
.
flipboard .
.
.
.
.
.
merriam webster .
.
.
.
.
.
nike run club .
.
.
.
.
.
onenote .
.
.
.
.
.
quizlet .
.
.
.
.
.
spotify .
.
.
.
.
.
tripadvisor .
.
.
.
.
.
trivago .
.
.
.
.
.
wattpad .
.
.
.
.
.
webtoon .
.
.
.
.
.
wish .
.
.
.
.
.
youtube .
.
.
.
.
.
zedge .
.
.
.
.
.
average .
.
.
.
.
.
notes mn shows the total number of covered methods in test group n. mn mn m1 m1 .
table distinct crash statistics.
app nameape monkey wctester c1 c2 c3 c1 c2 c3 c1 c2 c3 accuweather autoscout24 duolingo flipboard merriam webster nike run club onenote quizlet spotify tripadvisor trivago wattpad webtoon wish youtube zedge total notes cn is for total triggered unique crashes in test group n. of the aforementioned activity id on the given trace.
note that we require the appearance to be continuous so as to exclude the cases where the tool such as ape chooses to restart the app.
if the appearance lasts for at least tmin we regard that an issue of unresponsive uis is found in the trace.
in order for a detected issue to be considered covered by our general purpose algorithms we require that at least one algorithmidentified exploration tarpit region covers at least of the time length within which the detected issue appears.
.
.
results.
we apply the specialized approaches on all collected traces.
as a sanity check we find that all the manuallydiscovered app logout and unresponsive uis issues are covered by the specialized approaches.
we compare the results against identified regions from vet s general purpose algorithms and perform manual confirmation.
we find only several cases where the vet algorithms do not yield accurate results discussed as follows on one trace from applying monkey on zedge vet misses one unresponsive uis issue by not reporting any covered region.
wefind that the excessive local exploration algorithm prioritizes another region over any region covering this issue.
however we find that this issue is also present in other traces from the same tool app pair and vet identifies and addresses this issue.
on each of two other traces from ape on duolingo and monkey on zedge there are two instances of the same unresponsive uis issue and vet reports only one of them.
the inaccuracy is also caused by the prioritization strategy of the excessive local exploration algorithm.
however since two instances point to the same issue on both traces vet is still effective.
on one trace from ape on quizlet ape logs out about only minutes after testing starts but vet reports only minutes of exploration tarpit.
we find quizlet s ui design to be somewhat unique the app has a special entry to some of the main functionalities in its landing page that is accessible without logging in.
the entry is buried within a paragraph of texts and the texts are shown only after a specific combination of swiping.
ape is able to find this special entry in this run making vet confused.
nevertheless vet is still able to find the correct trigger action from other traces.
discussion and limitation we are mainly focusing on making vet useful in the context of automated ui testing.
however it should be noted that vet s potential usage scenarios are beyond automated ui testing.
one usage case is for app ui quality assurance where an app might have ui design issues with one or more functionalities.
as a result human users may face difficulties locating their desired features.
when a human user runs into such situations he she will then likely search for the desired features through repeated and ineffective exploration around a few functionalities and the difficulties can be reflected by the collected user behavior statistics.
by subsequently utilizing vet s identification of exploration tarpits we can quickly know which functionalities likely have the aforementioned ui design issues potentially from numerous traces collected from end users and address these issues in a more timely manner.
92vet identifying and avoiding ui exploration tarpits esec fse august athens greece one question is whether vet is capable of differentiating testing scenarios that a tool is supposed to handle but does not i.e.
tool issues and that a tool is not expected to handle i.e.
beyond tool capabilities such as apps requiring special inputs .
we would like to point out that it is inherently difficult to differentiate these two types of scenarios due to lacking specifications over tool capabilities.
on the other hand vet can help users identify and mitigate the cases beyond a tool s capabilities being already useful.
we acknowledge that toller the utility that monitors captures and manipulates uis for vet still has limitations.
for example the current implementation of toller does not capture text inputs.
however adding support for text capturing is achievable with engineering efforts.
moreover toller s limitations do not prevent vet from being generalizable.
threats of validity a major external threat to the validity of our work is the environmental dependencies of our subject apps.
more specifically many of the industrial apps in our experiments require networking for main functionalities to be usable and it is possible for such dependency to change the behaviors of these apps despite our efforts to make our experiment environment consistent across different runs.
in order to reduce the influences of environmental dependencies in our experiments we repeat each experiment setting by three times and use aggregated metrics in our paper.
we additionally control each tool s internal randomness by setting a constant random seed for each app.
nevertheless this threat can be further reduced by involving more repetitions in our experiments.
a major internal threat to the validity of our work comes from the manual analysis of collected traces.
we need to manually determine whether the exploration tarpit regions reported by vet indeed reveal any tool issue.
consequently related evaluation results can be influenced by subjective judgments.
however it should be noted that any work involving manual judgments in the evaluation is vulnerable to this threat.
related work automated ui testing for android.
there have been various tools over years of development.
the earliest efforts include monkey a randomized tool that does not consider app uis or coverage information.
the superior efficiency brings strong competitiveness to the simple tool.
subsequent efforts result in tools mainly driven by randomness evolution ui modeling and systematic exploration .
recent work proposes time travel testing referred to as ttt to help android ui testing escape from ineffective aut states including loops and dead ends.
ttt uses checkpoint and restore checkpointing progressive states and restoring those states after loops and dead ends are detected.
vet is different from ttt in terms of design goals.
first ttt aims to recover from ineffective exploration while vet mainly focuses on prevention.
second exploration tarpits in vet are more general than ttt s lack of progress definitions.
one example is logging out where tools assisted by vet can still explore a fraction of app functionalities such as registering and resetting passwords.
loops and dead ends are not necessarily present when exploringan app with only a few functionalities.
third vet aims to enhance existing testing tools without the need to understand their internal design or implementation instead of building a new testing tool that excels at all apps.
the design of vet brings a few advantages.
first as acknowledged by ttt the state recovery may lead to inconsistent app states when testing apps with external state dependencies that are maintained at the server side.
note that controlling server side states is challenging e.g.
many industrial apps use external services that the apps have no control of.
vet s preventive strategy avoids this limitation.
second vet s preventive strategy does not incur overhead for lack of progress detection or state recovery in guided runs.
this strategy is specifically useful when a tool repeatedly gets into exploration tarpits.
third vetdoes not require additional device support for state recovery such as ram data restoring .
trace analysis.
our work is related to log and trace analysis.
existing work has been focusing on analyzing the logs generated by program code through techniques including anomaly detection cause analysis failure reproduction and performance issue detection .
our work focuses on ui traces with the goal of understanding ui exploration tarpits which are different from logs produced by program code.
parallel testing.
our work is also related to parallel testing in the sense of producing multiple variants of the target program i.e.
customizing the aut by manipulating the ui entries for the testing tool to work on.
related work on parallel testing includes parallelizing mutation testing symbolic execution and the debugging process .
conclusion we have exploited the opportunities of improving android ui testing via automatically identifying and addressing exploration tarpits.
specifically we have presented vet a general approach and supporting system for effectively identifying and addressing exploration tarpits.
we have designed specialized algorithms to support vet s concepts.
our evaluation results have shown that vet identifies exploration tarpits that cost up to .
of testing time budget revealing various issues hindering testing efficacy.
by trying to automatically fix the discovered issues vet helps the android ui testing tools under evaluation with achieving up to .
higher code coverage relatively and triggering up to .1x distinct crashes.