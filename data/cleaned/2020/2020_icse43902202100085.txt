fast outage analysis of large scale production clouds with service correlation mining yaohui wang k guozheng liy zijian wang k yu kangz yangfan zhou k hongyu zhangx feng gao jeffrey sun li yang pochian lee zhangwei xu pu zhaoz bo qiaoz liqun liz xu zhangz qingwei linz school of computer science fudan university china kshanghai key laboratory of intelligent information processing china yschool of electronics engineering and computer science peking university beijing china zmicrosoft research beijing china xschool of electrical engineering and computing the university of newcastle australia microsoft azure redmond usa abstract cloud based services are surging into popularity in recent years.
however outages i.e.
severe incidents that always impact multiple services can dramatically affect user experience and incur severe economic losses.
locating the rootcause service i.e.
the service that contains the root cause of the outage is a crucial step to mitigate the impact of the outage.
in current industrial practice this is generally performed in a bootstrap manner and largely depends on human efforts the service that directly causes the outage is identified first and the suspected root cause is traced back manually from service to service during diagnosis until the actual root cause is found.
unfortunately production cloud systems typically contain a large number of interdependent services.
such a manual root cause analysis is often time consuming and labor intensive.
in this work we propose cot the first outage triage approach that considers the global view of service correlations.
cot mines the correlations among services from outage diagnosis data.
after learning from historical outages cot can infer the root cause of emerging ones accurately.
we implement cot and evaluate it on a real world dataset containing one year of data collected from microsoft azure one of the representative cloud computing platforms in the world.
our experimental results show that cot can reach a triage accuracy of .
.
which outperforms the state of the art triage approach by .
.
.
index terms cloud computing root cause analysis outage triage machine learning i. i ntroduction cloud computing has become increasingly popular in recent years.
many companies have migrated their services to various cloud computing platforms e.g.
microsoft azure amazon aws and google cloud.
these platforms provide a variety of services to millions of users from all over the world every day.
availability is one of the most critical concern to cloud computing platforms influencing the user experience and the cloud providers revenue significantly.
although tremendous efforts have been devoted to maintaining high service availability cloud computing platforms still encounter many incidents i.e.
unplanned interruptions of the services.
these incidents especially outages i.e.
this work was done at microsoft research beijing china .
yaohui wang and guozheng li contribute equally to this work.a group of related severe incidents that may impact multiple services cause significant economic losses.
according to a study conducted on .
million us businesses1 a failure that takes a top cloud provider offline in the us for to days would result in billion of economic loss.
in this regard once an outage occurs it should be mitigated in a timely manner to minimize its impact.
one important step of such outage mitigation is locating its root cause service which allows the corresponding responsible team to fix the outage.
in current industrial practice outages are typically declared on an originating service i.e.
the service where the outage manifests.
the team responsible for this service will analyze the outage and may redirect the outage to the team of another service until it eventually reaches the root cause service where the outage can be fixed.
our empirical study discussed in section ii on a representative large scale cloud shows that assigning an outage to wrong services leads to a timeconsuming root cause analysis process.
unfortunately fast and accurate root cause analysis of outage is a very challenging task.
firstly the number of services is vast in large scale cloud computing platforms.
the dependencies among the services are incredibly sophisticated.
many services have interdependencies e.g.
the micro service management service is used to deploy the resource management service while the computing node where the micro service management service is deployed is managed by the resource management service .
many dynamic dependencies are even implicit for engineers e.g.
asynchronous communication virtual routers virtual disks and some services deployed on the same node may affect each other e.g.
monitoring services and functional services .
when an outage occurs massive noisy alerts might be reported usually as incident tickets due to the notorious flooding alarm problem in cloud computing platforms .
as a result it is difficult to decide the root cause service.
according to our empirical study the originating service and root cause service are different for over cloud down ieee acm 43rd international conference on software engineering icse .
ieee outages.
secondly during the root cause analysis process the engineers of one service typically have only a partial view of the outage since they may only be familiar with their own service and its closely related ones.
consequently the outage may be passed from one service team to another resulting in a long team assignment chain for root cause analysis.
our empirical study shows that such a long chain dramatically prolongs the time required for root cause analysis.
incident triage which aims at locating the root cause of an incident in a small scope of services has been extensively investigated in the literature.
for example deepct is a state of the art incident triage method.
however such methods rely only on the information of the incident per se e.g.
the title and summary of incident report and engineers discussions during diagnosis.
they do not take the global view of service correlations into consideration.
as a result they are not suitable for root cause analysis of outage since an outage typically involves many correlated services.
in this paper we propose cot correlation based outage triage to facilitate fast and accurate outage root cause analysis.
cot is a first attempt in the literature to our knowledge which provides a global view of service correlations for outage root cause analysis.
cot collects all the incidents in the same region and the adjacent time range with the outage.
based on the historical outage diagnosis data it finds the correlations among these incidents and builds an incident correlation graph.
by mapping incidents to their owning services cot can further obtain the service correlation graph which indicates the anomaly propagation patterns among services.
it then takes the features extracted from these graphs of historical outages with their root cause services as training samples to train a machine learning model.
this model can then be used to predict the root cause service for emerging outages.
we evaluate cot in two ways.
we first conduct a quantitative experiment to compare the efficiency and effectiveness of cot and deepct the state of the art approach.
the result shows that cot can perform root cause analysis of an outage within one minute and outperforms deepct by .
.
in accuracy.
second we evaluate the usability of cot through a representative real world case study.
the results show that cot could facilitate engineers to understand outages and locate the root cause services.
the contributions of this work are summarized as follows.
we conduct the first empirical study on the outage triage problem in large scale production clouds.
the results are based on representative real world cloud services and can facilitate further follow up research.
we propose cot a generic cross service outage triage approach.
it is a correlation based approach which predicts the root cause service at the early stage of an outage with high accuracy.
we comprehensively evaluate cot by comparing it with the state of the art triage approach and by conducting a case study of two real world outages.
the rest of the paper is organized as follows section ii presents the results of an empirical study of outage triage.
fig.
the life cycle of an outage.
section iii describes the overall design of the correlationbased outage triage approach and section iv introduces the implementation details of cot and the experimental settings.
in section v we evaluate the performance and time efficiency of cot and compare it to the state of the art incident triage method.
in section vi we conduct a case study of two realworld outages to show the effectivness of cot.
section vii discusses the threats to the validity of our work.
section viii presents some related work.
section ix concludes the paper.
ii.
a nempirical study of outage triage this section presents the first empirical study of outage triage in real world cloud computing platforms.
the study is based on microsoft azure one of the representative cloud computing platforms in the industry.
in the following we first introduce the life cycle of an outage in microsoft azure especially the differences between outage triage and incident triage.
next we analyze noutages collected from microsoft azure ranging from to and present an empirical investigation of the outage reassignment during the outage triage process.
due to the confidential policy of microsoft azure we use variables to represent the sensitive data instead of disclosing the specific figures.
a. the life cycle of outages in cloud computing platforms incidents outages are the unplanned interruptions of the services which could be caused by many factors such as power failures hardware failures configuration problems and code bugs.
for each incident outage engineers assign a severity level based on their potential impact on users.
the severity level ranges from to where incidents with severity have the highest priority and may bring negative impacts to customers and severity incidents are least important and do not need to be dealt with immediately.
engineers only declare a small portion of incidents as outages which are severe incidents and often involve multiple services in the cloud computing platform.
previous research has investigated four typical incident management procedures including incident reporting triage mitigation and resolution .
as illustrated in figure the life cycle of outages is similar to that of incidents.
the following mainly demonstrates the detailed practice of outage triage and emphasizes the differences between outages and 886applicationapplication infrastructurecomputing storagecomputing infrastructurestorage network networkoriginating services root cause services fig.
outages originating services and root cause services.
incidents.
after an incident is reported on call engineers oces will declare it as outage if they find the incident may cause severe impact to end users.
next oces will try to restore the service as soon as possible through a set of pre defined instructions i.e.
steps which tell engineer how to minimize the impact.
these instructions are related to the incident which the outage is declared from.
however as many outages may have cross service impacts section ii b the instructions usually do not work.
for these outages engineers need to do outage triage to locate the root cause service and the responsible team.
during the outage triage process after assigning the outage to a team the engineers in this team will confirm whether or not they are responsible for the outage.
if not they will reassign the outage to another team which is potentially responsible.
this process is called outage reassignment and it is repeated until the responsible team of the root cause service is found.
after that to contain the outage s impact to end users the responsible team will try to debug the service and use quick fixes to mitigate the outage.
finally to avoid the same failure from happening again engineers need to study the outage in depth and fix the problem permanently in the outage resolution phase.
at the same time to learn the lessons from the outage and assess its impact engineers from all impacted services may dive into the details of the outage find the related incidents and mark the correlations among those incidents.
b. an empirical analysis of outage triage based on the outage data collected from microsoft azure we conduct an empirical investigation of outage triage.
the services in the online computing platform have complicated dependencies.
to better understand the relationships among different services we divide the services into five categories according to their functionality namely infrastructure networking storage compute and application .
then we investigate the practice of outage triage from two aspects the number and time cost of outage reassignments respectively.
services of different categories play various roles in the cloud computing platform.
the infrastructure category lies on the lowest level of the cloud computing platform and is responsible for the hardware availability.
all other services rely on the infrastructure category.
the network category is responsible for network connectivity.
the storage category is responsible for the management of storage resources.
the cross level outages cross service outages all outagesapplication networkingcompute storage infrastructure0 n n n n 2fig.
the amount of the cross service and cross level outages sqrt scale .
computing category is responsible for the management of computing resources.
the application category lies on the highest level of the cloud computing platform.
services in this category are exposed to customers and rely on the supporting services of other categories directly or indirectly.
number of outage reassignments.
the incident management system icm records the reassignment behaviors among different services.
figure shows the relationships between originating services and root cause services of the collected outages.
we can learn that the cross level outages whose originating service level and root cause service level are different exist in all categories except infrastructure .
in particular for the outages whose root cause services belonging to the networking category around are crosslevel.
note that each category contains multiple services and the outages which are not cross level outages could still be cross service outages.
we categorize the outages according to their originating service level and compute the amount of the cross level outages and cross service outages.
the results in figure show that for storage networking and infrastructure categories nearly all the cross service outages are also crosslevel outages.
for cross service outages in the application and compute categories about of them are cross level.
we further analyze the average number of outage reassignments from different originating service categories.
the result in table i shows that the average reassignment number of all outages is nearly one.
in particular the outages whose originating services are in the compute category have significantly higher average reassignment rate.
this fact further motivates the necessity of accurate outage triage at the service level.
table i the average number of outage reassignments from different originating service categories.
level networking storage compute application all avg .
.
.
.
.
among all cross level outages around are transferred from low level categories to high level categories.
we analyze those outages and summarize the following two reasons.
method caller errors.
services in high level categories always call the functions of services in the low level categories but the root cause of outages may exist in the caller side beside the callee side.
for example 8871t 6t 2t 4t 6t 8t networking storage computeapplica g415on fig.
the outage reassignment time cost in different categories.
anapplication service query data through a function of a storage service but wrong parameters lead to outages.
these outages originating services belong to thestorage category but the root cause services are from the application category.
symbiotic errors.
the services of different categories could influence each other because they run on the same physical nodes.
for example a service about managing network resources is from the networking category and a monitoring application also runs on the same node.
some network outages occur to the cloud computing platform because of monitors taking too many physical nodes resources.
these outages originating services are from the networking category but the root cause service is the monitoring application.
outage reassignment cost.
the reassignment cost indicates the increment of the outage triage time because of assigning outages to the wrong services.
the icm records an accurate time when transferring the outage to the specific services.
we calculate the time difference of outages transferring into and out of a wrong service as the cost of one outage reassignment.
we can obtain the total reassignment cost of an outage by adding up all the reassignment cost of this outage.
we categorize the outages according to the originating service level and compute each category s average reassignment cost.
the results in figure demonstrate that thenetworking category has a relatively larger reassignment cost than other categories.
based on the above analysis of outage triage from the aspects of outage reassignment rate and cost the outage reassignment at the service level could incur huge time cost.
accurate outage triage could significantly improve the efficiency of outage mitigation and reduce the cost.
c. a real world outage triage example fig.
the triage path for the unexpected vm restart outage.
to help better understand the outage triage process we show a real world triage process for an unexpected vm restart outage.
figure shows the triage path for this outage.
first the vm virtual machine monitor detects the unexpected vm restart event on some vms and reports an incident to the icm.
engineers assess the incident at first.
they consider that this incident may cause severe impact to end users and declare it as an outage.
when diagnosing the vmservice engineers find that the storage the vms use is not accessible and they transfer the outage to the related storage service.
then engineers of the storage service find that the cpu and memory load of some storage services is unusually high and the storage service is crashed due to an oom out of memory exception.
they also find the reason for this phenomenon is a flood of new connections to the storage cluster which is caused by the restart of the slb service load balancer .
finally the outage s root cause is confirmed by the related network service which is responsible for the slb and engineers in thenetwork service mitigate this outage by slowing down the slb restart process.
according to this case engineers transfer the outage from service to service and the engineers of each service need to understand the outage first diagnose the service to confirm whether they are responsible for the outage and if not they need to assign the outage to the engineers of another service manually.
iii.
t heproposed approach to outage triage the example explained in section ii c shows that the service teams only know their direct dependency and always triage the outage following dependency relations.
therefore locating the root cause service without reassignment requires a global view of dependencies for all related services.
it is not a trivial task for large scale cloud computing platform because the dependencies of inner services are incredibly complicated and change frequently.
many services have interdependencies many dependencies abstracted by the platform are even implicit for engineers and some services deployed on the same node may affect each other.
besides service dependencies are not static but related to specific problems.
cot constructs the service correlation graph of an outage based on the incidents.
when a new outage occurs in the cloud computing platform cot collects all the incidents in the same region and adjacent time range with the outage.
then it filters the incidents which are directly or indirectly related to the outage and construct an incident correlation graph from these incidents.
finally it maps the incidents to services to construct a service correlation graph for the outage.
the outages caused by the same root cause service have similar symptoms i.e.
service correlation graph section vi which inspires us to train a machine learning model to predict the root cause.
our model takes the service correlation graph of historical outages as training samples and the label of each outage is the root cause service which is labeled by engineers after outage diagnosis.
figure gives an overview of our method which contains five steps incident data fetching meta incident id generating incident correlation graph building service correlation graph 888fig.
an overview of cot.
building and root cause service prediction.
in the following we further explain the details of each step.
a. incident data fetching since the impact of one outage usually has locality in time and space we only fetch the incidents which are reported in the same region and adjacent time range with the outage.
in microsoft azure the medium time of finding the correct rootcause service is t. and our statistics on historical outages show that most of the early incidents related to an outage are reported within about 2tbefore the outage declaration time.
so we set the start time of the time window to 2tbefore the outage declaration time which allows us to capture most of the related early incidents.
also to examine how our approach is better than the traditional outage triage process the end time should be smaller than t. so we choose three time windows 2t 3t 2t 3t and .
the last time window is to examine the performance of our approach when we get the same incident information as human engineers.
note that the range of the time window should be different in different systems as the statistics of the early outage related incidents and the average outage triage time are different.
but the method to determine the time window should be general thus people can follow this method to evaluate the performance of the approach using different time windows and choose the best time window that fits their system.
b. meta incident id generating to utilize the historical outage diagnosis data we need to distinguish incidents by the abnormal symptoms they represent i.e.
find past incidents that represent the same abnormal symptoms as newly occurring incidents.
the ideal way is to use monitor ids to identify different abnormal symptoms from incidents.
this is reasonable for a well designed monitoring system where every monitor has a unique id.
but according to our experience of microsoft azure the icm system is a hub that gathers incidents reported by many different sources from hundreds even thousands of services.
it is hard to define general rules to constrain how the monitors should report incidents.
so some monitors may use the same identification some may monitor several different services at the same time fig.
an example of parsing the template from an incident title.
and some different monitors may monitor the same properties of the same resource type in different regions.
in microsoft azure the title of an incident usually summarizes the symptom of the incident.
however we cannot directly use incident titles to distinguish different abnormal symptoms since they are usually log like texts and may contain variables like service metrics time or location.
thanks to the well studied log parsing technology in recent years we can use the log parsing method to extract incident report templates from their titles and use the templates to distinguish incidents by the abnormal symptoms they represent.
we go through the historical incident data and assign a unique id called metaincident id to each of the incident template to obtain an incident template meta incident id mapping.
we follow the widely used log parsing approach slct to extract the template from incident titles automatically.
the original slct log parser builds the word vocabulary over all the log texts picks out candidate words that appear frequently and uses these words to extract templates from log texts.
but in our case directly using slct to parse incident titles may bring one problem some location representations may contain multiple words and numeric values like west us but the slct will treat each part of the location representation separately.
to make the incident parsing more precise we manually add the location representations in microsoft azure to the vocabulary.
figure shows a log parsing example.
the log parser first turns all words to lower cases.
the words in the built vocabulary stay unmodified and words representing locations are recognized as location tokens and others are recognized as variable tokens.
besides to prevent the case that two different services use the same template we combine the owning service with the template to identify the incidents.
889for each incident in the past we can use this method to extract its template.
we consider each incident template as a different abnormal symptom in the system and give each of them a unique meta incident id.
for a new incident we can use the same incident parsing method to extract the incident template and obtain its corresponding meta incident id.
c. incident correlation graph gi since a large scale cloud computing platform contains a large number of services the reported incidents may come from many different services and the amount is usually very large even within a small time window.
among them only a small proportion of incidents are related to the outage section vi .
in current practice finding related incidents of a new outage relies on heavy manual work of experienced engineers.
we use the historical outage diagnosis data to solve this problem.
we first build the meta incident id correlation graph gmfrom historical data to infer the potential links for newly occurring incidents.
for each pair of incidents labeled as correlated in the outage resolution phase section ii a we parse them and obtain a pair of meta incident ids.
then we take the meta incident ids as nodes and the their correlations as edges to build an meta incident id correlation graph gm.
gmis further used to build the incident correlation graphs for new outages.
now we can build the incident correlation graph gias we already get the meta incident ids for the incidents fetched earlier and the correlations among the meta incident ids in gm.
algorithm illustrates this process.
its input contains three parts gmis the meta incident id correlation graph i is the set of incidents that are reported near the outage in time and locality iois the incident of the outage i.e.
the incident where the outage is declared from .
its output is the incident correlation graph gi.ngiandegihold the incidents and edges in gi respectively.
at the beginning of the procedure iois the only element in ngi and egiis empty.
then for each loop we go through each incident in i get its metaincident id and check whether there exist any links in gm that can link this incident to the existing graph.
we will repeat this loop until no new incidents can be added to the graph.
d. service correlation graph gs as we have mentioned in section ii a every outage is well studied after mitigation so the same root cause bug of a service is unlikely to happen again in the future.
even for two outages that have the same root cause service their rootcause bugs are usually different so as the abnormal metrics observed by monitors and the reported incidents.
thus it is hard to refer to historical incident patterns for new outages.
however although the malfunction of the root cause service is usually caused by different bugs the spreading paths of the anomaly among dependent services caused by the rootcause service have similar patterns section vi .
so we group the incidents in the incident correlation graph giby their reporting services to generate the service correlation graph gs.algorithm the procedure of building gi input gm i io output gi procedure build incident correlation graph ngi fiog egi fg n0 gi ngi e0 gi egi loop foreach incident iaini n0 gido ida meta incident id of ia foreach incident ibinn0 gido idb meta incident id of ib if id a idb ingmthen add incident iatongi add edge ia ib toegi ifsize of n0 gi6 size of ngithen n0 gi ngi e0 gi egi goto loop gi fngi egig return gi the edges in giare mapped to gsaccordingly.
algorithm illustrates this process.
we first gather all the owning services of the incidents in gitongs.
these services are nodes in gs.
and then we map the incident links in gito the corresponding service links in gs.
these links are the edges in gs.
this graph helps us refer to historical anomaly spreading patterns at the service level and helps us predict the root cause service using the machine learning algorithm.
algorithm the procedure of building gs input gi output gs procedure build service correlation graph ngi nodes in gi egi edges in gi ngs fg egs fg foreach incident iiinngido s owning service of ii add service stongs forincident link ia ib inegido sa sb owning services of ia ib add edge sa sb toegs gs fngs egsg return gs 890e.
training and predicting after the previous steps although we have reduced the search space of the outage related incidents and services the incident correlation graph giand the service correlation graph gscan still be large since the impact of an outage is usually wide range.
besides these services may have interdependencies among each other.
so engineers still need to dive into the specific services and spend much time for analysis.
however as mentioned in section iii d the spreading paths of the anomaly among dependent services caused by the same root cause service have the same pattern so we can utilize machine learning algorithms to learn the patterns from historical service correlation graphs and to predict the rootcause service automatically.
we tried two machine learning algorithms the svm support vector machine algorithm and the decision tree algorithm which are widely used in a variety of classification tasks.
model training for each outage in the past we use the methods mentioned above to build its incident correlation graph giand service correlation graph gs.
the label of each outage is its root cause service which is labeled manually after outage diagnosis.
and we take the structural information ofgsas a feature vector which contains two parts the number of incidents in each service and the links between these services.
for the first part each element represents a service and its value is the number of incidents included by this service ings.
for the second part each element represents a link between two services and its value is either or indicating the existence of the link in gs.
note that we have a large number of services and the links among them form a sparse matrix.
this part only includes the service links which occur at least once in the past.
root cause service prediction to predict the root cause service of a new outage we use the same methods to build its incident correlation graph giand service correlation graph gs.
we then extract the feature vector from gsusing the same method as in the model training part.
finally the machine learning model takes the feature vector as the input and predicts the root cause service as the output.
iv.
e xperiment in this section we describe the details of our experiments.
we will first present the details of the data we collect for experiments.
then we introduce the techniques we use to implement our approach.
finally we introduce deepct the state of the art triage approach we use for comparison.
a. data collection all the data we use in this paper are collected from the production environment of microsoft azure.
in the icm system of microsoft azure the data are stored in a distributed nosql database.
we collect months of incident and outage data from the icm system for training.
specifically we first use the months of data to build the incident template meta incident id mapping and the meta incident id correlation graph gm.we build the incident correlation graph giand the service correlation graph gsfor each outage in the months.
we then use the method in section iii e to train the machine learning model.
we collect another months of incident and outage data from the icm system for testing.
the outages in the test dataset happen after the ones in the training dataset.
we build the incident correlation graph giand the service correlation graph gsfor each outage in the months and use the method in section iii e to predict the root cause service for each outage.
all these outages involve underlying services in microsoft azure and all the data occupies about 133gb of disk space.
b. implementation our programs are written in python3 a scripting language that is widely used in data mining and machine learning tasks.
we also use some third party python3 packages to facilitate our development process.
we use the python3 library scikitlearn to build the machine learning models.
scikit learn is an open source machine learning library that supports supervised and unsupervised learning.
c. compared method we compare cot which is a correlation based triage approach with deepct which is the state of the art triage approach based on text similarity of incident reports.
deepct incorporates a novel gru gated recurrent unit model with an attention based mask strategy and a revised loss function.
it can incrementally learn knowledge from discussions and update incident triage results .
deepct has demonstrated its effectiveness on large scale online service systems.
the data required by deepct contains three parts the title and summary of an incident report the incremental discussions about an incident and the occurring environment information of an incident.
deepct uses a cnn based text encoder to produce feature vectors from the first two parts of data which are textual.
the third part of the data is a finite set of discrete values so it uses representation learning to embed each input datum value into a fixeddimension vector .
deepct uses these data to train the designed gru based model to help predict the root cause of an incident.
we follow the original paper of deepct to implement the cnn based text encoder the representation learning model and the gru based model.
since outages are declared from incidents the three parts of data deepct requires are available in outages.
so we can easily migrate deepct to fit the outage triage problem.
we use the same training and test dataset as cot uses to train and evaluate deepct.
v. e xperimental results in this section we present the experimental results of our approach and try to answer the following three research questions.
891fig.
the accuracy of cot and deepct in predicting the root cause service for outages.
rq1 how does our approach work in outage triage comparing to the state of the art triage approach?
rq2 how is the performance of our approach on different kinds of outages?
rq3 how is the time efficiency of our approach?
a. evaluation metrics the goal of cot is to facilitate the outage triage process i.e.
to help engineers find the correct root cause service at the early stage of the outage.
for each outage we use cot to predict its root cause service compare it to the ground truth and calculate the accuracy.
besides to better understand the result we further inspect its performance on different kinds of outages i.e.
the outages whose root cause services belong to different service categories.
b. performance figure shows the accuracy of cot and deepct in predicting the root cause service with different time windows.
as it shows cot sv m outperforms cot decisiontree slightly at any time window.
and these two models outperform deepct a lot.
the highest accuracy of deepct is just .
and the accuracy of cot sv m is higher than deepct by .
.
.
as the result shows the accuracy of cot sv m and cot decisiontree changes slightly as the time window grows.
this tells that the early signals of the outage supply enough information for us to find the root cause service and as the time window grows the accuracy does not increase much.
answer to rq1 cot can help predict the root cause service of an outage at its early stage with high accuracy.
it outperforms deepct by .
.
in predicting the rootcause service.
we use the result of cot decisiontree with time window 2t 3t as an example to show the performance of cot on different kinds of outages figure .
other results have the similar pattern with only slight difference.
as figure shows for outages whose root cause services belong to the computing storage or network category the accuracy is fig.
the accuracy of cot decisiontree for different kinds of outages when setting time window to 2t 3t time windowstime seconds fig.
the average time cot spends on predicting the rootcause service for an outage in different time windows.
relatively high.
for outages whose root cause services belong to the application orinfrastructure category the accuracy is relatively low.
different from the underlying supporting services the application services change more frequently which causes a lot of new signals to emerge.
therefore the service correlation graphs for these outages change a lot over time.
this phenomenon is natural since the development of useroriented services should satisfy the rapidly changing business requirements.
this causes the low accuracy of the application category.
the low accuracy of the infrastructure category is because the number of outages of this kind is small in our dataset as the infrastructure services is more robust and fails less frequently.
the machine learning algorithm only gains limited knowledge from small samples.
its result should be better as we collect more training samples in the future.
answer to rq2 the performance of cot is different on different kinds of outages.
for the cloud system we studied cot performs better for outages whose root cause service is in thecomputing storage or network category.
further research should be taken to tackle the application andinfrastructure categories.
c. time efficiency to evaluate the time efficiency of cot we record the time it spends to predict the root cause services of outages in different time windows.
for the convenience of deployment we package the cot project as a docker image which is a widely used container technique.
we deploy it on a machine 892equipped with a quad core intel xeon e5 v4 cpu and with gb memory.
the operating system is ubuntu .
.
lts the docker version is .
.
and the python version used in the docker container is .
.
figure shows the results.
for each outage as the range of the time window grows the average time of prediction is .
.62s.
this indicates that cot is efficient for the outage triage task.
our experiment shows that data fetching takes a large proportion of the time.
this is because the amount of incidents within the time window is usually large and the incidents are stored in a large scale distributed nosql database.
in the future we can improve the time efficiency of cot by caching the incident data and reducing the data fetching time.
answer to rq3 the average running time of cot is within one minute which is efficient for the outage triage task.
vi.
c ase study cot is shown to be very helpful for production outage management.
in particular the service correlation graphs built by cot can substantially facilitate the understanding of the complex multi hop dependencies among services which consequently helps with quick mitigation of the outages.
in this section we conduct a case study of two real world outages outage aandoutage b in microsoft azure to understand how cot works and how cot helps save time in the realworld outage triage scenarios.
a. two real world cases the root cause of outage ais some chained reactions of the system followed by the activation of the ddos defense policy in microsoft azure.
the ddos attack is quite frequent in the cloud computing scenario.
thus cloud computing platforms usually build ddos defense policy at the network service level to defense such attack.
in outage a the ddos defense policy is triggered by some vicious traffic but its reaction is too aggressive so it also treats other normal network traffic as vicious too.
this causes a network configuration server to suffer from network package loss and fail to synchronize the configurations it stores.
one important configuration in this server is the mapping from service component id to vips virtual ip address .
due to the failure of configuration synchronization this mapping is missing for a dns service.
when other services read this configuration they cannot gain the exact vip which matches the dns service.
and following the longest match of the service id s prefix what they get is a scope of vips belonging to a subnet.
this causes abnormal network traffics to the subnet and triggered the ddos defense policy to protect the subnet which chooses to drop some network packages targeting at the subnet for some time.
outage bis caused by the misconfiguration of some routers.
two engineers intend to modify the tcp mss maximum segment size configuration of two routers.
they should have modified this in the network control plane protocol but they did this in the data plane.
the consequence is that every tcp networkx data factory storagex speech services data movement sql db a the anomaly spreading pattern among services in outage a networkx data factory storagex speech services data movement sql db b the anomaly spreading pattern among services in outage b fig.
the anomaly spreading pattern among services in outage aandoutage b. each circle in the figure indicates a service in microsoft azure.
the size of the circle indicates the relative number of incidents reported in that service.
two circles are linked if they are related.
some unimportant services are ignored in this graph for the convenience of explanation.
packet received by the router uses intensive cpu time for checking.
this causes the cpu usage level to be very high and the router starts to drop some packages due to the protection policy.
the root cause service is called networkx for both outages.
but their root cause bugs are different and are both quite complicated.
this causes the monitored metrics in the system to be different and further causes the reporting incidents to be different.
for example the incident reported by networkx in outage ais a node connectivity failure and in outage bit is a configuration synchronization failure.
however the spreading patterns of the anomaly among services are similar.
figure shows the service correlation graphs of these two outages.
they all cause the availability of the network services to be low in a subnet.
this affects many supporting services which are widely deployed in different regions and sensitive to packet loss and network latency like the data management services data movement anddata factory and the storage services storagex andsql db .
these supporting services further affect the user oriented services.
cot is able to catch the spreading pattern of anomaly starting from the networkx service in outage a and use this knowledge to predict the root cause service of outage bwith high accuracy.
the triage practice for both outages suffers from flooding alarm problems .
for example during the impact of outage b there are kincidents reported by services from the affected region of outage bin total.
among them only around incidents reported from services .
of are related to outage b. so it is hard for engineers to find related incidents services directly in the icm system.
the outage is first declared from a user oriented service speech services which is used to convert spoken audio to text and the triage practice of these two outages is like the case we show earlier in section ii c i.e.
from the user oriented service to the real root cause service with many reassignments 893in this triage process.
this causes the triage time of these two outages to be very high.
cot can save engineers from such a tedious triage process by predicting the root cause service at the early stage of the outage quickly and accurately.
for example when setting the time window to 2t 3t cot can correctly predict the root cause of outage band can help save nearly of the triage time in theory.
b. lessons learned we summary the lessons learned from the above two cases as follows the root cause bug of an outage is usually very complicated.
as these bugs are well repaired after the outage the same bug may not happen again in the future.
this causes the indicators of the impacted service metrics to be different even if the root cause services of the two outages are the same.
so using only incident content data is not sufficient in the outage triage task.
the traditional outage triage practice is heavily affected by the flooding alarm problem in the icm.
during the impact of an outage the proportion of related incidents services is very low.
so it is hard for engineers to identify the most important incidents and services.
therefore in traditional triage practice manual diagnosis and chained reassignment process are inevitable.
cot catches the anomaly spreading patterns of different root cause services from the historical outage diagnosis data.
it uses such knowledge to predict the root cause service for newly occurring outages effectively and efficiently.
this helps to save a large amount of outage triage time and reduce the time to mitigate an outage.
vii.
t hreats to validity cot highly depends on the historical incident correlation data and outage diagnosis data.
currently we cannot handle zero day incidents and new anomaly spreading patterns well.
but this can be mitigated by continuously updating our model i.e.
regenerate the meta incident id correlation graph and retrain the machine learning model regularly.
we also plan to handle the cases where the services and their dependencies change more frequently e.g.
the application category in section v b in our future work.
another threat comes from our implementation of deepct .
we let two members of our team to carefully follow the paper of deepct to re implement it.
we test the performance of our implementation using the incident triage data of microsoft azure and the results we get are consistent with the results described in the original paper only with small differences.
so our implementation should be consistent with the original deepct.
limited to the data we can obtain the effectiveness of our approach has only been confirmed in microsoft azure.
however as microsoft azure is a leading cloud computing platform with a mature icm system our experience should be representative and should be applicable to other largescale cloud computing platforms in the market.
besides although the data we use cannot be disclosed the method is reproducible as we have well explained the required details to implement cot in section iii.
in the future we will try to evaluate cot on other cloud computing platforms to better understand its generality.
viii.
r elated work a. incident triage some work focuses on incident triage in large scale online systems .
chen et al.
perform a comprehensive empirical study of incident triage on real world largescale online service systems.
their work studies the status of incident triage from many aspects and explores the practicability of bug triage methods on incident triage.
it concludes that traditional bug triage approaches may benefit the incident triage task but they still need to be further improved to fit the context of incident triage.
the work conducted by chen et al .
is the most relevant one to our work.
they propose a deep learning approach named deepct to solve the continuous incident triage problem.
the authors utilize the historical discussion data of incidents use the domainspecific text encoding method to extract features from these discussions and train the deep learning model.
they show the effectiveness of their approach on real world online service systems.
the above work is different from ours because they mainly focus on incident triage within a single online service while our work focuses on the outage triage problem in the largescale cloud computing platforms which may involve hundreds of different services of different types.
b. fault detection and localization fault detection and localization in cloud systems have been widely studied in previous work .
these methods require constructing the causality graph among components in the system.
for example mariani et al.
exploit machine learning to detect anomalies in kpis and exploit the causal relationships among kpis and centrality indices to identify the causes of the failures.
nguy et al.
propose fchain which detects anomaly from system metrics e.g.
cpu memory network statistics and locate the cause by using both fault propagation patterns and inter component dependencies.
however existing methods are not suitable for outage triage in large scale production clouds.
firstly the dependencies among components in cloud computing platforms are incredibly sophisticated.
many services have interdependencies and many dynamic dependencies are even implicit for engineers.
secondly when a highly impactful outage occurs massive incidents are reported by different services simultaneously.
it is also infeasible to apply other methods in our problem settings as they address incidents one by one and greatly suffer from scalability issues.
our method jointly analyzes these incidents.
specifically cot focuses on the relationship among incidents services and conducts mitigations altogether to avoid redundant efforts.
894c.
bug triage bug triage has been widely studied in previous work .
some of the work uses the learning based approaches.
jonsson et al .
propose an ensemble learning model that combines several classifiers to help assign a bug to the correct development team automatically.
lee et al .
propose to use a convolutional neural network cnn and word embedding techniques to build an automatic bug triager.
some of the work is based on information retrieval methods.
xia et al.
propose a method that uses topic modeling to map the bug reports to their corresponding topics.
it assigns the bug to the developer by considering the correlation between the developer and the topics of the bug.
hu et al.
model the relationships among developers source code components and their associated bugs from historical bug fixing data and use this knowledge to help assign a bug to the correct developer.
different from these work that targets at bug triage for traditional software systems our work focuses on outage triage for large scale cloud computing platforms which involve hundreds of distributed services.
ix.
c onclusion in this paper we conduct the first systematic empirical study on the cross service outage triage problem in largescale cloud computing platforms.
we also propose cot a novel data driven correlation based outage triage approach.
cot mines the anomaly spreading patterns among services caused by different root cause failures from historical outage diagnosis data.
it then uses machine learning algorithms to help predict the root cause service of a new outage and to accelerate the outage triage process.
we evaluate our approach on the production environment of microsoft azure one of the top cloud providers around the world.
the data we collected occupies about 133gb of disk space and contains records of outages and incidents from a whole year.
experiments show that our model outperforms the state of the art triage approach which is based on text similarity by .
.
in accuracy and its overhead is within one minute.
in the future we intend to conduct in depth research on the cases where services and service dependencies change frequently.
we will also apply our approach to more production cloud systems to have a deeper understanding of the approach s generality.
acknowledgement we thank our colleagues in microsoft azure who developed the incident management system for their kind help and support in this work.
this work was supported by the national natural science foundation of china project no.
.
hongyu zhang is supported by arc dp200102940.