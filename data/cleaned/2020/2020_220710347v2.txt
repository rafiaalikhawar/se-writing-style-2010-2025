demystifying dependency bugs in deep learning stack kaifeng huang fudan university chinabihuan chen fudan university chinasusheng wu fudan university china junming cao fudan university chinalei ma the university of tokyo japanxin peng fudan university china abstract deep learning dl applications built upon a heterogeneous and complex dl stack e.g.
nvidia gpu linux cuda driver python runtime and tensorflow are subject to software and hardware dependencies across the dl stack.
one challenge in dependency management across the entire engineering lifecycle is posed by the asynchronous and radical evolution and the complex version constraints among dependencies.
developers may introduce dependency bugs dbs in selecting using and maintaining dependencies.
however the characteristics of dbs in dl stack is still under investigated hindering practical solutions to dependency management in dl stack.
to bridge this gap this paper presents the first comprehensive study to characterize symptoms root causes and fix patterns of dbs across the whole dl stack with dbs collected from stackoverflow posts and github issues.
for each db we first investigate the symptom as well as the lifecycle stage and dependency where the symptom is exposed.
then we analyze the root cause as well as the lifecycle stage and dependency where the root cause is introduced.
finally we explore the fix pattern and the knowledge sources that are used to fix it.
our findings from this study shed light on practical implications on dependency management.
ccs concepts software and its engineering software libraries and repositories .
keywords dependency bug deep learning stack empirical study acm reference format kaifeng huang bihuan chen susheng wu junming cao lei ma and xin peng.
.
demystifying dependency bugs in deep learning stack.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse k. huang b. chen s. wu j. cao and x. peng are with the school of computer science and shanghai key laboratory of data science fudan university china.
b. chen is the corresponding author.
l. ma is also with university of alberta canada.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
san francisco ca usa.
acm new york ny usa pages.
introduction the significant breakthroughs in deep learning dl have brought great success to many dl enabled applications e.g.
machine translation medical diagnosis voice assistants and autonomous vehicles .
such dl applications are built upon a heterogeneous and complex dl stack including hardware e.g.
nvidia gpu os e.g.
linux drivers e.g.
cuda and cudnn runtime e.g.
python and libraries e.g.
tensorflow .
in other words engineering dl applications requires software and hardware in the dl stack as prerequisite dependencies.
one common challenge in engineering dl applications is dependency management across the dl stack i.e.
to properly manage versions and configurations of the software and hardware dependencies in the entire dl stack.
motivation.
dependency management is challenging for three main reasons.
first software and hardware dependencies are complex and evolve quickly in an asynchronous and radical way.
dependency complexity originates from two sources i.e.
deep stack and rich vendors.
for example many vendors provide dl libraries e.g.
google s tensorflow facebook s pytorch and microsoft s cntk.
besides dependency evolution is performed at the vendor s own pace and may introduce incompatible changes.
for example the micro architecture of nvidia gpu has evolved several generations over the years from old versions such as tesla to new versions such as ampere.
in the meantime cuda has evolved from version .
to .
to support different gpus distinguished by compute capability which ranges from .
to .
.
therefore developers may miss some dependencies and build an incomplete stack or have troubles in selecting updating and migrating dependency versions.
second software and hardware dependencies need to satisfy complex version constraints to work together properly.
for example each tensorflow version only works compatibly with certain cudnn versions cuda versions and nvidia gpu versions.
a developer set up an environment with tensorflow gpu version .
.0rc0 python .
.
cuda .
.
cudnn .
and a gpu card with compute capability .
on windows .
the setup failed to recognize a valid gpu card as this tensorflow version required a gpu card with compute capability .
or higher.
these version constraints are scattered across documentations of software and hardware.
therefore developers may build an incompatible stack or introduce incompatibilities when updating versions or deploying to a new environment.
third each dependency version may contain bugs or need proper configuration.
while dependency version constraints are satisfied there might be bugs in specific versions under certain circumstances.
forarxiv .10347v2 sep 2023esec fse december san francisco ca usa k. huang et al.
example a developer created a seq2seq model using tensorflow .
but encountered an error .
it was caused by a bug only in tensorflow .
and could be alleviated by upgrading to .
or downgrading to .
.
in addition there might be misconfigurations during the installation of dependencies.
for example some kernel modules are required to be signed on secure boot enabled systems when the nvidia driver is installed.
however this may cause unknown errors raised from cuda which could be fixed by disabling secure boot.
therefore developers might use a buggy dependency version or misconfigure a dependency version.
in summary developers may introduce various dependency management problems in selecting using and maintaining dependencies in the dl stack during the entire engineering lifecycle i.e.
environment setup development deployment and maintenance .
we refer to these problems as dependency bugs dbs .
literature.
on the one hand a lot of advances have been made to investigate dbs in different ecosystems e.g.
java c c javascript python go and debian and red hat .
they only consider dbs at the homogeneous library layer.
however dbs in the dl ecosystem are different because they can occur across all the heterogeneous layers in the dl stack.
on the other hand a lot of efforts have been made to investigate characteristics e.g symptoms root causes and fix patterns of general bugs and specific bugs in dl applications.
however these studies are not specifically designed for dbs and thus only uncover partial characteristics of dbs in dl stack.
therefore although it is necessary to understand the characteristics of dbs in dl stack no systematic study exists yet.
our study.
to bridge this gap we present the first comprehensive study to characterize dbs in dl stack.
an overview of our study is presented in fig.
.
after introducing the dl stack see sec.
we first collect dbs from stackoverflow posts and github issues and then analyze these dbs to answer three rqs see sec.
.
rq1 symptom what are the symptoms of dbs?
at which lifecycle stages and dependencies are they exposed?
rq2 root cause what are the root causes of dbs?
at which lifecycle stages and dependencies are they introduced?
rq3 fix pattern what are the fix patterns of dbs?
which knowledge sources are used to fix dbs?
through these research questions we aim to provide useful findings for developers and researchers see sec.
and .
for example .
of the dbs manifest dl specific errors or anomalies in software and hardware dependencies behavior model and data mostly leading to crashes.
violation of constraints among software and hardware dependencies causes .
of the dbs.
development is the most bug affecting lifecycle stage which exposes .
of the dbs while environment setup is the most bug prone lifecycle stage which introduces .
of the dbs.
.
of the dbs are not introduced and exposed in the same dependency.
changing dependency version and adding dependency are the most common fix patterns which are leveraged to fix .
and .
of the dbs.
source code documentation issue tracker and other online resource are important knowledge sources of fixing dbs.
our findings provide practical implications for developers and researchers on dependency management across the entire engineering lifecycle see sec.
e.g.
construct dependency knowledgegraph for the entire dl stack recommend dependencies in the entire dl stack detect localize and fix dependency bugs and upgrade and migrate dependencies.
to demonstrate the usefulness of our implications we design a prototype of db detection and fixing.
in summary our work makes the following contributions.
we conduct the first comprehensive study to explore symptoms root causes and fix patterns of dbs in dl stack.
we provide implications for developers and researchers on dependency management in engineering dl applications.
deep learning stack developers need to set up a dl environment before developing or deploying dl applications.
the setup process often involves the following steps.
first developers need to choose a physical machine with gpus and operating system installed.
besides developers can use a virtual machine on the physical machine or choose a virtual machine on the cloud supported by cloud service providers e.g.
amazon sagemaker .
second to fully empower upper libraries and dl applications developers need to install the corresponding gpu drivers and gpu accelerated sdks e.g.
cuda and cudnn .
third developers need to select a runtime environment based on the programming language that dl applications are developed with e.g.
python and java .
forth a number of libraries should be leveraged to boost the development of dl applications from different perspectives.
finally developers could develop and deploy dl applications on top of the software and hardware dependencies.
this setup process is complicated by involving a wide scope of software and hardware dependencies.
to reduce the complexity and provide a complete solution a dl stack is proposed by organizing dependencies into layers.
for example patterson shows a generic program stack consisting of modeling code framework storage driver operating system and hardware .
by following the setup process and referencing the dl stack at patterson consulting intel huawei and nvidia we summarize a dl stack in fig.
.
it consists of five layers.
from top to bottom they are application library runtime anddriver os container and hardware .
specifically the application layer contains dl applications from various domains e.g.
autonomous driving.
the library layer contains the dependencies the upper layer dl applications directly or transitively depend on.
it covers a wide range of libraries including frameworks e.g.
tensorflow pytorch and cntk which provide abstraction and generic functionality implementation for dl algorithms front end libraries providing high level abstraction or language bindings e.g.
keras ktrain and neupy and other libraries in the ecosystem.
the runtime layer includes interpreters for dynamically typed languages e.g.
python and javascript and virtual machines for statically typed languages e.g.
java and .net .
the driver layer contains the dependencies for interacting with gpus including gpu drivers computing platforms and gpu accelerated sdks e.g.
nvidia gpu driver cuda and cudnn .
the library layer can directly interact with the runtime anddriver layer and thus they are put at the same layer.
the os container layer contains operating systems containers and other virtual environments e.g.
ubuntu windows macos docker and amazon sagemaker .
the hardware layer contains fundamental hardware like cpu gpu mobile chips and vendor specific chips e.g.
google s tpu .demystifying dependency bugs in deep learning stack esec fse december san francisco ca usa study method sec.
research questions sec.
and implications sec.
linux hardwareos containerruntime driverlibraryapplication ... ... deep learning stack sec.
dependency bug collection dependencybuglabelingdependency bugs dependency recommendationdependency bug detection fixingso posts facialregonition autodriving recommend.
system nlp objectdetecion ... ... ... ... rq1 symptoms of dependency bugsrq2 root causes of dependency bugsrq3 fix patterns of dependency bugs dependency knowledgegraphconstructiondependency upgrade migrationenvironment setupdevelopmentdeploymentmaintenance github issues figure an overview of our empirical study on dependency bugs in dl stack empirical study methodology we first introduce the design of our empirical study and then present our process of data collection and data labeling.
.
study design our symptom analysis inrq1 aims to characterize the observable consequences of dbs which is helpful to assess impacts and provide insights for db diagnosis and detection.
moreover it aims to identify the lifecycle stage and dependency where the symptom is exposed which is helpful to guide both developers and researchers to focus more effort on these bug affecting stages and dependencies so as to achieve the most benefit for db diagnosis and detection.
our root cause analysis inrq2 seeks to understand the fundamental nature of dbs which is helpful to provide insights for db detection and localization.
further it seeks to locate the lifecycle stage and dependency where the root cause is introduced which is helpful to guide both developers and researchers to spend more effort on these bug prone stages and dependencies in order to achieve the most benefit for db avoidance detection and localization.
our fix pattern analysis inrq3 attempts to characterize the fixes of dbs which is helpful to provide insights for db fixing.
moreover it explores the distribution of fix patterns for root causes as well as the knowledge sources that are used to fix dbs which is helpful for both developers and researchers to achieve db fixing in a more automated and effective fashion.
comparison to dbs in other domains.
unlike general programming dbs in deep learning exhibits a higher prevalence of lowlevel issues e.g.
driver configuration problems.
to the best of our knowledge there is no literature on dbs in high performance computing or platform specific binaries which also encounter configuration problems that may be as prevalent as those found in deep learning.
the existing literature covers a range of topics related to dependencies including empirical studies on dependency smells dependency conflicts and dependency related build failures .
during the analysis of rq1 rq2 andrq3 we compare the symptoms root causes and fix patterns and discuss the differences from existing literature.
.
data collection to obtain a comprehensive understanding of dbs we collect relevant posts on stackoverflow and relevant issues on github.
we selected stackoverflow and github because i they are popularsites containing a wide range of problems raised by world wide developers in real life development activities and ii they have a high potential to contain problems about the dependencies in the entire dl stack due to their diversity.
.
.
collecting so posts.
our collection of so posts has two steps.
step dependency tag selection.
developers often attach several tags to a post to indicate the topics or concepts related to the question.
therefore tags can be used to select the posts that are relevant to dependency problems in dl stack and we need to determine a set of tags that have a high coverage of the dependencies in dl stack.
to this end we first collected all the posts from stack exchange data dump on december .
then for each post with an accepted answer we iterated its tag list and searched for tags that co occurred with the tag deep learning or neural network .
in this way we obtained an initial set of tags.
we did not directly use the tag deep learning or neural network to select posts as it may miss posts that were not tagged with deep learning and neural network but with other dependency related tags.
next two of the authors independently determined whether each of the tags was related to the dependencies in dl stack by reading the excerpt provided by stackoverflow and online materials obtained by search engines.
we used cohen s kappa coefficient to measure agreement and it reached .
.
a third author was involved to resolve disagreements.
finally we obtained library tags drivertags runtime tags os container tags and hardware tags.
moreover we conducted a comprehensive analysis of these tags on significance and relevance scores following previous work .
out of these tags of them have non zero scores in terms of both significance and relevance while the remaining tags have zero scores.
these tags exhibit an average significance score of .
and an average relevance score of .
.
compared with the thresholds used in previous work our results suggest that our set of dl stack tags is significant and relevant.
step dependency post selection.
we picked dependencyrelated posts in two steps.
first we chose from the posts the ones whose tags contained one of the library tags and drivertags or contained the tag deep learning or neural network as well as one of the runtime tags os container tags and hardware tags.
as runtime os container andhardware tags often have a weaker correlation with dl than library anddriver tags here we enforced their co occurrence with either deep learning or neural network to reduce noisy posts.
this led to posts.esec fse december san francisco ca usa k. huang et al.
second to focus on high quality posts we removed posts that did not have an accepted answer and posts that did not contain dependency version information.
the information of dependency versions was considered as important to determine root causes and fix patterns of dbs.
we used regular expression matching to check the existence of version information.
this restricted our selection to posts.
.
.
collecting github issues.
our collection of github issues consists of two steps.
step github repsitory selection.
to obtain dependencyrelated issues we need to select a set of repositories across the dl stack.
however github mainly hosts repositories at the application andlibrary layer.
therefore we first searched the library tags in github which linked to github repositories.
the repository size is smaller than the tag size as i some tags share the same repository ii some repositories are archived and iii some libraries are not hosted on github.
then we selected the top repositories in the application layer by querying github using deep learning .
step dependency issue collection.
we collected closed issues in the selected repositories using github api which led to issues.
similar to dependency post selection we used regular expression matching to check the existence of version information in issues which resulted in issues.
as the issue size is still large we randomly sampled issues with a confidence level of and a margin of error of .
.
.
db identification.
we manually verified the posts and issues to reduce noise that was not about dbs in dl stack.
in particular two of the authors independently investigated each post and issue to identify dbs.
the cohen s kappa coefficient was .
.
.
a third author was involved to resolve disagreements.
finally we identified dbs.
are from posts and are from issues.
.
data labeling to answer the three research questions we manually labeled each of the dbs with respect to eight aspects i.e.
symptom exposing stage and dependency root cause introducing stage and dependency fix pattern and knowledge source for fixing.
in particular two of the authors first randomly sampled dbs for a pilot labeling following an open coding procedure .
they separately read all contents of a post or issue including title question issue description comments answers commits and reference links mentioned during discussion and relied on search engines to carefully label dbs.
basically the symptom of a db was determined by analyzing the question issue description.
the root cause fix pattern and knowledge source for fixing of a db were inferred from the question issue description the fixing commit or the accepted answer.
the exposing stage and dependency of a db were determined by analyzing where its symptom was exhibited while the introducing stage and dependency of a db were determined by analyzing where its root cause was located.
a group discussion was conducted to summarize the initial taxonomies.
then two of the authors independently labeled all the posts based on the initial taxonomies and finally reached cohen s kappa coefficients of .
.
.
.
.
.
.
and .
for the eight aspects.
a third author resolved disagreements in pilot dbsymptomsyntactic error dl specific error anomaly warning termination performance anomaly behavior anomaly data anomaly software error anomaly model error long execution time memory anomaly processor anomaly hardware error anomaly figure taxonomy of db symptoms and final labeling.
the manual effort involved in our data collection and labeling required eight person months.
rq1 symptom analysis we present the taxonomy of db symptoms and explore the stages and dependencies where symptoms are exposed.
.
symptom taxonomy the taxonomy of db symptoms is reported in fig.
.
it is organized into five inner categories i.e.
syntactic error dl specific error anomaly performance anomaly termination andwarning and eight leaf categories.
the number in parentheses is the number of dbs exhibiting the corresponding symptom.
syntactic error.
.
of the dbs exhibit general syntactic errors that are similar to those in traditional programs.
it is the most common symptom.
specifically .
of the dbs manifest element not found errors i.e.
the used syntactic elements like module class function key and attribute cannot be retrieved.
further .
of the dbs exhibit type mismatch errors i.e.
the variable type is inconsistent with the one that is expected.
in addition .
and .
of the dbs result in illegal value and illegal argument errors respectively where a variable receives an illegal value and a function call receives an illegal argument.
moreover .
of the dbs report undefined variable errors denoting that the variable is not defined or initialized.
besides some infrequent errors e.g.
compilation errors are included in the others category which account for .
of the dbs.
dl specific error anomaly.
.
of the dbs exhibit dl specific errors or anomalies.
it is the second most common symptom and is divided into five leaf categories.
software error anomaly means errors or anomalies raised by software dependencies accounting for .
of the dbs.
there are four cases.
.
of the dbs exhibit software internal errors indicated by an error message that contains the software name e.g.
cuda error unknown.
.
of the dbs report that required software dependencies cannot be found.
.
of the dbs manifest dependency initialization failures indicating that required dependencies are not properly set up.
.
of the dbs report that required software dependency versions do not match.
moreover hardware error anomaly denotes errors or anomalies raised by hardware dependencies e.g.
the gpu card is not correctly connected.
it accounts for .
of the dbs.
further .
of the dbs manifest behavior anomaly e.g.
abnormal accuracy metricsdemystifying dependency bugs in deep learning stack esec fse december san francisco ca usa and unexpected return values of apis.
in addition .
of the dbs exhibit model error which is indicated by an error message that contains model elements e.g.
computation operator missing model save load failure tensor conversion error and layer unrecognized.
besides .
of the dbs manifest data anomaly reporting that input data has abnormal values or mismatched property e.g.
size .
performance anomaly.
.
of the dbs manifest abnormal performance with respect to execution time memory usage and processor usage.
specifically .
of the dbs exhibit long execution time i.e.
a program takes a long time to initialize or execute dl tasks or even hangs in the middle of the execution.
further .
of the dbs cause memory anomaly including abnormal memory utilization memory leak or even out of memory errors.
besides two dbs result in processor anomaly i.e.
high gpu utilization .
termination.
.
of the dbs caused the program directly terminated without any informative error code or error message.
for example it only reports a segmentation fault or it simply reports that the task is killed or canceled.
warning.
.
of the dbs show warning messages including warnings about function change version compatibility and semantic mismatch in api arguments.
for example a version compatibility warning reveals that the installed version violates the working version requirements.
these warnings forecast the potential dbs due to using versions with changed elements.
comparison to dbs in other domains.
compared to previous work distinct symptoms of the dbs in our study are highlighted in dotted rectangles in fig.
which include hardware error anomaly model error data anomaly andperformance anomaly .
they account for .
of the dbs.
these differences owe to the fact that previous work is focused on dbs raised in homogeneous dependencies in the application andlibrary layer in traditional software applications while dbs across heterogeneous dependencies are not studied.
our study investigates dbs across the whole dl stack to collect symptoms revealed not only in dependencies within one layer but also in dependencies across layers.
summary.
general syntactic errors and dl specific errors and anomalies are the most common symptoms which account for .
of the dbs and mostly cause crashes.
besides .
of the dbs slow executions down or consume high resources.
these wide ranging impacts motivate the importance of dbs.
.
exposing stage and dependency we identify the stage and dependency where the symptom of each db is exposed and analyze db distribution over them.
exposing stage analysis.
we classify the entire lifecycle of engineering dl applications into four stages i.e.
environment setup development deployment and maintenance.
we report the db distribution over the exposing stages in the right part of fig.
.
development is the most bug affecting stage where .
of the dbs are exposed.
this indicates that although the setup process of dl stack is presumably finished more than half of the dbs will not occur until dl application development.
environment setup is the second most bug affecting stage where .
of the dbs are exposed.
it indicates that the setup of a feasible dl stack is not easy.
apart from the two dominating stages deployment exposes figure exposing dependency vs. exposing stage .
and maintenance exposes .
of the dbs which are relatively smaller than in environment setup and development.
the remaining .
dbs have no clear indication about the exposing stage and thus are included in the unknown category.
summary.
the most bug affecting stages are development and environment setup exposing .
and .
of the dbs.
exposing dependency analysis.
we show the db distribution over the exposing dependencies in the left part of fig.
which is organized by the layer hierarchy in dl stack see sec.
with dominating dependencies separately highlighted.
the library layer is the most bug affecting layer where .
of the dbs are exposed.
specifically keras tensorflow and pytorch in the library layer expose .
.
and .
of the dbs respectively which are the most bug affecting libraries.
this is reasonable as they are currently the most popular dl frameworks.
the application layer exposes .
of the dbs while thedriver layer exposes .
of the dbs.
cuda and cudnn both expose .
of the dbs.
besides there are at most .
of the dbs that are exposed at the dependencies at the runtime os container orhardware layer.
the sankey diagram in fig.
illustrates where the dbs exposed in a dependency are exposed across the lifecycle stages.
the width of the flow is proportional to the number of dbs.
generally a db can be exposed at any dependency at any layer in dl stack at any stage of the engineering lifecycle.
this indicates the complexity of dbs.
summary.
library application anddriver are the most bugaffecting stack layers.
keras tensorflow and pytorch are the most bug affecting libraries.
rq2 root cause analysis we report the taxonomy of db root causes and analyze the stages and dependencies where root causes are introduced.
.
root cause taxonomy the taxonomy of db root causes is shown in fig.
.
we first classify the root causes based on the criterion that whether a db is caused by one dependency i.e.
intra dependency cause or by constraints among dependencies across dl stack i.e.
inter dependency cause .
then we summarize six leaf categories.esec fse december san francisco ca usa k. huang et al.
db root cause intra dependency causes inter dependency causes unsuccessful installation mismatched software mismatched hardware buggy software version disabled os privilege incompatible software version figure taxonomy of db root causes intra dependency cause.
.
of the dbs are caused solely by one dependency itself and are divided into two leaf categories.
particularly .
of the dbs are caused by buggy software version i.e.
a db is caused by triggering bugs in software dependencies in dl stack.
moreover .
of the dbs are caused by unsuccessful installation of dependencies.
there are two cases.
dependency installation does not complete.
for example a developer found that there was no file named cudnn64 6.dll which was caused by the missed installation of cudnn on his her machine .
dependency installation completes but lacks proper path configuration e.g.
missing path configuration or configuring incorrect path .
inter dependency cause.
.
of the dbs are caused by constraints among software and hardware dependencies across dl stack i.e.
multiple dependencies have to be considered together to have a feasible dl stack otherwise dbs might be introduced.
it is divided into four leaf categories.
specifically .
of the dbs are caused by incompatible software version which is the most common root cause.
an incompatible software version is introduced if it violates the version constraint that has to be satisfied for it to work with other dependencies.
for of the dbs detailed api level incompatibility information is provided in the post issue and we classify the incompatibility based on api changes i.e.
api removal api addition api replacement api movement api parameter list change api renaming and api behavior change.
api addition api behavior change and api removal are the most common root causes of api incompatibility which respectively account for at least .
.
and .
of the dbs.
api replacement api parameter list change api movement and api renaming respectively cause at least and dbs.
for the of the dbs we can only distinguish whether they are caused by backward incompatibility for .
of the dbs or forward incompatibility for .
of the dbs .
for the remaining of the dbs we can only determine they are caused by incompatibility due to the limited information in the posts issues.
.
of the dbs are caused by mismatched software i.e.
while different software can provide similar functionalities only some of them can work with the other dependencies in dl stack but others are regarded as mismatched.
specifically of the dbs are caused by selecting wrong software as dependency.
for example the dl framework keras and the tf.keras module introduced in tensorflow .
provide similar apis but keras does not support tensorflow .
.
in that sense if tensorflow .
is used in dl stack keras would be mismatched and thus cannot be used .
further of the dbs are caused by choosing wrong software distribution.
for example the official pre built tensorflow .
requires cuda toolkit .
.
developers have to re build tensorflow .
with cuda toolkit .
to work with cuda toolkit .
.
thus using pre built tensorflow .
with figure introducing dependency vs. introducing stage cuda toolkit .
could cause a db .
further of the dbs are caused by selecting multiple conflicting software.
for example loading both tensorflow and tensorflow gpu would cause a db.
.
of the dbs are caused by mismatched hardware i.e.
the hardware does not meet requirements of dependencies in upper stack layers.
for example tensorflow .
used avx feature of cpus which is supported by sandy bridge or newer cpu architectures.
hence using tensorflow with non avx cpus would cause a db .
.
of the dbs are caused by disabled os privilege i.e.
permissions required by software dependencies are not allowed from the os or container.
for example system integrity protection sip is enabled on macos .
to prevent unauthorized code execution but sip prevents a path variable from being overridden causing dependencies not found .
comparison to dbs in other domains.
compared to previous work distinct root causes of the dbs in our study are highlighted in dotted rectangles in fig.
which include mismatched hardware and disabled os privilege .
they account for .
of the dbs.
it is worth mentioning that although most root causes are shared with previous work the dependencies that cause dbs can be different see sec.
.
as our study further considers the runtime driver os container andhardware layers.
summary.
violation of constraints among dependencies in dl stack causes .
of the dbs where incompatible software version is the major root cause.
moreover bugs in software dependencies cause .
of the dbs.
.
introducing stage and dependency we locate the stage and dependency where the root cause of each db is introduced and analyze db distribution over them.
introducing stage analysis.
the taxonomy of stages is the same to the one in sec.
.
.
we show the db distribution over the introducing stages in the right part of fig.
.
environment setup is the most bug prone stage where .
of the dbs are introduced while no db is introduced in development because the dl stack is already determined in environment setup.
it indicates that the setup of a feasible dl stack is important but challenging.
besides deployment and maintenance introduce .
and .
of thedemystifying dependency bugs in deep learning stack esec fse december san francisco ca usa dbs.
the remaining .
dbs have no clear indication about the introducing stage and thus are put in the unknown category.
summary.
the most bug prone stage is environment setup which introduces .
of the dbs.
introducing dependency analysis.
for the dbs caused by inter dependency causes their root causes can be introduced by any of the involved dependencies.
for example a db is caused by version constraint violation between tensorflow and cuda and then both tensorflow and cuda can be the introducing dependency of this db.
if there is no clear indication about the introducing dependency in the posts issues we consider all involved dependencies as the introducing dependencies otherwise we use the introducing dependency that is decided in the posts issues.
it is worth mentioning that the introducing dependency of dbs are only mentioned in the answers of the posts issues indicating that questioners are not aware of the introducing dependency.
in of the dbs questioners only mention the dependency list to provide more detail but there is no clue indicating that they are aware of the introducing dependency.
in of the dbs questioners indicate assumptions on the introducing dependency.
specifically of the dbs that are caused by inter dependency causes dbs have their introducing dependencies clearly indicated in the posts issues.
we show the db distribution over the introducing dependencies in the left part of fig.
which is organized in the same way in fig.
.
no db is introduced in the application layer as it is the client of dependencies.
the library anddriver layers are the most bug prone layers respectively introducing .
and .
of the dbs.
the number is larger than the summation of dbs in all libraries or drivers because a db can have multiple introducing dependencies.
specifically keras tensorflow and pytorch introduce the most bugs in the library layer introducing .
.
and .
of the dbs.
cuda and cudnn introduce the most bugs in the driver layer introducing .
and .
of the dbs.
there are at most .
of the dbs introduced at the dependencies at the runtime os container orhardware layer.
besides the sankey diagram in fig.
shows where the dbs introduced in a dependency are introduced across the lifecycle stages.
generally a db can be introduced at any dependency at any layer except for application at any lifecycle stage except for development .
it reveals the complexity of db localization.
summary.
library anddriver are the most bug prone stack layers which introduce .
and .
of the dbs.
keras tensorflow and pytorch introduce the most bugs in thelibrary layer while cuda and cudnn introduce the most bugs in the driver layer.
.
introducing and exposing dependency we further analyze where the dbs introduced in a dependency are exposed across the dependencies in dl stack.
overall .
of the dbs are not introduced and exposed in the same dependency.
for example .
of the dbs introduced in tensorflow are exposed in keras and .
of the dbs introduced in cuda and cudnn are exposed in tensorflow.
at the stack layer level .
of the dbsare not introduced and exposed at the same stack layer.
for example .
of the dbs introduced at the hardware layer are exposed at the library layer.
these results indicate that db localization need systematic knowledge of the entire dl stack.
summary.
.
of the dbs are not introduced and exposed in the same dependency and .
of the dbs are not introduced and exposed at the same layer.
rq3 fix pattern analysis we present the taxonomy of db fix patterns and report their distribution for root causes and the knowledge source of fixing.
.
fix pattern taxonomy the taxonomy of db fix patterns is listed in fig.
.
it is grouped into four inner categories i.e.
change application code change dependency change dl stack andchange environment and leaf categories.
a db can be fixed by applying multiple fix patterns.
hence the summation of the number of dbs in fig.
is larger than .
change application code.
.
of the dbs are fixed via changing the application code although their root causes are not introduced by the application.
specifically fixing api usage is used to fix .
of the dbs i.e.
the library api usage has to be changed with the incompatible library version evolution.
moreover adding missing code logic is utilized to fix .
of the dbs.
in such cases some library apis are removed or the behavior of some library apis is changed and hence developers have to implement the code logic of these library apis by themselves at the application code level.
further reformatting data is used to fix .
of the dbs for making the data format compatible with the changed library apis.
besides changing hyper parameter e.g.
batch size and learning rate is used to fix .
of the dbs because the constraints on hyperparameters are changed with library version evolution.
change dependency.
this is the most common fix pattern which is leveraged to fix .
of the dbs.
in particular changing dependency version is used to fix .
of the dbs indicating that it is the most common pattern to fix dbs.
of these dbs upgrading dependency version is used in the fix of dbs and downgrading dependency version is used in the fix of dbs.
in of the dbs dependency version is changed but there is no clear indication in the posts issues to determine upgrade or downgrade.
further adding dependency is used to fix .
of the dbs where some required dependencies are missing or not successfully installed.
moreover re building dependency is used to fix .
of the dbs.
in such cases the source code of dependencies is re built with other required dependencies to properly work with them or the source code of dependencies is first changed e.g.
to fix bugs or to remove incompatibilities and then re built potentially because of the huge maintenance effort in changing dependency versions.
in addition changing dependency configuration is leveraged to fix .
of the dbs e.g.
disabling sip in macos.
besides removing dependency is applied to fix .
of the dbs in order to remove conflicted dependencies.
change dl stack.
.
of the dbs are fixed by changing the dl stack i.e.
some dependencies are switched to alternatives and the dl stack becomes fundamentally different.
it is divided intoesec fse december san francisco ca usa k. huang et al.
dbfix patternchange application code change dependency change environment change dl stack fix api usage reformat data change dependency version add dependency re build dependency fix pathvariable switch software clearenvironment switch os create environment change dependencyconfiguration change hyper parameter add missing code logic switch hardware remove dependency figure taxonomy of db fix patterns three leaf categories i.e.
switching software libraries drivers and runtimes switching hardware and switching os accounting for .
.
and .
of the dbs.
for example a developer switched os to ubuntu to support distributed tensorflow .
change environment.
.
of the dbs are fixed by changing the environment where libraries drivers and runtimes can be found.
specifically fixing path variable is used to fix .
of the dbs i.e.
the path variable is fixed to point to the correct directory that contains the required dependencies.
besides clearing environment andcreating environment are used to respectively fix .
and .
of the dbs.
in these cases the virtual environment i.e.
a directory that contains a specific collection of installed packages of package managers e.g.
pip and conda is cleared or created.
notice that .
of the dbs can be fixed by applying one fix pattern while .
.
and .
of the dbs can be fixed by combining two three and four fix patterns at the same time.
the summation here is larger than as dbs can be fixed by different combinations of fix patterns.
comparison to dbs in other domains.
compared to previous work distinct fix patterns of the dbs in our study are highlighted in dotted rectangles in fig.
which include reformat data change hyper parameter switch hardware switch os and change dependency configuration .
they are used to fix .
of the dbs.
moreover multiple fix patterns need to be combined to fix some dbs in dl stack which is not the case in fixing dependency conflicts where only one fix pattern is needed.
summary.
the most common fix pattern is to change dependency versions which is used to fix .
of the dbs.
adding dependency is the second most common pattern which is leveraged to fix .
of the dbs.
.
of the dbs can be fixed by combining multiple fix patterns.
.
distribution of fix patterns for root causes we report the distribution of fix patterns for root causes in fig.
where each cell denotes the number of dbs that are caused by a particular root cause and fixed by a particular fix pattern.
specifically except for switching software all fix patterns are utilized in fixing dbs that are caused by incompatible software version for at least once.
while fixing api usage andchanging dependency version are the two major fix patterns there exist diverse ways to fix the most common root cause incompatible software version .
the challenge is to decide which fix pattern to use given a db context.
fix api usageadd missing code logic reformat datachange hyper parameterchange dependency version add dependencyre build dependencychange dependency configuration remove dependency switch softwareswitch hardware switch osfix path variableclear environmentcreate environment buggy software version unsuccessful installation incompatible software version mismatched software mismatched hardware disabled os priviledge4 number of dbs figure distribution of fix patterns for root causes further changing dependency version is used in mitigating five root causes and is involved in the fix for .
of the dbs.
it has a strong correlation to the root causes buggy software version andincompatible software version .
while the fix pattern itself is very simple the key challenge is to determine which dependency version to use for addressing a db.
besides adding dependency rebuilding dependency andclearing environment are the other three fix patterns spanning at least four root causes.
notice that adding dependency is the accompanied fix pattern for fixing dbs caused by incompatible software version .
for example upgrading dependency version solves an incompatible software version but this upgraded dependency version may further depend on a new dependency.
summary.
incompatible software version can be fixed by diverse patterns whereas fixing api usage and changing dependency version are the two major fix patterns.
changing dependency version is also the major fix pattern for buggy software version .
.
knowledge source of db fixing to fix a db developers usually rely on knowledge about dl stack e.g.
dependency version constraints and dependency bugs.
to characterize how fixes of dbs are derived we investigate the knowledge sources that are used to fix dbs.
we identify five knowledge sources.
multiple knowledge sources can be used in fixing one db and hence the summation of the number of dbs below is larger than .
library source code.
.
of the dbs are fixed after digging into the source code of libraries.
the source code of libraries is a good knowledge source to know library version evolution e.g.
how a library api is renamed and how a library api s code logic is changed.demystifying dependency bugs in deep learning stack esec fse december san francisco ca usa dependency documentation.
.
of the dbs are fixed after looking into dependency documentation.
documentation of libraries drivers and hardware often provide informative knowledge about dependency s installation requirements and version constraints.
for example tensorflow documentation lists both the hardware requirements and software requirements .
issue tracker.
.
of the dbs are fixed after being aware of the dependency bugs.
such bugs are tracked on issue trackers with their symptoms and affected versions described.
other online resource .
.
of the dbs are fixed by referencing other online resources e.g.
mailing lists stackoverflow posts and technical blogs.
unknown.
for .
of the dbs there is no clear indication about the used knowledge source in the posts issues and thus we include them into the unknown category.
however such posts issues themselves become a knowledge source.
summary.
library source code dependency documentation issue tracker and other online resource are important knowledge sources that are directly leveraged to fix dbs.
7implication application and threat we discuss the implications of our study demonstrate an application and analyze the threats to our study.
.
implication to developers and researchers application developers.
our study uncovers the common db symptoms that developers should be aware of when engineering dl applications for detecting potential dbs as early as possible.
our study also identifies the common root causes and fix patterns of dbs that could be useful for application developers to diagnose localize and fix dbs.
our study also shows the most bug introducing and bug affecting dependencies where application developers should pay more attention when installing using or maintaining them so that most dbs could be avoided or detected at the first place.
moreover our findings provide some engineering suggestions.
application developers should be trained to have a comprehensive understanding of the dl stack as our study reports that a db could be introduced or exposed across the entire dl stack and engineering lifecycle.
in this way application developers are equipped with the sufficient knowledge to deal with dbs.
appilcation developers should carefully look into dependency documentation to learn version constraints and be aware of the bugs and api changes in library version evolution.
in this way dbs caused by the most common root causes i.e.
buggy software version andincompatible software version might be effectively reduced.
library developers.
our study reveals that around half of the dbs are not introduced and exposed in the same dependency.
this requires library developers to write informative error messages in exposing dependencies to help indicate the root causes in introducing dependencies.
in addition our study identifies incompatible software version as the common root cause and change dependency version as the common fix pattern for dbs.
this highlights the importance of providing precise version constraints by library developers to allow application developers to follow and thus prevent db occurrences.
furthermore if library developers integrate certainversion constraint checking in dependency installation scripts and provide potential version constraint violation hints for application developers it would eliminate dbs at the first place.
researchers.
our findings provide future research implications in four directions.
first a dependency knowledge graph for the entire dl stack is needed to provide fundamental knowledge for the ease of dependency management.
as uncovered by our root cause analysis a diversity of dependency knowledge is involved in dbs e.g.
version constraints among software and hardware dependencies bugs in dependencies and api changes in version evolution.
however such knowledge is scattered across different sources e.g.
documentation issue tracker and source code as revealed by our investigation of the knowledge source of db fixing.
online resources like stackoverflow posts and github issues also provide practical solutions to fix dbs.
hence the main challenges to construct the knowledge graph are that i designing a high level schema to fuse various knowledge into a graph ii leveraging various techniques like natural language processing and program analysis to automatically extract knowledge from different sources and keep them up to date and iii developing graph analysis techniques for various dependency management tasks.
this knowledge graph serves as the foundation of the following three research directions.
along this direction ye et al.
and cheng et al.
construct a knowledge graph for the library andruntime layers for general python programs but fail to support lower layers in the dl stack.
second dependency recommendation techniques are needed .
our introducing stage analysis reveals that environment setup is the most bug prone stage which introduces .
of the dbs.
therefore developers often face difficulties in setting up a feasible dl stack.
further our root cause analysis shows that .
of the dbs are caused byincompatible software version although dependency documentation provides prerequisite information about setting up dependencies and their version constraints.
therefore developers might not always refer to the documentation.
in that sense dependency recommendation techniques become useful for developers to ease the setup of a feasible dl stack i.e.
given some dependencies installed they recommend other dependencies to form a complete dl stack.
for example given the available hardware and os they suggest required dependencies in driver runtime andlibrary layers.
third db detection localization and fixing techniques are needed.
our study indicates that .
of the dbs are introduced in environment setup while only .
of the dbs are exposed in environment setup.
thus it may indicate that many dbs stay undetected until later lifecycle stages.
to detect or localize dbs as early as possible one possible remedy is to identify the dependencies currently adopted in the dl stack and then check against our dependency knowledge graph to detect potential dependency constraint violations.
here the challenge is to automatically identify all heterogeneous dependencies as well as their versions across the entire dl stack.
along this direction tan et al.
proposed a technique to identify homogeneous dependencies at the application andlibrary layer.
moreover as many dbs are caused by software bugs or api incompatibilities fine grained call graph analysis is needed to accurately detect and localize dbs i.e.
to decide whether such bugs or incompatible apis are in the execution path and thus can be triggered.
besides our study indicates that questioners are often unaware of the introducing dependencies of the dbs whichesec fse december san francisco ca usa k. huang et al.
calls for automated db localization techniques.
once a db is localized automated fixing techniques can use the fix patterns derived from our study to fix it.
however the challenge is to decide which fix pattern or combination of fix patterns is applicable and how a fix pattern is instantiated.
a way is to use search based approach by applying fix patterns to generate potential fixes and using the dependency knowledge graph to decide the fix fitness.
fourth dependency upgrading and migration techniques are needed.
our introducing stage analysis uncovers that some dbs are introduced in deployment and maintenance.
more specifically dl stack in deployment environment can be different from the one in development environment.
hence dependency migration techniques are needed to check whether dependencies in development environment can be replaced with the ones in deployment environment.
besides dependency versions can be upgraded for the benefit of fixed bugs and improved features.
however it may also introduce incompatibilities.
therefore dependency upgrading techniques are needed to analyze api changes and assess the risk in terms of potential dbs and the effort in terms of potential code adaptation.
.
application for usefulness demonstration to demonstrate the usefulness of our implications we design a prototype to automatically detect and fix dbs.
prototype design.
our prototype has one knowledge base i.e.
dependency constraint knowledge and two components i.e.
db detection anddb fixing .
to collect dependency constraint knowledge we target at the documentations of tensorflow pytorch and keras as i they expose and introduce the most dbs at the library layer and ii their documentations often list requirements for dependencies at lower layers e.g.
python at the runtime layer cuda and cudnn at the driver layer linux at the os container layer.
we then manually extract dependency constraints from their online documentations via reading installation guides of each version where they are either described in natural language or illustrated with a table.
each dependency constraint is denoted as a tuple depa dep b va vb1 vb2 where version vaof dependency depadepends on depbunder the condition that the version of depbis within the range of vb1 andvb2.vb2can be null to represent an opening scope.
overall we collect dependency constraints in days.
our prototype takes a docker image as an input and detects whether it contains a db.
if yes it also tries to fix it.
to detect dbs we first need to identify dependencies used in the dl stack.
to this end we support two package managers i.e.
pypi and conda at the library layer.
we obtain the virtual environment location of pypi virtualenv by find grep bin activate or the environment names of conda by conda env list .
then we activate the corresponding environments by source path bin activate or conda activate envname where we retrieve the whole list of dependency versions under each environment.
for the runtime layer we identify python runtime in path echo path where there exists an executable named python or python3 .
for the driver layer we use nvcc version to identify installed version of cuda and which nvcc to identify installed path of cuda.
under cuda path lib64 we find the cudnn version by checking if there exists a dynamic linking library of cudnn i.e.
cudnn.so.
version .
for the os container layer we use uname a cat etc centos release lsb release a etc.
to identify hosting os.
we do not identify hardware versions as the docker image does not contain hardware information.
then we search the identified dependency versions of each environment for a combination of dependencies depa dep b va vb that violates a dependency constraint depa dep b va vb1 vb2 which is regarded as a db.
we first anchor depaat version vaand change depb s version indicated in version constraint from vb1tovb2.
meanwhile as depb s version changes we also change depc s version to satisfy version constraint in depb dep c vb vc1 vc2 if needed.
the process is conducted recursively.
if there is no satisfied version combinations we anchor depa s version into a newer version of vausing our knowledge base and repeat the above process.
once a satisfied version is found we change the dependency version by running uninstall and install script to fix the db.
comparison with related tools.
we find and review three closely related tools.
dockerizeme is a tool to infer the dependencies needed to execute a python code snippet without import error.
the inference is based on a knowledge base which contains packages their versions and resources and the relationships between them.
the knowledge base is built by applying static and dynamic analysis to top ten thousand python packages on pypi and applying association rule mining to public github python projects.
pyego extends the knowledge base of dockerizeme by further including python interpreters and system libraries and achieves a better accuracy on inferring compatible dependencies.
dockerizeme and pyego mainly support packages installed by commands of pip and apt.
different from dockerizeme and pyego our prototype extracts dependencies and version constraints knowledge from official documentation and supports package installation commands beyond pipandapt.
while further work is needed to automate the knowledge extraction our approach can offer more generalizability across different types of dependencies at different dl stack layers.
different from the knowledge based inference in dockerizeme and pyego pydfix takes a trial and error approach i.e.
it first identifies dependency errors and possible dependencies causing the errors from build log and then iteratively re runs the build with intermediate patches until the error disappears.
differently our prototype does not rely on error logs since not all dbs indicate explicit error logs or reveal dependency names in their error logs.
effectiveness evaluation.
to evaluate our prototype we reproduce dbs from our study and export them as docker images.
we randomly sample dbs from our study and successfully reproduce dbs.
the reasons of unsuccessful reproduction are twofold.
first the exposing or introducing dependency of dbs locates inhardware oros container which does not match with our machines.
second only part of the dl stack is revealed in the posts or issues and hence we fail to derive the full dl stack to reproduce dbs.
our prototype successfully detects and fixes of the dbs.
three dbs caused by mismatched software two dbs caused by buggy software version and one db caused by unsuccessful installation are not detected as our prototype is focused on violated version constraints.
of the twelve dbs caused by incompatible software version five dbs are detected and fixed using version constraint between tensorflow and cuda two are detected and fixed using version constraint between cuda and cudnn and one is detected and fixed using version constraint between tensorflow and cudnn.
the other four dbs are not detected because the root cause dependencies are not indemystifying dependency bugs in deep learning stack esec fse december san francisco ca usa db1db2db3db4db5db6db7db8 uni00000013 uni00000013 uni00000011 uni00000018 uni00000014 uni00000011 uni00000013 uni00000014 uni00000011 uni00000018 uni00000015 uni00000011 uni00000013 uni00000034 uni00000058 uni00000044 uni0000004f uni0000004c uni00000057 uni0000005c uni00000003 uni0000000b uni00000006 uni0000000c uni0000002b uni00000058 uni00000050 uni00000044 uni00000051 uni00000003 uni00000034 uni00000058 uni00000044 uni0000004f uni0000004c uni00000057 uni0000005c uni00000033 uni00000055 uni00000052 uni00000057 uni00000052 uni00000057 uni0000005c uni00000053 uni00000048 uni00000003 uni00000034 uni00000058 uni00000044 uni0000004f uni0000004c uni00000057 uni0000005c uni0000002b uni00000058 uni00000050 uni00000044 uni00000051 uni00000003 uni00000037 uni0000004c uni00000050 uni00000048 uni00000033 uni00000055 uni00000052 uni00000057 uni00000052 uni00000057 uni0000005c uni00000053 uni00000048 uni00000003 uni00000037 uni0000004c uni00000050 uni00000048 uni00000013 uni00000014 uni00000013 uni00000013 uni00000015 uni00000013 uni00000013 uni00000016 uni00000013 uni00000013 uni00000017 uni00000013 uni00000013 uni00000018 uni00000013 uni00000013 uni00000019 uni00000013 uni00000013 uni0000001a uni00000013 uni00000013 uni0000001b uni00000013 uni00000013 uni0000001c uni00000013 uni00000013 uni00000014 uni00000013 uni00000013 uni00000013 uni00000014 uni00000014 uni00000013 uni00000013 uni00000014 uni00000015 uni00000013 uni00000013 uni00000014 uni00000016 uni00000013 uni00000013 uni00000014 uni00000017 uni00000013 uni00000013 uni00000014 uni00000018 uni00000013 uni00000013 uni00000014 uni00000019 uni00000013 uni00000013 uni00000037 uni0000004c uni00000050 uni00000048 uni00000056 uni00000003 uni0000000b uni00000056 uni00000048 uni00000046 uni00000052 uni00000051 uni00000047 uni00000056 uni0000000c figure results of the quality and time of fixing dbs our scope of dependency constraint knowledge acquisition.
these results demonstrate the potential of our prototype.
moreover we try to apply pyego and pydfix to fix the dbs.
notice that dockerizeme is not selected because pyego has achieved better performance than it.
we successfully run pyego against the dbs.
it successfully detects and fixes only one db.
it successfully detects dbs but generates wrong version recommendation on all of them.
besides it fails to detect the rest dbs.
unfortunately we fail to launch pydfix due to the limited setup documentation.
however pydfix relies on analyzing error logs to fix dbs.
consequently we can conclude that pydfix are unable to fix at least of the dbs since these dbs produce normal outputs instead of error logs.
these results indicate the potential of our prototype.
human study.
we observe from our effectiveness evaluation that our prototype takes seconds for the dbs that are not successfully fixed and these dbs are even not detected by our prototype.
in other words it takes negligible time for our prototype to determine whether a given db is out of the scope of our prototype.
therefore we are interested to investigate how much effort can be saved for developers for the dbs that are in the scope of our prototype.
to this end we conduct a human study with participants to manually fix the dbs that can be automatically fixed by our prototype.
the participants are recruited voluntarily at our college who are familiar with linux shell and packages and has sufficient background in deep learning.
four participants have worked on at least one or two research projects that employ dl techniques and the other four participants have hands on experience with open source dl projects.
the tasks are reproduced dbs in a docker environment where the error trace of each db could be invoked via a command i.e.
python script.py .
the participants are told that the error is caused by a db and they are required to locate and fix the dbs with their expertise and any online resources.
the order of the tasks are randomized for each participant to avoid bias.
we use two indicators to compare participants manual fixes and our automated fixes.
the first indicator is the quality of the fix in each task.
we use to indicate a successful and perfect fix to indicate a successful but imperfect fix and to indicate an unsuccessful fix.
the success of the fix is judged by the dismissing of the db s errors when re launching scripts.
the perfection and imperfection of the fix is judged by two of the authors on whether the fix steps would have any side effect.
after the discussion and mutual agreement from two of the authors a final quality is resolved.
the second indicator is the consumed time on finishing each task.
fig.
shows the result of fix quality and time.
in terms of quality all participants obtain full score in dbs.
the rest dbs are notfixed successfully and perfectly by all.
participant db pairs are not fully scored.
specifically we assign to participant db pairs.
of these participant db pairs participant db pairs fix a db by using soft links to redirect the incorrect dependency into a correct dependency and participant db pairs fix a db by replacing dynamic linked libraries i.e.
change a correct dependency s file name into the original one using mv .
they are imperfect because such tricks are unstable and confuse other users.
the rest participantdb pairs freshly reinstall tensorflow using an up to dated version.
we assign them to because setting up a new environment carries the risk of disrupting the initial environment making it impractical when there are multiple users and applications.
we also assign to participant db pairs.
they fail to fix as it still has the reported error.
in terms of time none of the manual fix from participant db pairs surpasses our prototype.
the manual fix takes averagely .
times longer than our prototype.
generally our prototype achieves a higher quality of against the human group with a score of .
and costs averagely .
seconds against the human group with averagely .
seconds.
therefore our prototype can be useful for developers to provide high quality fix and greatly saving fixing time.
related work dependency bugs.
dependency bugs have been explored for different ecosystems e.g.
debian and red hat javascript java python c c and go .
to the best of our knowledge our work is the first to systematically investigate dependency bugs in dl ecosystem.
deep learning bugs.
empirical studies have been conducted to characterize bugs in dl systems.
some are focused on a general scope of bugs and others are focused on a specific type of bugs .
these studies uncover partial characteristics of dependency bugs in dl stack.
there lacks a comprehensive study to characterize dependency bugs in dl stack and our work fills this gap.
several advances have also been made to detect dl bugs e.g.
numerical bugs and shape bugs .
however little attention has been received to detecting dependency bugs in dl stack and our work sheds light on it.
empirical studies about dl.
many studies have empirically investigated various aspects in developing deploying and maintaining dl systems and dl frameworks .
these studies motivate the importance of dependency management.
for example incompatible dependency installation or environment setup is recognized as a common challenge .
however they lack an in depth analysis of the characteristics.
our work is inspired by them to systematically characterize dependency bugs across the dl stack.
conclusions we have conducted the first comprehensive study to characterize dbs across the entire dl stack.
we provide useful findings to raise the awareness of dbs in dl stack in the dl community and provide actionable implications for developers and researches.
data availability the data of our study is available at december san francisco ca usa k. huang et al.