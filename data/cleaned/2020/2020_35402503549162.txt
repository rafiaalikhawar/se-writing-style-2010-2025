natgen generativepre trainingby naturalizing source code saikatchakraborty columbia university new york ny usa saikatc cs.columbia.edutoufiqueahmed universityof california davis davis ca usa tfahmed ucdavis.eduyangruiboding columbia university new york ny usa yrbding cs.columbia.edu premkumart.devanbu universityof california davis davis ca usa ptdevanbu ucdavis.edubaishakhi ray columbia university new york ny usa rayb cs.columbia.edu abstract pre trained generative language models e.g.
plbart codet5 spt code forsourcecodeyieldedstrongresultsonseveraltasks in the past few years including code generation and translation.
thesemodelshaveadoptedvaryingpre trainingobjectivestolearn statistics of code construction from very large scale corpora in a self supervisedfashion thesuccessofpre trainedmodelslargely hinges on these pre training objectives.
this paper proposes a new pre training objective naturalizing of source code exploiting code sbimodal dual channel formal naturalchannels nature.
unlike natural language code s bimodal dual channel nature allows us to generate semantically equivalent code at scale.
we introduce six classes of semantic preserving transformations to introduce un natural forms of code and then force our model to produce more natural original programs written by developers.
learning to generate equivalent but more natural code at scale over large corpora of open source code without explicit manual supervision helps the model learn to both ingest generate code.
we fine tuneour modelinthree generative software engineering tasks code generation code translation and code refinement with limited human curated labeled data and achieve state of the art performance rivaling codet5.
we show that ourpre trained model isespeciallycompetitiveatzero shotandfew shotlearning and betterat learningcode properties e.g.
syntax data flow .
ccs concepts softwareanditsengineering languagefeatures computing methodologies knowledge representation and reasoning.
keywords source code pre training neural network source code transformer semantic preservingtransformation permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 14 18 singapore singapore associationfor computing machinery.
acm isbn ... .
format saikatchakraborty toufiqueahmed yangruiboding premkumart.
devanbu and baishakhi ray.
.
natgen generative pre training by naturalizing source code.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of softwareengineering esec fse november14 18 singapore singapore.
acm newyork ny usa 13pages.
introduction statistical models of the naturalness of code have proven useful for a range of software engineering tasks including code generation repair summarization retrieval andclonedetection .theearlierworkinthis area trained models directly on tasks including the early work on type recovery de obfuscation repair and summarization .
training on task requires a lot of labeled data.
while labeled data is abundant for tasks like code completion where the corpus inherently provides supervision other tasks like code generation translation summarization repair etc.
requirewell curated high qualitydata.simplygrabbingdatafrom github might yield poor quality highly duplicated data .
withincreasingmodelcapacity hundredsofmillions evenbillions ofparameters are pretty common largermodels tend to perform better this unacceptable disparity between vast model capacityandthelimitedavailabilityofwell curated high quality labeleddata has increasedandwilllikely worsen.
thisshortageofhigh qualitylabeleddataforon tasktrainingis notuniquetosoftwareengineering se althoughitiscomplicated herebytheincreased specializedskillrequiredforlabelingsedata.
toaddresstheissueoftraininglargemodelsinthepresenceofdata scarcity suchmodelsareoftenpre trainedonsomegenerictasks whichrelatetoactualdownstreamtasks.forexample considertwo se tasks code generationandcode translation.both tasksrequire mlmodels tolearnhowtogeneratenatural syntactically andsemantically correct code.
this commonality across tasks motivates aquestforbetterpre trainedmodels usingaself orun supervised task which transfers well to other downstream tasks.
such pre trainedmodelscanalsolearnagenericrepresentationofthe inputdata which inturn transfers to diversedownstream tasks.
a popular approach for dealing with this problem involves derivativesofbertstylemodels e.g.
codebert graphcodebert etc.thesemodelsaregoodatcapturinggenericcode representations.forcodegenerationtasks gpt 3orbart style models e.g.
codex codet5 plbart sptcode etc.
esec fse november14 18 singapore singapore s.chakraborty t. ahmed y. ding p. devanbu b.ray arepopular.theimportantinsighthereisthatindependentoffinaltasks when veryhighcapacitymodelsaretrainedwithhuge code corpora to learn simple self supervised busy work they stilllearn general syntactic and semantic constraints of writing code .
different approaches adopt different techniques to train the model to write code.
for instance gpt style models e.g.
codex learn to generate code sequentially mimicking the left to right language model.
codet5masks out some tokens andasks the model to generateonlythosemaskedtokens.ontheotherhand plbartand spt code present the model with erroneous code with deleted or masked tokens and ask the model to generate the corrected completecode.themodels abilitytogeneratecodedependsmainly onthe pre training objective that the modelisoptimizedfor.
we propose a novel pre training task we ask the model to naturalize code i.e.
take weird syntheticcodeasinputand output semantic equivalent natural code that a human developer would havewritten.thisisaverydemandingpre trainingtask themodel has to learn both code naturalness andcode semantics.
we were inspiredby notingtheworkofhumaneditors of books journals newspapers they digest imperfectly written but mostly correct text understand the intent and then produce more perfect text withprettymuchthesamemeaning.editingis hard askillededitor has to have very high levels of language comprehension to understand given potentially badly written text and then deploy very high level writing skills to generate well formed text.
if editing could be used as an at scale pre training task the learned model wouldpresumablyhaveexcellentlanguagecomprehensionandalso generate excellent text.
however it s not obvious how to generate at scale training data for this editing task say for english.
a. natural code scanner sc newscanner ... while sc.hasnext string ln sc.next ... ...b. un naturalcode scanner sc newscanner ... for sc.hasnext string ln sc.next ... ... figure example of a natural code fragment written by developersandits un naturally transformedcounterpart.
if theinitialization andupdatepart of the forloop were to left empty developers would writethe whileloop.
butourconcernhereiscode notnaturallanguage.westartwith the argument that because of thebimodal dual channel nature of code itisindeedpossibletogenerateat scaletrainingdatafor theeditingtask a.k.a.refactoringinsoftwareengineeringterminology .codehasaformalchannel withwell definedsemantics becauseofthis it spossibletotransformcodeintoendlessforms allmeaning equivalent .
essentially we can deploy a set of meaning preserving transformations to rewriteexisting code from widelyusedgithubprojects whichpresumablyhavegood qualitycode thathaspassedhumancodereview .theserewrites e.g.
figure1 preserve meaning but will make the code into an artificial often unnatural form1.
1studies withhuman subjects suggestthathumansfindsuchrewrittenbut semanticallyidenticalformsharderto read and understand.nevertheless wenowhave amatchedpairoftwosemantically equivalent forms of code a de naturalized form and the original natural form.furthermore wecanproducethesepairsat scale and then pre train on a code naturalization task.
by analogy with human editors as described above such pre training forces themodeltolearntwohardthings capturethemeaningofthe input code and generate an output that more closely resembles human written code.
we hypothesize that the resulting model will bothlearnbettermeaningrepresentations andalsogeneratebetter code.
tothisend wepre trainedour natgenmodel using codenaturalizing task.
natgenis based on a transformer based sequenceto sequence model and learns to naturalize artificially generated de naturalized codebackintotheformoriginallywrittenbydevelopers.
we emphasize thatnatgenlearns to generate the whole code thislearnedskilltransferstodownstreamfine tuningtasks that require code generation.
we show that our pre training objective helps model generate more natural code complete code with high syntactic and semantic similarity with the original humanwritten code .
with proper fine tuning natgen achieves stateof the artperformanceinvariousdownstreamfine tuningtasks includingcodegeneration codetranslation bugfix thatdemand codegeneration.wealsoshowthat natgen isspeciallyeffective when labelleddata isscarce.
we summarize our main contributions.
weintroducetheideaof codenaturalization asapre training task.
usingcodefromgithub andcustomtooling wehavegenerated and released a large dataset for pre training models on the naturalization task.
wehavebuiltandreleasedalargesequence to sequencemodel pre trainedonnaturalization.
we show that when appropriately fine tuned natgenoutperforms sota onseveral settings.
we publish our source code and data download script for pretrainingnatgen anonymously in natgen.wealsosharethepre trainedmodelin and all the finetuned model in natgen fine tuned models .
2background problem formulation this section presents the relevant technical background that leads to this work andan overviewof the main research questions.
.
the dual channels ofcode humanscanreadandwritebothnaturallanguagesandcode.however unlikenaturallanguage sourcecodeinvolves twochannelsof information formal natural .
the formal channel unique to code affordsprecise formalsemantics interpreters compilers etc.
use this channel.
on the other hand the natural channel perhaps moreprobabilisticandnoisy reliesonvariablenames comments etc.
and is commonly used by humans for code comprehension and communication .
the formal channel s precision enables semantic preserving code transformation which supports staticanalysis optimization obfuscation etc.forinstance major refactoringofasourcecodemaydrasticallychangethesyntactic 19natgen generative pre trainingby naturalizing source code esec fse november14 18 singapore singapore structure while preserving the semantics .
however not all thesemanticallyequivalentcodeis natural theusualway developerswritecodeandthus amenabletostatisticalmodels .
in fact deviation from such naturalness may lead to unintended bugs andincreasedifficultyofhumancomprehension .
weleveragethenatural formaldualityforourpre trainingobjective in this work.
we keep the formal channel constant not changing the meaning for a given code and modify the syntax by creating unnatural code.
then we train the model to take the unnatural code as input and do what a human editor does withnaturallanguagetext understandthe unnatural codeand generate more natural code that a developer would write.
thus the model simultaneously learns to both comprehend code and generate natural code.
.
naturalizing vs.de noising naturalizing pre training essentially follows in the tradition of denoising pre training although arguably theformer ismore subtle and challenging.
denoising pre training is a wellestablishedpre trainingstrategyforencoder decodermodels the encoder is presented with a noised up input and the decoder is asked to generate the original noise free input.
by training the modeltoidentify remove noise inanoisyoutput intheory one teaches it to reason about andcorrectly generate text.exactly what a model learns largely depends on the noise types.
for instance plbart usessyntacticnoise2 i.e.
tokenmasking token deletion etc.
.
thus denoising pre training enables plbart to learn both about the syntax of input source code andlearn to generatesyntacticallycorrectcode.naturalizingpre training onthe otherhand beginswithsyntacticallycorrectbutartificially created unnatural source code and forces the model to generate correct semantically equivalent natural code that is just what a human originallywrote.suchpre trainingrequiresmoresubtlechangesto thecode.wehypothesizethatthisprovidesamoredemandingpretraining setting which will lead to better on task code generation performance.
.
research questions ourhypothesisisthatour naturalizing task seesection .
endows ourpre trainedmodelwiththeabilitytogeneratesyntacticallyand semantically correct andnaturalcode.this leads to several rqs.
rq1.
does naturalization help to improve code generation?
in contrast to existing de noising techniques that help the model learn lexical syntactic structure the naturalizing task which is arguably more demanding than de noising forces natgengenerating better code with higher syntactic and semantic correctness.
the pre training data we use in natgen challenges the model tonaturalizecodethatwas de naturalized inseveralways such as dead code inserted variable renamed etc.
we investigate the relative performance underdifferentnaturalizationchallenges.
2noisethat breaks the syntax structureof coderq2.howdodifferentcomponentsin natgen contribute to codegeneration?
we evaluate the performance under different challenges on a held out validation dataset.this datasetissampledwiththe same distributionofde naturalizingtransformsasthetrainingdataset dt on this set the model to reconstruct the original code.
our exploratory investigation reveals that variable renaming is the hardest transformation to undo the model reconstructs original codewithonly accuracy.deadcode ontheotherhand isthe easiestwith accuracy.
wefurtherinvestigate natgen sperformancefordownstream sourcecode generationtasks.
rq3.
how effective is natgen when fine tuned for differentgenerative tasksinsource code?
we fine tune the pre trained natgen on task specific training dataset for a certain time budget and evaluate the fine tuned model on the benchmark testing dataset for corresponding task.
thesetasksincludesourcecode java generationfromtext code translation from javatoc andc tojava andbugfixing.
after fine tuning natgenachievesthestate of the artperformancein allthesetasks.inaddition wealsodiscoverthat codegenerated bynatgen aresyntacticallyandsemanticallymoreclosertothe expectedcode.
we observe that training a model for a complex task requires sufficient labeled data.
however for most software engineering tasks finding labeled data is a significant challenge .
we investigate potential scenario where size of the training data is extremely small.
rq4.howwelldoes natgen spre traininghelpintasks where labelled data is scarce?
we simulate training data scarcity in two different ways zeroshotlearning andfew shotlearning .for zero shot learning we evaluatethepre trained natgenindifferenttasks withoutanytask specific fine tuning.
for few shot setting we simulate training data scarcity by sub sampling the benchmark training datasets.
we fine tune the pre trained natgen on these limited training examplesandmeasuretheperformance.weobservethat natgenis veryefficientinlow datatraining.since natgenlearnstogenerate syntactically and semantically correct code as part of pre training itfaces less burden whilelearninginlow data training.
methodology our approach comprises three steps i de naturalize source code to accumulate pre training data for natgen 3. ii pretrainnatgen using this data for naturalization task .
iii fine tunepre trained natgenwithtaskspecific dataset .
.
.
de naturalizing sourcecode forthefirststepabove weusesixrulestotransformanaturalcode intoitsunnaturalcounterpart.thesetransformationsaresemanticpreserving but rewrite an original natural human written code toanartificialform.givenanaturalcodeelement wedeployan 20esec fse november14 18 singapore singapore s.chakraborty t. ahmed y. ding p. devanbu b.ray 1intsearch int arr intkey intlow inthigh while low high intmid low high low if arr key return mid else high mid return a original code1intsearch int arr intkey intlow inthigh for low high intmid low high low if arr key return mid else high mid return b loop transformation 1intsearch int arr intkey intlow inthigh while low high intmid low high low while i i high mid ... rest of the code return c deadcodeinsertion1intsearch int arr intkey intlow inthigh while high low intmid low high low if arr !
key high mid else return mid return d blockand operand swap 1intsearch int arr intkey intlow inthigh while low high intmid low high low if arr key return mid else high mid return e inserting confusing codeelement1intsearch int var 1 intkey intlow intvar 2 while low var 2 intmid low var 2 low if var 1 key return mid else var 2 mid return f variable renaming figure semanticpreserving transformationused to prepare thepre training data for natgen .
appropriatetransformation basedonitsaststructureandrewrite the code to de naturalize it.
.
.
designing transformation rules.
we use six classes of denaturalizing transformations.
these transformations are motivated bypriorworkonfunctionalreasoningaboutsourcecode andsemantic bug seeding .
figure2showthe details.
loop transformation figure 2b .this rule modifies for loopsintoequivalent whileloopandvice versa.werewritea while loopoftheform while condition loop body intoafor loop asfor condition loop body .
likewise to transforma forloop into a whileloop we move theinitializer of thefor ifany beforetheloop andtheupdate expression ifany oftheforloopasthelast statement intheloop.wealsoaddthis updatestatementbeforeanyloopbreakingstatement i.e.
break continue .forexample wetransform for int i i i if i foo continue bar as int i while i if i foo i continue bar i .
deadcodeinjection figure 2c .weinjectblocksofdeadcode atrandom positionsintheoriginalcode.by deadcode wemeancode that appears in the source but is never executed.
in figure 2c we inject the code block high mid at line of the original code figure 2a .toaddchallengetothemodel wetransplantthese insertedstatementsfromthesameinputcode.toensurethe death of inserted code we put the inserted statements in a block headed byeitheralooporabranch guardedbyaunsatisfiableconditionso that thecode inside theblock will never execute.in figure 2c the condition i iis always false and the code in line is quite dead.
blockswap figure 2d .hereweswapthe then blockofachosenifstatementwiththecorresponding elseblock.topreserve semantic equivalence we negate the original branching condition.
for instance figure 2dreplaces the ifblock line in figure 2a with the elseblock line in figure 2a .
we negate the original condition arr key as arr !
key .
operand swap figure 2d .here we swap the operands of binarylogicaloperations.forinstance wechangetheexpression low highwithhigh lowinline2infigure 2d.whenswapping the operands of a logical operator we change the operator to makesure themodifiedexpression isthe logicalequivalentto the onebeforemodification.incaseofasymmetricinequalityoperators 21natgen generative pre trainingby naturalizing source code esec fse november14 18 singapore singapore for try if step2 select transformationast extractionfor .
if .locating transformation sites filter applicable transformationtransformation poolapplicable transformations random selection apply transformationstep1 find tranformation location step3 apply tranformationnatural code while ... ... try if ... while try ifwhile context adaptation un natural codecode regenerationfor ... ... try if ... figure de naturalization workflow in natgen .
we change the direction keep as is for symmetric operators i.e.
!
.
confusing code insertion figure 2e .we introduce confusing code patterns in the code as outlined by gopstein et al .
.
in particular we introduce two forms ofconfusing code.
first we modify the of the form i j j toi j .
second we introduce ternaryoperator as applicable.
for example we transform the code if x!
y p else y q to y x!
?p q .
variable renaming figure 2f .we rename some variables tovar i. while renaming a variable we analyze the dataflow of that variable and rename all occurrences of that variable in the entire code.
from all the variables used in the code we change just a certain percentage.
for instance in figure 2f we renamed variablearrtovar 1 and variable hightovar 2 leaving all othervariablesunchanged.notethat unlikeothertransformations variablerenamingdoesnotcreateastofdataflowgraphdifference.
however this challenging task forces the model to learn to generatenaturalvariablenames.thisresemblesthede obfuscation pre training taskof .
.
.
applying transformation.
assume a set of transformation rules 1 2 3 ... .
given original code ci j ci transforms thecode changingthestructurewhilepreservingsemantics.figure3shows how to apply such transformation to ci.
it works in three steps findtransformationlocation.
givenapieceofsourcecode ci we first use tree sitter3to parse out the ast tci .
from the ast we extract potential locations for de naturalization.
these locations are nodes nk intci.
while choosing location nkfrom we consult we extract the nodes where at least one of j isapplicable.
select transformation rule.
once we have a set of such nodes we filter out the transformation rules that cannot be applied to any node of in tci.
after such a filtration we have a set of transformations a .atthisstage werandomlyselectone transformationpattern j atoapplyatanapplicationlocation ast node nk.
apply transformation.
we apply jtonkto get the transformed noden k. we then structurally match n kwith the original ast tci specifically nk.weadaptthecontextof nktothetransformed node s n k context.
in that way we get the transformed ast t ci whichwe then translate to getthe transformedcode c i. we designed the transformation function jand subsequent context adaptation in such a way that preserves the meaning or functionality of the original code.
we use ast analysis and approximated data flowanalysisoncode ast.
.
pre training once we have a pool of unnatural code using the transformation in section .
i.e.
transform code cias un natural code j ci we use a neural sequence to sequence translation model m to reconstruct cifrom ci i.e.
we wantm j ci to approximate ci.inparticular givenatrainingdataset dt c1 c2 ... consisting ofdeveloperswrittencode setof de naturalizing transformations 1 2 3 ... we optimize the following function to learn m soptimal parameter .
argmin ci dtcrossentropy parenleftbigm parenleftbig j ci parenrightbig ci parenrightbig .
fine tuning theobjectiveofourpre trainingistolearntobothcomprehendand generate general purpose source code.
however different tasks relatedtosourcecodegeneration e.g.
texttocodegeneration codeto codetranslation bugfixing callfortask specifictrainingofthepretrainedmodel.thistrainingphaseonapre trainedmodelisknown as fine tuning .
we consider the fine tuning in natgen as a translationtaskandfollowthestandardtransformerbased machine translationprocedure .first theencodergeneratestheencoded representation r x given the input x .
the decoder then sequentially generates the output y .
whileencodinganinputtoken xk theencoderlearnstheattention matrixw.r.t.every token in the input including xk.
such attention matrixisknownas self attention .whilegeneratinganoutputtokenym the decoder learns the attention matrix with all previously generated tokens throughself attention and the encodergeneratedrepresentation r x throughcross attention .we refertovaswanietal .
formoredetailabouttransformer based translation.
experimentalsetup this section details the experimental designof natgen.
pre training data.
following prior works we primarily use codesearchnet dataset for the pre training purpose.
codeserachnetisapubliclyavailabledatasetwithsixlanguages 22esec fse november14 18 singapore singapore s.chakraborty t. ahmed y. ding p. devanbu b.ray java python go javascript ruby andphp.inadditiontocodesearchnet codet5 uses additional data for c and c .
we also use 1mfunctionseachforcandc .forthesetwoadditionallanguages we collected active projects from github and randomly selected 1m functions considering the maximum sequence length of the model.
table statistics of fine tuning datasets.
task dataset train dev test text code generation concode code code translation codexglue text code code bugfix small medium fine tuningdata.
weevaluatedifferentvariationsofthreebenchmark tasks related to source code generation.
the first task is text tocodegeneration wheretheinputisannldescriptionofajava method and the output is the code.
the second task is code translationbetweenjavatoc andc tojava .forthistask weevaluate java c paralleldatasetproposedbyluetal .
.thethirdand final task is bug fix where the given a buggy code and a summary of the fix model generates the fixed code.
for this task we used thetwodifferentversionsofthedataset small withlessthan50 tokens and medium with up to tokens proposed by tufano etal.
.notethat similartomodit weevaluateon concrete versionoftherefinementdatasets.table 1showsthedatasetsand their statistics.
for text to code generation and code translation wereusethesamesplitfromcodexglue andforbugfix we reuse the same split as modit.
pre training model configurations.
we use layer transformers with attention heads on both encoder and decoder following the codet5 architecture.
as discussed in section we use denaturalizationgenerativeobjectivesforpre training.weinitialize ourmodelwithcodet5 s releasedparameters.inparticular we initializenatgenwith codet5 base model.wepre train natgen on nvidia geforce rtx gpus for 25k steps maintaining the effective batch size at with learning rate 5e .
we train natgenfor approximately168hours.
evaluationmetric.
throughouttheexperimentsinthiswork we evaluate accuracies w.r.t.
exact match em syntax match sm dataflow match dm and codebleu cb .
sm is the proportionofmatchingsubtreesbetweenoutputcodeandtadgetcode s astsw.r.t.number of all possible subtrees in the target code s ast.
dmisthepercentageofmatched withtargetcode anonymized dataflowedge def useedge ofoutputcode w.r.t.alldataflowedges in the target code.
note that both the sm and dm are components of cb.
we explicitly evaluate these for understanding the syntactic and semantic correctness of generated code.
we reuse microsoft codexglue tool to compute sm dm andcb.
baselines.
whilecomparingtheevaluationresultsfordifferent tasks we compare with large scale pre trained models including gpt codegpt plbart spt code and codet5 .mostofourfine tuningevaluationisonbenchmarkeddataset thus we report the available results from codexglue leaderboard .
there are some task specific baselines which we discuss whiledescribingcorresponding task.
empirical results weevaluate natgen on i pre trainingand ii threefine tuning tasks.
we also check natgen s effectiveness in zero shot and fewshotsettings.
.1natgen seffectiveness on pre training rq1.does naturalization helpto improvecodegeneration?
motivation.
weinvestigatewhetherpre trainingonnaturalizing taskhelpsthemodelgeneratecorrectandnaturalcode codethat issyntactically andsemantically similar to the originalcode .
experimental setup.
we compare three large scale pre trained models i codet5 ii plbart and iii natgen.
note that since plbart is only pre trained on java and python we compareplbartonlyforthoselanguages withthecorresponding resultsofothermodels.weaskeachofthesemodelstoreconstruct developers written codefromits de naturalized but semantically identical see .
3. .
variants.weusetheheld outvalidation datafromourtrainingprocedureforthisevaluation.weevaluate the models for generating the exact match em syntax match sm anddataflowmatch dm .
table evaluation of natgen for code generation task.
cs isthepercentageofexampleswhereoutputisdirectlycopied from source and ed is the median edit distance between input codeandoutputcode.
eval data model em sm dm cb cs ed fullcodet5 .
.
.
natgen .
.
.
.
.
java pycodet5 .
.
.
plbart .
.
.
.
natgen .
.
.
.
.
results.table2showsthe evaluation results.
syntaxmatch.
wefindthatthecodegeneratedbyplbartand natgenare mostly syntactically correct.
however codet5 s does not always generate syntactically valid code suggesting an advantagefornaturalizationpre training.forinstance figure 4shows code generated by different models from the given input.
as we cansee codet5generatesasyntacticallyerroneousfragment.in contrast plbart made a minor edit on the input code just removingthe protected keyword.bothplbartand natgen are pre trainedtogeneratecompletecoderatherthanfragments which isthecaseofcodet5 thus theformertwogenerallydobetter at generatingsyntactically correctcode.
semantic match.
natgen is effective at recovering developers writtencodefromitsde naturalizedsemanticvariants around70 ofthegeneratedcode codeblue exactlymatches theoriginal code.
plbart which deploys syntactic denoising is at the second position interms ofcodeblue.
23natgen generative pre trainingby naturalizing source code esec fse november14 18 singapore singapore .input protected sdv iam sdv in ... if i i return new iam ... return new iam ... .plbartoutput sdv iam sdv in ... if i i return new iam ... return new iam ... .natgenoutput protected sdv iam sdv in ... return new iam ... .codet5output if in return figure4 exampleofinputgeneratedcodebydifferentpretrained models slightly simplified .
natgen also dominates the other two models in generating syntactically sm semantically dm valid code.
while plbart appearstogeneratesyntacticallycorrectcode itmostlycopiescode fromtheinput medianeditdistancefromplbart sinputandthe generatedcodeis3 seetable .infact in7.
ofcases plbart just copies the input!
by contrast natgenlearns to generate variantsoftheinputcode withonly0.
directcopyandamedian editdistanceof10.sinceplbartistrainedtoremovesyntaxerrors from the input we conjecture that itdoes not inherentlylearn the semantic variation of the code.
by contrast we expose natgento semanticcodevariations forcingittolearntogeneratecodethat isboth more natural andsemantically equivalent.
closer look into codet5.
unlikenatgenand plbart codet5 is not explicitly trained to generate complete code.
during pretraining codet5 learned to unmask masked token sequences.
thus to better measure codet5 s generation capacity we conduct another experiment where we replaced all occurrences of some of the variable names in code with a special mask1 mask2tokens and asked codet5 to generate.
this is one of the objectives masked identifiersprediction codet5ispre trainedtooptimize.
wetake the codet5 s output and identify all potential identifiers4.
surprisingly in only0.
of the cases could codet5 generate all the variables and in .
of cases halfof the masked variables.
whilenatgen successfully translates .
of those examples backtoitsoriginalcode includingcorrectlypredictingthereplaced variable names.
in addition codet5 s generated token sequence contained a lot of other tokens than the variable names figure .
for example .
result1 naturalizationenables natgentoreasonaboutcode semantics and thus help generate more natural code variants than existingpre training modelsand pre training objectives.
we also did an ablation study evaluating the effect of natgen s differentcomponentsonthe results.
rq2.howdodifferentcomponentsin natgen contributetocodegeneration?
motivation.
inthisrq westudyhowdifferenttransformation rules see .
contribute to learn generating natural code from different semantic variants .
we also evaluate how well natgen learns that in different programming languages over training time.
4weuseregex to find identifiers.experimental setup.
while pre training we checkpoint the natgenmodelevery1ktrainingsteps forafullrunof25ksteps.at each checkpoint we evaluatethenaturalization taskperformance.
before training we held out .
of the total data as validation data.
notethat since our goalin this experiment is to understand natgen spre trainingbetter we de naturalized thevalidation datausingthesametrainingdatadistribution.thissettinggives usacontrolledenvironment for experimentation.
figure performance of natgen pre trained model under differentcodetransformations.
results.figure5showsnatgen sperformance underdifferent types of semantic variants.
results show that natgen has most troublerecreatingtheoriginalcode just40 exactmatch withthe variablerenamingtask.variablerenamingischallengingevenfor human developers different developers may propose different names for the same object.
nevertheless on this task natgen achieves good syntax and dataflow match and respectively indicatingthat natgen preservessyntaxorsemanticsin mostcaseswhilegeneratingcode withrenamedvariables.
on the other hand natgencan eliminate dead code with accuracy.
this result may be an artifact of our specific implementation of this transformation.
our dead code insertion rule is simple andformulaic sothe natgenquicklylearnstoidentifyandremove such dead code.
a more complex pattern of dead code may challenge the model more and help make it more robust we leave this forfuturework.fornaturalizingothertransformations natgen achieves more than exactmatch accuracy forblock swap and confusion removing and more than exact match accuracy for therest.inallcases syntaxmatch dataflowmatch andcodebleu are well above .
figure6shows how validation performance improves for differentlanguages withmoretrainingsteps.acrossallthelanguagesthe performance rapidly increases over the first few thousand training steps.infact atthebeginningof step0 of natgen spre training theoverallexactmatchis0 syntaxmatchis13.
dataflowmatch is19.
andcodebleuis9.
seetable 2fordetails5 .however afterjust1000stepsoftraining theexactmatchrisesto61 syntax matchto97 dataflowmatchto94 andcodebleuto95 .these metricscontinueimprovingastrainingprogresses.theseresults confirmthatacrossallthelanguages natgen graduallylearns to generatemore naturalcode from semantic variants.
5natgen s pre training start from codet5 base.
thus codet5 base is natgen s checkpointat step0.
24esec fse november14 18 singapore singapore s.chakraborty t. ahmed y. ding p. devanbu b.ray figure progression of codebleu of different language in validationdatasetovernumberpre training steps.
result2 pre training performance depends on the types of semantic variants while variable renaming seems the most difficult accuracy dead code elimination appears to be an easier task accuracy to learn.
.2natgen seffectiveness on fine tuning tasks this section evaluates natgen s performance on three benchmark sourcecode generative tasks.
rq3.
how effective is natgenwhen fine tuned for different generative tasks in source code?
table3 resultsoftexttocodegeneration.
impliesthat those results arenot reported by corresponding approaches.
mlastisthemodelaftercompletingthefintuning and mbest is the intermediate model with best validation performance.
approach em sm dm cb seq2seq .
.
guo et al.
.
.
iyeret al.
.
gpt .
.
codegpt .
.
plbart .
.
codet5 base22.
.
reported codet5 mlast21.
.
.
.
mbest21.
.
.
.
natgenmlast22.
.
.
.
mbest22.
.
.
.
ourreproduced result using codet5 spubliclyavailable pre trained model.
baselines.
in addition to the baselines discussed in section for thetextto java code generation task we compare with a group of baselines with no pre training involved.
these baselines include lstmbasedsequencetosequencemodels guoetal .
s andiyer et al.
s proposed techniques.
we also report our reproduced version of codet5 resultsin differenttasks slightly differentfromwhattheyreported.forboththe bugfixtask wecomparewiththe reportedresultsofmodit andourreproducedcodet5result.
results.
text to code generation.
table3shows evaluation results for text to code generation.
we trained for epochs.
we stopped the trainingisthevalidationperformancedoesnot increaseformore thanthree consecutiveepochs.forbothcodet5and natgen we report the performance of final model after the fine tuning terminated mlast and the performance of the model with best validation perfomance mbest .
interestingly for both codet5 and natgen themlastmodel performs better than the corresponding mbestmodel.
the result shows that natgen s generated code aremoresyntacticallyandsemanticallyclosertothetargetcode.
themlastmodel of natgenoutperforms codet5 s mlastmodel by .
insm .
indm and .
incb.
we conjecture that natgen spre trainingwith naturalization helpgeneratemore naturalcode.
table4 codetranslationresults.
impliesthatthoseresults are notreported by corresponding approaches.
approachjava c c java em sm dm cb em sm dm cb pbstm .
.
.
.
codebert .
.
.
.
spt code .
.
plbart .
.
.
.
codet565.
.
reported codet5 .
.
.
.
.
.
.
.
natgen .
.
.
.
.
.
.
.
ourreproduced result using codet5 spubliclyavailable pre trained model.
code translation.
table4shows the results of natgen and different baselines for code translation.
for java to c translation natgen achievesexactmatchaccuracyof66.
whilecodet5 s accuracyis65.
.inc tojavatranslation natgenachieves67.
exactmatchaccuracy whichcodet5achieves66.
.inaddition the syntactic match sm dataflow match and codebleu are also higher thanthat ofcodet5.
table resultofbug fix top1 fixaccuracy .
approachbugfix small bugfix medium unimodal multimodal unimodal multimodal modit .
.
.
.
codet5 .
.
.
.
natgen .
.
.
.
bug fix.
similar to modit we evaluate the top accuracy of the generatedfixedcode.wealsoevaluateuni modalsettings where the fix description is unavailable and multi modal settings where wehaveaccesstothefixdescription.table 5showstheresultsof bug fix.
for the bugfix smalldataset natgen outperforms both codet5 and modit in both unimodal and multi modal settings.
25natgen generative pre trainingby naturalizing source code esec fse november14 18 singapore singapore a java to c translation b c to javatranslation c textto code generation d bugfix small multimodal figure zero shot transfer learning capability of natgen infordifferenttasks.
a java to c translation b c to java translation c textto code generation d bugfix small multimodal .
figure few shot learning evaluation of natgen .
in each case the pre trained model is fine tuned on training examples for10epochandtheresultison thefulltestset.
a javato c translation.
b c to javatranslation.
c text to codegeneration.
d bugfix small multimodal .
figure natgen sresults on differenttaskswith few shot settings.x axisshowsnumberoftraining examples.
forforthebugfix mediumdataset natgen performsbetterthan codet5andmoditinunimodalsettingandslightlyworsethan codet5 inthe multi modalsetting.
result3 natgenperforms better than most of the existing baselines.
natgen s improvement in syntax match and dataflow match signifies natgen s ability to generate code syntactically and semantically closer to targetcode.
finally weevaluate natgen sperformanceinthepresenceof data scarcity.
rq4.
how well does natgen s pre training help in tasks where labelled datais scarce?
motivation.
learningtogeneratecodeusuallyrequiresalarge amount of annotated training data.
a lot of time and effort goes into curatinghigh qualitytrainingdata .
unsupervised pretraining endowsmachinelearningmodels withnecessarydomain knowledge about the task .
in practice this knowledge appears to transfer across multiple tasks.
such pre training reduces the effort to learn each different task.
we therefore study the effectiveness ofnatgen s domain knowledge about source code syntax and semantics.
in particular we stress test whether the knowledgenatgenlearnedduringpre trainingisusefulfordownstreamtasks bylimitingavailable task specific training data.
experimental setup.
we evaluate natgen s over several datalimited tasks text to code generation code translation andbug fix.
we consider two different settings.
first we consider zeroshot evaluation.
here we evaluate different pre trained modelswithoutanytask specifictraining.naturally wedon tsee goodperformanceinthissetting.nevertheless thisstress testmeasuresthecodegenerationabilityofmodels.second wetryfew shot learning .
we randomly choose a few training examples for eachtask and fine tune thepre trained models on those examples and evaluate their performance.
we gradually increase the number oftraining examples over several few shotsettings.
results.figure7showsthe natgen sandcodet5 szero shot performance.lackingtask specifictraining wecanseeherehow much transferable knowledge each model learned just during pretraining.therearelargedifferencesinallthetasksbetween natgenandcodet5acrosssyntaxmatchanddataflowmatch.itsignifiesnatgenlearnstogeneratebothsyntacticallyandsemantically correct code during pre training which codet5 rarely can do.
figure8showstheperformanceof natgenandcodet5whentrained 26esec fse november14 18 singapore singapore s.chakraborty t. ahmed y. ding p. devanbu b.ray on training examples.
natgen also has an advantage over codet5 here.
we notea largerperformance gap in the translation tasks figure7a 7b and bug fix figure 7d tasks compared to text to code generation task figure 7c in both the zero shot and the few shot figure experiments.weconjecturethatsuchdiscrepancyis theartifact ofthenatureofthetasks.the cross lingual alignment between nl and java code is the key factor in generating text to code.incontrast boththeinputandoutputaretheprogramming language in the translation and bug fix task.
thus we hypothesizethatnatgen leveragesitssharedknowledgeacrossdifferent programminglanguageslearnedduringthe pre training.
we further stress test natgen s with few shot learning we graduallyincreasedthenumberoftrainingexamplesandtrained bothcodet5and natgen.figure9showstheperformanceprogress as the number of training examples increase.
for all four tasks natgensignificantlyimprovesovercodet5 when thenumberof trainingexamplesisminimal.withincreasingtrainingexamples theperformancegap graduallydecreases.arguably withenough labeleddata and enough resources all high capacity models will get better at generating source code.
nevertheless we learn two criticallessonsfrom natgen sbetterperformanceinzero shotand few shotlearning.first natgen sbetterperformanceacrossall tasks suggests that that the coding knowledge it learns from the naturalization task is more generic and transferable.
second for anypre trainedmodeltobeeffectiveincodegeneration especially inalimitedtrainingdatascenario thepre trainingshouldexplicitly teachthemodelhowtowritecode.otherwise wehypothesizethat abigchunkoffine tuningresourceswillbespentonthemodels learningto writecode.
result4 natgenis very effective in source code generative tasks whenminimal training resource isavailable.
since natgen explicitly learns to generate code during pre training it can avoid learning suchduringfine tuning saving fine tuning resource.
limitations threats biasintroducedby de naturalizing transformations.
in section3.
wedescribedoursixtransformationsto de naturalize sourcecode.the natgenmodellearnstorevertonetransformation at a time.
in fact we found empirically that when given code with more than one de naturalization transformation applied the model reverses only one of them.
there is thus a threat our limited applicationofde naturalizationlimitstheabilityofour natgen.
regardless weconsider natgenasaproof of conceptandthefirst work towards teaching a model to write natural code.
we leave the investigationmorenaturalcodepatternsandtheireffectoncode generationas apotentialfuture work.
table natgen sperformanceincodesummarization approach go java js python php ruby overall plbart .
.
.
.
.
.
.
codet5 .
.
.
.
.
.
.
natgen .
.
.
.
.
.
.55knowledgeretentionfromcodet5.
asmentionedinsection westartnatgen spre trainingfromcodet5 basemodel .starting further pre training from an existing pre trained checkpoint is very common in large scale pre training.
for instance graphcodebert is pre trained based on codebert model which was pre trained based on roberta model.
both the open ai codex and github copilot models are further pretrained in openai gpt3 .
nevertheless when we further train a pre trained model on different tasks it is subject to catastrophic forgetting of the knowledge learned in the base model.
in ordertotestwhether natgen isforgettingcodet5 sknowledge aboutnaturallanguagegeneration wealsoevaluate natgen for codesummarization.heretheinputissourcecode andtheoutput isnaturallanguage.afterfine tuning natgen soverallbleuin .
while codet5 s was .
suggesting that natgenmostly retainscodet5 scapacity togeneratenl see table 6fordetailed results .
fair comparison with codet5.
we initialize natgen with pre trainedcheckpointfromcodet5 alreadypre trained75ksteps with their objective and train natgenfor 25k steps with naturalcode writingobjective.askepticreaderwouldwanttoknowwhat happenswhen we pre train codet5for 25k more steps with their trainingobjective.wearguethatsincethepre trainingobjective doesnotexplicitlyaccountforgeneratingcode seesection3.2of codet5 soriginalpaper furthertrainingwiththecodet5objective doesnotnecessarilyincreaseitscodegenerationcapacity.wedo acknowledgecodet5 sabilitytounderstandandreasonabout input.
sincethepre traininglargemodelisextremelyexpensive we leverage such knowledge by initializing natgen from codet5 s publicly available pre trained model.
moreover codet5 release neithertheircodeforpre training onlyforfine tuning norany earlierorlatercheckpointsforustocarryoutfurtherinvestigation.
naturalization withprogram analysis.
natgenisaprototypeofagenerativepre trainedmodelwith naturalization task trained to revert six classes of de naturalization transformations seefigure .however perfectperformance w.r.t.thesetransformation isnotthe main objective of this research.
tools to accomplish naturalization couldsurelybe builtusingtraditionalrefactoring methods however our goal is to train natgenso that it learns to generatenaturalcode withthe helpof this naturalization task.
natgen as code refactoring tool.
natgen suggests the promise of neural transformers to build meaning preserving coderefactoringtools.however torealizeamoreaccurateandpowerful neuralre factoringtool moretrainingdata withalargervarietyof transformations wouldbe required.we leave this as future work.
related works theapproachofpre traininglargetransformerswithouthuman labels started in nlp domain with bert which introduces two pre training objectives i.e.
mask language modeling and nextsentenceprediction .later liuetal.showthatroberta outperforms bert only using mask language modeling mlm with new training strategies and hyper parameter tuning.
mlm is 6codet5waspre trainedon16 nvidia a100s with40gmemory each for12 days!
one mightreasonably assumeit was alreadywell trained onthe originalobjective 27natgen generative pre trainingby naturalizing source code esec fse november14 18 singapore singapore a self supervised task that the model randomly masks or modifies acertainnumber oftokens andtries to recover them.
following the success of the pre trained model in the nlp domain researchers applied these models to code related tasks.
codebert is one of the earliest that was specially trained for code andrelevantnaturallanguagedescriptions.itispre trainedwith twoobjectives i.e.
mlm andreplacedtoken detection and demonstrated pre training s effectiveness for code.
later an architecturallyequivalentmodel graphcodebert wasintroduced it improvedovercodebertonmosttasksbyincorporatingdata flow information.
though codebert graphcodebert dietcodebert dowellatcodeunderstandingtasks thesemodelsare not as good at generative tasks.
both models are encoder only and have to start with an untrained decoder in fine tuning for generativetasks suchascoderepair codegeneration codesummarization andcodetranslation.toaddressthislimitation ahmad etal.introducedplbart pre trainedasagenerativedenoising autoencoder.
a specific set of noises is introduced to code and relevant natural language description and used as the input to the model.
the model s objective is to encode the noisy input in the encoder and generate noise free code or text in the decoder.
plbart buildsonbart outperformsbothcodebert andgraphcodebert onbothunderstandingandgenerative taskswithapre trainedencoderanddecoder .dobf uses de obfuscation recovering variable names as their pre training task however rather than generating code they just generate a dictionary ofrecoverednames.
codet5 basedt5 isthelatestdenoisingmodel.codet5 usesthedeveloper assignedidentifiersincode addingtwocodespecificpre trainingobjectivestotheoriginalt5 identifiertagging and masked identifier prediction.
codet5 is an encoder decoder model and excels at both understanding and generative tasks compared to other models.
similar to codet5 are also built based on t5 architecture and perform reasonably well in the different downstream tasks.
natgen has a similar architecture to codet5 butrather thancodet5 spre trainingobjectives we denaturalize code usingtheformalchannelofcodetoinjectmeaningpreserving transforms and then force natgen to recreate the original natural code.
rewriting semantically equivalent code requiressemanticunderstanding andthatcanbeappliedtocode only because of its dual channel nature.
our evaluation shows thatrewritingsemanticallyequivalentprogramsinthepre training stageresultsinperformancegainsinatleastthreepopularsoftware engineeringtasks.
conclusion we introduce the code naturalization pre training objective for generative models of code.
as proof of concept we pre trained our natgen towrite natural sourcecode from un natural counterpart.
with this pre training natgenlearns to write code syntacticallyandsemanticallyclosertodevelopers writtencode.we denaturalize existing developers code using six kinds of semanticpreserving transformations.
we further fine tune the natgenon different variations of three downstream tasks that require code generation.
natgenachievesstate of the artperformanceinthesedownstream tasks and natgen s generated code are syntactically and semantically closer to the target code.
our pre training on the naturalizing task is especially effective in resource constrained settingi.e.
zero shot andfew shottransfer learning.
data availabilitystatement we publicly code and all processing scripts of natgen s pretraining .natgen pre trainedmodelisalsoavailablethrough .