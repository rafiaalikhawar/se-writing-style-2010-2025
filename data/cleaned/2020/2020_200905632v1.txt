a principled approach to graphql query cost analysis alan cha ibm research usa alan .cha1 ibm .comerik wittern ibm germany erik .wittern ibm .comguillaume baudart ibm research usa guillaume .baudart ibm .com james c. davis purdue university usa davisjam purdue .edulouis mandel ibm research usa lmandel us .ibm .comjim a. laredo ibm research usa laredoj us .ibm .com abstract the landscape of web apis is evolving to meet new client requirements and to facilitate how providers fulfill them.
a recent web api model is graphql which is both a query language and a runtime.
using graphql client queries express the data they want to retrieve or mutate and servers respond with exactly those data or changes.
graphql s expressiveness is risky for service providers because clients can succinctly request stupendous amounts of data and responding to overly complex queries can be costly or disrupt service availability.
recent empirical work has shown that many service providers are at risk.
using traditional api management methods is not sufficient and practitioners lack principled means of estimating and measuring the cost of the graphql queries they receive.
in this work we present a linear time graphql query analysis that can measure the cost of a query without executing it.
our approach can be applied in a separate api management layer and used with arbitrary graphql backends.
in contrast to existing static approaches our analysis supports common graphql conventions that affect query cost and our analysis is provably correct based on our formal specification of graphql semantics.
we demonstrate the potential of our approach using a novel graphql query response corpus for two commercial graphql apis.
our query analysis consistently obtains upper cost bounds tight enough relative to the true response sizes to be actionable for service providers.
in contrast existing static graphql query analyses exhibit over estimates and under estimates because they fail to support graphql conventions.
ccs concepts security and privacy denial of service attacks software and its engineering domain specific languages .
keywords graphql algorithmic complexity attacks static analysis introduction web apis are the preferred approach to exchange information on the internet.
requirements to satisfy new client interactions have led to new web api models such as graphql a data query language.
a graphql service provides a schema defining the data entities and relationships for which clients can query .
graphql has seen increasing adoption because it offers three advantages over other web api paradigms.
first graphql reduces most of the work performed while at ibm research usa.
most of the work performed while at virginia tech.network traffic and server processing because users can express their data requirements in a single query .
second it simplifies api maintenance and evolution by reducing the number of service endpoints .
third graphql is strongly typed facilitating tooling including data mocking query checking and wrappers for existing apis .
these benefits have not been lost on service providers with adopters including github and yelp .
however graphql can be perilous for service providers.
a worstcase graphql query requires the server to perform an exponential amount of work with implications for execution cost pricing models and denial of service .
this risk is not hypothetical a majority of graphql schemas expose service providers to the risk of high cost queries .
as practitioners know the fundamental problem for graphql api management is the lack of a cheap accurate way to estimate the cost of a query .
existing dynamic and static cost estimates fall short .
dynamic approaches are accurate but impractically expensive relying on interaction with the backend service and assuming specialized backend functionality .
current static approaches are inaccurate and do not support graphql conventions .
we present the first provably correct static query cost analysis for graphql.
we begin with a novel formalization of graphql queries and semantics .
after extending the formalization with simple configuration information to capture common schema conventions we define two complexity metrics reflecting server and client costs for a graphql query .
then we show how to compute upper bounds for a query s cost according to these metrics .
our analysis takes linear time and space in the size of the query.
our analysis is accurate and practical .
we studied query response pairs from two commercial graphql apis.
unlike existing analyses our analysis obtains accurate cost bounds even for pathological queries.
with minimal configuration our bounds are tight enough to be actionable for service providers.
our analysis is fast enough to be used in existing request response flows.
this paper makes the following contributions we give a novel formalization of graphql semantics .
we propose two graphql query complexity measures that are used to estimate the cost of a query.
we then prove a lineartime static analysis to obtain an upper bound for these measures on a given query .
we evaluate our analysis against two public graphql apis and show that it is practical accurate and fast .
we also identify causes of over and under estimation in existing static analyses.
we share the first graphql query response corpus unique query and response pairs from the github and yelp apis.arxiv .05632v1 sep 2020alan cha erik wittern guillaume baudart james c. davis louis mandel and jim a. laredo figure proposed applications of our query analysis.
the client s malicious query requests an exponentially large result from github s graphql api.
at the time of our study github permitted the shown query but halted its execution after it exceeded a time limit.
using our techniques client side query inspection can provide feedback during composition see complexities inset .
server side policy enforcement can reject queries and update rate limits based on provider defined policies.
we disclosed this denial of service vector to github and it has since been patched .
.
.
we illustrate applications of our analysis by exploiting a flaw in github s static analysis figure .1we issued an exponentialtime query to github.
github s analysis incorrectly estimated the query s cost accepted it and wasted resources until the evaluation timed out.
our analysis can help service providers avoid this situation.
some large queries are accidental and our measure of type complexity would permit clients to understand the potential sizes of responses before issuing queries reducing accidental service and network costs.
both type complexity and our measure of resolve complexity would permit service providers to understand a query s potential execution cost.
using these metrics will allow service providers to identify high cost queries and respond appropriately.
background and motivation in this section we motivate the need for graphql query cost analysis .
and then discuss existing query analyses .
.
.
motivation our work is motivated by two aspects of software engineering practice the majority of real world graphql schemas expose service providers to high cost queries and existing strategies employed by service providers are inadequate.
high complexity graphql schemas are common in practice.
hartig and p rez showed that a graphql query can yield an exponential amount of data in the size of the query .
such a query requests the nested retrieval of the same data and is only possible if the schema defines self referential relationships loops of lists and if the underlying data contains such relationships.
wittern et al.
extended their analysis to identify schemas with polynomial sized worst case responses and analyzed a corpus of graphql schemas for these properties .
in their corpus they found that over of the commercial or large scale open source schemas had exponential worst case behavior and that under ofallschemas guaranteed linear time queries.
1github s api also has a runtime defense so the risk to their service was minimal.many public graphql apis do not document any query analysis.
we manually studied the documentation for the 30public apis listed by apis.guru a community maintained listing of graphql apis .
we used public apis listed as of february 28th other graphql apis are unlisted or private .
disturbingly apis describe neither static nor dynamic query analysis to manage access and prevent misuse.
22apis make no reference to rate limiting or to preventing malicious or overly complex requests.
three apis perform rate limiting but only by request frequency ignoring the wide range of query complexities.
a few apis have incorporated customized query and or response analysis into their management approach.
five apis describe analyzing graphql queries to apply rate limiting based on the estimated or actual cost of a query or response.
github shopify and contentful estimate the cost of queries before executing them.
shopify and yelp update remaining rate limits by analyzing responses i.e.
the actual data sent to clients.
but these approaches have shortcomings that are discussed in .
.
.
existing graphql query cost analyses a graphql query cost analysis measures the cost of a query without fully executing it.
service providers can use such an analysis to avoid denial of service attacks as well as for management purposes.
there are two styles of graphql query analysis dynamic and static .
the dynamic analysis of considers a query in the context of the data graph on which it will be executed.
through lightweight query resolution it steps through a query to determine the number of objects involved.
this cost measure is accurate but expensive to obtain because it incurs additional runtime load and potentially entails engineering costs .
static analyses calculate the worst case query cost supposing a pathological data graph.
because a static analysis assumes the worst it can efficiently provide an upper bound on a query s cost without interacting with the backend.
the speed and 2in particular their analysis repeatedly interacts with the graphql backend and assumes that the backend supports cheap queries for response size.
this is plausible if the backend is a traditional database but graphql is backend agnostic .a principled approach to graphql query cost analysis generality of static query analysis makes this an attractive approach for commercial graphql api providers.
our static approach follows a similar paradigm as existing static analyses but we differ in several ways we provide two distinct definitions of query complexity which is used to measure query cost our analysis can be configured to handle common schema conventions to produce better estimates we build our analysis on formal graphql semantics and prove the correctness of our query complexity estimates and we perform the first evaluation of such an analysis on real world apis.
overall our evaluation shows the benefits of a formal and configurable approach identifying shortcomings in existing static analyses.
a novel graphql formalization in this section we introduce graphql schemas and queries.
then we give a novel formalization of the semantics of query execution based on the graphql specification and reference implementation .
compared to hartig and p rez our semantics is more compact closer to the concrete graphql syntax and includes the context object for graphql convention aware static analysis.
.
graphql schemas and queries for a visual introduction to graphql queries see figure .
on the left is an excerpt of github s api s schema.
in the center is a sample query requesting a topic named graphql the names of two relatedtopics the totalcount of stargazers and the names of two stargazers.
the right side of the figure shows the server s response.
a graphql schema defines the data types that clients can query as well as possible operations on that data.
types can be scalars e.g.
int string enumerations object types defined by the provider e.g.
topic or lists e.g.
.
in addition there are also input types used to define object types for arguments.
each field of an object type is characterized by a name e.g.
relatedtopics and arguments used to constrain the matching data e.g.
first .
all schemas define a query operation type which contains fields that form the top level entry points for queries .
schemas may also define a mutation operation type which contains fields that allow queries to create update or delete data or a subscription operation type which provides event based functionality.
the syntax of a graphql query is as follows q label field args basic query ...on type q filter q q concatenation label field args q nesting abasic query label field args requests a specific field of an object with a list of named arguments args a1 v1 .
.
.
an vn.
for example topic name graphql in figure queries the topic field with the argument name graphql .
the label renames the result of the query with an arbitrary name.
in the graphql syntax label can be omitted if it matches the field and the list of arguments can be omitted if empty.
a simple field is thus a valid query.
3we omit some syntactic sugar of graphql constructs.
they can be expressed as combinations of our kernel elements.inline fragments ...on type q filter a query qon a type condition only executing query qfor objects of the correct type e.g.
...on starrable in figure .
a query can also concatenate fields q1q2 or request a sub field of an object via nesting with the form label field args q .
before execution graphql servers validate incoming queries against their schema.
.
query execution to support a schema a graphql server must implement resolver functions .
each field of each type in the schema corresponds to a resolver in the graphql backend.
the graphql runtime invokes the resolvers for each field in the query and returns a data object that mirrors the shape of the query.
the evaluation of a query can be thought of as applying successive filters to a virtual data object that initially corresponds to the complete data graph.
these filters follow the structure of the query and return only the relevant fields.
for example when executing the query in figure given the topic whose name is graphql the resolver for field relatedtopics returns a list of topic s and for each of these topic s the resolver for name returns a string .
the indirection of resolver functions makes the semantics of graphql agnostic to the storage of the data.
the data object is an access point populated e.g.
from a database or external service s that a resolver contacts.
a resolver must return a value of the appropriate type but the origin of that value is up to the implementation.
semantics.
formally figure defines the semantics of the kernel language as an inductive function over the query structure.
the formula jqk o ctx o means that the evaluation of query qon data object owith context ctxreturns an object o .
the context tracks information for use deeper in a nested query.
in our simplified semantics we track the type field and arguments of the parent.
querying a single field jlabel field args k o ctx calls a resolver function resolve o field args ctx which returns the corresponding field in object o. the response object contains a single field label populated with this value.
the interpetation the arguments args in the resolver is not part of the semantics it is left to the service developers.
a fragment j...on type q k o ctx only evaluates the subquery qon objects of the correct type typeof o returns the type of its operand .
in the example of figure the field stargazers is only present in the response if the topic is a starrable type.
querying multiple fields jq1q2k o ctx merges the returned objects collapsing repeated fields in the response into one.
a nested query jlabel field args q k o ctx is evaluated in two steps.
first resolve o field args ctx returns an object o .
the second step depends on the type of o .
ifo is a list the returned object contains a field label whose value is the list obtained by applying the sub query qto the all the elements o1 .
.
.
on with a new context ctx containing the type field and arguments list of the parent.
otherwise the returned object contains a field label whose value is the object returned by applying the sub query q ono in the new context ctx .
by convention the top level field of the response which corresponds to the query resolver has an implicit label data see e.g.
the response in figure .
4merge o1 o2 recursively merges the fields of o1ando2.alan cha erik wittern guillaume baudart james c. davis louis mandel and jim a. laredo schema query query type query topic name string topic type topic relatedtopics first int name string stargazers after string last int stargazerconnection type stargazerconnection totalcount int edges nodes type stargazeredge node user cursor string type user name string query topic name graphql relatedtopics first name ...on starrable stargazers last after y3... totalcount edges connections pattern node name cursor data topic relatedtopics name api name rest stargazers totalcount edges node name xxx cursor y3v... node name xxx cursor y3v... figure a graphql schema left with a sample query that uses the connections pattern center and response right .
jlabel field args k o ctx label resolve o field args ctx j...on type q k o ctx jqk o ctx iftypeof o type otherwise jq1q2k o ctx merge jq1k o ctx jq2k o ctx jlabel field args q k o ctx label ifo label jqk o ctx otherwise where o resolve o field args ctx andctx type typeof o field field args args figure semantics of graphql.
query complexity a graphql query describes the structure of the response data and also dictates the resolver functions that must be invoked to satisfy it which resolvers in what order and how many times .
we propose two complexity metrics intended to measure costs from the perspectives of a graphql service provider and a client resolve complexity reflects the server s query execution cost.
type complexity reflects the size of the data retrieved by a query.
graphql service providers will benefit from either measure e.g.
leveraging them to inform load balancing threat prevention resolver resource allocation or request pricing based on the execution cost or response size.
graphql clients will benefit from understanding the type complexity of a query which may affect their contracts with graphql services and network providers or their caching policies.
complexity metrics can be computed on either a query or its response.
for a query in .
we propose static analyses to estimate resolve and type complexities before its execution given minimal assumptions on the graphql server.
for a response resolve and type complexity are determined similarly but in terms of the fields and data in the response object.
the intuition behind our analysis is straightforward.
a graphql query describes the size and shape of the response.
with an appropriate formalization of graphql semantics an upper boundon resolve complexity and type complexity can be calculated using weighted recursive sums.
but unless it accounts for common graphql design practices the resulting bound may mis estimate complexities.
in .
we show this problem in existing approaches.
in the remainder of this section we describe two commonly used graphql pagination mechanisms.
if a graphql schema and query uses these mechanisms either explicitly .
or implicitly .
we can obtain a tighter and thus more useful complexity bound.
research reported that both of these conventions are widely used in real world graphql schemas so supporting them is also important for practical purposes.
.
graphql pagination conventions at the scale of commercial graphql apis queries for fields that return lists of objects may have high complexity e.g.
consider the very large cross product of all github repositories and users.
the official graphql documentation recommends that schema developers bound response sizes through pagination using slicing or the connections pattern .
graphql does not specify semantics for such arguments so we describe the common convention followed by commercial and open source graphql apis.
resolvers can return lists of objects which can result in arbitrarily large responses bounded only by the size of the underlying data.
slicing is a solution that uses limit arguments to bound the size of the returned lists e.g.
relatedtopics first in figure .
theconnections pattern introduces a layer of indirection for more flexible pagination using virtual edge andconnection types .
for example in figure left the field stargazers returns a single stargazerconnection allowing access to the totalcount of stargazers and the edges field returning a list of stargazeredge s. this pattern requires limit arguments to target children of a returned object e.g.
stargazers last in figure middle applies to the field edges .
the size of a list returned by a resolver can thus depend on the current arguments and the arguments of the parent stored in the context.
ensuring that limit arguments actually bound the size of the returned list is the responsibility of the server developers a principled approach to graphql query cost analysis assumption .
if the arguments list args or the context ctx contains a limit argument arg val the list returned by the resolver cannot be longer than the value of the argument val that is length resolve o field args ctx val .
if this assumption fails it likely implies a backend error.
pagination not panacea.
while slicing and the connections pattern help to constrain the response size of a query and the number of resolver functions that its execution invokes these patterns cannot prevent clients from formulating complex queries that may exceed user rate limits or overload backend systems.
our pagination aware complexity analyses can statically identify such queries.
.
configuration for pagination conventions as we discuss in ignoring slicing arguments or mis handling the connections pattern can lead to under or over estimation of a query s cost.
understanding pagination semantics is thus essential for accurate static analysis of query complexity.
since graphql pagination is a convention rather than a specification we therefore propose to complement graphql schemas with a configuration that captures common pagination semantics.
to unify this configuration with our definitions of resolve and type complexity we also include weights representing resolver and type costs.
here is a sample configuration for the schema from figure resolvers topic.relatedtopics limitarguments defaultlimit resolverweight topic.stargazers limitarguments limitedfields defaultlimit resolverweight 1types topic typeweight stargazer typeweight this configuration specifies pagination behavior for slicing and the connections pattern.
in this configuration resolvers are identified by a string type .field e.g.
topic.relatedtopics .
their limit arguments are defined with the field limitarguments .
for slicing the limit argument applies directly to the returned list see topic.relatedtopics .
for the connections pattern the limit argument s apply to children of the returned object see limitedfields for topic.stargazers .
the defaultlimit field indicates the size of the returned list if the resolver is called without limit arguments.
we must make a second assumption using javascript dot and bracket notation to access the fields of an object assumption .
if a resolver is called without limit arguments the returned list is no longer than the configuration c s default limit.
length resolve o field args ctx c.resolvers .defaultlimit in the following limit c type field args ctx returns the maximum value of the limit arguments for the resolver type .field if such arguments are present in the arguments list args or the context ctx and the default limit otherwise.
if a resolver returns unbounded lists the default limit can be set to but we urge service providers to always bound lists for their own protection.
from assumptions and we have property .
given a configuration cand a data object oof type t if the context ctx contains the information on the parent of o we have length resolve o field args ctx limit c t field args ctx .
concise configuration.
researchers have reported that many graphql schemas follow consistent naming conventions so we believe that regular expressions and wildcards are a natural way to make a configuration more concise.
for example the expression .
edge .nodes can be used for configuring resolvers associated with nodes fields within all types whose names end in edge.
or the expression user.
can be used for configuring resolvers for all fields within type user.
graphql query cost analysis in this section we formalize two query analyses to estimate the type and resolve complexities of a query.
the analyses are defined as inductive functions over the structure of the query mirroring the formalization of the semantics presented in .
we highlight applications of these complexities in .
.
like other static query cost analyses .
our analysis returns upper bounds for the actual response costs.
for example if a query asks for topic s our analysis will return as a tight upper bound on the response size.
if there are only topic s in the data graph our analysis will over estimate the actual query cost.
.
resolve and type complexity analyses as mentioned in we propose two complexity metrics resolve complexity and type complexity.
in this section we formalize the resolve complexity analysis and then explain how to adapt the approach to compute the type complexity.
we will work several complexity examples relying on the w1 0configuration a resolver weight of for fields returning object and list of object and for all other fields and the top level operation resolvers i.e.
query mutation subscription and a type weight of for all objects including those returned in lists and for all other types and the top level operation types i.e.
query mutation subscription .
resolve complexity.
resolve complexity is a measure of the execution costs of a query.
given a configuration c the resolve complexity of a response r rcx r c is the sum of the weights of the resolvers that are called to compute the response.
each resolver call populates a field in the response.
the resolve complexity of a response is thus the sum of the weights of each field.
with the w1 0configuration the resolve complexity of the response in figure right is 1topic 1relatedtopics 1stargazers 1edges 2nodes .
the query analysis presented in figure computes an estimate qrcx of the resolve complexity of the response.
each call to a resolver is abstracted by the corresponding weight and the sizes of lists are bounded using the limit function.
limit uses the limit argument if present otherwise it will use the default limit in the configuration.
the analysis is defined by induction over the structure of the query and mirrors the semantics presented in figure with one major difference the complexity analysis operates solely on the types defined in the schema starting from the top level query type.
the formula qrcx q c t ctx xmeans that given a configuration c the estimated complexity of a query qon a type twith a context ctxalan cha erik wittern guillaume baudart james c. davis louis mandel and jim a. laredo qrcx label field args c t ctx c.resolvers .resolverweight qrcx ...on type q c t ctx qrcx q c t ctx ift type otherwise qrcx q1q2 c t ctx qrcx q1 c t ctx qrcx q2 c t ctx qrcx label field args q c t ctx w l qrcx q c t ctx ift w qrcx q c t.field ctx otherwise where w c.resolvers .resolverweight andl limit c t field args ctx andctx type t field field args args figure resolve complexity analysis.
the analysis operates on the types defined in the schema starting from query .
isx n .
for example using the w1 0configuration the resolve complexity of the query in figure middle is also .
theorem .
given a configuration c the analysis of figure always returns an upper bound of the resolve complexity of the response.
for any query qover a data object o and using to denote the empty initial context qrcx q c query rcx jqk o c proof.
the theorem is proved by induction on the structure of the query.
the over approximation has two causes.
first limit returns an upper bound on the size of the returned list property .
if the underlying data is sparse the list will not be full the maximum complexity will not be reached.
second we express the complexity ofmerge o1 o2 as the sum of the complexity of the two objects.
this is accurate if o1ando2share no properties but if properties overlap then the merge will remove redundant fields cf.
.
.
type complexity.
type complexity is a measure of the size of the response object.
given a configuration c the type complexity of a response object r tcx r c is the sum of the weights of the types of all objects in the response.
using the w1 0configuration the type complexity of the response in figure right is 1topic related topic s 1stargazerconnection 2stargazeredge s 2users.
similar to our resolve complexity analysis qrcx figure our type complexity analysis bounds the response s type complexity without performing query execution.
we call the estimated query type complexity qtcx.
to compute qtcx we tweak the first and final rules from the qrcx analysis the call to a resolver is abstracted by the weight of the returned type t t .
qtcx label field args c t ctx c.types .typeweight .
when a nested query returns a list t the type complexity must reflect the cost of instantiating every element.
every element thus adds the weight of the returned type t to the complexity.
qtcx label field args q c t ctx l w qtcx q c t ctx where w c.types .typeweight .
with the w1 0configuration the type complexity of the query in figure is also .theorem .
given a configuration c the type complexity analysis always returns an upper bound of the type complexity of the response.
for any query qover a data object o and using to denote the empty initial context qtcx q c query tcx jqk o c the proof is similar to the proof of theorem .
.
time space complexity of the analyses the type and resolve complexity analyses are computed in one traversal of the query.
the time complexity of both analyses is thus o n where nis the size of the query as measured by the number of derivations required to generate it from the grammar of graphql.
both analyses need to track only the parent of each sub query during the traversal.
this implies that the space required to execute the analyses depends on the maximum nesting of the query which is at worst n. the space complexity is thus in o n .
we emphasize that these are static analyses they do not need to communicate with backends.
.
mutations and subscriptions so far we have considered only graphql queries.
graphql also supports mutations to modify the exposed data and subscriptions for event based functionality .
our resolve complexity approach also applies to mutations reflecting the execution cost of the mutation.
api providers can use resolve weights to reflect costly mutations in resolve complexity calculations.
however our type complexity approach only estimates the size of the returned object ignoring the amount of data modified along the way.
assuming the user provides the new data computing the type complexity of arguments passed to mutation resolvers may give a reasonable approximation.
we leave this for future work.
our analysis can also produce resolve and type complexities for subscription queries.
policies around subscriptions may differ though.
for rate limiting for example the api provider could reduce the remaining rates based on complexities when a subscription is started and replenish them once the client unsubscribes.
evaluation we have presented our query analysis and proved its accuracy.
in our evaluation we consider five questions rq1 can the analysis be applied to real world graphql apis especially considering the required configuration?
rq2 does the analysis produce cost upper bounds for queries to such apis?
rq3 are these bounds useful i.e.
close enough to the actual costs of the queries for providers to respond based on the estimates?
rq4 is our analysis cheap enough for use in api management?
rq5 how does our approach compare to other solutions?
our analysis depends on a novel dataset .
.
we created the first graphql query response corpus for two reputable publicly accessible graphql apis github and yelp .
table summarizes these apis using the metrics from .
to answer rq1 we discuss configuring our analysis for these apis .
.
to answer rq2 rq4 we analyzed the predicted and actual complexities in our query response corpus .
.
for rq5 wea principled approach to graphql query cost analysis table characteristics of the evaluated apis.
schema github yelp number of object types total fields on all object types lines of code loc pagination w. slicing arguments yes yes pagination w. connections pattern yes yes configuration github yelp number of default limits loc of schema loc .
.
compare our findings experimentally to open source static analyses and qualitatively to closed source commercial approaches .
.
.
a graphql query response corpus answering our research questions requires a multi api queryresponse corpus.
none existed so we created one.
we automatically generated queries for github and yelp and collected 000unique query response pairs each from april to march .
general approach.
we developed and open sourced a graphql query generator .
its input is a graphql schema and usersupplied parameters.
following standard grammar based input generation techniques it generates queries recursively and randomly by building an abstract syntax tree and rendering it as a graphql query string.
the depth probability dictates the likelihood of adding nested fields.
the breadth probability controls the likelihood of adding additional adjacent fields into each query level.
providing argument values.
the ast generated by this approach includes argument variables that must be concretized.
we populate these values using a provider map that maps a graphql argument to functions that generate valid values.
queries for github and yelp.
we tuned the query generation to our experimental design and context.
we used breadth and depth probabilities of .
.
for numeric arguments we used normally distributed integer values with mean and variance of .
github and .
yelp .5forenum arguments we randomly chose a valid enum value.
for named arguments e.g.
github projects yelp restaurants we randomly chose a valid entity e.g.
the100moststarred github repositories.
for context dependent arguments we used commonly valid values e.g.
readme.md as a github filepath.
ethically because we are issuing queries against public apis we omitted overly complex queries and queries with side effects.
we issued queries with estimated type or resolve complexity nesting depth and query not mutation or subscription operations.
query realism.
our query generation algorithm yielded diverse queries for each api.
to assess their realism we compared them to example queries provided by each api.
yelp did not provide any examples but github s documentation includes example queries .
we analyzed them using the w1 0configuration cf.
.
to determine the size of a typical query.
only 14could be analyzed using the same github schema version.
of these all but 5yelp has a maximum nesting depth.
larger values yielded complex shallower queries.one had a type complexity of 302or less.
therefore we used a type complexity of 300to categorize typical queries .
.
rq1 configuration our aim is a backend agnostic query cost analysis.
as prior work cannot be applied to estimate query cost for arbitrary graphql servers we first assess the feasibility of our approach.
per .
our analysis requires graphql api providers to configure limit arguments weights and default limits.
we found it straightforward to configure both apis.
our configurations are far smaller than the corresponding schemas .
limit arguments.
github and yelp uses consistent limit arguments names first and last for github limit for yelp which we configured with regular expressions.
github has a few exceptional limit arguments such as limit on the gist.files field which we configured with both strings and regular expressions.
following the conventions of the connections pattern we set all connection types to have the limited fields edges and nodes for github.
github also has a few fields that follow the slicing pattern so we did not set any limited fields for these.
the yelp api strictly follows the slicing pattern so no additional settings were required.
weights.
for simplicity we used a w1 0configuration cf.
.
.
this decision permitted us to compare against the open source static analysis libraries but may not reflect the actual costs for graphql clients or service providers.
for example we set the type weights for scalars and enums to supposing that these fields are paid for by the resolvers for the containing objects.
default limits.
we identified default limits for the fields github and fields yelp .
these fields are unpaginated lists or lists that do not require limit arguments.
we determined these numbers using api documentation and experimental queries.
.
rq2 rq4 complexity measurements using this configuration we calculated type and resolve complexities for each query response pair in the corpus.
figure summarizes the results for yelp and github using heat maps.
each heat map shows the density of predictions for request response complexity cells.
cells above the diagonal are queries whose actual complexity we over estimated cells below the diagonal are under estimates.
.
.
rq2 upper bounds.
in figure all points lie on or above the diagonal.
in terms of our analysis every query s predicted complexity is an upper bound on its actual complexity.
this observation is experimental evidence for the results theorized in .
.
as an additional note the striations that can be seen in figure 5b are the result of fields with large default limits.
queries that utilize these fields will have similar estimated complexities.
.
.
rq3 tight upper bounds.
answering rq2 we found our analysis computes upper bounds on the type and resolve complexities of queries.
other researchers have suggested that these cost bounds may be too far from the actual costs to be actionable .
our data show that the bounds are tight enough for practical purposes and are as tight as possible with a static data agnostic approach.
figure indicates that our upper bound is close or equal to the actual type and resolve complexities of many queries this can be seen in the high density of queries near the diagonals.alan cha erik wittern guillaume baudart james c. davis louis mandel and jim a. laredo a yelp type complexities actual response cxty0150300pred.
query cxty resp.
of preds.
b yelp resolve complexities actual response cxty0100200pred.
query cxty resp.
of preds.
c github type complexities actual response cxty0300600pred.
query cxty resp.
of preds.
d github resolve complexities actual response cxty0150300pred.
query cxty resp.
of preds.
figure actual response complexities and predicted query complexities using our analysis on the corpus.
each figure has text indicating the percentage of responses that are shown the remainder exceed the x axis and the percentage of the corresponding predictions that are shown the remainder exceed the y axis .
table over estimation of our type and resolve complexities.
the first table summarizes our approach on all queries.
the second shows the results for typical queries i.e.
those with type complexities of up to cf.
.
.
yelp github all qeries resolve type resolve type underestimation none none none none no overestimation .
.
.
.
overestimation .
.
.
.
overestimation .
.
.
.
yelp github typical qeries resolve type resolve type underestimation none none none none no overestimation .
.
.
.
overestimation .
.
.
.
overestimation .
.
.
.
our bounds are looser for more complex queries.
this follows intuition about the underlying graph larger more nested queries may not be satisfiable by an api s real data.
data sparsity leads responses to be less complex than their worst case potential.
table quantifies this observation.
it shows the share of queries for which our predictions over estimate the response complexity by and .
over estimation is less common for the subset of typical queries whose estimated type complexity is .however per the proofs in .
our upper bounds are as tight as possible without dynamic response size information.
the overestimates for larger queries are due to data sparsity not inaccuracy.
for example consider this pathological query to github s api query organization login nodejs repository name node issues first nodes repository issues first nodes ... this query cyclically requests the same repository and issues .
with two levels of nesting the query complexities are resolve and type .
if the api data includes at least issues the response complexities will match the query complexities.
.
.
rq4 performance.
beyond the functional correctness of our analysis we assessed its runtime cost to see if it can be incorporated into existing request response flows e.g.
in a graphql client or an api gateway.
we measured runtime cost on a macbook pro core intel i7 processor gb of memory .
as predicted in .
our analysis runs in linear time as a function of the query and response size.6the median processing time was .0ms for queries and .1ms for responses.
even the most complex inputs were fairly cheap of the queries could be processed in .3ms and of the responses in 4ms.
the open source analyses we consider in .
also appear to run in linear time.
.
rq5 comparison to other static analyses in this section we compare our approach to state of the art static graphql analyses .
.
to permit a fair comparison across different notions of graphql query cost we tried to answer two practical bound questions for the github api with each approach.
bq1 how large might the response be?
bq2 how many resolver functions might be invoked?
bq1 is of interest to clients and service providers who both pay the cost of handling the response.
various interpretations of large are possible so we operationalized this as the number of distinct objects non scalars in the response.
bq2 is of interest to service providers who pay this cost when generating the response.
we configured and compared our analysis against three opensource analyses experimentally with results shown in figure .
the static graphql analyses performed by corporations are not publicly available so we discuss them qualitatively instead.
.
.
configuring our analysis to answer bq1 and bq2.
our measure of a query s type complexity can answer bq1.
using the w1 configuration cf.
.
the type complexity measures the maximum number of objects that can appear in a response i.e.
response size.
our measure of a query s resolve complexity is suitable for answering bq2.
we assume that the cost of a resolver for a scalar or an enum field is paid for by some higher level resolver.
thus we again configured our analysis for github using the w1 0configuration from .
.
the resolve complexity resulting from this configuration will count each possible resolver function execution once.
6we define query size as the number of derivations required to generate the query from the grammar presented in .
.
it can be understood as the number of lines in the query if fields inline fragments and closing brackets each claim their own line.a principled approach to graphql query cost analysis a our github complexities actual response size0300600pred.
response size resp.
of preds.
b liba s github complexities actual response size0300600pred.
response size resp.
of preds.
c libb s github complexities actual response size0300600pred.
response size99 resp.
of preds.
d libc s github complexities actual response size0300600pred.
response size99 resp.
of preds.
figure bq1 actual and predicted response sizes based on type complexity from our analysis and the libraries on the github data.
libb and libc produce identical values under our configuration.
all static approaches over estimate due to data sparsity.
the libraries have sources of over estimation beyond our own.
libb and libc also under estimate cells below the diagonal .
.
.
comparison to open source static analyses.
we selected open source libraries for comparison.
we describe their approaches in terms of our complexity measures configure them to answer the questions as best as possible and discuss their shortcomings.
library selection.
we considered analyses that met three criteria existence they were hosted on github discoverable via searches ongraphql static cost query cost analysis complexity relevance they statically compute a query cost measure and quality they had 10stars a rough proxy for quality .
three libraries met this criteria liba libb and libc .
understanding their complexity measures.
at a high level each of these libraries defines a cost measure in terms of the data returned by each resolver function invoked to generate a response.
users must specify the costof the entity type that it returns and a multiplier corresponding to the number of entities.
these libraries then compute a weighted recursive sum much as we do.
problematically these libraries do not always permit users to specify the sizes of lists leading to over and under estimates.
in terms of the analysis from .
the list field size lcannot always be obtained see qtcx last rule above theorem .
we discuss the details below.
configuring the libraries to answer bq1.
we summarize our approach here.
our artifact includes the configuration details .
liba we set the cost of all objects to 1and all other types to .
the library allows the maximum size of a list to be set.
therefore we set the maximum sizes of unpaginated lists using values identified in .
.
unfortunately the library cannot use arguments and consequently cannot take advantage of pagination.
to configure paginated lists we instead set their maximum sizes to github s maximum list size .
libb as with liba we set the cost of objects to 1and other types to .
we also leveraged libb s partial pagination support.
we configured it to use the limit arguments of paginated lists.
however libb treats unpaginated lists are treated as a single non list entity.
it also does not support the connections pattern.
libc libb and libc appear to be related.
their support for bq1 is equivalent and we configured them similarly.
comparing outcomes on bq1.
figure illustrates the effectiveness of each approach when answering bq1.
the same query response corpus was used in each case and the response sizes were calculated using the method discussed in .
the variation is on the y axis the predicted response size estimated from the query.
as impliedby our proofs of correctness our approach consistently provides an upper bound on the actual response size no under estimation .
as mentioned liba cannot use arguments as limits.
configuring all paginated lists to have a maximum list size equal to github s maximum list size resulted in significant over estimation.
additionally this configuration caused in the striations in figure 6b which lie at intervals of the maximum list size queries fall into bands based on the number of paginated lists they contain.
in contrast to the striations found in figure 5b these fields nest within each other allowing for repeated and regularly spaced stripes.
to illustrate the overestimation about .
of liba s predictions were more than double the actual response size.
in contrast of our analysis s predictions over estimate less than cf.
table .
furthemore the median over estimation of our analysis i.e.
median error is whereas that of liba is .
the percentile over estimation of our analysis is .
and that of liba is .
.
as a result our analysis conclusively performs better than liba.
libb and libc both over estimate andunder estimate.
because they do not support default limits and treat unpaginated lists as a single non list entity they are prone to under estimation.
the under estimation can compound geometrically when multiple unpaginated lists are nested.
about of their predictions were under estimations.
additionally because they do not support the connections pattern style of pagination and do not properly utilize limit arguments in these cases they are prone to over estimation.
in any case because libb and libc can under estimate they do not reliably produce upper bounds which is a problem in security critical contexts.
in contrast our analysis consistently produces upper bounds and notably tight upper bounds.
because our analysis is not at risk of this security problem our analysis also performs better than libb and libc.
configuring the libraries to answer bq2.
we were unable to configure these libraries to answer bq2.
fundamentally liba libb and libc measure costs in terms of the entities returned by the query not in terms of the resolvers used to obtain the results.
trying to apply the multipliers to count resolvers will instead inflate them.
the trouble is illuminated by our resolve complexity analysis figure in the first clause of the final rule the resolver multiplier should be used to account for the luses of each child resolver but the parent should be counted just once.
in contrast when answering bq1 the type multiplier should be applied to both a field and its children.alan cha erik wittern guillaume baudart james c. davis louis mandel and jim a. laredo this finding highlights the novelty of our notion of resolve complexity.
we believe its ability to answer bq2 also shows its utility.
.
.
comparison to closed source analyses.
github and yelp describe their analyses in enough detail for comparison.
github s analysis can approximate bq1 and bq2 while yelp s cannot.
github.
github s graphql api relies on two static analyses for rate limiting and blocking overly complex queries prior to executing them .
both analyses support pagination via the connections pattern as described in .
.
for bq1 their node limit analysis disregards types associated with the connections pattern.7we can replicate this behavior with our analysis by setting the weights of types associated with the connections pattern to 0and1otherwise.
for bq2 their call s score analysis only counts resolvers that return connection types.
we can also replicate this behavior by setting a weight of 1to these resolvers and 0otherwise.
in any case because github s metrics cannot be weighted they cannot distinguish between more or less costly types or resolvers.
github s focus on the connections pattern may have caused them issues in the past.
when our study began github shared a shortcoming with libb and libc it did not properly handle unpaginated lists which would not employ the connections pattern .
we demonstrated the failure of their approach in figure .
we reported this possible denial of service vector to github.
they confirmed the issue and have since patched their analysis.
yelp.
yelp s graphql api analysis has both static and dynamic components.
statically yelp s graphql api rejects queries with more than four levels of nesting.
this strategy bounds the complexity of valid queries but expensive queries can still be constructed with this restriction using large nested lists.
dynamically yelp then applies rate limits by executing queries and retroactively replenishing the client s remaining rate limits according to the complexity of the response.
it is therefore possible to significantly exceed yelp s rate limits by submitting a complex query when a client has a small quota remaining.
using our type complexity analysis yelp could address this problem by rejecting queries whose estimated complexities exceeded a client s remaining rates.
discussion and related work configuration and applicability.
our experiments show that our analysis is configurable to work with two real world graphql apis.
applying our analysis was possible because it is static i.e.
it does notdepend on any interaction with the graphql apis or other backend systems.
this contrasts with dynamic analyses which depend on probing backends for list sizes .
our analysis is more broadly applicable and can be deployed separately from the graphql backend if desired e.g.
in api gateways cf.
.
the static approach carries greater risk of over estimation however and api providers may consider a hybrid approach similar to github s a static filter then dynamic monitoring.
we have identified three strategies for managing over estimation.
first an unpaginated list field may produce responses with a wide range of sizes leading our approach to overestimate.
schema designers may respond by paginating the list which will bound the 7a possible explanation virtual constructs like edge andconnection types may not significantly increase the amount of effort to fulfill a query.degree of overestimation.
second in our tests we used the w1 0configuration which assigned all types and resolvers the same weights.
in contexts where different data and resolvers carry different costs schema designers can tune the configuration appropriately.
lastly service providers may resort to a hybrid static dynamic system to leverage the graph data at runtime.
the design of such a system is a topic for further research.
the value of formalization.
our formal analysis gives us provably correct bounds provided that list sizes can be obtained from an analysis configuration.
this contrasts with the more ad hoc approaches favored in the current generation of graphql analyses used by practitioners.
a formal approach ensured that we did not miss corner cases as in the unpaginated list entities missed by libb libc and github s internal analysis.
although our formalisms are not particularly complex they guarantee the soundness and correctness missing from the state of the art.
data driven software engineering.
our approach benefited from an understanding of graphql as it is used in practice specifically the use of pagination and naming conventions.
although pagination is not part of the graphql specification we found that the graphql grey literature emphasized the importance of pagination.
a recent empirical study of graphql schemas confirmed that various pagination strategies are widely used by practitioners .
we therefore incorporated pagination into our formalization viz.
that list sizes can be derived from the context object and supported both of the widely used pagination patterns in our configuration.
this decision differentiates our analysis from the state of the art enabling us to avoid common sources of cost under and overestimation.
in addition the prevalence of naming conventions in graphql schemas inspired our support for regular expressions which allowed our configuration answer bq1 and bq2 remarkably concisely.
in contrast the libraries we used required us to manually specify costs and multipliers for each of the hundreds of github schema elements they did not scale well to real world schemas.
bug finding.
one surprising application of our analysis was as a bug finding tool.
when we configured yelp s api we assumed that limit arguments would be honored assumption .
in early experiments we found that yelp s resolver functions for the query.reviews and business.reviews fields ignore the limit argument.
yelp s engineering team confirmed this to be a bug.
this interaction emphasized the validity of our assumptions.
database query analyses.
there has been significant work on query cost estimation for database query languages to optimize execution.
our analysis is related to the estimation of a database query s cardinality and cost .
however typical sql servers routinely optimize queries by reordering table accesses which makes static cost evaluation challenging .
in comparison our analysis takes advantage of the limited expressivity of graphql and the information provided in the schema e.g.
via pagination mechanism to guarantee robust and precise upper bounds before execution.
application example api gateway we designed our graphql query cost analysis as a building block for a graphql api gateway that offers api management for graphql backends figure .
we worked with ibm s product division to implement a graphql api gateway based on our ideas.
followinga principled approach to graphql query cost analysis figure screenshot of the configuration gui in ibm s datapower api gateway.
the warnings indicate incomplete configuration and make recommendations.
patterns for api gateways for rest like apis this gateway is backend agnostic made possible by our data and backendindependent query cost analysis.
this gateway was incorporated into v10.
.
of ibm s api connect and datapower products .
a weakness of our approach is the need for configuration.
during productization we explored two ways to support this task a graphical user interface gui and automatic recommendations.
the gateway automatically ingests the backend s schema using introspection .
users can then configure using the gui depicted in figure .
they can manually configure fields with weights limit arguments and or default limits either one at a time or bulk apply to all types fields matching a search.
to mitigate the security risks of schemas with nested structures the gui automatically identifies some problematic fields and proposes an appropriate configuration based on schema conventions.
for example it flags fields that return lists and infers possible configurations based on type information.
threats to validity construct validity.
our study does not face significant threats to construct validity.
we believe our definitions of type complexity and resolve complexity are useful.
we do not rely on proxy measures but rather measure these complexities directly from real queries.
internal validity.
these threats come from configuration and query realism.
in our evaluation we created configurations for the github and yelp apis.
errors would affect the accuracy of our bounds.
our evaluation showed that we did not make errors leading to under estimation but we may have done so for over estimation.
although rq2 showed that our analysis produces upper bounds in rq3 our conclusions about the practicality of our bounds rely on the realism of our query response corpus.
our evaluation is basedon randomly generated queries parameterized as described in .
.
some of our queries used popular projects which are more likely to have associated data decreasing over estimates from data sparsity .
other queries lacked contextual knowledge and may result in unnatural queries unlikely to be filled with data.
to avoid harming the public api providers we bounded the complexity of the queries we issued and this may have skewed our queries to be smaller than realistic queries.
we plan to pursue a more realistic set of queries e.g.
obtained through collaboration with a graphql api provider or by mining queries from open source software.
external validity.
our work makes assumptions about the properties of graphql schemas and backend implementations that may not hold for all graphql api providers.
for example the complexity calculations depend on the presence of slicing arguments in queries on resolver function implementations to enforce these limits and on a proper configuration.
by relying on default limits .
we enable our analysis to function even if slicing arguments are not enforced in parts of a schema.
we demonstrated that proper configuration is possible even when treating the backend as a grey box as we did when evaluating on the github and yelp apis .
.
conclusion graphql is an emerging web api model.
its flexibility can benefit clients servers and network operators.
but its flexibility is also a threat graphql queries can be exponentially complex with implications for service providers including rate limiting and denial of service.
the fundamental requirement for service providers is a cheap accurate way to estimate the cost of a query.
we showed in our evaluation that existing ad hoc approaches are liable to both over estimates and under estimates.
we proposed instead a principled approach to address this challenge.
grounded in a formalization of graphql semantics in this work we presented the first provably correct static query cost analyses.
with proper configuration our analysis offers tight upper bounds low runtime overhead and independence from backend implementation details.
we accompany our work with the first graphql query response corpus to support future research.
reproducibility an artifact containing the graphql query generator the queryresponse corpuses library configurations and corpus measurements can be found here institutional policy precludes sharing our analysis prototype.