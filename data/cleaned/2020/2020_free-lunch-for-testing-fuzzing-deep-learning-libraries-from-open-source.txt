free lunch for testing fuzzing deep learning libraries from open source anjiang wei stanford university anjiang stanford.eduyinlin deng university of illinois at urbana champaign yinlind2 illinois.edu chenyuan yang nanjing university cy1yang outlook.comlingming zhang university of illinois at urbana champaign lingming illinois.edu abstract deep learning dl systems can make our life much easier and thus are gaining more and more attention from both academia and industry.
meanwhile bugs in dl systems can be disastrous and can even threaten human lives in safety critical applications.
todate ahugebodyofresearcheffortshavebeendedicatedtotestingdlmodels.however interestingly thereisstilllimitedwork fortestingtheunderlyingdllibraries whicharethefoundation for building optimizing and running dl models.
one potentialreasonisthattestgenerationfortheunderlyingdllibrariescanberatherchallengingsincetheirpublicapisaremainlyexposedin python making it even hard to automatically determine the api input parameter types due to dynamic typing.
in this paper we propose freefuzz the first approach to fuzzing dl libraries viaminingfromopensource.morespecifically freefuzzobtains code modelsfromthreedifferentsources codesnippetsfromthelibrarydocumentation librarydevelopertests and3 dlmodels in the wild.
then freefuzz automatically runs all the collected code modelswithinstrumentationtotracethedynamicinformationforeachcoveredapi includingthetypesandvaluesofeach parameterduringinvocation andshapesofinput outputtensors.
lastly freefuzz will leverage the traced dynamic information to perform fuzztesting foreach coveredapi.
theextensive studyof freefuzz on pytorch and tensorflow two of the most popular dl libraries showsthatfreefuzzisabletoautomaticallytracevalid dynamic information for fuzzing popular apis 9x more than state of the artlemonwith3.5xloweroverheadthanlemon.
todate freefuzzhasdetected49bugsforpytorchandtensorflow with already confirmed by developers as previously unknown .
acm reference format anjiang wei yinlin deng chenyuan yang and lingming zhang.
.
freelunchfortesting fuzzingdeep learninglibrariesfromopensource.
in44th international conference on software engineering icse may the work was done during a remote summer internship at university of illinois.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
pittsburgh pa usa.
acm new york ny usa pages.
https introduction deeplearning dl hasbeenplayingasignificantroleinvarious applicationdomains includingimageclassification natural languageprocessing gameplaying andsoftwareengineering .throughsuchapplications dlhassubstantially improved our daily life .
the great success achieved by dl is attributed to the proposal of more and more advanced dl models the availability of large scale datasets and inevitably thecontinuousdevelopmentofdllibraries.meanwhile deploying a dl model without thorough testing can be disastrous in safety critical applications.
for example a critical bug in the dl systeminuber sself drivingcarshasunfortunatelytakenthelifeof a pedestrian .
due to the popularity of dl models and the critical importance of their reliability a growing body of research efforts havebeen dedicated to testing dl models with focus on adversarial attacks for model robustness the discussion on variousmetricsfordlmodeltesting andtesting dlmodelsforspecificapplications .meanwhile both runningand testingdlmodelsinevitably involvethe underlying dl libraries which serve as central pieces of infrastructures for building training optimizing and deploying dl models.
for example the popular pytorch and tensorflow dl libraries with 50k and159kstarsongithub arebyfartwoofthemostpopulardl libraries for developing and deploying dl models.
surprisingly despitetheimportanceofdllibraryreliability thereisonlylimited work for testing dl libraries to date.
for example cradle leverages existing dl models for testing keras and its backends andresolvesthetestoracleproblemviadifferentialtesting.
later lemon further augments cradle via leveraging variousmodelmutationrulestogeneratemorediversedlmodelsto invokemorelibrarycodetoexposemorepossibledllibrarybugs.
despitetheirpromisingresults existingworkontestingdllibraries suffers from the following limitations.
firstly only limited sourcesfortestinputgenerationareconsidered.forexample cra dle uses30pre traineddlmodelsandlemon usesonly 12dlmodels.ourlaterempiricalresultsshowthattheycanatmost cover59apisfortensorflow leavingadisproportionatelylarge number of apis uncovered by such existing techniques.
secondly thestate of the artmodelmutationtechniqueproposedbylemoncanberatherlimitedforgeneratingdiversetestinputsfordlapis.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anjiang wei yinlin deng chenyuan yang and lingming zhang forexample theintact layermutation requiresthattheoutput tensor shape of the layer api to be added deleted should be identicaltoitsinputtensorshape.asaconsequence onlyafixedpattern of argument values for a limited set of apis are explored in modellevelmutation whichsubstantiallyhindersitsbug findingabilities.
thirdly model level testing can be rather inefficient.
the inputs for the original mutated models are obtained from the external dl datasets and each of them will need to be completely executed end to endtogetthefinalpredictionresultsfordifferentialtesting whichcanconsumehoursevenforasinglemutatedmodel.besides it requires an additional bug localization procedure to find the specific apiinvocation causing the inconsistenciesbetween different backendsintheoriginal mutateddlmodels.duringlocalization carefully designed metrics are required to eliminate false positives.
the false positives can be due to uncertainty and variances e.g.
floating pointprecisionloss indlapis furtheramplifiedinthe model level testing scenario.
in this work we overcome the aforementioned limitations for testingdllibrariesviafullyautomatedapi levelfuzztesting.comparedwithpriormodel leveldllibrarytestingwhichresembles systemtesting api leveltestingismorelike unittesting whichisat amuchfiner grainedlevel.thebenefitofapi leveltestingisthatit can be a more general and systematic way for testing dl libraries.
with api instrumentation we can get various and diverse input sourcesfromopensourcetoservethepurposeoftesting.moreover api level mutation is free of unnecessarily strict constraints on mutation compared with model level mutation.
besides api level mutationneitherdependsoniteratingoverexternaldatasets nor requirescomplexlocalization procedures sincetestingoneapi ata time does not incur accumulated floating point precision loss.
one main challenge that we resolve for api level fuzz testing of dllibrariesisfullyautomatedtestinputgenerationfordlapis.
thepublicapisindllibrariesaremainlyexposedinpython makingitdifficulttoautomaticallydeterminetheinputparametertypes due to dynamic typing.
to this end we propose freefuzz the first approach to fuzzing dl libraries via mining from actual model andapiexecutions.morespecifically weconsiderthefollowing sources codesnippetsfromthelibrarydocumentation library developer tests and dl models in the wild.
freefuzz records the dynamic information for all the input parameters for each invoked api on the fly while running all the collected code models.
the dynamicinformation includesthe types values ofthe arguments andtheshapesoftensors.thetracedinformationcanthenforma value space for each api and an argument value space where values can be shared across arguments of similar apis during testing.lastly freefuzzleveragesthetracedinformationtoperform mutation basedfuzzingbasedonvariousstrategies i.e.
typemutation random value mutation and database value mutation and detectsbugswithdifferentialtestingandmetamorphictestingon differentbackends.ourinitialevaluationoffreefuzzonpytorch and tensorflow shows that freefuzz can automatically trace valid dynamic information for fuzzing out of all considered apis whilestate of the arttechniquescanatmostcover59apis for tensorflow .
to date we have submitted bug reports detected by freefuzz to developers with already confirmed by developers as previously unknown bugs and already fixed to date.in summary our paper makes the following contributions dimension.
this paper opens a new dimension for fully automatedapi levelfuzzingofdllibrariesviaminingfrom actual code and model executions in the wild.
technique.
we implement a practical api level dl library fuzzing technique freefuzz which leverages three different input sources including code snippets from library docu mentation library developer tests and dl models in the wild.freefuzztracesthedynamicapiinvocationinformation of all input sources via code instrumentation for fuzz testing.
freefuzz also resolves the test oracle problem with differential testing and metamorphictesting.
study.our extensive study on the two most popular dl libraries pytorch and tensorflow shows that freefuzz can successfully trace1158 out of 2530apis and effectivelydetect bugs with already confirmed by developers as previously unknown and already fixed.
background .
preliminaries for deep learning libraries figure example dl library pytorch inthissection wewillgiveabriefintroductiontothepreliminaries of deep learning libraries based on pytorch .
training and inference.
as shown on the left hand side of figure developers usually leverage dl libraries to support the trainingandinferencetasksondeepneuralnetworks dnns .conceptually dnns are composed of multiplelayers which are what the adjective deep in deeplearning refersto.
in themodel definition part of figure conv2dandmaxpool2d are the apis invoked to add twolayersintotheexamplednn.thenthe forwardfunctiondefines how the input data should flow in the defined layers.
before the actual training and inference the datasets should also be loaded with necessary pre processing e.g.
torchvision.transforms.normalize isacrucialstepindatapre processing whichaimstorescalethe values of input and target variables for better performance.
trainingistheprocessforadnntolearnhowtoperformatask withitsweightsupdatedalongtheway .forexample forimage classification by feeding a dnn with known data and corresponding labels we can obtain a trained dl model.
training a dl model involves iterating over the dataset defining a loss function e.g.
torch.nn.crossentropyloss tocalculatethedifferencebetweenthe network outputs and its expected outputs according to the labels authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
free lunch for testing fuzzing deep learning libraries from open source icse may pittsburgh pa usa figure2 theapidefinitionfor2d convolutioninpytorch and updating the weights of the dnn via a back propagation procedure i.e.
loss.backward .differentfromthetrainingphase inference is the process of using a trained dl model to complete a certain task with its weights unchanged e.g.
making predictions against previously unseen data based on the trained model.
abstraction for hardware.
shown on the right hand side of figure dl libraries such as pytorch and tensorflow usually provide a unified abstraction for different hardware which can be easily configured by the end users to perform the actual execution.
theyusuallyintegratedifferentbackendsindllibrariesforflexibility and performance.
take pytorch as an example aten is a backend implemented in c serving as a tensor library for hundreds of operations.
it has specialized low level implementationsforhardwareincludingbothcpusandgpus.besidesaten cudnn is another backend integrated into pytorch which isawidely usedthird partyhigh performancelibrary developed specificallyfordeeplearningtasksonnvidiagpus.furthermore asshowninfigure1 pytorchnownotonlysupportsgeneral purpose devices such as cpus and gpus but also allows users to run dl modelsonmobiledevicesduetothegrowingneedtoexecutedl models on edge devices.
.
fuzzing deep learning libraries as shown in the previous subsection hundreds or even thousands of apis are implemented in a typical dl library to support various tasks.
therefore it is almost impossible to manually construct test inputsforeachapi.meanwhile mostpublicapisfromdllibraries areexposedinpythonduetoitspopularityandsimplicity which makesitextremelychallengingtoautomaticallygeneratetestinputs given the api definitions.
the reason is that we cannot determine the types of the input parameters statically because python is a dynamicallytypedlanguage.
taketheoperator2d convolutionfrompytorchasanexample thedefinitionofwhichisshowninfigure2 a snapshot captured from pytorch official documentation .
from the definition of 2d convolution shown figure we do not know whattypesofparameters in channels out channels kernel size are.also onemayconcludethatparameter stridemustbeaninteger inferredfromthedefaultvalue stride andparameter padding must also be an integer inferred from the default value padding .
however this is not the case actually.
the documentation below not included in figure due to space limitations says that stride controls the stride for the cross correlation a single number or a tuple and padding controls the amount of padding applied to the input.
it can be either a string valid same or a tuple of ints giving theamountofimplicitpaddingappliedonbothsides .infact the parameters kernel size stride padding dilation canbeeitherasingleintor atuple of two ints and paddingcan even be a string besides the two types mentioned above.
therefore there can exist multiple valid types for a specific argument and the valid types of arguments cannot be easily inferred from the definition.
due to the above challenge of test generation for dl apis cradle proposes to directly leverage existing dl models totestdllibraries.theinsightofcradleistocheckthecrossimplementation inconsistencies when executing the same dl models on different backends to detect dl library bugs.
it uses models and datasets.
after detecting inconsistencies when executing models between differentbackends by feeding the input from datasets a confirmation procedure to identify bug revealing inconsistencies and a localization procedure to precisely localize the sourceoftheinconsistencieshavetobelaunched.insuchmodelleveltesting whereinconsistenciescanbeeitherduetorealbugsor accumulatedfloatingpointprecisionlosspropagatedthroughthe executionofmultipleapis carefullydesignedmetricsareneeded to distinguish real bugs from false positives.
furthermore such model leveltestingtechniqueonlycoversalimitedrangeofapis indllibraries e.g.
allmodelsusedbycradleonlycover59apis for tensorflow.
basedoncradle lemon advancestestingdllibraries by proposing model level mutation.
a set of model level mutation rules are designed to generate mutated models with the goal of invoking more library code.
model level mutation is composed of intact layer mutation and inner layer mutation.
the intact layermutation rules pose very strict constraints on the set of apis tobe mutated and the arguments passed to them.
as stated in the lemonpaper oneexplicitconstraintforintact layermutation is that the output shape of the api to be inserted and the input shape of it must be identical.
as a result only a limited set of apis with fixed parameters can used for mutation in order to meet such constraints whichsubstantiallyhinderslemon sabilityinbugfinding.
moreover selecting such apis with specific arguments for layer levelmutationrequiresexpertknowledgeoftheinput output relation of each api.
for example only a limited range of apis e.g.
convolution linear and pooling with fixed parameters can be added or deleted during model level mutation.
according to our later study lemon s various mutation rules can only help cover more apis in total for all the studied models.
approach figure3showstheoverviewofourapproach freefuzz whichis mainly composed of four different stages.
the first stage is codecollection section3.
.asshowninthefigure freefuzzobtainscode from three different sources code snippets from library documentation librarydevelopertests and3 variousdlmodelsinthewild allofwhichcanbeobtainedfromopensourceautomatically.thesecondstageisdynamictracingwithinstrumentation section .
.
freefuzz first hooks the invocation of apis and then executes the code collected in the first stage to trace various dy namic execution information for each api invocation including value and type information for all parameters of all executed apis.
as a result of this stage freefuzz constructs the type space api valuespace andargumentvaluespaceforthelaterfuzzingstage.
thethirdstageismutation basedfuzzing section3.
.basically authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anjiang wei yinlin deng chenyuan yang and lingming zhang figure freefuzz overview freefuzzeffectivelygeneratesmutantsfor thetestinputs i.e.
the argument lists used to invoke the targeted apis based on the traced information collected in the second stage.
the mutation strategies are composed of type mutation random value mutation and database value mutation.
the last stage is running all the generated tests with oracles section .
.
freefuzz resolves the test oracle problem by differential testing and metamorphic testing on different dl library backends and hardware.
freefuzz is able to detectvarioustypesofbugs includingwrong computationbugs crash bugs and performance bugs for dl libraries.
.
code collection freefuzz is a general approach and can work with dynamic api informationtracedfromvarioustypesofcodeexecutions.inthispaper we mainly explore code collection from the following sources code snippets from librarydocumentation.
inordertohelp usersbetterunderstandtheusageofapis almostalldllibraries willprovidedetaileddocumentationsonhowtoinvoketheapis.
usually detailed specifications written in natural languages are presentedtoshowtheusageofeachparameterofeachapiindetail.
meanwhile tobetterhelpthedevelopers suchnatural languagebasedspecificationsarealsooftenaccompaniedbycodesnippets forbetterillustrations.toillustrate anexamplecodesnippetforinvoking the 2d convolution api within pytorch is shown in figure4.ofcourse itisworthnotingthatnotallapishaveexample code and example code cannot enumerate all possible parameter values.
therefore it is also important to consider other sources.
figure4 examplecodefor2d convolutionfrompytorch sdocumentation library developertests.
softwaretestinghasbecomethemost widelyadoptedwayforqualityassuranceofsoftwaresystemsin practice.
as a result dl library developers also write generate a large number of tests to ensure the reliability and correctness ofdl libraries.
for example the popular tensorflow and pytorch dllibrarieshave1493and1420testsfortestingthepythonapis respectively.
wesimplyrunallsuchpythontestsastheydominate dl library testing and this work targets python api fuzzing.dl models in the wild.
popular dl libraries have been widely usedfortraininganddeployingdlmodelsinthewild.therefore we can easily collect a large number of models for a number of diverse tasks eachof which will covervarious apis during model trainingandinference.morespecifically frompopularrepositories ingithub weobtain102modelsforpytorch and100modelsfor tensorflow.
these models are diverse in that they cover various taskssuchasimageclassification naturallanguageprocessing rein forcementlearning autonomousdriving etc.thedetailedinforma tion about the leveraged models can be found in our repository .
.
instrumentation in this phase freefuzz performs code instrumentation to collect variousdynamicexecutionfortest inputgeneration.wefirstget thelistofpythonapistobeinstrumentedfromtheofficialdocumentations of the studied dl libraries in this work i.e.
pytorch andtensorflow.morespecifically wehooktheinvocationofthe listof 630apis frompytorchand1900 apisfrom tensorflowfor dynamic tracing.
the list includes all the necessary apis for training and inference of neural networks as well as performing tensor computations.freefuzzisabletocollectdynamicinformationfor eachapiinvokedbyallthethreesourcesofcode modelexecutions includingthetypeandvalueforeachparameter.
nomatterhow the apis are invoked e.g.
executed in code snippets tests or models the corresponding runtime information of the arguments is recorded to form the following type value spaces for fuzzing customized type space.
freefuzz constructs our customized typemonitoringsystem fuzztype forapiparametersbydynamicallyrecordingthetypeofeachparameterduringapiinvocation.
comparedwithpython soriginaltypesystem thecustomizedtype systemis atafiner grained level whichcan betterguidethe next mutation phase for fuzzing.
in python s dynamic type system the typeofparameter stride showninfigure4 canbecalculated by running type .
this will return class tuple which does not encode all the necessary useful information for fuzzingbecauseweonlyknowthatitisatuple.inourtypemonitor ingsystem fuzztype wecollectsuchinformationatafiner grained level fuzztype returns int int atupleoftwointegers .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
free lunch for testing fuzzing deep learning libraries from open source icse may pittsburgh pa usa similarly for tensors executing type torch.randn simply returns class torch.tensor in python s type system while we can obtain finer grained type tensor float32 a 4dimensionaltensorwith torch.float32 asitsdatatype byrunning fuzztype torch.randn .ourcustomizedtypemonitoring system used to guide api level fuzzing of dl libraries is formally defined in figure .
figure customized type monitoring system fuzztype note that type inference for dynamically typed languages such as ruby and javascript via dynamic program tracing has been explored in the literature for traditional applications .
inthiswork wefurtherextendsuchtechniquesforfuzzingdeep learninglibraries.
differentfrompriorwork freefuzzcollectsdynamic traces from various sources including developer tests code snippetsdocuments anddlmodelsinthewild also freefuzzaugments the python built in types to trace and mutate tensor shapes and heterogeneous data types.
api value space.
freefuzz constructs the value space of each api from theconcrete values passed intothe api during dynamic tracing.
one entry in the api value space stands for one api invocationwithitscorrespondinglistofconcretearguments which is later used in our mutation phase as the starting point to gen erate more mutants tests.
take figure as an example entry1 is added to the value space of the api torch.nn.conv2d after executing the documentation code in the code collection phase.
more specifically in channels out channels kernel size together with some other values not shown in figure due to limitedspace arerecordedin entry1.
thereturnvalueof nn.conv2d is a callable object and it expects a tensor as its input which is initialized as input torch.randn indicating that input isa four dimensionaltensorwith asits shapeand the values are randomly initialized.
note that we also record thecorresponding shape and data type information for tensors i.e.
input tensor shape input tensor type float32 .
allthefunctionargumentsmentionedaboveconstitute oneentry in the value space for nn.conv2d .
each invocation can add a new entry into the value space of the invoked api.
argument value space.
as shown in figure the argument valuespaceiscomposedofdifferentargumentnamesandtypes e.g.
in channels oftypeint togetherwiththeirvaluesrecordedwheninvokingdifferentapis.forexample fortheargument in channels oftheapi torch.nn.conv2d thevaluesrecordedinclude etc.
the argument value space is constructed based on the information collected in the api value space to speed up the queries in our database value mutation strategy discussed later.
more specifically argument value space aggregates values from different apis based on argument names.
the argument value space is formedbased on the idea that values for an argument of one api canserve as potentially reasonable values for the argument of othersimilar apis.
for example torch.nn.conv2d andtorch.nn.conv3d can be considered similar.
the api definition of 3d convolution is torch.nn.conv3d in channels out channels kernel size stride padding dilation groups bias true padding mode zeros device none dtype none and many parameters share the same namesas torch.nn.conv2d showninfigure2 .theconstructionof the argument value space is useful for the database value mutation to be introduced in the next section.
.
mutation in this phase freefuzz applies various mutation rules to mutate theargumentvaluestracedinthe second phasetogeneratemore tests for fuzzing dl libraries more thoroughly.
mutation rules.
themutationrulesforfreefuzzarecomposed of twoparts type mutation andvalue mutation shownin tables and respectively.
type mutation strategies include tensor dim mutation thatmutates n1 dimensionaltensorsto n2 dimensional tensors tensordtypemutation thatmutatesthedatatypesoftensorswithoutchangingtheirshapes primitivemutation thatmutates one primitive type into another as well as tuple mutation andlist mutation thatmutatethetypesofelementsincollectionsofheterogeneous objects.
value mutation strategies are divided into two classes one is randomvalue mutation andtheother isdatabasevalue mutation.
random value mutation strategies include random tensor shape usingrandomintegersasshapestomutate n dimensionaltensors random tensor value using random values to initialize tensors random primitive random tuple andrandom list.
database mutationstrategiesinclude databasetensorshape anddatabasetensor value whichrandomlypicktheaccordingshapesorvaluesfromdatabaseofargumentvaluespace togetherwith databaseprimitive databasetuple and databaselist whichrandomlypickthe corresponding entries from the argument value space based on the argument names and types.
note that all the mutation rules are type aware i.e.
they are applied according to the types.
algorithm.
showninalgorithm1 theinputtoourfuzzingalgorithmistheapitobemutated entriesintheapivaluespace vs and thedatabaseofargumentvaluespace db.ofcourse wealsoneed todefinethemutationrulesasdescribedabove.ineachiteration the algorithm always samples the next entry from the api value spacevs to start the mutation process line .
freefuzz then computesthenumberofarguments argnumintheentry line4 and randomlyselectsanintegerbetween1and argnumasthenumberof arguments to be mutated i.e.
nummutation line .
then freefuzz startsaninnerlooptomutate nummutation argumentstogenerate anewtest.theargumentsaremutatedonebyoneviarandomly authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anjiang wei yinlin deng chenyuan yang and lingming zhang table type mutation mutation strategies t1 t2 tensor dim mutation tensor angbracketleftn1 dt angbracketrighttensor angbracketleftn2 dt angbracketright n2 n1 tensor dtype mutation tensor angbracketleftn dt1 angbracketright tensor angbracketleftn dt2 angbracketright dt2 dt1 primitive mutation t1 int bool float str t2 t2 t1 tuple mutation ti ...n i type mutate ti i ...n list mutation ti ...n i table value mutation mutation strategies t v random tensor shape tensor angbracketleftn dt angbracketrighttensor shape datatype dt random tensor value v tensor angbracketleftn dt angbracketrighttensor shape v.shape datatype dt .rand random primitive int float bool str rand int float bool str random tuple ti ...n i value mutate ti i ...n random list ti ...n i database tensor shape tensor angbracketleftn dt angbracketright pick shape database tensor angbracketleftn dt angbracketright database tensor value tensor angbracketleftn dt angbracketright pick value database tensor angbracketleftn dt angbracketright database primitive int float str pick database int float str argname database tuple ti ...n i pick database t1 t2 ... tn argname database list ti ...n i pick database argname selectingarandomargumentindex argindex line7 .afterdeterminingtheargumenttobemutatedeachtime freefuzzgetsthe typeofitusingourcustomizedtypesystem fuzztype theargument nameargname andtheargumentvalue argvalue lines8 9and10 .
the type mutation will be performed nondeterministically if it is enabled freefuzz will mutate the argument type according to our typemutationstrategies line12 .
selectrandoverdb isanotherrandomfunctioncalledtodeterminewhethertoperformrandomvalue mutation line or database value mutation line according tothecorrespondingmutationrules.aftermutating nummutation arguments for entry freefuzz generates a new test which will be executed for testing the api line .
then the main loop will continue to generate the next test until the termination criterion is met e.g.
generating a specific number of new tests.
we next discuss function valuerule dbin more detail to explain the process for mutating the value of an argument for a specific apibasedontheargumentvaluespace.showninthealgorithm thefunctiontakestheapiname api thetypeofargument argtype the name of the argument argname and the database db as input parameters.
it then queries the database to collect all the apis which share the same argument name and type as the current api under test line .
next freefuzz computes the text similaritiesbetweenthecurrentapiundertestandeachofthereturned apisbasedonthelevenshteindistance betweenapidefinitions line .
take the query valuerule db torch.nn.maxpool2d dilation db as an example the text similarity is computed using api definitions of those containing the same argumentname dilation andthetype tupleoftwointegers .
more specifically the similarity between the current api under test andapii one of the returned apis can be computed by the following formula sim apii api levenshtein apii api max len apii len api where function levenshtein computes levenshtein distance between the two strings representing apiiandapi and it is divided bythemaximumlengthofthetwostrings.thewholeformulacom putesthetextsimilarityofthetwoapidefinitions.forourexample theresultshowsthatthedefinitionof torch.nn.conv2d hasthehighesttextsimilaritywiththetargetapi torch.nn.maxpool2d kernel size stride none padding dilation return indices false ceil mode false .
then we normalize the text similarities to transformthemintoprobabilities summingupto1 forselectingsimilar apis line .
the basic idea is that apis with higher similarity scores should get higher probabilities to be selected.
freefuzz does this by performing the softmax computation prob apii esim api i api m j 1esim api j api wheremdenotesthenumberofapissharingthesameargument name and type as the current api under test.
after sampling a random api according to the probabilities line the values arethenrandomlysampledfromthevaluesrecordedfortheapi line .
in this way the values stored in the database from one api can be transferred to serve as the arguments for another api.
.
test oracle in this phase we leverage the following ways to resolve the test oracle problem and detect potential dl library bugs wrong computationbugs.
weconsiderthreemodestoruneach api cpu gpu with cudnn disabled and gpu with cudnn enabled.inthisway wecandetectwrong computationresultsbycomparing the results between different execution modes.performancebugs.
we leveragemetamorphic relations to detect performance bugs with freefuzz.
more and more datatypes and hardware accelerators have been proposed in order to boostthe dllibrary performancein recentyears.
severalfloating pointdatatypesarespeciallydesignedfortensors including float32 float16 tf32 bfloat16 which also appear in our aforementioned tensor data type system.
we observe the fact that on the samemachine hardware m apis with the same function arguments argsandtensorsofthesameshapes tensor angbracketleftn dt angbracketrighttendtoholdthe following metamorphicrelationship in terms of time cost precision dt1 precision dt2 cost m api args tensor angbracketleftn dt1 angbracketright cost m api args tensor angbracketleftn dt2 angbracketright authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
free lunch for testing fuzzing deep learning libraries from open source icse may pittsburgh pa usa algorithm mutation algorithm input api the api under test to be mutated vs api value space db argument value space define typerule type mutation strategies valuerule rand random value mutation strategies valuerule db database value mutation strategies 1function mutate api vs db 2whilenotfinished do entry selectnext vs argnum len entry number of arguments nummutation random.get argnum whilenummutation 0do argindex selectnext argnum argtype fuzztype entry argname entry .name argvalue entry .value ifdotypemutation then argtype typerule argtype ifselectrandoverdb then next valuerule rand argtype argvalue else next valuerule db api argtype argname db entry next nummutation nummutation run entry 20function valuerule db api argtype argname db 21apis db.query argtype api argname angbracketleftapii sim angbracketright sim apis api angbracketleftapii prob angbracketright softmax angbracketleftapii sim angbracketright 24api prime sample angbracketleftapii prob angbracketright 25val sample db api prime argtype argname 26returnval thisindicatesthat dt1carryinglessprecisioninformationthan dt2 tends to execute faster.
for instance dt1can befloat16whiledt2 isfloat32 as long as the api supports both data types of tensors.
crashbugs.
ifanapicrashesorthrowsruntimeexception thenit may be considered as a crash bug.
meanwhile it could also be due to invalid test inputs which can be generated during the fuzzing process.
to automatically filter out such false alarms we build scripts to heuristically remove crash bugs which throw meaningful exceptions on all backends for invalid inputs e.g.
valueerror invalidargumenterror etc.
asaresult ifthetestprogramcrashes e.g.
segmentation fault or throws unexpected exception for valid inputs on certain backend s it is considered as a crash bug.
experimental setup in the study we address the following research questions rq1 how do the three different input sources of freefuzz without any mutation contribute to dl library testing?
rq2 howdoesfreefuzzwithdifferentnumbersofmutations for each api perform for dl library testing?
rq3 how do different mutation strategies impact freefuzz s performance?
rq4 how does freefuzz compare with existing work?
rq5 can freefuzz detect real world bugs?
ourexperimentsaremainlyperformedonthestablereleaseversionsofdllibraries pytorch1.8andtensorflow2.
.themachine for running experiments is equipped with intel xeon cpu cores .20ghz nvidia a100 gpus ubuntu .
and python .
.
.
implementation code model collection.
code model collection is essential to form the original seed test pool for our fuzzing technique.
to build an extensive pool for documentations we download all 512pieces of code snippets from the official documentations of pytorch tensorflow.more specifically we use the bs4python package to automatically parse the documentations to obtain the codesnippets.
notethatnotallcodesnippetscrawledfromdoc umentations are immediately executable.
thus we also build asimplisticrepairtooltoinsertomittedcodeintheexamples e.g.
importsections to make more code snippets executable.
for developer tests we run all existing python tests for pytorch by running python run test.py in the test directory while for tensorflow we runallpythonfileswithsuffix test.py.
fordlmodels weobtain adiversesetof102 100dlmodelsfromofficialmodelzoosofpytorch tensorflow andpopulargithubrepositories.thedetailed information about the models can be found in our repository .
instrumentation.
we get the lists of all python apis from official documentation of pytorch and tensorflow and hook them in init .py afileforapackagethatwillbeautomaticallyexecuted if the package is imported in the root of the library package by addingawrapperforeachapiinthelist.thisisdonetransparently and fully automatically for the users so that they do not need to modify any of their code model code for instrumentation.
in this way 630apisfrompytorchand1900apisfromtensorfloware instrumented for dynamic value tracing.
furthermore we leveragethemongodbdatabase torecordapivaluespaceand argument value space.mutation.
weimplementourmainalgorithm1formutationwith standard python packages.
the implementation details can also be found in our project repository .
test oracle.
theimplementationofdifferentialtestingissimple.
theexamplecodeforpytorchisshowninfigure10.meanwhile the implementation of metamorphic testing is to wrap the invocation of apis with code for timing.
.
metrics to thoroughly evaluate freefuzz we use the following metrics numberofcoveredapis.
duetothelargenumberofapisindl libraries itisofgreatimportancetoshowthenumberofcovered apisasanimportantmetricoftestingadequacy.surprisingly such an important metric has been largely overlooked by prior work on dl library testing .size of value space.
each api invocation can add one entry into the api value space.
therefore we use the total size of value space authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anjiang wei yinlin deng chenyuan yang and lingming zhang for all apis to serve as the metric to analyze and compare different inputsources.tobemoreaccurate wecountthenumberofentries intheapivaluespaceafterremovingduplicateentries.pleasenote that this is just used to show the scale of the traced data and does not necessarily indicate fuzzing effectiveness.line coverage.
code coverage is a widely adopted test adequacy criterion for testing traditional software systems and even the recent tensor compilers .
for example it is impossible foratesttodetectbugsincodeportionswithoutexecutingthem.
surprisingly althoughstate of the artdllibrarytestingtechniques e.g.
lemon claimedtoinvokemorelibrarycode theydidnot reportanycodecoverageintheirexperiments.wespenttremendous time and efforts setting up the environment for collecting the most widely used line coverage via gcov for both pytorch andtensorflow.morespecifically weevenfixedabuginthebazelbuildsystem usedforbuildingtensorflowtoperformcoverage collection.notethatweonlytracec c codecoveragebecausethec c implementationprovidesthefundamentalsupportfor operators in dl libraries and almost all the high level python apis finally invoke the c c code.number of detected bugs.
following prior work on software testingingeneralanddllibrarytesting wealsoreport the number of actual bugs detected for the studied dl libraries.
result analysis .
rq1 input source study inthisrq weaimtostudytheeffectivenessofdirectlyapplying freefuzz s traced dynamic information without any mutation for testing dllibraries.
themain experimentalresults areshown in table where we explore different settings including using documentationsonly testsonly modelsonly andallinformation together for both tensorflow and pytorch.
for each setting we show the number of covered apis row api the number of traced unique api invocations row vs and the line coverage achievedwhendirectlyrunningthetracedapiinvocations row linecov.
.fromthetable wecanobservethatdifferentsourcesof informationalltendtobehelpfulfortestingdllibraries.forexample although the test information covers the least number of apis fortensorflow itcanstillhelpdirectlycover216apisand31293 lines of code similarly although the model information covers the leastnumberofapisforpytorch itcanstillhelpdirectlycover145 apis and lines of code.
also another interesting observation is that the settings covering more apis tend to also achieve higher code coverage.
the reason is that different apis usually implementdifferentfunctionalities andthususuallycoverdifferentdllibrary behaviors paths.
this actually also demonstrates the effectiveness and necessityof api level testingfor dl libraries sinceit is much easier to cover more apis at this level than traditional model level dl library testing .
we can also observe that putting all three sources of information together can achieve even better results than using any singlesourceofinformation.forexample itcancover470 688apis for pytorch tensorflow and lines of code for pytorch tensorflow.tobetteranalyzethecontributionofeachsource of information we further leverage the venn diagrams in figure andfigure7topresentthedetailedbreakdownofthenumberoftable statistics about different sources freefuzz pytorch freefuzz tensorflow doc test model all doc test model all a p i v s line cov.
figure venn diagram for covered apis figure venn diagram for code coverage coveredapisandcoveragerespectively.fromthefigure forboth tensorflow and pytorch each source of inputs exclusively covers some apis and only a small number of apis are covered by allthree sources of information.
for example only out of all the covered apis are covered by all three sources of inputs on pytorch tensorflow.
meanwhile although each source of in puts still exclusively covers different code portions the majorityof covered code tends to be shared by all three sources of inputs.
thereasonisthatalthoughdifferentapisimplementdifferentcode logic they can be decomposed to a set of common low level op eratorsimplementedinc c .overall theexperimentalresults furtherconfirmthatitisnecessaryandimportanttoconsiderdifferent sources of information for effective dl library testing.
tracingthethreesourcesofinputsisa one timeeffort andcan beusedfortestingallsubsequentversionsofthesamedllibraries.
meanwhile itis alsoimportant to demonstratethat the timeoverheadisacceptableandnotextremelyhigh.therefore wefurther discusstheoverheadforconstructingthethreesourcesofinputs.
forthedocumentationsource the codesnippetsareusuallyquite shortandfasttorun.intotal freefuzztakeslessthan20minfor tracingallthedocumentationcodesnippetsforbothtensorflow and pytorch.
for the developer tests tracing the official tests written by developers from pytorch tensorflow consumes about .
.
hours.
lastly for the model source freefuzz runs all the102 100modelsstatedinsection4.1withinstrumentationfor pytorch tensorflow consuming less than hour for each of them.
.
rq2 coverage trend analysis in thisrq we presentthe effectiveness of freefuzzwith different numbersofmutationsforeachapiundertest.theexperimental results are shown as the blue lines with legend freefuzz in figure8andfigure9 wherethe xaxispresentsthenumberofmutants generated foreach api from to1000 with theinterval of100 whilethe yaxisshowstheoverallcoverageachievedviagenerating differentnumberofmutantsforeachapi theunionofallcoverage authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
free lunch for testing fuzzing deep learning libraries from open source icse may pittsburgh pa usa figure coverage trend analysis for pytorch figure coverage trend analysis for tensorflow sets for all tested apis .
note that the start point denotes the code coverage achieved by directly executing the original test inputs traced without any mutation.
from the figure we can observe that for both pytorch and tensorflow freefuzz can indeed cover more lines of code with more mutations enabled for each api under test demonstrating the overall effectiveness of our mutation strategies.
furthermore wecanalsoobservethatthecoveragebecomeslargely stableafterrunning600mutationsforeachapi indicatingthat600 mutations can be a cost effective choice in practice.
regarding the timecost thetotalrunningtimeforgeneratingandrunningall1000mutants for all apis is .
hours for pytorch and .
hours for tensorflow.
note that such overhead is totally acceptable for fuzzing e.g.
traditionalbinaryfuzzingtechniquesareusuallyappliedfor 24h and lemon takes over 24h .
.
rq3 different mutation strategies after tracing the initial inputs from various sources freefuzz performs three different mutation strategies in tandem as detailed in algorithm .
therefore in this rq we further study the impact of eachmutationstrategybydisablingit.tothisend wehavethree freefuzzvariants freefuzz typemu disablingthetypemutation strategy freefuzz randmu disabling the random value mutation strategy freefuzz dbmu disablingthedatabasevaluemutation strategy .notethatwealsoincludethevariantthatdoesnotperform any mutation at all i.e.
freefuzz allmu.
the experimental resultsforallthestudiedvariantswithdifferentnumberofmutationsforeachapiarealsoshowninfigure8andfigure9.notethat the start point for all other variants denotes the coverage achieved by freefuzz allmu.
from the figure we can have the following observations.first thedefaultfreefuzzoutperformsalltheothertable comparison on input coverage freefuzz tf1.
full lemon cradle api v s line cov.
table comparison with lemon on mutation freefuzz tf1.
full freefuzz models only lemon api v s line cov.
time 7h min 25h studied variants indicating the importance and necessity of all the three mutation strategies of freefuzz.
second we can also observe that random value and database value mutation strategies performsimilarlyintermsofcodecoverage whiletypemutation can be even more effective since the low level implementations for different types tend to be more different.
.
rq4 comparison with prior work inthisrq weaimtocomparefreefuzzwiththestate of the art lemon andcradle workfordllibrarytesting.wefirst compare their sources of inputs in terms of the number of covered apis and coverage.
lemon only uses models cradle uses models andfreefuzzconsidersthreedifferentsourcesofinputwith manymoremodelsinthewild.sincebothlemonandcradle use keras without supporting pytorch the comparison here is onlyconductedontensorflow.also duetothefactthatlemon and cradle do not support tensorflow used in our earlierexperiments we apply freefuzz on an old tensorflow version v1.
.
for a fair comparison with prior work we enforce freefuzz touseexactlythesamemodelsfromlemonasthedlmodelinput source.
to prepare the other two input sources for freefuzz wecollect developer tests and documentation code for tensorflow v1.
.theexperimentalresultsarepresentedintable4 column freefuzz tf1.
full simply runs the inputs traced by running the same models from lemon as well as documentation code and testsfromtensorflowv1.
columns lemon and cradle simply run the input dl models used in their original work.
from the table we can observe that when no mutations are allowed the inputsourcesusedbyfreefuzzcanachievemuchhigherapiand code coverage than lemon and cradle.
we next study the effectiveness of the mutation strategies used by freefuzz and existing work i.e.
lemon because cradleperforms no mutation .
we follow the same methodology as theoriginal lemon work when running its model level mutations.
for freefuzz we also use the default setting i.e.
generating and running mutants for each covered api.
the experimental resultsareshownintable5.notethatbesidesthedefaultfreefuzzandlemon column freefuzz modelsonly furtherincludesthe results of freefuzz with only the models from lemon withoutdocumentationcodeanddevelopertests astheinputforamore thoroughcomparisonwithlemon.fromthetable wecanobserve thatthedefaultfreefuzzcancover 9xmoreapisthanlemon while consuming .5x less time!
although the coverage improvement is not as significant as the number of covered apis freefuzz can still outperform lemon by .
also surprisingly lemon authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anjiang wei yinlin deng chenyuan yang and lingming zhang 1m torch.nn.conv2d .cuda 2tensor torch.rand .cuda 3torch.backends.cudnn.enabled true 4output1 m tensor with cudnn enabled 5torch.backends.cudnn.enabled false 6output2 m tensor with cu dnn disabled print output1.
sum output2.
sum debugging 8assert torch.allclose output1 output2 fail figure differential testing for 2d convolution only covers more apis via various model mutations compared to theoriginalmodels sinceonly5unusedlayerspreservethestrict input output shape constraints imposed by lemon and are added into the mutated models.
furthermore freefuzz with models only can already outperform lemon in terms of code coverage within 20min i.e.
75x faster than lemon!
this further demonstrates the benefits of api level testing compared with model level testing.
.
rq5 bugs detected forbugdetection wetargetpytorch1.8andtensorflow2.
which arebothofficiallyreleasedstableversions withthedefaultfreefuzz setting i.e.
generating mutants for each api.
note that we donottargettensorflow1.14becausedevelopersdonotactively support it anymore.
table shows the detailed statistics about thereal worldbugsdetectedbyfreefuzzanditsvariousvariants studied in section .
.
we can observe that freefuzz is able todetect bugs in total with already confirmed as previouslyunknown bugs for the two studied dl libraries and of them havebeenfixedbythedeveloperstodate.furthermore wecanalso observethateachmutationstrategycanhelpdetectcertainbugs that other mutation strategies fail to detect further demonstrating theimportanceofallfreefuzzmutationstrategies.lastly ofallthe bugs detected by freefuzz only one of them can be detected by lemon and cradle.
table summary of detected bugs freefuzz confirmed freefuzz typemu randmu dbmu allmu fixed pytorch tensorflow note that all the detailed issue ids for the bugs detected can be found on our github page .
we next present the case studies wrong computationbug.
figure10showsanexamplebugautomatically detected by freefuzzby comparing the computation results for 2d convolution between two backends one with cudnn enabled output1 and one disabled using aten backend instead output2 .
it throws assertionerror when executing the last line.
thesumofvaluesofoutputtensorsinline7showsthat output1 .
whileoutput2 .
indicating a significant difference in computation results.
further comparing the computationresultsexecutedbycpudemonstratesthattheresultiswrong only on gpu with cudnn disabled.
this is a confirmed bug by developers and fixed in latest master.performancebug.
freefuzzdetectsoneperformancebugbymetamorphictestingfor torch.nn.functional.conv transpose2d .accordingtothemetamorphicrelations thetimecostfor float16computation should be less than that for float32given the same parameters and tensor shapes.
however our results on nvidia a100 gpu1 fromtorch.nn import conv3d 2x torch.rand 3conv3d padding mode reflect x crash figure crash bug in conv3d 1m gpu torch.nn.maxunpool2d stride .cuda 2m cpu torch.nn.maxunpool2d stride 3tensor torch.rand 4indices torch.randint 5gpu result m gpu tensor.
cuda indices.cuda 6cpu result cpu tensor indices only crash on cpu figure invalid test input for torch.nn.maxunpool2d forpytorchshowthat float16 cost .377s float32 cost .101son some inputs.
the bug detected by freefuzz has spurred a heated discussion among pytorch developers.
they confirmed this performance bugand are tryinghard to figure outthe reason.crashbug.
figure11showsacrashbugdetectedbyfreefuzz.the program crashes on line when invoking torch.nn.conv3d .
the reason is that argument padding mode is set to value reflect and theprogramwillnotcrashif padding mode issettoitsdefaultvalue zeros .thebugistriggeredbythedatabasemutationstrategy.the argument name padding mode of type string appears in the argumentvaluespace andthereexistsavalue reflect whichisoriginally recorded for the argument padding mode oftorch.nn.conv2d .
freefuzz applies the database mutation strategy to query the argument value space and selects reflect fromconv2dto serve as the input for argument padding mode ofconv3d.
we confirm this bug according to the documentation of torch.nn.conv3d where 4stringvalues i.e.
zeros reflect replicate or circular shouldbevalidfor padding mode .developershaveacknowledged this bug and triaged it.
.
threats to validity the threats to internal validity mainly lie in the correctness of theimplementationofourownapproachandthecomparedtechniques.
to reduce such threats the authors worked together to perform testing and code review of freefuzz also we obtained the implementationof prior work from the official websites.
thethreatstoexternalvaliditymainlylieintheevaluationbenchmarks used.
to demonstrate that our freefuzz can be applied generalized to different dl libraries we have evaluated freefuzz on twomostwidelyuseddllibraries pytorchandtensorflow.furthermore although freefuzz is fuzzing against apis each with times and the randomness can be largely mitigated by such a large number of apis it is still possible that the non determinism in freefuzz can affect the effectiveness of freefuzzin different runs .
therefore following existing fuzzing work we rerun the experimental comparison between freefuzzandlemon for5times.theresultsshowthat freefuzzachievesanaveragelinecoverageof35997 35473intable5 whilelemon saverageis29769 29766intable5 .bothare quite stable with the coefficient of variation of only .
.
demonstrating the effectiveness of freefuzz in different runs.
thethreatstoconstructvaliditymainlylieinthemetricsused.
toreducesuchthreats weadoptthenumberofdetectedbugsused by prior work on dl library testing.
furthermore we also include thewidelyusedcodecoveragemetricintraditionalsoftwaretesting.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
free lunch for testing fuzzing deep learning libraries from open source icse may pittsburgh pa usa discussion and future work generalizability and specificity.
different from lemon andcradle thatspecificallytargettestingdllibrariesviadl models thefreefuzz workcanpotentiallybe generalizedtomore than just dl library fuzzing.
of course in this work we do have various dl specific components including mining dl models as inputs tensor related types and mutation rules and dlspecificoracles i.e.
differentialtestingforwrong computationbugs andmetamorphictestingforperformancebugs .meanwhile the basic idea of leveraging code snippets from library documentation and developer tests can be generalized to fuzzing library apis in variousdynamicallytypedlanguages.wehopeourworkcaninspire more research on the direction of mining for fuzzing.
validityoftestinputs.
ourinputminingandtype aware dbbased mutations can all help generate more valid inputs.
meanwhile freefuzz still does not always generate valid inputs due tosomecomplicatedinputconstraints.interestingly eventheinvalid inputs helped detect various bugs in pytorch tensorflow e.g.
unexpected crashes .figure 12showsone suchbug detected intorch.nn.maxunpool2d .theinputparameter indicesisatensor whosevaluesarerandomlysampledintegers withrespecttothe random primitive strategy which is invalid.
according to the documentation thevalid indicesshouldbeobtainedfromthereturnedvalueof torch.nn.maxpool2d .thebugwasdetectedbecause the program onlycrashes when running oncpu i.e.
line 6fails butproduces awrong resultsilently withoutthrowing anyerror messageongpu i.e.
line5passes .thus thegpuimplementation should add the missing check.
the developers have confirmed this bug and even labelled with high priority .
future work .
freefuzz currently only focuses on testing the correctnessofsingleapis.whileapi leveltestinghasmanyadvan tages over model level testing it may still miss bugs which can onlybetriggeredbyinvokingasequenceofapis.besides when reproducing detected bugs we find that some tests will fail on one machine but pass on other machines given exactly the same script andthesamelibraryversion.thisisprobablyduetothedifferences inunderlyinginfrastructureandhardware.thistypeoftestsare called implementation dependent flaky tests described in prior workontestflakiness .futureworkmayexplorehowto better detect and fix flaky tests in deep learning libraries.
related work dl model testing.
there has been a growing body of research for improving the quality of dl models.
various adversarial attack techniques havebeenproposed to generatethe so called adversarialexamples byaddingperturbationsimpercep tible to humans to intentionally fool the classification results given by dl models.
to mitigate such attacks researchers have also proposedvariousadversarialdefensetechniques includingadversarialtraining detection andothers .another recentlineofresearchhasexploredthepossibilityofimprovingtherobustnessofneuralnetworkfromajointperspectiveoftraditional software testing and the new scenario of deep learning.
deepxplore proposesametriccalledneuroncoverageforwhitebox testingofdlmodelsandleveragedgradient basedtechniquesto searchformoreeffectivetests.whilevariousothermetrics have also been proposed recently the correlation between suchmetrics and the robustness of models is still unclear .
meanwhile there are also a series of work targeting specific ap plications such as autonomous driving including deeptest deeproad anddeepbillboard .varioustechniqueshave alsobeenproposedtodetectnumericalbugsintroducedwhenbuilding a dnn model at the architecture level with static analysis and via gradient propagation .
lastly researchers have also explored concolic testing to achieve higher coverage for dnn models mutation testing to simulate real faults in dl models and test input generation for dnns by exploiting features learnedfromdatawithgenerativemachinelearning.differentfrom allsuchpriorwork ourworktargetstheunderlyingdllibraries whicharethebasis fortraininganddeployingvariousdlmodels.
dl library testing.
cradle is the trailblazing work for testingdllibraries.themaincontributionofcradleisresolving the test oracle challenge with differential testing on keras.lemon further advanced cradle by proposing mutation rules to generate more models as claimed by lemon to invokemore code in dl libraries.
lemon s mutation strategies include intact layerandinner layermutationrules whichmustconform tostrictconstraints e.g.
forintact layermutation thelayertobe inserted or deleted should preserve the shapes of input tensors.
actually according to our experimental results the mutation rules applied by lemon can hardly help cover more dl library code.
a more recent work on testing dl library is predoo which only mutatestheinputtensorvalueswithallotherapiparameters manuallysetupfor precisiontesting.asaresult it wasonlyappliedto 7apis operatorsfromtensorflowandweexcludeitinourcomparison.
to our knowledge we propose the first general purpose and fullyautomatedapi levelfuzzingapproachforpopulardllibraries.
furthermore we adopt traditional code coverage for dl library testing andrevealvariousinterestingfindings e.g.
state of the art lemon can hardly improve the dl library code coverage .
conclusion we have proposed freefuzz the first approach to fuzzing dl librariesviaminingfromopensource.morespecifically freefuzz considersthreedifferentsources librarydocumentation developertests and3 dlmodelsinthewild.then freefuzzautomaticallyruns allthecollectedcode models withinstrumentation totracethedynamicinformationforeachcoveredapi.lastly freefuzz will leverage the traced dynamic information to perform fuzz testingforeachcoveredapi.theextensivestudyoffreefuzzon pytorch and tensorflowshows thatfreefuzz is ableto automaticallytracevaliddynamicinformationforfuzzing1158popularapis 9xmorethanstate of the artlemonwith3.5xloweroverhead.
freefuzzhasdetected49bugsforpytorchandtensorflow with already confirmed by developersas previously unknown bugs .