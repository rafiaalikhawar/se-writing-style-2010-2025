deepperf a deep learning based approach for improving software performance spandan garg spgarg microsoft.com microsoft redmond washington usaroshanak zilouchian moghaddam rozilouc microsoft.com microsoft redmond washington usacolin b. clement coclemen microsoft.com microsoft redmond washington usa neel sundaresan neels microsoft.com microsoft redmond washington usachen wu chen.wu microsoft.com microsoft shanghai china abstract improving software performance is an important yet challenging part of the software development cycle.
today the majority of performance inefficiencies are identified and patched by performance experts.
recent advancements in deep learning approaches and the wide spread availability of open source data creates a great opportunity to automate the identification and patching of performance problems.
in this paper we present deepperf a transformer based approach to suggest performance improvements for c applications.
we pretrain deepperf on english and source code corpora and followed by finetuning for the task of generating performance improvement patches for c applications.
our evaluation shows that our model can generate the same performance improvement suggestion as the developer fix in of the cases getting of them verbatim in our expert verified dataset of performance changes made by c developers.
additionally we evaluate deepperf on open source c repositories on github using both benchmark and unit tests and find that our model is able to suggest valid performance improvements that can improve both cpu usage and memory allocations.
so far we ve submitted pull requests with different performance optimizations and of these prs have been approved by the project owners.
introduction performance bugs are usually non functional bugs that can cause poor user experience reduced throughput increased latency and wasted resources.
performance bugs may not cause system failure and may depend on user input therefore detecting them can be challenging .
they also tend to be harder to fix than nonperformance bugs .
as a result better tool support is needed for fixing performance bugs.
in recent years a variety of performance bug detection approaches have emerged to help developers identify performance issues.
however a majority of existing performance bug detection approaches focus on specific types of performance problems.
for instance prior work investigated the detection of inefficient loops database related performance issues low utility data structures corresponding author false sharing specially in multi threaded code etc.
approaches that fix specific performance issues due to repeated computations software misconfigurations loop inefficiencies etc.
have also been developed.
many of these approaches rely on expert written algorithms or pre defined set of rules to detect and fix performance issues based on patterns in abstract syntax tree control flow graphs profiles etc.
building rule based analyzers is a non trivial task as it requires achieving the right balance between precision and recall.
once developed maintaining these rules can also be costly as it requires continuous effort by performance experts.
with the recent rise of large transformer models and wide spread availability of open source software artifacts there is an opportunity to learn patterns of performance improvements directly from mined data.
transformer based approaches have been shown to achieve state of the art performance not only in various natural language processing nlp problems but also a variety of software engineering tasks such as code completion documentation generation unit test generation bug detection etc.
in this paper we draw inspiration from these techniques in an attempt to solve the problem of automatically suggesting performance improvements.
we present an approach called deepdev perf that uses a large transformer model to suggest changes at application source code level to improve its performance.
we first pretrain our model using masked language modelling mlm tasks on english text and source code taken from open source repositories on github followed by finetuning on millions of performance commits made by .net developers.
through our evaluation we show that our approach is able to recommend patches to provide a wide range of performance optimizations in c applications which is not possible through any existing analyzer alone.
most suggested changes involve modifications to high level constructs like api data structure usages or other algorithmic changes often spanning multiple methods which cannot be optimized away automatically by the c compiler and could therefore lead to slow downs on the user s side.
further by suggesting changes to a set of real world repositories and measuring the impact of our suggestions through benchmark tests we show that our changes provide actual performance gains to these applications.
prs containing our model suggestions have already been accepted by the developers of these projects showingarxiv .13619v1 jun 2022that our suggestions are considered to be correct and useful by the project owners.
in summary our work makes the following main contributions we propose a novel transformer based model called deepperf which finds performance optimization opportunities in a c application and automatically generates performance improvement patches.
we extensively evaluate deepperf using a curated dataset of real world performance improvement changes made by c developers to a hold out set of open source repos on github.
through our empirical evaluation we demonstrate that deepdev perf is able to generate a wide variety of performance improvements.
we show real world evidence that deepperf generates changes that lead to tangible performance improvements to various open source c projects on github.
we submit prs containing the suggested changes to these repos many of which have since been approved showing that our fixes are considered useful by developers.
motivating examples figure shows examples of two suggestions made by deepperf to two open source c projects on github.
in the first example the code prior to the change uses linq .
linq expressions have an inherent allocation associated with them.
as a result linq usage on the application hot path often leads to unnecessary allocations which can cause spikes in garbage collection gc depriving the application of cpu resources and reducing its throughput.
in the top change deepperf recognizes that the use of linq call to skip the first position is unnecessary and it recommends a change to unroll the linq query and use an explicit for loop which starts indexing from .
by executing the unit and benchmark tests in this repository we verified the correctness of the change as well as the performance gain.
looking at the benchmark results the change achieves a reduction in allocations as well as gen gc compared to prior code.
the second change shows a case where the code unnecessarily allocates a character array from an input string using the tochararray method.
the array is then being used to iterate over and index characters at various positions within the string as well as passed to a user defined helper function to count the number of uppercase characters within the string.
deepperf notices that the array allocation is redundant as c strings can be indexed directly.
therefore it removes the redundant allocation and replaces the usages of the array with the original string.
it also defines an overload to the helper method that accepts a string instead of a readonlyspan to count the number of uppercase letters in the string.
this change results in fewer allocations as well as improved cpu usage.
pull requests containing both of these changes were submitted to the corresponding github repos and have since been approved by their owners.
figure two examples of changes suggested by deepperf that were submitted to the corresponding repos as prs and have since been accepted i the change on top taken from following pr1 suggests a change to unroll a linq query into an explicit for loop.
this change results in lower allocations and gen garbage collection compared to prior code ii the change on the bottom is from another pr2on a different c project.
the original code unnecessarily converts a string to a character array to index into the string.
since the array allocation is redundant deepperf suggests a change to remove the allocation in favor of just using the original string instead.
it also overloads a user defined helper method previously being used to count the number of upper case characters in the string to accept a string instead of a readonlyspan .
our approach we describe the details of our proposed model below.
figure shows an overview of our pipeline.
we begin by first describing how we take an english pretrained bart large model and further pretrain it on source code.
we then describe our data collection and example generation pipeline for finetuning.
this is followed by a description of our two step finetuning process using the examples generated by the example generation step.
2data collection pretraining finetuning c repos with starscrawl commitsexample generationexamples english pretrainingsource code pretrainingall commits perf commitsdata collection model training figure our model data collection and training pipeline.
we first crawl the commit history of all 45k c repos with stars on github and generate examples for each modified method with various contextual elements important to performance using statements class attributes caller callee methods etc.
.
for training we first pretrain bart large on denoising objectives over english text and source code followed by a two step finetuning.
in our finetuning step we first finetune the model on examples generated from all commits followed by a smaller finetuning step done only on examples from commits where developer included a performance related keyword perf performance reduce allocation etc.
in the commit message.
.
pretraining for pretraining we collected 45k github repositories with stars that were composed primarily of c code.
we de duplicate this data on a file level using a hash function and then pretrain a 406m parameter bart large model using span masking objective on this data set.
span masking corrupts code by replacing random spans of input tokens with a mask token and the model is trained to reconstruct the original code by predicting the tokens replaced by the mask tokens.
such pretraining has been shown to significantly improve model performance for a various downstream nlp tasks including many software engineering tasks .
.
data collection below we present the details of our data collection and example generation steps.
for our data we collect 45k repositories with stars on github whose primary language is c and had a commit within the last years.
.
.
crawling commits.
after cloning these projects we crawl themain branch s commit history.
this history involves a commit message and a diff representing the difference between current and previous version of the changed files.
this yields 11m commits and their corresponding commit messages.
.
.
identifying performance related commits.
for each commit we look for performance related keywords perf performance reduce allocation etc.
in the commit message to determine if it is performance related.
table shows the number of commits and examples that come from performance related commits.
.
.
generating code transformation pairs.
to generate the code transformation pairs input output sequences from the crawled commits we follow the following process.
within each commit we parse the modified c files that end with the extension .cs using the tree sitter parser.
we first extract the classes within the file and its corresponding member methods.
we apply some pre processing steps on the method bodies by normalizing white space and removing comments.
this allows us to ignore any trivial modifications towhitespace and comments.
using the method signature we identify the corresponding versions of the method in the before and after files.
we then compare the normalized method bodies between the two versions of the file and discards the methods whose bodies do not appear to be have been modified.
from here on we refer to the remaining modified methods as focal methods.
next we construct an input output pair for each focal method.
figure shows an example of one such input output pair.
we start by including the focal method itself in the example input whose location is indicated to the model using c style comments edit and end before and after the focal method.
performance changes often require changes beyond the focal method itself as seen in the second example in figure such as adding new class level attributes or additional imports or even changes to other methods within the class that make calls to or are called by the focal method.
prior work has shown that including additional class file level context information with the focal method results in a higher quality predictions in such code generation tasks .
we believe that such contextual information would prove useful in generating performance patches as well.
below are the file class level context elements we include as part of the input using statements these tell the model what import statements exist within the file and whether new imports need to be added for the new methods or apis used in the recommended changes.
class attributes these are the containing class s member attributes.
the underlying types of class attributes can often be important information in determining the right performance fix.
this may include fixing incorrect usages of variables of certain types that cause performance issues or recommending a more appropriate data structure e.g.
replacinglist t with ahashset t etc.
caller callee methods these are the methods that directly make calls to or are in turn called by the focal method.
this information can help the model learn patterns of changes that involve hoisting memoizing calls across methods to optimize computations or simply modifying the caller callee to be consistent with the modified focal method.
3using system using system.collections.generic using system.linq using shared using shared.data.extractors using shared.helpers using windowsactivitytracker.data using windowsactivitytracker.models class dayfragmentationtimeline basevisualization ivisualization begin public override string gethtml var html string.empty var orderedtimelinelist queries.getdaytimelinedata var sum orderedtimelinelist.sum i i.durationinseconds if orderedtimelinelist.count sum html vishelper.notenoughdata dict.notenoughdata return html html getactivityvisualization orderedtimelinelist return html end private string getactivityvisualization list windowsactivity list var categories list.select a a.activitycategory .tolist var html string.empty html script type text javascript var onload window.onload window.onload function if typeof onload function onload html html script html div style align center p id hover defaulthovertext p div html div id activitytimeline align center div html legendhelper.getlegendforcategories categories return html private string defaulthovertext private string activitytimeline private string readableprocesses list windowprocessitem list private string formatwindowtitle string windowtitle private string formatcontextcategory activitycategory category private string formatcategories list activitycategory list using system.text class dayfragmentationtimeline basevisualization ivisualization public override string gethtml var html string.empty var orderedtimelinelist queries.getdaytimelinedata var sum orderedtimelinelist.sum i i.durationinseconds if orderedtimelinelist.count sum html vishelper.notenoughdata dict.notenoughdata return html sb.clear getactivityvisualization orderedtimelinelist sb html sb.tostring return html private void getactivityvisualization list windowsactivity list stringbuilder sb var categories list.select a a.activitycategory .tolist sb.append script type text javascript var onload window.onload window.onload function if typeof onload function onload sb.append sb.append script sb.append div style align center p id hover .append defaulthovertext .append p div sb.append div id .append activitytimeline .append align center div sb.append legendhelper.getlegendforcategories categories private stringbuilder sb new stringbuilder using statements focal method caller callee class attributes method signatures figure an example of an input output pair used in finetuning.
the input consists of the focal method along with various class file level context elements such as using statements within the file class member attributes focal method s caller callee methods other method signatures.
we also include c style comments begin and end before and after the focal method indicating its location to the model.
the example shown above comes from an actual performance commit made by a c developer to an open source repo and shows a perf change to replace a sequence of string concatenations with a stringbuilder .
such a change would save allocations as each string concatenation leads to a new string allocation whereas thestringbuilder defers the allocation until after all the component strings are gathered and a call to tostring is made.
additionally the change also caches and re uses the stringbuilder instance as opposed to allocating a new one each time.
in this case the output also consists of an additional using statement importing the namespace containing the definition of stringbuilder along with modified versions of the focal and callee method and the new class attribute.
method signatures finally we add the signatures for any other methods that aren t caller or callee methods of the focal method.
due to limited token space we are unable to add the bodies of each method.
this information could shed some light on the nature of the class itself and provide context as to what other methods are present in the class for the model to use in the generated patch.
due to the input token window for bart large being limited to only tokens we construct the example input in an iterative fashion.
we start by including the focal method in the example input and then incrementally add each contextual element in the list above.
before adding each type of contextual elements we ensure that the resulting sequence will be within the allowed range of tokens i.e.
tokens.
this way we try to incorporate as much context into the limited span of tokens while staying within the allowed limit.for the output in addition to changes to the focal method we include any of focal method s caller callee methods that are modified by the commit.
we also include any additional imports that may have been added as well as class attributes defined modified that are used by the focal method or modified caller callee methods.
this way we allow the model to output patches that make changes to not only the focal method but also the caller callee methods class attributes as well as add any new methods attributes and import statements as needed.
figure shows an example of an input output pair generated using the steps above.
.
.
data splitting and de duping.
we split the finetuning data on the project level.
we leave out two sets of test and validation repos each containing repos that are not included in either step s training data.
we also dedupe the examples in each set as well as remove any near duplicates among them to ensure no overlap between train and test data.
4table number of commits and examples in our training data for the all commits and perf commits finetuning steps.
commit type data of commits of examples all commits 11m 16m perf commits 535k .5m .
finetuning we finetune the code pretrained bart large model on the task of generating a performance improvements given an input sequence containing the focal method and contextual elements as explained in section .
.
.
we perform a two stage finetuning.
we first teach the model how to c developers make changes in general by first finetuning our pretrained transformer model on examples from all commits.
we refer to the resulting model as deepdev c .
we then perform a second finetune step over deepdev c model using the set of code transformations examples extracted from performance commits to teach it specifically how developers make performance optimization changes.
we refer to the final model as deepperf.
to better understand whether the first finetuning step has any significant impact on the results we train a third model finetuned directly on code transformations extracted from performance commits.
we refer to this model as deepdev perf commits.
in our evaluation we compare the three models and discuss possible reasons for differences in their performances.
empirical evaluation in this section we first explain our baselines and evaluation metrics.
we then cover our quantitative and qualitative analysis methodology and results.
.
baselines and evaluation metrics we compare deepperf model with the following two models deepdev c which was first pretrained on english and source code followed by finetuning on code transformations extracted from all c commits in our training data.
deepdev perf commits which was first pretrained on english and source code followed by finetuning on code transformations extracted from only performance commits commits with a performance related keyword in the commit message in our training data.
in order to compare the models performances we define the following metrics verbatim match we report the number of examples where one of the model s suggestions was found to match verbatim with the developer patch i.e.
the ground truth output in the input output pair.
abstracted match for our comparison to be independent of variable name matching we replace variable names with generic names of the form var i e.g.
var 0 var 1 etc.
where i is determined based on the relative order in which variables are encountered when traversing the parse tree.
we then compare the abstracted versions of themodel suggestions with the similarly abstracted developer patch and report how many were found to match.
codebleu we measure the codebleu scores to gauge the similarity of model output with the actual developer patches.
in addition to n gram matching of bleu codebleu also compares abstract syntax trees ast and data flow between two programs.
thus it takes into account the syntactic as well as semantic code similarity.
we use the hyperparameters that were shown to have the highest correlation to human scores in the study i.e.
.
.
.
.
.
through this experiment we intend to answer the following research questions rq1 are both finetuning steps all commits and perf commits step in our two step finetuning necessary?
rq2 is deepperf able to provide a wide range of performance optimizations?
.
quantitative analysis for the purposes of our evaluation we picked a random set of repos from our test set none of which had previously been seen by our models.
we then collected all performance commits commits with a performance related keyword in the commit message that change only one .cs file to filter out squash merges that include a variety of changes across multiple files.
this process yields commits resulting in a total of examples.
we use this set of examples for our evaluation.
.
.
two step finetuning ablation .table shows the results of our models over this dataset using the metrics we defined earlier.
we see that our deepperf model performs better than the other two models and is able to get of the examples in this dataset verbatim and verbatim when the variable names are abstracted away.
the models achieve very similar codebleu scores which is expected since due to the nature of the task most of the code will be shared between the model output and ground truth for any well trained model.
the major difference between deepperf and the deepdev perf commits is that deepperf is first finetuned on examples generated from all c commits followed by a smaller finetuning step on examples generated using commits with a performance related keyword in commit title description.
on the other hand deepdev perf commits is finetuned directly on the smaller set of performance commit examples.
deepdev c differs from the two as it s only finetuned on all c commits but not directly on performance commits.
comparing the results of the three the reason both deepperf and deepdev c perform better than deepdev perf commits could be because finetuning on all commits allows the models to learn better representations for code by seeing more examples of how changes are made by c developers since the dataset for all commits step is almost times larger than the one containing only examples from perf commits.
there is also a possibility that there may be some performance improvement changes that aren t explicitly annotated within the commit message.
for example developers may not always explain every change they make in their commits and squash multiple changes into a single commit mentioning 5table three categories of performance issues in expert verified dataset of performance improvements.
change category some examples of kinds of performance optimizations within category of examples high level change c1 memoize results using dictionary concurrentdictionary hoist computation allocation to outer scope loop method class etc.
cache and re use types like list dictionary stringbuilder etc.
introduce fast path to avoid unnecessary computation etc.
suggest different api data structure c2 replace a series of string concatenations with a stringbuilder use more suitable data structure e.g.
list t .contains hashset t .contains remove linq usage e.g.
list t .any count list t .count unroll query to explicit for foreach loop etc.
etc.
improve existing api data structure usage c3 condense optimize linq queries e.g.
count !any where lambda .firstordefault firstordefault lambda etc.
use more appropriate method in api or fix api usage e.g.
initialize list dictionary with size when known beforehand remove unnecessary calls to tolist toarray if enumerable is enumerated once read directly to memorystream rather than reading to buffer and then to stream etc.
etc.
table summary of the results of our three models over dataset comprising of all the examples generated from commits that had a performance related keyword in the commit message in a hold out set of test repos.
model verbatim abstracted codebleu match match deepdev c .
.
.
deepdev perf commits .
.
.
deepperf .
.
.
only the most important in the commit message.
we found several examples of such commits in our training data where the changes contained performance optimizations but the commit message did not include any performance related discussion.
while we don t know pervasive this is and just how many such phantom performance changes exist outside of performance commits their presence would imply that models trained on all commit data would overall be seeing more performance improvement code transformations during training than one that s been directly trained on performance commits.
.
qualitative analysis to better understand the different types of performance improvements deepperf can suggest we performed a qualitative evaluation on a subset containing performance commits from our test set in the previous section.
this resulted in a set of examples demonstrating a variety of performance fixes each of which were verified with two c performance experts.
based on our understanding of performance changes in c we observe that the changes fall into the following three broad categories of performance improvements category high level changes these consist of algorithmic changes that require modifications to the overall code structure.
these changes could include hoisting calls allocations to an outer scope adding caching memoization to avoid repeated computation introducing a fast path etc.
category suggesting different api data structure these are language api specific changes to replace or remove anexisting api or data structure usage in favor of a better alternative.
these changes could include removing linq by unrolling queries into explicit loops suggesting a different data structure e.g.
replace list with a hashset when performing look ups etc.
category improving existing api data structure usage these are also language api specific changes but suggest modifications to existing usage of an api or data structure when deemed incorrect or sub optimal.
these may include changes like condensing linq queries to be more optimal fixing incorrect uses of a data structure using a better suited overload of a library function etc.
table shows some example performance changes found within the categories as well as the number of examples in our dataset that fell within that category.
we found that majority of these changes required deep knowledge of apis data structures or involved high level algorithmic modifications which is not possible for the compiler to make automatically.
we expect some analyzers to be able to fix a small portion of issues that fall in the second or third categories but even these examples we found to be were quite varied.
.
.
human evaluation methodology .for each example in our dataset we sample hypotheses from the model and take the top suggestions based on the average likelihood of tokens.
since we have so many suggestions suggestions for examples it would be difficult to evaluate each of them by hand even with a team of experts.
therefore we use the following evaluation metric to help us approximate the model s top k accuracy.
we report this in addition to the metrics mentioned earlier i.e.
verbatim match abstracted match and codebleu closest match top k accuracy using a code search technique such as aroma we find the document within the corpus of model suggestions that is closest to the developer patch for each example.
we then verify the most similar suggestion with two performance experts neither of whom are on the author list.
the experts are shown both the developer change and model suggestion and asked to 6assess whether they considered the model suggestion to be making the same performance optimization and are semantically the same as the developer change.
table summary of the results of our three models over the manually curated dataset.
closest match verbatim abstracted model top k accuracy match match codebleu deepdev c .
.
.
.
.
.
.
deepdev perf commits .
.
.
.
.
.
.
deepperf .
.
.
.
.
.
.
figure closest match top k accuracy plot of our models on the manually curated dataset of performance changes.
we can see that deepperf achieves the best score among the models.
.
.
results .table and figure show the results of the models using our defined metrics.
we see that our best model is able to solve of the examples in our dataset getting verbatim as the developer fix.
the closest match top k accuracy plot was computed based on the associated rank of the retrieved suggestion among the top hypotheses when found to be correct by both of the two c performance experts.
in majority of the cases the main reasons for dissimilarities from the developer were the model suggesting different variable names or other slight variations like using thevar keyword instead of the variable s type or using a forloop as opposed to a foreach loop where both are appropriate difference in order of statements where relative order did not matter such asusing statements at the start of file etc.
figure shows the performance of our models in the categories of performance changes.
we also see that our best model out performs the other two models in each category.
rq1 rq2 in summary we conclude that both finetuning steps were necessary as deepperf clearly demonstrates better performance than the other two models on the overall testcategory category category performance improvement categorypercentage deepdev c deepdev perf commits deepperf figure performance of our models on the three categories of performance issues high level changes category suggesting different api data structure category improving existing api data structure usage category .
we can see that the deepperf model green tends to perform the best among the models in all three categories followed by deepdev perf commits and deepdev c .
dataset of performance commits as well as the smaller expert verified dataset of performance optimizations.
additionally we see that deepperf can generate suggestions that span a wide variety of performance optimizations encompassing both highlevel algorithmic to low level api data structure related performance changes.
furthermore these changes were considered equivalent to the developer made performance improvements by two performance experts in c .
figure an example of the output generated from executing a benchmarkdotnet test suite.
the first column shows the different benchmarks defined by the user.
benchmarkdotnet automatically runs each of these user defined benchmarks multiple times and reports metrics such as sample mean standard deviation standard error median first and third quartile of the test duration.
for allocations it reports the memory allocated on average during each test run.
in the wild evaluation to see whether our approach can suggest real performance improvements to existing c projects we performed an in the wild evaluation of deepperf on a set of c github repos not previously seen by our model.
for this evaluation we chose repos that contained both benchmark and unit tests to allow us to verify that our suggested changes are correct using unit tests and lead to measurable performance improvements using benchmark 7tests .
benchmark tests in c are usually written using the benchmarkdotnet library.
shown in figure is an output summary of a benchmarkdotnet test.
benchmarkdotnet automatically runs each benchmark test in the user written test suite multiple times and reports metrics for the duration of a given benchmark test as well as the amount of memory that is allocated on average on each run.
it can also give other information such as how frequently generational gc is triggered.
through this experiment we intend to address the following research questions rq3 what are the reasons behind some of our model s changes failing to compile?
how could this be improved?
rq4 is deepperf able to suggest changes that lead to real performance improvements?
if so how much performance improvement do these changes typically provide?
are these suggestions considered useful by the developers?
rq5 how effective are unit and benchmark tests in ensuring changes are correct performance improvements?
.
experiment setup to find repos with benchmark tests we sampled c repos where benchmarkdotnet nuget package is mentioned in one of the build configuration files .csproj in the repo.
to limit the methods we need to generate changes for we select methods that satisfy following two criteria are present on the execution path of the repo s benchmark tests have a high line branch coverage with the repo s unit tests.
this yields methods across the test repos.
we then generate model inputs for each of these methods including contextual information as described in example generation step sec.
.
.
.
we use our best model deepperf and sample suggestions for each of these inputs.
we then pick the top suggestions based on their average token likelihood and test each of these suggestions against the repo s main branch.
from these suggestions we first filter out changes that are syntactically incorrect.
we found that of the suggestions had a syntax error.
most of there were due to early truncation or repetition when generating long outputs which are known issues when generating text using such language models.
.
running unit tests we then run unit tests for each of the remaining suggestions.
this step filters out suggestions that fail to compile or are found incorrect based on the unit test cases provided by the developer.
table shows a breakdown of how many suggestions fail at this stage.
as we can see at the end of this step we are left with of the suggestions we started with.
table breakdown of the results of running unit tests.
result occurrences of suggestions syntax error .
compilation error .
failed unit tests .
passed unit tests .
total .
analyzing build failures rq3 table shows the main reasons of compilation errors.
after grouping together the first compilation error in each suggestion that fails to compile we found that they fell into major error categories undefined identifier incorrect argument passing incorrect using statements andincorrect return type .
upon looking at some instances of each category we identified patterns of mistakes in the model s suggestions that cause these errors.
we noticed that the undefined identifier errors tend to happen when the model tries to use methods or classes outside provided context.
as the model can only guess what other classes exist in the project and the methods contained within it sometimes makes calls to methods that do not exist.
we believe this could be improved by incorporating additional information regarding other classes within the project to the input such as the classes being used in the focal method or within imported namespaces.
theincorrect argument errors also tend to occur when the model calls a method outside of provided the context.
this results in the model passing in the wrong arguments types or number of arguments by making calls to method overloads that don t exist.
we often saw this occur when the model tried to call member methods within some project specific classes that were instantiated somewhere in the input code.
cases for the incorrect using statements follow a similar pattern as well.
here the model tries to import namespaces within the repo that don t exist or from packages that aren t in the build files.
since it doesn t know what other files exist in the project or the packages included in build it often adds incorrect import statements.
the fourth category type mismatch occurs when the model suggests modifications that change the types of one or more class attributes which get used elsewhere in the class.
since it can only modify the methods that are included in the input context due to limited window it is unable to modify these other methods.
other reasons for these errors include mismatch caused by changing the return type of a method when the input class implements an interface since changing the type would cause the method in the parent to not be overridden leading to a compiler error.
based on these observations we believe a significant portion of above errors could be resolved by including a larger context containing more methods in the input class or even other classes files in the project through extended context .
we leave this exploration to future work.
table main reasons for compilation errors.
error cause error codes occurrences of errors undefined identifier cs1061 cs0117 .
cs0246 cs0103 cs1579 etc.
incorrect arguments cs1503 cs1501 .
cs1729 cs7036 cs0305 cs0029 cs0019 etc.
incorrect using statements cs0234 .
type mismatch cs0266 cs0738 .
cs0508 etc.
other mistakes cs0021 cs0122 misc.
codes .
total .
.
running benchmark tests the next step is to run benchmark tests for each of the changes that pass unit testing stage.
however before we run the benchmark tests we had to make some changes to the provided benchmark test suite to ensure the tests track the right metrics and that results are comparable among separate runs.
by default benchmarkdotnet tests do not track allocations.
for out of the test repos we found that memory tracking wasn t enabled and we had to enable it ourselves by adding a attribute to the class containing the benchmarks.
changing this does not affect the results for other metrics tracked by the benchmarks like test durations.
another change we had to make to make the numbers comparable between separate runs was to add seeds to instances of random number generators instantiated in the benchmarking code.
this is to ensure that the tests are deterministic so that the results can be compared between separate runs of the tests.
additionally to ensure no interference from background processes we run the benchmark test in a sterile work environment with minimal workload other than the test itself.
we first run the benchmark tests without any changes to measure the baseline performance of the application and then once after applying each of the changes that passed unit testing.
.
analyzing benchmark results rq4 .
.
comparing against baseline.
allocations are expected to stay consistent for c applications as long as the benchmark tests are deterministic so it is easy to tell if the change has improved memory usage by comparing the allocated column as shown in figure .
we consider a change to be a performance improvement in terms of memory if it reduces allocations compared to the baseline.
for test duration we use the mean stddev and iterations columns representing the sample mean standard deviation and size respectively.
we make the assumption that the test duration readings are normally distributed.
for each benchmark in the suggestion sample we conduct a one tailed welch s t test at significance level to determine if the population mean of the suggestion code s sample is less than the population mean of the baseline unmodified code sample for the corresponding benchmark.
in other words our null hypothesis is that the population means of two samples are equal and the alternative hypothesis is that the population mean for suggestion is lower than baseline.
we discard the suggestions that fail to reject the null hypothesis.
following this we conduct some additional checks on the remaining suggestions to reduce false positives and to ensure the change provides a significant enough improvement to be reported to the user.
for this check we use the q1 and q3 columns which represent the first and third quartiles of the sample respectively.
we consider a suggestion to be a significant performance improvement over the baseline in terms of test durations if the suggestion s upper tukey fence is found to be lower than the the baseline s lower tukey fence i.e.
if q3suggestion .
iqr suggestion q1baseline .
iqr baseline where iqr is the interquartile range q3 q1.
since there may be noise from background processes this criteria also allows us to be robust to outliers and have fewer false positives.
finally we also ensure that an improvement in allocation or test duration does not cause the other to deteriorate.
.
.
submitting a perf improvement pr.
upon comparing the results against the baseline we found that suggestions improve performance metrics.
these changes were saturated within of the methods.
for each method we verify up to suggestions that pass unit tests and improve memory test duration with a performance expert and submit a pr containing the first change that is a valid improvement.
in case a project has correct suggestions for multiple methods we squash all changes into a single pr.
for the cases where the model had generated correct suggestions it was usually able to suggest the correct patch within the first or the second suggestion that passed unit tests and improved performance.
often times it suggested multiple distinct correct patches that seemed to improve performance.
figure shows two examples of valid performance improvement patches suggested during this evaluation that have been approved by the project owners.
figure above boxplots show improvement in benchmark test durations and allocations over baseline due to deepperf suggestions.
top two boxplots show the absolute and relative improvement over baseline allocations respectively.
similarly the bottom two show the absolute and relative improvement over baseline test durations.
figure shows the improvement in benchmark durations and allocations due to deepperf s suggestions compared to baseline numbers.
looking at the relative improvement we see that deepperf s suggestions typically provide improvement in terms of both allocations and test duration.
interestingly for allocations a few of our suggestions provide improvement on the order of kbs or even mbs.
while others on the lower end do seem to veer in the territory of micro optimizations it should be noted that this is from benchmark tests and the developer may have simply written their benchmark test to be small i.e.
fewer iterations .
we also don t 9know how often the tested code is run when the application sees use by a real customer.
depending on how often the code being tested is exercised during the application s runtime especially if it appears on the application s hot path even these smaller improvements could improve performance significantly.
rq4 in summary we found that for out of the methods deepperf had at least one correct performance improvement suggestion.
these changes usually provided a improvement over baseline in terms of memory or test times.
we ve submitted a total of prs of which have since been approved by the project owners demonstrating the usefulness of our suggestions.
.
false positives rq5 one of our prs was closed because the repo was not open to external contributions.
however the developer did not comment as to whether they considered the changes to be incorrect.
our remaining prs are still open waiting for a response from the project owner.
out of the methods deepperf found an optimization for turned out to be false positives i.e.
they only had incorrect suggestions that seemed to improve benchmark results and somehow managed passed unit tests.
this is a known issue in such models as they often generate suggestions that are test suite adequate but otherwise turn out to be incorrect.
while we make sure the methods we test have a high code coverage that doesn t guarantee that the unit test will detect all mistakes as it may not be written to specifically test the particular method being modified.
another reason could be that the test suite itself is lacking.
one way to combat these cases would be to generate additional unit cases and use them as further validation in addition to user provided unit tests.
one could also train an additional classifier to determine whether a change is correct and use it for filtration.
we leave these explorations for future work.
rq5 for a majority of methods that deepperf found improvements for the changes were found to be valid and correctly passed the unit tests.
however in out of methods we only found invalid suggestions some of which were able to pass unit tests.
while this is not insignificant we believe this can be addressed by the means of generating additional unit tests or training a classifier to identify such cases.
we leave these explorations to future work.
threats to validity deepperf focuses on single file performance improvements but often performance changes require modifications to multiple classes or even files.
to generalize our approach to multiple file performance improvements one could build on ideas like extended context and extending the transformer s input embedding matrices to be able to pass in a larger context potentially from multiple classes or files.
another challenge when constructing input output pairs from a given commit would be determining which changes within the commit are related.
a possible way to addressthis could be to by establishing caller callee information between methods across files or use import statements to see which files are likely connected to the change.
at inference time one could generate the input based on caller callee relationship among files and apply suggested changes to all the files involved.
our in the wild evaluation used benchmark tests to validate the performance gains from our suggestions.
it is difficult to know how much if any performance improvement this would provide to the end user of the application.
but the fact that the developer wrote benchmark tests for these methods is a strong indication that the code must be frequently exercised and expected to be on the application s hot path.
future work could replace benchmarking and combine our model with profiling or load testing instead to assess the performance gains in a more realistic usage scenario.
related work we describe how are work complements prior work in performance bug detection as well as automated bug detection.
.
performance bug detection fix there is a rich history of building tools for detecting performance bugs and improving performance.
the majority of these tools identify code locations that take a long time to execute.
several tools generate or select tests for performance testing .
other performance detection tools focus on detecting a specific type of performance bug.
for instance a set of tools have been developed for detecting runtime bloat low utility data structures database related performance anti patterns false sharing problem in multi threaded software and detecting inefficient loops .
approaches fixing specific performance issues such as repeated computations software misconfigurations loop inefficiencies etc.
have also been developed.
our tool extends the prior work on performance bug detection and fix by developing a system that focuses on alleviating general performance problems and considers both source code features as well as performance symptoms through benchmarking.
.
automatic bug detection prior work has investigated the use of static analyzers for detecting software bugs .
more recently researchers have started to explore the usage of machine learning for both software bug detection and bug fix.
for instance in c c vuldeepecker uses deep learning to detect two types of vulnerabilities.
similarly russell et al.
propose a machine learning based method vulnerability detection in c c code bases.
in java pang et al.
trained a machine learning model to predict static analyzer labels for java source code.
deepfix leverages deep learning to generate fixes for simple syntax erros .
we are uniquely contributing to this area of research by leveraging neural networks for detecting optimization opportunities and suggesting performance improvements.
conclusions detecting and fixing performance bugs remains an important yet challenging problem in the software development process.
our work makes three contributes to address this problem.
first we 10present a novel transformer based model to automatically generate patches providing performance improvement.
second we conduct an empirical evaluation of our model to show that it outperforms the baselines over a dataset of performance optimizations collected from performance commits made by c developers to open source repos on github.
through this evaluation we showed that our model is able to provide a wide range of performance optimizations which were verified by performance experts.
finally we present a highly practical end to end pipeline showcasing our vision for automatically generating performance improvements for real world projects.
this pipeline consists of our model alongside unit testing and benchmarking which are used to validate the generated patches.
we show that our model is able to suggest valid performance improvements that lead to tangible performance gains to real world applications.
we submit pull requests containing the optimizations generated by this pipeline.
several of these prs have since been merged showing that our changes are considered valuable by the project owners.