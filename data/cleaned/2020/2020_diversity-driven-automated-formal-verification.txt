diversity driven automated formal verification emily first university of massachusetts amherst amherst ma usa efirst cs.umass.eduyuriy brun university of massachusetts amherst amherst ma usa brun cs.umass.edu abstract formallyverifiedcorrectnessisoneofthemostdesirableproperties ofsoftwaresystems.butdespitegreatprogressmadeviainteractivetheorem provers such as coq writing proof scripts for verification remainsoneofthemosteffort intensive andoftenprohibitivelydifficult software development activities.
recent work has cre ated tools that automatically synthesize proofs or proof scripts.
for example coqhammer can prove .
of theorems completelyautomatically by reasoning using precomputed facts while tactok andastactic whichusemachinelearningtomodelproofscripts and then perform biased search through the proof script space canprove12.
and12.
ofthetheorems respectively.further these three tools are highly complementary together they can prove .
of the theorems fully automatically.
our key insight is that control over the learning process can produce a diverse set of models and that due to the unique nature of proof synthesis the existenceofthetheoremprover anoraclethatinfalliblyjudgesa proof s correctness this diversitycan significantlyimprove these tools proving power.
accordingly we develop diva which uses a diversesetofmodelswithtactok sandastactic ssearchmechanism to prove .
of the theorems.
that is diva proves more theorems than tactok and more than astactic.
complementary to coqhammer diva proves theorems added value that coqhammer does not and theorems no existing tool has proved automatically.
together with coqhammer divaproves .
of thetheorems the largest fractionto date.
weex plore nine dimensions for learning diverse models and identify which dimensions lead to the most useful diversity.
further we develop an optimization tospeedup diva s execution by .
our studyintroducesacompletelynewideaforusingdiversityinmachinelearningtoimprovethepowerofstate of the artproof script synthesis techniques and empirically demonstrates that the im provement is significant on a dataset of 68k theorems from open source software projects.
ccs concepts software and its engineering software verification formal software verification theory of computation automated reasoning.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
automated formal verification language models coq interactive proof assistants proof synthesis acm reference format emily first and yuriy brun.
.
diversity driven automated formal verification.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa 13pages.
introduction buildingprovablycorrectsystemsiscriticalinhigh stakesdomains such as aerospace engineering and software for medical devices.
however mostindustrialverificationtoolseitheraimtosimplify the verification process by sacrificing soundness or significantlyrestricttheprogramminglanguageinwhichthesystemis written .apromisingmethodforbuildingcorrectsoftwarehas beentouseprogramminglanguagesthataredesignedtoinherently support program verification such as interactive theorem provers itps including coq agda and isabelle hol .
itps havehadsignificantimpactonindustry.forexample airbusfrance usesthecoq verifiedcompcertccompiler toensuresafety and improve performance of its aircraft .
chrome and android both use cryptographic code formally verified in coq to secure communication while mozilla has its own verified cryptographiclibraryforfirefox improvingperformance .multiple companies have been successful in using proof assistants to provide formal verification services including bedrock systems who builds formally verified solutions for the healthcare infrastructure and financial domains certora who formally verifies smart contracts andgalois inc. whoverifiescompilercorrectness and hardware design .
meanwhile amazon successfully applies formal verification to cloud security problems in amazon webservices providingtoolsforuserstodetectentireclassesof misconfigurationsthat can potentially expose vulnerable data .
withitps theuser aprogrammer specifiesatheoremabout a property of the software and writes a proof script a series of annotated prooftactics that theinteractive theorem prover uses to attempt to construct a proof of the theorem.
still even with the help of an itp the effort required to write proof scripts is oftenprohibitive.
the coq proof of the c compiler is more than three timesthatofthecompilercodeitselfandtookthreepersonyears ofwork .meanwhile ittook11personyearstowritetheproof script to verify a microkernel .
as a general rule because of theexpenseofverification nearlyallsoftwarecompaniesshipis unverified.
however someformalverificationcanbefullyautomatedbysynthesizing either the underlying proofs or the guiding proof scripts.
aseries oftools called hammers e.g.
coqhammer use aset of precomputed mathematical facts to attempt to hammer out ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa emily first and yuriy brun aproof.evaluatedonthecoqgymbenchmark coqhammer canautomaticallyprove26.
oftheoremsfoundinopen source coq projects.
but hammers are restricted by their precomputed facts and cannot reason about proof approaches such as induc tion greatlylimitingtheirpower.toovercometheselimitations researchers have used machine learning to model existing proof scripts andthen givenanewtheorem appliedthatmodeltoguide metaheuristic search to attempt to synthesize a new proof script .
while these tools tend to prove fewer theorems e.g.
astacticproves12.
andtactokproves12.
they are capable of applying higher order proof approaches learnt from existing proofs including induction and so are complementary to hammers.
together with coqhammer they prove .
of the theorems.
the central goal of this paper is to improve on this fraction particularly focusing on the tools that model existing proofs.
we make two key observations that enable us to improve the proving powerofproof script synthesis techniques.first theformalverificationdomainisauniqueapplicationofmachinelearningbecauseithasacorrectnessoracle.inmostmachinelearningappli cations itisnotknownwhenthemodeliscorrect.thisiswhymodels are typically evaluated for precision or accuracy.
in the formal verificationdomain however theinteractivetheoremprovercan use a synthesized proof script to determine whether it truly proves the underlying theorem.
if the prover can get to qed then the synthesized proof script must be correct.
thus proof script synthesis systemsalwayshaveaprecisionof100 theyneverreturnafailed script instead continuing the search or timing out.
while recall maybelow precisionisalwaysperfect.second variationsinthe modelscanalterthesearch basedsynthesisofaproofscriptenough that two models can potentially produce different scripts for the sametheorem.this inturn can hypothetically leadtomodelsthat prove complementary sets of theorems.
and because of our first observation they can be combined without sacrificing their power.
the combined system can synthesize successful proof scripts for all theorems each one of the models can prove individually if one model fails to synthesize a successful script the theorem prover unequivocallytellsusso andweinsteadusetheothermodel ssuccessful script.
thus if one can learn models that differ in a way to producedifferentscripts potentially thissetofmodelsmaybeable to prove far more theorems than a single model.
the central question this paper answers is whether model diversity can be created to improve the proving power of proof script synthesis techniques and whether such an approach improves on the state of the art automated formal verification techniques.
we find that the answer to both questions is yes.
as we will demonstrate on a benchmark of 501theoremsfrom122open sourcesoftwareprojectsincoq weareabletocreateasetof62modelsbyvaryinglearningparameters and learning data that together prove more theorems than tactokand77 morethanastactic despiteusingthesamesearch method.
combining our approach diva with coqhammer wecanprove33.
ofallthetheorems thehighestsuchresultto date.
diva proves theorems that none of the prior tools have been able to prove.
the difficulty of manually writing proof scripts for formal verification is so great that even small improvements in proving power can be significant and the savings in human effort that our approach represents are quite substantial.ourinsightsenableforacompletelynewwaytocombinemachine learning models.
of course the idea of combining models is notnew.ensemblelearningallowsweightingtheresultsofmultiple models to improve the precision or recall of a single model .
and stacking uses a classifier to decide which model to apply to eachinput .whileboththesemethodscanimproveprecision andrecallinpractice theycanalso hypothetically reducethem and often cannot properly amplify the correct results of a smallminority of models.
by contrast in our domain our method for combining models can never produce a wrong result or ignore the correctresultproducedbyevenasinglemodel.thisrepresentsa killer app for ensemble learning and stacking.
we are the first to combinetheideaofensemblelearningwithanoracletoproduce optimal stacking.
thispaperexploresninedimensionsforlearningdiversemodels andidentifieswhichdimensionsleadtothemostusefuldiversity.
altering the types of information the proof script state and term themodellearnsfromresultedinthegreatestdiversity whilevarying the depth of the proof script and the learning rate provided the secondmostdiversity.asrunningalargenumberofmodelscanbe inefficient we develop a model interrupts optimization that speeds up diva s execution by .
the main contributions of our work are a novel approach for combining varied machine learning models to formally verify software properties.
a systematic exploration of which learning dimensions provide usable model diversity.
an implementationof our approach diva that proves moretheoremsthantactokand77 morethanastactic the prior work most closely related to ours.
diva is open source and is available at an optimization for improving diva s performance.
aplatformforevaluatingmodelsandrerunningexperiments and all data and source code used in our experiments for replications .
therestofthispaperisstructuredasfollows.section 2explains verificationincoq.section 3presentsdiva andsection 4evaluates ouruseofdiversitytoincreasetheprovingpowerofautomatedformal verification tools.
section 5places our research in the context of related work and section 6summarizes our contributions.
theorem proving in coq coq is a dependently typed language with a small kernel whichprovides a high assurance that coq verified programs are truly correct.
however program verification in coq is not automatic.
to proveatheoremincoq aprogrammermustwriteaproofscript in ltac which when executed helps automatically generate a proof in gallina of the theorem.
alternatively metaheuristic search techniques can automatically search for a proof script thus alleviating the burden for the programmer .
however metaheuristicsearchisonlyasgoodasthepredictivemodelthat is used to bias the search.
in this section we will discuss how a programmer interactively writes proof scripts in coq section .
how metaheuristic search can be used to automatically generate a proof script section .
and design considerations for building a predictive model to generate proof scripts section .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diversity driven automated formal verification icse may pittsburgh pa usa .
interactively writing proof scripts whenatheoremisprovenincoq thismeansthatingallina coq s internallanguage a proofterm ofthedesiredtypehasbeenconstructed.
the type of this term is the theorem itself.
a programmer could write the gallina proof term themselves but this can be a long unforgiving process .
to simplify this task coq has a meta programming language called ltac in which programmers canwrite proofscripts whichwhencompletedandrun generate the gallina proof term automatically.
programmersusean interactiveproofassistant e.g.
coqideor proofgeneral towriteproofscripts whichconsistofasequence ofproof tactics.
the proof assistant executes aproof script even a partialone andprovidesimmediatehuman readablefeedbackafter each tactic s execution.
this feedback is coq s internal proof state which includes the goalsto prove the local context of assumptions and theenvironment of proven so far set of facts.
the programmer can even ask to see the intermediate gallina proof term by writingandexecutingthe show proof commandintheirproofscript.
when starting to prove a theorem coq s proof state is a single goal whichisthetheoremitself thecorrespondingprooftermis ?goal .theaimistomanipulatetheproofstatethroughtheuseof tactics until the goal is proven and thus removed from the proofstate.
since the search space of goal manipulation is too large aprogrammer helps manage the exploration by using the current proof state to select a sequence of proof tactics to try.
the interactive proof assistant checks that a partially written proofscriptisvalidandupdatesthecurrentproofstate allowingthe programmer to incrementally develop a proof script.
the programmercanchooseatactic examinetheoutputfromtheproofassistant andthenchoosethenexttactic.iftheprogrammerchoosesaninvalidtactic theproofassistantdisplaysanerror.iftheprogrammer choosestacticsthatarevalid butdonotmakeprogress theycan usetheproofassistanttobacktracktoanearlierproofstateandtry adifferentapproach.theprogrammercontinuesselectingtactics until the proof assistant prints no more subgoals and then uses qed to complete the proof script.
.
proof script synthesis via metaheuristic search ininteractiveproofscriptgeneration theburdenisontheprogrammer to choose the sequence of tactics.
to remove this burden from theprogrammer metaheuristicsearchtechniquescansometimes automatically generate a proof script.
the space of possible proof scripts is infinite and quite complex.
becauseofthissizeandcomplexity automaticallysearchingblindly through this space for a proof script that might prove a theoremis unlikely to succeed.
metaheuristic search can help guide the search to improve the chances of success and in fact is often successful .
such search starts with an empty proof script and predicts a first most likely proof step.
this prediction can be made based for example on the theorem being proven and examples of past successful proof scripts.
the search executes the partial proof script and determines using some heuristic basedfitness function whether adequate progress has been made.
if it has not the proof search can try another likely proof step.
if it has the search can iteratively augment the partial proof script adding subsequent predicted proof steps making progress toward provingsearch and predict proof script model next step new proof script 1input if compiles proof state is not duplicate and subgoals still exist update proof scriptif doesn t compile or proof state is duplicate predict another tactic final proof if no more subgoals apply qedintros n induction n intros n induction n apply h simpl intros n induction n simpl qed next step 1apply h next step 3trivial beam size new proof script intros n induction n simpl new proof script intros n induction n trivial apply figure the process of synthesizing a proof script using metaheuristicsearch biasedbyablackboxpredictivemodel.
given an incomplete proof script a model can predict thenext proof step.
here using a beam search of width the model predicts likely next steps.
if using the step satisfies certain criteria here the proof script must compile and the resulting proof state must not have been previously seen within this proof script the process iterates until either theproofstatehasnosubgoalsandtheproofscriptcanbe completed using qed or the search reaches a timeout.
the theorem.
making reasonably accurate predictions is of course a critical part of successful metaheuristic search and section .
will describe possible ways to do that.
figure1shows how beamsearch can use a predictive model to biasametaheuristicsearchforaproofscript.inthisexample the modelpredicts3likelynextproofscriptsteps thebeamwidthis3 .
thesearchthenusesaheuristic basedfitnessfunctiontodetermine criteriaforapplyingthecandidateproofsteps.here thecriteriaarethatthepartialproofscriptcompilesandresultsinaproofstatethat hasnotbeenpreviouslyseenwithinthissearch.ifsuccessful the searchappendstheproofsteptothescriptanditerates growingthe script.
once the proof state has no more subgoals the proof script can be completed by using qed.
the search fails if it times out.
.
proof script modeling priorproofscriptsynthesistools suchasastacticandtactok use the predictions from learnt proof script models to bias the metaheuristicsearchforaproofscript.suchamodelislearntfromaset ofexisting successfulproofscriptstopredictthenextproofstep tacticandarguments ofanincompleteproofscript.recallfrom section2.1that there are three relevant aspects of proof scripts we may want to encode to serve as input to such a model the proof state theproofscript andthegallinaproofterm.astacticonlyen codestheproofstate whiletactokencodesboththeproofstateandtheproofscript.therehasyettobeaproofscriptsynthesistoolthatencodesthegallinaproofterm.next sections .
.
.
.
and .
.
describe how to encode the proof state proof script and gallina proof term respectively.
.
.
encoding the proof state.
theproofstateconsistsofthegoals to be proven local context and the environment.
while the programmerseestheminahuman readableformat eachtermofthe proof state has an underlying abstract syntax tree ast representation.
astactic and tactok serialize these asts and encode them using a neural model specifically a treelstm .
prior work has authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa emily first and yuriy brun environment local context goal proof script proof termasts seqsproof state encoder proof script encoder proof term encoderembeddings tactic and arguments next step ast tactic decoder figure2 modelofproofscript whichdivausesinternally to drive search.
empiricallyarguedthatneuralmodelsaremuchmoreeffectivethan other architectures .
.
.
encoding proof script features.
the proof script is comprised of a sequence of tokens in ltac.
for the model to encode these tokens each proof script needs to be preprocessed to remove highfrequency low signal tokens such as punctuation.
then encoding suchasequenceistraditionallydoneusingalanguagemodel .
language models are widely used in natural language processing tasks .
the primary function of a language model is to predict the next token in a sequence of tokens.
while prior work has used n grams to model coq tactok found neural language models work better to encode the sequence of tokens because it can generate a representative vector embedding forthe sequence that can then be combined with other types of inputs.amongthemostextensivelyusedneurallanguagemodelsaretransformers andrnns .tactokusesanrnn specifically abidirectionallstm becausetransformersrequiremassive amountsofdatatotrain whichistypicallynotavailableinthe formal verification domain.
.
.
encoding the proof term.
prior tools have not encoded proof terms but conceptually thegallinasequenceissimilartotheproof script ltac sequence and we encode it in a similar way using a bidirectionallstm .thisallowsallthree theproofstate proof script and proof term to be encoded with a single model.
diva diversity driven synthesis machinelearningmodelscanbesensitivetonoiseinthetraining data and to parameters applied during the learning process .thissensitivitycancausegreatvariabilityintheaccuracy of models.
of course this can hurt the generalizability of machine learningresults butwepositthatintherightdomain thissensitivity and the diversity of models it can produce can provide a significant benefit.
in the formal verification domain tools such as astactic proverbot9001 and tactok use a learnt model of a proof script to guide metaheuristic search toward synthesizing a proof script for a theorem.
variations in the models can alter the search resultinginpotentiallydifferentattemptedsynthesizedscripts.thekeyuniquenessofthisdomainisthataninteractivetheoremprover canactasanoracleforeachproofscript.iftheproofscriptleads thetheoremprovertogenerateaproofterminatingin qed thenthe proofscriptis bydefinition correct.thisallowsasynthesistoolto tryapplyingmanydifferentmodelstobiasthesearchindifferenttreelstm astparse embedding a proof state encoderbidirectional lstm seqparse embedding b proof script and term encoders figure3 theneuralmodelsusedtoencodetheproofstate ast proof script sequence and the proof term sequence.
ways and then pick out just the successful synthesis attempts discarding the failed ones.
thisis notthe typicalcase inapplications ofmachine learning.
ensemblelearning andstacking attempttocombinethe resultsofmultiplemachinelearningmodelstoimproveprecision or recall.
however without an oracle ensembles and stacks are unlikelytoalwayspickthecorrectresult especiallywhenrelatively fewofthediversemodelsproduceit.bycontrast inourdomain withthetheoremproveractingasanoracle evenasinglemodel producing the correct proof script can establish an answer.
to demonstrate this insight we develop diva a proof scriptsynthesis tool that uses the diversity in machine learning to sig nificantly improve its proving power.
diva is open source and is available at .
diva s key contributions are the generation of a diverse set of modelscapableofprovingcomplementarysetsoftheorems amech anismforcombiningthebenefitsofthemodels andanoptimization to make running a large number of searches using independent models feasible.
toautomateproofscriptsynthesis divausesalearntmodelofa proofscripttoguidemetaheuristicbeamsearch.duringthissearch divasamplesafixednumber beamwidth ofthemostlikelytactics predicted by the model across all search tree nodes at the samelevel and then uses these tactics to search for a complete proofscript.divabacktrackswhenthecoqcompilerfailstocheckthe attempted proof script step or detects a duplicate proof state.
diva usesthesamebeamsearchconfiguration widthof20 searchdepth limit of and a timeout of minutes as astactic and tactok.
to intentionally produce a diverse set of models that prove complementary sets of theorems control over the learning processis key.
when training a model of proof scripts diva varies thelearning parameters and which features of the training data to encode.next section .1describeswhatadivamodellookslike sections3.2and3.3detailhowdivageneratesadiversesetofmodels by controlling learning parameters and the encoded features of thetrainingdata respectively andsection .4explainsourdiva efficiency optimization.
.
diva s learnt model figure2illustratesdiva sproofscriptmodel learntfromasetof existing proof scripts.
diva uses the predictions from this model to drive the search for a complete proof script.
figure3detailstheencodersusedinthedivamodeltoencode relevantaspectsofproofscripts.figure a presentstheproofstate encoder which diva uses to encode the goal local context andenvironment inastform.toencodeatree itusesatreelstmnetwork which generates embeddings for each proof state term.
figure b details the proof script encoder which diva uses toencodetheproofscriptsequence.weencodetheparsedsequence authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diversity driven automated formal verification icse may pittsburgh pa usa of previous tokens using a bidirectional lstm which generates an embeddingforthesequence.abidirectionallstmimprovesonthelstm by capturing more contextual information by processing the input sequence in two ways forward and backward allowing theoutputlayertosimultaneouslyseebothdirectionsofinformation.divaencodesthegallinaproofterm thefirstsynthesistoolto do this using the same encoder in figure b .
similar to the proof script sequence encoding we choose to encode the sequence of prooftermtokensusingabidirectionallstm generatinganembedding.
diva jointly learns embeddings for the sequences and asts.
diva s tactic decoder is modified from the tactic decoder first used in astactic and later tactok.
this tactic decoder is con ditioned on the sequence of embeddings.
in diva however the embeddingsareaconcatenationofasubsetoftheembeddingsgeneratedfrom theproof script proof term and proof stateencoders.
this allows for modeling of more relevant proof script aspects and thechoiceofwhichsubsettocombineallowsustocreatevariability in the models see section .
.
the tactic decoder then generates a tacticbysequentiallygrowinganast .itchoosesaproduction rule from the context free grammar of the tactic space at a nonterminal node in the ast while it synthesizes arguments based on semantic constraints at a terminal node.a gru controls thisprocessofgrowingthetree asitupdatesitshiddenstateusing the input embeddings of the partially generated ast.
diva trains the model on a set of existing proof scripts.
each proofscriptinthissetisbrokendownintotraininginstances whicharetheinputstothemodel.atraininginstanceiscomprisedofthe proof state before the tactic execution the proof script up to the tactic execution the gallina proof term before the tactic execution and the next step of the proof script.
the diva model jointly learns embeddings for the proof state asts the proof script and proofterm sequence and then uses these embeddings to predict thenext proof script step in the form of an ast.
the model sends the predicted ast along with ground truth nexttactic ast to the trainer where the trainer compares these tactic asts and backpropagates the loss.
unlike prior tools diva jointly trains a language model over the tokens in the proof term.
section .2details further modifications in this training process for creating diva s diverse models.
.
diversity via varying learning parameters onewayinwhichwecreateadiversesetofmodelsisbyvarying the learning parameters which affects the model s size and the learningalgorithmitself.forthis westartwiththetacmodelfrom tactok and explore varying six dimensions sequence tactic depth sequence token depth the learning rate the embedding size the number of layers and the order of the training data.
tacticandtokensequencedepth.
thesequencedepthdenotesthe size of the input the learning algorithm considers.
when training themodelcanconsidertheentireproofscriptwrittensofar orpartofit suchasonlythemostrecenttacticanditsarguments orseveral most recent tactics with arguments or only several most recent tokens.
the proof script encoder considers only that portion of the proofscript and symmetrically thedecoderwillconsiderthesamedepthwhendecodingthenextproofstep .divavariesthesequence depth along both tactics and tokens from a depth of which doesnot consider the proof script at all it considers only proof state making the model equivalent to astactic s model to the entireproof script.
diva considers sequence depth sizes excluding the start token of and .
learningrate.
duringtraining thealgorithmupdatesthemodel s weightsineveryiteration.thelearningrateisahyperparameter that determines how much the weights can be changed in each iteration.alargerlearningrateislesslikelytoresultinthetraininggettingstuckinalocaloptimum butmayalsotakelongerto converge or fail to explore a region long enough to find an optimal solution.
accordingly the models produced by varying the rate canbequitedifferent.divaconsiderslearningratesof 10kfor k .
modelsize embeddingandlayers .
themodel ssizeisdefinedby two hyperparameters the number of model layers and embedding size whichisthesizethevectorspaceinwhichaproofaspectis embedded.
diva varies the proof script encoder size by trying and layers and embedding sizes of and .
training data order.
the order of the training data can affect the model .
we vary the order in which diva sees the training instances by creating ten random orders.
.
diversity via varying training data the second way in which we create a diverse set of models is by varying aspects of the training data available to the learning algorithm.therearethreetypesofdatainthetrainingproofscripts the proof state the proof script tactics and tokens and the proof term that the proof assistant generates when it executes the proof script recallsection .
.whentrainingamodel weeitherinclude each of these three types of data or we exclude them.
for the proofscript weincludeeitherthetacticsorthetokens sincetheyencode fundamentally the same information.
this leads us to a total of models.
for the models that do not include proof state when we encodethetraininginstancethatrepresentstheverystartofproofscriptsynthesis weincludethetheorembeingproven otherwise the model would not know what it is trying to prove .
similarly at test time when synthesizing the first proof script step we include the theorem being proven.
.
efficiently combining model executions executingalargesetofmodelsinsequenceisslowsincedivahasto waitforamodeltofinishitsproofscriptsynthesisattemptbeforeit cantrythenextone.wedevelopmodelinterruptstoimprovediva s efficiency.
in model interrupts given a set of models diva assigns anarbitraryorderofmodelapplication.inorder eachmodelwillbe given a specified amount of time to try to synthesize a proof script.
once the time runs out the next model attempts to synthesize a proof script from scratch.
figure 4illustrates this concept.
the first model attempts synthesis from scratch for xseconds at which point if a complete proof script is not generated the partial proof scriptisstoredandthesecondmodelattemptstosynthesizeaproof scriptfromscratchfor xseconds.andsoon.onceeachmodelis given an opportunity to try for xseconds and a complete proof scriptisnotfound themodelswillbegivenmoretimetosynthesize authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa emily first and yuriy brun model1 model2 timetactic 1model prediction tactic1.
... tactic k.... ... tactic .
tactic k tactic k proof script... ...... ... x s 2x s... tactick figure model interrupts allows diva to let models take turns synthesizing proof scripts from scratch.
aproofscriptstartingfromthestoredpartialproofscriptassociated with the model.
evaluation weevaluatedivatomeasurehowmuchdiversityofmodelsofproof scripts can increase the effectiveness of proof script generation.
wefollowthemethodologiesofpriorevaluationsofproof script synthesistools intermsofthedataset section .
.
and metrics section .
.
used we compare to two state of the art proof script synthesis tools astactic and tactok which usethesamemetaheuristicsearchforproof scriptsynthesisasdiva.
we further compare diva to the state of the art proof synthesis tool coqhammer .
our evaluation answers four research questions rq1 does diverse modeling significantly improve proofscript synthesis over state of the art approaches coqhammer astactic and tactok?
rq2 howmuchmodeldiversityresultsfromvaryingthe modellearningparameterssequencedepth learning rate number of layers size of embeddings and training order and how does this model diversity affect proof script synthesis effectiveness?
rq3 how much model diversity results from varying which aspects of the training proofs tactics tokens proofstate gallinaproofterms areavailable to the learning process and how does this model diversity affect proof script synthesis effectiveness?
rq4 howeffectiveisourinterruptsmechanismforimproving diva efficiency?
all of our evaluation data and the source code to reproduce our results are available .
.
evaluation methodology we first describe the dataset and metrics we use to evaluate diva.
.
.
dataset.
inourevaluation weusecoqgym thestate ofthe art benchmark used in prior evaluations of formal verification tools .
the benchmark consists of theorems from 123open sourcesoftwareprojectsincoq.thecoqgymbenchmarkcomes with a preselected training set of projects with human writtenproofscripts andtestsetoftheremaining13 theorems from projects.
our earlier tactok evaluation was unable to reproduce prior results for astactic s performance for oneproject coqlibrary undecidability due to internal coq errors when processing the proof scripts.
accordingly we exclude this project from our evaluation.wewereabletoreproducetheresultsfortheremaining 26projectsof10 782theorems.intotal ourtrainingandtestsets have theorems from projects.
.
.
metrics.
we measure four quantities in answering our research questions success rate added value diversity and mean time to prove a theorem.
success rate.
the success rate of a tool widely used in prior evaluations is the fraction of all theorems for which the tool generates a succesful proof script.
added value.
the added value of tool a over tool b is the numberofnewtheoremstoolaprovesthattoolbdoesnot divided by the number of theorems tool b proves.
diversity.
givenasetofmodels wewishtoknowhowmuch diversity they yield with respect to their ability to prove theorems.
and so we think of the diversity of a set of models as the diversity ofthecorrespondingsetsoftheoremsthatthemodelsprove.our goalwiththediversitymeasureistobeabletocomparehowmuch diversity results from various methods for creating models so that we can compare the different methods.
informally given a set of sets of objects theorems we define a familyofdiversityfunctions suchthatthe kthdiversityfunction dk measures the relative increase in objects contained in ksets as compared to k sets.
so for example for a set of models d5denotes the fraction of the additional theorems out of all the theoremsprovedbyat leastonemodel thatareableto be proved by adding a fifth model to a set of four models on average.
moreformally let tbeasetofobjectsandlet mbeasetofsubsets oftsuch that the union of all sets in mis equal to t. then for eachk ... m thekthdiversity function dk 2t r isthe averageincrease in termsof thefraction of t that theunion ofkelements of mcontains over the union of k elements of m. thus for all mk m such that mk k and for all mk m such that mk k dk m is the average value of mk mk t .
givenasetofmodels wecomputethediversityfunctionsempirically.
we use each model to attempt to synthesize proof scripts to prove theorems.
we then compute t the set of all theorems that can be proven by at least one model.
then to compute dk w e for each model compute how many additional theorems it proves comparedtoeachsetof k 1models.wethencomputetheaverage ofthosenumbers anddivideitby t fornormalization.intheend dk m istheaverage fractionoftheoremsproven byaddinga kth modelto asetof k 1models.notethat thesumof dkforallkis and that diversity is monotonically non increasing with respect tok that is dk dk.
meantimetoproveatheorem.
tomeasureefficiency we compute the mean time it takes to generate a proof script for a theorem averagedoverallthetheoremsforwhichweproducea successful proof script and over all the possible orderings of the models used in the metaheuristic search.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diversity driven automated formal verification icse may pittsburgh pa usa tool theorems proven diva s value added astactic .
.
tactok .
.
coqhammer .
.
all prior tools .
.
diva .
diva coqhammer .
figure5 theoremsprovenbyandthesuccessrateofdiva astactic tactok coqhammer andthecombinationofthese tools out of the theorems in coqgym s test dataset.
divaprovidesvalueaddedovereachofthesetools and11.
value added over the combination of all three.
.
rq1 does diversity help diva outperform the state of the art?
we created models by varying learning parameters and aspects of proof scripts to encode recall the models described in sections3.2and3.
.overall wegeneratedthese62modelsfordiva to use.
we compare diva to the state of the art synthesis tools astactic tactok and coqhammer .
astactic and tactok like diva learn from existing proof scripts to predict the next step of the proof script.
coqhammer uses a fundamentally different approach.
whereas coqhammer produces proofs in coq s logic gallina diva searches the proof script space.
when the coq compiler executes a proof script it generates a proof.
proofs cannot bewrong whileproofscriptscanbe e.g.
aproofscriptthatconcludes with proof completed may not lead to a valid proof when it is checked by the coq compiler .
thus it is reasonable to compare proof script synthesis tools such as diva to coqhammer with respecttothetheoremstheyareabletoprove.however sincetheirapproaches are so fundamentally different it is expected that these tools are likely to be complementary performing well for different theorems.
while coqhammer and diva are likely to perform similarly well for some simpler classes of theorems coqhammer is at a fundamental disadvantage though for other classes of theorems such as ones that require induction to prove.
on our evaluation set of theorems astactic proves .
and tactok proves .
theorems.
coqhammer proves .
theorems.
prior to performing our evaluation weexpectedthatdivawouldprovestrictlymoretheoremsthanastacticandtactok thoughhowmanymoreremainedanimportant question thatitwouldnotprovemoretheoremsthancoqhammer but that itwould prove some complementary theorems thus providingsignificantaddedvaluecomparedtocoqhammer aswas the case in astactic and tactok evaluations .
figure5showsthesuccessrates aswellastherawnumberof theorems proven by the four tools and the value diva adds over eachtool aswellastheircombination.divaproves2 .
of the theorems.
this means diva proves2 .
more theorems than astactic and2 .
more theorems than tactok.
since these tools use the same search mechanism .
tactokastactic .
unproven theoremsdiva .
.
.
.
.
.
.
.
.
.
.
.
coqhammer115 .
.
figure the breakdown of how many theorems are proven byeachcombinationoftools.divaproves364theoremsno other tool proves.
thesesignificantimprovementsaredueentirelytotheuseofmodel diversity.
whilecoqhammerprovesmoretheoremsthandiva divaproves theorems that coqhammer does not an added value of781 .
.
figure 6shows a venn diagram of the theorems diva astactic tactok andcoqhammerprove.together thesefourtools prove theorems for a success rate of .
whereas without diva theotherthreeto olsprove 282theorems.
becauseastactic and tactok have an added value of over diva coqhammer anddivaprovethe3 282theoremsontheirown withouttheother tools help.
diva adds a value of .
over the combined state of the art and proves theorems no tool has previously proven.
ra1 our diva diversity mechanisms are successful in creatingmodeldiversitysufficienttosignificantlyimprove the proving power of metaheuristic search based tools addedvalue .divaalsogenerates27.
added valueovercoqhammer andp roves364 theoremsnoprior tool has proven.
together with coqhammer diva reaches a new milestone proving over one third of all theorems completely automatically.
.
rq2 learning parameter diversity to investigate the effectiveness of varying learning parameters on generating diverse models we conduct a series of experiments by generatingmodelsvaryingthoseparameters usingtheresulting modelstosynthesizeproofscripts andthenmeasuringthe diversity of the sets of theorems the models prove.
as section .2described thefactorsweinvestigatearesequencedepth learningrate number of layers embedding size and training order.
figure 7details how muchdiversitydivaproducesbyvaryinglearningparametersin training its models.
tactic depth diversity.
we vary the tactic sequence depth consideringdepthsof0 29and a total of models.
note that the depth model is equivalent to astactic and the depth3 model isequivalent to the tacmodel intactok.
overallthese16modelsprove1 858theorems whereas authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa emily first and yuriy brun kth model added0.
.
.
.
.
.
.
.7average diversity of adding kth model k050010001500average theorems proven by k models a tactic sequence depth total theorems kth model added0.
.
.
.
.
.
.
.7average diversity of adding kth model k050010001500average theorems proven by k models b token sequence depth total theorems kth model added0.
.
.
.
.
.
.
.7average diversity of adding kth model k02004006008001000120014001600average theorems proven by k models c learning rate total theorems kth model added0.
.
.
.
.
.
.
.
.8average diversity of adding kth model k0200400600800100012001400average theorems proven by k models d embedding size total theorems kth model added0.
.
.
.
.
.
.
.
.
.9average diversity of adding kth model k0200400600800100012001400average theorems proven by k models e number of layers total theorems kth model added0.
.
.
.
.
.0average diversity of adding kth model k020040060080010001200average theorems proven by k models f training data order total theorems figure the diversity exhibited by altering learning parameters tactic sequence depth a token sequence depth b learning rate c embeddingsize d numberoflayers e andtrainingdataorder f .theleftgraphineachpairshowsthediversity measure as a function of the number of models e.g.
the k 5bar is the mean fraction of additional theorems proven by pickingarandom5thmodelthatarandomdisjointsetof4modelshasnotproven .therightgraphineachpairshowsthe meannumberoftheoremsprovenby kmodels.thebox and whiskersindicatethemaximum and25 tiles and minimum values.
on average a single mo del proves theorems.
diva s diversity isresponsiblefora74.
increaseinprovingpower!theleftgraph infigure a showsthediversityofthesetoftacticsequencedepth models recall the diversity metric from section .
.
.
the kthbar showsdkfor the models.
that is the kthbar states the fraction ofextratheoremsprovenby krandommodels thatarandomsetof k 1modelsdoesnotprove.forexample the k 1barissimply theeffectiveness ofusinga singlemodel .
onaverage .
of the theorems proven by all models together are proven by using one random model .
the remaining .
need diva s diversity mechanism.for k thediversityis0.
meaningthatadding the second model on average adds an additional .
of the totaltheorems proven.
two randomly chosen models prove on average .
.
.
of all the theorems proven by at least one model.therightgraphinfigure a showstheaveragenumberoftheoremsthat kofthetacticsequencedepthmodelsprove.theboxand whiskers indicate the variability in the choice how important is it to select specific kmodels or can they simply be selected at random.
for example a single model can prove between .
and1 .
theoremsfromthetestset.weleavedeveloping mechanisms for selecting models to future work.
token depth diversity.
similar to tactic depth we considered tokendepthsof0 29and30 a totalof16models.
notethatthedepth0modelis again equivalenttoastactic andthedepth30modelisequivalenttothetokmodel in tactok.
overall these models prove theorems slightly fewerthantacticdepthdiversitymodelsdid whereasonaverage a single model proves theorems.
diva s diversity is responsible for a .
increase in proving power.
the left graph in figure b shows the diversity of the token depth models.
a single randommodel proves a slightly larger fraction .
of all the proven authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diversity driven automated formal verification icse may pittsburgh pa usa theorems than was the case for tactic depth models indicating again that token depth provides slightly less useful diversity.
still the remaining .
of the theorems require diva s diversity to beproven.therightgraphinfigure a showsthevariabilityin aselected kmodels.here a singlemodelcanprovebetween992 .
and .
theoremsfrom thetest set.overall token depth provides significant diversity but less than tactic depth did.
learning rate.
we explore different learning rates 10k fork .
overall these models prove 730theorems slightlyfewerthanthedepthdiversitymodelsdid whereas on average a single model proves theorems.
diva s diversity is responsible for a .
increase in proving power.
the left graph in figure c shows the diversity of the learning rate models.
a single random model proves .
of all the proven theorems.
the remaining .
of the theorems require diva sdiversity to be proven.
the right graph in figure c shows the variabilityinaselected kmodels.here asinglemodelcanprove between .
and .
theorems from the test set.
overall learningrateprovidessignificantdiversity andthemodelsaremorediversefromoneanotherthanthesequencedepthmodels but overall result in slightly less proving power.
embedding size.
we explore different embedding sizes .overallthese4modelsprove1 496theorems fewer thanthealreadydiscussedmodels whereasonaverage asingle modelproves1 100theorems.diva sdiversityisresponsiblefora .
increaseinprovingpower.theleftgraphinfigure d shows thediversityoftheembeddingsizemodels.asinglerandommodel proves .
ofall the proven theorems.
theremaining .
of thetheoremsrequirediva sdiversitytobeproven.therightgraph in figure d shows the variability in a selected kmodels.
here a single model can prove between .
and .
theorems from the test set.
overall embedding size provides some diversity though less than sequence depth and learning rate.
number of layers.
we explore different numbers of layers and .
overall these models prove theorems similar to the embedding size whereas on average a single model proves theorems.
diva s diversity is responsible for a .
increase inprovingpower.theleftgraphinfigure e showsthediversityof the number of layers models.
a single random model proves .
ofalltheproventheorems.theremaining24.
ofthetheorems requirediva sdiversitytobeproven.therightgraphinfigure e showsthevariabilityinaselected kmodels.here asinglemodel canprovebetween1 .
and1 .
theoremsfromthe test set.
overall varying the number of layers provides a similar amount of diversity as embedding size.
both parameters effect the size of the learnt model.
trainingdataorder.
weexplore10randomlychosenorderings of the training data.
overall these models prove theorems the smallest number of all the learning parameters whereas on average asinglemodelproves1 073theorems.diva sdiversityis responsiblefora14.
increaseinprovingpower.theleftgraph in figure f shows the diversity of the training data order models.
a single random model proves .
of all the proven theorems.
the remaining .
of the theorems require diva s diversity to be proven.
the right graph in figure f shows the variability in a selectedkmodels.
here a single model can prove between .
and1 .
theoremsfromthetestset.overall even just varying the training data order provided some useful diversity and enabled proving more theorems though the diversity benefits were much smaller than those of the other parameters.
ra2 varyinglearningparametersresultedinsignificant diversity which in turn led to significant improvementin proving power.
varying the depth of the tactics and tokensthemodellearntfromandthelearning rateledto the greatest diversity while varying the size of the model ledtomoderatediversity.varyingtheorderofthetraining data marginally increased the proving power.
.
rq3 training data diversity recall from section .3that there are three types of data in the trainingproofscripts theproofstate theproofscripttacticsand tokens andthegallinaproofterm.wetrainmodelsforallpossible combinationsofthesedatatypes exceptnomodelincludesbothtactics and tokens and we exclude the model that is the empty combination.
in total we learn models.
we first measure the value added by adding each of the three types of information.
the value of adding proof script tactics toa model already encoding the proof state and the gallina proof termis .
proving anadditional345 theorems.
thevalue of adding proof script tokens instead of tactics is similar .
theorems .
the value of adding gallina proof term to a model alreadyencodingtheproofscriptandtheproofstateismuchsmaller .
proving an additional theorems.
if using tokens instead oftactics theaddedvalueis10.
124theorems.
finally thevalueofaddingproofstatetoamodelalreadyencodingtheproofscriptandthegallinaprooftermis21.
provinganadditional135theorems.
if using tokens instead of tactics the added value is .
theorems.
we observe that while in previous scenarios tactics and tokens behaved similarly here tokens exhibit much more diversity thantactics.
inallthreecases tokensexhibitedgreaterdiversity than tactics in encoding the proof script suggesting that tokens areamoredifferentrepresentationthantacticsoftheothertypes of information.
the gallina proof term contained theleast diversitycompared totheother typesof data whereasthe proofscript contained the most.
overall these models prove theorems which is significantly more than any of the learning parameter models from section4.
.asinglemodel onaverage proves785theorems.diva s diversityisresponsiblefora161.
increaseinprovingpower!theleftgraphinfigure 8showsthediversityofthetraining data types models.asinglerandommodelproves38.
ofalltheproventheo rems.theremaining61.
ofthetheoremsrequirediva sdiversity to be proven.
the right graph in figure 8shows the variability in a selected kmodels.
here a single model can prove between .
and .
theorems from the test set.
overall trainingdatatypesprovidethemostdiversityofallthedimensions we explored leading to the greatest proving power.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa emily first and yuriy brun kth model added0.
.
.
.
.
.
.
.7average diversity of adding kth model k0500100015002000average theorems proven by k models figure training data aspects total theorems ra3 including different data types in training resulted in the most diversity of all the dimensions we considered leading to the greatest proving power increase.
adding proofscripttacticsortokensprovidedthemostdiversity followed by the proof state.
.
rq4 synthesis efficiency to explore improving diva s efficiency we implement model interrupts described in section .
.
we evaluate the efficiency improvementofmodelinterruptsbymeasuringthemeantimetoprovea theoremwithandwithoutinterrupts.ofcourse theorderinwhichdivaconsidersthemodelsmatters.withoutinterrupts intheworst case the last model produces the successful proof script and diva wastes minutes on each of the other models before they time out.
for our evaluation we measure the mean time over a random sample of possible model orderings.
withoutinterrupts themeantimetoproveatheoremis685.
seconds.
however we observe that most models either synthesize theproofscriptrelativelyquickly ordon tatall thoughwithsomenotableexceptions.usingmodelinterruptsallowsustobenefitfromprovingtheoremsquicklyintheinitialburstofeachmodel without spending the long time in the tail of each model s distribution unlessitisnecessary.withmodelinterrupts weexplore15differentswitchingtimes and60seconds.
we explore two different interrupt schemes.
first we attempt to synthesize a proof script using each model for xseconds.
if noneofthemodelsfindaproofscriptinthattime wereturnand giveeachmodelanother xseconds.andsoon untileachmodel has attempted its search for minutes.
the left graph in figure shows the mean time to prove a theorem for this interrupt scheme.
forx second this interrupt scheme achieves the minimal mean time to prove a theorem of .
seconds and the provingtime increases monotonically for larger x. forx the speed upcomparedtonotusinginterruptsis97 or40 .thissuggests that many theorems are proven very early in the synthesis process andwhilesometheoremsdogetprovenafteralengthysynthesis search prioritizing the first seconds of synthesis using the diverse models greatly improves synthesis efficiency.
second weallow eachmodeltoattempttosynthesizeaproof script for xseconds and then give each model the remainder of its xseconds thus switching only once per model.
the right graph in figure 9shows the mean time to prove a theorem for this interrupt scheme.
for x seconds this interrupt scheme123456789101520253060 switching time0200400600800100012001400mean time to prove theorem switching time0200400600800100012001400mean time to prove theorem figure mean time to prove a theorem using the model interrupts optimization for different switching times in seconds .executingeachmodel ssearchfor xseconds andthen again each model s search for xseconds and so on until eachmodel ssearchhasbeenexecutedfor600seconds left graph achieves the minimal mean time to prove a theorem of17.2secondswhen x 1second.interruptingeachmodel s search once first executing each model for xseconds and then each model for xseconds right graph achieves theminimalmeantimetoproveatheoremof44.7seconds whenx 5seconds.thebox and whiskersindicatethemaximum and25 tiles andminimumvaluesover different model orderings.
achievestheminimalmeantimetoproveatheoremof44.7seconds aspeedupof or compared to not using interrupts.
ra4 modelinterruptsisincrediblyeffective cuttingdown the mean time to prove a theorem by up to .
.
threats to validity thecoqgymbenchmarkweevaluateourworkonhasbeenused by prior evaluations of proof script synthesis and uses theoremsfrom122open sourceprojects improvingthelikelihoodthat our results generalize.
our analysis focuses on the coq interactive proof assistant and may not extend to other assistants such as hol4 andhollight .transformershaveoutperformed bidirectionallstminsomenaturallanguagetasks andmaybe able to improve diva s performance beyond what we find here buttheyrequiresignificantlylargertrainingsetsthanwhatisavailable today in projects written in coq.
accordingly future work should explore other neural modeling architectures.
related work we now place our research in the context of related work.
interactive theorem provers itps .
itps such as coq agda dafny f liquid haskel mizar isabelle hol4 and hol light are semi automated systemsforformallyprovingtheorems.wefocusoncoq butour approach is applicable to other itps.
coq has been used to build andverifyaccompiler anoperatingsystemkernel an x86 model a file system distributed protocols and systems a browser and network controllers .
automation for proof systems.
heuristic based search can partially automate itps .hammers use external atps authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
diversity driven automated formal verification icse may pittsburgh pa usa to automatically find proofs for itps .
classical search algorithms such as a can also search for proofs in hol4 as can reinforcement learning basedmethods .bycontrast divamodelsexistingproofscripts usesnativetactics andprovestheorems within the itp framework.
software engineering for interactive proof assistants.
pumpkinpatchgeneratesproofpatcheswhensoftwareevolves by learning from a template of a human written fix to a similar evolution.unlikepumpkinpatch divadoesnotrequireanearly working proofscriptandgeneratesproofscriptsfromscratch.pumpkinpi repairstheprooftermofabrokenproofandthenusesadecompiler togenerateaproofscript .pumpkinpi sproofscriptconsists of predefined tactics whereas diva predicts tactics from a learnt model.
icoq findsfailingproofscriptsinevolvingprojectsbyprioritizing proof scripts affected by a revision.
icoq tracks fine grained dependencies between coq definitions propositions and proof scripts to narrow down the potentially affected proof scripts.
diva does not require a failing proof script but our ideas could potentially be used to repair proof scripts.
quickchick a random coqtestingtool searchesforcounterexamplestoexecutabletheoremsandhelpsaprogrammergainconfidencethatatheoremis correct.
languagemodelsforcode.
languagemodelingofsourcecode can detect bugs and generate tests .
modeling code with n gramscan helpcode completion .modified n gramscan beusedasacachetocapturelocaldependenciesincode .however such applications have not been applied to itps.
applyinglanguage models to coq and hol4 proof scripts showed that ngram models outperform recurrent neural networks .
unlike diva thisapproach didnot considerthe proofstate orproof term and does not synthesize complete proof scripts.
machine learning in formal verification.
machine learning can simplify formal verification ml4pg helps coq users construct proof scripts by showing proof scripts of similar theorems .
machinelearningcansimilarlyhelpwithpremiseselection thetaskof selecting lemmas that are relevant to a given theorem .
neurotactic representstheorems andpremises withgraph neural networksforprediction .gamepad andproverbot9001 modeltheproofstateincoqusingrnns.divasimilarlycaptures theproofstate butunlikegamepad andproverbot alsomodels theproofscriptandgallinaprooftermforscriptsynthesis.divais a generalization of astactic and tactok .
these tools use the coqgym benchmark for evaluation which is also a learning environment.
large transformer models can be applied to theorem proving in the metamath formalization language and the leaninteractiveproofassistant .however thesepowerful models require much larger training sets than what is available today in coq projects.
metaheuristicsearch.
metaheuristic search basedsoftwareengineering has been used for developing test suites finding safety violations refactoring project management and effort estimation and automated program repair .
insearch low qualityfitness functionscanlead tolow qualityresults such as for example incorrect bug patches .
with diva theinteractivetheoremproverprovidesastrongassurancethatthefinalproducedproofscriptleadstoacorrectproof andthus proof script synthesis is particularly well suited for metaheuristicsearch based methods.
ensemble learning.
ensemble learning is the generation and combinationofmultiplemodelstomakeadecision.thisistypically used in supervised machine learning tasks .
the idea is that weighing and combining several opinions is better than simply choosingasingleone.whengeneratingamodeltobeusedinan ensemblelearningmethod themodelshouldbesufficientlydiverse for the ensemble to achieve a desired predictive performance and the individual model s predictive performance should be as highaspossible.thereareseveralapproachestogeneratingdiverse models including input manipulation manipulation of the learning algorithm and combinations of strategies.
ensemblelearningmethodseitherhavedependentmodels where theoutputofeachmodelaffectsthegenerationofthenext orindependentmodels whereeachmodelisconstructedindependentlyfromtheothers .anotherwaytocombineclassifiersisthrough stacking whichusesaclassifiertodecidewhichmodeltoapply toeachinput.divadiffersfromthesemethodsbyusingindependentmodelsinseparatesearchesoftheproofscriptspacesincethe coqproofassistantservesasanoracleforwhethertheresulting proof scripts are valid.
contributions we have identified a method for using diversity to significantly improve the proving power of proof script synthesis tools.
we create diva implementing our diversity based approach which proves more theorems than tactok and more than astactic two state of the art proof script synthesis tools.
diva automaticallyproves theorems no existing tool has proved.
together with coqhammer diva proves more than a third of all the theorems in our benchmark of open source projects the largest fraction to date.ourmodelinterruptsoptimizationimprovesdiva srunning timeby40 .alongthewayweidentifyakillerappforensemble learning by using the theorem prover as an oracle for optimallyaggregating learnt model results.
our findings strongly suggest that using diversity for improving automated formal verification is fruitful and warrants further research.