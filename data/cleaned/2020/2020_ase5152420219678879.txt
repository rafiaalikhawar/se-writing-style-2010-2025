learning highly recursive input grammars neil kulkarni university of california berkeley neil.kulkarni berkeley.educaroline lemieux university of california berkeley clemieux cs.berkeley.edukoushik sen university of california berkeley ksen cs.berkeley.edu abstract this paper presents a rvada an algorithm for learning context free grammars from a set of positive examples and a boolean valued oracle.
a rvada learns a context free grammar by building parse trees from the positive examples.starting from initially flat trees a rvada builds structure to these trees with a key operation it bubbles sequences of sibling nodes in the trees into a new node adding a layer of indirection to thetree.
bubbling operations enable recursive generalization in thelearned grammar.
we evaluate a rvada against glade and find it achieves on average increases of .
in recall and .
in f1 score while incurring only a .
slowdown and requiring only .
as many calls to the oracle.
a rvada has a particularly marked improvement over glade on grammars with highlyrecursive structure like those of programming languages.
i. i ntroduction learning a high level language description from a set of examples in that language is a long studied and difficult problem.
while early interest in this problem was motivatedby the desire to automatically learn human languages fromexamples more recently the problem has been of interest inthe context of learning program input languages.
learning alanguage of program inputs has several relevant applications including generation of randomized test inputs aswell as providing a high level specification of inputs whichcan aid both comprehension and debugging.
in this paper we focus on the problem of learning contextfree grammars cfgs from a set of positive examples s and a boolean value oracle o. this is a similar setting as glade .
like glade and unlike other recent relatedworks we assume the oracle is black box ourtechnique can only see the boolean return value of the oracle.we adopted the use of an oracle as we believe that in practice an oracle e.g.
in the form of a parser is easier to obtain thangood information carrying negative examples.
in this paper we describe a novel algorithm a rvada for learning cfgs from example strings sand an oracle o. at a high level a rvada attempts to create the smallest cfg possible that accommodates all the examples.
it usestwo key operations bubbling and merging to generalize thelanguage as much as possible while not overgeneralizingbeyond the language accepted by o. to create this context free grammar a rvada repeatedly performs the bubbling and merging operations on tree repre sentations of the input examples.
this set of trees is initializedwith one flat tree per input example i.e.
the tree with a singleroot node whose children are the characters of the input string.the bubbling operation takes sequences of sibling nodes in the equal contribution.trees and adds a layer of indirection by replacing the sequencewith a new node.
this new node has the bubbled sequence ofsibling nodes as children.
then a rvada decides whether to accept or reject the proposed bubble by checking whether a relabeling of the newnode enables sound generalization of the learned language.essentially labels of non leaf nodes correspond to nontermi nals in the learned grammar.
merging the labels of two distinctnodes in the trees adds new strings to the grammar s language the strings derivable from subtrees with the same label can beswapped.
we call this the merge operation since it merges the labels of two nodes in the tree.
if a valid merge occurs thestructure introduced by the bubble is preserved.
thus mergesintroduce recursion when a parent node is merged with oneof its descendants.
if the label of the new node added in thebubbling operation cannot merge with any existing node in thetrees the bubble is rejected.
that is the introduced indirectionnode is removed and the bubbled sequence of sibling nodesis restored to its original parent.
these operations are repeateduntil no remaining bubbled sequence enables a valid merge.
in this paper we formalize this algorithm in a rvada .
we introduce heuristics in the ordering of bubble sequencesminimize the number of bubbles a rvada must check before find a successful relabeling.
we implement a rvada in .2k loc in python and make it available as open source.we compare a rvada to glade a state of the art for grammar learning engine with blackbox oracles.
we evaluateit on parsers for several grammars taken from the evaluation ofglade reinam mimid as well as a few new highly recursive grammars.
on average across these benchmarks a rvada achieves .
higher recall and .
higher f1 score over glade.
a rvada incurs on a slowdown of .
over glade while requiring .
as many oracle calls.
we believe this slowdown is reasonable especiallygiven the difference in implementation language a rvada is implemented in python while glade is implemented injava.
our contributions are as follows we introduce a rvada which learns grammars from inputs strings and oracle via bubble and merge operations.
we distribute a rvada s implementation as open source we evaluate a rvada on a variety of benchmarks against the state of the art method glade.
ii.
m otiv ating example arvada takes as input a set of example strings sand an oracleo.
the oracle returns true if its input string is valid andfalse otherwise.
a rvada s goal is to learn a grammar 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee gwstart stmt stmt while boolexpr do stmt if boolexpr then stmtelse stmt l numexpr stmt stmt boolexpr boolexpr boolexpr boolexpr numexpr numexpr false true numexpr numexpr numexpr l n s while true false do l n l n l n n o i braceleftbigg true ifi l gw false otherwise fig.
example inputs s and oracle owhich returns true if its input is in the language of the while grammar gw.
gwhich maximally generalizes the example strings sin a manner consistent with the oracle o. that is strings i l g in the language of the learned grammar should with high probability be accepted by the oracle o i true.w e formally describe maximal generalization in section iii.
fundamentally a rvada learns a grammar by learning parse trees for the examples in s. these parse trees are initialized with flat trees for each example in s. then a rva da adds structure turning sequences of sibling nodes into new subtrees.
the particular subtrees a rvada keeps are those which enable generalization in the induced grammar.
from any set of trees twe can derive an induced grammar.
in particular each non leaf node in a tree t t with label tparent and children with labels tchild tchild ... t child ninduces the ruletparent tchild 1tchild t child n. the induced grammar oftis then the set of induced rules for all nodes in the trees.
for example the trees in fig.
induce the grammar t0 while true false dol n t0 l n l n n and the trees under in fig.
induce the grammar in fig.
.
because of this mapping from trees to grammars we will use the term nonterminal interchangeably with label of anon leaf node when discussing relabeling trees.
a. walkthrough we illustrate a rvada on a concrete example.
we take the set of examples sand oracle oshown in fig.
.
this oracle o accepts inputs as valid only if they are in the language of the while grammar gw shown at the top of the figure.
a rvada treatsoas blackbox that is it has no structural knowledge ofgw gwis shown only to clarify the behavior of o. arvada begins by constructing na ve flat parse trees from the examples.
these are shown in fig.
.
essentially thesetrees simply go from the start nonterminal t 0to the sequence of characters in each example s s. lettdesignate the set of trees a rvada maintains at any point in its algorithm.
bubbling the fundamental operation a rvada performs is to bubble up a sequence of sibling nodes in the current treestinto a new nonterminal.
to bubble a sequence s1in the treest we create a new nonterminal node ts1with childrent0 while true false do l n t0 l n l n n fig.
initial set of parse trees tcreated by a rvada when run ons oin fig.
.
each terminal chas a nonterminal parenttcwith rule tc c omitted for simplicity.
bubblet1 hile t0 wt1 hiletrue false do l n bubblet2 n n t0 l n l t2 n n fig.
two possible bubbles applied to the trees in fig.
.
s1.
then we replace all occurrences of s1in eacht t with ts1.
fig.
shows two such bubbles applied to the trees in fig.
.
on top we have bubbled the sequence hile intot1 the second tree unchanged is not illustrated.
on the bottom we have bubbled n n intot the first tree is unchanged.
merging after bubbling a sequence s1 arvada either accepts orrejects the bubble.
a rvada only accepts a bubble if it enables valid generalization of the examples.
that is if arelabeling of the bubbled nonterminal merging its label withthe label of another existing node expands the language ac cepted by the induced grammar while maintaining the oracle validity of the strings produced by the induced grammar.
consider again fig.
.
on top we have the bubble t hile.
there is no terminal or nonterminal whose label can bemerged with the label t 1and retain a valid grammar it can t be merged with t0 since hile on its own is not accepted byo.
nor can it be merged with the label of any individual character as just one example merging with lwould cause theo invalid generalization h i l e n hile n n .
on the bottom of fig we have the bubble t2 n n .
we can in fact merge the label t2with the label tn the implicit nonterminal expanding to n. notice that if we replace nwith the strings derivable from t2 we get examples like while true false do l n n andl n n l n n n n which are all valid.
conversely if we replace occurrencesoft 2withn we get examples like l n l n .w e accept this bubble which expands the language accepted by 457t0 while true false do l nt0 l n l n n bubble t2 n n merge t n into t3 t0 while true false do l t3 nt0 l t3 n l t3 t3 n t3 n bubble t4 l t3 merge t t0 into t0 t0 while true false do t0 l t3 nt0 t0 l t3 n t0 l t3 t3 n t3 n bubble t false t6 true merge both into t7 t0 while t7 true t7 falsedo t0 l t3 nt0 t0 l t3 n t0 l t3 t3 n t3 n bubble t8 t7 t7 merge t t7 into t9 t0 while t9 t9 true t9 falsedo t0 l t3 nt0 t0 l t3 n t0 l t3 t3 n t3 n fig.
the state of trees tand the accepted bubbles of a full run of a rvada ons oin fig.
.
the induced grammar.
thus t2andnare merged and relabeled ast3.
the trees after the relabel are shown after in fig.
.
note this merge has introduced recursive generalization the induced grammar now includes the rules t0 l t3t0 l t3 l t3t3 t3 t3 t3 n in practice a rvada checks whether labels ta tbcan be merged by checking candidate strings against the oracle.
if the oracle accepts all these candidate strings the relabeling is valid and the labels are merged.
to create these candidates a rvada creates mutated trees from the trees in twhere subtrees rooted at taare replaced subtrees rooted at tb and subtrees rooted at tbare replaced subtrees rooted at ta.
the candidate strings are then the ones derived from these trees i.e.
theordered sequence of a tree s leaf nodes.
section iii c describesthe conditions under which a bubble is accepted in more detail.section iii d describes how to create these candidate strings and the soundness issues this introduces.
double bubbling after accepting a bubble a rvada continues to try and create new bubbles.
it bubbles differentt0 whilet9dot0 l t3 t0 t0 t9 t9 t9 true false t3 t3 t3 n fig.
grammar produced by the run of a rvada in fig.
.
sequences of children in the current trees t checking if they are accepted and updating taccordingly.
fig.
shows a potential run of a rvada with the state of the trees tas they are updated by bubbles and label merging.
in fig.
after accepting the bubble t2 n n arvada finds and accepts the bubble t4 l t3 whose label can be merged with the start nonterminal t0.
at this point arvada will find no more bubbles which can be merged with any existing nodes in t. for example if a rvada creates the bubblet5 true it will find that the label t5cannot be merged with the label of any existing node and reject it.
to cope with this a rvada also considers bubbles.
in a bubble two distinct sequences of children say s1and s2 in the trees are bubbled at the same time i.e.
replacing boths1withts1 s1and some other s2withts2 s2.
the two sequences can be totally distinct or sub super sets but notoverlapping s true s2 false is ok as is s true s2 true false but s rue f s2 e fal is not.
arvada accepts a bubble only if the labels ts1andts2can be merged with each other not with another existing node.
otherwise either ts1orts2could be accepted as a bubble.
termination in the run in fig.
a rvada applies and accepts the bubble s true s2 false and merges these sequences into t7.
this bubble enables one final single bubble to be applied and accepted t8 t7 t7can be merged with t7.
after this no more bubbles or bubbles can be accepted so a rvada simply outputs the grammar induced by the final set of trees t. fig.
shows the grammar.
effect of bubbling order first note that multiple orderings of bubbles can result in an equivalent grammar.
for exam ple we could have applied s true s2 true false in then bubbled up false alone in .
second while fig.
shows an ideal run some accepted bubbles may impedefurther generalization of the grammar.
for example in theinitial flat parse trees t e false can be merged with e. in the presence of the additional example while n n doskip this merge prevents maximal generalization.
as such the order in which bubbles are applied and checked has a large impact on a rvada s performance.
in section iii b we describe heuristics that order the bubbles forexploration based on the context and frequency of the bubbledsubsequence.
these heuristics increase a rvada s probability of successfully finding the maximal generalization of swith respect to o as discussed in section iv b. maximality of learned grammar the grammar in fig.
is not identical to that in fig.
.
however it contains allthe rules in g wdemonstrated by the examples s t3has taken on the role of numexpr t9in the role of boolexpr andt0is effectively stmt.
however the rule boolexpr 458numexpr numexpr does not appear in fig.
.
fundamentally this is because no substring derivable from this rule exists ins as such it is not part of s s maximal generalization.
iii.
t echnique we formally describe the high level a rvada algorithm in section iii a sections iii b iii c iii d and iii e delve intothe heuristic decisions made in a rvada s implementation.
first we formalize our problem statement.
a rvada accepts as input a set of example strings sand a boolean valued oracle owhich judges the validity of the strings.
a rvada s goal is to learn a context free grammar gwhich maximally generalizes the set of example sin a manner consistent witho.
maximal generalization letsbe a set of input strings andobe a boolean valued oracle accepting strings as input.
assume each s sis accepted by the oracle i.e.
s s o s true.
let gobe a context free grammar such that its language of strings l go is equal to i o i true the set of strings accepted by the oracle o. sinceo s true for eachs s then each s l go .
we callgoas the target grammar.
thus for each s there exists a derivation dsfrom the start symbolt0tos i.e.ds t0 1 2 n s. this derivation is a sequence of nonterminal expansions accordingto some rules g o. letrsbe the set of rules in goused in the derivation ds.
letrs s srs andgs obe the subset of gowhich contains only those rules r rs.
intuitively gs o is the sub grammar of gowhich is exercised by the s s. finally a grammar which maximally generalizes sw.r.t.
ois a grammar gsuch that l g l gs o i.e.
it accepts the same language as gs o. a. main algorithm algorithm shows the main a rvada algorithm.
it works as follows.
first a rvada builds na ve flat parse trees from the input strings line .
considering each si sas a sequence of characters si c1 ic2i cni i the tree constructed for sihas a root node with the start symbol label t0andnichildren with labelstc1 i tc2i ... tcni i. eachtchas a single child whose label is the corresponding character c. fig.
shows these flat parse trees for the examples strings s sin fig.
although the tc care not illustrated for simplicity.
arvada tries to generalize these parse trees by merging nodes in the tree into new nonterminal labels line .
to merge two nodes ta tbin a tree we replace all occurrences of the labels ta tbwith a new label tc.
this creates new trees t prime the merge is valid if the language of the induced grammar oft primeonly includes strings accepted by the oracle o. in practice we check if a merge of ta tbis valid by checking whether tacan replace tbin the example strings and vice versa.
the strings derivable from an arbitrary nonterminalnintare the concatenated leaves of the subtree rooted at n. we check whether t areplacestbby checking whether the strings produced by replacing strings derivable from taby strings derivable from tb are accepted by the oracle.
that is we take the strings derivable from the trees t with holes inalgorithm arvada s high level algorithm input a set of examples s an language oracle o. output a grammar gfitting the language.
besttrees naive parse trees s besttrees merge allvalid besttrees o updated true while updated do updated false allbubbles getbubbles besttrees for bubble inallbubbles do bbldtrees apply besttrees bubble accepted mergedts check bubble bbldtrees o ifaccepted then besttrees mergedts updated true break g induced grammar besttrees returng place of strings derived from tb.
then we fill the holes with strings derivable by ta.
if all the strings are accepted by o arvada judges the replacement as valid.
section iii d details this check and its soundness.
now the main a rvada loop starts.
from the current sderived trees t a rvada gets all potential bubbles for the trees algorithm line .
for each tree t t getbubbles collects all proper contiguous subsequences of children in t. that is if the tree contains a node tiwith children c c1 c2 ... c n the potential bubbles include all subsequences of cof length greater than one and less thann.g etbubbles returns all these subsequences as 1bubbles and all non conflicting pairs of these subsequencesas bubbles.
two subsequences are non conflicting if theydo not strictly overlap they can be disjoint or one can be a proper subsequence of the other.
so c c2 c3 c2 c3 c4 conflict but c1 c2 c3 c2 c3 and c1 c2 c3 c4 c5 do not.
the order in which a rvada explores these bubbles is important for efficiency we discuss this further in section iii b. then for each potential bubble a rvada tries applying it to the existing set of trees t. suppose we have a bubble consisting of the subsequence ci ci ... c j. to apply this bubble we replace any sequence of siblings tci tci ... t cj with labels ci ci ... c jin the tree with a new subtree tnew tci tci ... t cj.
fig.
shows two such bubblings hile is bubbled into the nonterminal t1at the top and n n is bubbled to t2on the bottom.
if the bubbled nodes have structure under them that structure is maintained e.g.
thebubbling of t t7intot9at in fig.
.
for a bubble the same process is repeated for the two subsequences involved.
after applying the bubble a rvada checks whether it should be accepted line .
section iii c formalizes c heck bubble but essentially c heck bubble accepts a bubble if the new nonterminals introduced in its application can bevalidly merged with some other nonterminal node in the tree.
459t0 whilen n do t0 skip tree 1t0 while t1 truedo t0 skip tree 2t0 if t1 falsethen t0 skipelse t0 skip tree fig.
partial parse tree tduring run of a rvada on while with guide examples while n n do skip if false then skip else skip and while true do skip .
a rvada has applied the bubble skip which merged with t0 and the bubble false true .
the contexts for n n are highlighted inyellow and for t1arehighlighted ingreen.
if the new bubbled nonterminal allows a valid merge with some other nonterminal c heck bubble returnstrue as well as the trees with the merge applied line .
we update the best trees tto reflect the successful merge line and getbubbles is called again on the new t. if the bubble is not accepted a rvada continues to check the next bubble returned by g etbubbles line .
the algorithm terminates when none of the bubbles are accepted i.e.
when the trees tcannot be further generalized and returns the grammar ginduced by the trees t line .
we can guarantee the following about a rvada as long as merges are sound once we consider the notion of partially merging two nonterminals discussed in section iii c2.
existence theorem there exists a sequence of kbubbles that when considered by a rvada in order enable arvada to return a grammar gs.t.l g l go so long as the input examples sare exercise all rules of g. proof outline the optimal bubble order always chooses the right hand side of some n 1 ningas the sequence to bubble either as bubble if there exists an ex pansion for nin the trees already or as a bubble otherwise.
our technical report gives a formal treatment of this and the generalization theorem which shows that k bubbles monotonically increase the language of the learned grammar .
b. ordering bubbles for exploration as described in paragraph of section ii and alluded to above the order of bubbles impacts the eventual grammar returned by a rvada .
unfortunately the number of orderings of bubbles is exponential.
to have an efficient algorithm inpractice we must make sure the algorithm finds the correctorder of bubbles early in its exploration of bubble orders.
assuch g etbubbles returns bubbles in an order more likely to enable sound generalization of the grammar being learned.
as described in the prior section bubble sequences consist of proper contiguous subsequences of children in the currenttreest.
we increase the maximum length of subsequences considered once all bubbles of shorter length do not enable anyvalid merges.
these subsequences and their pairs form thebase of bubbles and bubbles returned by g etbubbles .
recall that a bubble should be accepted if the bubbled nonterminal s can be merged with an existing nontermi nal or each other .
thus g etbubbles should first return those bubbles that are likely to be mergeable.
we leveragethe following observation to return bubbles likely to enablemerges.
expansions of a given nonterminal often occur ina similar context.
the k context of a sequence of sibling terminals nonterminals sin a tree is the tuple of ksiblings to the left of sandksiblings to right of s. fig.
shows an example of a run of a rvada on the while language after the application of the1 bubble skip and the bubble false true .the set of contexts for the sequence n n i s i l e d o .
similarly t s contexts are s i f t h e i l e d o s i s a dummy element indicating the start of the examplestring.
note that n n and t share the context i l e d o with this in mind g etbubbles orders the bubbles in terms of their context similarity.
given two contexts c0 l0 r0 andc1 l1 r1 where li lk i lk i ... l0 i andri r0 i ... rk i rk i w eh a v econtextsim c0 c1 ktuplesim l0 l1 ktuplesim r0 r1 where ktuplesim t0 t1 braceleftbigg 2ift0 t1 summationtextk i ti ti1 2i where is the indicator function returning if its arguments are equal and otherwise.
this similarity function gives most weight to the context elements closest to the bubble.
with this in mind we define set context similarity as the maximum similarity of two contexts within the set setcontextsim c0 c1 m a x c0 c0 c1 c1contextsim c0 c1 .
in our running example the context similarity is because n n s context set is a subset of t1 s context set.
to form bubbles g etbubbles first traverses all the trees tcurrently maintained by a rvada .
it considers each proper contiguous subsequence of siblings in the trees.
for eachsubsequence s it collects the k contexts for s as well as the occurrence count of the subsequence occ s .i nf i g .
occ while occ t and occ n n .
in our implementation we take k .
arvada then creates a bubble for each pair of sequences s1 s2 where both s1 1and s2 .
the similarity score of this bubble is setcontextsim contexts s1 contexts s2 and its frequency score is the average frequency of the two sequences in the bubbleocc s occ s .
additionally for each sequence s0with s0 a rvada creates a bubble s0 .
lets1be the set of length one subsequences.
the similarity score of s0 ismax s1 s1setcontextsim contexts s0 contexts s1 and its frequency score is occ s0 .
460t0 if t1 n nthe n t0 l nelse t0 l n t0 if t1the tnt0else t1 tn tn t0 l tn tn n t0 if t1the tn1t0else t1 tn2 tn3 t0 l tn4 tn1 n tn2 n tn3 n tn4 n fig.
example tree and rules in its induced grammar which havetnin their expansion and the same grammar with tn split at different positions.
for simplicity nonterminals of theformt c c other than tnin are collapsed to c. finally g etbubbles takes the top n bubbles as sorted primarily by similarity and secondarily by frequency.
intu itively high frequency sequences may correspond to tokensin the oracle s language.
the order of bubbles is shuffled toprevent all runs of a rvada from getting struck in the same manner.
we find n to be effective in practice.
c. accepting bubbles the second key component of a rvada is deciding whether a given bubble should be accepted this section formalizeshow c heck bubble works.
at the core of c heck bubble is the concept of whether two labels ta tbcan be merged.
we say thattaandtbcan be merged i.e.
m erges ta tb i fa n d only if r eplaces ta tb that is all occurrences of tbcan be replaced by tain the grammar and r eplaces tb ta .w e formalize how r eplaces is checked in the next section.
bubbles arvada accepts a bubble s1 s2 with labelsts1 ts2only if m erges ts1 ts2 .
intuitively this is because both bubbles should be kept only if they togetherexpand the grammar.
for example suppose we apply the bubble n n lse to the trees in fig.
resulting in nonterminals tn n n nandtlse lse.
while tn n can merge with t1 tlsedoes not contribute to this merging.
so n n should be accepted only as a bubble.
bubbles recall that a rvada scores bubbles highly if they are likely to merge with an existing nonterminal.
letnts t be the nonterminal labels present in the current set of treest.
given a bubble s with label ts1 we go through eachti nts t and check whether m erges ti ts1 .
if m erges ti ts1 is true for some ti nts t then check bubble accepts the bubble s1 .
however if ts1cannot merge with any ti nts t arvada also looks for partial merges.
partial merging works as follows.
let cnts t be the character nonterminal labelspresent in the current set of trees t.acharacter nonterminal is a nonterminal whose expansions only of a single terminalelement e.g.
t n nort1 .
for each tc cnts t the partial merging algorithm identifies all the different occurrences of tcin the right handside of expansions in t s induced grammar.
for instance in the grammar fragment of fig.
we see the nonterminalt n corresponding to n occurs distinct times in righthand sides of expansions.
the partial merging algorithm thenmodifies the grammar so that the i thoccurrence of tcis replaced with a fresh nonterminal tci.
eachtciexpands to the same bodies as tc i.e.tci c. this replacement process is illustrated in the grammar fragment of fig.
thefour occurrences of t nhave been replaced with tn1 tn2 tn3 andtn4.
finally we get to the merging inpartial merging for each tci the algorithm checks if m erges tci ts1 .i f merges tci ts1 for anytci a rvada accepts the bubble s1 andts1is merged with all such tci.
thetcjwhich cannot be merged with ts1are restored to the original nonterminal tc.
the term partial merge refers to the fact that we have effectively merged ts1with some of the occurrences of tcin rule expansions.
this step is useful when a rvada s initial trees which map each character to a single nonterminal use the same nonterminal for characters that are conceptuallyseparate.
for instance consider the bubble n n withlabelt n n .
given the tree in fig.
m erges tn t n n fails because n n cannot replace the n i n then .
in fact t n n cannot merge with any ti nts t initially.
but the partial merge process splits tnintotn1 tn2 tn3 tn4 and arvada finds that t n n in fact merges with tn2 tn3and tn4.
so it is merged with those nonterminals and accepted.
note though we consider only partial merges on character nonterminals for efficiency reasons the concept of partialmerging can be applied to any pair of nonterminals.
in summary a bubble s with label ts1is accepted if either for some ti nts t m erges ti ts1 or for sometc cnts t ts1can be partially merged with tc.
d. sampling strings for replacement checks the final important element affecting the performance of arvada is how exactly we determine whether the merge of two nonterminals labels is valid.
recall that m erges ta tb if and only if r eplaces ta tb and r eplaces tb ta .
we implement r eplaces treplacer treplacee as follows.
from the current parse trees we derive the replacee strings the strings derivable from the parse trees in trees but with holes instead of the strings derived from treplacee .
then we derive a set of replacer strings the strings derivable from treplacer in the trees.
finally we create the set of candidate strings by replacing the holes in the replacee strings with the replacerstrings.
if orejects any candidate string the merge is rejected and r eplaces returns false.
fig.
shows how replacer and replacee strings are computed in the call to r eplaces t0 t4 i.e.
whether t0can replace t4.
replacee strings for a node in the parse tree are computed by 461t0 t4 t4 4t4 4tp t4 4t0 tl t0 3tr replacee strings for t4 replacer strings from t0 bracehtipupleft bracehtipdownright bracehtipdownleft bracehtipupright level derivable44 bracehtipupleft bracehtipdownright bracehtipdownleft bracehtipupright level derivable fig.
two partial parse trees and examples of replacee and replacer strings.
the symbol designates holes which will be replaced by level n derivable replacer strings.
taking the product of replacee strings for all its children thenonterminal being replaced becomes a hole.
level replacer strings for t iare just the strings that directly derivable from tiin the tree in fig.
the level0 derivable strings of t0are44 and the level derivable strings of t4are44 .
then the set of level n derivable strings for a node is the set derived from taking theproduct of all level n derivable strings for each child of a node.
the level replacer strings for t 0are shown in fig.
.
when r eplaces is run in the full m erge allvalid call or while evaluating a bubble we use only level replacerstrings.
however we found that level replacer strings greatlyincreased soundness at a low runtime cost for bubbles.intuitively this is because nonterminals from new bubblestend to have less structure underneath them than existingnonterminals in the trees.
so it is faster to compute level 1replacer strings for these new bubble induced nonterminals.
note that the both the number of replacee strings and of level n derivable replacer strings grows exponentially.
so instead of taking the entire set of strings derivable in thismanner if there are more than pof them we uniformly sample pof them.
in our implementation we use p to make the number of parse calls reasonable in terms of runtime.
unfortunately this process allows unsound merges where all candidate strings are accepted by the oracle but the mergeadds oracle invalid inputs to the language of the learned gram mar.
first because only pcandidates are sampled.
second because the replacee strings are effectively level and thus not reflective of the current induced grammar from the trees.third because a candidate string is produced by replacing allits holes with a single replacer string rather than filling holeswith different replacer strings.
taking p n for the level n replacer strings and filling different holes with different replacer strings would ensure sound merges.
e. pre tokenization since a rvada considers bubbles it is effectively n4in the total length of examples n. so to improve performance as ngets large and reduce the likelihood of creating breaking bubbles in our implementation we use a simple heuristic to pre tokenize the values at leaves rather than consideringeach character as a leaf.
we group together sequences ofcontiguous characters of the same class lower case upper case whitespace digits into leaf tokens.
punctuation andnon ascii characters are still treated as individual characters.we then run the a rvada as described previously.
to ensure generalization we add a last stage which tries to expand thesetokens into the entire character class e.g.
if t abc cde w e check whether t1can be replaced by any sequence of lowercase letters letters or alphanumeric characters.
we constructthe replacee strings as described above and sample stringsfrom the expanded character classes as replacer strings.
iv .
e v aluation we seek to answer the following research questions rq1.
do a rvada s mined grammars generalize better have higher recall than state of the art?
rq2.
do a rvada s mined grammars produce more valid inputs have higher precision than state of the art?
rq3.
how does the nondeterminism in a rvada cause its behavior to vary across different invocations?
rq4.
how does a rvada s performance compare to that of deep learning approaches?
rq5.
what are a rvada s major performance bottlenecks?
rq6.
what do a rvada s mined grammars look like?
a. benchmarks we evaluate a rvada against state of the art blackbox grammar inference tool glade on benchmarks.
the first benchmarks consist of an antlr4 parser for the ground truth grammar as oracle and a randomly gener ated set of training examples s.sis sampled to cover all of the rules in the ground truth grammar while keeping the length ofeach example s ssmall.
the test set is randomly sampled from the ground truth grammar.
essentially this ensures thatthe maximal generalization of scovers the entire test set.
other than turtle andwhile these benchmarks come from prior work arith operations between integers can be parenthesized fol a representation of first order logic including qualifiers functions and predicates json json with objects lists strings with alpha numericcharacters booleans null integers and floats lisp generic s expression language with .
cons ing mathexpr binary operations and a set of function callson integers floats constants and variables turtle logo like dsl for python s turtle while simple while language as shown in fig.
xml supporting arbitrary attributes text and a few labels the next benchmarks use as oracle a runnable program and use a random input generator to create sand the test set.
sconsists of the first oracle valid inputs generated by the generator and the test set of the next oracle valid inputsgenerated.
in this case there is no guarantee that the maximalgeneralization of scovers the test set.
curl the oracle is the curl url parser.
we use thegrammar in rfc to generate sand test set.
a recall.
higher is better.
b precision.
higher is better.
c f1 score.
higher is better.
fig.
recall precision and f1 score for each of the runs of a rvada plotted with and glade plotted with .
table i summary of results for a rvada and glade.
r is recall p is precision.
results for a rvada are listed as the means over runs with the standard deviation.
bolded results are better.
arvada glade bench.
recall precision f1 score time s queries r p f1 time s queries arith .
.
.
.
.
.
.
.
.
.3k fol .
.
.
.
.
.
33k .7k .
.
.
20k json .
.
.
.
.
.
16k 1k .
.
.
11k lisp .
.
.
.
.
.
.6k .
.
.
.8k math.
.
.
.
.
.
.
11k .1k .
.
.
19k turtle .
.
.
.
.
.
10k .1k .
.
.
14k while .
.
.
.
.
.
13k .5k .
.
.
.1k xml .
.
.
.
.
.
14k .4k .
.
.
15k curl .
.
.
.
.
.
25k .1k .
.
.
30k tinyc .
.
.
.
.
.
.4k .2k 112k 32k .
.
.
252k nodejs .
.
.
.
.
.
46k 22k 142k 90k .
.
.
38k 113ktable ii results for clgen s core lstm .
model time isthe logged model training time.
clgen lstm bench.
time s model time s precision arith .
fol .
json .625lisp .367mathexpr .393turtle .367while .012xml .
curl .
tinyc .062nodejs .
tinyc the oracle is the parser for tinyc a compiler for a subset of c. we use the same golden grammar asin mimid to generate sand the test set.
nodejs the oracle is an invocation of nodejs check which just checks syntax .
to generate sand the test set we use zest s javascript generator.
the average length of training examples in the set sis below for all benchmarks except tinyc and nodejs .
we adjust the maximum bubble length hyperparameter ref.
section iii b accordingly the default is to range from3t o1 b u to ntinyc andnodejs we range from to .
b. accuracy evaluation first we evaluate the accuracy of a rvada and glade s mined grammars with respect to the ground truth grammar weran both a rvada and glade with the same oracle example strings.
three key metrics are relevant here recall the proportion of inputs from the held out test set generated by sampling the golden grammar generator thatare accepted by the mined grammar.
we use a test set sizeof for all benchmarks.
precision the proportion of inputs sampled from the mined grammar that are accepted by the golden grammar oracle.
wesample inputs from the mined grammar to evaluate this.
f1 score the harmonic mean of precision and recall.
it is trivial to achieve high recall but low precision mined grammarcaptures any string or low recall but high precision minedgrammar captures only the string in s f1 measures the tradeoff between the two.results.
as a rvada is nondeterministic in the order of bubbles explored we ran it times per benchmark.
asglade is deterministic we ran it only once per benchmark.
table i shows the overall averaged results fig the individual runs.
we see from the table that on average a rvada achieves higher recall than glade on all benchmarks and it achieves higher f1 score on all but benchmarks.
a rvada achieves over higher recall on benchmarks and over higher f1 score on benchmarks.
even for those benchmarks where a rvada does not have a higher f1 score on average fig.
9c shows that a rvada outperforms glade on some runs.
for nodejs on runs arvada achieves a higher f1 score ranging from .
to .
.
for curl on runs a rvada achieves f1 scores greater than or equal to glade s .
and .
.
it makes sensethat glade performs well for curl the url language is regular and the first phase of glade s algorithm works bybuilding up a regular expressions.
nonetheless fig.
9a showsthat a rvada achieves consistently higher recall on curl.
overall on average across all runs and benchmarks a rva da achieves .
higher recall than glade while maintaining .
its precision.
so on our benchmarks the answer to rq1 is in the affirmative while the answer to rq2is not.
given that a rvada still achieves a .
higher f1 score on average and that higher generalization in the form of recall is much more useful if the mined grammar is usedfor fuzzing we find this to be a very positive result.
however we see from the standard deviations in table i that a rvada s performance varies widely on some benchmarks 463notablefol lisp while and fol.
fig.
which shows the raw data confirms this.
in fig.
9a we see that the performance on the lisp benchmark is quite bimodal.
all of the mined grammars with recall around .
fail to learn tocons parenthesized s expressions.
this may be because theminimal example set did not actually have an example of thisnesting.
on nodejs the two runs with recall less than .
find barely any recursive structures suggesting that on largerexample sets a rvada may get lost in bubble order.
overall the answer to rq3 is that a rvada s nondeterministic bubble ordering can have very large impacts on the results.
we discusspossible mitigations in section v. c. comparison to deep learning approaches recently there has been interest in using machine learning to learn input structures.
for instance learn fuzz trains a seq seq model to model the structure of pdf objects it uses information about the start and end of pdf objects aswell as the importance of different characters in its samplingstrategy.
deepsmith trains an lstm to model openclkernels for compiler fuzzing adding additional tokenizationand pre processing stages to clgen .
a natural question is how a rvada compares to these generative models.
we trained the lstm model from clgen the generative model behind deepsmith on our benchmarks.we removed all the opencl specific preprocessing stagesfrom the pipeline.
we used the parameters given as examplein the clgen repo creating a layer lstm with hiddendimension trained for epochs.
we used n!!
n as an eof separator.
each sample consisted of characters split into different inputs where the eof separator appeared.
table ii shows the runtime of the model on each benchmark as well as the precision achieved on the first samplestaken from the model.
generally we see that the precisionis much lower than that of glade or a rvada .o narith the model over trains on the eof separator adding nand !throughout samples.
since the model is generative it can generate samples but not provide a judgement of samplevalidity we cannot measure recall as in table i. however qualitative analysis of the samples suggests there is not muchlearned recursive generalization.
for json of the valid samples are a single string e.g.
f the other valid samples are numbers false o r .f o rnodejs of the valid samples are empty are a single identifier e.g.a are a parenthesized integer or identifier e.g .
and are a single identifier throw e.g.
throw a .
these results are not entirely unexpected because the lstm underlying clgen is learning solely from the input examples.
both a rvada and glade extensively leverage the oracle effectively creating new input examples from whichto learn.
this explains why the runtimes look so differentbetween tables i and ii.
we see in table ii that the totaltime to setup and train the model is around minutes forall benchmarks and the core training time is around 20seconds.
we see the model training time is slightly higher fortinyc andnodejs which had longer input examples.arithfoljson lispmath turtle whilexml curltinycnodejs0 percent of runtimeoracle calls bubble ordering string sampling fig.
average percent of runtime spent in different compo nents of a rvada .
error bars show std.
deviation.
overall we expect these deep learning approaches to be more well suited to a case where an oracle is not available but large amounts of sample inputs are.
these models may alsobe more reliant input format specific pre processing steps likethose used on opencl kernels in clgen and deepsmith.
d. performance analysis the next question is about a rvada s performance.
table i shows the average a rvada runtime and number of queries performed for each benchmark and the same statistics for glade.
on of benchmarks a rvada is on average slower than glade overall across benchmarks this amountsto an average .
slowdown.
this is quite respectable since a rvada has a natural runtime disadvantage due to being implemented in python rather than java.
for the three bench marks on which a rvada is over2 slower than glade it has huge increases in f1 score .
.91forfol .
.96forxml and .
.81fortinyc.
the story for oracle queries performed is inversed a rvada requires more oracle queries on average on only benchmarks.for all of these except nodejs a rvada also had much higher f1 scores.
however nodejs is a benchmark with high variance.
on the run with highest f1 score .
higher thanglade s .
a rvada takes s to run and makes 270k oracle calls.
on the fastest run where a rvada only gets f1 score .
a rvada takes s and makes 41k oracle calls.
that is the higher performance cost correlateswith the slower runs on this benchmark of the slowerruns also have higher f1 scores.
overall across all benchmarks a rvada performs only .
as many oracle queries as glade .
this is encouraging as it gives more room for performance optimizations.
fig.
breaks down the average percent of runtime spent in a rvada s most costly components calling the oracle creating scoring and ordering bubbles and sampling stringfor replacement checks.
the error bars show standard de viation note the aforementioned high variance for nodejs appears here too.
on the minutes long benchmarks on which a rvada is at least seconds slower than glade of the runtime is spent in sampling strings for replacement.the current implementation of this re traverses the trees t after each bubble to create these examples.
464while stmt while skip l stmt while bool and space do while if bool and space then whileelse bool false bool true num num and space and space bool and num l n num num fig.
a rvada minedwhile grammar with recall.
nonterminals renamed for readability.
json str dict false true pos int float start digits float start pos int int json list end null na t str str start dict dict lst str json dict lst dict pos int na t int pos int na t float start int.
pos int.
list end json list end str start chars pos int str start pos int chars chars chars pos int chars alnums digits na t alnums fig.
a rvada minedjson grammar with maximum f1 score.
nonterminals renamed for readability.
digits na t and alnums are tokens expanded after the sec.
iii e pass.
on the particularly slow benchmarks tinyc andnodejs arvada spends a long time ordering bubbles.
this makes sense because of the larger example length of the benchmarks.
it is nonetheless encouraging to see this room for improve ment as g etbubbles re scores the full set of bubbles each time a bubble is accepted.
it should be possible to bring downruntime by only scoring the bubbles that are modified by theapplication of the just accepted bubble.
on nodejs a rvada also spends a long time in oracle queries because the time foreach query is much longer ms vs. 3ms for tinyc .
overall a rvada has runtime and number of oracle queries comparable with glade while achieving much higher recalland f1 score.
as for rq3 when the length of the examples insis small oracle calls dominate runtime.
as example length grows the ordering and scoring of bubbles particularly com puting context similarity starts to dominate runtime.
e. qualitative analysis of mined grammars the statistics discussed in the prior section show that a rvada s mined grammars can closely match the groundtruth grammars in terms of inputs generated and accepted.
for rq5 we consider their human readable complexity.
mined grammar readability varies across benchmarks.
for instance on the runs where a rvada achieves recall forwhile the mined grammars look similar to gwfig.
fig.
shows the grammar mined in one of these runs randomly selected from the three.
fig.
shows the grammarwith maximum f1 score mined by a rvada onjson it splitssome expansions at unusual places e.g.
the use of float start but is readable after some examination.
fortinyc the mined grammars are somewhat overbubbled on average they have nonterminals and rulesof average length .
.
on nodejs the grammars have on average nonterminals and rules of average length .
.because glade s grammars are not meant to be human readable they are significantly larger nonterminals with4417 rules of average length .
for tinyc and nonterminals with rules of average length .
for nodejs.
v. d iscussion and threats to validity our implementation of a rvada relies on some heuristic elements which we developed while examining some smallerbenchmarks i.e.
arith while on a particular set of example strings.
to prevent overfitting on these benchmarks for eval uation we used a freshly generated set of example strings.
the definition of maximal generalization assumes that the language accepted by the oracle is context free.
thus wehave no formal guarantees on how the algorithm will react tocontext sensitive input languages.
while our results comparedto glade are promising there is no guarantee they willgeneralize to all benchmarks.
the fact that a rvada smaximum results consistently beat state of the art suggests a few directions for im provement.
if runtime is not a constraint a rvada can be parallelized as is.
to choose the winner first measure precisionwith respect to the oracle.
then evaluate the grammars oninputs sampled from the other mined grammars and choosethe one which captures the most of those samples.
a less wasteful way to parallelize would be to conduct some sortof beam search perhaps using the just described comparativegeneralization metric or to backtrack bad bubbles.
there remains much room to optimize the order in which bubbles are explored and pre tokenization of inputs.
we chosetwo natural metrics for ordering context similarity and fre quency but have not exhaustively examined how to combinethem.
from the difference in performance between the largerbenchmarks tinyc which had simple regex structure and nodejs regexes in the training set are more complex it appears that a rvada could benefit from running at a higher token level.
developing better heuristics for tokenization orpairing a rvada with a more complex regex learning algorithm than that described in section iii e may yield benefits.
vi.
r elated work automatically synthesizing context free grammars from examples is a long studied problem in computer science lee and stevenson and cordy give a survey ofsome techniques.
gold s theorem states that grammarscannot be learned efficiently from a set of positive examplesalone.
angluin and kharitonov show that pure black boxapproaches face scalability issues on arbitrary cfgs.
but real world grammars may not be so adversarial.
our heuristics usestatistical information to heavily prune the search space.
465the core idea in solomonoff s algorithm is to for each example find substrings of the example that can be deleted.
if a substring can be deleted solomonoff proposes toadd a recursive repetition rule for the substring.
rather thantrying to generalize each example string individually a rvada considers all example strings together when producing candi date strings.
unlike arvada knobe and knobe assume ateacher that can provide new valid strings if the current pro posed grammar does not match the target grammar.
for eachnew valid string their algorithm adds the most general validproduction of the form s b 1b2 b nto the grammar wherebiare terminals or existing nonterminal.
it adds new nonterminals by merging nonterminal sequences which havethe same left and right contexts in expansions.
glade learns context free grammars in two phases.
first it learnsa regular expression generalizing each input example.
then it tries to merge subexpressions of these regular expressionsin a manner similar to our label merging.
reinam usesreinforcement learning to refine a learned cfg allowing fuzzymatching through a pcfg.
it is complementary to our work as the module that learns a cfg in their evaluation glade could be replaced by a rvada .
l and rpni are two classic algorithms for the learning of regular languages.
l learns regular languages with the stronger assumption of a minimally adequate teacher whichcan both act as an oracle for the target language and given a learned regular language assert whether it is identicalto the target language or give a counterexample.
rpni learns regular languages in polynomial time assuming a setof positive and negative examples.
glade was found tooutperform both these algorithms for program input grammars.the original l paper also describes lcf an algorithm for learning context free languages in polynomial time assumingthat the set of terminals and non terminals is known ahead oftime.
this assumption is not reasonable in most contexts.
closely related is the field of distributional learning.
clark et.
al present polynomial algorithms for learningbinary context feature grammars which capture context freelanguages in addition to more complex languages fromstrings.
the algorithms rely on the representation of wordsby their contexts an interesting relation to a rvada s use of k contexts.
unfortunately polynomial does not mean fast in practice.
we implemented these algorithms in python even themore efficient one took nearly hours to run on our while benchmark.
work on strong learning learns grammarswith good parse trees over tokenized inputs.
again becauseit uses full context information it does not scale to large ex ample sets and overgeneralizes on non substitutable grammars.this highlights the practical importance of k contexts.
also related is the field of automata learning learnlib is a state of the art java framework implementing several ofthese algorithms.
in particular it provides an implementationof the ttt algorithm for learning vpda.
these automataaccept a subclass of deterministic context free languages .ttt is optimized for situation where the key structure of in puts used to query the oracle can be collected in a prefix closedset as in learning from logs of system behavior.
this is lesswell suited to program inputs with multiple distinct recursivestructures.
ttt also relies on the stronger assumption of aminimally adequate teacher rather than a blackbox oracle.
another branch of works use grey or white box information about the oracle to learn grammars.
lin et al.
s work exam ines execution traces in order to reconstruct program inputsgrammar .
a utogram tracks input flows into variables and uses this dataflow information to learn a well labeled grammar.
mimid goes a step further tracking thecontrol flow nodes in which input characters are accessed.it directly maps this control flow structure to the grammarstructure and again can take advantage of function names.
theuse of this additional oracle information may make the finalgrammars more robust and speed up the inference process.on the other hand a rvada s blackbox assumption makes it flexible when this information is not readily accessible orfor strangely structured programs.
our tinyc benchmark was taken directly from mimid s evaluation and a rvada achieved an average f1 score .
compared to mimid s .
.
this isimpressive given that a rvada uses the oracle as blackbox.
section iv c discussed the use of deep learning to learn input structures for fuzzing.
other techniques do somethinglike grammar mining to increase the effectiveness of fuzzing.parser directed fuzzing uses direct comparisons to inputbytes to automatically figure out tokens of the input structure it works best on recursive descent parsers.
grimoire leverages a sort of one level grammar by denoting nontermi nal regions of the code as those which can be changed whilemaintaining a certain kind of branch coverage.
lastly the sequitur compression algorithm resembles the bubbling phase of a rvada bubbling sequences that appear with high frequency .
sequin extends sequitur tomine attribute grammars.
neither algorithm allows for recur sive generalization by merging bubble induced nonterminals.
vii.
c onclusion we presented a rvada a method for learning cfgs from example strings and oracles.
we found that a rvada outperformed glade in terms of increased generalization on11 benchmarks with a higher f1 score on average on ofthese benchmarks.
these two benchmarks on which a rvada performs relatively less well are a regular language for urls and a language with more complex regular expressions fortokens.
this along with qualitative analysis of the inputsgenerated by a rvada and glade suggests that a rvada does best in learning recursive structures over tokens andthat a compelling avenue for improvement is a separatetoken learning step.
a rvada is available as open source at