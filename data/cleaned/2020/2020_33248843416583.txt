detecting and explaining self admitted technical debts with attention based neural networks xin wang school of computer science wuhan university wuhan china xinwang0920 whu.edu.cnjin liu school of computer science wuhan university wuhan china jinliu whu.edu.cnli li faculty of information technology monash university melbourne australia li.li monash.edu xiao chen faculty of information technology monash university melbourne australia xiao.chen monash.eduxiao liu school of information technology deakin university geelong australia xiao.liu deakin.edu.auhao wu school of information science and engineering yunnan university kunming china haowu ynu.edu.cn abstract self admitted technical debt satd is a sub type of technical debt.
it is introduced to represent such technical debts that are intentionally introduced by developers in the process of software development.
while being able to gain short term benefits the introduction of satds often requires to be paid back later with a higher cost e.g.
introducing bugs to the software or increasing the complexity of the software.
to cope with these issues our community has proposed various machine learning based approaches to detect satds.
these approaches however are either not generic that usually require manual feature engineering efforts or do not provide promising means to explain the predicted outcomes.
to that end we propose to the community a novel approach namely hatd hybrid attention based method for self admitted technical debt detection to detect and explain satds using attention based neural networks.
through extensive experiments on comments in projects we show that hatd is effective in detecting satds on both in the lab and in the wild datasets under both within project and cross project settings.
hatd also outperforms the state of theart approaches in detecting and explaining satds.
ccs concepts software and its engineering software notations and tools software maintenance tools.
keywords self admitted technical debt word embedding attention based neural networks satd jin liu is corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn .
.
.
.
reference format xin wang jin liu li li xiao chen xiao liu and hao wu.
.
detecting and explaining self admitted technical debts with attention based neural networks.
in 35th ieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
introduction technical debt is a metaphor proposed by cunningham used to denote a suboptimal solution that developers take shortcuts to achieve rapid delivery during development.
some typical examples of technical debts include introducing hard coded values into the code making code changes while ignoring failing unit tests copypasting code from other modules etc.
those technical debts are usually introduced to a software system unintentionally by developers or intentionally but without adequately documented.
as time goes by those undocumented technical debts if not resolved in time might be faded away from the developers minds and become equivalent to unintentionally introduced technical debts .
to mitigate this issue developers may decide to document via comments their intentionally introduced technical debts.
the documented debt is often known as self admitted technical debt satd in short .
no matter intentionally or unintentionally the compromises made to introduce technical debts will likely lead to negative effects on the maintenance of the software in the long run .
indeed as argued by megan horn1 technical debts may cause software systems to behave unexpectedly and those surprising behaviors could be notoriously difficult to test and fix.
often fixing one issue could introduce several new issues resulting in high costs for resolving technical debts.
as revealed in a recent study by cast software2 a provider of software analysis and measurement tools the amount of technical debts that have to be addressed after the application is deployed in production on average is over one million for each business application.
35th ieee acm international conference on automated software engineering ase ase september virtual event australia xin wang and jin liu et al.
table examples of satd comments.
pr oject sa td comments apache ant todo allow user to request the system or no parent what is the property supposed to be?
cannot remove underscores due to protected visibility jfr eechart do we need to update the crosshair values?
defer argument checking... do we need to update the crosshair values?
jmeter don t try restoring the url todo wy not?
can be null not sure why maybe move to vector if mt problems occur squirr el do we need this one.
is this right???
verify this argouml why does the next part not work?
shouldn t we throw an exception here?!?!
the following can be removed if selectall gets fixed despite the fact that technical debts will introduce negative impacts to the maintenance of software technical debt is still widespread and seems to be unavoidable in software systems .
hence there is a strong need to invent automated approaches to detect technical debts and fix them as earlier as possible.
nevertheless it is a challenging endeavor to handle all the types of technical debts at once .
as the initial step towards achieving such a purpose many state of the art works start by focusing on the detection of satds .
unlike other technical debts satds are often highlighted as comments in the code of the software .
table enumerates some satd examples identified in popular open source projects.
for example the comment may be replaced later in the jmeter project indicates that the current code is a temporary solution and needs to be replaced in the future.
although explicitly highlighted as shown in table satds are still largely presented in open source software projects.
our community has hence spent various efforts to experimentally characterize satds.
for example potdar et al.
attempt to manually explore satds in code comments of java projects and eventually summarize patterns that can be used to identify satd.
huang et al.
have proposed a text mining method to identify satd and have achieved substantial improvements compared with patternbased methods.
yan et al.
propose a change level method for detecting satds utilizing software change features.
unfortunately existing state of the art approaches are either involving heavy manual pre processes such as identifying the satd relevant patterns or extracting features to fulfill ml based classifiers or lacking reliable explanations to elucidate why a satd is flagged as such .
to fill this gap we propose to the community a novel deep learning based approach to detect and explain self admitted technical debts in open source software projects.
we design and implement a prototype tool called hatd.
first it leverages positional encoder and bi directional long shortterm memory bi lstm network to capture the sequential characteristics of satds from code comments.
then it leverages diverse attention mechanisms to highlight the importance of the automatically prepared features that have contributed to the detection of satds.
the most important features will subsequently be leveraged to explain the classification result e.g.
why a comment is not flagged as satd?
through extensive experiments on satd non satd0246810number of commits figure distribution of changes made to satd involved and non satd involved source code files.
comments in projects hatd demonstrates its superior performance and explainability in both within project and cross project satd detections over the state of the art methods.
the main contributions of this paper can be summarized as follows we provide an overview of code comment characteristics that could be challenging for creating automated tools to detect satds in software projects.
we design and implement a prototype tool called hatd which leverages a hybrid attention based method combining both singlehead attention and multi head attention mechanisms for pinpointing satds from code comments.
we demonstrate the effectiveness of hatd by comparing with state of the art methods on open source benchmark projects detecting satds in real world and popular software projects and providing explanations to the classification results.
motivation and background to help readers better understand our work and motivate the necessity of developing automated approaches for pinpointing satds we first present in this section a simple empirical investigation to check whether satds impact the maintainability of software projects.
then we enumerate some characteristics of code comments that make it challenging to automatically interpret the semantics of the comments i.e.
pinpointing self admitted technical debts .
.
satd vs. code changes intuitively satds will trigger more future code changes i.e.
increasing the maintainability of the software project .
to quantify the relationship between satds and code changes we conduct a simple empirical study count the number of commits related to files that have been changed with and without technical debts to check if satds will trigger more future code changes.
to this end we select a popular open source project called apache ant to fulfill the experiments.
the reason why this project is chosen is that the satds introduced over the development of this project are known.
indeed as of jan the apache ant project has introduced in total satds accounting for roughly .
of total comments cf.
.
872detecting and explaining self admitted technical debts with attention based neural networks ase september virtual event australia in order to explore the long term impact of satds on the maintenance of the project we set a cutting point on jan and recount the satds introduced at that point.
among satds of them have already been introduced into the project and have not been resolved involving java files.
in other words at this point java files are not involved with satds.
starting from this cutting point we revisit all upcoming commits and count the times of changes related to each java file.
the final results are plotted in figure which illustrates the distribution of changes made to satd involved and non satd involved java files.
it is quite clear that satd involved files require more changes compared to the files without involving satds.
this difference is further confirmed by a mann whitney wilcoxon mww test i.e.
the difference is significant at a significance level3of .
.
this simple empirical investigation shows that satds indeed negatively impact the maintenance of software projects and hence there is a strong need to invent promising approaches to detect and resolve them .
.
characteristics of code comments code comments are often provided by developers to remind themselves to record unsatisfactory design decisions question design decisions of other developers and make future changes.
since code comments are usually written in natural language by different developers with different writing styles they may come with characteristics that are difficult to interpret .
in this section we summarize some of the characteristics of code comments that make automated detection of satds challenging domain semantics.
code comments often contain jargons method names and abstract concepts that only programmers can understand.
for example the comment fixme i use a for block to implement end node because we need a proc which captures in jruby project contains a term for an abstract concept end node.
level of details comment length .
the length of phrases in code comments can vary widely ranging from one word like unused load to short phrases like call workaround should probably error and to sentences like this is such a bad way of it but ... .
this varying length text feature in code comments makes it challenging as well to detect satds.
project uniqueness.
satd comments on different projects will show different styles due to different developers as empirically confirmed by potdar et al.
in their manual pattern summarization work.
in addition the coding habits and vocabulary gaps among different developers aggravate this phenomenon.
polysemy.
polysemy is common in sentence expressions.
it is often accompanied by domain knowledge in code comments.
for example code comments may contain method names.
it is challenging to distinguish method names and common words which often requires contextual information.
abbreviation.
to facilitate memorization and writing efficiency developers often use abbreviations in code comments.
project differences often exacerbate this phenomenon.
for example the comment no default in db.
if nullable use null.
in project squirrel in which dbis the abbreviation of database.
3given a significance level .
if p value there is one chance in a thousand that the difference between the datasets is due to a coincidence.
code comment word embeddinglabel of code commenthatd feature learning satd detectionfigure the overall architecture of our approach.
these characteristics exhibited frequently in satd comments make traditional pattern based and text mining based satd detection approaches difficult to achieve high performance.
therefore there is a strong need to mine the in depth implicit and semantic features from the code comments so as to satisfy the different challenges in effectively detecting satds.
our approach we propose a hybrid attention based method for self admitted technical debt detection named hatd.
hatd analyzes code comments of different projects or application domains challenge and aiming to learn comprehensive implicit features of satds.
it can flexibly extract and aggregate variable length text features of satd detections challenge .
to overcome polysemy and abbreviation issues challenge and we use elmo to obtain dynamic word embedding across contexts for satd detections.
as depicted in figure hatd has three modules.
the word embedding module maps the words in the comments to vectors.
different word embedding techniques can be incorporated into our approach by default the elmo algorithm is implemented .
in the feature learning module we use positional encoder and bi lstm to learn sequential features then we leverage diverse attention mechanisms to further obtain potential representation of words and distinguish the importance of different words.
finally the learned features are fed into the detection module to predict satds.
.
word embedding word embedding is a collective term for language models and representation learning techniques in natural language processing nlp .
conceptually it refers to embedding a high dimensional vector whose dimension is the number of all words into a vector of a much lower dimension.
the goal of word embedding module is to overcome the aforementioned challenges cf.
section .
in handling code comments.
let us consider the example of keyword proc from code comment fixme i use a for block to implement end node because we need a proc which captures in jruby project.
the proc here is the abbreviation of process which may be seen as oov out of vocabulary .
because the labeled code comment is limited and we cannot guarantee that any test word exists in training corpus.
according to morphological knowledge fasttext solves the oov problem to some extent by learning character based embedding representation of subwords.
static word embedding techniques cannot deal with the phenomenon of polysemy which is common in comments.
dynamic word representation technique such as elmo can deal with this problem to some extent by adjusting the word vector according to the context.
elmo embedding from language models is a new type of deep contextualized word representation language model which can model the complex features of words such as syntax and semantics and the change of words across linguistic contexts i.e.
to polysemy .
since the 873ase september virtual event australia xin wang and jin liu et al.
comment matrixfeature learning single head attention encoder multi head attention encoderdimension fixme formatters are not thread safes1 s2 figure the working process of the feature learning module.
meaning of words is context sensitive elmo s principle is to input sentences into a pre trained language model in real time to get dynamic word vectors which can effectively deal with polysemy.
considering that the main application scenario of elmo is to provide dynamic word vectors acting on downstream tasks our model integrates elmo to identify the satds.
detailed comparison of existing word embedding schemes can be found in section .
.
.
feature learning the feature learning module consists of two sub modules a singlehead attention encoder sae and a multi head attention encoder mae adopting different attention mechanisms and sequence feature extractors.
the single head attention encoder first inputs the embedding representation of the comment into a bi lstm network then we can use an attention mechanism to assign different weights to different words according to the importance of words for the purpose of interpreting detection results.
in order to further encode the current word the multi head attention encoder allows the encoding of words considering other words information via the so called self attention mechanism.
by combining the sae and mae modules we hope to exploit the strengths of the rnnbased model and the transformer based model to improve satd detection performance and interpret the detection results with attention mechanism.
figure 3shows the internal structure of the feature learning module.
the details of these two internal sub modules will be given later.
.
.
single head attention encoder sae .
given a code comment sae module firstly learns the features of the comment from the preceding as well as the following tokens with a two layer bi lstm network.
in this process we can exploit an attention mechanism to assign different attentions to words for the purpose of interpreting detection results.
figure 4shows the working process of sae which is carried out in two steps extracting sequential features of the comment and assigning attention weight to each word.
s1 denotes the obtained implicit feature vector of the comment.
word encoder.
given a code comment we assume that the comment contains lwords.xtrepresents the tth word s vector.
we use a bi lstm to get implicit representation of words by summarizing information from both directions for words.
the bi lstm contains the forward lstm which reads the comment cfromx1to bi lstmbi lstm tanh2 layersattention layer comment matrix s1saefigure workflow of the sae module.
self attentionnormalization positional encoding s2 comment matrixmae figure workflow of the mae module.
xland a backward lstm which reads from xltox1 ht lstm xt t h t lstm xt t .
we obtain the implicit representation of a given word xtby concatenating the forward hidded state htand back hidden state h t i.e.
ht h t .
assigning attention weights.
in a code comment each word contributes differently on satd detection.
for example potdar et al.
manually summarized patterns for detecting satds.
considering the evolution of the project and gaps between developers the manually summarized patterns lack generalizability and adaptability to newly generated code comments.
based on these above considerations we exploit an attention mechanism to distinguish contributions of different words and aggregate the representation of those informative words to form a sentence vector.
ut tanh ww ht h t bw at softmax ut t uw s1 tat ht h t we firstly feed word representation ht h t through one layer perception to get utas a hidden representation of ht h t then we measure the importance of the word utwith a word level context vectoruwand get a normalized importance weight atthrough a softmax function.
finally we add all the word vectors with attention weight as the final feature representation s1 of the code comment.
.
.
multi head attention encoder mae .
the purpose of mae is to focus on all the words in the entire input sequence to help the model better encode the current word.
figure 5illustrates the working process of mae in two steps encoding positional information of words and encoding words considering other words information.
s2denotes the obtained implicit feature vector of the comment.
874detecting and explaining self admitted technical debts with attention based neural networks ase september virtual event australia capturing positional information.
to learn the order of the words in the input sequence we firstly add a positional vector following the one proposed by vaswani et al.
which follows the specific pattern that the model learns.
then we perform a linear transformation on the result.
the propagation rule is defined by the following equation p i t sin t 100002i d if i 2k cos t 100002i d if i 2k xt xt pt wp bp wheretis the position iis the dimension and dis the dimension of the produce output.
assigning self attention.
given a code comment the representation of a word is closely related to the other words in the comment.
to obtain further word vector representation we introduce self attention mechanism to better encode the words.
the details are given by the following equations qj xwq kj xwk vj xwv headj softmax qjkt jp dk vj s2 norm cat head ... headh wo here x rl dis the word vector matrix.
we compute the attention function on a set of queries simultaneously and pack them together into a matrix q. the keys and values are also packed together into matrices k and v .wq rd dq wk rd dk andwv rd dvare three learnable weight matrices.
we use eight attention heads h 8by default to expand the model s ability to focus on different words.
for each head we have dq dk dv d h wo rhdv d. after concatenating the embedding vectors from all the heads we introduce layer normalization for fixing the mean and the variance of the accumulated inputs.
.
satd detection finally as is shown in figure s1ands2are concatenated to form an abstract representation of the code comment and then input into a fully connected layer ffcfor satd detection.
prediction results are calculated by a sigmoid function.
the classification rule is defined by the following equations y sigmoid ffc s1 s2 here denotes the concatenate operation and ydenotes the prediction result of satd detection.
as shown in table satd and non satd comments have a very imbalanced distribution in the open source projects .
to address the data imbalance problem instead of downsampling or applying feature selection that may sacrifice the feature learning capability we introduce a weighted cross entropy loss as the loss function.
suppose there are asatd comments and bnon satd s1 s2non satd satdsatd detection sfully connected layer w s bfigure workflow of module combination and satd detection.
comments in the training data then the loss function is defined as weights b a bifsth class is satd a a botherwise loss y t weights stslog ys here tand yare the groundtruth label and predicted label of the comment respectively.
all the parameters in the model are trained via gradient descent.
experimental setup in this section we have conducted comprehensive experiments on satd detections aiming to answer the following research questions rq1 is hatd effective in detecting self admitted technical debts in software projects?
rq2 how does hatd compare with the state of the art approaches for recognizing self admitted technical debts?
rq3 how effective is the word embedding technique integrated into hatd compared with other baseline techniques?
rq4 to what extent are self admitted technical debts existing in real world software projects?
.
dataset description in our experiments we use the dataset provided by .
this dataset contains labeled code comments for ten open source projects including apacheant argouml columba emf hibernate jedit jfreechart jmeter jruby and squirrel.
the detailed descriptions of the dataset are shown in table .
these ten projects belong to different application areas and have different numbers of contributors comments as well as scale and complexity.
we can see that only a small ratio of satd comments in each project.
.
evaluation metrics in this work we adopt three commonly used metrics namely precision recall and f1 score the balanced view between precision and recall to evaluate the performance of our approach.
875ase september virtual event australia xin wang and jin liu et al.
table statistics of satd comments in the benchmark software projects.
pr oject description release contributions comments satd of satd apache ant java library and command line tool .
.
.
argouml uml modeling tool .
.
columba e mail client .
.
emf eclipse model driven architecture .
.
.
hibernate orm framework .
.
.
jedit text editor .
.
jfreechart char library .
.
.
jmeter performance tester .
.
jruby ruby interpreter .
.
.
squirrel sql client .
.
.
a verage .
precision tp tp fp recall tp tp fn f1 precision recall precision recall here tp true positive means the number of satd comments that are detected as such fp false positive means the number of non satd comments that are detected as satd and fn false negative means the number of satd comments that are detected as non satd.
.
parameters and experimental environment in the bi lstm component of our model the number of layers is two the number of hidden units is set to be two branches produce outputs with units respectively.
the adam optimizer with default setting except that learning rate is configured to be 1e4 is used for training.
in order to avoid overfitting we add a dropout layer between every two layers with a drop probability of .
.
we setl2loss weight as 5e .
due to the unbalanced distribution of satd and non satd comments we introduce a weighted crossentropy loss as the loss function.
we compute the mean values of the precision recall and f1 score across ten experiments to estimate the performance of models.
the experiments are run on a linux system ubuntu .
lts with 64gb memory and a rtx2080ti gpu implemented by leveraging the deep learning library pytorch4.
result we now present our experimental results towards answering the aforementioned research questions.
.
rq1 effectiveness of hatd towards evaluating the effectiveness of our approach we apply hatd to study the ten projects maintained by since these ten projects have been provided with ground truth.
based on these ten projects we evaluate the performance of hatd from two aspects within project.
for each project we perform fold crossvalidation to assess the classification capability of hatd.
in particular we first equally divide the project s comments ten subsets.
then we train our model with nine subsets of comments and leverage it to classify the remaining subset of comments.
we repeat this experiment times to ensure that each subset has been considered as a testing set once.
the final performance is reported as the average score of the ten rounds of experiments.
cross projects.
in this aspect we also conduct our experiments in ten rounds with one project tested in each round.
specifically for each project we train our model based on the other nine projects and leverage it to predict all the comments of the project under testing.
the classification results will be directly attached to the testing project for the sake of simplicity despite that the classification models are trained based on the other nine projects.
table the performance of our approach over both withinproject and cross projects satd detections.
pr ojectwithin pr oject cr oss projects pr ecision recall f1 pr ecision recall f1 apache ant .
.
.
.
.
.
argouml .
.
.
.
.
.
columba .
.
.
.
.
.
emf .
.
.
.
.
.
hibernate .
.
.
.
.
.
jedit .
.
.
.
.
.
jfreechart .
.
.
.
.
.
jmeter .
.
.
.
.
.
jruby .
.
.
.
.
.
squirrel .
.
.
.
.
.
a verage .
.
.
.
.
.
table 3presents the experimental results of our approach w.r.t.
precision recall and f1 score metrics.
the best and worst results are respectively highlighted in bold and underline and the difference is quite significant.
this finding shows that the our approach might be sensitive to the training dataset.
indeed our approach yields relatively poor performance for the jedit project because many of its comments are short and sometimes meaningless e.g.
inner classes or unsplit method resulting in noises or a lack of effective semantic information to be learned by our approach.
in addition the fact that some projects achieved better performance in the within project setting e.g.
jfreechart while others achieved better performance in the cross projects setting e.g.
jedit further confirms that the quality of the training set is important to our approach.
876detecting and explaining self admitted technical debts with attention based neural networks ase september virtual event australia nevertheless the experimental results show that our approach generally yields good performance for most of the projects concerning both within project and cross projects settings.
indeed on average our approach achieves a precision recall and f1 score of .
.
and .
respectively for within project satd detection and a precision recall and f1 score of .
.
f1 score of .
respectively for cross projects satd detection.
it is worth mentioning that our approach does not perform significant differences between within project and cross projects settings.
this evidence suggests that our approach could be effective and practical for pinpointing satds in real world software projects.
answer to rq1 our approach can achieve highperformance for both within project and cross projects settings and hence is effective and practical to be used to pinpoint satds for real world software projects.
.
rq2 performance comparison with the state of the art approach table the performance of our appraoch compared to the state of the art.
pr ojectswith pr oject cr oss project ren s ours gains ren s ours gains apache ant .
.
.
.
.
.
argouml .
.
.
.
.
.
columba .
.
.
.
.
.
emf .
.
.
.
.
.
hib ernate .
.
.
.
.
.
jedit .
.
.
.
.
.
jfr eechart .
.
.
.
.
.
jmeter .
.
.
.
.
.
jruby .
.
.
.
.
.
squirr el .
.
.
.
.
.
a verage .
.
.
.
.
.
we now compare our approach with the state of the art approach proposed by ren et al.
.
to the best of our knowledge this is the closest work to ours since both approaches are implemented based on deep learning to detect satds.
their method leverages basic neural networks to accomplish its purposes while our approach brings attention mechanism into neural networks to achieve our objectives.
we compare our approach with their method on the same dataset i.e.
the ten projects with ground truth.
table 4summarizes the comparison results.
in within project satd detection compared with ren s method our method achieves performance improvements on all projects ranging from .
to .
.
especially in the emf project which only contains comments our method gets a .
improvement.
this evidence suggests that our approach is more resilient to small training corpora.
on average our method obtains an f1 score of .
which is .
higher than that of ren s method.
in cross projects satd detection our method achieves even better performance improvements ranging from .
to .
compared to the within project setting.
on overage for the ten projects our method achieves an f1 score of .
which is .
higher than that of ren s method.
it is worth highlighting that forthe jedit project our method achieves .
improvement.
the performance improvement for the emf project also exceeds .
these experimental results demonstrate that our approach can learn more useful information from other projects than ren s method.
hence our approach is more practical to be applied to detect satds in real world software projects.
finally despite that we have only compared our approach with the state of the art ren s approach our approach should also outperform other similar ones targeting satd extractions or classifications.
indeed as experimentally demonstrated by ren et al.
over the same dataset their method can outperform three baseline approaches including a pattern based satd extraction approach a traditional text mining based satd classification approach and kim s simple cnn based sentence classification approach.
therefore since our approach can outperform ren s method in the same regard we believe that our approach should also be able to outperform the aforementioned state of the art approaches.
answer to rq2 hatd outperforms the state of the art approaches including neural network based ones such as ren s approach and pattern based ones as well as traditional text mining based approaches.
.
rq3 effectiveness of the selected word embedding technique word embedding allowing to embed a high dimensional space with the number of all words into a continuous vector space with a much lower dimension has become one of the most important techniques in the natural language processing nlp community.
apart from the elmo technique adopted in this work the nlp community has invented various other methods to realize word embedding.
towards answering the third research question we conduct a comparative study between elmo and some of the other methods to justify our selection of elmo and its effectiveness.
to the best of our knowledge there are at least four additional popular word embedding techniques available in the community that we can compare with.
one hot is the simplest encoding method.
each word has its own unique word vector representation.
the dimension of the word vector is the length of vocabulary.
only one position in each word vector has a value of and the others are .
one hot encoding is to assume that all words are independent of each other.
it ignores the semantic similarity between words and causes severe data sparsity problems when the corpus is very large.
word2vec is an open source toolkit for generating word vectors launched by google in containing two traning modes skip gram and continuous bag of words cbow .
the skip gram model is concerned with using one word to predict the surrounding words while cbow model is concerned with using the surrounding words to predict the central word.
in order to improve model accuracy and training speed word2vec usually uses negative sampling and hierarchical softmax.
word vectors generated by word2vec can better express the similarity and analogy relationship between words.
877ase september virtual event australia xin wang and jin liu et al.
table the comparison results of elmo and other state of the art word embedding techniques.
pr ojectswithin pr oject cr oss projects elmo one hot w ord2vec glo ve fastt ext elmo one hot w ord2vec glo ve fastt ext apache ant .
.
.
.
.
.
.
.
.
.
argouml .
.
.
.
.
.
.
.
.
.
columba .
.
.
.
.
.
.
.
.
.
emf .
.
.
.
.
.
.
.
.
.
hib ernate .
.
.
.
.
.
.
.
.
.
jedit .
.
.
.
.
.
.
.
.
.
jfr eechart .
.
.
.
.
.
.
.
.
.
jmeter .
.
.
.
.
.
.
.
.
.
jruby .
.
.
.
.
.
.
.
.
.
squirr el .
.
.
.
.
.
.
.
.
.
a verage .
.
.
.
.
.
.
.
.
.
glove global vectors for word representation is essentially a log bilinear model with a weighted least squares objective.
the model is inspired by the word co occurrence probability that may encode global information for words.
this just makes up for the weakness of word2vec just using local word co occurrence information.
each word of glove involves two word vectors one is the vector of the word itself and the other is the context vector of the word.
the final representation of the word vector is obtained by adding the two vectors.
fasttext can be regarded as a derivative of word2vec which performs word vector training based on the knowledge of language morphology.
for each word entered a word based n gram representation is performed and then all n grams are added to the original word to represent morphological information.
the advantage of this method is that in english words the morphological similarity of prefixes or suffixes can be used to establish relationships between words.
to conduct a fair comparison we respectively reimplement the word embedding module with one of the aforementioned four techniques and keep the remaining implementation of hatd unchanged.
we then launch the new version of hatd on the same dataset with the same parameters and experimental settings i.e.
within project and cross projects .
the word embedding size determines the ability of the model to capture the feature information of a code comment.
in order to avoid the influence of different word embedding size on the experimental results we uniformly set the embedding size as except onehot embedding method whose dimension is always equal to the size of the corpus.
table 5presents the experimental results for both within project and cross projects settings.
in the within project setting due to the small size of corpus even onehot can perform well and even slightly better than other word embedding techniques word2vec glove and fasttext .
elmo achieves the best performance for eight projects.
this result shows that elmo is a promising word embedding technique to be used for supporting the automated classification of satds.
this evidence can be easily observed in the cross projects experiments as well.
indeed as also shown in table for nine out of the ten selected projects elmo achieves the best performance in supporting hatd pinpoint satds.
moreover as suggested by the average performance for both within projectand cross projects experiments at least for supporting satd classifications elmo performs much better than that of other word embedding techniques i.e.
one hot word2vec glove fasttext .
this can be explained by the fact that elmo will generate different vector representations for the same word depending on the context.
for example some teams use the term rework to specify routine refactoring which is not always td.
elmo can alleviate this situation to some extent.
in the within project satd detection the used training set and testing set come from the same project with similar comment characteristics due to the same batch of developers.
while in the crossprojects satd detection the comments of the testing set come from a new project which is different from the training set.
overall the performance of cross project satd detection is generally lower than the within project satd detection to varying degrees in all word embedding techniques.
answer to rq3 elmo outperforms other word embedding techniques by achieving mostly the best or second best performance when supporting neural network based satd detections.
moreover under the same context dynamic word embedding technique seem to be more reliable than static ones.
.
rq4 satds in real world software projects to evaluate the practicality of hatd in this last research question we assess to what extent can hatd be leveraged to detect satds in real world software projects which have no ground truth constructed beforehand.
in addition we also evaluate whether hatd is applicable to other programming languages such as python javascript etc.
to this end we resort to github to search for hot projects and eventually we select ten software projects taking different technical fields into consideration.
table 6summarizes the ten projects along with their metadatas such as short descriptions number of total comments etc.
we then train our approach based on the ten benchmark apps as enumerated in table and apply it to classify the comments of each of the aforementioned ten real world software projects.
the last second column of table 6illustrates the experimental results.
in total among comments hatd flags of them as 878detecting and explaining self admitted technical debts with attention based neural networks ase september virtual event australia table the list of our selected real world popular software projects.
pr oject description release stars contributions comments satd of satd spring boot a framework for creating spring based applications.
.
.
.0k .
dubbo a high performance java based open source rpc framework.
.
.
.9k .
okhttp square s meticulous http client for java and kotlin.
.
.
.7k .
guava google core libraries for java.
.
.8k .
vue a progressive javascript framework for building ui on the web.
.
.
.0k .
werkzeug the comprehensive wsgi web application library.
.
.
.3k .
pytorch tensors and dynamic neural networks in python.
.
.
.9k .
django the web framework for perfectionists with deadlines.
.
.
.7k .
scikit learn a python module for machine learning.
.
.
.3k .
tensorflow an open source machine learning framework for everyone.
.
.
.0k .
a verage .0k .
satds.
the ratios of satds in all the project s comments are quite low ranging from .
to .
giving an average of .
.
compared with the ground truth projects we observe that there are fewer satds in these ten real world software projects we selected which may be because they are very popular on github and more contributors participate in code commits and maintenances.
the average number of code contributors in these software projects is ten times that of the ground truth projects.
manual verification of the results reported by hatd.
we randomly select samples each comments in total from the code comments marked as having technical debts and no technical debt for further analysis.
we then invite five programmers to manually judge these selected comments and compare their decisions with the results yielded by our attention based deep learning model.
the five programmers manually flag the comments as satd or non satd independently and then discuss with each other to reach consensus if inconsistent decisions are made in the first place.
among the code comments marked as satd of them are not flagged by five programmers as such.
among the code comments marked as non satd only five comments differ from the manual annotations.
overall our approach yields an accuracy of in detecting satds in popular real world software projects.
considering that the actual projects we selected come from different technical fields and have significant domain differences some domain characteristics will not be fully covered by our training set based on the ten benchmark apps .
with the increase of training data we believe that the performance of our model can be further improved.
answer to rq4 in a practical scenario our approach achieves an accuracy of in detecting satds in realworld software projects.
the fact that all the selected software projects have more or less involved with satds shows that satd is commonly spread in software projects.
therefore we argue that software developers and maintainers should pay more attention to satds.
discussion we now discuss the explainability of our approach to satd detection and introduce potential threats to validity of this study.
this is a hack for the multiline column comment .
.
.
.
.
todo there is probably a better way to do this hack comment .
.
.
.
.
.
.24figure visualization of weights for two code comments comment this is a hack for the multiline column comment todo there s probably a better way to do this hack .
.
interpretability of hatd in order to interpret the result of satd classification potdar et al.
have manually summarized frequent satd patterns for detecting satd comments.
however this manual process is time consuming and labor intensive and difficult to summarize all satd patterns.
ren et al.
exploit the backtracking mechanism to map cnn extracted features to key h grams and summarize key h grams into satd patterns.
their method however requires a backtracking mechanism to propagate the extracted features back into the same structure of the model.
in this work we propose an attention based neural network to detect and explain satd classifications.
through the visualization of weights we find our method can immediately provide word level and phrase level interpretability for each comment regardless of project differences and time consuming pattern summary issues.
figure 7shows two examples of the comments we have examined.
the color shade indicates the attention weight of words.
the higher the attention weight the darker the color.
for visualization purposes we remove padding words to ensure that only meaningful words are emphasized.
from figure we can see that our method highlights the word hack in the first comment.
while in the second comment our method highlights the word todo and the phrase a better way.
we find an interesting phenomenon that the keyword hack carries a significant satd tendency in the first comment which is different from that in the second comment.
this shows that the same keywords will play different roles for satd detection 879ase september virtual event australia xin wang and jin liu et al.
in different comments which further exacerbates the difficulty of manually summarizing satd patterns to identify satds.
furthermore we summarize in table 7the top satd patterns for each of the ten open source projects.
interestingly we can observe that many projects share some common one gram satd patterns underlined such as todo xxx fix fixme hack work perhaps workaround better and bug .
at the same time some satd patterns appear in only one or two projects.
for instance a public setter is only used in apache ant project model element model extent and behind addtrigger are only used in argouml project programming error and tweak are only used in jmeter and jfreechart projects respectively.
we observe that some patterns bold are not discovered by potdar et al.
and ren et al.
suggesting that manual summary of satd patterns will likely miss some less evidential satd patterns and will also encounter the problem of project differences.
our method can supplement those works by automatically identifying satds and highlight keywords or phrases contributing to such identifications regardless of project differences.
.
threats to validity threats of internal validity are related to errors in our implementation and personal bias in data labeling.
to avoid implementation errors we have carefully checked our experimental steps and parameter settings.
to avoid the personal biases of manually marking the code comments the dataset used in our experiments has a high inter rater agreement cohen s kappa coefficient of .
which has reported by maldonado et al.
.
in addition we do not use the fine grained satd categories proposed by and only consider the binary classification of code comments.
in other words design debt requirement debt defect debt documentation debt and test debt are all simply treated as satds.
threats of external validity are related to both the quantity and quality of our experimental dataset and the generalizability of our experiment results and findings.
to guarantee the quantity and quality of our dataset we have taken into account ten open source projects with different functions and fields and a different number of comments as well as comment characteristics.
in order to guarantee the generalizability of our experiment results and findings we have crawled another ten popular software projects from github to test our trained model.
by manually verifying the results reported by our method we conclude that our method has achieved an accuracy of on randomly selected comments.
related work self admitted technical debt satd is a variant of technical debt that is used to identify debt that is intentionally introduced during the software development process.
the detection of satd can better support software maintenance and ensure high software quality .
this paper aims to propose a deep learning based method to detect satds.
to the best of our knowledge there is very limited research on the use of deep learning technology for satd detection.
therefore we divide the related work into two main parts code comment analysis existing studies in selfadmitted technical debt.
.
code comment analysis code comments play an important role in software development.
the significance of comments is to enable developers to easily understand and manage codes .
code comments are usually written in natural language texts containing functional descriptions and task annotations aiming to assist project development.
code comments will have different characteristics due to differences in developers projects and programming languages.
many studies investigated the characteristics of code comments.
fluri et al.
investigated the level of developers adding comments or adapting comments when evolving code in three opensource systems.
as a result they found that when the relevant code changed the comment changes were usually made in the same version.
steidl et al.
and sun et al.
conducted a code comment quality analysis to improve the quality of code comments.
more specifically steidl et al.
provide a semi automated approach to conduct quantitative and qualitative evaluation of comment quality.
sun et al.
extended their work and provide a more accurate and comprehensive comment assessments and recommendations.
.
studies in self admitted technical debt currently many researchers have focused on proposing approaches to detect and manage technical debts.
indeed many empirical studies have applied on technical debts .
the concept of self admitted technical debt satd is proposed by potdar et al .
satd means that the debt is intentionally introduced and reported in code comments by developers.
they manually summarized specific patterns from different java projects for the detection of satd comments.
wehaibi et al.
examined the relationship between satds and software defects and found that satds caused software systems to consume more resources in the future rather than equating to software defects.
according to different characteristics of satd comments maldonado et al.
further classified satd into five types namely design debt defect debt document debt requirement debt and test debt.
in recent years researchers in the field of software engineering have made significant efforts to address the issue of satd detections .
huang et al.
proposed a text mining based method for satd detections.
in their work they leverage feature selectors to select useful features for classifier training and detect satd comments in the target project based on the results of the classifier votes from different source projects.
maldonado et al.
built a maximum entropy classifier for automatically identifying design and requirement satd comments based on natural language processing nlp technologies.
they conducted extensive experiments in open source projects and outperformed the current state of the art based on fixed keywords and phrases.
yan et al.
proposed a change level method for satd detections utilizing change features.
in addition they investigated the most important features diffusion that impact satd detections.
with the rapid development of deep learning ren et al.
exploited a deep learning based method as support of satd detections.
in their work they utilized a convolutional neural network cnn to automatically learn key features in satd comments.
they introduced a weighted loss function to deal with the issue of data 880detecting and explaining self admitted technical debts with attention based neural networks ase september virtual event australia table top satd patterns in each project extracted by our approach apache ant argouml columba emf hibernate x xx to do to do to do to do to do not needed fixme b etter b etter what is should be function need remove work a public setter model extent work why not this support fixme mo del element implement factor up into bug p erhaps mor e work replace with subprogressmonitor workaround hack fixme hack r evisit fix b etter b ehind addtrigger better genbaseimpl p erhaps should we not in uml workaround instead of fixme this comment hack use d currently fix r ender jedit jfreechart jmeter jruby squirrel hack to do to do to do to do w ork che ck hack fixme w ork to do defer argument checking... perhaps r equire pop data type fix fixme not used hack r ender workaround tw eak work b etter bug bug happ en appear bother hack to deal with xxx implement this fix fix follo w broken assert a value programming error perhaps w orkaround stupid cr osshair values bail out not allocatable allocator is this right look bad not trigger bug not very efficient better way imbalance.
in addition for the explainability of detection they designed a backtracking method to extract and highlight key phrases and patterns of satd comments to explain the prediction results.
conclusion the objective of this work is to automatically detect and explain self admitted technical debts in software projects allowing software developers to fix those issues in a timely manner so as to avoid long time maintenance efforts.
to fulfill this objective in this work we have provided an overview of code comment characteristics that make it challenging to automatically detect satds.
then we propose to the community a hybrid attention based method for satd detection named hatd which has been equipped with the flexibility to switch word embedding techniques based on project uniqueness and comment characteristics.
after that we experimentally demonstrate the efficiency of hatd by outperforming state of the art methods on benchmark datasets detecting satds in real world software projects and demonstrating the explainability of hatd in highlighting the core words or phrases justifying why a given satd is flagged as such.
acknowledgment this work was supported by the grands of the national natural science foundation of china nos.
u163620068 .