identifying software performance changes across variants and versions stefan m hlbauer leipzig university germanysven apel saarland university saarland informatics campus germanynorbert siegmund leipzig university germany abstract we address the problem of identifying performance changes in the evolution of configurable software systems.
finding optimal configurations and configuration options that influence perfor mance is already difficult but in the light of software evolution configuration dependentperformancechangesmaylurkinapotentially large number of different versions of the system.
in this work we combine two perspectives variability and time intoanovelperspective.weproposeanapproachtoidentifyconfiguration dependent performance changesretrospectively across thesoftwarevariantsandversionsofasoftwaresystem.inanutshell we iteratively sample pairs of configurations and versions andmeasuretherespectiveperformance whichweusetoupdatea modeloflikelihoodsforperformancechanges.pursuingasearch strategy with the goal of measuring selectively and incrementally further pairs we increase the accuracy of identified change points related to configuration options and interactions.
we have conducted a number of experiments both on controlled synthetic data sets as well as in real world scenarios with differ ent software systems.
our evaluation demonstrates that we canpinpoint performance shifts to individual configuration optionsand interactions as well as commits introducing change points withhighaccuracyandatscale.experimentsonthreereal world systems explore the effectiveness and practicality of our approach.
ccs concepts softwareanditsengineering softwareperformance software evolution.
keywords softwareperformance softwareevolution configurablesoftware systems machine learning active learning acm reference format stefanm hlbauer svenapel andnorbertsiegmund.
.identifyingsoftwareperformancechangesacrossvariantsandversions.in 35thieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
introduction softwareperformanceplaysacrucialroleinusers perceptionof software quality.
excessive execution times low throughput or otherwiseunexpectedperformancewithoutaddedvaluecanrender software systems unusable.
poorperformance is often a symptom of deficiencies in particular software components or the overall softwarearchitecture.changesintheobservedperformanceofa software system can be attributed to changes to the software at different levels of granularity architecture code etc.
.
typically modern software systems provide configuration options to enable users and admins customizing behavior to meet different user requirements.
configuration options usually correspond to pieces of selectable functionality features which contribute to overallperformancewithdifferentproportions.thatis different configurations of a software system exhibit different performance characteristicsdependingonconfigurationdecisionsmadebythe user.
performance changes during software evolution intended or not can affect all or only a subset of configurations since changes to a softwaresystemoftenrelatetoaparticularconfigurationoptionorsetofoptions.thisiswhyperformancebugsarerarelyvisibleinde faultconfigurations butrevealedonlyincertainconfigurations .
ifundetected suchperennialbugscanpersistforthe lifetimeofa softwaresystemassoftwareevolves.addingthis technicaldebt constantlycanaccumulateandentailtrendsofdegradingperformance quality .
performance assessment and estimation is a resource intense task withmanypossiblepitfallsprevailing.bestpracticesforconducting performancemeasurementsemphasizeadedicatedandseparated hardware setup to prevent measurement bias by side processes and subsequently obtain reproducibleresults .whatisoften overlooked is the fact that most software systems are configurable whichintroducesanotherlayerofcomplexity.duetocombinatorics even for a small number of configuration options the range ofpossible valid configurations renders exhaustive measurements infeasible.
ourgoalistoenablethe retrospective detectionofchangesinthe performance ofconfigurable softwaresystems andpinpoint them to a specific option or interaction.
for example a patch of a certainfeatureislikelytoaffectonlyconfigurationswheresuchfeature is selected.
we would like to know in which revision this patch was introduced and which features were affected.
many softwaresystemsareconfigurable buthavenoperformanceregression testingroutinesetinplace.touncoverperformancedeficiencies that emerged from revisions in the past development history itis infeasible to test each and every commit and configuration.
in essence wefaceacombinatorialexplosionalongtwodimensions 35th ieee acm international conference on automated software engineering ase time versions and configuration space variants .
first we aim at finding changes in software performance from one version to another.
the detection of such changes is referred to as change point detection .
the main limiting factor is that exhaustivemeasurementsacrosstheconfigurationspaceareneither available nor feasible .
change point detection techniques with both exhaustive measurement as well as limited data availability havebeensuccessfullyappliedtoidentify performancechanges.
second weaimatassociatingperformance changes fromoneversiontoanother withparticularconfigurationoptions orinteractions amongthem.
there issubstantial work regardingarelatedproblem whichisestimatingtheinfluenceof individualoptionsonperformance forasingleversion of the software system ignoring the temporaldimension.
instead ofestimatingtheinfluenceofoptionsandinteractionsonperformance wewanttoknow whichoptionorinteractionis responsible foraparticularchangepointintheversionhistoryofasoftware system?
our main idea is as follows we address the configuration complexityofthisproblembyselectingrepresentativesamplesetsof configurations.
then we uniformly sample a constant number ofcommitsforeachconfigurationandconductrespectiveperformance measurements.
based on these measurements we learn aprediction model.
for each configuration we estimate the likelihoodofeachcommitbeingachangepoint i.e.
thatperformance of some configurations changes abruptly compared to the previous commit .next weleveragesimilaritiesintheperformancehistories of configurations that share common options.
since we sample thecommitsforeachconfigurationindependently weobtainfor each change point many estimations using measurements of dif ferent commits.
overall this allows us to obtain more accurateestimations of performance changing commits with tractable effort.
based on a mapping from configurations to predicted change pointprobabilitiesforeachconfigurationandcommit wederive the options responsible for each particular change point which we callconfiguration dependent change points.
in summary we offer the following contributions a novel technique to effectively identify shifts in performance ofconfigurablesoftwaresystems.itisabletopinpointcausativecommitsandaffectedconfigurationoptionswithhighaccuracy and tractable effort.
afeasibilitydemonstrationofourapproachbyimplementing an adaptive learning strategy to obtain accurate estimations with acceptable measurement cost.
anevaluationusingbothsyntheticandreal worldperformance data from three configurable software systems.
synthetic data letus assess ourmodeland approach conceptuallyandatscale whereas we are able to assess practicality with real world data.
acompanionwebsite1providingsupplementarymaterialincluding a reference implementation of our approach performance measurement data and additional visualizations.
or an archivedversionat com ai se changepoints across variants and versions configuration dependent change points performanceasapropertyemergesfromavarietyoffactors.besides external factors including hardware setup or execution environ ment the configuration of a software system can influence performance to a large extent .
most modern software systems exhibitconfigurationoptionsthatcorrespondtoselectablepieces offunctionality.withconfigurationoptions weturnfeatureson and off creating a variant of the software system.
depending on the configuration different system variants with different behavior and performance can be derived.
.
performance influence models consider the following running example of a database manage ment system dbms with two selectable features encryption andcompression.eitherfeatureaddsexecutiontimetotheoverall performance but if both features are selected the execution time issmallerthanthesumoftheindividualfeatures contributionto performance less data is encrypted if it is compressed beforehand.
aninteraction in this setting is the combined effect of features encryption and compression.
we can assign each feature andinteraction an influence that it contributes to the overall performance if selected.
in our example the individual influences of the two features are positive increasing execution time whilst the interaction s influence is negative decreasing execution time .
the influence of features on performance can be described using a performance influence model .
a performance influence model is a linear prediction model of the form c r whereby cdenotes a configuration vector assignment of concrete values to configuration options and the estimate is a system variant s real valued performance c tc ... f t c ... c f i r2 f ci f fdenotesthesetofallfeatures.ourlinearmodelhas 2f terms whereby each term t fcorresponds to a subset of fand is described by a coefficient fand a configuration parameter cf.
coefficientsencodetheperformance influenceoffeaturesandinteractions.
cis a vector and denotes the assignment of concrete values to configuration options or interactions.
a configuration parameter ctevaluatesto1 ifallconfigurationoptionsofthecorrespondingsubset t fareselected and0otherwise.theemptyset correspondstotheinvariablecorefunctionalityofthesoftwaresystem.singletonsubsetscorrespondtoindividualfeatures compound subsets to interactions of features.
forourdbmsexample wepresentafullydeterminedperformanceinfluencemodelinequation2.thefirsttwotermscorrespondto thefeaturesencryptionandcompression respectively.thethird term represents the interaction of both features.
the configuration parameter ccompr encrypt evaluates to if both cencryptand ccomprevaluate to .
that is ccompr encrypt cencrypt ccompr.
in the last term we omit the configuration parameter c as this term represents invariable functionality.
612 dbms c encrypt cencrypt compr ccompr compr encrypt ccompr encrypt prediction models of configuration dependent performance of this formarenottheonlypossiblerepresentation butlinearmodelsareeasytointerpret .wereviewextensionsandalternatives to linear performance influence models in section .
.
evolution of performance influences performance influence models describe how features contribute to performance buttheydonotallowforunderstandingconfigurationdependent performance changes over time.
we expand our dbms example with a temporal dimension considering a development historyofhundredcommits.inparticular weassumethefollowing three events.
at commit feature compression has been modified increasing the execution time feature encryption is introduced at commit and can be combined with compression.
atcommit50 theinteractionof compressionandencryption changes resultinginanincreasedexecutiontime.
atcommit80 the core functionality of the dbms is refactored which decreased execution time for all variants.
figure performance of variants with change points the performance histories of the four valid variants in figure exhibit three change points commits and .
these change points however d onotaffectallconfigurations.forinstance execution time decreases for all configurations at commit but increasesatcommit50onlyforoneconfiguration.giventhisexample the target outcomes of our approach are the locationsof the three change points commits and the asso ciation of such commits to the features and interactions compr compr encrypt and the invariable functionality respectively.
.
taming complexity a naive approach for identifying change points in the evolution of configurable software systems is to simply combine existing work on performance modeling of commit histories and performance prediction of software configurations.
for our example we could measure all variants for each commit buildingaperformance influencemodelpercommit.having4con figurationsand100commits thiswouldresultin400measurements.
sincethe numberof configurationsgrows exponentiallywiththe number of features we end up with 2ntimestmeasurements wherenis the number of features and tis the number of commits.
clearly this does not scale along both dimensions already a few features would render even small commit histories intractable commitsvanillacompressionencryptioncompr.
encr.. con guration speci c change points?
exploration new con gurations exploitation new con gurationsexploration new commits exploitation new commits existing measurements figure active sampling strategies for a dbms with con figurations and even small configuration spaces make this approach infeasible given realistic commit histories of a few thousands of commits.
toobtainaccurateestimationswithalimitedbudgetofmeasurements werequireadifferentapproach.weproposeaniterativeand adaptive sampling approach.
that is we use an initial but small samplesettoexploretheproblemspaceandthenincreasethelevel ofgranularityatpromisingregionsanddimensions development history segments as well as individual features and interactions in our case .
basedontheintroductorydbmsexample cf.figure1 weillustrate our sampling strategy in figure .
here each box depicts a possible measurement i.e.
a pair of a configuration and a commit .
blackbarsrepresentchangepointshiddeninthesoftwaresystem s performance histories i.e.
measurements immediatelybefore and afterachangepointaredissimilar .thecurrentstateofthesample set comprises all measurements that are filled in grey.
to include new measurements into the sample set we consider the followingsituations.ontheonehand oursamplesetmightalready hinttosomepossiblechangepointsandassociatedconfiguration options.
on the other hand our sample set might be too sparse such that we cannot infer somechange points yet.
to address this trade off we devise two strategies exploration and exploitation both of which address both configurations and commits.
forexploitation wesamplenewmeasurementstoverifyguesses basedonthecurrentstate.thatis infigure2 wemightincludethe measurements depicted by black filled symbols .
black filled stars representmeasurementsthatareofinterestbecausewe have already identified one change point for configuration compression.weexploitthisknowledgetotestfurtherconfiguration optionsinvolvedinthischangepoint.theblack filledpentagons representmeasurementsthatareinteresting becausewehaveidentified another early change point for compression but within a rather broad range of commits.
thus we include further measure ments from that range to narrow down the possible range for this particular change point.
forexploration we might include the measurements denoted by white symbols .
white stars represent measurements that explore new configurations.
in our example the interaction be tween compression and encryption is the only configurationleft to be measured.
we include several commits of a particularnew configuration to unveil possible performance variation thatindicates possible change points.
white pentagons represent 613measurementsthatareofinterestbecausetheyincreasetheoverall measurementcoverage.weincludemeasurementsfromlargeintervals of not yet measured commits since possible change points can be hidden there.
thesefourstrategies explorationandexploitationofconfigurations and commits prioritize measurements to be included next into the sampleset.adaptivesamplingtechniqueshavebeensuccessfully applied to obtain both performance influence models and performancehistories before.
however itis unclearwhether fewermeasurementsaresufficienttoassesstheperformance influence ofconfigurationoptionsandinteractionswithrespecttoversion changes.
this is what we address in this work.
an algorithm for change point detection weproposeanalgorithmtodetectsubstantialshiftsintheperformance of software configurations and associate them to individual commitsandoptionsorinteractions.welayoutourapproachasan iterative search across the commit history and configuration space.
we provide an overview of our approach in figure .
it starts with a small initial sample set of measurements i.e.
performance observations of varying configurations and varying commits .
based on thissampleset itcalculatesforeachconfigurationthelikelihood of each commit being a change point.
subsequently ouralgorithm estimatesa candidatesolution which is a set of pairs of a commit and a configuration option.
each of such pairsdescribes the estimatedinvolvement of aconfiguration option in the shift of performance.
interactions are conceived as multiple tuples with identical commits.
that is if a commit occurs inmultipletuples eachwithdifferentconfigurationoptions this indicates that the shift arises from an interaction among two or moreoptions.henceforth wewillrefertosuchpairsas associations.
afterobtainingonecandidatesolutionpersearchiteration weaugmentthesamplesetofmeasurementswithregardtotwoobjectives explorationandexploitation.
exploration aimsatincludingpreviouslyunseencommitsandconfigurationstoimprovecoverageof thesearchspace.
exploitation aimsatincludingmeasurementsin the sample set that based on the previous candidate solution may increaseconfidenceinassociationsorruleoutfalsepositives i.e.
commits falsely identified as change points or options falsely associatedwithacommit .asforexploitation wemakeaninformed decisionofwhichmeasurementsaretobeincluded whereasexploration is agnostic of previous candidate solutions.
aseachiterationyieldsacandidatesolution wekeeptrackofassociationsina solutioncache andrepeatedlyupdatetheconfidence of associations.
the rationale is not to lose previously identified change points but at the same time allow for removing identified changespointsthatarelikelyfalsepositivesduetonewmeasurements.
some associations especially in the beginning might be influenced by sampling bias and can be removed if successive iterationsdonotrepeatedlyrevisittheseassociations.thealgorithm terminatesifthesolutioncachedoesnotchangeforanumberof iterationsoramaximumnumberofiterations measurementshas beenreached.inwhatfollows wepresentthestepsofourapproach in detail.
initial sampling compute change point likelihoods for each configuration associate change points and con figuration options acquire new measurementsacquire commits...acquire configurations... update solution cache check stoppage criteria figure overview of our approach circlecopyrtselecting training data across configuration space and version history circlecopyrtestimatingthechangepointprobabilitydistributionperconfig uration circlecopyrtestimatingchangepointsascandidatesolutions circlecopyrtadaptively augmenting the training set and circlecopyrtupdating the solution cache and evaluating termination criteria.
.
initialization and sampling the first step of our approach is to select a sample set of performancemeasurements.weselectarelativelysmall butfixednumber of configurations nconfigurations .
for each configuration we select a fixed percentage of commits rcommits and assess their respective performance.theinitialsamplesetinthissetupiskeptsmalland may not represent all relationships between configuration options andperformanceevolution.therationaleisthat tomakeourapproachmorescalable weexplorethecombinedsearchspaceand refinethetemporalandspatial i.e.
configuration related resolution where necessary.
for the initial configuration sampling we use distance based sampling which is a form of random sampling that strives for uniformcoverageatlowcost.ingeneral uniformrandomsamplingofconfigurationsisconsideredtoyieldthemostrepresentativecoverage of a configuration space but it is prohibitively expensive for real world configurable software systems with constraints among options i.e.
notallcombinationsofconfigurationoptionsarevalid configurations .distance basedsamplingaddressesthisproblem by demandingthe number ofselected optionsto be uniformlydistributed to avoid local concentration.
thekeyideaofouralgorithmtoiterativelyaugmentthetrainingset addressestwoissues theconfigurationspaceexhibitsexponential complexity.
interactions of higher degrees i.e.
interactions involving two or more configuration options are possible but rela tivelyrareamongconfigurablesoftwaresystems .therefore insteadofexhaustivesamplingwithrespectinteractiondegrees weiteratively add new configurations to our training sample to search previously undetected influences of options and interactions.
for each configuration in our sample set the algorithm selects a smallnumberofcommits e.g.
oneortwopercentofallcommits forwhichitmeasuresperformance.therationaleofhavingonly few commits is that given a relatively large number of configu rations many similar configurations will exhibit change pointsof the same cause.
we mitigate the poor temporal resolution by selectingthecommitsindependently.compared toafixedsample ofcommitsacrossallconfigurations thisway eachcommitismorelikelytobemeasured atleast once.thatis weobtainchange point estimationsforrelatedconfigurationsfromindependenttraining 614samples.
for instance consider the third change point in the introductory dbms example at commit all configurations exhibit a performance change.
if we sampled two commits and for all configurations allthatwewouldlearnisthatthereisachangepoint somewherebetweenthesetwoversions.instead ourapproachsamples the commits and for the first two configurations and commits and as well as and for the remaining two con figurations respectively.
our best guess then is to assume a change point between commits and since all measurements agree withthisconclusion.thisway weincreasethetemporalresolution whilekeepingtheoverallnumberofperformancemeasurements manageable.
.
iteration change point likelihoods for each configuration in our sample set we estimate the probabilityofeachcommitbeingachangepointforthecorresponding configuration.
to this end we need to define what counts as a performancechange.weuseauser definedthreshold whichdiscriminates between measurement noise and performance changes such that different application scenarios as well as system specific peculiaritiescanbeaccountedfor.iftheperformancedifferencefor aconfigurationbetweentwocommitsexceedsthisthreshold we count this difference as a performance change.
although manually defined there are several possibilities to estimate this threshold automatically.
prior to learning the measurement variation obtained bytherepeatingmeasurementsforthesameconfigurationmultiple times can be estimatedand employed as a minimum threshold.
in addition arelativeorabsolutethresholdcanbederivedfromthe applicationcontext suchasatenpercentortensecondsincrease in execution time.
we encode the threshold in a step function a b braceleftbigg a b a b a b v r the function evaluates to if the difference between performance aand bof commits aandbexceeds the threshold and if not.
given a pair of commits we can now decide whether performance haschangedsomewherebetweenthetwocommits.however not eachpairofcommitsisequallyinformative.thefartherthedistance between two commits the lesser the information we can obtain as theremightbeseveralchangepointsinbetween.inaddition the effect of one change point between two commits can be shadowed by another change point in the opposite direction such as that one changepoint increasestheexecution timeanda seconddecreases theexecutiontimeagain.wedefinetheinfluenceofeachpaironourestimationbyweighingeachpairinverselyproportionallyto the distance between two commits.
p prime v summationdisplay.
a v a v summationdisplay.
b v b v a b bracehtipupleft bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehtipupright step function a b bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehtipupright weighting term p v p prime v summationtext.1n i 1p prime i for a given commit v v we can now estimate a change point probability by comparing each pair of commits before and after v. this is illustrated in equation where p prime v is the sum of the0 time0.
.
.
.006p v all versions appr o ximation sample measur ements ti m e246p erformance figure performance history with commits and two change points bottom ground truth and approximatedchange point likelihood p v top influence times the performance change indicator for each pair of commits.
in practice however measuring all commits vis undesirable.
therefore we sample a small number of commits t v instead and compare each pair of commits before and after vto obtain an approximation of p prime v .
last to obtain a proper probability distribution weneed tonormalize eachvalue p prime v asillustrated inequation5.consequently weobtainanapproximation p v that represents a probability distribution with v 0p v dv .
the resultingprobabilitydistributionaswellasitsapproximationare illustratedinfigure4 whereeachchangepointcorrespondstoa peak in the probability distribution.
.
iteration assembling a candidate solution we now have change point likelihood estimations for all configurations in our sample set.
different configuration options and interactions contribute to this change point likelihood as we have seen in the introductory dbms example.
in the following step we estimate the coordinates pair of commit and configuration option of likely change points.
so we first estimate candidate commits based on the change point likelihood from the previous step.
then we associate these candidate commits with configuration options.
.
.
candidate commits.
for each configuration we compute an approximation of the change point likelihood over commits cf.
figure to identify local maxima peaks .
we select such peaks undertheconditionthatthepeakchangepointlikelihoodisgreater than a threshold tcpl v ncpl radicalbigg v summationdisplay.
v v parenleftbig1 v p prime v parenrightbig2 bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipupright standard deviation of p prime v over all commits thethreshold tcplistheaveragechangepointlikelihoodoverall commits plus a factor ncpltimes its corresponding standard deviation over all commits.
the factor ncpl default value allows 615usto filterpeaksthat donotstand outenoughand mightbefalse positives.
the greater ncplis the stricter the filtering of peaks.
that is for each configuration in our learning set we obtain a setofcommits configuration dependentcandidatecommits that representpossiblechangepoints.toreducevariationamongthe obtainedcommits weclusterthecommitsusingkerneldensityestimation kde .thepurposeofkdeinoursettingistoestimatethe probabilitymassfunctionofwhetheracommitisachangepoint.
thelocalmaximaofthiskde subsequently representoursetof candidate commits.
note that if two distinct change points are almost coincident commit wise they can be mistakenly identified by our approach as one change point that subsequently can be associatedwithconfigurationoptionsbelongingtothetwoindividual change points.
.
.
associatingcommitsandoptions.
foreachcandidatecommit that we obtain we want to know which configuration op tions are most likely responsible for the respective peak.
hence we estimate the influence of each configuration option on the change point likelihood.
the core idea is to train a linear model m noptions in which each configuration option s coefficient corresponds to its influence.
instead of an ordinary linear regression model we use a linear model that implements the l1norm for regularization lasso .
in addition to the least squares penalty this technique favors solutions with more parameters coefficients set to zero.
this technique is commonly used to help prevent over fitting decrease model complexity andeliminatenon influentialmodelparameters configuration options in our case .
the effect of l1regularization inamodelisspecifiedbyanadditionalhyper parameter .wetune this hyper parameter using threefold cross validation.
for each model for a candidate commit we consider aconfiguration option as associated with a commit if its influence ci i.e.
the absolute value of its coefficient is greater than a threshold tinfluence noptions ninfluence radicalbigg noptions summationdisplay.
parenleftbig1 noptions ci parenrightbig2 bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipupright standard deviation of ci we consider options as influential if their estimated influence is greaterthanifeachconfigurationoptionwereequallyinfluential.
in addition weadd to thisthreshold the standard deviationof the influences of all configuration options.
in theory our regressionmodel eliminates non influential configuration options.
if more thanconfigurationoptionisinfluential i.e.
twoormoreoptions interact their respective estimated influence should be roughly equal.toavoidfalsepositivesinheestimationofinfluentialconfigurationoptions weusethestandarddeviationandtheparameter ninfluence default value to filter only estimations with low variationamongtheestimatedinfluences.iftheinterceptofamodel i.e.
the coefficient not related to any regression model variable exceedsthisthreshold weconsiderthiscandidatecommitachange point thatis notconfiguration dependent i.e.
it affects allconfigurations .theoutcomeisalistof associations pairsofcandidate commits and likely influential configuration options.
.
iteration acquiring new measurements the last step in each iteration is the acquisition of new measurements.
we extend the existing sample with both new commitsfor configurations already sampled as well for an additional set ofnewconfigurations.theroleofincludingnewdataistwofold.
first previouslyassessedconfigurationsorcommitsmightnothave capturedunseenperformanceshifts.therefore aportionofnew dataisacquiredwithoutfurtherknowledge exploration .second a candidate solution might over or under approximate associations andcancontributetothealgorithm soverallestimation.therefore samplingthesecondportionofdataisguidedbyexploitingeach iteration s candidate solution exploitation .
.
.
acquiringcommits.
theexplorationofnewcommitsfollows asimplerule foreachconfiguration wesampleanumbercommits that exhibit the maximum distance to already sampled commits.
theexploitationofaniteration scandidatesolutionemploysthe estimation of a configuration s change point likelihood cf.
section3.
.werandomlysampleamongthosecommitsforwhichthe change point likelihood indicates a possible change point but is notconfident.indetail weselectanumberofcommitsforwhichthechangepointlikelihoodisgreaterthantheaverage 1divided by thenumber of commits butsmaller than the averageplus the standard deviation of change point likelihood over all commits.
by sampling in this range of commits we incorporate existing knowledge above average likelihood but control over fitting by only sampling commits with maximum likelihood.
.
.
acquiringconfigurations.
theexplorationofnewconfigurationsissimilar totheinitialsampleselectionstrategy distancebased sampling described in section .
.
we select a constant number of configurations default value for exploration.
the exploitation part of acquiring new configurations is guided by the current set of candidate solutions.
each candidate solution describes a change point and associated configuration options.
these associated options can be a correct assignment or be an over or under approximation.inthelattercases toomanyortoofewoptions are associated with a change point.
we exploit the existingcandidate solutions in a way that addresses both under as well asover approximation.weselectnewconfigurationsusingaconstraintsolverandthereforecanspecifyadditionalconstraints.in thecaseofunder approximation toofewrelevantconfigurationop tions are associated with a change point.
given a candidate change point we require that all associated options keep enabled and that ofallnot associatedoptionscanbeselected.thisway wekeep already associated options but allow new configuration optionsto be included.
likewise in the case of over approximation too manyirrelevantconfigurationoptionsareassociatedwithachange point.
given a candidate change point we require that variation only occurs among of the options associated with the change point.thatis wecanremoveupto50 ofconfigurationoptions and narrow down the selection of relevant configuration options.
we limit the total number of new measurements per iteration with a budgetnmeasurements .
this budget is split between acquisition for commitsandconfigurationswithafactor ncommits to configs .
.
the measurement budget for commit acquisition is further split between exploration and exploitation with a factor ncommits explore .
.thatis initially ofthebudgetareused for configuration acquisitionand25 forcommitexplorationandexploitation respectively.
as the algorithm proceeds we want to shift the budget from an equal split between exploration and exploitation towards exploitation.therationaleisthatinearlyiterations wefocuson identifyingchangepointsintheconfigurations performancehistories whereas in later iterations our approach focuses on pinpointing configuration options to change points.
therefore we multiply the factor ncommits to configs by .
and the factor ncommits explore by .
.
that is the algorithm shifts towards sampling configura tionsindepthand focusesonexploitationinlateriterations.the rationale is that we consider pinpointing commit to configuration options a more difficult task than finding performance changing commits.
.
solution cache and stoppage criteria after each iteration we insert the candidate solution in a solution cache.this solutioncache isamappingofassociationstoweights indicatingadegreeofconfidence.therationaleisthat ifanassociation isincluded repeatedly inan iteration s candidatesolution it islikelyatruepositive i.e.
atruechangepoint .bycontrast an associationthatisincludedonlyafewtimesislikelyafalsepositive and can be discarded.
we update the solution cache after each iteration in three steps.
first all associations that are either newly included orhave been seen beforehave their weight increasedby a constant factor wincrease .
our default value for kis set to .
second the weights of all associations in the solution cache aremultiplied by a constant decrease factor edecrease .w es e t thedefaultvaluefor edecreaseto0.
.last weremoveallassociations from the solutioncache if theirweight is smaller thana threshold tdrop.
we define tdropas the weight an association exhibits if it is included once in a candidate solution but not in k nsuccessive iterations.
effectively value for tdropis edecrease ksince the increment wincreaseis .
similarly to the conditions for dropping an association from the solutioncache thealgorithmterminatesifnoassociationisdroppedfromthesolutioncachefor kiterationsinarowandallassociation s weights are greater than .
as a fallback termination criterion the algorithm alsoterminates ifa user specifiedmaximum numberofmeasurements m maxor number of iterations imaxis reached.
evaluation when evaluating our approach with a single experiment we face a conflict between internal and external validity a problem that is prevalent in software engineering research .
to assure internal validity theassessmentofourapproachwithrespecttoaccuracy requires prior knowledge of change points as ground truth which is hardly obtainable as this would require exhaustive performance measurements across commits and configurations.
moreover to assureexternalvalidity werequireagreatdegreeofvariationamong subject systems e.g.
number of commits and options domains of subjectsystems etc.
tolearnaboutscalabilityandsensitivity.so for a fair assessment we require not only a large set of systems butalsotherespectivegroundtruthperformancemeasurements.the caveat is that it is practically impossible to conduct such exhaustiveperformancemeasurementsinthelarge whichwasthe main reason for proposing our approach in the first place.
toaddressthisdilemmainourevaluation weconducttwoseparateexperiments basedonsyntheticandreal worldperformance measurements.thefirstsetofexperimentsusessynthesizedperformance data providing a controlled experimental setup to assess scalability accuracy andefficiencyatlowcostwhilesimultaneouslybeingabletosimulatedifferentscenariosbyvaryingthenumberof change points and affected configurations in the synthesized data.
thesecondsettofexperimentsusesabatchofreal worldperformance measurements of three software systems as a necessarily incomplete ground truth to explore whether our algorithm can be practically applied to real world systems.
withthissplitexperimentsetup weaimatansweringthefollowing two research questions rq1 canweaccurately andefficiently identifyconfiguration dependent performance change points?
rq2 can wepractically identify configuration dependent change pointsin a real world setting ?
.
controlled experiment setup we break down the research question rq1into three objectives to study the influence of the size of configurable software systems change point properties and measurement effort.
.
.
influenceofsystemsize.
weareinterestedinhowthesize ofaconfigurablesoftwaresysteminfluencestheaccuracyandefficiencyofourapproach.toanswerthisquestion wesynthesize performancedataforsystemsofvaryingsizeintermsofnumberof configuration options and the number of commits.
for the number of configurations noptions we selected a range that resembles configurable software systems studied in previous work likewise we selected arange forthe number ofcommits ncommitstocover young as well asmature software systems.
we present the ranges of the two size parameters in table .
.
.
influence of change point properties.
a change point may correspond to a single option or an interaction among multiple options.furthermore twochangepointsmightbeonlyafewormanycommitsapart.therefore forthesynthesizedsoftwaresystems we varyboththetotalnumberofchangepointsaswellasthedegreeof interactionsthatasoftwaresystemcontains.thenumberofchange points ranges from one to ten reflecting findings of a recent study about performance change points .
we sample the degree of interactions from a geometric distribution the discrete form of an exponential distribution which is specified by a single parameter pbetween and henceforth called pinteraction.
the greater the valueofpinteraction thelesslikelywegeneratehigher orderinteractions.
the rationale of this setting stems from previous empirical work that has shown that by far most performance issues arerelatedtoonlysingleoptions orinteractionsoflowdegree.we present the specific ranges of the two parameters in table .
.
.
influenceofmeasurementeffort.
wewanttounderstandhow the invested measurement effort affects accuracy and efficiency.
aninitialsamplesetchosentoosmallorlargemightfailtocover change pointsto exploitor wastemeasurement effort.
in addition 617table parameter ranges for the synthetic experiment.
parameter range synthesized systemsnoptions ncommits nchangepoints pinteraction .
.
.
initializationninitial nmeasurements thenumberofmeasurementsperiterationcanbeselectedirrespectiveofthesoftwaresystemsize resultingintoofewmeasurements per configuration and thus failing to identify change points.
that is fortheinitializationofouralgorithm wevaryboththeinitial number of configurations and the number of measurements per iteration.weselectthenumberofconfigurationsintheinitialsamplesetas nchangepoints timesthenumberofconfigurationoptions and the number of measurements per iteration from a range of threevalues.wefixthepercentageofcommitsperconfiguration at3percent whichhasbeenapromisingsamplingrateinprevious work .
we present the ranges of the two parameters in table .
.
.
operationalization.
we synthesize performance data by initializing each option with a randomly selected influence cf.
coefficients from equation from the range .
in addition we randomly select a number of interactions among options to introduce interactionsofvaryingdegrees.theinteractiondegree i.e.
number of selected configuration options follows a geometric distribution.
we assign to each interaction a real valued influence uniformly fromthe range .
we definesix parameterranges cf.table for the number of configuration options noptions the number of commits ncommits the number of change points nchangepoints the parameter for the geometric distribution of interaction degrees pinteraction afactor ninitial andthenumberofmeasurementsper iterationnmeasurements .
the initial number of configurations is the product of noptionsandninitial.
we construct the cartesian product ofallparameterrangesandspecifyamaximumnumberof30iterations.inaddition wesynthesizeforeachparametercombination in the parameter grid five different software systems by employingdifferentseedssuchthatnoseedisusedtwice.thetotalnumberof experiments with seeds is .
for each parameter combination we record the number of measurementsateachiteration therequirednumberofiterationsfor termination as well as each iteration s candidate solution.
to assesstheaccuracyofaparametercombination soutcome aswell asofintermediateiterations weusethe f1score acombination ofprecision andrecall.
precision refersto thefractionof correctly identifiedassociations truepositives amongtheassociationsoftheretrieved candidate solution.
recallisthefractionoftotalnumber of the relevant associations i.e.
those we intend to find .
the f1 score is the harmonic mean of precision p and recall r defined asf1 p r p r. in our context we defined a correctly identified changepoint whena commitsfalls ina narrow5 commitinterval fromthegroundtruth.toassessefficiency weemploytherequired numberofiterationsforterminationandrequiredmeasurementstable project characteristics of subject systems.
name options commits xz lrzip oggenc in relation to the f1score as proxy metrics.
the idea is that we consider an experiment run as efficient if it provides an accurate change point estimation with few iterations or few measurements.
.
real world experiment setup inthesecondexperiment weevaluateourapproachfromapractitioner s perspective where the properties and whereabouts ofchange points are unknown.
by means of three real world con figurable software systems we investigate in particular practi cal challenges and check whether our algorithm is able to detectperformance relevant commits with respect to configuration options.
.
.
exploratorypre study.
ofcourse wecannotobtaincomplete ground truth data of our selected subject systems since the search space is exponential in the number of configuration options.
however toprovidesomecontextforinterpretationofourresults we measuredperformanceforarepresentativesubsetofconfigurations across all commits.
to be precise we sampled configurations using feature wise negative feature wise pair wise and also uniform random sampling strategies which have been successfully applied to learnperformance influencesbefore .
we sampledas many randomconfigurationsastherearevalidconfigurationssampled with pairwise sampling which amounts to configurations for lrzip for oggenc and for xz.
an overview of the three software systems is given in table .
as a workload for the file compression tools lrzip and xz we used the silesia corpus which contains over mb of files of different types.itwasdesignedtocomparedifferentfilecompressionalgorithmsandtoolsandhasbeenusedinpreviousstudies .forthe audio transcoder oggenc we encoded a raw wave audio file of over mb from the wikimedia commons collection.
for all three subject systems we assess performance by reporting the execution time.
all measurements were conducted on clusters of ubuntu machines with intel core quad cpus .
ghz and gb of ram xz and lrzip and gb of ram oggenc .
to mitigate measurement bias we repeatedeachmeasurement fivetimes.the coefficientof variation the ratio of standard deviation and the arithmetic mean across allmachines waswell belowten percent.for commits that did not build we reported the performance measurement of the most recent commit that did not fail to build.
.
.
operationalization.
for the actual experiment we apply our approachontheperformancemeasurementsobtainedintheprestudy with a few adjustments.
first although our study provides a broadandrepresentativesamplesetofconfigurations wecannot arbitrarilysamplevalidconfigurationsforacoupleofreasons.in particular theseincludethecommitsthatdonotbuildaswellas 618hidden configuration options and constraints.
while we collected configuration options and constraints thoroughly from documentationartifacts wecannotassurethatourselectioniscomplete.we discuss this limitation further in section .
instead when acquiring new configurations cf.
section .
we selectconfigurationsfromourbatchofmeasurementsthat have not been used already by our algorithm and are closest to the requested configuration i.e.
with the minimal hamming distance .
the rationale is that this allows us to quickly run rapid repetitions with different initializations of our algorithm.
since we employ multiple sampling strategies in our pre study we are confident that our broad batch of measurements is representative.
second we set the number of measurements per iteration nmeasurements to .third asaninitialsampleset werandomlysample5configurations from our pre study set.
last we initialize our approach with a relative performance threshold cf.
equation of ten percent.inthecontextofthereportedrelativevariationamongperformance measurements on the machines used we consider this a rather conservative threshold.
we repeat the experiments times with different seeds to quantify the robustness of our approach andtoaccountfortherandomnessinexploringtheconfiguration space.weassesspracticalitybyreportingmeasurementeffort number of iterations in relation to the pre study s results which arebased on a vastly greater measurement budget.
we qualitativelyinvestigate whether the detected commits and options are actu ally causes of performance changes.
that is we analyze commit messages of the respective repositories for the identified commits tofindoccurrencesofoptionnamesorindicatorsofperformance changes.moreover welookedintothecodechanges e.g.
whenthe commitmessagejuststates merge torationalizeaboutpossible performance affecting code changes.
.
results .
.1rq1 controlledexperiment .
weillustratetheresultsofour first set of experiments in figures 5a 5b and 5c.
the vast majorityofexperimentsterminatedwithinthelimitof30iterations as showninfigure5b.asmallportionofexperiments however did not meet our termination criteria more on that below .
from allexperiments that terminated in figure 5a we depict the f1 scoreafter they have terminated that is after their last iteration .
for most iterations the mean f1 score falls around or over .
with thefirstquartileonlyslightlybeingaslowas0.
atiteration29 .
in the grand scheme of things the vast majority of experiments terminatedwithreasonablyhighf1score.regardingmeasurement effort for small systems with configuration options we required upto50 ofallpossiblemeasurements.forgreatersystems the requiredmeasurementeffortwaswellbelow1 .thehighreported measurement effort for smaller systems is due to our choice of the measurementsper iterations which vastlyover approximates.
since we are able handle systems with more configuration options with similar effort we are confident that a smaller number of measurements per iteration would reduce the relative measurement effort required substantially.
2duetospacelimitations wereportprecisionandrecallonthepaper scompanion web site.
a f1 score of experiments terminating at different iterations.
iterations required for termination02004006008001000 b frequency of experiments terminating at different iterations.
c influenceofthenumberofchangepointsandinteractiondegree on accuracy.
figure result for the synthetic experiment.
in figure 5c we decompose the reported f1 scores to learn howthe number of change points or the interaction degree influence ourapproach.thex axisshowsthehighestinteractiondegreeof changepointsforanexperimentrun.forinteractionsofadegree up to the f1 score ismostly above .
.
we observe adownward trend in the box plots the more change points a system contained the less accurate our algorithm s prediction is.
fortheexperimentsnotterminatingwithinlimit therightmostbar infigure5b weconductedanadditionalanalysis.wecompared whetherandhowtheparametersettingoftable1explainsthisnontermination.onesettingstandsoutasthecausefornotfinishingthe experiment within iterations the number of measurements per iterations.
the lower the number of measurements nmeasurements the more iterations our approach needs.
summary rq1 we are able to identify and pinpoint configuration dependent performance change points accurately and atscale.thenumberofmeasurementsperiterationisthemain factor influencing how fast our algorithm terminates.
.
.2rq2 pre study .
for our three subject systems we have manually identified a number of commits for which performance changed substantially.
we have found change points for lrzip for xz and for oggenc.
for lrzip of and for xz of performance changes affect multiple configurations.
by contrast changepointsforlrzipaffectallmeasuredconfigurations.allofthe measuredconfigurationsforoggencshowashiftinperformance.
that is the identified change points for oggenc as well as the two for lrzip are likely not configuration dependent.
to further understand possible relations of change points with configuration options wesearchedincommitmessagesforclues.wediscusstwo notable findings.
for xz the commit messages for both change points referenced three particular configuration options hc3 hc4 matchfinder .
for lrzip one commit message