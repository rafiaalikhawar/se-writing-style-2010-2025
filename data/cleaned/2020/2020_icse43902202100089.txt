champ characterizing undesired app behaviors from user comments based on market policies yangyu hu1 haoyu wang2 tiantong ji3 xusheng xiao3 xiapu luo4 peng gao5and yao guo6 1chongqing university of posts and telecommunications chongqing china 2beijing university of posts and telecommunications beijing china 3case western reserve university usa4the hong kong polytechnic university hong kong china 5university of california berkeley usa6peking university beijing china abstract millions of mobile apps have been available through various app markets.
although most app markets have enforced a number of automated or even manual mechanisms to vet each app before it is released to the market thousands of low quality apps still exist in different markets some of which violate the explicitly specified market policies.
in order to identify these violations accurately and timely we resort to user comments which can form an immediate feedback for app market maintainers to identify undesired behaviors that violate market policies including security related user concerns.
specifically we present the first large scale study to detect and characterize the correlations between user comments and market policies.
first we propose champ an approach that adopts text mining and natural language processing nlp techniques to extract semantic rules through a semi automated process and classifies comments into pre defined types of undesired behaviors that violate market policies.
our evaluation on real world user comments shows that it achieves both high precision and recall in classifying comments for undesired behaviors.
then we curate a large scale comment dataset over million user comments from apps in google play and popular alternative android app markets and apply champ to understand the characteristics of undesired behavior comments in the wild.
the results confirm our speculation that user comments can be used to pinpoint suspicious apps that violate policies declared by app markets.
the study also reveals that policy violations are widespread in many app markets despite their extensive vetting efforts.
champ can be a whistle blower that assigns policy violation scores and identifies most informative comments for apps.
index terms user comment app market undesired behavior i. i ntroduction although the mobile app ecosystem has seen explosive growth in recent years app quality remains a major issue across app markets .
on the one hand it is reported that millions of android malicious apps were identified every year using more and more complex and sophisticated malicious payloads and evasion techniques .
on the other hand a large number of fraudulent and gray behaviors e.g.
ad fraud were found in the mobile app ecosystem from time to time .
furthermore apps with functionality performance issues such as diehard apps and devious contents such as anti society contents still remain in the markets .
the first two authors contributed equally to this work.
prof. haoyu wang is the corresponding author haoyuwang bupt.edu.cn .
package name com.beetteer.signal.booster app name signal booster store tencent myappexample comments too many ads.
once the app is started the notification bar is full of ads.
time dnt download it it is a virus it crashed my phone !
!!!!
time fig.
.
an example of user perceived undesired behavior.
most app markets have released strict developer policies along with inspection and vetting processes before app publishing seeking to nip the aforementioned threats in the bud and improve app quality in the markets.
for example google play has released a set of developer policies that cover main categories including privacy security and deception spam and minimum functionality and monetization and ads etc.
each category stands for a type of violation that may be associated with various undesired behaviors.
apps that break these policies should not be published on google play.
however it is challenging to automatically check policy compliance for mobile apps.
despite google play s efforts in adopting strict vetting processes by using automated tools malware and potentially harmful apps phas are recurrently found in google play .
third party app markets also show a significantly higher prevalence of malware fake and cloned apps .
on the one hand it has been reported that many malicious apps use sophisticated techniques to evade automated detection .
for example certain malicious behaviors could only be triggered at a specific time or environment such as checking whether the app is being inspected in emulating environments .
on the other hand even if malware can be detected by these automated tools many other fraudulent and gray behaviors such as ad fraud and malicious push notifications are hard to identify.
moreover functionality performance issues are typically app specific while devious contents are broad and difficult to detect without human inspection posing more challenges for automated tools .
in many cases whether an app s behavior has exposed severe security risks or performance issues depends on how users think of it .
as an important process for developers ieee acm 43rd international conference on software engineering icse .
ieee to improve app quality app markets allow users to leave their ratings and comments after downloading and using each app .
these comments can be considered as the direct feedback from users who have experienced the apps helping developers address the issues that might not have been spotted in testing.
for example as shown in figure two users gave star ratings for the app.
one user complained that this app contains aggressive advertising behaviors and the other even reported that this app might be malicious.
in fact this behavior is also one of the undesired behaviors explicitly prohibited by the developer policies.
when such comments are made aware to the market maintainers they should be able to warn the app developers about the behaviors immediately and remove the apps from the market if such undesired behaviors are not addressed by the app developers.
in other words user comments can form an immediate feedback for app market maintainers to identify user concerns and characterize the undesired behaviors that violate market policies.
ideally user reviews could serve as an effective source for app markets to identify policy violations in the apps after they have passed the initial vetting process.
however the number of user comments in an app market is huge given the rapidly increasing number of apps and there is a lack of automated tools to detect comments that are related to market policy violations and further perform deeper analysis on these comments.
furthermore these useful comments are often buried in a much larger number of irrelevant comments making it labor intensive and error prone to manually inspect these comments to obtain feedback.
while user comments have been studied for emerging issues app risks and app recommendation few research efforts have been spent in investigating how user comments can assist app markets in improving app vetting process.
thus little is known to what extent user comments can provide feedback on undesired behaviors that violate market policies and how app markets can utilize these feedback to improve their app vetting and maintenance process.
in this work we investigate the correlation between user comments and market policies i.e.
characterizing user perceived undesired behaviors prohibited by market policies.
first we create a taxonomy of kinds of undesired behaviors summarized from the developer policies of app markets.
then we propose champ an approach that adopts text mining and nlp techniques to identify comments that describe these kinds of undesired behaviors and classify them.
we refer to such comments as undesired behavior comments ubcomments .
more specifically champ first extracts semantic rules from a training dataset of user comments via a semi automated process.
champ then uses the extracted rules to automatically identify the undesired behaviors reflected in a given comment.
evaluation of champ on benchmarks from real word user comments suggests that it can successfully identify ubcomments with high precision and recall .
.
to further understand ubcomments in the wild we have curated a large scale dataset from app markets with over million user comments.
we applied champ on thesetable i the distribution of policies collected total .
mark et policies mark et policies googleplay 360market huawei market leno vo market meizu market oppo market vivo market xiaomi market tencent myapp comments to identify the ubcomments and study their characteristics.
we have a number of interesting findings ubcomments are prevalent in the app ecosystem which can be found in of the apps we studied.
ubcomments account for for the star comments.
our manual verification on sampled apps suggested the existence of undesired behaviors of them could be verified .
it confirms our assumption that users can still perceive a large number of undesired behaviors prohibited by market policies even though these apps have already passed the comprehensive vetting process .
user perceived undesired behaviors even some securityrelated ones can be found in both malware and benign apps the apps that were not flagged by any anti virus engines on virustotal .it suggests that user comments can be a complementary source for providing insights of malware detection.
although each market has explicitly declared developer policies roughly to of apps in each market were still complained about their undesired behaviors against the policies.
this observation further indicates that it is hard for app markets to identify all policy violations during app vetting while user comments could further help detect these violations continuously.
moreover policies from most markets are inadequate as we have identified many apps to showing undesired behaviors that are not covered in their policies.
to the best of our knowledge this is the first large scale study on the correlation between user comments and market policies of mobile apps .
we believe that our research efforts can positively contribute to the app vetting process promote best operational practices across app markets and boost the focus on related topics for the research community and market maintainers.
we have released the champ tool along with the policies and dataset to the research community at github .
ii.
a t axonomy of undesired behaviors as we seek to identify the ubcomments and investigate the correlation between user comments and market policies we first collect a dataset of market policies and compile a taxonomy of the undesired behaviors described in them.
market policy dataset.
considering that google play is the dominating market in the world except china we seek to collect policies from popular markets including google play and top chinese third party app markets as shown in table i. for each market we crawl all the listed policies from 934table ii ataxonomy of undesired behaviors and the distribution across market policies .
the!refers to the market declaring the policies .
the number refers to the of apps with ubcomments we identified from each market in section vi.
category beha vior360 mark ethuawei lenovo meizu oppo vivo xiaomitencent myappgoogle play functionality and performancefail to install !
!
!
!
!
!
!
fail to retrieve content !
!
!
!
!
fail to uninstall !
!
!
!
!
!
!
!
fail to start e.g.
crash !
!
!
!
!
!
!
badperformance e.g.
no responding !
!
!
!
!
fail to login or register !
!
!
fail to exit !
!
powerboot !
!
!
!
!
!
advertisementdrive by download !
!
!
addisruption !
!
!
!
!
!
!
!
addshortcuts in launching menu !
!
!
!
adsin notification bar !
!
!
!
!
!
securityvirus !
!
!
!
!
!
!
!
!
privacy leak !
!
!
!
!
payment deception !
!
!
!
!
!
illegal background behavior e.g.
sms !
!
!
!
excessive network traffic !
!
!
!
!
hidden app !
!
!
!
!
illegal redirection !
!
!
!
permission abuse !
!
!
!
!
!
illegitimate update e.g.
update to other app !
!
browser setting alteration !
!
!
illegitimate behavior of developersapprepackaging !
!
!
appranking fraud !
!
!
!
contentvulgar content e.g.
pornography anti society !
!
!
!
!
!
inconsistenc y between functionality and description !
!
!
total of apps with undesired behaviors total of apps with undesired behaviors declared policies total of apps with undesired behaviors undeclared policies the corresponding webpages.
in total we have collected policies.
note that the developer policies of google play were in english while the other market policies were in chinese.
google play has more complete and fine grained policies than any of the third party app markets.
summary of undesired behaviors.
as the policies defined by each market vary greatly some are coarse grained and some are fine grained it is non trivial to automatically classify them.
thus the first two authors of this paper manually went through these policies and classified them into main categories including distinct undesired behaviors.
table ii shows the taxonomy of the summarized undesired behaviors and the distribution of the corresponding policies across markets.
note that one behavior may correspond to one or more market policies.
we observe that all of the undesired behavior regulations can be found in google play.
as for the third party markets vivo andxiaomi have declared policies related to the most types of undesired behaviors covering and behaviors respectively.
we believe that this taxonomy covers most of the commonly observed undesired behaviors.
even though it may still be incomplete our approach is generic and can be adapted to support new behaviors and different granularities of behaviors see section vii .iii.
a utomated classification of ubc omments a. overview figure shows the overview of champ which builds a training dataset of user comments the training dataset building phase extracts semantic rules from the labelled comments the semantic rule extraction phase and uses the rules to identify and classify ubcomments the detection phase .
the major reason why we prefer semantic rules instead of text similarity is that most comments are short and often use a few key phrases in specific orders such as icon disappears while semantic rules have shown promising results in identifying sentences with specific purposes .
on the contrary text similarity approaches based on word similarity without emphasis on key phrases are optimized for general purposes and thus these approaches require extra tuning to focus on certain words that play important roles in the sentences of market policies .
additionally these approaches generally require a substantial amount of labelled samples to train the weights which is less effective in our context due to the limited number of labelled samples.
in the training dataset labelling phase we collect the comments of the apps from google play and third party app markets and resort to text clustering model to help to label the user comments.
in the topic modeling and topic labelling 935semantic rule generationtopic modeling and topic labellingcomment classification labelled topicsuser commentsbehavior identification behavior behavior behavior n semantic rule extraction phase training dataset building phase detection phase market policiessemantic rule checkinguser comments classified commentslabelled commentsword segmen tationkeywords extractionmanual inspectionfig.
.
overview of champ.
step we first merge the market policies that describe a same undesired behavior into a single document documents in total .
then champ applies a short text topic modeling algorithm to identify a set of topics where each topic contains a set of words.
at last champ labels each topic with related undesired behavior based on the similarity between the documents of policies and the words in the topics.
in the comment classification step champ uses the labelled topics to classify each comment into related undesired behaviors.
we further manually inspect the classified comments to confirm whether these comments are related to the corresponding undesired behavior.
this is necessary because we could only classify each comment based on the keywords with the highest weight under each topic which may introduce false positives.
for example if a comment contains the keyword notification it is considered to be likely to related to the behavior ads in notification bar .
however the word notification may also appear in comments that talk about alerts and notifications e.g.
notifications and alerts for weather apps .
in the semantic rule extraction phase champ applies a generation algorithm on the labelled comments and generates semantic rules for each undesired behavior automatically.
in the detection phase champ accepts user comments as input and uses the semantic rules to classify comments into the undesired behaviors defined in market policies.
b. training dataset labelling training dataset.
to label training dataset we randomly select of the comments for each app in our dataset discussed in iv .
in total we extract comments including english comments and chinese comments.
note that these comments were used separately for training two models for both english and chinese comments.
topic modeling and topic labelling .
unlike traditional documents e.g.
news articles the descriptions of undesired behaviors in market policies consist of only one or a few short sentences.
thus the lack of rich context makes it infeasible to use the topic modeling algorithms such as plsa and lda which implicitly model document level word cooccurrence patterns.
to address this problem we apply btm biterm topic model a widely used model for short texttopic modeling to learn the set of topics for market policies.
btm explicitly models word co occurrence patterns using biterms where each biterm is an unordered word pair cooccurred in a short context.
the output of btm are a set of topics where each topic consists of a list of words and their weights.
for each topic z btm draws a topic specific word distribution z dir and draws a topic distribution dir for all of the documents where and are the dirichlet priors.
for each biterm bin the biterm set b it draws a topic assignment z multi and draws two words wi wj multi z wherewiandwjare words appearing in the same document.
following the above procedure the joint probability of a biterm b wi wj can be written as p b x zp z p wijz p wjjz thus the likelihood of all the documents is p b y i j x z z ijz jjz we conduct topic modeling based on the merged english and chinese policies respectively.
we set the number of topics as which corresponds to the number of undesired behaviors.
champ then labels the proper undesired behaviors for the topics by computing the probability of each document being allocated to each topic.
it assumes that the topic proportion of a document equals to the expectation of the topic proportion of generated biterms during topic modelling p zjd x bp zjb p bjd wherezrepresents topic brepresents biterm and drepresents document.p zjb can be calculated via bayes formula based on the parameters estimated in btm p zjb z ijz jjzp z z ijz jjz 936table iii representative stopwords used in champ.
removed stopwordsadded stopwords miss high ask give can not how able stop without allow obtain othergod sex s t s d silly blah r h d n d b da n horrible p bjd can be estimated by the empirical distribution of biterms in the document where nd b is the frequency of the biterm b in the document d p bjd nd b p bnd b at last champ selects the highest score of p z d and labels the proper undesired behaviors for each of the topics.
comment classification.
champ then classifies each comment into related topics.
it computes the probability of each comment being allocated to each topic.
if the probability is above a certain threshold champ considers that the comment is related to the topic.
we follow the same empirical approach to set the threshold and find that .
is a good indicator.
in total we obtain comments that are related to distinct behaviors.
manual inspection.
considering that the automated classified comments may be not related to the undesired behaviors see iii.a we further manually inspected the comments that are classified into related topics to confirm whether these comments are ubcomments .
besides if a comment is related to more than one behavior we split the comment into several sentences and each sentence is related to a kind of undesired behavior.
two authors inspect the comments independently.
for the disagreements of category labelling a further discussion is performed.
eventually we obtained comments that are related to distinct behaviors.
after splitting some comments we obtained labelled comments in total which will be used for semantic rules generation.
note that we did not find any comments that are related to the behavior of browser setting alteration .
c. automated semantic rule extraction based on the labelled comments given a new comment the goal of champ is to determine whether the comment describes the same or similar behavior as the labelled comments.
to achieve this goal we propose to automatically extract semantic rules from the labelled comments for each undesired behavior.
firstly for each undesired behavior champ extracts and sorts the representative words from the related comments.
then champ analyzes the relations of the keywords by merging the keywords that usually appear in the same comments.
after that we can get one or more keyword sets containing different representative keywords.
at last champ generates semantic rules for each keyword set by combining keywords and calculating the distance constraints of the keywords.word segmentation.
in this step champ groups the comments related to each undesired behavior into a corpus corpora in total .
for each corpus it segments the comments into words removes meaningless words and sorts the remaining words in descending order based on the tf idf weighting to generate a word list wordlist .
stopwords are the words considered unimportant in text analysis tasks.
thus we take advantage of the stopword lists provided by hit and a public english stopwords list stopwords iso .however we find that the general stopword lists cannot well fit the app comment study .
on one hand when some traditional stopwords e.g.
can are combined with other words they become key phrases for describing undesired behaviors in user comments.
for example the comment always have to download other apps is related to the undesired behavior drive by download thus the traditional stopwords always and other should not be removed.
we summarized and removed stopwords including english stopwords and chinese stopwords that are important for describing undesired behaviors from the stopwords list.
on the other hand existing research found that there exist noises and spams e.g.
offensive comments in app comments which are meaningless for describing undesired behaviors.
therefore we adapt the selected stopword list and add over new stopwords that are regularly appeared in user comments.
the representative stopwords are shown in table iii offensive words are sanitized .
representative keywords extraction.
the goal of this step is to identify the most representative keywords that can cover the labelled comments in a given corpus.
thus for each keyword in the wordlist of a given corpus champ first collects the comments in the corpus that contain the keyword and adds them into a comment set comtset word.
then a traversal operation begins to select the keywords in order based on tf idf weight and compare the comtset word of different words.
for the comment set comtset wordmof the m th word word min thewordlist if part of the comments in it are overlapped with the comments in the n th word s n m comment set comtset wordn champ will merge word mand word ninto a keyword set.
otherwise champ will assign the wordword minto a new keyword set.
note that the traversal operation will stop if the union set from comtset word 1to comtset wordmcontains all of the labelled comments in the corpus.
based on the traversal operation champ could extract one or more keyword sets for each corpus.
semantic rule generation.
for each of the extracted keyword sets in a corpus champ automatically generates semantic rules.
we observe that a behavior can be generally described by two keywords of different part of speech in a comment.
for example the verb steal and the noun money in the comment it steals money from the credit card!!!
are related to behavior payment deception .
another example the adverb how and the verb uninstall in the comment who can tell me how to uninstall this app are related to behavior fail to uninstall .
thus for the extracted keyword sets champ combines the keywords of different part of speech pairwise.
furthermore we observe that most 937table iv representative semantic rules for 4behaviors .
beha vior semantic rules virus virus null null trojan null null mal ware null null adsin notification bar notification ads notification full remo ve notification permission abuse ask permission require permission unnecessary permission need permission want permission ubcomments are short and often include key phrases in specific orders.
therefore the semantic rules not only contain keywords but include order and distance constraints on matching the keywords.
for two keywords keyworduandkeywordv comtset u comtset u6 champ will generate two semantic rules fkeywordu keywordv constraintsgand fkeywordv keywordu constraintsg the constraints is used to limit the distances of these two keywords.
for example semantic rulefask permission 3gmeans that ask appears before permission and their distance is less than words.
champ automatically calculates the f1 score under different distance constraints we set it range from to for each semantic rule and select the best one.
note that if all of the keywords in a keyword set are noun each keyword will generate a semantic rule fkeyword null null g. eventually champ generates semantic rules for the undesired behaviors in total the list of rules can be found in in which semantic rules are for english comments and semantic rules are for chinese comments.
note that there are no comments related to the behavior of modify browser setttings and thus we use the description in the related policies to extract semantic rules rules in total .
the major differences between chinese comment rules and english comment rules are synonyms .
synonyms in chinese are more frequently used than in english leading to more rules for some undesired behaviors.
for example two keywords uninstall and remove of the semantic rules for behavior fail to uninstall are generated in english comments while champ has extracted synonyms of these two keywords in chinese comments.
table iv shows representative semantic rules for undesired behaviors in english comments the complete set of rules can be found at github .
as our semantic rules are trained to detect similar sentences that describe the behaviors in the policies thus the detected sentences are all high quality which will be evaluated in v. d. semantic rule checking based on these semantic rules champ classifies each comment into a type of ubcomments or others.
given a comment champ first removes the stopwords and performs word segmentation to extract words from the comment.champ then applies the semantic rules one by one to determine whether the comment matches any rules.
it searches the extracted words to see whether the keywords appear in the extracted words and checks the order and distance of successful matching keywords to determine whether they meet the constraints of the semantic rules.
as shown in fig.
the motivating app violates two behaviors i.e.
ads in notification bar and virus .
based on the rules defined in table iv champ determines that the first comment too many ads ... the notification bar is full of ads matches semantic rules of the undesired behavior notification bar since the comment has the keywords of notification ads and full .
similarly the other comment contains the keyword of virus and thus matches the undesired behavior virus .
iv.
s tudy design a. research questions we seek to answer the following research questions rqs rq1 how effective is champ in detecting ubcomments ?
as we aim to apply champ to extract ubcomments in the wild it is necessary to first evaluate the effectiveness of champ on extracting undesired behaviors using a benchmark dataset.
rq2 what kinds of undesired behaviors can be perceived by users?
it is important to explore to what extent we can infer undesired behaviors from user comments and which behaviors can be perceived by users.
rq3 how well do the policies in each app market capture the undesired behaviors reflected by user comments?
as each app market has its own policies we want to know whether they are effective in flagging undesired behaviors during the app vetting process.
app markets with weak app vetting processes are more likely to be exploited.
b. dataset collecting app candidates to answer the rqs we first need to harvest a comprehensive dataset that covers as many undesired behaviors as possible.
we take advantage of existing efforts and use a large scale android app repository .
this repository contains over .
million app items collected from google play and third party app markets.
the dataset also provides the detection result of virustotal a malware analysis service that aggregates over anti virus a v engines.
to better understand the distribution of ubcomments across apps with different maliciousness levels we classified our app candidates into categories malware grayware and benign apps.
as previous studies suggested that some a v engines may not always report reliable results we regard the apps labeled by over half of the a v engines as malware which is supposed to be a reliable threshold by previous work .
we consider apps flagged by no a v engines as benign apps and the other apps as grayware.
this roughly classification of malware and grayware might not be accurate enough but this is not the focus of this paper.
as the number of reported engines can be used as an indicator of the maliciousness of the apps we only want to study the diversity across apps with 938table v overview of our comment dataset .
mark etmalware grayware benign apps apps comments apps comments apps comments 360market huawei lenovo meizu oppo vivo xiaomi tencent myapp google play na na total different levels of maliciousness.
we randomly selected target app candidates chinese apps and google play apps from the dataset of wang et al.
including malware grayware and benign apps.
note that the google play apps include benign apps and grayware as all the malware samples were removed by google play and we cannot get their comments na in table v .
harvesting the user comments all the app markets we studied only provide a limited number of user comments.
for example google play review collection service only allows reviews of last week to be crawled for each app.
instead we built the comment dataset using two alternative approaches.
for the apps we selected from the chinese markets we resort to a third party app monitoring platform named kuchuan which has maintained the app metadata including comments from all the chinese markets we studied.
for the apps from google play we developed an automated tool to continuously fetch the user comments everyday within the span of months.
table v shows the distribution of collected comments.
in total we have collected over .
million user comments from apps1 including comments from malware comments from including chinese apps and google play apps grayware and comments from including chinese apps and google play apps benign apps.
this dataset will be used in the large scale measurement study see vi .
v. e valuation of champ a. benchmark datasets we curated two benchmark datasets english and chinese to evaluate champ.
we first select the apps which are confirmed to have undesired behaviors in the training dataset see iii b .
for each app we exclude the comments already used in training dataset.
at last two authors of this paper manually inspected and labelled these comments.
within our affordable efforts we aim to collect and label comments for each undesired behavior except for some behaviors with few related apps.
figure shows the distribution of our benchmark chinese comments and english comments .
note that we cannot find comments for the behavior browser setting alteration .
1note that for the selected 10k app candidates over of them have no user comments or very few user comments which were discard by us.b.
rq1 effectiveness of champ overall results table vi shows the evaluation results.
it shows that champ is very effective in identifying ubcomments.
the average precision and recall are and for the chinese benchmark and and for the english benchmark.
in particular champ achieves of precision and recall for out of types of ubcomments .
false positives negatives we further manually analyze the mis classified comments and obtain two observations.
first the false negatives are colloquial expressions instead of phrases.
for example the comment a window of card application pops up continuously is describing the behavior ad disruption .
but the key phrase ad is not in it.
moreover if we add a new semantic rule with the phrases window or pop up it may lead to other false positives.
second the false positives are generated owing to our insufficiently conservative rules .
for example the comment the app is completely useless btw i thought that this built in app can not be uninstalled but it succeeded.
is irrelevant to undesired behaviors.
however it is classified to the behavior fail to uninstall since it has the phrases can not and uninstall .
analogously if we upgrade our rules to be more conservative it may lead to more false negatives.
these are the inherent limitations of rule based matching methods.
we will further discuss it in vii.
comparison with text similarity approach we compare champ with the text similarity approach which classifies a comment to a type of undesired behavior based on text similarity between the comment and the classified comments in the training dataset see iii b .
we regard the behavior with the highest similarity score as the classification result.
as shown in table vi champ achieves significantly better results than the text similarity approach.
the average precision and recall achieved by the text similarity approach are and v.s.
and achieved by champ for the chinese comment dataset and and v.s.
and achieved by champ for the english comment dataset respectively.
in particular champ outperforms the text similarity approach on all behaviors.
such results indicate that the order and distance constraints adopted by our semantic rules can greatly reduce the false positives negatives.
for example the comment i can not install the app is similar to i installed but it can not help me back up files considering their text similarity but they are describing different types of undesired behaviors.
champ correctly distinguishes these two comments while the text similarity approach classifies both of them to the same type of undesired behavior.
vi.
l arge scale measurement study a. rq2 ubcomments in the wild overall results from the dataset we harvested see iv champ identifies ubcomments belonging to apps .
each app has received ubcomments from multiple users on average.
this indicates that ubcomments are prevalent in the mobile app ecosystem and the users who are sensitive to those policy violations are of labelled commentschinese englishfig.
.
distribution of labelled benchmarks.
table vi evaluation results on the benchmark datasets best results are shown in bold .
category beha viorbenchmark chinese benchmark english champ similarity based tool champ similarity based tool precision recall f1 precision recall f1 precision recall f1 precision recall f1 functionality and performancefail to install fail to retrieve content fail to uninstall fail to start e.g.
crash badperformance e.g.
no responding fail to login or register fail to exit powerboot na na na na na na advertisementdrive by download addisruption addshortcuts in launching menu na na na na na na adsin notification bar securityvirus privacy leak payment deception illegal background behavior e.g.
sms na na na na na na excessive network traffic hidden app illegal redirection permission abuse illegitimate update e.g.
update to other app na na na na na na browser setting alteration na na na na na na na na na na na na illegitimate behavior of developersapprepackaging appranking fraud contentvulgar content e.g.
pornography anti society inconsistenc y between functionality and description table vii distribution of ubcomments bycategories .
category comment app functionality performance advertisement security illegitimate behavior content .
total willing to report them in the comments.
table vii shows the distribution of ubcomments and apps across different categories of behaviors.
over of the ubcomments and over of the corresponding apps were complained to have functionality and performance issues.
this shows that users are most sensitive to the issues that directly affect their uses of the apps.
for the behaviors we summarized of them could be perceived by users.
the most popular behaviors ofubcomment are fail to start ad disruption and payment deception accounting for .
of the ubcomments .
both fail to start and ad disruption are related to user experiences while payment deception shows users security concerns.
manual verification of undesired behaviors.
to analyze whether the undesired behaviors described in user comments reflect the real behaviors of mobile apps we make effort to perform a manual verification here.
for each of the identified perceived behaviors we randomly select three apps apps in total and manually verify if indeed the apps violated the policies as described.
our manual verification follows a series of steps.
we first install them on smartphones to see whether they have shown undesired behaviors as user complained e.g.
ad disruption and malicious behaviors etc.
.
then we rely on testin a service that provides app testing on thousands of real world smartphones to check the functionality and performance issues e.g.
fail to start and fail to install .
furthermore we leverage static analysis 940table viii distribution of ubcomments byrating stars .
dataset star star star star star malware .
.
.
.
.
chinese grayware .
.
.
.
.
chinese benign apps .
.
.
.
.
gplay grayware6.
.
.
.
.
.
.
.
gplay benign apps1.
.
.
.
.
.
.
.
.
.
total .
.
.
.
.
tools e.g.
libradar and flowdroid to extract and inspect behavior related app information e.g.
sensitive code permissions and libraries .
at last for the apps in behavior app ranking fraud we compare their comments based on existing approaches proposed in to find fake comments.
overall apps have been confirmed with the undesired behaviors as user commented.
for the unconfirmed cases one in the vulgar content category and two in the payment deception category our dynamic analysis found that their services have stopped and our static analysis failed due to they have adopted heavy obfuscation and code protection using packing services.
nevertheless we show that most of the undesired behaviors can be confirmed .
low rating comments vs. high rating comments rq2.
we study the distribution of ubcomments across comments with different ratings from star to star .
quantitative analysis.
as shown in table viii it is apparent that low rating comments i.e.
star and star are more likely to describe undesired behaviors .ubcomments account for roughly and for the star comments and the star comments and .
and .
for the 4star and the star comments respectively.
note that the proportion of ubcomments in google play is much lower than that of chinese markets.
the major reason is that the crawled comments from google play contain a large amount of blank comments i.e.
the comments with only a rating but no descriptions.
we further eliminate such comments and report the result see the percentage in brackets in table viii .
qualitative analysis.
as shown in figure the distributions of ubcomments in chinese markets and google play show great diversity and thus we discuss them separately.
for app comments in chinese markets the distribution of undesired behaviors does not show much diversity across ubcomments with different ratings.
behaviors of the functionality and performance and advertisement types are most prevalent across all the ratings with the fail to start and ad disruption types are quite noticeable.
moreover we find that security related behaviors are prevalent in both low rating and high rating comments of malware but only prevalent in low rating comments of grayware and benign apps.
it is quite surprising that users complain about the security issues e.g.
payment deception but give the app malware a high rating.
thus we make efforts to manually examine allsuch contradictory comments in total and identify two major reasons.
first the default comment rating of most chinese app markets is star thus a number of users may only complain the app in the comments but forget to assign a rating.
second it is quite possible that some users misunderstand the meanings of star and star.
for example we find that several users assign totally opposite ratings in all their comments i.e.
star with really good comments but star with negative comments including the ubcomments .
it suggests the poor knowledge of the rating system for market users and the new challenges in analyzing the comments of third party app markets.
nevertheless champ can reveal how the users feel about their experiences and even could improve the techniques of app risk assessments based on user comments .
in google play the distribution of ubcomments in lowrating comments and high rating comments are quite different.
users generally give star in their comments when they find undesired behaviors in the app even if the behaviors do not belong to the security category.
we only find a few comments that are related to the vulgar content type in other comments.
this might be due to the high quality market which pays more attention to policy regulations and this more mature and regulated ecosystem enables users to better comprehend the ratings when providing comments.
malware vs. grayware vs. benign apps rq2.
for chinese markets over of malware samples have ubcomments and they have occupied of the comments.
as a contrast over of benign app samples and of grayware samples have ubcomments and the percentages of these comments are and respectively.
for google play over of benign apps and of grayware apps have ubcomments and they account for .
and of the overall comments .
and .
after removing the empty ones from the overall comments respectively.
in general one would think that malicious apps have more ubcomments than gray and benign apps as their behaviors are more likely to inconsistent with users expectation.
however the results are different for what we expected i.e.
the percentage of ubcomments does not show much difference across malware grayware and benign apps.
there are mainly two reasons.
first the policy violation behaviors of two major types functionality and performance and advertisement are prevalent in both malicious and benign apps e.g.
over of the ubcomments in third party benign apps are related to functionality and performance .
second some malware samples were removed in time by markets and thus malicious apps have not received much complaints than expected.
note that the security related undesired behaviors show different distributions across malicious gray and benign apps see figure .
as to chinese markets over of the ubcomments belong to the security category for malware while the percentages for grayware and benign apps are and .
as to google play over of the ubcomments in grayware are security related v .s.
in benign apps .
furthermore we observe that many user perceived undesired behaviors including security related ones were found in both malware and benign apps .
it suggests that some 941malware grayware chinese markets benign apps chinese markets benign apps google play grayware google play .
.
.
.55functionality and performance advertisement securitybehavior of developerscontent .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
overall overall overall overall overallfig.
.
distribution of ubcomments across different ratings y axis and undesired behaviors x axis in different app categories.
each row adds up to with each cell representing the percentage of a specific undesired behavior in the ubcomments for a specific rating e.g.
stars in an app category e.g.
malware .
the depth of the color is also used to indicate the percentage in the cell i.e.
a deep color indicating a large percentage.
for each app category a behavior category box represents the ubcomments of a specific behavior category e.g.
security and the number below the box shows the percentage of the ubcomments in that behavior category.
malicious behaviors are hard to detect by a v engines but user comments could provide insights for capturing them.
b. rq3 undesired behaviors across markets we perform market level analysis to investigate the differences across markets.
on one hand for the undesired behaviors declared in the policies of each market we seek to measure how many such behaviors have been identified in our dataset.
this result could be used to measure the effectiveness of market regulation i.e.
how many of these undesired apps have bypassed the corresponding auditing process.
on the other hand for other undesired behaviors that were not declared in the policies of a market we seek to explore whether we could find such behavior related comments in the corresponding markets.
table ii shows the results.
roughly to of the apps the numbers in bold from each market have found comments for undesired behaviors described in each of their market policies.
over of the apps in huawei market have violated its market policies while the percentage of such apps in google play is .
from another point of view roughly to of the apps besides google play as it covers all the behaviors we summarized in this paper have been complained of having undesired behaviors that are not captured by the markets policies.
for example over of the apps in the market have undesired behaviors that are not listed in its marketpolicies.
this may open doors for malicious developers to exploit the insufficient vetting process.
vii.
d iscussions a. relation with program analysis a large number of papers were focused on using program analysis to detect the security privacy ads third party library and functionality issues of mobile apps.
in contrast this paper focuses on a different perspective i.e.
how the users feel about their experiences .
users expectations play a big role on how much the users can tolerate the apps behaviors.
first although program analysis could be adopted to identify whether some sensitive behaviors exist in mobile apps it is non trivial to verify whether the behaviors violate the policy.
the borderline between policy violation and tolerable misbehaviors is fuzzy and highly dependent on users subjective expectations .
for example program analysis can easily identify ad libraries used in apps.
however aggressive mobile ads cannot be simply conflated with the detection of ad libraries.
the detection of ad libraries enabled by program analysis techniques cannot take what users really feel about the ads into consideration.
second a number of the policy violation behaviors e.g.
payment deception and 942vulgar content are difficult to be triggered and detected by program analysis techniques .
however they are indeed much easier revealed by user comments.
thus champ is complementary to program analysis which can provide insight to identify the boundary between policy violation behaviors and tolerable misbehaviors.
instead of identifying the policy violation behaviors directly champ can serve as a whistle blower that assigns policyviolation scores and identifies most informative comments for apps e.g.
putting security related comments at top .
note that not all the apps with ubcomments should be removed by the app market.
app vetting is aimed at promoting the overall quality of apps in the market.
thus app markets would generally give developers warnings and buffer time to fix undesired behaviors in their apps rather than removing them directly .
with the help of champ it will be possible to pinpoint more urgent violations accurately such as securityrelated ones so that the markets could choose their reaction accordingly.
b. threats to validity first the taxonomy we summarized may be incomplete.
although we have manually summarized undesired behaviors our taxonomy may still be incomplete since it was built based on current policies.
however our approach is generic and can be reused to support the detection of new types of undesired behaviors.
second our approach inherits the drawbacks of rule based approaches.
though our approach was proven to be quite effective during our evaluation the semantic rules we summarized may not be complete and could introduce false positives negatives as mentioned in section v a. nevertheless market policies are rarely updated.
furthermore our approach has strong expansibility of extracting new semantic rules for emerging app store policies.
when policies evolve new training can be performed to obtain new rules.
note that only the training process is semi automated as we need to manually label the classified comments.
our rules are extracted automatically from the labelled comments which can be applied to identify ubcomments automatically.
third we are not able to verify all the undesired behaviors for all the apps we identified .
we only sample apps for manual verification and found of them can be confirmed.
we found most of the behaviors cannot be easily identified using automated tools that is the reason why ubcomments are prevalent even though these apps have already passed the market vetting process .
this motivates the research community to develop better tools for identifying such behaviors.
nevertheless as aforementioned see section vii a instead of identifying the policy violation behaviors directly champ could raise alarm based on the number of undesired comments and reported users.
viii.
r elated work to the best of our knowledge our paper is the first one that identifies undesired behaviors from user comments.
nevertheless there are a number of studies focusing on app comments from different perspectives.
we present and discussbriefly related works on general app comment analysis and using nlp techniques in mobile app analysis.
app comment analysis.
mobile app comments have been extensively studied from other perspectives including mining user opinions app comment filtering and exploring other concerns .
for example chen et al.
pioneered the prioritization of user comments with ar miner.
chen et al.
conducted a study on the unreliable maturity content ratings of mobile apps which will result in inappropriate risk exposure for the children and adolescents.
nguyen et al.
proposed to analyze the relationship between user comments and securityrelated changes in android apps.
kong et al.
presented a machine learning technique to identify pre defined types of security related comments.
although app comments have been extensively studied from other perspectives none of the above work correlates user comments to the undesired behaviors described in market policies and none of them can be easily adopted extended to study this issue.
nlp in mobile app analysis.
besides user comments nlp techniques have been widely adopted to study app descriptions privacy policies and other meta text information related to mobile apps.
whyper and autocog adapt nlp techniques to characterize the inconsistencies between app descriptions and declared permissions.
ppchecker is a system for identifying the inconsistencies between privacy policy and the sensitive behaviors of apps.
chabada adapts nlp techniques to cluster apps using description topics and then identifies the outliers of api usage within each cluster.
our work is the first to investigate the correlation between user comments and market policies.
ix.
c onclusion we present the first large scale study to investigate the correlation between user comments and market policies.
in particular we propose champ a semantic rule based approach that effectively identifies ubcomments .
we apply champ to a large scale user comment dataset and observe that ubcomments are prevalent in the ecosystem even though app markets explicitly declared their policies and applied extensive vetting.
champ offers a promising approach to detect policy violations so as to help market maintainers identify these violations timely and further improve the app vetting process.
acknowledgment this work was supported by the national natural science foundation of china grant numbers and nsf cns and hong kong rgc projects no.
17e 18e cityu c1008 16g .