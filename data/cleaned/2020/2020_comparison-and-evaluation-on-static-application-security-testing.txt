singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems comparison and e valuation on static application security t esting comparison and e valuation on static application security t esting sast t ools for ja va sast t ools for ja va kaixuan li sen chen lingling f an ruitao feng singapor e management univ ersity rtfeng smu.edu.sg han liu see next page for additional authors follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the information security commons citation citation li kaixuan chen sen f an lingling feng ruitao liu han liu chengwei liu y ang and chen yixiang.
comparison and e valuation on static application security t esting sast t ools for ja va. .
esec fse pr oceedings of the 31st a cm joint e uropean softwar e engineering conf erence and symposium on the f oundations of softwar e engineering december san f rancisco .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
author author kaixuan li sen chen lingling f an ruitao feng han liu chengwei liu y ang liu and yixiang chen this conf erence pr oceeding ar ticle is a vailable at institutional k nowledge at singapor e management univ ersity https ink.libr ary.smu.edu.sg sis r esear ch comparison and evaluation on static application security testing sast tools for java kaixuan li east china normal university shanghai china kaixuanli stu.ecnu.edu.cnsen chen college of intelligence and computing tianjin university tianjin china senchen tju.edu.cnlingling fan college of cyber science nankai university tianjin china linglingfan nankai.edu.cn ruitao feng university of new south wales sydney australia halertfeng gmail.comhan liu east china normal university shanghai china hanliu stu.ecnu.edu.cnchengwei liu nanyang technological university singapore singapore chengwei001 e.ntu.edu.sg yang liu nanyang technological university singapore singapore yangliu ntu.edu.sgyixiang chen east china normal university shanghai china yxchen sei.ecnu.edu.cn abstract static application security testing sast takes a significant role in the software development life cycle sdlc .
however it is challenging to comprehensively evaluate the effectiveness of sast tools to determine which is the better one for detecting vulnerabilities.
in this paper based on well defined criteria we first selected seven free or open source sast tools from existing tools for further evaluation.
owing to the synthetic and newly constructed realworld benchmarks we evaluated and compared these sast tools from different and comprehensive perspectives such as effectiveness consistency and performance.
while sast tools perform well on synthetic benchmarks our results indicate that only .
of real world vulnerabilities can be detected by the selected tools.
even combining the detection capability of all tools most vulnerabilities .
remain undetected especially those beyond resource control and insufficiently neutralized input output vulnerabilities.
the fact is that although they have already built the corresponding detecting rules and integrated them into their capabilities the detection result still did not meet the expectations.
all useful findings unveiled in our comprehensive study indeed help to provide guidance on tool development improvement evaluation and selection for developers researchers and potential users.
these authors contributed equally to this work.
corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts software and its engineering software maintenance tools security and privacy software security engineering general and reference empirical studies .
keywords static application security testing benchmarks empirical study acm reference format kaixuan li sen chen lingling fan ruitao feng han liu chengwei liu yang liu and yixiang chen.
.
comparison and evaluation on static application security testing sast tools for java.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
https introduction the early detection and handling of vulnerabilities in software code is a matter of concern for software development.
in recent years security vulnerabilities such as log4shell and spring4shell have raised alarm bells.
researchers have also proposed various methods to detect software vulnerabilities such as formal verification static application security testing sast dynamic application security testing dast and interactive application security testing iast .
practically sast is the most popular technology due to its lower cost faster operation and stronger capability to detect bugs or vulnerabilities without executing a program.
hence the development of sast technology has obviously evolved and the number of corresponding tools has rapidly grown in recent years .
however it is still challenging for users to objectively evaluate and select the appropriate sast tools due to the following reasons.
firstly existing studies are mainly conducted on synthetic datasets where vulnerabilities are usually implemented and injected into programs manually.
compared toesec fse december san francisco ca usa kaixuan li sen chen lingling fan ruitao feng han liu chengwei liu yang liu and yixiang chen selected tools owasp benchmark vulnerable versions patched versions java cve benchmarkbenchmarks comprehensive evaluation perspectives rq3 consistency evaluation consistency among tools detected vs. claimed rq2 detection result dissection best worst detected cwe classes composition of false negatives large scale study on java oss programsrq1 effectiveness evaluation synthetic vs.real world benchmark approximate false positives analysis rq4 performance analysis effectiveness vs. performance mapping detecting rules mapping figure overview of our study.
real world vulnerabilities they are much simpler in design and easier to be detected.
therefore it is hard to objectively reflect the detection capability of tools in real world programs.
openssf also emphasized the importance of using real world vulnerability data to evaluate the effectiveness of sast tools and developed a benchmark which contains over cves common vulnerabilities and exposures .
however their benchmark only includes javascript and typescript cves.
some studies have also been exhibited to evaluate the effectiveness of sast tools on open source programs in which the datasets used are often small in size and limited in the number and types of contained vulnerabilities.
furthermore the focus of existing studies concerns more on quality issues e.g.
code styles performance and bad practices rather than security vulnerabilities.
for example thung et al.
performed an evaluation on java sast tools in which they explored to what extent the java sast tools detect realworld defects on three open source java programs and analyzed five kinds of defects including code style and bad practices.
thirdly specifically for java sast tools the shortage of knowledge on commonly detected types of vulnerabilities makes researchers even harder to gain deeper insights into the strengths and weaknesses of a given tool.
besides the consistency of vulnerability types that actually reported in detection and claimed to support in documentation is also an interesting research question to explore.
java is one of the most popular and well developed programming languages with a broad scope of application scenarios .
however till now there is still a lack of effort in evaluating sast tools on real world programs especially for java.
in a concurrent work evaluating sast tools lipp et al.
focused on sast tools for c programs in which they evaluated the effectiveness of six tools on real world vulnerability datasets using open source c projects.
but the results on c sast tools may be not feasible for java sast tools because of the different language constructs.
meanwhile the corresponding sast tools can differ in their usage and it is a question that how to choose a java sast tool that is suitable for scanning speed or workflow integration besides the effectiveness of vulnerability detection.
as shown in figure to bridge these gaps we evaluated representative java sast tools filtered from tools.
then we used common weakness enumeration cwe as a reference to map the detecting rules of these tools and cves contained in our collected benchmark datasets to cwe and automatically compared the effectiveness of each tool.
we collected types of benchmark datasets including a synthetic dataset i.e.
owasp benchmark anda real world benchmark i.e.
the java cve benchmark .
the latter includes open source java programs with unique cves.
the dataset covers unique vulnerability types cwe weaknesses belonging to cwe classes incwe .
for this we evaluated the tools effectiveness against the benchmarks.
based on their poor effectiveness on the java cve benchmark we further dissected the composition of false negatives.
moreover we performed a consistency evaluation on the vulnerabilities detected by these tools between the actually detected ones and what is claimed in the detecting rules.
finally we performed a performance analysis on representative java open source programs.
our study unveils that the evaluation of sast tools on synthetic datasets does not objectively reflect the detection capability of the tools.
in particular the selected tools overlooked most .
realworld vulnerabilities in the java cve benchmark while they have been shown to perform well on the owasp benchmark.
meanwhile over of vulnerabilities still remain undetected when combining the results of sast tools especially those beyond the scope of cwe andcwe .
for consistency analysis we observed that these tools generally overstate their detection capabilities even with .
overstatement on our real dataset.
meanwhile their analysis time increases sharply when the line of code loc is over 50k.
in particular insider and contrast are the fastest while semgrep and codeql require the longest time.
in summary we made the main contributions as follows we constructed a real world benchmark containing opensource java programs with unique cves on the method level which is considered the largest real world vulnerability benchmark for java.
it costs .
person months for the construction.
to fairly compare the sast tools detecting rules we mapped and grouped rules of tools studied and vulnerability data in our benchmarks into cwe classes and analyzed the detection consistency among tools as well as that between the detected vulnerability types and those claimed in their detecting rules.
we conducted a large scale empirical evaluation of the selected tools from comprehensive perspectives including effectiveness consistency and runtime performance.
to this end i.e.
scanning tasks are conducted.
based on the evaluation results we discussed the lessons learned and detailed the guidance on sast tool development improvement evaluation and selection for sast tool developers researchers and potential users.
overview .
tool selection we aim at gathering a representative set of sast tools since it is infeasible to give a complete set of all existing tools.
therefore we searched tool lists from recent scientific literature and snowballed from them as they also recommend further lists.
eventually we obtained several prominent websites giving recommendations for sast tools including github kompar nist owasp and wikipedia.
this process resulted in a very substantial set of sast tools even after removing duplicates out of .
we designed the following criteria to select our evaluation subjects.comparison and evaluation on static application security testing sast tools for java esec fse december san francisco ca usa table tool profile.
technology semantic data flow and control flow analysis or syntactic pattern matching within the code .
stars indicates github stars.
tool technology stars version codeql semantic .1k v2.
.
contrast semantic .
.
horusec syntactic v2.
.
insider syntactic v3.
.
sbwfsb semantic .1k v4.
.
v1.
.
semgrep semantic .3k v0.
.
sonarqube semantic .8k v9.
.
v4.
.
1java supported.
first we only included sast tools that support java programs and obtained java sast tools in total.
2free of charge.
second the java sast tools must be free of charge.
while commercial tools are indeed prevalent in the industry they often entail substantial costs and do not disclose their internal rule implementations thereby posing analytical limitations for our study.
thus commercial tools were excluded.
3being maintained.
third we eliminated tools that were no longer maintained.
specifically we manually checked whether the tool s open source repository had been active for the last years.
after this step sast tools were further removed.
4command line interface cli .
moreover we did not consider tools that have usage limits or purely operate through a graphical user interface as we aim to conduct a large scale experiment in this study.
therefore we excluded tools such as reshift hcl appscan codesweep and github code scanning based on this selection criterion we excluded tools.
5security related.
we try to select tools that identify generalized security vulnerabilities rather than those aimed to detect specific vulnerabilities or code quality issues such as linters .
initially we selected tools that claim they can detect vulnerabilities security issues and other similar terms in their documentation.
furthermore to facilitate the comparison and evaluation of tool effectiveness between synthetic and real world benchmarks it is required that tools should demonstrate an ability to detect vulnerabilities in synthetic benchmarks such as the owasp benchmark i.e.
be able to detect at least two vulnerability types.
finally we excluded tools including error prone facebook infer checkstyle pmd google java format and mega linter .
6well documented with detecting rules.
note that we intend to select sast tools with well documented detecting rules which allows us to analyze the effectiveness of each tool by mapping them to cwe.
meanwhile we explore whether the detecting capacity they claim in the rules is consistent with that in practice.
after applying this criterion we excluded tools that did not provide publicly available documentation of their detecting rules.
based on these criteria we finally selected tools codeql contrast codesec scan contrast horusec insider spotbugs with findsecuritybugs sbwfsb semgrep and sonarqube community edition sonarqube .
notably these tools except for contrast not only encompass a diverse range of sast techniques but also reflect the popularity within the developer and security communities as indicated by the number of github stars.
this underscores their widespread use and relevance in the field.
we have uploaded the full candidate sast tool list .
.
benchmark collection .
.
owasp benchmark .for the synthetic dataset selection we considered owasp benchmark as it is consistently maintained and updated compared with other synthetic datasets such as the juliet test suite java and owasp top benchmark .
although the vulnerabilities within it are synthetic we can use them to draw preliminary conclusions about the detection capabilities of sast tools.
the latest version of owasp benchmark i.e.
v1.
contains test cases.
each case has either a genuine exploitable vulnerability in total or a non vulnerable control instance mimicking a false positive in total .
.
.
java cve benchmark .in response to openssf s call for real world vulnerability data in sast tool evaluation we constructed a java cve benchmark by involving four steps as follows java programs collection we first searched java open source programs with disclosed cves and corresponding patch commits from advisory sources such as nvd debian and red hat bugzilla initially obtaining a list of programs.
version range extraction and method level locating we utilized szz to extract the vulnerable version range of programs affected by each cve ensuring accurate identification of affected versions.
meanwhile we employed ctags to locate methodlevel information for both vulnerable and patched versions which is essential for a detailed analysis of the vulnerabilities.
program packaging since the tools under evaluation accept different types of input e.g.
source code and binaries we further excluded the programs that failed to be packaged.
we finally obtained package able programs .
cross validating to ensure the benchmark quality we engaged three security experts from our industry partner.
they verified the vulnerability locations identified by our automated process and cross validated each other s work.
each expert thoroughly reviewed the details provided in the vulnerability and patch information obtained from advisory sources.
after that they crossvalidated each other s results.
if disagreements arose during the cross validation a majority voting was used to make the decision.
in cases where the votes were evenly split a discussion was held to resolve the conflict.
the vulnerability was then labeled with detailed information such as its location the affected versions and the specific methods where the vulnerabilities and patches were located.
finally we got package able open source programs containing unique cves where each program owns a vulnerable version and a patched version with the location of vulnerabilities and patches labeled at the method level.
the entire process of collecting the java cve benchmark took us .
person months with an additional person months spent on cross validating the vulnerability and patch locations.
to the best of our knowledge it isthe largest real world java vulnerability benchmark .
.
mapping vulnerability data in benchmarks and rules of tools to cwe since sast tools use different identifiers for the vulnerability types they support e.g.
insider uses cwes in the reported issues while others introduce their own vulnerability identifiers.
these differentesec fse december san francisco ca usa kaixuan li sen chen lingling fan ruitao feng han liu chengwei liu yang liu and yixiang chen table cwe mapped by our benchmarks and each sast tool.
vulnerabilities rules cwe weaknesses cwe cwe cwe cwe cwe cwe cwe cwe cwe cwe owasp benchmark java cve benchmark codeql contrast horusec insider sbwfsb semgrep sonarqube cwe taintedpath partialpathtraversal detecting rulescwe weaknessescwe classes cwe cwe cwe cwe 22cwe mapping mapping figure example of mapping and grouping rules to cwe.
identifiers make it difficult to automatically determine whether a sast tool hit a specific vulnerability type.
cwe refers to a community developed list of software and hardware weakness types including security vulnerabilities which is also used in cve reports and supported by many sast tools .
in addition all of these tools have mapped their own rules to cwe in their documentation or github repository except for spotbugs 1so we consider it a valid approach by evaluating them according to cwe.
in this study we selected cwe research concepts as a reference since it aims at facilitating research into weaknesses including inter dependencies among cwe entries when compared with the other two cwe views i.e.
cwe software development and cwe hardware design .
moreover cwe claims to try to include every weakness within cwe as well as with minimal overlap.
considering that direct mapping of rules to cwe weaknesses poses some hierarchical inconsistencies as shown in figure which may distinguish the effectiveness of the tools that map rules to different levels.
similarly cwe also has hierarchical structure issues .
we considered mapping detecting rules directly to the pillar level hereafter denoted by cwe classes incwe for the purpose of unifying them to the same level of cwe.
therefore to enable us to automate the evaluation of the tools studied we use cwe as a reference with the vulnerability data in the two benchmarks and the tool s rules mapped to cwe classes incwe respectively.
mapping vulnerability data to cwe.
since all of the vulnerabilities in our two benchmarks have been mapped to cwe weaknesses we thereby mapped them to cwe classes according to cwe .
mapping detecting rules to cwe.
similarly since these tools have mapped their detecting rules to cwe weaknesses except for spotbugs we only need to map them to cwe classes according to the hierarchy of cwe .
for spotbugs we manually mapped its rules to both cwe weaknesses andcwe classes .
this process involved three co authors independently performing the mapping.
they consulted the rule documentation and the hierarchy of cwe1000 during this process.
any conflicts in mapping results were resolved through majority voting .
finally we determined the support for cwe classes by the tools.
a cwe class was considered 1we obtained the mapping documentation of contrast from its technical support team.
.
.
.
.
.
.
.
recall precision f1 scorefigure effectiveness on owasp benchmark.
supported by a tool if the rule documentation stated it implemented a check for at least one cwe weakness within that class.
table shows each class in cwe is included supported or not .
the number of corresponding vulnerabilities rules is further displayed if included supported.
totally vulnerabilities included in owasp benchmark are grouped into cwe weaknesses and cwe classes while our real world benchmark owns more coverage than it does i.e.
including cwe weaknesses grouped into cwe classes except for cwe and cwe .
comparison and evaluation the evaluation aims to answer the following research questions rq1 effectiveness analysis.
how effective are these sast tools in detecting vulnerabilities on owasp benchmark and our constructed java cve benchmark?
rq2 detection result dissection.
what are the root causes of the detection results in rq1?
rq3 consistency analysis.
are the detection results consistent among these tools in terms of the detected vulnerability types?
are the detected vulnerability types consistent with what was claimed in each tool?
rq4 performance analysis.
how is the performance of these tools i.e.
the time cost of detection ?
.
rq1 effectiveness analysis .
.
setup .we evaluated the effectiveness of the tools on the two benchmarks.
for the owasp benchmark we compute recall precision and f1 score as the evaluation metrics.
for the java cve benchmark we calculate the proportion of detected cves denoted ascve r dvul allcvesinbenchmark and the proportion of cves still detected in the patched versions denoted as cve rpatch dvul dpatch dvul .
here dvulanddpatch represent the detected cves in the vulnerable and patched versions respectively.
the latter metric approximates the rate of false positives.
inspired by previous works we evaluated the real world detection capabilities of these tools with respect to file level andmethod level and divided them into four different scenarios as follows comparison and evaluation on static application security testing sast tools for java esec fse december san francisco ca usa detected cves codeql contrast horusec insider sbwfsb semgrep sonarqubesf asf csm asm cmanualcheck figure number of detected cves in different scenarios.
file level detection with any cwe class sf a a cve is considered detected if vulnerable file is hit by the tool regardless of the cwe class reported.
file level detection with correct cwe class sf c a cve is considered detected if vulnerable file is hit by the tool with the correct cwe class reported.
method level detection with any cwe class sm a a cve is considered detected if vulnerable method is hit regardless of the cwe class reported.
method level detection with correct cwe class sm c a cve is considered detected if vulnerable method is hit with the correct cwe class reported.
.
.
results .the overall results on these two benchmarks are shown in figure and figure respectively.
effectiveness on the owasp benchmark.
figure shows that contrast and sbwfsb can detect close to all synthetic vulnerabilities with f1 score .
and .
respectively.
however insider failed to detect most synthetic vulnerabilities with .
recall .
precision and .
f1 score.
as displayed in figure the effectiveness on three cwe classes varies from tools.
however synthetic vulnerabilities belonging to cwe are easier detected by tools especially those involving insecure cryptographic algorithms or insufficiently random values.
while those related to path traversal cwe and trust boundary violation cwe are hardly detected by these tools.
in particular horusec and insider failed to detect all of these two types.
for insider the number of its detecting rules is limited with no strong coverage of diverse java vulnerabilities i.e.
in total with only rules related to cwe .
moreover insider has no rule supporting cwe and cwe although claiming to cover the owasp top .
while for horusec cwe is also not supported by its rules.
however we further noticed that cwe is supported by its rules but not detected.
more specifically horusec implements related rules but all of them are based on primitive regular expressions which only detect few related vulnerabilities related to the hard coded use of either javax.ws.rs.pathparam or jakarta.ws.rs.pathparam .
finding sast tools generally perform well on the synthetic dataset i.e.
the owasp benchmark especially contrast and sbwfsb which both got an f1 score over while insider showed the lowest detection rate i.e.
.
f1 score following sonarqube with an f1 score of .
.
effectiveness on the java cve benchmark.
as shown in figure the effectiveness of the tools is analyzed in the scenarios aforementioned.
note that there may exist some cases in sm cwhere some tools hit the true cwe class of a cve but a wrong cwe weakness was actually reported.
to ensure the accuracy of our results we further checked the cwe weaknesses reported by the toolsinsm c. the corrected results are presented as manual check in figure .
contrary to the effectiveness of owasp benchmark it displays poor effectiveness on real world vulnerabilities of these tools reflecting that over of cves were ignored by selected tools.
even the top performing tool horusec achieved a mere .
cve r. although horusec is a syntax based tool whose rules are implemented by regular expressions it outperforms the three semantic based tools sbwfsb codeql and semgrep .
this unveils that the semantic analysis method is not always more effective than the less complex syntactic ones when in practice.
moreover we observed that horusec integrated with some other tools e.g.
gitleaks and trivy .
it is worth noting that horusec also integrates owasp dependency check within it a software composition analysis sca tool helping horusec hit another correct cves by scanning the vulnerable tpls used in programs.
although it is not a sast tool such an approach may inspire us for detecting more vulnerabilities during devsecops .
following horusec sbwfsb and codeql detected .
and .
cves.
both tools are equipped with data flow analysis dfa and control flow analysis cfa especially taint analysis.
to detect vulnerabilities sbwfsb uses resource files to list and store vulnerable sources and sinks to search taint paths which can limit its search scope in practice.
while codeql models source code as database records allowing its queries to search when scanning which is considered a stronger technology than sbwfsb.
but we found that the default rules within codeql are still simple which limits its effectiveness when detecting complex vulnerabilities such as cves.
for instance consider a codeql rule designed to detect cwe vulnerabilities as shown in listing .
it tracks tainted data from user input source to a file path creation sink l34 .
however it fails to detect indirect influences of user input on path creation.
additionally it deems a variable as guarded if it undergoes any form of check or sanitization which may not be sufficient.
for example simply replacing .. in user input could not prevent an attacker from constructing a path traversal string such as ... .
... .
.
this inadequate sanitization could still lead to an exploit which this default rule would not detect.
1from dataflow pathnode source dataflow pathnode sink pathcreation p expr e taintedpathlocalconfig conf 3where e sink .
getnode .
asexpr and e p. getaninput and conf .
hasflowpath source sink and not guarded e 5select p source sink flows to here and is used in a path .
source .
getnode listing taintpathlocal.ql in codeql.
notably contrast the most effective tool according to the owasp benchmark nearly failed to detect all the cves in all scenarios.
thus the evaluation using synthetic datasets may yield discrepancies or even opposite conclusions from those on real world ones.
finding these tools overlook more than of cves false negatives although performing well against synthetic benchmarks.
horusec and sbwfsb perform better than the other tools with a cve rof only .
and .
respectively.
effectiveness vs. cvss severity.
we try to explore the relationship between the effectiveness of sast tools and the severity of vulnerabilities by leveraging the common vulnerability scoringesec fse december san francisco ca usa kaixuan li sen chen lingling fan ruitao feng han liu chengwei liu yang liu and yixiang chen .
.
.
.
.
.
.
recall precision f1 score a effectiveness on cwe .
.
.
.
.
.
.
.
recall precision f1 score b effectiveness on cwe .
.
.
.
.
.
.
.
recall precision f1 score c effectiveness on cwe .
figure effectiveness of each class on owasp benchmark.
table detection results according to cvss severity.
cvss severity detected total high medium low high medium low680 java programs java cve benchmarkfigure severity distribution in the java cve benchmark.
system cvss associated with each cve.
due to the absence of cvss v3 for some cves we finally used cvss v2 for a fair evaluation.
as shown in figure the severity distribution of the java cve benchmark aligns with that of the original programs suggesting that the distribution of our benchmark does not significantly skew the results.
as outlined in table sast tools detected of low severity cves.
for medium severity cves the detection ratio dropped to .
.
interestingly the detection ratio for high severity cves was slightly higher at .
.
meanwhile we also observed distinct detection patterns within each severity level.
for instance all four high severity injectionrelated vulnerabilities which fall under the improper neutralization cwe class were detected.
however no input validation issues also under cwe were detected at this level.
the discrepancy in detection rates could be due to the complexity of highseverity vulnerabilities or the nature of the vulnerability itself.
for instance injection vulnerabilities which often involve improper input handling might be easier to detect than deserialization vulnerabilities which require complex object processing.
additionally only of medium severity deserialization of untrusted data cwe vulnerabilities were detected.
out of these vulnerabilities were found in the widely used jackson databind project yet none were detected highlighting the importance of effective vulnerability detection in popular software components.
to analyze approximate false positives we focused on sf c andsm csince there is no focus on cwe classes in the other scenarios as mentioned before.
as shown in table while most of the tools are generally poor in detecting real world vulnerabilities there are still rather the same vulnerabilities reported in the patched versions.
overall it reflects that the selected tools are not vulnerability sensitive enough since they are not sensitive enough to distinguish pieces of code before and after patching the vulnerability.
especially in sm c it unveils that horusec and codeql are more effective than the other tools on the patched versions with acve rpatch at .
and .
respectively while sbwfsb still reported the same cves.
it was observed that there existtable approximate false positives in sf candsm c. toolssf c sm c dvul dvul dpatch dvul dvul dpatch codeql contrast horusec insider sbwfsb semgrep sonarqube minor differences between each vulnerable and patched version on the syntax level but the patched version does fix the corresponding cve.
however the detecting rules of these tools are coarse grained.
this results in these tools capturing only simple patterns of these cves on the syntax level instead of capturing their exact patterns on the semantic level.
meanwhile we observed that there are certain vulnerability types are more often labeled as false positives than others in the tools.
for instance incorrect type conversion or cast cwe which is mapped by hs java by horusec is very likely to be false positives in our scope especially on the jackson dataformats binary .
.
therefore identifying these kinds of vulnerability types and disabling corresponding rules probably contributes to reducing false positives although a few true positives may be omitted.
finding these tools are not vulnerability sensitive when performing on patched versions which reflect tools false positives.
in particular horusec and codeql perform better than the others.
while sbwfsb seems less sensitive than the other tools in sm c. .
rq2 detection result dissection we further focused on analyzing the detection results of these tools on the java cve benchmark including cwe classes and cwe weaknesses that are easier and harder to detect as well as the underlying reasons for overlooking certain cves.
.
.
setup .to observe the effectiveness of the tools when focusing on specific vulnerability types we concentrated on studying thecwe classes that are most frequently detected regardless of tools.
to analyze the root causes for the poor effectiveness of the selected tools we manually reviewed the detected cves and the missing ones in the java cve benchmark as well as the detecting rules implementation.
this process was conducted in three rounds involving three co authors.
initially each co author individually analyzed all the cves and detection rules of each tool checking for any rules mapped to cwe weaknesses for undetectedcomparison and evaluation on static application security testing sast tools for java esec fse december san francisco ca usa detected cves codeql contrast horusec insider sbwfsb semgrep sonarqube figure distribution of detected cves in each cwe class .
cves.
subsequently they discussed their findings to reach a consensus resolving any disagreements through further discussion.
in the final round of the cves were randomly selected for review by all authors.
disagreements were resolved through discussion potentially leading to updates on the root causes.
finally we grouped the root causes into three main categories.
.
.
results .best vs. worst detected cwe classes.
overall figure reflects that among the vulnerabilities in our java cve benchmark cwe are the easily detected cwe classes .
specifically cwe refers to improper control of a resource through its lifetime which involves the management of system resources such as memory allocation and deallocation and cwe refers toimproper neutralization which includes vulnerabilities related to the improper handling of input or data.
interestingly although vulnerabilities related to the two classes of cwe andcwe710are theoretically supported by all the deployed tools most of the associated cves remained undetected except the sonarqube which lacks rules for cwe .
it is due to their low proportion .
in our real world benchmark.
in particular cves related to cwe are detected by all the tools.
after eliminating the overlapping in detected vulnerabilities unique cves in this class were found.
especially cves of cwe cwe cwe200 and cwe are the most frequently detected types within this class accounting for .
.
moreover unique of the cves related cwe were detected by all tools except for contrast.
within this class os command injection cwe and 3improper input validation cwe were detected.
finding real world vulnerabilities related to cwe and cwe are more easily detected especially those relevant to cwe cwe are more effectively detected.
however these tools still missed .
of the cwe vulnerabilities and .
of the cwe ones.
composition of missing vulnerabilities false negatives .
the successfully detected cves can be grouped into seven cwe classes cwe cwe cwe cwe cwe cwe and cwe .
we observed that it is because their patterns are easy for sast to spot e.g.
cve is a typical vulnerability related to cwe caused by not checking the use of which is a common path traversal pattern for sast tools to detect.
as displayed in table the composition of overlooking the cves can be summarized into three categories 1c1 no detecting rules supported by tools .
.
.
in this category although only .
cves are not supported by any tool s pre defined rules we found that each tool generally fails to support .
cves which sast is typically sufficient to detect but no rule was implemented.
in particular even codeql which1for javax .
servlet .
http .
cookie thecookie thecookies if thecookie .
getname .
equals benchmarktest00002 param java .
net .
urldecoder .
decode thecookie .
getvalue utf break 7filename org .
owasp .
benchmark .
helpers .
utils .
testfiles dir param 8fos new java .io.
fileoutputstream filename false listing testcase cwe in the owasp benchmark.
1public classpathresource string path classloader classloader assert .
notnull path path must not be null string pathtouse stringutils .
cleanpath path if pathtouse .
startswith pathtouse pathtouse .
substring this .
path pathtouse this .
classloader classloader !
null ?
classloader classutils .
getdefaultclassloader override 11public url geturl throws ioexception url url this .
classloader .
getresource this .
path if url null throw new filenotfoundexception this .
path cannot be resolved to url because it does not exist return url listing simplified code snippet for cve .
has the most rules among these tools still failed to implement rules for cves in the java cve benchmark.
for instance cve is related to use of insufficiently random values where sast is generally sufficient to detect most relevant instances although false negatives may occur if custom cryptography is used.
2c2 inadequate detection capabilities of tools .
.
.
cves were undetected due to inadequate detection capacities of these tools indicating that the predefined rules in the tools are not sufficiently effective in identifying real world vulnerabilities.
on the one hand the primitive implementation of predefined rules including source and sink lists significantly impacts the tools detection e.g.
.
of cves related to cwe went undetected despite targeted rules.
while a base search for objectinputstream andreadobject in source code could detect some related vulnerabilities most cases required additional dfa and cfa in rule implementation.
on the other hand code patterns in the cves were notably more complex than those in the synthetic cases a finding that is also revealed in a concurrent study on android .
for instance despite owning rules targeting cwe22 tools such as sbwfsb and codeql only detected .
of related cves even though they could detect all synthetic cases labeled with the same cwe in the owasp benchmark.
as shown in listing the synthetic code pattern is relatively straightforward and follows a linear flow within the same scope.
the user controlled input is taken directly through a cookie l1 and then used in constructing a file path l7 .
this pattern is rudimentary and primarily tests if the sast tool can track data flow within a single method.
conversely the code pattern in the java cve benchmark example listing is more nuanced and encapsulated within a class structure.
it arises from two separate methods for user controlled input and usage respectively with an implicit connection of a class field.
the constructor of classpathresource l1 accepts a user controlledesec fse december san francisco ca usa kaixuan li sen chen lingling fan ruitao feng han liu chengwei liu yang liu and yixiang chen table examples of missing cves by categories.
categories cve id cwe weakness cwe class c1cve cwe cwe cve cwe cwe c2cve cwe cwe cve cwe cwe c3cve cwe cwe cve cwe cwe path as an input but does not perform adequate validation before saving it as a field.
although there is some normalization l5 it is not sufficient to prevent path traversal sequences.
the vulnerability manifests when another method geturl attempts to read the field this.path and resolve it l11 and an attacker can exploit this by providing specially crafted paths.
detecting it requires the ability of inter procedural dfa and insufficient validation detection which is not required by listing .
the marked distinction between these patterns highlights the need for sast tools to excel in analyzing real world code especially when dealing with object interactions and method calls.
while the owasp benchmark is useful for basic testing it lacks the complexity present in real world scenarios.
thus using a real world benchmark is vital for evaluating the practical effectiveness of tools.
3c3 hard to be detected by sast .
.
in this case we found that there are cves difficult for sast to detect.
for example cve is a vulnerability related to uncontrolled resource consumption cwe .
however sast typically has limited utility in recognizing resource exhaustion problems since determining boundary values on integers requires a strong capacity in propagating boundary value information across any control flow units including loops.
moreover in addition to certain practical restrictions there exists a theoretical limit when inferring based on the undecidability of sast .
for instance invariants and post conditions are supposed to be deduced even for a loop.
finding over .
overlooked cves are caused by insufficient support of these tools especially those mapped to cwe cwe .
while .
are hard for sast to detect including those related to checking boundary value issues e.g.
cwe .
.
rq3 consistency analysis .
.
setup .inspired by findings on the java cve benchmark as shown in rq1 we further constructed two consistency analyses the consistency of detected cves among the tools and the consistency of detected cves between tools actually detect and what they claim to support.
for the latter task we try to explore whether these tools keep their promises based on the mapping results of detecting rules and cves.
to weaken the impacts of cves that are hard for sast to detect c3 in section .
.
we place the scope on those cves that sast technically has the ability to detect.
.
.
results .the fact is that the number of detected cves in each cwe class varies for each tool as displayed in figure .
consistency among the tools.
as indicated in figure there is no cve that was detected simultaneously even by the four bestperforming tools which reflects these tools have different focuses.
by comparing the detected vulnerabilities by all tools we found that unique cves are detected only by horusec with and ones size of each list .
co d e q l2 h o r u s e c1 s b w f s b9 s e m g r e p number of elements specific or shared by ... lists codeqlhorusec sbwfsb semgr epfigure tools combination in sm c. only detected by sbwfsb and semgrep respectively.
specifically the most detected cves by horusec are in cwe especially those related to cwe .
meanwhile the most detected cves by sbwfsb are in cwe including cwe .
while codeql detected most cves related to cwe e.g.
the use of a broken or risky cryptography algorithm cwe .
however there are .
certain cves being detected by no less than two tools such ascve cwe cve cwe and cve cwe etc.
it is observed that these cves patterns are easy for sast to detect.
for instance cve a vulnerability found in the apache qpid proton j transport is related to improper certificate validation which even syntax based tools can hit by searching for well known dangerous sinks such as x509trustmanager and checkclienttrusted .
besides although sbwfsb and codeql detected cves related to cwe we found that codeql even reported a more precise vulnerability type path traversal .. filename cwe for cve2018 .
specifically there are rules for detecting related vulnerabilities covering both absolute path traversal and relative path traversal including cwe cwe cwe cwe etc.
it unveils that codeql has more complete coverage and fined granularity on path traversal vulnerabilities.
since none of the single tools performs well on the java cve benchmark and there are different focuses among tools we try to analyze the effectiveness improvement by combining multiple sast tools.
here we selected and combined the sast tools with the most cves found.
a cve is thereby considered found if at least one tool was able to detect it.
as the combination of tools can also result in an increase in false positives we selected those that contain the fewest sast tools and also output the fewest false positives.
the best combination of the tools is codeql horusec sbwfsb semgrep which can cover unique cves as shown in figure .
however the cve rreaches .
andcve rpatch at .
which is an improvement of only .
but .
increase in false positives compared with horusec.
for a combination of three tools the best one is codeql horusec sbwfsb withcve rat .
and cve rpatch at .
.
finding the combination of tools can improve vulnerability detection .
but is not high as expected which still fails to detect over real world vulnerabilities with an approximate cost of a .
.
increase in false positives.
consistency between detected and claimed by each tool.
as revealed in table by mapping their rules to cwe each tool is able to support a wide range of vulnerabilities but still misses some cwe classes .
in detail cves belonging to cwe are less supported by tools than those of the other cwe classes although codeql implements rules supporting cwe .
itcomparison and evaluation on static application security testing sast tools for java esec fse december san francisco ca usa cves supported detected figure consistency between the detected cves and those claimed to be supported by each tool.
unveils that it is generally consistent with the most detected cwe classes i.e.
cwe as mentioned in section .
.
.
however vulnerabilities related to cwe are only supported by codeql while codeql and contrast claim to support those belonging to cwe .
these two classes did not appear in our benchmarking experiment so we are not able to analyze their consistency.
when it comes to the two cwe classes they concern vulnerabilities related to incorrect calculation and incorrect comparison respectively.
through our analysis we observed these vulnerabilities are only caused by security critical calculations comparisons most likely causing security unrelated issues including code smell etc.
therefore the influence of the lack of real world vulnerabilities under the two cwe classes has been weakened.
as unveiled in figure there is much over statement by these tools .
specifically these tools are generally over claimed to support .
vulnerabilities than their actual capacity in our real world benchmark.
it indicates that potential users should select tools cautiously instead of only relying on tools claims.
even the best performing tool horusec overstates that .
of cves can be detected by its support whereas actually they are not.
moreover codeql has the most default detecting rules with support for unique cwe weaknesses but it only detected .
cves of those claimed to support.
however for specific cwe classes horusec owns .
rules related to cwe especially for cwe and cwe since it is integrated with gitleaks.
as a result it also detected cwe related vulnerabilities including cve which is caused by an xml signature cryptographic issue.
but sbwfsb detected none of the related cves although owning rules belonging to cwe .
finding these tools are generally over stated in their capacity for vulnerability detection.
specifically there are over cves failing to be detected although they are claimed to be supported.
especially contrast it over claims to support .
cves in the java cve benchmark.
.
rq4 performance analysis apart from evaluating the effectiveness of these tools we also intended to analyze their performance.
we try to explore whether there is a correlation between the detecting technology used and performance when scanning programs of different sizes.
.
.
setup .to mitigate the bias in dataset construction we collected representative java open source programs in various sizes by following the two criteria.
we first collected java open source 0100200300400500time consumed s loccodeql contrast horusec insider sbwfsb semgrep sonarqubefigure average performance of sast tools programs from the repositories published in the package manager including maven since they are more likely to be packaged successfully.
we got programs as the initial list.
we then selected representative programs by setting two sub criteria 1each program s package should be relied on by at least one package and 2there exist new packages relying on them within the last three years.
finally we obtained programs that can be packaged of which the versions are all up to date till august .
to ensure robustness and consider potential infrastructure variability we performed each performance measurement three times for each tool.
the reported results represent the average of these trials.
.
.
results .we analyzed their runtime performance based on the lines of code loc of the programs.
performance analysis.
as displayed in figure it is clear that the scanning time required by the selected sast tools increases as the loc of java programs increases.
specifically the performance does not vary considerably when the program s loc is less than 50k but it increases significantly above 50k particularly for codeql.
insider is the fastest among the sast tools studied requiring on average less than seconds to scan code when loc is no more than 50k and about .
seconds to scan even for programs over 100k loc.
it is because insider is a syntax based sast tool by comparing the source code directly against the pre defined keywords.
contrast is also efficient surpassing insider as the fastest tool when loc is greater than 20k since its input must be a jar or war file of the java programs and the scan is performed by uploading the jar or war file to a cloud server.
it can be observed that codeql requires more time than the other tools when the loc of programs is over 50k.
codeql involves the aforementioned two steps first generates the codebase based on the given program then performs semantic analysis involving dfa and cfa on the codebase with queries.
moreover some queries take a long time to scan such as taint path checking even over hours if setting no time limit.
finding the analysis time increases sharply on programs over 50k loc.
insider and contrast are the fastest while semgrep and codeql require the longest time to finish the scan with an average of 267s and 193s respectively.
effectiveness vs. performance.
syntax based tools insider horusec generally run faster than semantic based ones codeql sbwfsb sonarqube and semgrep which coincides with our aforementioned assumption.
horusec is the slowest syntax based tool which even generally takes longer than sonarqube to complete a scan.
in particular it takes an average of seconds to complete aesec fse december san francisco ca usa kaixuan li sen chen lingling fan ruitao feng han liu chengwei liu yang liu and yixiang chen scan when the loc is less than 50k and seconds when the loc is over 50k.
it is considered since horusec needs to copy the folder of programs to prepare for the scan and it is integrated with various tools including gitleaks trivy and owasp dependencycheck.
horusec is equipped with a more complex syntax based analysis than insider by further comparing their implementation of detecting rules.
meanwhile semgrep takes more time than others on each program when loc is no more than 50k generally taking seconds on average.
interestingly it is not affected much by the program size with an average scan time of seconds on loc less than while on loc of 10k 15k it takes an average of seconds to complete the scan.
although both semgrep and codeql perform semantic based analysis when scanning the performance of semgrep was not influenced by loc.
we summarize the two reasons besides a combination of syntax analysis and semantic analysis there are also some trade offs between detecting capacity and scan speed in semgrep including limited intraprocedural dfa no pointer or shape analysis and individual elements in arrays or other data structures are not tracked etc.
it also results in the aforementioned limited effectiveness in vulnerability detection in section .
.
.
semgrep takes an optimization called single file analysis that directly links scanning with the number of rules independent of loc.
specifically semgrep slices and runs single files in a given program which also deprives it of the ability to detect certain complex inter procedural issues.
these insights could guide the development and refactoring of sast tools to handle continuously updated rules and increasingly complex software implementations especially for those with over 100k loc.
finding there is a trade off between semantic based analysis and the performance within semgrep which contributes to its scanning performance well.
meanwhile its deployed file slicing technology is considered useful when analyzing large programs.
discussion .
lessons learned .
.
for java sast developers .
improve effectiveness with efficient rules.
since detecting capacity is the foundation of sast tools developers should first ensure their effectiveness.
1implement rules by extracting exact semantic patterns of vulnerabilities e.g.
to detect cwe vulnerabilities it is not enough to only search for common sinks such as readobject dfa and cfa should also be used to trace the tainted path section .
.
.
2tools should excel in analyzing real world code.
this requires developers to observe and summarize the features of real world vulnerabilities when designing rules e.g.
tools should be enhanced by analyzing vulnerabilities with object interactions and method calls section .
.
.
improve the scalability on large programs.
since users would not consider a time consuming sast tool even though it could hit some vulnerabilities developers should consider the performance when scanning large programs e.g.
the single file analysis in semgrep would be a useful inspiration section .
.
.
.
.
for java sast researchers .
a unified mapping reference is essential.
as mentioned in section .
it is desirable to use publicly available