towards practical robustness analysis for dnns based on pac model learning renjue li lirj19 ios.ac.cn sklcs institute of software cas university of chinese academy of sciences chinapengfei yang yangpf ios.ac.cn sklcs institute of software cas chinacheng chao huang chengchao nj.iscas.ac.cn nanjing institute of software technology iscas pazhou lab china youcheng sun youcheng.sun qub.ac.uk queen s university belfast united kingdombai xue xuebai ios.ac.cn sklcs institute of software cas university of chinese academy of sciences chinalijun zhang zhanglj ios.ac.cn sklcs institute of software cas university of chinese academy of sciences china abstract to analyse local robustness properties of deep neural networks dnns we present a practical framework from a model learning perspective.
based on black box model learning with scenario optimisation we abstract the local behaviour of a dnn via an affine model with the probably approximately correct pac guarantee.
from the learned model we can infer the corresponding pac model robustness property.
the innovation of our work is the integration of model learning into pac robustness analysis that is weconstructapacguaranteeonthemodellevelinsteadofsample distribution whichinducesamorefaithfulandaccuraterobustness evaluation.this isin contrastto existingstatistical methodswithout model learning.
we implement our method in a prototypical tool named deeppac.
as a black box method deeppac is scalable and efficient especially when dnns have complex structures or high dimensionalinputs.weextensivelyevaluatedeeppac with4baselines usingformalverification statisticalmethods testingandadversarial attack and dnn models across datasets including mnist cifar and imagenet.
it is shown that deeppac outperformsthestate of the artstatisticalmethodprovero andit achieves more practical robustness analysis than the formal ver ification tool eran.
also its results are consistent with existing dnn testing work like deepgini.
ccs concepts security and privacy software and application security computing methodologies artificial intelligence.
corresponding authors permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
neural networks pac model robustness model learning scenario optimization acm reference format renjueli pengfeiyang cheng chaohuang youchengsun baixue andlijunzhang.
.towardspracticalrobustnessanalysisfordnnsbased on pac model learning.
in 44th international conference on software engineering icse may21 pittsburgh pa usa.
acm newyork ny usa 13pages.
introduction deepneuralnetworks dnns arenowwidelydeployedinmany applications such as image classification game playing and the recent scientific discovery on predictions of protein structure .
adversarial robustness of a dnn plays the critical role for its trustworthyuse.thisisespeciallytrueforforsafety criticalapplications suchasself drivingcars .studieshaveshownthatevenfora dnnwithhighaccuracy itcanbefooledeasilybycarefullycrafted adversarialinputs .this motivatesresearchon verifyingdnn robustnessproperties i.e.
the predictionof thednnremains the same after bounded perturbation on an input.
as the certifiable criterion before deploying a dnn the robustness radius should be estimated or the robustness property should be verified.
in this paper we propose a practical framework for analysing robustness of dnns.
the main idea is to learn an affine model which abstracts local behaviour of a dnn and use the learned model instead of the original dnn model for robustness analysis.
different from model abstraction methods like our learned modelisnotastrictlysoundover approximation butitvariesfrom thednnuniformlywithinagivenmarginsubjecttosomespecifiedsignificance level and error rate.
we call such a model the probably approximately correct pac model.
there are several different approaches to estimating the maximum robustness radius of a given input for the dnn includingformal verification statistical analysis and adversarial attack.
inthe following we will first briefly explain the pros and cons of each approach for and its relation with our method.
then we will highlight the main contributions in this paper.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa renjue li et al.
bound via formal verification is often too conservative.
a dnn is a complex nonlinear function and formal verification tools cantypically handlednns withhundreds to thousandsofneurons.thisisdwarfedbythesizeofmoderndnns usedintherealworld suchastheresnet50model usedinour experiment with almost million hidden neurons.
the advantage of formal verification is that its resulting robustness bound is guaranteed buttheboundisalsooftentooconservative.forexample thestate of the artformalverificationtooleranisbasedonabstract interpretation that over approximates the computation inadnnusingcomputationallymoreefficientabstractdomains.
iftheeranverificationsucceeds onecanconcludethatthenetworkislocallyrobust otherwise duetoitsover approximation no conclusive result can be reached and the robustness property may or may not hold.
estimationviastatisticalmethodsisoftentoolarge.
ifweweaken the robustness condition by allowing a small error rate on the robustnessproperty itbecomesaprobabilisticrobustness orquantitative robustness property.
probabilistic robustness characterises thelocalrobustnessinawaysimilartotheideaofthelabelchangerateinmutationtestingfordnns .in statisticalmethodsareproposed toevaluatelocalrobustnesswith a probably approximately correct pac guarantee.
that is witha given confidence the dnn satisfies a probabilistic robustness property andwecallthis pacrobustness.however aswearegoing toseeintheexperiments section thepacrobustnessestimation via existing statistical methods is often unnecessarily large.
in thiswork ourmethodsignificantlyimprovesthepacrobustness bound without loss of confidence or error rate.
boundviaadversarialattackhasnoguarantee.
adversarialattack algorithmsapplyvarioussearchheuristicsbasedone.g.
gradient descentorevolutionarytechniquesforgeneratingadversarialinputs .these methodsmaybeable tofindadversarial inputsefficiently butarenotabletoprovideanysoundnessguarantee.whiletheadversarialinputsfoundbytheattackestablishanupper bound of the dnn local robustness it is not known whether there are other adversarial inputs within the bound.
later we will use this upper bound obtained by adversarial attack together with the lower bound proved by the formal verification approach dis cussed above as the reference for evaluating the quality of our pac model robustness results and comparing them with the latest statistical method.
contributions .weproposeanovelframeworkofpac model robustness verification for dnns.
inspiredby the scenario optimisation technique in robust control design we give an algorithmto learn an affine pac model for a dnn.
this affine pac model captureslocalbehaviouroftheoriginaldnn.itissimpleenough forefficient robustnessanalysis andits pacguarantee ensuresthe accuracyoftheanalysis.weimplementouralgorithminaprototypecalleddeeppac.weextensivelyevaluatedeeppacwith20 dnnsonthreedatasets.deeppacoutperformsthestate of the art statistical tool provero with less running time fewer samples and moreimportantly muchhigherprecision.deeppaccanassess the dnn robustness faithfully when the formal verification and existing statistical methods fail to generate meaningful results.organisation of the paper.
the rest of this paper is organized as follows.insect.
wefirstintroducethebackgroundknowledge.we then formalize the novel concept pac model robustness in sect.
.
themethodologyisdetailedinsect.
.extensiveexperimentshave been conducted in sect.
5for evaluating deeppac.
we discuss related work in sect.
6and conclude our work in sect.
.
preliminary in this section we first recall the background knowledge on thednn and its local robustness properties.
then we introduce thescenario optimization method that will be used later.
in this followingcontext wedenote xiastheithentryofavector x rm.
forx rmand r we define x as x0 ... xm latticetop.
givenx y rm we write x yifxi yifori ... m.w e use0todenotethezerovector.for x rm itsl normisdefined as bardblx bardbl max1 i m xi .
we use the notation b x r x rm bardblx x bardbl r torepresenttheclosed l normballwiththe center x rmand radius r .
.
dnns and local robustness adeepneuralnetworkcanbecharacterizedasafunction f rm rnwithf f1 ... fn latticetop wherefidenotes the function correspondingtothe ithoutput.forclassificationtasks adnnlabelsan inputxwiththeoutputdimensionhavingthelargestscore denoted bycf x argmax i nfi x .adnniscomposedbymultiple layers the input layer followed by several hidden layers and an outputlayerintheend.ahiddenlayerappliesanaffinefunction oranon linearactivationfunctionontheoutputofpreviouslayers.
the function fis the composition of the transformations between layers.
example .
.
we illustrate a fully connected neural network fnn where each node i.e.
neuron is connected with the nodes from the previous layer.
each neuron has a value that is calculated as the weighted sum of the neuron values in the previous layer plus a bias.
for a hidden neuron this value is often followed by an activation function e.g.
a relu function that rectifies any negative valueinto0.infig.
thefnncharacterizesafunction f r2 r2.
theweightandbiasparametersarehighlightedontheedgesand the nodes respectively.
for an input x x1 x2 latticetop w e havef x f1 x f2 x latticetop.
input input 2output output figure an fnn with two input neurons two hidden neu rons and two output neurons.
for a certain class label lscript we define the targeted score difference function as x f1 x f lscript x ... fn x f lscript x latticetop.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards practical robustness analysis for dnns based on pac model learning icse may pittsburgh pa usa straightforwardly thisfunctionmeasuresthedifferencebetween thescoreofthetargetedlabelandotherlabels.forsimplicity we ignoretheentry f lscript x f lscript x andregardthescoredifferencefunction as a function from rmtorn .
for any inputs xwith the class label lscript it is clear that x 0if the classification is correct.
for simplicity when consideringan l norm ballwith thecenter x we denote by the difference score function with respect to the label of x. then robustness property of a dnn can therefore be defined as below.
definition2.
dnnrobustness .
givenadnn f rm rn an input x rm andr we say that fis locally robustinb x r if for allx b x r we have x .
intuitively local robustness ensures the consistency of the behaviour of a given input under certain perturbations.
an input x prime b x r that destroys the robustness i.e.
x prime is called anadversarialexample.notethatthispropertyisverystrictsothat the corresponding verification problem is np complete and the exact maximum robustness radius cannot be computed efficiently except for verysmall dnns.
even estimatinga relatively accurate lowerboundisdifficultandexistingsoundmethodscannotscale to the state of the art dnns.
in order to perform more practical dnnrobustnessanalysis thepropertyisrelaxedbyallowingsome errors in the sense of probability.
below we recall the definition of pac robustness .
definition .
pac robustness .
given a dnn f rm rn a n l normball b x r aprobabilitymeasure ponb x r asignificancelevel andanerrorrate thednn fis pacrobust inb x r if p x with confidence .
pacrobustnessisanstatisticalrelaxationandextensionofdnn robustness in def.
.
.
it essentially only focuses on the input samples butmostlyignoresthebehavioralnatureoftheoriginalmodel.
whentheinputspaceisofhighdimension theboundariesbetween benigninputsandadversarialinputswillbeextremelycomplexand therequiredsamplingeffortwillbealsochallenging.thus anaccu rateestimationofpacrobustnessisfarfromtrivial.thismotivates us to innovate the pac robustness with pac model robustness in this paper sect.
.
.
scenario optimization scenariooptimizationisanothermotivationfordeeppac.ithas been successfullyused in robustcontrol design forsolving a class of optimization problems in a statistical sense by only consider ing a randomly sampled finite subset of infinitely many convex constraints .
let us consider the following optimization problem min rmb latticetop s.t.f wheref isaconvexandcontinuousfunctionofthe m dimensional optimization variable for every and both and are convex and closed.
in this work we also assume that is bounded.
inprinciple itischallengingtosolve asthereareinfinitelymanyconstraints.
calafiore et al.
proposed the following scenario approach to solve with a pac guarantee.
definition2.
.
letpbeaprobabilitymeasureon .thescenario approach to handle the optimization problem is to solve the followingproblem.weextract kindependentandidenticallydistributed i.i.d.
samples i k i 1from accordingtotheprobability measure p min rmb latticetop s.t.k logicalanddisplay.
i 1f i .
thescenarioapproachrelaxestheinfinitelymanyconstraints in by only considering a finite subset containing kconstraints.
in a pac guarantee depending on k between the scenario solution in and its original optimization in is proved.
this isfurtherimprovedby inreducingthenumberofsamples k. specifically the following theorem establishes a condition on k for which assures that its solution satisfies the constraints in statistically.
theorem .
.if is feasible and has a unique optimal solution k and k ln1 m where and are the pre defined error rate and the significance level respectively then with confidence at least the optimal ksatisfies all the constraints in but only at most a fraction of probability measure i.e.
p f k .
in this work we set pto be the uniform distribution on the setin .
itis worthy mentioningthat theorem .5stillholds even if the uniqueness of the optimal kis not required since a unique optimal solution can always be obtained by using the tie break rule if multiple optimal solutions exist.
thescenariooptimizationtechniquehasbeenexploitedinthe context of black box verification for continuous time dynamicalsystems in .
we will propose an approach based on scenario optimization to verify pac model robustness in this paper.
pac model robustness the formalisation of the novel concept pac model robustness is our first contribution in this work and it is the basis for developing our method.
we start from defining a pac model.
let fbe a given set of high dimensional real functions like affine functions .
definition3.
pacmodel .
let rm rn b rmandpa probability measure on b. let be the given error rate and significance level respectively.
let be the margin.
a function tildewide b rn fisapacmodelof onbw.r.t.
and denoted by tildewide i f p tildewide x x with confidence .
in def.
.
we define a pac model tildewide as an approximation of the original model with two parameters and which bound the maximal significance level and the maximal error rate for the pac authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa renjue li et al.
model respectively.
meanwhile there is another parameter that bounds the margin between the pac model and the original model.
intuitively the difference between a pac model and the original one is bounded under the given error rate and significance level .
foradnn f ifitspacmodel tildewidefwiththecorrespondingmarginis robust then fispac modelrobust.formally wehavethefollowing definition.
definition .
pac model robustness .
letf rm rnbe a dnn and the corresponding score difference.
let be thegivenerrorrateandsignificancelevel respectively.thednn fis pac model robust in b x r if there exists a pac model tildewide such that for all x b x r tildewide x .
weremindthat isthescoredifferencefunctionmeasuringthe difference between the score of the targeted label and other labels.
a locally robust dnn requires that x and a pac model robust dnn requires the pac upper bound of i.e.
tildewide x i s always smaller than .
infig.
weillustratethepropertyspaceofpac modelrobustness by using the parameters and .
the properties on the axis are exactly the strict robustness since x is now strictly upper bounded by tildewide x .
intuitively for fixed and a smaller margin impliesabetterpacapproximation tildewide x oftheoriginal one x and indicates that the pac model robustness is closer to the strict robustness property of the original model.
to estimatethemaximumrobustnessradiusmoreaccurately weintend to compute a pac model with the margin as small as possible.
moreover the proposed pac model robustness is stronger than pac robustness which is proved by the following proposition.
proposition3.
.
ifadnnfis pac modelrobustin b x r then it is pac robust in b x r .
proof.
with confidence we have p x p x tildewide x p tildewide x x which implies that fis pac robust in b x r .
square in this work wo focus on the following problem given a dnn f a nl norm ball b x r a significance level and an error rate we need to determine whether fis pac model robust.
beforeintroducingourmethod werevisitpacrobustness def.
.
in our pac model robustness theory.
statistical methods like infer pac robustness from samples and their classification output in the given dnn.
in our pac model robustness framework these methods simplify the model to a function b x r where0referstothecorrectclassificationresultand1awrongone and infer the pac model robustness with the constant function tildewide x 0o nb x r as the model.
in the model is modified to a constantscoredifferencefunction tildewide c.thesemodelsaretooweak todescribethebehaviourofadnnwell.itcanbepredictedthat ifwelearnapacmodelwithanappropriatemodel theobtained tusjdu robustnesspac model robustness ogiven and figure property space of pac model robustness.
pac modelrobustnesspropertywillbemoreaccurateandpractical and this will be demonstrated in our experiments.
methodology inthissection wepresentourmethodforanalysingthepac model robustnessofdnns.
theoverallframeworkis showninfig.
.in general our method comprises of three stages sampling learning and analysing.
s1 we sample the input region b x r and obtain the corresponding values of the score difference function .
s2 we learn apacmodel tildewide x x of the scoredifference function from the samples.
s3 weanalysewhether tildewide x isalwaysnegativeintheregion b x r by computing its maximal values.
fromthedescriptionabove weseeitisablack boxmethodsince we only use the samples in the neighbour and their corresponding outputs to construct the pac model.
the number of samples is independentofthestructureandthesizeoforiginalmodels which willbringthe goodscalabilityandefficiency.
moreover weareessentiallyreconstructingapropermodeltodepictthelocalbehavior oftheoriginalmodel.comparedwiththestatisticalmethods the pacmodelcanpotentiallyextractmoreinformationfromthescore differences of these samples which supports us to obtain more accurate results.
note that our framework is constructive and the pac model andits maximalpoints intheregion willbe constructedexplicitly during the analysis.
then we can obtain the maximal values of the pacmodel andinferthattheoriginaldnnsatisfiesthepac model robustness when all maximal values are negative.
thus deeppac can be considered as a sound approach to verify the pac model robustness.
.
learning a pac model to obtain a pac model of the original score difference function x we first create a function template and then determine its parameters by model learning from the samples.
hereafter we set fto be the set of affine functions and consider the pac model tildewide x tobeanaffinefunctionwithboundedcoefficients.areason forchoosinganaffinetemplateisthatthebehavioursofadnninasmall l norm ball b x r are very similar to some affine function due to the almost everywhere differentiability of dnns.
in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards practical robustness analysis for dnns based on pac model learning icse may pittsburgh pa usa dnn score differenceoutput input pac modelmaximal valuesmodel learningsampling analysing pac model robustnesssubsect.
.2subsect.
.
.
figure3 frameworkofpac modelrobustnessanalysisbase on model learning otherwords anaffinefunctioncanapproximatetheoriginalmodel well enough in most cases to maintain the accuracy of our robustnessanalysis.specifically forthe ithdimensionofthednnoutput layer weset tildewide i x c latticetop ix ci ci 1x1 ci mxm.withextracting a set of kindependent and identically distributed samples x b x r weconstructthefollowingoptimisationproblemfor learning the affine pac model tildewide x .
min 0 s.t.
c latticetop ix i x x x i nequal lscript l ci k u i nequal lscript k ... m. in the above formulation of pac model learning the problem boils down to a linear programming lp optimisation.
we reuse to denote the optimal solution and tildewide ito be the function whose coefficients ciare instantiated according to the optimal solution .
specifically we aim to compute a pac model tildewide of .
by theorem2.
the confidence and the error rate can be ensured by a sufficientlylargenumberofsamples.namely tomake holdwith confidence1 wecanchooseany k ln1 m n corresponding to the number of the variables in .
forfixed and thenumberofsamples kisino mn sothe lp problem containso mn variables and o mn2 constraints.
therefore the computational cost of the above lp based approach canquicklybecomeprohibitivewithincreasingthedimensionof input and output.
example4.
.
forthemnistdatasetthereistheinputdimension m and output dimension n .
even for .
.
weneedtosolveanlpproblemwith7 065variables andmorethan630 000constraints whichtakesuptoomuchspace memory out with 10gb memory .
to further make the pac model learning scale better with highdimensionalinputandoutput wewillconsiderseveraloptimisations to reduce the complexity of the lp problem in section .
.
fromthelpformulationineq.
itcanbeseenthatthepac model learning is based on the sampling set xinstead of the norm ballb x r .
that is though in this paper for simplicity b x r is assumed to be an l norm ball our method also works with lp norm robustness with p .
.
analysing the pac model we just detailed how to synthesise a pac model tildewide of the score difference function .
when the optimisation problem in is solved we obtain the pac model tildewide x x of the score differencefunction.namely tildewide x approximatestheupper lower boundofthescoredifferencefunction withthepacguarantee respectively.
as aforementioned all maximal values of tildewide being negative implies the pac model robustness of the original dnn.
according to the monotonicity of affine functions it is not hard to computethe maximumpoint x i of tildewide i x in theregion b x r .
specifically for tildewide i x in the form of c0 summationtext.1m j 1cjxj we can infer its maximum point directly as x i j braceleftbigg xj r cj xj r cj .
notethatthechoiceof x i jisarbitraryforthecase cj .here we choose xj ras an instance.
then let xbe the x i corresponding to the maximum tildewide i x i and the pac model robustness of the original dnn immediately follows if tildewide x .
besides each x i is a potential adversarial example attacking the original dnn with the classification label i which can be further validated by checking the sign of i x i .
example .
.
we consider the neural network in fig.
.
given an input x latticetop the classification label is cf x .
the networkisrobustif f2 x f1 x forx b x orequivalently f2 x f1 x .thus ourgoalistoapplythescenarioapproachto learnthescoredifference x f2 x f1 x .inthisexample we taketheapproximatingfunctionoftheform tildewide x c0 c1x1 c2x2 with constant parameters c0 c1 c2 to be synthesised.
for ease of exposition we denote c c1 c2 c3 latticetop.
we attempt to approximate x by minimising the absolute difference between it and the approximating function tildewide x .
this process can be characterised as an optimisation problem min c s.t.
tildewide x x x c .
to applythe scenarioapproach wefirst needto extract aset of k independentandidenticallydistributed samples x and thenreducetheoptimisationproblem tothelinearprogramming problem by replacing the quantifier x 2with x x in the constraints.
theorem .5indicates that at least ln1 samples are required to guarantee the error rate within i.e.
p tildewide x x with confidence .
taking the error rate .
and the confidence .
we need at least k samples in .
by solving the resultinglinearprogramagain weobtain c0 .
c1 .
c2 .
and .
.
for illustration we restrict x1 and depict the functions and tildewide in fig.
.
our goal is to verify that the first output is always larger than the second i.e.
x f2 x f1 x .
as described above according to the signs of the coefficients of tildewide we obtain authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa renjue li et al.
.
.
figure the functions and tildewide inx2are depicted by fixing x1 .itismarkedredwhere x isnotboundedby tildewide x .
that tildewide x attains the maximum value at x latticetopin .
therefore the network is pac model robustness.
.
strategies for practical analysis we regard efficiency and scalability as the key factor for achieving practical analysis of dnn robustness.
in the following we propose three practical pac model robustness analysis techniques.
.
.
component basedlearning.
asstatedinsection .
thecomplexityofsolving canstillbehigh soweproposecomponentbased learning to reduce the complexity.
as before we use tildewide i to approximate i x fi x f lscript x for eachiwith the same template.theideaistolearnthefunctions 1 ... nseparately andthencombinethesolutionstogether.insteadofsolvingasingle large lp problem we deal with n individual smaller lp problems eachwith o m linearconstraints.asaresult wehave tildewide i x i i x from which we can only deduce that p parenleftbig logicalanddisplay.
i nequal lscript tildewide i x i x i parenrightbig n withtheconfidencedecreasingtoatmost1 n .toguarantee theerrorrateatleast andtheconfidenceatleast1 weneedto recomputetheerror between tildewide x and x .specifically wesolve the following optimisation problem constructed by resampling min s.t.
tildewide i x i x x x i nequal lscript.
where xis a set of ki.i.d samples with k ln1 .
applying theorem .5again we have tildewide x x as desired.
we have already relaxed the optimisation problem into a familyof n small scalelpproblems.if nistoolarge e.g.for imagenetwith1000classes wecanalsoconsidertheuntargeted scoredifferencefunction u x f lscript x maxi nequallfi x .byadopting the untargeted score difference function the number of the lp problems is reduced to one.
the untargeted score difference function improves the efficiency at expense of the loss of linearity which harms the accuracy of the affine model.
.
.
focused learning.
in this part our goal is to reduce the complexity further by dividing the learning procedure into two phases with different fineness i in the first phase we use a small set of samplestoextractcoefficientswithbigabsolutevalues andii thesecoefficientsare focused inthesecondphase inwhichweusemoresplit learn split split learn linear programming c fixed coefficients margin linear programmingpac model phase ii phase i figure a workflow of the stepwise splitting procedure.
the red color indicates the significant grids whose coefficients will be further refined while the yellow color indicates the grids whose coefficients have been determined.
samples to refine them.
in this way we reduce the number of variablesoverall andwecallit focusedlearning whichnamelyrefers to focusing the model learning procedure on important features.
it is embedded in the component learning procedure.
the main idea of focused learning is depicted below firstlearningphase weextract k i.i.d.samplesfromthe input region b x r .
we first learn ion thek samples.
thus ourlpproblemshave o k constraintswith o m variables.
for large datasets like imagenet the resulting lp problemisstilltoolarge.weuseefficientlearningalgorithms such as linear regression ordinary least squares to boost the first learning phase on these large datasets.
key featureextraction after solvingthe lp problem or the linear regression for large datasets we synthesise tildewide ias the approximating function.
let keyfi x1 ... xm denotethesetofextractedkeyfeaturesforthe ithcomponent corresponding to the coefficients with the largest absolute values in tildewide i. focused learning phase we extract k i.i.d.
samples from b x r .
for these samples we generate constraints only for our key features in keyfi by fixing the other coefficients using those in tildewide i and thus the number of the undeterminedcoefficientsisboundedby .bysolvinganlpproblemcomprisedoftheseconstraints wefinallydeterminethe coefficients of the features in keyfi .
wecandeterminethesamplesize k andthenumberofkey features satisfying k ln1 which can be easily inferred from theorem .
.
it is worth mentioningthat focusedlearningnotonlysignificantlyimprovesthe efficiency butitalsomakesourapproachinsensitivetosignificance level and error rate because the first phase in focused learning canprovideahighlyprecisemodel andasmallnumberofsamplesaresufficienttolearnthepacmodelinthesecondphase.thiswill be validated in our experiments.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards practical robustness analysis for dnns based on pac model learning icse may pittsburgh pa usa .
.
stepwise splitting.
when the dimensionality of the input spaceisveryhigh e.g.
imagenet thefirstlearningphaseoffocusedlearningrequiresconstraintsgeneratedbytonsofsamples tomakeprecisepredictionsonthekeyfeatures whichisveryhard andevenimpossibletobedirectlysolved.forachievingbetterscal ability wepartitionthedimensionsofinput ... m intogroups gk .inanaffinemodel tildewide i forthevariableswithundeterminedcoefficients in each certain group gk they share the same coefficient ck.
namely the affine model has the form of summationtext.
k parenleftbig ck summationtext.
i gkxi parenrightbig .
then a coarse model can be learned.
we compose the refinement into the procedure of focused learningaforementioned seefig.
.specifically afteracoarsemodel islearned wefixthecoefficientsfortheinsignificantgroupsand extract the key groups.
the key groups are then further refined and their coefficients are renewed by learning on a new batch of samples.werepeatthisprocedureiterativelyuntilmostcoefficients oftheaffinemodelarefixed andthenweinvokelinearprogrammingtocomputetherestcoefficientsandthemargin.thisiterative refinementcanberegardedasmulti stagefocusedlearningwith different fineness.
inparticular foracolourimage wecanusethegridtodivideits pixels into groups.
the image has three channels corresponding to thered greenandbluelevels.asaresult eachgridwillgenerate groups matching these channels i.e.
gk r gk g andgk b. here we determine the significance of a grid with the l2 norm of the coefficients of its groups i.e.
c2 k r c2 k g c2 k b .
then the key groups saying corresponding to the top significant grids will befurtherrefinedinthesubsequentprocedure.onimagenet we initiallydividetheimageinto32 32grids witheachgridofthe size .
in each refinement iteration we split each significant grid into sub grids see fig.
.
we perform iterations of such refinementanduse20000samplesineachiteration.anexample onstepwisesplittingofanimagenetimagecanbefoundinfig.
in sect.
.
.
experimental evaluation in this section we evaluate our pac model robustness verifica tion method.
we implement our algorithm as a prototype calleddeeppac.
its implementation is based on python .
.
.
we use cvxpy as the modeling language for linear programming and gurobi as the lp solver.
experiments are conducted on a windows pc with intel i7 gtx 1660ti and 16g ram.
three datasets mnist cifar and imagenet and dnn models trained from them are used in the evaluation.
the details are in tab.
.
we invoke our component based learning and focusedlearning forall evaluations and applystepwise splitting for the experiment on imagenet.
all the implementation and data used in this section are publicly available1.
in the following we are going to answer the research questions below.
rq1 can deeppac evaluate local robustness of a dnn more effectively compared with the state of the art?
rq2 can deeppac retain a reasonable accuracy with higher significance higher error rate and or fewer samples?
network defense param source mnistfnn1 .86k fnn2 .71k fnn3 .41k fnn4 .01k fnn5 .61k fnn6 .65m cnn1 .61k erancnn2 diffai cnn3 pgd cnn4 .59m cnn5 pgd .
cnn6 pgd .
cifar 10cnn1 pgd .32k cnn2 pgd .07mcnn3 pgd 255resnet18 .17m resnet50 .52m resnet152 .16m imagenetresnet50a pgd .56m madryresnet50b pgd table1 datasetsanddnnsusedinourevaluation.theconvolutionalneuralnetworks cnn formnistandcifar are from eran .
the resnet50 networks for imagenetare from the python library robustness produced bymadrylab.
the rest networks are trained by ourselves.
rq3 isdeeppacscalabletodnnswithcomplexstructureand high dimensional input?
rq4 isthereaunderlyingrelationbetweendnnlocalrobustness verification and dnn testing especially the test selection ?
.
comparison on precision we first apply deeppac for evaluating dnn local robustness by computingthemaximumrobustnessradiusandcomparedeeppac with the state of the art statistical verification tool provero whichverifiespacrobustnessbystatisticalhypothesistesting.a dnnverificationtoolreturnstrueorfalseforrobustnessofadnn givenaspecifiedradiusvalue.abinarysearchwillbeconducted forfindingthemaximumrobustnessradius.forbothdeeppacandprovero we set the error rate .
and the significance level .
.
we set k and k for deeppac.
in addition we apply eran and pgd to bound the exact maximum radius from below and from above respectively.
eranisastate of the artdnnformalverificationtoolbasedon abstract interpretation and pgd is a popular adversarial attackalgorithm.
in the experiments we use the pgd implementationfrom the commonly used foolbox with iterations and a relative step size of .
which are suggested by foolbox as a defaultsetting.notethatexactrobustnessverificationsmttools like marabou cannot scale to the benchmarks used in our experiment.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa renjue li et al.
fnn fnn fnn fnn fnn fnn cnn cnn cnn cnn cnn cnn provero our deeppac eranpgd the ranges containing exact maximum robustness radii figure each dash represents the maximum robustness radius for an input estimated by deeppac blue or provero red while each bar white gives an interval containing the exact maximum robustness radius whose lower bound and upper bound are computed by eran and pgd respectively.
werunallthetoolsonthefirst12dnnmodelsintab.
1andthe detailedresultsarerecordedinfig.
.inallcases themaximumrobustnessradiusestimatedbytheproveroisfarlargerthanthose computed by other tools.
in most cases provero ends up with a maximum robustness radius over out of which is even larger than the upper bound identified by pgd.
this indicates that while a dnn is proved to be pac robust by provero adversarial inputs can be still rather easily found within the verified bound.
in contrast deeppacestimatesthemaximumrobustnessradiusmore accurately which falls in between the results from eran and pgd mostly.
since the range between the estimation of eran and pgd containstheexactmaximumrobustnessradius weconcludethat deeppac isa moreaccurate toolthan proverotoanalyse local robustness of dnns.
deeppac also successfully distinguishes robust dnn models from non robust ones.
it tells that the cnns especially the ones with defence mechanisms are more robust against adversarial perturbations.forinstance 24outof25imageshavealargermaximum robustnessradiusoncnn1thanonfnn1 and21imageshavea larger maximum robustness radius on cnn2 than on cnn1.
otherthanthemaximumrobustnessradiusforafixedinput the overallrobustness ofadnn subjecttosomeradius value canbe denotedbytherateoftheinputsbeingrobustinadataset called robustnessrate .infig.
weshowtherobustnessrateof100input images estimated by different tools on the cifar cnns.
here we setk and k .
provero similarly to the earlier experiment outcome results in robustness rate which is even higher than the upper boundestimation from the pgd attack and its robustness rate resulthardly changes when the robustness radius increases.
all suchcnn cifar 20406080100cnn cifar cnn cifar provero eran our deeppac pgd figure7 robustnessrateofdifferentcnnsundertheradius of and on cifar .
comparisons reveal the limitations of using pac robustness by provero that the verified results are not tight enough.
eran is a sound verification method and the robustness rate verifiedbyitisastrictlowerboundoftheexactresult.however this lower bound could be too conservative and eran quickly becomes not usable.
in the experiments we find that it is hard for eran to verify a robustness radius greater than or equal to out of .
deeppac verifies greater robustness rate and larger robustness radius withhighconfidenceandlowerrorrate.itsresultsfallsafelyinto the range bounded by eran and pgd.
we advocate deeppacasamorepracticaldnnrobustnessanalysistechnique.itisshown inourexperimentsthat thoughdeeppacdoesnotenforce100 guarantee itcanbeappliedintoawiderrangeofadversarialsettings in contrast to eran and the pac model verification results by deeppac can be more trusted in contrast to provero with quantified confidence in contrast to pgd .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards practical robustness analysis for dnns based on pac model learning icse may pittsburgh pa usa answer rq1 themaximumrobustnessradiusestimatedby deeppac is more precise than that by provero and our deeppac is a more practical dnn robustness analysis method.
.2deeppac with different parameters inthispart weexperimentonthethreekeyparametersindeeppac theerrorrate thesignificancelevel andthenumberofsamples k in the first learning phase.
the parameters and control the precision between the pac model and the original model.
the numberofsamples k determinestheaccuracyofthefirstlearning phase.
we evaluate deeppac under different parameters to check thevariationofthemaximalrobustnessradius.weseteither k 20000ork 5000inourevaluationandthreecombinationsof theparameters .
.
.
.
and .
.
.here wefixthenumberofkeyfeaturestobefifty i.e.
andcalculate the corresponding number of samples k in the focused learning phase.
theresultsarepresented intab.
.deeppacrevealssomednn robustnessinsightsthatwerenotachievablebyotherverification work.itisshownthat thednns theresnetfamilyexperimented canbemorerobustthanmanymaythink.themaximumrobustnessradiusremainsthesameorslightlyalters alongwiththeerrorrate andsignificance level varying.this observationalso confirms that the affine model used in deeppac abstraction converges well and the resulting error bound is even smaller than the specified large error bound.
please refer to sect.
.1for more details.
deeppac is also tolerant enough with a small sampling size.
when the number of samples in the first learning phase decreases fromk tok we can observe a minor decrease of the maximal robustness radius estimation.
recall that we utilisethelearnedmodelinthefirstphaseoffocusedlearningto extract the key features and provide coefficients to the less important features.
when the sampling number decreases the learned modelwouldbelesspreciseandthusmakevaguepredictionson key features and make the resulting affine model shift from the original model.
as a result the maximum robustness radius can be smaller when we reduce the number of sampling in the first phase.
inpractice asitisshownbytheresultsintab.
wedonotobserve a sudden drop of the deeppac results when using a much smaller sampling size.
answerrq2 deeppacshowsgoodtolerancetodifferentconfigurations of its parameters such as the error rate the significance level and the number of samples k .
.
scalability robustnessverificationis awell knowndifficultproblemon complex networks with high dimensional data.
most qualitative ver ification methods meet a bottleneck in the size and structure of the dnn.
the fastest abstract domain in eran is gpupoly a gpuacceleratedversionofdeeppoly.thegpupolycanverifya resnet18model onthecifar 10datasetwith anaveragetime of 1021secondsunderthesupportofannvidiateslav100gpu.to the best of our knowledge eran does not support models on imagenet which makes it limited in real life scenarios.
the statisticalinput image network andk .
.
.
.
.
.
20k 5k 20k 5k 20k 5k resnet18 resnet50 resnet152 resnet18 resnet50 resnet152 resnet18 resnet50 resnet152 resnet18 resnet50 resnet152 resnet18 resnet50 resnet152 table2 themaximumrobustnessradiusestimatedby deeppaconcifar 10datasetusingdifferentparameters i.e.significancelevel errorrate andthenumberofsamplesin the first learning phase k .
methodsalleviatethisdilemmaandextendtheirusefurther.the state of the artpacrobustnessverifierproveroneedstodraw samples for vgg16 and samples for vgg19 on average for each verification case on imagenet.
the average running time isnear .
secondsand .
seconds .
seconds per sample underthesupportofannvidiateslav100gpu.wewill showthatdeeppaccanverifythetighterpac modelrobustness on imagenet with less samples and time on much larger resnet50 models.
in this experiment we apply deeppac to the start of the art dnn with high resolution imagenet images.
the two resnet50networks are from the python package named robustness .
wecheckpac modelrobustnessofthetwodnnswiththesame radius4 outof255 .thefirstevaluationisonasubsetofimagenetimagesfrom10classes .thesecondoneincludesimagenetimagesofall1 000classesandtheuntargetedscoredifferencefunction is configured for deeppac.
to deal with imagenet the stepwise splitting mechanism in sect.
.
.3is adopted.
an illustrating example of the stepwise splitting is given in fig.
.
as we expect the splittingrefinementproceduresuccessfullyidentifiesthesignificant featuresofagolfball i.e.theboundaryandthelogo.itmaintains authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa renjue li et al.
pac model robust significant grids significance from to .
.
.
.
.
.
.
figure stepwise splitting procedures of deeppac illustrated by heatmaps of grid significance.
top significant grids are colored yellow in the heatmap which is split andrefined iteratively.
the margin of different refinement stage is under the heatmap.
the accuracy of the learned model with much less running time.
the results are shown in tab.
.
for the10 class setup we evaluatethe pac modelrobustness on images and it takes less than seconds on each case.
deeppacfindsout30and29casespac modelrobustforresnet50a and resnet50b respectively.
because the two models have been defensed whenweperformthepgdattack onlyoneadversarial examples were found for each model which means that pgd gives noconclusionfortherobustnessevaluationonmostcasesunder this setting.
for the class dataset the untargeted version of deeppachasevenbetterefficiencywiththerunningtimeofless than seconds each which mainly benefits from reducing the scoredifferencefunctiontotheuntargetedone.deeppacproves and out of cases to be pac model robust on the class setup respectively.forbothsetups deeppacuses121600samples to learn a pac model effectively.
method network robust min max avg targeted classes resnet50a .
.
.8resnet50b .
.
.
untargeted classes resnet50a .
.
.7resnet50b .
.
.
table the performance of deeppac analysing the two resnet50 models for imagenet.
robust represents the ro bustness rate.
min max and avg are the minimum maximum andaverageoftherunningtime second respec tively.
answer rq3 the deeppac robustness analysis scales well tocomplexdnnswithhigh dimensionaldatalikeimagenet whichisnotachievedbypreviousformalverificationtools.it shows superiority toproveroin bothrunningtime andthe number of samples.network deeppac eran provero fnn1 .
.
.
fnn2 .
.
.
fnn3 .
.
.
fnn4 .
.
.
fnn5 .
.
.
fnn6 .
.
.
cnn1 .
.
.
cnn2 .
.
.
cnn3 .
.
.
cnn4 .
.
.
cnn5 .
.
.
cnn6 .
.
.
table the pearson correlation coefficient between the maximum robustness radius estimation and the deepginiindex.thednnsaremarkedby iftheyaretrainedwith defense mechanisms.
.
relation with testing prioritising metric we also believe that there is a positive impact from practical dnn verification work like deeppac on dnn testing.
for example the tooldeepginiusesginiindex whichmeasurestheconfidenceof adnnpredictiononthecorrespondinginput tosortthetesting inputs.
in tab.
we report the pearson correlation coefficient between the deepgini indices and the maximal robustness radii obtained by deeppac eran and provero from the experiment in sect.
.
.
as in tab.
themaximumrobustnessradiusiscorrelatedtothe deepginiindex alargerabsolutevalueofthecoefficientimplies astrongercorrelation.
itrevealsthedatathathas lowprediction confidence is also prone to be lack robustness.
from this phenomenon webelieve deepginican bealso helpful indata selectionfor robustness analysis.
interestingly the maximum robustness radius computed by our deeppac has higher correlations with deepgini indexonthecnns whicharemorecomplex thanonfnns.furthermore deeppacshowsthestrongestcorrelationonthecnns trainedwithdefensemechanisms whilethecorrelationbetween provero or eran and deepgini is relatively weak on these networks.
intuitively complex models with defense are expected to be more robust.
again we regard this comparison result as theevidence from dnn testing to support the superior of deeppacover other dnn verification tools.
from the perspective of test ing technique it is promising to combine these two methods for achieving test selection with guarantee.
answer rq4 themaximumrobustnessradiusestimatedby deeppac eran andproveroareallcorrelatedtothedeepgini index where deeppac and deepgini show the strongest correlation on robust models.
.
case study verifying cloud service api to show the practicality of deeppac we apply it to analyse the robustnessofblack boxmodelsforreal worldcloudservices.the case we studyhere is the image recognition apiprovided by baidu authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards practical robustness analysis for dnns based on pac model learning icse may pittsburgh pa usa figure the original image left gain the score dandelion .
sky .
andtheadversarialexamplegainthescore s k y .
dandelion .
.
aicloud2 whichacceptsanimageandreturnsapairlistintheform of labeli scorei to indicate the top classes the input recognised to be.
we use the image of a dandelion as the input which is an official example in its illustration.
by setting .
and .
we verify the pac model robustnessforitstoplabel dandelion withintheradiusof5 .
a total of samples are utilised in the whole procedure.
by deeppac we obtain the pac model of the difference function but unfortunately its maximal value in the input l ball is larger than zero.asanintermediateoutput wegenerateapotentialadversarial example via the pac model.
by feeding it back into the model we checkedthatitisatrueadversarialexamplewith sky asitstop label see fig.
.
aninterestingobservationisthatthelabelsoutputbytheimage recognition api may be not independent.
for instance the class labels dandelion and plant may appear in the output list at the sametime andbothofthemcanbeconsideredcorrectlabels.therefore we believe that in the future new forms of dnn robustness propertiesalsoneedtobestudiede.g.
thesumoftheoutputscores forthecorrectlabels dandelion and plant shouldbelargerthan some threshold.
deeppac is a promising tool to cope with these emergingchallengeswhenconsideringreal worldapplicationsof dnn robustness analysis by conveniently adjusting its difference function.
related work herewediscussmoreresultsontheverification adversarialattacks and testing for dnns.
a number of formal verification techniques have been proposed for dnns including constraint solving abstractinterpretation layerby layerexhaustivesearch globaloptimisation convexrelaxation functionalapproximation reduction totwo playergames andstar set basedabstraction .
sampling based methods are adopted to probabilistic robustness verificationin .mostofthemprovidesounddnn robustness estimationin the formof a norm ball but typically for verysmallnetworksorwithpessimisticestimationofthenormball radius.bycontrast statisticalmethods more efficient and scalable when the structure of dnns is complex.theprimarydifferencebetweenthesemethodsanddeeppac is that our method is model based and thus more accurate.
we use samplestolearnarelativelysimplemodelofthednnwiththepac guarantee via scenario optimisation and gain more insights to the analysis of adversarial robustness.
the generation of adversarial inputs itself has been widely studied by a rich literature of adversarialattackmethods.somemostwell knownrobustnessattack methodsincludefastgradientsign jacobian basedsaliency mapapproach c wattack etc.thoughadversarialattackmethodsgenerateadversarialinputsefficiently theycannot enforceguaranteeofanyformforthednnrobustness.testingis stilltheprimaryapproachforcertifyingtheuseofsoftwareproducts andservices.
in recentyears significant work hasbeen done forthetestingfordnnssuchastestcoveragecriteriaspecialisedfor dnns and different testing techniques adopted for dnns .
in particular our experiments show that the results from deeppac are consistent with the dnn testing work for prioritising test inputs but with a stronger guarantee.
this highlights again that deeppac is a practical verification method for dnn robustness.
conclusion and future work weproposedeeppac amethodbasedonmodellearningtoanalyse the pac model robustness of dnns in a local region.
with the scenario optimisation technique we learn a pac model which approximates the dnn within a uniformly bounded margin with a pacguarantee.withthelearnedpacmodel wecanverifypacmodelrobustnesspropertiesunderspecifiedconfidenceanderror rate.
experimental results confirm that deeppac scales well on large networks and is suitable for practical dnn verification tasks.
as for future work we plan to learn more complex pac models rather than the simple affine models and we are particularly interestedinexploringthecombinationofpracticaldnnverification by deeppacanddnn testing methodsfollowing thepreliminaryresults.