online testing ofrestfulapis promisesand challenges albertomartin lopez alberto.martin us.es scorelab i3usinstitute universidadde sevilla seville spainsergiosegura sergiosegura us.es scorelab i3usinstitute universidadde sevilla seville spainantonio ruiz cort s aruiz us.es scorelab i3usinstitute universidadde sevilla seville spain abstract online testing of web apis testing apis in production is gaining traction in industry.
platforms such as rapidapi and sauce labs provideonlinetestingandmonitoringservicesofwebapis24 typicallybyre executingmanuallydesignedtestcasesonthetarget apis on a regular basis.
in parallel research on the automated generationoftestcasesforrestfulapishasseensignificantadvances in recent years.
however despite their promising results in the lab itisunclearwhetherresearchtoolswouldscaletoindustrialsize settings and more importantly how they would perform in an online testing setup increasingly common in practice.
in this paper we report the results of an empirical study on the use of automatedtestcasegenerationmethodsforonlinetestingofrestful apis.specifically weusedtherestestframeworktoautomatically generate and execute test cases in industrial apis for days non stop resultinginoveronemilliontestcases.toscaleatthis level we had to transition from a monolithic tool approach to a multi bot architecture with over bots working cooperatively in tasks like test generation and reporting.
as a result we uncovered about390kfailures whichweconservativelytriagedinto254bugs of which have been acknowledged or fixed by developers to date.amongothers weidentifiedconfirmedfaultsintheapisof amadeus foursquare yelp andyoutube accessedbymillionsof applicationsworldwide.moreimportantly ourreportshaveguided developersonimprovingtheirapis includingbugfixesanddocumentation updates in the apis of amadeus and youtube.
our results show the potential of online testing of restful apis as the next must have feature in industry but also some of the key challenges to overcome for its full adoption inpractice.
ccs concepts informationsystems restfulwebservices software andits engineering software testinganddebugging .
keywords rest webapi bot black box testing onlinetesting permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 14 18 singapore singapore associationfor computing machinery.
acm isbn ... .
format albertomartin lopez sergiosegura andantonioruiz cort s. .online testing of restful apis promises and challenges.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november 14 singapore singapore.
acm newyork ny usa 13pages.https introduction webapisallowsystemstointeractwitheachotheroverthenetwork typically using web services .
most modern web apis comply with the rest architectural style being referred to as restfulwebapis.restfulwebapis provideaccesstodata andservicesbymeansofcreate read update anddelete crud operations over resources e.g.
a video in the youtube api or aplaylistinthespotifyapi .restfulapisareubiquitousin themodern daysociety publicinstitutionssuchastheamerican government expose their existing assets as a set of restful apis softwarecompaniessuchasmicrosoft andnetflix basemanyoftheirsystemscommunicationsontheirrestfulapis evennon softwarecompaniessuchasmarvel provideapisfor developerstobuildapplicationsontopofthem.theimportanceand pervasivenessofwebapisisalsoreflectedonthesizeofpopular apirepositoriessuchasprogrammableweb andrapidapi whichcurrently index over 24kand30kapis respectively.
checkingthecorrectfunctioningofwebapisiscritical.asingle bug in an api may affect tens or hundreds of other services leveragingit.inthisscenario testthoroughnessandautomation are of utmost importance.
industry standard tools and libraries for testing restful apis such as postman and rest assured automatetestexecution buttestcasesstillneedtobemanuallyimplemented.industrialeffortsare alsoshifting towardthemodelof onlinetesting whereapisarecontinuouslytestedwhileinproduction.thisisthecaseofplatformssuchasdatadog rapidapi testing and sauce labs which offer web api testing and monitoringasaservice.underthismodel apisaretestedandmonitored with pre defined api requests and output assertions following a black box approach.
customers may choose among differentpricingplans determiningthenumberofapicallsper day month theintegration with ci cdplatforms and thetype of notifications among others.
these platforms completely automate testcaseexecution buttheirgenerationstillrequiresmanualwork eitherfor the creation oramplification of test cases.
research on the automated generation of test cases for restful apishasseensignificantadvancesinrecentyears.mostapproaches follow a black box strategy where test cases are automatically derivedfromthespecificationoftheapiundertest typicallyinthe openapi specification oas format .
these approaches are 408esec fse november14 18 singapore singapore alberto martin lopez sergio segura andantonio ruiz cort s capable of automatically generating sequences of http requests sendingthemtotheapiandassertingthevalidityoftheresponses.
test data generation strategies are varied while test oracles are mostly limited to checking the conformance with the api specification and the absence of api crashes.
despite their promising results research approaches have typically been evaluated in controlled environments with a few open source apis a limited number of test cases orwithintheboundariesofasingleorganization .
thus thereisnoevidenceoftheirapplicabilityandgeneralizability to industrial settings and more importantly of how they would fit into the online testing model increasingly found in practice.
hence asamatteroffact thereisacleargapbetweenindustrial solutions mostly concerned with test case execution and research approaches focusedonautomatedtestcasegeneration butwith evaluation results limitedto lab settings.
in this paper we report the results of an empirical study on the useofautomatedtestcasegenerationmethodsforonlinetesting of restful apis.
specifically we automatically generated and executed test cases on industrial apis during the course of days non stop.
we assessed different black box testing strategies includingfuzzing adaptiverandomtesting andconstraint based testing among others resulting in over one million test cases.
the evaluation was conducted using restest an open source testing framework for restful web apis.
however to scale at this level we had to transition from a monolithic tool approach to a multi bot architecture resulting in over bots highly cohesive andautonomousprograms workingcooperativelyintaskssuch as test case generation test case execution and test case reporting.
we uncovered about 390k test failures which we automatically clusteredinto4 818potentialfaults.afterasystematicselectionand manualinspectionof586faultclusters weconservativelyidentified reproducible issues in all the apis under test of which have been acknowledged or fixed by the api developers to date .
thebugsdetectedarevaried includingapicrashescausedbothby valid andinvalidapiinputs unexpectedclienterrors andsecurityvulnerabilities amongmanyothers.ourbugreports led to fixes in the documentation and the implementation of theapisofyoutubeandamadeus usedbymillionsofusersworldwide.
the results provide helpful insights on the fault detection capabilityofonlinetestingofrestfulapisaswellasitslimitations includingits high computational costand thedebugging effortrequired.
we also report the differences observed among different testing techniques and how these techniques complement each other for finding more and more varied bugs.
overall our work contributestonarrowingthegapbetweenresearchandpracticeon online testing of restful apis showing the lights and shadows of currenttest casegenerationtechniques at scale.
insummary afterdiscussingrelatedwork section andpresentingtheempiricalstudyperformed section thispapermakes the following originalresearchandengineeringcontributions anassessmentonthefailureandfaultdetectioncapability ofonlinetestingofrestfulapis sections .1and4.
.
acomparisonoftestdataandtestcasegenerationtechniques for onlinetestingofrestfulapis section .
.
asetofproblemsuncoveredbyonlinetestingin13industrial apis including254reproduciblebugs section .
alistofchallengesfortheadoptionoftestcasegeneration techniques for onlinetestingof restfulapis section .
a publicly available implementation of a multi bot architectureforonlinetestingofrestfulapis andadatasetofover 1mtest casesand390k test failures .
finally we address threats to validity in section 7and conclude the paper insection .
related work .
automated testingofrestful apis research on the automated generation of test cases for restful apishasthrivedinrecentyears.mostapproachesfollowablackbox approach where test cases are automatically derived from the api specification typically in the openapi specification oas format .anoasdocumentrepresentsacontractonhowarestful apimaybeused i.e.
itdescribestheapiintermsoftheallowedinputs httprequests andoutputs httpresponses inamachinereadableformat.givenanoasdocument currenttechniquesautomatically generate pseudo random test cases sequences of http requests and test oracles assertions on the http responses.
approaches mainly differ in the way in which they generate input data and test cases.
regarding test data generation strategies these include using default and example values fuzzing dictionaries purely random inputs perturbed data custom datageneratorsand datadictionaries dataextracted from knowledge bases e.g.
dbpedia and contextual data i.e.
extracted from previous api responses .
test case generationstrategiesincluderandomtesting parametersareassigned valuesrandomly adaptiverandomtesting aimingto diversify test cases as much as possible and constraint based testing parameters areassignedvaluessuch thatinter parameter dependencies are satisfied .
in terms of test oracles these are mainly limited to checking the absence of api crashes statuscodes andtheconformancewiththespecification.otheroracles include checking thestatus code metamorphicrelations andsecurity properties .
white boxtechniques require access tothecodeoftheapiundertestandarefarlesscommonthanblackboxtechniques.existingapproaches employsearch algorithmsto maximizecode coverageandfaultdetection .
related approaches on the automated generation of test cases for restful apis have beenmostly evaluatedin labsettings using a few open source apis a limited number of testcases up to a few thousands during a limited amount of time up to a few hours or within the boundaries of a singleorganization .
incontrast our workcomplements previousworkbyassessingthestrengthsandlimitationsofdifferent state of the arttestcasegenerationtechniquesinanonlinetesting scenario.specifically weautomaticallygeneratedandrantestcases on13industrialapiswithmillionsofusersworldwideduring15 daysnon stop.thisprovidesanovelperspectiveonthepotential ofautomatedtestcasegenerationtechniquesforrestfulapisin practice we uncovered uniquebugs in industrial apis ofwhichhavebeenacknowledgedorfixedbydeveloperstodate and some of which have led to changes in the apis of amadeus 409online testingof restful apis promises andchallenges esec fse november14 18 singapore singapore andyoutube.moreimportantly ourworkopensnewpromising researchdirections toaddresssome of the challenges identified in areas such as debuggingandtestingas aservice.
.
online testing onlinetestingreferstothetestingperformedonproductionsoftware as opposed to offline testing which is done on a development environment before the software is released.
online testingcanberegardedbothasa passivemonitoringactivity or as anactivetesting process .
in the former the system is observed under real world usage aiming to detect inconsistencies or anomalies .in thelatter thesystem iscontinuously stimulated with new test cases possibly derived from the feedback obtainedfromprevioustestresults .inthispaper weadopt theseconddefinitionandrefertoonlinetestingasanactivetesting process consisting in generating and executing test cases i.e.
api requests inthe apis inproduction.
online testing platforms for web apis are gaining popularity in industry.
these platforms provide continuous testing and monitoringcapabilitiesasaservice withdifferentpricingplansdetermining featuressuchasthetestexecutionfrequency theautomationdegree and the number of users among others.
datadog is a monitoringplatformfortrackinganapi savailabilityandresponsetime by periodically executing a set of manually configured api calls.
rapidapi testing and sauce labs formerly api fortress allow to create functional tests composed of one or more http requests with custom assertions inthehttp responses.
the data used in the requests can be static contextual i.e.
obtained from a previous response or random based on a data category e.g.
yellow forthecategory color .however testcasesmustbeindividuallycreated rapidapi oramplifiedbasedonatemplate sauce labs .
the same test cases are executed continuously at regular intervals.intheresearcharena somepapersaddressonlinetesting ofrestfulapis i.e.
testingwebapisinproduction but with a limited number of test cases or during a few hours and typicallyusingasingletestingtechnique.onlinetestinghasalso been applied to service oriented architectures soa including non functional testing and other less related domains such as mobile applications embedded systems andsoftware product lines among others.
compared to prior work we report the first empirical study on theuseofautomatedtestcasegenerationtechniquesforonlinetesting ofrestful apis inindustrial likesettings.thisresembles the modeloftestingasaserviceofferedbypopularindustrialplatforms.
thisallowedustostudythefailureandfaultdetectioncapability ofdifferentblack boxtestcasegenerationtechniquesovertime days aswellassomeofthekeychallengestoovercomefortheir adoption inpractice.
empiricalstudy inthissection wereporttheresultsofourstudyononlinetestingof restful apis including both automated generation and execution oftest cases.
.
research questions we aim to answer the following researchquestions table restful apis under test.
o operations p parameters r read w write.
api category serviceundertest o p r w amadeus travel hotelsearch x dhl shipping service point search x fdic banking all x foursquare social mapping venuessearch x languagetool languages textproofread x marvel entertainment characters x ohsome mapping all x omdb media all x restcountries reference all x spotify media playlists x x stripe financial products x x yelp recommendations businesses search x youtube media comments x x youtube media search x total rq1 what is the failure detection capability of online testing of restful apis?
we aim to investigate the number and types of failures detectedover time.
rq2 what is the fault detection capability of online testing of restful apis?
out of all the failures uncovered we aim to find out the number andtypes of uniquefaultsdetectedover time.
rq3 whatarethedifferencesobservedbetweendifferenttesting approaches?
we wish to compare different testing strategies in terms of api coverage and fault detection capability aiming to understandhowdifferentapproachescomplementeachother.
.
apis undertest table1depicts the restful apis under test.
for each api we specifyitsname theservicetested all ifthewholeapiistested the number of operations and parameters tested columns o and p respectively and whether it provides read and write operations columns r and w respectively .wetestthe searchandcommentsservicesofyoutubeasdifferentapis asdonebyprevious authors since they are subject to different testing strategies e.g.
the former is a read only api while the latter provides write functionality .
our benchmark comprises large and complex real worldapistestedbypreviousauthors which includes both commercial apis used by millions of users worldwide e.g.
spotify and yelp but also industrial size open source apis e.g.
ohsome1and languagetool .
we add another api to our benchmark fdic federal deposit insurance corporation as a good representative of complex apis developed by public institutions i.e.
theamericangovernment .theoasspecificationsofthe apis required to test them were obtained from the apis websites or from public repositories such as apis.guru .
exceptionally foursquare restcountriesandyelpdidnothavepublicoasdocuments sowereusedthemfromourpreviouswork ensuring they accurately reflectedthe api documentations.
overall we selected apis from varied application domains whichcomprehend189operationsand1 694inputparameters.most of the apis tested provide only read operations.
this is a common 1ohsome developers showed interest in our preliminary results and deployed an api instance identical to the production one where we could run our experiments without restrictions.
410esec fse november14 18 singapore singapore alberto martin lopez sergio segura andantonio ruiz cort s table taxonomyoftestbotsandtotal bots used pertype according to theinclusion criteriaforour experiments.
botdimension botid botname description inclusioncriterion total write safetyr read only testsonlyread operations i.e.
httpgetrequests the apicontainsread operations rw read write testsbothread and writeoperations e.g.
post and delete requests the apicontainswriteoperations testdata generation techniquefd fuzzing dictionaries uses type awarefuzzing dictionariesand malformed strings for all request parameters allapis cdg customized datageneratorsparametersareconfigured with customized datagenerators e.g.
ageneratorofdates in yyyy mm dd format allapis dp dataperturbation makesaminor change tothe dataused inan apirequest tomake it invalid allapis sd semantically related datauses conceptsrelated tothe semantics ofthe parameters extracted from knowledge bases like dbpedia the apicontainsparametersfrom which to extractsemantically related concepts5 cd contextualdata uses values observed inprevious apiresponsesthe apiresponsesinclude reusable datafor subsequentrequests10 testcase generation techniquert randomtesting parametersareassigned values randomly allapis cbt constraint based testing constraintsolvers areused tosatisfyall inter parameter dependencies ofthe api the apicontainsinter parameter dependencies art adaptive randomtesting similar tort or cbt butaimingtogenerate asdiversetest cases aspossible the apicontainsat leastone operation with morethan10parameters9 art bots employrt or cbt depending onwhether the apihas inter parameter dependencies or not asabasis togenerate test casecandidates from which the most diversetestcases areselected .
pattern in commercial apis where api users can only query existing data e.g.
characters from marvel comics and this data can onlybemodifiedbytheapiproviders.however wealsotestserviceswhosemainfunctionalityisinvokedviawriteoperations e.g.
proofreadingatextinlanguagetool andevenserviceswhichrequirestatefulinteractions e.g.
inspotify creatingaplaylist adding songsto it anddeleting it .
.
experimentalsetup inwhatfollows wedescribethetoolingusedandhowitwasconfigured and we explain several considerations to answer the research questions.alltheexperimentswereperformedinasingle virtual machine equipped with 512gb ram cpu cores and 2tb hdd running redhatenterprise linux java 8andnodejs .
.
.
testing ecosystem.
for the generation and execution of test cases we used restest anopen source framework integrating varied test case generation strategies a key requirement for our study.
after somepreliminaryexperiments we soon realizedthat themonolithicapproachofthetool combiningtestcasegeneration executionandreportinginthesameprocess wouldhinder the development and deployment of the application at the desired scale.inspiredbythewesplatformoffacebook weconceived a novel architecture decoupling the key parts of the testing process into highly cohesive and autonomous programs called bots illustrated in figure .
bots can be independently developed and deployed using different technologies.
we distinguish two types of bots input and output bots.
input bots support the generation and execution of test cases.
output bots are responsible for analyzingandleveragingthetestoutputs.botsarestarted stoppedand monitored automaticallybya controller component and they can optionallyinteractwitheachother e.g.
bytriggeringtheupdate oftest reports.
we devise two types of input bots test bots which generate and executetestcases and garbagecollectors whichdeleteresourcescreated by test bots.
garbage collectors are implemented in javascript and testbotsareimplemented usingrestest.
asillustrated intable2 test bots are classified along three dimensions namely write safety which concerns the types of operations invoked by the bot test data generation technique which refers to the input figure testing ecosystem architecture.
data usedin the apirequests and test case generation technique whichrelatestohowapirequests i.e.
testcases arebuilt.testbots arecreatedbycombiningthesethreedimensionsinanyway.as an example r fd rt represents a read only r bot that performs fuzzing i.e.
random testing rt with fuzzing dictionaries fd .
in total we have write safety test data generation technique testcasegenerationtechnique garbagecollector 31types ofinputbots.
regardingoutputbots weusedboth test reporters tr whichgenerategraphicaltestreportswiththeallure test reporting framework andtest coverage computers tcc whichcomputethetestcoverageachievedbytestbotsaccording to standardcoveragecriteriafor restfulwebapis .
.
.
bots selection criteria.
we selected a subset of bots for testing each api based on their potential applicability for example read writebotsareonly applicabletoapiswithwriteoperations.
table2illustrates the inclusion criteria for selecting test bots in theapisundertest andhowmanybotsofeachtypewereselected.
according to these criteria we selected test bots most of which areread only bots use customized data generators and applyrandom testing whereas read write bots botsthatuse semantically relateddata andbotsthatapply adaptiverandom testing are the least common.
we may remark that we do not aim at a rigorous comparison of testing techniques.
instead weaimtoassesshowdifferenttestgenerationstrategiesperform inanonlinescenarioandhowtheycomplementeachother.table shows the full list of test bots used in our experiment.
besides test bots we configured three garbage collectors for the apis of spotify stripe and youtube and one test reporter and test coverage 411online testingof restful apis promises andchallenges esec fse november14 18 singapore singapore computerpertestbot.asillustratedinfigure weconfigureda total of test bots garbage collectors test reporters testcoveragecomputers 228bots.
.
.
botsconfiguration.
alltestbotswereconfiguredtocontinuouslygenerateandexecutetestcasesduring15days 378hours .
bots were configured to comply with the quotas imposed by the apis.
quotas describe the limitations of use for a fixed period of time e.g.
5krequests dayinyelp .torespectquotas test botsworkbyiterations.ineveryiteration theygenerateasetof test cases execute them and sleep for a given time.
we configured alltestbotsofthesameapitogeneratethesamenumberofapi calls periteration however the sleep time between iterations was adjustedaccordingtothetestdatagenerationtechniqueusedby the bot.
fd fuzzing dictionaries and dp data perturbation test bots sleep four times longer than the rest therefore they generate fourtimeslessapicalls.thisisbecausethesebotsarenotexpected togeneratevalidtestcasesthatachieveahighapicoverage and so theyare assigned a lowertest budget .
the specific configurationusedforeachbot i.e.
testcasesperiterationandsleep time isavailable inthe supplementary materialofthe paper .
test reporters and test coverage computers are invoked after every test bot iteration continuously updating test reports and coverage.garbagecollectorsareexecutedaftertestbotsarestopped thus deleting allresourcescreatedduringthe experiments.
.
.
test oracles.
to detect failures we consider six test oracles namely 5xxstatuscodes servererrors returnedwhensendingavalidrequest f 5xxv 5xxstatuscodesreturned when sending an invalid request f 5xxi 2xx status codes successfulresponses withrequestsviolatingsomeparameterconstraint f 2xxp 2xx statuscodeswith requests violating some interparameterdependency f 2xxd 400statuscodes client errors withvalidrequests f 400 and disconformitieswiththe oas specification of the api f oas .f 5xxvandf 5xxifailures are due to server errors and so they can reveal api crashes statuscodes ortemporaryoutages 503statuscodes amongothers.f 2xxpandf 2xxdfailures reveal inconsistencies in the form of invalid api inputs which expect a 4xx client error response wrongly handled as valid since they obtain a 2xx successful response .
for instance if a required parameter is not included in an api call it should notobtain a 2xx status code f 2xxp .
likewise iftwomutuallyexclusiveparametersarebothincludedinanapi call itshould notobtaina2xxstatuscode f 2xxd .f 400failures revealunexpectedclienterrors 400statuscode withapicallsthat arevalid thereforetheyshouldobtaina2xxsuccessfulresponse.
lastly f oasfailuresrevealconformanceerrors forexample an api response not including a property defined as required in the api specification.
itis worthclarifying that not all testoraclesareavailabletoall test bots and therefore different bots uncover different types of failures.
in particular disconformities with the api specification f oas and 5xx status codes f 5xxvandf 5xxi can be uncovered by any bot.
on the other hand to detect an unexpected client error f 400 theapirequestmustbevalid i.e.
itmustusevalid input data.onlycdgbots canuncover this typeoffailures since theyalwaysusevaliddataandthereforeexpectsuccessfulapiresponses.
while sd and cd bots i.e.
semantic and contextual databots also have potential to uncover these faults they are reported as warnings since they may use invalid data unintentionally for instance when extracting country names instead of country codes from a knowledge base .
regarding f 2xxpandf 2xxdfaults they can only be detected by dp bots since they purposely manipulatetheinputdatausedtomakeitinvalid byviolatingparameter constraints or inter parameter dependencies so they expect client errorresponses.
.
.
evaluation metrics.
we used the following metrics total numberoffailuresandfaultsdetected overallandclassifiedbytype i.e.
accordingtotheoracleviolated failureandfaultdetection ratio fdr and ftdr respectively average percentage of faults detected apfd and apitestcoverage.theapfdmeasuresthe weighted average ofthepercentageof faults detected over thelife ofthetestsuite anditisagoodrepresentativeofhowfastfaultsare found .theapitestcoverageismeasuredaccordingtostandard black boxcoveragecriteriaforrestfulwebapis implemented intoolslikerestest restats andopenapispecification coverage and previously used to compare black box testing techniques .thesecriteriameasurethedegreetowhichaset ofapirequestsandresponsescovertheelementsdefinedintheapi specification.apirequestscoverinputelementssuchasoperations andparametervalues whileapiresponsescoveroutputelements such as status codes and response body properties.
as a difference compared to the original test coverage approach we consider input criteria to be covered onlyif the api request covering the elementsobtainedasuccessfulresponse i.e.
theapirequestwas valid.
for instance if an api request obtains a status code we considersuch codeas covered buttheparameter values used in the request are not covered since theywere rejected by theapi clienterror .wefollowthisapproachbecauseinvalidapirequests do not generallyexercise the core functionalityof the api.
.
.
classifyingfailuresintofaults.
wefollowedatwo stepsemiautomated approach to classify failures into unique faults first failures were automatically grouped into fault clusters then fault clusterswereeitherconfirmed iftheyrepresenteduniquefaults or discarded manually.
a fault cluster represents a unique potentially incorrect behavior in the api e.g.
a server error when setting a parameterwithcertainvalue anditcomprehendsallthefailures revealing that issue.
we consider that two failures belong to the samefaultcluster andthereforetheyarecausedbythesamepotentialfault if theyviolatethesametestoracle theyoccurin thesameapioperation e.g.
get search theyhavethesame statuscode e.g.
andcontent type e.g.
application json and they are similar enough.
similarity between failures was measured using the normalized levenshtein distance nld to compare http requests http responses and error logs.
if the distance is lower than certain threshold failures are considered similar.theonlyexceptionwas f oasfailures whoseerrorlogs arecomposedofstructurederrormessagescontainingdynamicvalues see example in figure .
these failures are considered similar if after replacing dynamic values with wild cards and removing duplicate error messages the resulting sanitized error logs are the same.table 3summarizes the heuristics andthresholds used.
412esec fse november14 18 singapore singapore alberto martin lopez sergio segura andantonio ruiz cort s figure2 f oaserrorlog.redboxesdepicterrormessages.
yellow highlighting depicts dynamic values.
table heuristics to determinesimilar failures.
type failures f1andf2aresimilarif f 5xxvnld response f1 response f2 .
f 5xxinld response f1 response f2 .
nld error logf1 error logf2 .
f 2xxpnld error logf1 error logf2 .
f 2xxdnld request f1 request f2 .
f 400 nld response f1 response f2 .
nld request f1 request f2 .
f oas unique wilcard error logf1 unique wilcard error logf2 results next wedescribetheresultsofourstudyandhowtheyanswerthe target researchquestions.
.
rq failuredetectioncapability table4shows the number of test cases api coverage failures and faults obtained in every api under test and by each test bot.
as illustrated a total of failures were uncovered i.e.
every test cases.
the majority of failures consist in disconformities of the responses with the api specification f oas .
in fact this is one of the main conclusions derived from our study api specificationspoorlyreflecttheactualapiimplementation .wefound inconsistencies in all apis under test except in restcountries but this is because the documentation of this api did not explicitly statetheformatoftheresponses hencenoinconsistenciescould befound.disconformitiesincludestatuscodesandcontent types not listed in the specification as well as violations in the format ofthe response bodies e.g.
anobjectmissing a required property.
systematicviolationsoftheapicontractsmeanastronglimitation to client applications which could crash if they run into a scenario not described or even forbidden in the api specification e.g.
a non nullablepropertybeing null .besidesoasdisconformities weuncoveredthousandsofotherfailuresrelatedtoservererrors 5xx status codes client errors status codes and wrongly returnedsuccessfulresponses 2xx statuscodes .
we studied the fdr for all apis under test.
overall the fdr ranged from .
failures out of test cases in the apiofrestcountriesto0.
811outof40 intheapiof languagetool.
we also studied the evolution of the fdr over time.
this allowed us to detect different tendencies and anomalies in severalapis.figure 3depictsthefdroverthetestsuitefractionin fiveapis amadeus dhl ohsome spotify andyelp.theremaining apisshowasimilartrendtotheoneobservedinspotify andare not includedforthe sake ofclarity chartsfor allapisare available in the supplementary material .
spotify represents a common scenario where the fdr remains constant i.e.
the same failures are uncovered all thetime.in amadeus thefdrslightly declines meaning that test cases fail less often over time.
we suspect this figure fdr inspotify amadeus dhl ohsomeand yelp.
happensbecauseapidevelopersarecontinuouslyfixingissues.the apis of dhl ohsome and yelp are clear anomalies.
in dhl all testcasesstartedtofailat60 oftheoveralltestbudget because we were banned from the api even thoughwe always complied with the imposed quota.
in ohsome a similar phenomenon was observed but the failures were due to status codes all with thesameerrormessage the timerange metadata could not be retrieved from the db .
in yelp the fdr spiked four times because the api only returned status codes likely due to some non functionalbugcausing temporary outages.
summary ofanswers to rq1 automaticallygeneratedtestcasesuncoveredabout390kfailures out ofevery 3test cases in the apis undertest.
failures were revealed in all apis with the fdr ranging from .
to .
.
offailuresareduetoinconsistenciesbetweentheapiresponses andtheirspecifications.
the fdr evolution revealed anomalies temporary outages and apibans in dhl ohsome andyelp.
.
rq faultdetectioncapability our fault clustering approach yielded potential faults i.e.
faultclusters fromthe389 216testfailuresrevealed.fdic and ohsome weretheapisforwhichmostpotentialfaultswere detected and respectively.
we discarded these apis when assessing the faultdetection capability of our onlinetesting setup as we could not manually analyze all these fault clusters.
we did analyze a subset of them nonetheless and we identified numerous bugs more details in section .
without considering fdicandohsome atotalof218 419testfailureswereautomatically clustered into potential faults which after manual revision resulted in reproducible or already fixed issues in all the apis under test.besides the issues extracted from the faultclusters we spotted8additionalissuesfromthereportsgeneratedbythetest reporter bots available at .
overall we uncovered a total of faults in apis.
half of thesefaults aredisconformitiesbetweentheapiresponsesand theirspecifications f oas whichiscoherentwiththefindings 413online testingof restful apis promises andchallenges esec fse november14 18 singapore singapore table per bot breakdown of test cases coverage failures and faults.
last column depicts unique faults i.e.
only uncovered by that bot.rowsinboldface depictbots uncoveringthemostfaults.rowshighlighted ingray depictthe total stats perapi.
api bot test cases coverage failures faults unique f 5xxvf 5xxif 400 f 2xx pf 2xxdf oas total faults amadeus r cdg art .
amadeus r cdg cbt .
amadeus r dp cbt .
amadeus r sd cbt .
amadeus r cd cbt .
amadeus r fd rt .
amadeus r cdg rt .
amadeus all bots dhl r fd rt .
dhl r cdg rt .
dhl r dp rt .
dhl r sd rt .
dhl r cd rt .
dhl all bots fdic r cdg art .
fdic r fd rt .
fdic r cdg rt .
fdic r dp rt .
fdic all bots foursquare r cdg art .
foursquare r cdg cbt .
foursquare r dp cbt .
foursquare r fd rt .
foursquare r cdg rt .
foursquare all bots languagetool rw cdg art .
languagetool rw cdg cbt .
languagetool rw dp cbt .
languagetool rw sd cbt .
languagetool rw cd cbt .
languagetool rw fd rt languagetool rw cdg rt .
languagetool all bots marvel r cdg art .
marvel r fd rt .
marvel r cdg rt .
marvel r dp rt .
marvel r cd rt .
marvel all bots ohsome r cdg cbt .
ohsome r dp cbt .
ohsome r fd rt .
ohsome r cdg rt .
ohsome all bots omdb r cdg cbt .
omdb r dp cbt .
omdb r sd cbt .
omdb r cd cbt .
omdb r fd rt .
omdb r cdg rt .
omdb all bots restcountries r fd rt .
restcountries r cdg rt .
restcountries r dp rt .
restcountries r sd rt .
restcountries r cd rt .
restcountries all bots spotify r cdg rt .
spotify rw fd rt .
spotify rw dp rt .
spotify rw cd rt .
spotify all bots stripe r cdg cbt .
stripe rw cdg art .
stripe rw cdg cbt .
stripe rw dp cbt .
stripe rw cd cbt .
stripe rw fd rt .
stripe rw cdg rt .
stripe all bots yelp r cdg art yelp r cdg cbt yelp r dp cbt yelp r fd rt .
yelp r cdg rt yelp all bots youtubecomments r cdg cbt .
youtubecomments r cdg rt .
youtubecomments rw cd art .
youtubecomments rw dp cbt youtubecomments rw cd cbt .
youtubecomments rw fd rt youtubecomments all bots youtubesearch r cdg art .
youtubesearch r cdg cbt .
youtubesearch r dp cbt .
youtubesearch r fd rt .
youtubesearch r cdg rt .
youtubesearch all bots total all apis 414esec fse november14 18 singapore singapore alberto martin lopez sergio segura andantonio ruiz cort s obtainedintermsoffailuredetectioncapability rq .aboutone third of the faults are caused by invalid api inputs handled as valid f 2xxpandf 2xxd .
this type of faults are pervasive apis are sometimes designed to return successful responses even when receiving invalid inputs e.g.
by ignoring them .
for instance in thestripeapi whenusingtwomutuallyexclusiveparameters the api ignores one of them instead of properly returning a bad request statuscode.thiswasconfirmedbythestripeapi developers.
invalid inputs do however cause api crashes in some cases due to poor input validation.
we found bugs of this type f 5xxi in4apis dhl marvel omdbandyelp .servererrorson validinputs f 5xxv areamuchmoreseverefault sincetheymay be causedby misconfigurationsor datacorruption among others.
wefoundoneoftheseinthespotifyapi unveiledwhenreordering or replacing the tracks of a playlist.
lastly we uncovered client errorsobtainedwithvalidapiinputs f 400 .thesefaultsgenerally revealinconsistenciesbetweenthedocumentationandtheimplementationofanapi.forinstance inthedhlapi thecountrycode kv kosovo is rejectedbytheapi 400statuscode althoughit should be supportedaccording to the api documentation .
theftdrrangedfrom0.
11faultsoutof258 297test cases in the api of restcountries to .
out of in the api of amadeus.
an overview of when and how often faults arefoundisshowninfigure whichshowstheapfdforevery api under test.
for all apis two thirds of the test budget i.e.
the test cases generated in days was enough to uncover all faults.languagetool restcountriesandstripeexposedalltheir faults within the first day of testing.
these faults were related to disconformities with the api specifications f oas and wrong handling of invalid inputs f 2xxpandf 2xxd .
on the other hand amadeus dhl marvel and youtubecomments required over one week to uncover all faults.
more varied issues were found inthesecases as explainedinsection .
summary ofanswers to rq2 automaticallygeneratedtest casesfound147faultsin 11apis ofwhichconfirmed orfixed by developers .
halfofthe faults arecaused by oas disconformities.
aboutone third of thefaults are caused by invalidapi inputs handled as valid.
only6faults aredue to internalservererrors 5xxstatuscodes .
of faults were found in the first days of testing with days being enoughto uncoverallfaults.
.
rq comparisonoftestingapproaches alltestingapproachesuncoveredfaults evenafter3daysoftesting but we foundseveral differences among the testdata and test case generation techniques under evaluation discussed below.
as previously mentioned the results for the apis of fdic and ohsome are discussed later section .
due to the number of failures revealed.
thus the results reportednextrefer to the remaining apis.
all test data generation techniques except semantically related data sd founduniquebugsorcontributedtoimprovethecoverage inallapisundertest.regardingcoverage customizeddatagenerators cdg and contextual data cd obtained the best results in mostapis 8outof11 .thiswasexpected sincevaliddatatends figure apfdperapi.markersdenote faults.
figure5 coverageandfaultsovertestcasesclassifiedperbot fortheyoutubesearch api.markersdenote uniquefaults.
to cover more input elements e.g.
parameter values and to obtain successfulapiresponses thuscoveringmoreoutputelements e.g.
responsebodyproperties .invaliddata dataperturbation dp and fuzzing dictionaries fd achieved the highest coverage in apis because they covered client and server errors 4xx and 5xx status codes thatvaliddatacouldnotcover.intermsoffaultsfound valid andinvaliddatagenerationuncovered76and81faults respectively 10ofthesefaultswerefoundbybothapproaches .infact thebugs uncovered by dp are often unique i.e.
not uncovered by other techniques .
figure right hand side illustrates this phenomenon in the api of youtubesearch charts of all apis available in the supplementary material where allbugs foundby the dp bot were unique.
overall dp found the largest amount of unique bugs in total followed by cdg with .
different techniques uncover different types of bugs.
for instance f 2xxpandf 2xxdfaults can only be detected by dp bots since they purposely manipulate theinputdatausedtomakeitinvalid.similarly f 400faultscan only be detected by cdg bots since they use manually set valid data whichshould never obtain 400status codes.
whenusingthesametestdatagenerationtechnique different test case generation strategies achieved similar coverage.
however constraint based testing cbt was more cost effective than randomtesting rt .forinstance inyoutubesearch left handside of figure cbt required just about test cases to achieve the samecoverageasrtwithabout700.regardingfaultsfound rt cbt and adaptive random testing art found and unique bugs respectively.
however in the apis where it was evaluated 415online testingof restful apis promises andchallenges esec fse november14 18 singapore singapore cbtfoundmorebugsthanrt.atthesametime artdidnotyield any improvement over rt or cbt except in amadeus where morebugswerefound comparedto cbt.
inparticular thesefaults belong to the f oascategory therefore the diversification of api requests may have led to more diverse api responses where more variedoasdisconformitieswere found.
summary ofanswers to rq3 validandinvalidtestdatagenerationstrategies i.e.
positiveand negativetesting areclearlycomplementary astheyuncovereda similar numberofunique bugs.
all test data generation strategies contributed to detect unique faultsorimprovethecoverageinsomeapi beingdataperturbation the most effectiveone.
theonlyexception was semantic data.
all test case generation strategies contributed to detect unique faultsor improvethe coverageinsomeapi being constraint based testingthe most effectiveone.
testcasediversityintermsofapiparametersandinputvaluesdoes not seem tohelp tofind new bugs orachieve higherapi coverage.
discussion in this section we discuss theresults offdic andohsome where thousandsofpotentialfaultswereidentified aswellassomeofthe issues detectedin allapis andthe overall costof our experiments.
.
fdic andohsomeapis in the apis of fdic and ohsome a total of and test failures were classified into and potential faults i.e.
fault clusters respectively.
in fdic of these clusters of the total are related to disconformities with the oas specification.
after a quick analysis we conjecture that most of these clusters represent indeed uniquefaults.thisisbecausethespecification of this api is so complex and its responses are so varied that every testcasecanpotentiallyuncovernewinconsistenciesintheoas specification.
in ohsome the same types of faults seem to occur in allits122operations.thisexplainsthehighnumberoffaultclusters found.althoughitispossiblethatasinglefaultmanifestinmultiple operations this cannot be known in a black box testing setup therefore we adopt the same criteria as previous authors where faultsindifferentoperations are consideredunique .
weanalyzedasubset offault clustersinfdic and ohsometo graspabetterunderstandingofhoweffectivelyourfaultclustering approachfoundreal worldbugsintheseapis andwhatshapethey have.wecreatedthesubsetasfollows wesortedfaultclustersover timeandweselected42clusters meannumberofclustersfoundin theotherapisundertest equidistanttoeachother.forexample in ohsome out of the faults clusters found we analyzed thoseatpositions .infdic allfault clusters were unique faults.
we also spotted additional bugs inthegraphicaltestreports conformingatotalof75unique bugsinthisapi.inohsome 28outof42faultclusterswereunique faults.
additionally we spotted faults related to status codes in the test reports making a total of unique bugs.
in both cases more faults could have been manuallyfound in the test reports for instance faults affecting other api operations in ohsome or more disconformitieswiththe oasspecification infdic.
.
issues detected testing apis in production can be helpful to uncover problems that escaped offline testing.
in fact online testing allowed us to detectnotonlyfunctionalbugs butalsoproblemsrelatedtononfunctionalrequirementsoftheapisundertest.wedetailthesefirst andthen we delve intosomeof the functional bugsfound.
.
.
non functional requirements.
we found problems related to security reliability availability andsla compliance.
security.whenusingmalformedinputdata wefound500status codesshowingthestacktraceoftheexceptionthrownintheomdb api and status codes with the error message threat detected in the dhl api .
disclosing this information to a potential attacker is dangerous since they could exploit vulnerabilitiesrevealed in thestack trace e.g.
outdated libraries or findan inputcompromisingthe integrityof the system.
reliability and availability .
we detected temporary outages in the form of 5xx status codes in apis dhl foursquare languagetool marvel ohsome omdb restcountries spotifyand yelp .
outages are generally caused by limitations of the servers and they can affect the overall user experience especially when they are prolongedintime e.g.
as inyelp andohsome .
sla compliance .
in the ninth day of testing we were banned from the dhl api even though we always complied with the specifiedquotalimitations.sla awareapispecificationssuchas sla4oai could help detect more sla violations like this in a more systematic way.
.
.
functionalbugs.
we identified 254reproduciblebugsin all apis under test 209 extracted from fault clusters found in the test reports and found by the garbage collector bot of spotify when unfollowing playlists the bot would start obtaining server errors if requests were sent quickly e.g.
every milliseconds .thisisaconservativeapproximation sincemorebugs could have been found with a more sophisticated and precise fault clustering approach section .
.
or with a more thorough manualanalysisofthepotentialfaults foundinfdicand ohsome.to date bugs from apis amadeus dhl foursquare omdb restcountries yelp and youtube have been acknowledged by developers reproducedby other usersoralreadyfixed.next we describesometypesofbugsuncoveredinourexperiments allbugs are documentedinthe supplementary material .
errorsduetoinvaliddataintroducedbyapiclients .intheamadeus api hotelobjectsmustcontaintwoproperties phoneandfax both ofwhichmustmatchtheregularexpression .wefound response bodies whose hotel objects contained invalid phone numbers e.g.
and missing fax numbers .
client applications may not be able to parse such api responses oreven crash.
amadeus developers confirmed that these errors are caused by hotel providers themselves as they may introduce invalid hotel data and they would update the api specification to reflect this newscenario.issueslikethesecanhardlybediscoveredwithoffline testing since they dealwithdata from the real world systems.
inconsistenciesbetween api implementation and api documentation.industrialapisarecomplexandevolverapidly andsodoes their documentation.
in some cases the implementation and documentationofanapimaynotbeinsync.forinstance theyoutube 416esec fse november14 18 singapore singapore alberto martin lopez sergio segura andantonio ruiz cort s search api documentation does not state that parameters location andchanneltype aremutuallyexclusive althoughwhen using both in an api request a status code is obtained .
similarly thedocumentationoftheyoutubecommentsapi statesthatparameters idandmaxresults aremutuallyexclusive although when using both in an api request a successful response is obtained .
youtube developers acknowledged both issues the former as incomplete documentation they updated the api documentationaccordingly andthelatterasan implementation defect they introduced a bug fix .
a continuous online testing setupcan rapidly detecttheseinconsistencies.
internalservererrorswithvalidinputdata .apicrashescausedby invalidrequests forinstance usinganout of rangevalueforan enumparameter in marvel or using a negative number for the limitparameterindhl arerelativelycommonandeasytofix namely by on valid inputs represent a more serious fault.for instance in the ohsomeapi avalidrequestobtaineda500statuscodewiththeerrormessage no message available .similarly inthefdic api multiplevalidrequestsobtainedservererrorswiththemessage cannot read property map of undefined .debugging thesefaultsishard especiallyduetothelittleinformationprovided inthe errormessages.
beyond all these bugs we uncovered dozens of other types of issuesrelatedtooasdisconformities inconsistenciesbetween the status codes and response bodies unparseable json responses and unexpected client errors among many others.
.
costofouronline testingsetup our testing ecosystem is heavily based on restest.
this greatly influencedthemanualworkrequiredtodeployalltestbots aswell as the overallconsumption ofcomputationalresources.
in terms of manual work we had to configure test bots restest instances which involved writing both test configuration files and data dictionaries mainly used by cdg bots .
for the testconfigurationfiles wewroteabout26klinesofcode locs althoughabout95 ofthemwereduplicated i.e.
copiedandpasted.
thishighpercentagerevealsthatthedataformatusedforconfigurationfilesinrestestshouldbeimprovednecessarily tomake it less verbose.
despite this limitation half of the bots required just or less manually written locs since the configuration of different bots of the same api can be reused.
regarding data dictionaries weused50dictionariescontaining52 084values about 1kvalues perdictionary.notethat datadictionariesaregenerally not manually written but rather extracted from the api documentation e.g.
foursquare categories and stripe tax codes orothergeneral purposeknowledgebases e.g.
countrycodesin wikipedia .
intermsofcomputationalresources theramanddiskconsumption kept increasing over the days of online testing.
preliminary experiments show that ram usage could be reduced by restarting bots regularly e.g.
every day to avoid potential memory leaks.
wealsomeasuredthecomputationalcostperbot.althoughwedid notfindnoticeabledifferencesacrossdifferentbotsofthesameapi wedidfindevidentdifferencesbetweendifferentapis.forinstance thebotstestingfdicandrestcountriesusedmuchmoreram upto20gb andtookupmuchmorediskspace upto250gb than therest.thisisduetotheshapeandsizeoftheresponsesobtained intheseapis someofwhichtookhundredsofmbs.handlingsuch responses e.g.
parsingandanalyzingtheminthesearchforfaults implies anon negligible overhead.
challenges weidentifythefollowingchallengesfortheadoptionoftestcase generationtechniques for onlinetestingof restfulapis.
challenge automated fault identification .
automatically determiningtherootcausesofthemanyfailuresfound about390kin ourwork ischallenging.ourfaultclusteringapproachhelpedus identify unique bugs out of potential faults but this also meansthat363potentialfaultsweremisclassified i.e.
duplicated non reproducible or not actual faults and bugs that we spotted in the test reports were missed in the fault clustering.
faults are misclassified due to the heterogeneity of error logs requests andespecially responses.thethresholdsused maynotbe equally effectivefor allfailures.moresophisticated faultidentificationapproaches are desirable e.g.
using dynamic thresholds according tothe type offailureand api applyingnon supervisedmachine learning techniques to improve the precision of the automatic classification or employing delta debugging to isolate failure inducing inputs .
challenge effectivehumaninteraction .whilebotsworkmostly autonomously they can benefit greatly from some human interaction.
for instance fault clusters could be formed by output bots andthencheckedbyahumantoimprovetheaccuracyoftheclassificationandtoavoidtheformationofduplicatedclusters.similarly botscouldreportsomefailuresas potentialfaults andrequesthumaninputtoconfirmordiscardthem.thiscouldbeaddressedwith active learning algorithms for example integrating human inputas apart ofthe testinganddebuggingprocess.
challenge optimalselectionoftestingstrategies .testersmay choose different testing strategies according to several factors such asthetypesofbugsthattheywishtouncover e.g.
crashes regressionsorinconsistenciesbetweentheapianditsdocumentation or the characteristics of the api under test e.g.
size output format and quota limitations .
automatically determining the most appropriatetestingtechniques basedontheseandotherfactorsis an open problem.
challenge optimal test execution scheduling .
the test execution schedule determines when how many and in what order tests shouldbe executed.this wasmanually configuredinourexperiments.ideally however thescheduleshouldbeautomatically computedtooptimizetheavailablequota basedontheapipricing plans thetimeandeconomicbudget e.g.
theinfrastructurecosts and the testing strategies used e.g.
prioritizing those strategies that are more cost effective .
challenge optimization of computational resources .
this is bothanengineering andaresearchchallenge.testbotsshouldbe optimizedtousetheleastresourcespossible especiallyavoiding memoryleaks.fromaresearchpointofview sophisticatedtechniquesfortestregression selectionandprioritizationcouldhelp 417online testingof restful apis promises andchallenges esec fse november14 18 singapore singapore in devisingmore effective test suites or reusingexisting ones e.g.
with metamorphic testing thus saving resources by avoiding the creation and execution of test cases that are not likely to uncover newfaults.
threats to validity ourworkissubjecttoanumberofvaliditythreats discussedbelow.
internalvalidity .threatstotheinternalvalidityrelatetothose factorsthatmayaffecttheresultsofourevaluation.faultsintheimplementationofourtestingecosystemmightcompromisethevalidityofourconclusions.eventhoughthetoolsused e.g.
restest and allure are thoroughly tested it cannot be guaranteed that theyarefreeofbugs.tomitigatethisthreat andtoenablereplicability of our results we provide a supplementary package containing thesourcecodeandtheimplementationofalltoolsusedinourtestingecosystem e.g.
thebotsandthecontrollercomponent aswell as scripts to fully replicate the results reportedinthis paper .
the use of restest as the selected test case generation tool may have influenced the results obtained in our experiments especially dependingonhowtheevaluatedtechniques e.g.
dataperturbation and adaptive random testing were implemented in the framework.
however thisdoesnotinvalidateourresults onthecontrary itsupports the potential of our multi bot architecture to integrate other testingtechniques andtools e.g.
restler andrestct .
to mitigate possible errors of our fault clustering approach section3.
.
wemanuallyreviewedallfaultclustersgenerated and reportedallidentifiedfaultstotheapidevelopers soastocounton their confirmation when possible.
we adopt a conservative approximation and report only faults that have been manually confirmed.
despite possiblelimitations of our fault clustering approach it automatically classified failures into potential faults in apis.fromthese weidentified139actualuniquebugs 65ofwhich have been acknowledgedorfixedbyapi developers to date.
externalvalidity .threatstotheexternalvaliditymightaffectthe generalizabilityofourfindings.themainthreatinthisregardisthe selectionofcasestudies.wetested13industrialapiscomprising operations in total although this might not be a sufficiently representative sample.
tominimize this threat we selectedhighly popular apis with millions of users worlwide from different application domains e.g.
media financial and social and with diverse characteristics andsizes.
conclusions in this paper we assessed the potential of automated black box test case generation approaches for online testing of restful apis resembling the modeloftesting as aservice increasingly foundin industry.
to this end we devised a multi bot architecture allowing ustogenerateandexecutetestcaseswithvariedstrategiesduring15 daysnon stopin13highlypopularapis.theresultsarepromising withover200bugsfound bothfunctionalandnon functional some of which have led developers to take actions including fixes and documentation updates inthe apisof amadeus and youtube.
this provides an encouraging vision on the future of testing of web apisasaservice whereplatformscouldofferarichcatalogofbots providingdiverseautomatedtestcasegenerationcapabilitiesunderdifferentpricingplans.severalchallengesstayinthewaythough.
in this regard our work paves the way for new promising research directions toovercome someofthe problems identified including theneedforautomateddebuggingmechanisms human in the loop models andoptimal test executionschedulingstrategies.
data availabilitystatement in order to enable reproducibility of the results obtained in this paper we provide a supplementary material containing the source codeofthescriptsandprogramsdeveloped theexecutablebinaries the data generated in the experiments and instructions on how to replicate the experiments reproduce the results and reuse the material for further research.
the artifact can be downloaded from zenodo at