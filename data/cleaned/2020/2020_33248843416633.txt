attend and represent a novel view on algorithm selection for software verification cedric richter paderborn university paderborn germany cedricr mail.upb.deheike wehrheim paderborn university paderborn germany wehrheim upb.de abstract today aplethoraofdifferentsoftwareverificationtoolsexist.when having a concrete verification task at hand software developers thus face the problem of algorithm selection.
existing algorithm selectors for software verification typically use handpicked program features together with either manually designed selection heuristicsor machinelearnedstrategies.whilethefirstapproachsuffers from not being transferable to other selection problems thesecondapproachlacksinterpretability i.e.
insightsintoreasonsfor choosing particular tools.
inthispaper weproposeanovelapproachtoalgorithmselection for software verification.
our approach employs representationlearning togetherwithan attentionmechanism.representation learningcircumvents feature engineering i.e.
avoidsthehandpick ingofprogramfeatures.attentionpermitsaformofinterpretability ofthelearnedselectors.wehaveimplementedourapproachand haveexperimentallyevaluatedandcompareditwithexistingapproaches.
the evaluation shows that representation learning does not only outperform manual feature engineering but also enables transferability of the learning model to other selection tasks.
keywords software verification algorithm selection representation learning attention.
acm reference format cedric richter and heike wehrheim.
.
attend and represent a novel viewonalgorithmselectionforsoftwareverification.in 35thieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
introduction today alargenumberofdifferenttoolsforsoftwareverificationexistasforinstancewitnessedbytheannualcompetitiononsoftware verification sv comp .
for software developers this poses the questionof algorithmselection forsoftwareverification which thisauthorwaspartiallysupportedbythegermanresearchfoundation dfg within the collaborative research centre on the fly computing sfb .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
likesv comportoolcomparisonsinstudies likethoseonandroid taint analysis tools only give a broad approximation of the best tool for an actual verification task while the tool might perform best on average it could very well show bad performance on the specific task at hand.
this insight has stimulated research on automatic algorithm selection for software verification taking the concrete program to be verified as input.
such approaches first of all fix featuresof programswhicharerelevantforsoftwareverification e.g.
theexis tenceofloops yes no ormorespecificallythetypeofloops.hence theseapproachesperformanexplicitfeatureengineeringtailored totheproblemofverification.second strategies forselectionbased onfeaturevaluesaredesigned eitherbyhand orlearnedvia machinelearning .whilesuchspecificallyengineered approachesoftenperformwellontheirspecifictask theyfallshorton a transferability and or b interpretability manually designed strategies cannot be transfered to other tasks even if they are closelyrelated andmachinelearningbasedapproachestypically cannot even partially explain reasons for selecting particular tools to users.
in this paper we propose a novel approach for algorithm selec tion in software verification.
the approach builds on a program representation inacommon format not specializedtothe taskof algorithm selection for software verification.
our employed format isasortofhierarchicalabstractsyntaxtree ast wherethehierarchy defines the contextin which a program entity like function or statement occurs.
these so called contextualized syntax trees are the inputs to our supervised representation learning method.
representation learning in general is a technique intertwining the learningofe.g.aclassificationtaskitselfwiththeautomaticdiscoveryoffeaturesnecessaryforthistask.
ourrepresentationlearner is a neural network operating on contextualized syntax trees.
the representation learner in addition employs an attention mechanism to allow for a form of interpretability of selection results.
attention mechanisms have first been proposed for neuralmachinetranslation andsincethenemployedinvarious areas .
attention mimics the way humans learn they focus their attention on particular aspects of a task and letthese aspects in the majority determine how the task is solved.in machine learning attention mechanisms determine simplyspeaking the set of features the learner should attend to most whensolvingaspecifictask.inoursetting attentionsuppliesus with weights on the program representation the weight fixing the attentiononprogrampartsinaspecificselectiontask.byhighlightingthesepartsaccordingtotheirweights thesoftwaredeveloper 35th ieee acm international conference on automated software engineering ase can be informed about the program constituents responsiblefor a particular tool selection.
wehaveimplementedourrepresentationlearnerforcontextualized syntax trees of c programs.
to evaluate its effectiveness wehaveexperimentallycomparedittootheralgorithmselection approachesforsoftwareverification aselectorwithhandpicked booleanfeatures anda manuallydesignedselection strategy a selector with the same handpicked boolean features and a decision tree algorithm for strategy learning and a selector with more sophisticated but still handpicked features and a support vector machineforstrategylearning.wefocusedonfour relativelysimilarselectiontasksforverification a theselectionofatooloutofthose participating in the software verification competition sv comp b theselection ofananalysisemployedwithintheconfigurable verification tool cpachecker c the selection between just two of these analyses and d the selection of analyses to be executedinsequence.asprogramstobeverifiedandthusinputs to the selector we have taken examples from the sv comp benchmark set.
the experiments show that our representation learner performs well on all four tasks often requiring even less time than the other machine learning based approaches.
moreover the representation learned for one such task can directly and successfully betransferredtoothertasks.finally attentionallowsustohighlight program parts relevant for a particular selection to users and thus naturally supports interpretability.
in summary we make the following contributions anovelrepresentationlearnerforsourcecode readytoalso be employed for learning other properties on code anattentionmechanismforpartialinterpretabilityoflearned selections both their implementation and application to four selection tasks and anextensiveevaluationandcomparisontoseveralrelated approaches.
source code and supplementary materials are available online2.
foundations in this section we briefly explain the methods underlying our approach inparticulartheconceptofattentioninmachinelearning.
westartbyclearlystatingourspecificalgorithmselectionproblem.
.
problem description theobjectiveofalgorithmselectionforsoftwareverificationisto choosea verifier outofsomesetofexistingverifiers foragiven program to be verified.
we let pbe the set of all programs and v v v p t f u bethesetofverifiers eachmappinga programtoaverificationoutcome t f u t programiscorrect f program contains error u unknown .
for simplicity we assume the correctness specification property to be checked to be encoded inside the program and the outcome of a verifier to be defined as uif it violates any resource constraint during its run.
notethatdependingontheselectiontaskwewillvarytheset v in the following.
i float result float x .
float grad .
for i i i grad x x x .
grad result x x assert result .
hasloophasfloat a strategy selectorint i float result float x .
float grad .
for i i i grad x x x .
grad result x x assert result .
loop it loop bound counterscalar float lbounded b verifolio int i float result float x .
float grad .
for i i i grad x x x .
grad result x x assert result .
short bound loop c bmc vs kiint i float result float x .
float grad .
for i i i grad x x x .
grad result x x assert result .
float multiply d p av sk i figure example program with different features similartobeyeretal.
westudyamorefocusedalgorithm selection problem over tools.
instead of also optimizing the runtimeor memory consumptionasin our goalisto maximize thenumberofverificationtaskssolvedcorrectly.moreformally let g p t f be the set of verification problems programs with correctness specifications annotated with a ground truth i.e.
p t gonly if the program pis correct.
in addition we definesol g 2vwithsol p y v v v p y as themappingtothesetofcorrectverifiersforaninstance p.no w letas a a p v be the set of considered algorithm selectors thentheproblemcanbeformulatedasfindinganoptimal selector a that maximizes the expectation of selecting a correct verifier over random p y g a argmax a ase bracketleftbig a p sol p y bracketrightbig notethatinpracticetheexpectationoperator eisevaluatedempirically on a finite subset of g. .
features for selection current algorithm selectors might they be hand madeor learned viamachinelearningalgorithms buildon features orcharacteristics or attributes of programs.
such features are typically manuallychosen and are specialized to the selection task.
the process of selecting features is known as feature engineering.
current algorithm selectors for software verification choose features based on knowledge about strengths and weaknesses of verifiers.
however as soon as these strengths and weaknesses change e.g.
support for specific program aspects say pointers is improvedornovelverificationtechniquesaredevelopedandimplemented ortheselectiontaskitselfchanges chosenfeaturesbecome inadequate and the tedious task of feature engineering has to be 1017repeated.
current algorithm selectors are often too rigid and static and hence cannot flexibly cope with changes.
weillustratethisshortcomingonsomecurrentverificationtechniques as implemented in tools .
consider the task of selecting between the two verification techniques of bounded model checking bmc andk induction ki .theyarecomplementary bmc isa viablesolution ifthe programhas afinite nottoo large number of execution paths.
ki on the other hand extends bmc with automatic loop invariant generation.
however the process of generatinginvariantsisoftencostly whichallowsbmctobemore efficientinthefinitecase .moreprecisely weexpectbmcto be superior to ki if loop executions are bounded by a small constant.anoptimalalgorithmselectorchoosingbetweenbmcand kishouldbeengineeredtoexploitthisobservationordiscoveritby learning from data.
consider now a slightly different selection task choosingbetweenkiandanothertechniquecalledpredicateabstraction pa both of which are good at proving correctness ofprogramswithunboundedloops.nowdifferentselectioncriteria become relevant.
for example performance differences between kiandpacanbeduetotheindividualhandlingoffloatingpoint operations and thus their detection becomes important.
example for a more detailed explanation take a look at figure .
it shows the same program snippet four times together with specificfeatures figure1 a forthestrategyselectorasemployedintoolcpachecker andfigure1 b forthetoolverifolio .figures c and d on the other hand show features which would be required for two selection tasks.
cpachecker bases its decision on simple features only for our snippet it detects the occurrence of a loopandafloatingpointvariable and fortheselectiontaskbmcki would choose technique ki.
verifolio extracts more specific features namely it in addition detects that the loop is bounded by findingthetypicalcomponentsofaforloop initialization bound andcounter .still itwouldalsoselectki altogetherabandoning bmcinthepresenceofloops.however thebestchoicewouldbe bmcastheloopiterationsareboundedbya smallconstant.hence for the specific task of bmc ki selection the features shown in figure1 c aremoreadequate.moreover anewselectiontaskmightrequiredifferent weightsonfeatures whilebmc kishouldfocusattention on feature short bound andloop a pa ki selector should detect and focus on float multiply as figure d indicates.
in summary different selection tasks require different features and different weights on features evenif we keep the general objective ofalgorithmselectionforsoftwareverification.thisobservation motivates a machine learning technique automating the discovery of problem dependent characteristics.
more specifically we aim at achievingtwolearningobjectivesatthesametime tolearna representation of programs as vectors mimicing the feature vectors obtainedforstaticfeaturesets ho wever withoutanexplicitfeature engineering and to learn the relevance and importance of such features for the specific selection task.
.
transformer to learn representations of verification tasks our approach builds uponthesocalled transformerarchitecture ofvaswanietal.
.
although such transformers were mainly developed for natural language processing it has been shown that they work well inibaked some cookies foryoucookiescookies sweets figure do we refer to web cookies or to sweets?
variousotherdomainslikeimagegeneration musiccomposition andprogramrepair .here weexplainthebasic principle behind the transformer along the lines of an example taken from natural language processing nlp .
the example is the sentence given in figure .
our objective is to find an appropriate representation for this and similar sentences thisrepresentation being conditioned onaspecific context.
the learned representation should allow to furthermore learn a classification task.
in the nlp setting a contextualized representation of a single word is often required in our example cookies .
the classification which we intend to learn is that the meaning of cookies in the context of this sentence is cookies sweets .
finding a contextualized representation entails two steps finding representations for each word and aggregating these representationsforanentiresentenceinameaningfulway.both of these steps are being learned i.e.
their calculation employs a number of learnable parameters.
the initial representation is obtained from an encoding table which assigns every word to a vector rm where mis the dimensionality hyperparameter.
the sentence is then represented as a sequence of word vectors mapping each word to a corresponding vector.
in our example the sequence length nis andh4is the initial representation of cookies .
a key principle employed in the second step is the concept of a context.asweashumanscaneasilyseeinourexamplesentence themeaningofcookiesisthatofsweets notwebcookies.wedetect thisby lookingat thecontext specifically theword baked .thus our learning process should learn a representation for cookies in this context which is completely different from a representationof cookies in the sentence today i deleted all my cookies in firefox.
.
attention.
thecoretechniqueaccomplishingafocustothemost relevantinformationinthecontextisthe attention mechanism .
attention computes the relevance of a context word jin the context of a query term i cookies as follows rij parenleftbig wqhi parenrightbig parenleftbig wkhj parenrightbigt wheretis matrix transposition and wq wkare learnable parameter matrices.
setting the relevance is done by tuning wqand wkduring training.
this is achieved by standard backpropagation techniques in machine learning.
infigure2 therelevanceofcontextwordswithrespecttothe query term cookies is depicted by thickness of lines whereas forinstance baked isconsideredrelevantforthewordmeaning cookies some is not.
distinguishing the parameters wq wk 1018for i 0i i forstmtforstmt bounded figure the detection of a small bounded for statement.
in equation allows to learn an asymmetric relevance relation.
in ourexample baked mightbemorerelevantforthemeaningof cookies than cookies for the meaning of baked .
to obtain the necessary aggregation for an entire sentence the wordrepresentationsareaggregated withrespect tothepreviously calculated relevance score ij exp rij m summationtext.1n k 1exp rik m ai n summationdisplay.
j 1 ijhj whereaijistheattentionweightcomputedasasoftmaxnormalizationoftherelevancescore.thecontextvector aiisaweightedsum of all context words weighted by their individual relevance for the query term hi.
inpractice thecontext word representation hjbeforeaggregationandtheattentionresult aiarefedthroughlearned linear transformations wvandwo respectively.
the transformation is here omitted for notational simplicity.
neuralnetworkencoders.
attentionsofarlearnsarepresentation specifictoagivensentence.overall wewouldliketogeneralizethis specific learning outcome to other contexts here other sentences .
this is the typical task of a neural network nn .
for simplicity we define the nn formally as a function .
for a more technical descriptionof wereferto .now thefinalrepresentationis obtainedbyapplying onthesumoftherepresentation hiandthe result of the attention mechanism ai h prime i hi ai inourexamplesetting anoptimal wouldfinallymaptheword cookies in our example sentence to a classification as cookies sweets and contrary therepresentationof cookies inthe sentence today i deleted all my cookies in firefox.
to an alternative vector representation of cookies web .
in practice transformers simultaneously calculate new representations for every wordresultinginarefinedsentencerepresentation h prime h prime ... h primen .
by applying multiple transformer layers composed of an attention mechanism and nn encoders one after another successively more precise representations are obtained.
approach inthissection weintroduceourapproachfor simultaneously learningthefeaturerepresentationsofverificationtasksandthealgorithmselectionitself.aftergivingageneraloverview wepresent a hierarchical model for source code suitable for software verifi cation.
for learning a source code representation we adapt the transformer architecture to explicit hierarchical learning.forstmt function program forstmt bounded function bounded loop program bmc figure the representation process of a program.
.
overview westartagainbygivinganexample nowfromtheareaofsoftware verification.
consider the program block given in figure and the selection task of bmc ki as already discussed before.
the block consistsofanumberoflow leveltokens i etc.
.theobjectiveis now to learn a representation of this statement block capturing its meaning and employ this representation to learn a classification of the statement.
for the task bmc ki the classification we would like to learn is some subsymbolic form of forstmt bounded becausethisisthecrucialinformationforselection.notethatwe neverfixsuchclassificationsbeforehandnorwilltherepresentation learner call this forstmt bounded at the end.
again we employ anattentionmechanismwhichhereshouldguaranteethetokens for and to be highly relevant for forstmt and not.
this reflectsthe higher importance ofloop bound and for token.
astheconcreterepresentationistask dependent bmc ki weaim to adapt the transformer architecture to learn a task dependent representation.
here therelevantblockmeaningcanbededucedfromitsconstituent tokens.
now we can apply the same idea to deduce the relevant meaning of a function from its constituent statements and of a program from its functions.
we hence require a hierarchical transformerarchitecture.oneverylevelinthishierarchy anappropriate aggregation oftheconstituentrepresentationsneedstobe learned.usingtheexampleofbmc ki wevisualizeapossiblehier archical representation process in figure .
on the lowest level welearnthemeaningoftheforblocktobe forstmt bounded which allowstolearnthefunctioncontainingthisblocktobe function bounded loop and the entire program to be program bmc which finally leads to the selection of bmc over ki.
tosummarizethenecessarystepsforobtainingarepresentation learner for algorithm selection in software verification we require an explicit model of the decomposition of a program into tokens blocksandfunctions.insection3.
wepresentan appropriate hierarchical model.
amachinelearningtechniquethatoperatesonourmodel while learning a program representation.
we describe our learning approach in section .
.
anoverviewofourfinalarchitecturecanbefoundinfigure5.a programisfirsttransformedtoasocalled contextualizedsyntaxtree cst see next .
the cst hierarchically composes program tokens redsquares intohigh orderconceptssuchasblocks functionsand programs blue circles .
in the first layer we encode every token as a vector and iteratively aggregate vectors by a transformer.
the transformer outputs attention weights for every aggregation step 1019int main int i float result float x .
float grad .
for i i i grad x x x .
grad result x x assert result .
return programr ... ... ... ... ... ... ...hierarchical transformer layer encoding layerrepresentation tokensstatementsfunctionsprogram architectureint main int i float result float x .
float grad .
for i i i grad x x x .
grad result x x assert result .
return attention bmc .
ki .
pa .
prediction figure hierarchical transformer architecture for programs whichwecanuseforvisualization.finally theobtainedprogram representationcanbeusedforpredictionandselectionofaverifier giving scores to each verifier in the set v .
.
contextualized syntax trees to model the composition of syntactic elements into the context of high order program structures we propose contextualized syntax trees cst asourprogrammodel.acstdistinguishesthreetypes of program structures functions statements and syntax tokens.
syntaxtokenslikeliterals variableaccessesoroperationsaredefined in the context of a statement while statements are composed to functions.
functions are defined in the context of a program which act as the root of our cst.
formalization formally wedefineacstas c n ctx n0 l wherenis the set of nodes ctx n nassigns every node to its context another node n0 nis the tree root and l n l assignseverynodetoalabelinafinitelabelset l.furthermore the set of nodes n p f s tcan be divided into disjoint subsets for programs p functions f statements sand syntax tokens t.w e restrict the context such that ctx t s ctx s f ctx f p whilepcontains only one element n0as the program root.
the depth of a node in a cst is the length of the path to the root.
nodes that belong to the same depth dare summarized to a set nd n n depth n d andn0 n0 .
independenceassumption bydefinition thecstintroduces a strong independence assumption it is assumed that there are no dependencies among tokens statements or functions.
while this is nottrueingeneralandtheassumptionismerelymotivatedfrom an efficiency perspective the intuition here is that every program elementisachallengeforaverifierbyitself.thedifficultyofthe individual challenge is then decided by its constituents.
this designdecision however imposesanotherchallengeforconstructing the cst how should a program be split into statements when statementsareoftencompositionsofotherstatements?although thereexistsmanypossiblesplits3 wechoosetosplittheprogramas proposed byzhang etal.
.
morespecifically we considerloop headsandbranchconditionsasblocksindependentoftheirbody while statements occuring e.g.
in a loop head are not recognized 3for instance possible splits are loop head together with loop body or loop head consisting of loop condition only.as independent blocks.
this leads us to the following construction procedure.
construction toconstructacst wefirstparsetheprogram intoitsabstractsyntaxtree ast .theastdoesnotonlysplit the program into the necessary syntax tokens but also models the syntactic structure of a program.
next the ast is traversed and the cst constructed function roots are identified by function declarations occurring in the ast.
each function is represented by a function nodeinthecstandassignedtotheprogramroot.ifanast node is only reachable by traversing a function root f then it belongs to the context of f. statement roots are identified.
every ast node that belongstoafunction looporbranchbodyisconsideredtobeastatement root.
the tree rooted at a statement root is considered as the statement s ast.
in the cst we create a statementnode per statement and assign it to its function context.
globallydefinedstatementsareassignedtoaspecial init function.
we flatten the statement ast by pre order traversal and createasyntaxtokenperastnodeinthecst.allcreated nodesareassignedtotheassociatedstatementnodeinthe cst.
ast labels are kept in the cst.
optimizations although the cst can now be used for modeling a program we further optimized the model to meet memory constraintsandavoidoverfittingofourlearnerinpractice.while the cst can be used to model programs of arbitrary languages the following optimizations are tailored to software verificationon c programs.
the adaptions allowed us to train our machinelearning model on large scale programs with thousands lines of codes e.g.
code of the linux kernel.
nodetyping.
followingtheapproachesin wereplaced concreteidentifiernames literalvaluesandunaryandbinaryoper ationsbyfixedlabels.forinstance wemapintegerstoidentifiersofcommonrangesdefinedbythecstandard i.e.
anintegerismappedto bitliteral byteliteral shortliteral integerliteral or longliteral .althoughidentifiersincludingfunctionnamesare replaced wedistinguisha special mainfunction andinitfunction.
mainfunction labelsthemainfunctionoftheprogramand initfunctionis assigned to the dummy function that summarizes all global 1020linear linear linear layernorm layernormhierarchical attention linear layernormgelulinear figure architecture of a hierarchical transformer layer statements.
statements are labelled according to their statement type.
an example for a forstatement can be found in figure .
representing statementsas sets.
to improve thespace efficiency ofacst weexperimentedwithreplacingthesequenceofsyntax tokens at every statement with a set representation.
together with aforementioned optimizations we noticed a significantly higher runtime efficiency and lower memory footprint while the accuracy degenerated only slightly.
therefore we decided for our final evaluation to represent statements as sets.
.
hierarchical transformer for contextualized syntax trees we now adapt the transformer to operate on hierarchies instead of sequences.
we denote the modifiedtransformerasthe hierarchicaltransformer ht .modificationsarepresentedonthebasisofagivencst c n ctx n0 l .
asthefirststep theinitialrepresentation h h0 h1 ... hn is obtained from an encoding table which assigns every label in lto a vector in rm.
every node niis then encoded by a vector hiaccording to its label l ni .
the objective of ht is to find a refinedrepresentation h prime h prime h prime ... h primen suchthat h prime iencodes all necessary information of niand its children.
depth wisetransformation.
incontrasttotheoriginaltransformer ht only considers the interaction between parent nodeand children the representation of nodes at depth dis only dependent on nodes at depth d .
exploiting the definition of a cst the initial representations can be split depth wise resultingin hd hi h ni nd .
now we define ht as a depth wise function by the following formulation h prime d ht parenleftbig hd h prime d parenrightbig sincethenodesatmaximaldepth dhavenochildren weset h prime d hd.notethathtcomputesallnewrepresentations h prime iforevery hi hdinparallel.weschematicallydepictthehtlayerinfigure6.
the visualization includes technical details which we discuss in the remaining section.
if we now iteratively apply the ht layerto acontextualized syntax tree wewill obtainour final program representation r h prime 0after steps.decl forstmt int varref for varref short figure masking for hierarchical attention.
dotted graylines will be ignored during attention computation.
hierarchical attention.
ifweapplytheattentionmechanism naivelybetweenallparentrepresentationsin hdandchildrepresentationsin h prime d wewouldallowthateveryparentcanattendto everychildevenifthechilddoesnotbelongtotheparent.toavoid this we replace the relevance calculation while masking scores between parents and non children rij braceleftbiggrij if ctx nj ni otherwise whererijiscalculatedasdescribedinsection2.3betweentwonode representations hiandh prime j.bysettingtherelevanceofanode njfor a parent node nito negative infinity we say that njis infinitely irrelevant for niasnjis not the child of ni.
as the exponential functionconvergesto0forlargenegativenumbers theattention betweenparentandnon childrenismaskedout inequation .figure7givesanexampleformaskedoutattention.the decl node is not allowed to attend to the children of the forstmt.
sparseevaluation.
inpractice attentionmasking asemployed inhierarchicalattention istypicallyappliedaftercomputingthe relevancescore.inotherwords anattentionmatrixbetweenparentandchildrenofthesize nd nd iscomputedfirst.then mostoftheresultsarethrownawayafterthemaskingoperation.
exploitingthesparsityofacst weimplementedmaskedattentionsparselyongpus.anattentionweightiscomputedifandonlyifit is not masked out.
as a result time and memory consumption decreasesto o nd perdepth d.inpractice weobservesignificant performance gains over dense computation on large csts.
furtherdeviations.
inadditiontothemodificationsmentioned above the following deviations were applied layer normaliza tion as pre norm geluactivation as motivated by and the addition of dummy nodes .
finally we omit multi headed attention to improve explainability.
algorithmselection tocompleteouralgorithmselector we feed the obtained representation to a linear unit followed by a softmaxactivation.theoutputprovidesuswithaconditionalprobability of solving the given problem instance p y correctly for every considered verifier v v p v sol p y p suchaprobabilityisforinstanceshowninfigure5forthethree available verifiers bmc ki and pa in v. in practice the verifier with the highest probability is selected.
similar to the algorithm selector is trained to minimize the averaged negative log likelihood of the reference set y sol p y loss y summationdisplay.
v ylogp v y p 1021figure web ui for model exploration .
model exploration attentionisnaturally interpretable .alearnedattentionweight indicates how important a child is for the representation of its parent.
clearly a node that has an high impact on the model de cision has to achieve an high attention score.
more importantly the program representation is mostly shaped by the cst nodes that achieve the highest overall attention.
therefore we believe an explainable insight of the model behavior and also the following algorithm selectiondecision canbe gained byanalyzing theattention weight.
to explore the behavior of our algorithm selectors we implemented a web ui to visualize the attention scores across our model.
an excerpt of the interface is shown in figure .
web ui features aftertraininganalgorithmselector theobtained decision model can be uploaded to this web interface.
afterwards itispossibletoquerythealgorithmselectorwithaprogram written in c. the ui does not only show the model decision but also highlights parts of the code with respect to its obtained attention.
a menu shows implemented functions and their attention.
by selectingafunction thefunctioncodewillbeshown.
wecolored every statement in a function by its attention score.
statements withhigherattentiongetadarkercolor.byselectingastatement we show the attention score per syntax token in a bar diagram.
alternatively attention can be depicted in the form given in figure .
in contrast to all other existing algorithm selectors for software verification our approach shows the reasons for a particular selectiontothesoftwaredeveloper.itthuspossessesacertainformof interpretability which naturally comes with attention based techniques.thewebinterfaceisincludedasapartofthesupplementary material.
evaluation ourevaluationisdesignedtoinvestigatewhetherrepresentation learning can relieve a developer from manual feature engineering in the context of algorithm selection for software verification.
the investigation intends to answer the following research questions rq1canrepresentationlearningimprovealgorithmselection?
rq2dolearnedprogramrepresentationstransfertonewselection tasks?
figure histogram of the sv18dataset.
tostudytheseresearchquestions wedesignedindividualexperimentsperquestion.westartbydescribingthegeneralsetupforall experiments in terms of dataset evaluation metrics and baselines.
.
dataset forourexperiments wechoosethebenchmarksetemployedby beyeretal.
intheirworkonalgorithmselection.werefertoit assv18.sv18consistsof5687programstakenfromthesoftware verificationcompetitionsv comp2018.everyverificationproblem program belongstooneof11categoriesofthecompetition including reachsafety onarray bitvectorandfloatprograms.using the data set statistics as shown in figure we can observe that programs in sv18are unusually large with a majority of over ofallprogramscontainingmorethan1000nodes.foracomparison recentcoderepresentationmethods wereperformedon programswithanaveragecodelengthofaround99tokens while the average program in sv18contains more than .
nodes.
now choosing sv18as our dataset gives us the unique opportunitytodefinethefollowingfouralgorithmselectiontasksonthe same dataset bmc ki.
in this setting an algorithm selector has to take a binarydecisionbetweenboundedmodelchecking bmc andk induction ki .
the choice of these is motivated by our example in section .
.
algorithms is a selection task over a set of verification algorithms implemented in the configurable analysis tool cpachecker .thesetincludesamongothersbmcand ki.
sequential composition sc considersthreesequentialcompositions formed by the previously mentioned verification algorithms.
for comparability we choose the same compositions as described by beyer et al.
.
tools.for this task the selector has to select one verification tooloutofseveralsv comp2018participants.wechoose the10toolsthatsolvedatleastoneverificationtaskinevery aforementioned category.
in the following we refer to these entities to select from simply as verifiers.
1022datapreparation foreachselectiontask welabeledprograms with the subset of verifiers that solve the problem correctly within 15min.
to measure the generalization performance of learning we split the dataset into a training and test set with ratio of .
the training set is again split with same ratio for obtaining a developmentset.generatedsplitsmaintainthesamerelativeratiofor verification problem per category as the source dataset.
if necessary algorithmselectorsaretrainedonthetrainingsetwhilethe development set serves for model selection.
experiment results are reported for the test set.
for reproduciblity splits and labeling are contained in supplementary material while programs can be obtained from the official sv comp repository4.
.
evaluation metric we evaluated all compared techniques with respect to their normalizedaccuracy nacc.todeterminetheaccuracyofanalgorithm selector aonatestset dconsistingoflabeledprograms weemploy the following formula acc d summationdisplay.
p y d1 n a c c acc oacc where 1is the indicator function a p is the selected verification techniqueonaprogram pandyisthesetofverificationtechniques that solve pcorrectly.
motivated by the oracle measure in we normalize the accuracy score based on the score of an oracle denoted by oacc.
an oracle always performs the best possible selectionforataskathand andthereforeitsaccuracyrepresents the theoretically highest achievable score for a given test set.
themeasure nacccalculatesanaccuracyscorebetween0 and .whileascoreof0 indicatesthatnotasingleproblemcanbe verified correctly a score of will be achieved if the algorithm selector assigns every program to the best possible verifier.
.
baselines we compare our representation learner against a number of other approaches for algorithm selection in software verification.
bestselector simplyselectsthetechniquethatperformsbeston the training set.
while this is not a feasible selector in practice we reportitsperformancetoinvestigatewhetheralgorithmselectors can improve the verification process.
cpa selector is a completely hand engineered algorithm selector.cpa selectorextractsthefourbooleanfeatures hasloop hasarray hasfloat and hascomposite.
then it employs fixed rules toselectoneofthethreesequentialcompositionsusedinoursc selection task.
these manually designed rules hence cannot be generalizedtoothertasks.therefore weonlyreportresultsforthe sc task.
dt selector.
generalizing the idea of cpa selector we constructedaselectorwiththesamefeatures butusingdecisiontrees dt asitslearningparadigm.dtsareabletolearnselection rulesinbooleanlogic.followingdemyanovaetal.
wetrain onedecisiontreeperverifier.thedtlearnstheprobabilitythatthis verifier belongs to the set of correctly solving techniques.
at the end the verifier with the highest predicted probability is selected.
basesitsfeaturesoncustomstaticanalysisand source code metrics including variable roles detection of syntactically bounded loops and branching counts.
verifolio trains support vector machines svms with a radial basis kernel per verifier.
the svm predicts whether a verifier achieves a correct result or not.
to achieve a probabilistic output the svms are extended with plattscaling .inourexperiment theverifierwiththehighest predicted probability is selected.
kernel rpc isaselectorwhichdoesnotemployanexplicitfeaturerepresentationforprograms.kernel rpc transformsprogramsintographscapturingcontrolandprogramdependencies.
to bypass feature engineering the authors designed a custom kernelfunctionwhichcandirectlyoperateongraphrepresentations.
the selection function is then learned by a support vector machine equipped with the kernel.
the output of the svms are rankings over all verifiers using ranking by pairwise comparison rpc .
.
experimental setup webrieflyreportonsomedatapreprocessing thetrainingsetup and the experiment protocol for our representation learner.
datapreprocessing toapplyourlearningmethodtothe sv18 dataset we first compute an ast for every program with the help oftheclangccompilerfrontend5.thisisfollowedbythecst transformation as described in section .
.
on the training set we perform further cleaning operations while the development and test set remains the same in all experiments.
first of all wededuplicated all isomorphic csts in our training set using the weisfeiler lehman test of isomorphism .
to further counter measure the memorization of our model we removed training exampleswhereanisomorphiccounterpartcanbefoundinthetest set.
finally we removed all examples which cannot be solved by anyverifierindividuallypertask.intotal weprunebetween495 and out of training examples dependent on the task.
trainingsetup ourapproachusesacommonhiddendimensiononalllayers.forthetasksbmc kiandalgorithms weemploy a hidden dimension of and a hidden dimension of on sc andtools.similartotheoriginaltransformer dropout is appliedoneverysublayerbeforeitisaddedtothepreviousinput and on the final representation.
for training we used the adamoptimizer with 1 .
2 .
and .
we employ a linear warmup and linear decay of the learning rate.
to avoid overfitting westopthetrainingifthevalidationaccuracyonthe developmentsetdoesnotimproveafter5trainingepochs.ashyper parameterofourtraining wetunethelearningrate warmup length weight decay batch size and number of epochs .
experimentprotocol forallourexperiments wetunehyperparameter via bayesian optimization bo .
bo is allowed totestupto150parametercombinationsperexperiment.theparametersettingwiththehighestvalidationaccuracyisselectedandthenthecorrespondingmodelisevaluatedonthetestset.wetrain our model on a tesla k80 accelerator.
techniques without gpu support are trained on a six core virtual machine with 32gb main memory.ifruntimesarereported theyaremeasuredonalimited 1023machine with .
ghz cpu and 15gb main memory.
runtime measurements are limited to 15mins per example.
results next we report on the results of our evaluation.
.
rq1 can representation learning improve algorithm selection?
to investigate whether algorithm selection benefits from representation learning we design two experiments.
during the first experiment we measure the selection accuracy of our algorithm selectorequippedwithanattentionbasedrepresentationlearner attn rl .
we compare the selection accuracy on all proposed selection tasks with the performance of our baselines.
the second experiment is designed to evaluate the runtime necessary for selection.
selectionaccuracy ourevaluationresultsarepresentedintable1.allmeasurementsarereportedinnormalizedaccuracy nacc asapercentageonthetestset.thebestresultsarehighlightedin bold.
for now the row attn rl tr can be ignored.
during our experiments verifolio aborted on two programs and kernel rpc ran into a timeout for out of test cases.
for thesetwo thereportedresultshencereflecttheaccuracyofa hybrid techniquethatusestheoriginalselectorwheneverpossibleandthe bestselector otherwise.
for a fair comparison of pure prediction accuracies we thus in addition compare the selectors on a reduced test set see below .
while comparing the different selectors we make the following observations selection helps verification withoutalgorithmselection we would be forced to always use the same verifier i.e.
optimally the one chosen by bestselector.
we see that everyother selection strategy is better than bestselector on all selection tasks.
learning of features improves over manual engineering kernel rpc and ourrepresentation learnerattn rl thetwo approaches with learning of features outperform the selectorswithmanualfeatureengineeringonalltasksbut sc.onsc attn rlisbetterthantheotherselectors.this indicatesthatthereareimportantprogramfeatureswhich manual feature engineering techniques have not discovered.
representation learning further improves selection when comparing just the two selectors without manual feature engineering we see that representation learning further improvesoverkernel rpc withagainofupto3.
with one slight exception task tools.
for the latter observation we had a closer look at the differencebetween kernel rpc and attn rl and performed the experiment again on a subset of sv18 for which the prediction of kernel rpc does not timeout i.e.
in which it does not work as a hybrid technique.
the results are given in table .
this shows that theprediction accuracy of attn rl is actually better or the same as that of kernel rpc on all selection tasks.
aspartof answeringrq1 wealsoperformeda runtimeexperiment to decide whether our representation learner can improvethe selection speed .
to measure the runtime we select the toolstable comparison of selectors on sv18 approach bmc ki algorithms sc tools bestselector .
.
.
.
cpa selector .
dt selector .
.
.
.
verifolio .
.
.
.
kernel rpc .
.
.
.
attn rl .
.
.
.
attn rl tr .
.
.
.
table comparison of selectors on a subset of sv18 approach bmc ki algorithms sc tools kernel rpc .
.
.
.
attn rl .
.
.
.
task as it provides us with most verifiers to select from.
therefore reported results can be seen as an upper bound for all our tasks.during this experiment we excluded bestselector cpa selectorand dt selector since their runtime for prediction is due to its simplicity negligible anyway.
figure gives a quantile plot of the compared techniques.
a point x y represents that a selection for xprograms can be obtained each within a timelimit of yseconds.
although the runtime of all selectors starts similar kernel rpc does not scale well tolarge verification problems.
attn rl and verifolio have similarruntimes.
a directcomparison of these twois given in figure .
wecanseethatverifolioisfasteronsmallprogramswhileattn rl becomes faster as soon as the programs get larger.
in summary representation learning can improve algorithm selectionanditcanoutperformmanuallyengineeredtechniques in both selection accuracy and runtime.
.
rq2 do learned program representations transfer to new selection tasks?
to answer rq2 we evaluated the transferability of learned representations.inotherwords ourobjectiveistofindoutwhether a representation learned for task acan be used for another task b.forthispurpose weusethe representation ofattn rltrained ontask aandthenapplylogisticregression lr forlearningthe algorithm selectorfor task b. similar to attn rl lr predicts the probability of a correct verification per verifier while selecting the approachwiththehighestpredictedprobability.selectionaccuracy is again reported in normalized accuracy and given in table .
the entryinrow xandcolumn ygivestheaccuracyofpredictionwhen using the representation learned for xfor the selection task y6.
6notethattheobtainedresultsformatchingrepresentationandlearningtaskneednot tobeequaltotheresultsintable1.inourpreviousexperiment thelearningprocedure optimizes the representation to learn a selection rule.
in contrast lr optimizes its selection rule to match the given representation.
1024figure quantile plot for the different approaches figure runtime comparison of attn rl and verifolio we make the following observations generality outperforms spezialisation thetasksbmc ki and sc perform specific selections.
algorithms containing theverifiersemployedinbmc kiandsc canbeseenasa generalizationofthesetwotasks whiletoolsiscomplementary to each of the other tasks .
we see that using the representationlearnedforalgorithmscanverywellbeusedfor the more specialized tasks bmc ki and sc even improving overtherepresentationslearnedforthetasksthemselves but not the other way round.
transfer of learned representations is competitive wecomparethe onaverage bestrepresentationofalgorithmswith our otherbaselines.
to easecomparison row attn rl tr intable1repeatsthegrayrowofalgorithmsintable3.we seethatthereareonlysmalldifferencestothebestselectors still outperforming manual feature engineering.
taking our runtime experiments into consideration we come to the following conclusion programrepresentationslearnedonageneraltaskwithmultiple verifiers transfer well to more specialized selection tasks.table knowledge transfer between tasks train eval bmc ki algorithms sc tools bmc ki .
.
.
.
algorithms .
.
.
.
sc .
.
.
.
tools .
.
.
.
.
interpretability the attention mechanism as applied by our representation learner allows us to reason about the decision process of a learned model.
therefore we can not only identify newly discovered features but also analyse erroneous predictions.
to demonstrate the advantage ofsuchaninsight wevisualizetheattentioninfigure12fortwo cases found in our test set.
in each case we present the function with the highest attention score while highlighting the most attendedlineinblue.wehighlightthethreemostattendedtokensin the same line in red.
in addition we provide predictions of all comparedmethodson thegivenprobleminthe settingofthesc7 selection task.
analysing the specific cases we make the following observations new features lead to better decisions inthecaseof rangesum10 attn rlprovidesacorrectselectionincontrastto most other methods.
for this the representation learner had to discover a new feature which supports this decision.
indeed looking at the program elements with the highest attentionscore weseethatourmethoddiscoveredanewrelevantprogramfeaturenamely longlong to int cast.more precisely the model bases its decision on the long long division which is then cast to an integer to fit the returntype.
in fact such a cast can become problematic on 64bit systemswhereaconversionfrom64bit long long toa32bit intcanresultintounpredictablevalues.inthepresenceof unsafe conversions a verification technique like bmc in this case is required that can handle integers effectively on a bit precise level.
limitations lead to erroneous predictions whileattn rl improves the selection performance on average there exists a few cases where the method fails.
we provide the pro gramseq mthreaded as an erroneous case.
when we view theattentionvisualizationinfigure12 itbecomesclearwhy the prediction fails the variable argis represented in the cstasavariableaccess toavaluewithacustomdatatype.
therefore the selector wrongly bases its decision on the occurrence of a variable with a custom datatype inside a branch instead of other program elements.
however bool is a native type introduced in the c99 standard and most verifiers can handle this type natively.
although attn rl containsthisimprecisiontogeneralizetoarbitrarycstandards theanalysisofsucherrorcasesenablesthetailoring 7scischosenasitistheonlyselectiontaskwhereeveryselectorprovidesaprediction.
1025rangesum10 sc seq mthreaded sc program... int rangesum int x int i long long ret ret int cnt for i i i if i ret ret x cnt cnt if cnt !
return ret cnt else return ...... void assert bool arg if !
arg error verifier error ground truth bmc bam pa va bam ki cpa seq cpa selector va bam ki cpa seq dt selector cpa seq cpa seq verifolio bmc bam pa cpa seq kernel rpc cpa seq cpa seq attn rl bmc bam pa bmc bam pa figure example for predictions of different selectors taken from the test set of the selector to specific selection tasks.
in contrast kernel methods like verifolio and kernel rpc do not provide such an insight for it remains unclear why they provide the wrong selection on the rangesum10 task.
related work in this work we propose to learn program representations in order to improve algorithm selection for software verification.
we dis cusshowourapproachrelates toprevious workin areassuch asalgorithm selection for software verification and representation learning for programs.
algorithmselection.
theautomaticselectionofalgorithmshas a long successful history in many research areas including satsolving constraint satisfaction programs and combinatorial search problems .
however its potential for software verification was only recently discovered with the development ofmanually engineered andmachine learning based selectors .
manually engineered techniques are designed for a specific selection task at hand.
beyer et al.
p r o posedafixedsetofrulesforselectingapromisingverifiercombina tion.afzaletal.
adaptedtheideafortheproprietaryverification tool veriabs while chen et al.
employ specific rules to decide between two implemented verification algorithms.
extending these approaches to new selection tasks would require a complete redesign.incontrast ourproposedrepresentationlearnercanbe applied to a variety of selection tasks without modification.
machinelearningbasedtechniquescanadapttoagivenselection problem by synthesizing a decision model from data.
machine learnerlikemux andverifolio extractafixedsetoffeatures of an input program while learning the correlation between features and verifier performance.
in contrast to their work representationlearningdoesnotrequireanexplicitfeatureextractor but it learns an appropriate feature representation during training.
recently czechetal.
decidedtoleavethefeatureselectiontothe learningprocess whichwaslaterextendedbyrichteretal.
.incontrasttothesemethods ourincorporatedattentionmechanism enables us to view program fragments that are most relevant for the model decision.
this does not only increase the interpretability of representation learning but also lets us validate the decisions of our approach.finally there existsautomatic selectiontechniques which do not consider the input program at all.
programrepresentations.
theideaofcombiningprogrammodels and representation learning is widely adopted with applicationson sequential models asts dependencegraphs andcombinations thereof .
inthis section we give a brief overview over two existing techniques that are mostly related to our approach by targeting the transferability of code representations.
code2vec also bases its representation on information contained in an ast.
the approach aims to predict method names.
in the process code2vec samples a fixed limitednumber of paths.
a neural network learns the representation of pathsandtheiraggregationtoaprogramrepresentation.theaggregationemploysasimilarattentionmechanism.however dueto random sampling of a restricted number of paths code2vec might easily miss crucial information contained in the ast.
in contrast our approachlearns a deterministic program representation considering all information necessary for algorithm selection.
recently codebert wasproposedtolearnatransferableprogram representation based on transformers.
the technique was trained to model the relationship between code and its code documentation.
afterwards they tested the representation for also predicting method names.
codebert is in general more expressive than our approach but very resource inefficient.
codebert was pre trained on16highperformancegpuswhileonlyconsideringsequences of tokens.
even after employing the learned representation thesequencelengthwastruncatedat200tokensduetoresourceconstraints.
in contrast our approach can be trained on a singlegpu with examples of more than 1m tokens and it outperformsprevious selectors in runtime on a consumer level system withonly two cores.
overall we found that virtually all existing approachesonlytargetsmallprogramsnippetsinthecontextoflarge datasets.
this observation has partially motivated our work on algorithmselectiononsoftwareverification adomaininwhichwe can demonstrate that our representation learning works well with relatively small datasets and large programs.
conclusion in this paper we showed that representation learning can improve algorithm selection for software verification.
our representationlearner learns an aggregation of ast nodes into programmingconcepts.
for this purpose we proposed a hierarchical program model while adapting the transformer architecture to hierarchicallearning.theevaluationshowsthatmanuallydevelopedtechniquescanbereplacedbyarepresentationlearnerimprovingtheefficiencyandeffectivenessofaverificationsystem.furthermore thelearned representation can be transferred to new selection tasks givingsoftware engineers easy access to high performance algorithm selection in the context of their own verification tool.
in the future we intend to apply our hierarchical representation learner to other classification tasks for source code.
1026references mohammad afzal supratik chakraborty avriti chauhan bharti chimdyalwar priyankadarke ashutoshgupta shrawankumar charlesbabu divyeshunadkat and r venkatesh.
.
veriabs verification by abstraction and test generation competition contribution .
in international conference on tools and algorithms for the construction and analysis of systems.
springer .
takuya akiba shotaro sano toshihiko yanase takeru ohta and masanori koyama.
.
optuna anext generationhyperparameteroptimizationframework.inproceedingsofthe25rdacmsigkddinternationalconferenceonknowledge discovery and data mining.
miltiadisallamanis marcbrockschmidt andmahmoudkhademi.
.
learning to represent programs with graphs.
arxiv preprint arxiv .
.
miltiadis allamanis hao peng and charles sutton.
.
a convolutional attention network for extreme summarization of source code.
in international conference on machine learning.
.
uri alon shaked brody omer levy and eran yahav.
.
code2seq gen erating sequences from structured representations of code.
arxiv preprint arxiv .
.
uri alon meital zilberstein omerlevy and eranyahav.
.
ageneral path basedrepresentationforpredictingprogramproperties.
acmsigplannotices .
uri alon meital zilberstein omer levy and eran yahav.
.
code2vec learning distributed representations of code.
proceedings of the acm on programming languages popl .
jimmy lei ba jamie ryan kiros and geoffrey e hinton.
.
layer normalization.arxiv preprint arxiv .
.
dzmitry bahdanau kyunghyun cho and yoshua bengio.
.
neural ma chine translation by jointly learning to align and translate.
arxiv preprint arxiv .
.
rohanbavishi michaelpradel andkoushiksen.
.
context2name adeep learning based approach to infer natural variable names from usage contexts.
arxiv preprint arxiv .
.
ira d baxter andrew yahin leonardo moura marcelo sant anna and lorraine bier.
.
clonedetectionusingabstractsyntaxtrees.in proceedings.international conference on software maintenance cat.
no.
98cb36272 .
ieee .
tal ben nun alice shoshana jakobovits and torsten hoefler.
.
neural code comprehension a learnable representation of code semantics.
in advances in neural information processing systems.
.
dirkbeyer.
.
softwareverificationwithvalidationofresults reportonsv comp2017 .in toolsandalgorithmsfortheconstructionandanalysisofsystems 23rd international conference tacas held as part of the european joint conferencesontheoryandpracticeofsoftware etaps2017 uppsala sweden april proceedings partii lecturenotesincomputerscience axellegay and tiziana margaria eds.
vol.
.
.
dirkbeyerandmatthiasdangl.
.
strategyselectionforsoftwareverification based on boolean features a simple but effective approach.
in leveraging applicationsofformalmethods verificationandvalidation.verification 8thinternational symposium isola limassol cyprus november proceedings part ii lecture notes in computer science tiziana margaria and bernhard steffen eds.
vol.
.springer .
dirk beyer matthias dangl and philipp wendler.
.
boosting k induction withcontinuously refinedinvariants.in internationalconferenceoncomputer aided verification.
springer .
dirkbeyerandm.erkankeremoglu.
.
cpachecker atoolforconfigurable software verification.
in computer aided verification 23rd international conference cav2011 snowbird ut usa july14 .proceedings lecturenotes in computer science ganesh gopalakrishnan and shaz qadeer eds.
vol.
.
springer .
dirkbeyer merkankeremoglu andphilippwendler.
.
predicateabstraction with adjustable block encoding.
in formal methods in computer aided design.
ieee .
g. chen d. wang t. li c. zhang m. gu and j. sun.
.
scalable verifica tionframeworkforcprogram.in 201825thasia pacificsoftwareengineering conference apsec .
.
rewon child scott gray alec radford and ilya sutskever.
.
generating longsequenceswithsparsetransformers.
arxivpreprintarxiv .
.
kevinclark urvashikhandelwal omerlevy andchristopherdmanning.
.
what does bert look at?
an analysis of bert s attention.
arxiv preprint arxiv .
.
edmund clarke armin biere richard raimi and yunshanzhu.
.
bounded model checking using satisfiability solving.
formal methods in system design .
mike czech eyke h llermeier marie christine jakobs and heike wehrheim.
.
predicting rankings of software verification tools.
in proceedings of the 3rd acm sigsoft international workshop on software analytics swan esec sigsoft fse paderborn germany september olga baysalandtimmenzies eds.
.acm .
hoa khanh dam truyen tran and trang pham.
.
a deep language model for software code.
arxiv preprint arxiv .
.
yuliademyanova thomaspani helmutveith andflorianzuleger.
.
empirical software metrics for benchmarking of verification tools.
in computer aidedverification 27thinternationalconference cav2015 sanfrancisco ca usa july proceedings part i lecture notes in computer science daniel kroening and corina s. pasareanu eds.
vol.
.
springer .
yuliademyanova thomaspani helmutveith andflorianzuleger.
.
empirical software metrics for benchmarking of verification tools.
formal methods in systemdesign .
jacob devlin ming wei chang kenton lee and kristina toutanova.
.
bert pre trainingofdeepbidirectionaltransformersforlanguageunderstanding.
arxiv preprint arxiv .
.
zhangyin feng daya guo duyu tang nan duan xiaocheng feng minggong linjun shou bing qin ting liu daxin jiang et al .
.
codebert apre trainedmodelforprogrammingandnaturallanguages.
arxivpreprint arxiv .
.
jianmei guo dingyu yang norbert siegmund sven apel atrisha sarkar pavel valov krzysztof czarnecki andrzej wasowski and huiqun yu.
.
dataefficient performance learning for configurable systems.
empirical software engineering .
rahulgupta sohampal adityakanade andshirishshevade.
.
deepfix fixing common c language errors by deep learning.
in thirty first aaai conference on artificial intelligence.
huong ha and hongyu zhang.
.
deepperf performance prediction for configurable software with deep sparse neural network.
in ieee acm 41st international conference on software engineering icse .
ieee .
vincent j. hellendoorn charles sutton rishabh singh petros maniatis anddavid bieber.
.
global relational models of source code.
in international conference on learning representations.
b1lnbrntwr dan hendrycks and kevin gimpel.
.
gaussian error linear units gelus .
arxiv preprint arxiv .
.
xing hu ge li xin xia david lo shuai lu and zhi jin.
.
summarizingsource code with transferred api knowledge.
in proceedings of the twentyseventhinternationaljointconferenceonartificialintelligence ijcai2018 july stockholm sweden j r melang ed.
.ijcai.org .
https xing hu yuhan wei ge li and zhi jin.
.
codesum translate program language to natural language.
arxivabs .
.
cheng zhiannahuang ashishvaswani jakobuszkoreit noamshazeer iansi mon curtishawthorne andrewmdai matthewdhoffman monicadinculescu anddouglaseck.
.musictransformer.
arxivpreprintarxiv .
.
e. h llermeier j. f rnkranz w. cheng and k. brinker.
.
label ranking by learning pairwise preferences.
artificial intelligence .
armand joulin laurens van der maaten allan jabri and nicolas vasilache.
.
learningvisualfeaturesfromlargeweaklysuperviseddata.in european conference on computer vision.
springer .
christian kaltenecker alexander grebhahn norbert siegmund and sven apel.
.
theinterplayofsamplingandmachinelearningforsoftwareperformance prediction.
ieee software .
diederik p kingma and jimmy ba.
.
adam a method for stochastic optimization.
arxiv preprint arxiv .
.
larskotthoff.
.
algorithmselectionforcombinatorialsearchproblems a survey.
in data mining and constraint programming.
springer .
wenchaoli hassensaidi huascarsanchez martinsch f andpascalschweitzer.
.
detecting similar programs via the weisfeiler leman graph kernel.
in icsr lncs vol.
.
springer .
minh thang luong hieu pham and christopher d manning.
.
effec tive approaches to attention based neural machine translation.
arxiv preprint arxiv .
.
dhruv mahajan ross girshick vignesh ramanathan kaiming he manohar paluri yixuan li ashwin bharambe and laurens van der maaten.
.
exploringthelimitsofweaklysupervisedpretraining.in proceedingsoftheeuropean conference on computer vision eccv .
.
ukasz maziarka tomasz danel s awomir mucha krzysztof rataj jacek tabor and stanis aw jastrz bski.
.
molecule attention transformer.
arxiv preprint arxiv .
.
stevenminton.
.
automaticallyconfiguringconstraintsatisfactionprograms a case study.
constraints .
lilimou geli zhijin luzhang andtaowang.
.
tbcnn atree based convolutional neural network for programming language processing.
arxiv preprint arxiv .
.
vivek nair tim menzies norbert siegmund and sven apel.
.
using bad learnerstofindgoodconfigurations.in proceedingsofthe201711thjointmeeting on foundations of software engineering esec fse paderborn germany september4 ericbodden wilhelmsch fer arievandeursen andandrea zisman eds.
.
acm .
toan q nguyen and julian salazar.
.
transformers without tears improving the normalization of self attention.
arxiv preprint arxiv .
.
niki parmar ashish vaswani jakob uszkoreit ukasz kaiser noam shazeer alexander ku and dustin tran.
.
image transformer.
arxiv preprint arxiv .
.
felixpauck ericbodden andheikewehrheim.
.
doandroidtaintanalysistoolskeeptheirpromises?.in proceedingsofthe2018acmjointmeetingon european software engineering conference and symposium on the foundations of software engineering esec sigsoft fse lake buena vista fl usa november04 garyt.leavens alessandrogarcia andcorinas.pasareanu eds.
.
acm .
john platt.
.
probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.
in advances in large margin classifiers.
mit press .
michael pradel and koushik sen. .
deepbugs a learning approach to namebased bug detection.
proceedings of the acm on programming languages oopsla .
lina qiu yingying wang and julia rubin.
.
analyzing the analyzers flowdroid iccta amandroid anddroidsafe.in proceedingsofthe27thacmsigsoft internationalsymposiumonsoftwaretestingandanalysis issta2018 amsterdam thenetherlands july16 franktipandericbodden eds.
.acm .
benjamin recht rebecca roelofs ludwig schmidt and vaishaal shankar.
.
classifiersgeneralizetoimagenet?
arxivpreprint arxiv .
.
johnrice.
.
thealgorithmselectionproblem.
advancesincomputers .
cedric richter eyke h llermeier marie christine jakobs and heike wehrheim.
.
algorithmselectionforsoftwarevalidationbasedongraphkernels.
autom.
softw.
eng.
.
cedricrichterandheikewehrheim.
.
pesco predictingsequentialcombinations of verifiers competition contribution .
in tools and algorithms fortheconstructionandanalysisofsystems 25yearsoftacas toolympics held as part of etaps prague czech republic april proceed ings part iii lecture notes in computer science dirk beyer marieke huisman fabrice kordon and bernhard steffen eds.
vol.
.
springer .
srasoulsafaviananddavidlandgrebe.
.
asurveyofdecisiontreeclassifier methodology.
ieee transactions on systems man and cybernetics .
b.sch lkopfandaj.smola.
.
learningwithkernels supportvectormachines regularization optimization and beyond.
mit press.
bobak shahriari kevin swersky ziyu wang ryan p adams and nando de freitas.
.
taking the human out of the loop a review of bayesian optimization.
proc.
ieee .
norbertsiegmund alexandergrebhahn svenapel andchristiank stner.
.
performance influence models for highly configurable systems.
in proceedings of the 10th joint meeting on foundations of software engineering.
.
nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov.
.
dropout asimplewaytopreventneuralnetworksfrom overfitting.
the journal of machine learning research .
varun tulsian aditya kanade rahul kumar akash lal and aditya v. nori.
.
mux algorithmselectionforsoftwaremodelcheckers.in 11thworking conference on mining software repositories msr proceedings may june hyderabad india premkumart.devanbu sungkim andmartinpinzger eds.
.
acm .
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin.
.
attention is all y o un eed .i nadvances in neural information processing systems.
.
qiang wang bei li tong xiao jingbo zhu changliangli derek fwong and lidiaschao.
.
learningdeeptransformermodelsformachinetranslation.
arxiv preprint arxiv .
.
wenhan wang ge li bo ma xin xia and zhi jin.
.
detecting code clones with graph neural network and flow augmented abstract syntax tree.
in ieee 27th international conference on software analysis evolution and reengineering saner .
ieee .
lin xu frank hutter holger h hoos and kevin leyton brown.
.
satzilla portfolio based algorithm selection for sat.
journal of artificial intelligence research32 .
jianzhang xuwang hongyuzhang hailongsun kaixuanwang andxudong liu.
.
anovelneuralsourcecoderepresentationbasedonabstractsyntax tree.
in2019 ieee acm 41st international conference on software engineering icse .
ieee .