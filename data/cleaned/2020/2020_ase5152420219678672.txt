towards exploring the limitations of active learning an empirical study qiang hu1 yuejun guo1 maxime cordy1 xiaofei xie2 wei ma1 mike papadakis1 and yves le traon1 1university of luxembourg luxembourg2nanyang technological university singapore abstract deep neural networks dnns are increasingly deployed as integral parts of software systems.
however due to the complex interconnections among hidden layers and massivehyperparameters dnns must be trained using a large numberof labeled inputs which calls for extensive human effort forcollecting and labeling data.
spontaneously to alleviate thisgrowing demand multiple state of the art studies have developeddifferent metrics to select a small yet informative dataset forthe model training.
these research works have demonstratedthat dnn models can achieve competitive performance using acarefully selected small set of data.
however the literature lacksproper investigation of the limitations of data selection metrics which is crucial to apply them in practice.
in this paper wefill this gap and conduct an extensive empirical study to explorethe limits of data selection metrics.
our study involves dataselection metrics evaluated over datasets image classificationtasks and text classification tasks dnn architectures and20 labeling budgets ratio of training data being labeled .
ourfindings reveal that while data selection metrics are usuallyeffective in producing accurate models they may induce a loss ofmodel robustness against adversarial examples and resilience tocompression.
overall we demonstrate the existence of a trade offbetween labeling effort and different model qualities.
this pavesthe way for future research in devising data selection metricsconsidering multiple quality criteria.
index t erms deep learning data selection active learning empirical study i. i ntroduction deep learning dl has achieved tremendous success in various cutting edge application domains such as image processing machine translation autonomous vehicles and robotics .
two key elements to achieve high performingpredictions are well designed deep neural networks dnns with appropriate architecture and parameters and a carefullychosen set of labeled training data.
however data labelingis expensive and time consuming because it requires a largeamount of human effort.
for example that it took more than 3years to prepare the first version of the imagenet dataset.thus acquiring labeled data is seen as a major obstacle to thewidespread adoption of dl .
one established solution to reduce data labeling cost is active learning i.e.
incremental methods to select infor mative subsets of training data to undergo labeling in a waythat the produced model is as accurate as if it was trained onall data.
with active learning engineers can thus compromiselabeling effort with model performance e.g.
classificationaccuracy .
there has been much research on devising active corresponding author.learning methods each relying on different data selectionmetrics.
these metrics typically exploit informationof the dnn under training e.g.
its gradient or uncertainty to select the most informative data to label next.
on the other hand recent work on dl testing and debugging have proposed different metrics for test generation and test selection i.e.
the problem of selecting test data that are more likely to be misclassified by the model .
as inactive learning scenarios these test data can then be used toimprove the model by retraining .
the proliferation of data selection metrics coming from active learning and testing makes it challenging for engineersto decide which one they should use.
indeed the differentmetrics have been evaluated on a restricted set of problems mostly image classification datasets under incomparable ex perimental settings different models and labeling budget .
there is therefore a need for a comprehensive study ofall these data selection metrics on a common ground involvingdifferent classification tasks.
another gap in the current body of knowledge is that most experimental studies evaluate the metric wrt.
the test accuracyof the trained models.
1other key quality indicators have been ignored such as the model robustness to adversarialattacks and the model performance after compression.
thelack of consideration for these indicators raises practical issueswhen deploying dl models trained using active learning consider e.g.
biomedical image segmentation or mobiledl applications .
hence one should make sure that activelearning can produce models whose quality is not limited toclassification accuracy but extends to allquality indicators relevant for the use case.
to fill these gaps in this paper we conduct a comparative empirical study to explore the potential limitations ofactive learning.
our study involves data selection metricsevaluated over datasets image classification tasks and3 text classification tasks dnn architectures and 20labeling budgets ratio of training data that can be labeled .specifically our study aims to answer the following fourresearch questions rq1 how effective are the different data selection metrics for producing accurate models?
we answer this question by measuring how fast i.e.
how many training dataare required the accuracy of the trained model converges to 1rarely some studies measure empirically the robustness of the model to adversarial attacks.
36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee the accuracy of the fully trained model i.e.
trained with the full training set .
besides we employ random selection asa baseline to compare the effectiveness of each metric.
ourresults indicate significant differences between metrics up to45.
of test accuracy especially when less than of thetraining data is used.
rq2 how robust are models trained with active learning?
we answer this question by measuring the theoretical robustness of the trained models using the clever score as well as their empirical robustness against multipleadversarial attacks.
for the image classification task ourresults reveal a gap between the fully trained models and thosetrained with active learning up to .
clever score and23.
of success rate whereas there are no such differencesin the text classification task.
rq3 do models trained using active learning maintain accuracy after compression?
model compression a wellknown technique to embed dl models in resource constraineddevices has the downside effect of reducing the accuracydue to precision loss in the computed model weights.
weinvestigate whether models trained with active learning aremore sensitive to this phenomenon than fully trained models.our results reveal that for the image classification task training on of data entails a loss in test accuracy upto .
higher compared to training with the full dataset.
rq4 what is the relationship between the amount of training data and model robustness and accuracy aftercompression?
we conduct additional experiments where we increase the data budget of data selection metrics.
our resultsindicate that with the growth of training data the robustnessof the model will also increase.
the model can achieve similarrobustness with the fully trained model only using data and even outperform it when more data are used.
this indicatesthat the training process promoted by active learning can beused as an effective way to increase robustness.
as for modelcompression there appears to be no relationship betweenaccuracy decay induced by compression and data budget.
with our extensive empirical study we provide practical guidance to engineers in balancing the benefits of data selec tion metrics with their potential side effects.
that is we showand quantify the existence of a trade off between the efficiencyof model training in particular the data labeling effort andmodel properties of interest viz.
robustness and accuracy afterquantization .
so we also open research directions toexplore this trade off and new data selection metrics aimedtowards the different quality criteria.
in summary the main contributions of this paper are we conduct the largest empirical study that investigatesthe effectiveness of training data selection metrics ondifferent classification tasks image and text .
beyond test accuracy we explore the effects of reducingthe number of training data on the adversarial robustnessof the models and their accuracy after compression.
we therefore reveal the potential effects that active learningcan have on these quality indicators.
thereby we reveal a potential trade off between labelingcost and the aforementioned quality indicators.
this pavesthe way for future research in designing multi objectivedata selection metrics aiming at optimizing this trade offunder a constrained labeling budget.
the rest of this paper is organized as follows.
section ii introduces some background knowledge of this work.
sectioniii presents an overview of the study.
section iv introducesthe implementation and empirical configurations.
section vdetails the results of our study.
section vi presents the relatedworks and section vii concludes this paper.
ii.
b ackground we briefly introduce the background related to our work including dnns test selection and active learning adversarialattacks on dnns and model compression.
throughout the paper xrefers to the training set for a n class classification dnn.
x x r dis an input and x primeis its adversarial example.
yandyxindicate the true and predicted labels respectively.
pi x i n represents the predicted probability of xbelonging to the ith class and correspondingly y a r gm a x i n pi x .
a. deep neural networks in general a dnn consists of multiple layers i.e.
an input layer several hidden layers and an output layer.
as shownin figure each layer comprises a number of neurons colorcircles .
the neuron with the parameters also called a unitor a node is the basic entity of computation of a dnn.
itreceives information from the input data or the other neuronsand computes an output by an activation function.
the trainingprocess of a dnn is mainly about tuning the parameters toreach a minimum prediction error concerning true labels.
inthis paper we focus on the classification task where the outputof a dnn classifier is the probability of belonging to eachcategory given an input.
for instance the input data in figure1 is predicted to be in class dog with a probability of .
.
generally there are two typical types of dnns i.e.
feedforward neural networks fnns and recurrent neural net works rnns .
in an fnn the information only moves in theforward direction from the input layer to the output layer.
thistype of dnns is widely used in image processing applications.on the other hand an rnn utilizes different inter units i.e.
memory cells control units to propagate the input informationin a backward way within an rnn layer allowing the networkto retain knowledge.
rnns usually deal with sequential dataprocessing due to their ability to capture temporal informationof the data.
in this work we study both fnns and rnns.
b. active learning active learning a well known concept in both the software engineering se community and machine learning ml community trains a model incrementally with several steps.in a typical active learning procedure in the beginning amodel is randomly initialized.
in each step of the training it selects a few data from the unlabeled dataset to label and fig.
.
an example of a dnn feed forward neural networks classifier.
then retrains the model for better performance.
in other words the goal for each step is to reduce the cost of labeling asmuch as possible by selecting the most informative data toannotate while improving the accuracy of a pre trained model.therefore active learning can help in model evolution.
forinstance in reality a model needs to be updated over timedue to the rapid growth of new unlabeled data.
c. test selection in the traditional field of se test selection has attracted extensive research attention for a long time.
for instance test selection is widely applied in regression testing whichaims at reducing the size of test suites since executing theentire set is remarkably costly.
depending on the purpose the selected test suites can help to eliminate redundant testcases test suite minimization test relevant changed partsof the software test case selection and locate faults early test case prioritization .
a similar concept to test selection isfeature selection that is thoroughly studied as well.the difference is that feature selection focuses on seeking theoptimal features in a data set to allow efficient execution whiletest selection tries to reduce the size of data.
in practice for dl based software systems collecting unlabelled data is easy and cheap but labeling all of them requiresheavy work and specific domain knowledge.
following thesame spirit of test selection in traditional se recent researchproposed some test selection metrics for dl systems likethe neuron coverage based test selection.
these metrics selectthe most useful subset of unlabeled test data for both testingdnns and improving the performance of pre trained dnnsvia retraining.
we consider that these test selection metrics arepromising to apply in active learning for two main reasons.first the procedure of test selection and then retraining generally speaking can be regarded as one step active learning active learning being by nature an incremental process.second the test selection metrics share the same goal ofdetermining the most useful data given a dnn.
for instance an active learning process could measure this utility as thenumber of new neurons that these data activate.
d. adversarial attacks on dnns dnns have been proven to be vulnerable to adversarial examples which causes considerable security concerns .
therefore evaluating the ability robustness of a dnn to dealwith adversarial examples is a crucial part of dnn testing.
anadversarial example is a variant of input data by introducing asmall perturbation that is hardly recognized by human beingsbut can easily fool dnns.
the perturbation is not just randomnoise but carefully calculated by some adversarial attacks.
inthis study we employ three powerful attacks fgsm jsma c w for image classification and two pwws and dwb for text classification.
fgsm.
goodfellow et al.
proposed the fast gradient sign method fgsm to generate adversarial examples whichis the first gradient based and one of the most used attacks.fgsm crafts x primebyx prime x epsilon1 sign triangleinv xj x y where epsilon1 controls the perturbation size.
sign is the sign function.
the sign of a real number is for a negative value fora positive value and for value .
triangleinv xj x y computes the gradient of the training loss jgivenxand its true class y. jsma.
the jacobian based saliency map attack jsma first computes a saliency map by the jacobian matrix.
themap presents how influential each feature e.g.
each pixel ofthe input is to predict a particular class.
through exploitingthis map with targeting a class that does not match the trueclass of a given test sample x jsma modifies xat where the pixels have high saliency values to generate an adversarialexample that might be classified within a predefined threshold maximum fraction of features being perturbed .
c w. proposed by carlini and wagner c w is known as one of the strongest adversarial attacks.
it usesa designed loss function fto replace the training loss then generates the adversarial example x primewhich minimizes dis x x prime c f x prime wheredisis a distance metric and cis a constant that controls the distance and the confidence of x prime.
pwws.
the probability weighted word saliency pwws a word level attack generates text adversarial examplesby replacing original words with synonyms searched from alexical database e.g.
wordnet .
given a word pwwsselects a substitution concerning two factors.
how does theclassification change if replacing this word with a substitution?
how does the classification change if ignoring this word?
dwb .
the deepwordbug dwb is a black box charlevel adversarial attack which follows two steps to generateadversarial examples.
first dwb determines the words tomodify by the change of predictions before and after replacingthe words with unknown tokens.
second it modifies the selected words slightly by changing at most two letters perword through a predefined manner e.g.
alter world towor1d.
e. model compression model compression is important for efficient model deployment in software systems especially when using large models e.g.
big dnns.
the goal of model compression is to reducethe model size while maximally maintaining the performancein terms of accuracy before practical deployment.
next weintroduce the two compression techniques considered in ourwork.
model pruning.
model pruning lightens the model by removing redundant and unimportant connections that havelittle impact on the performance.
in this study we apply two 919basic pruning strategies the weight level pruning and the neuron level pruning .
the weight level method sets theweights that are smaller than a threshold to zero.
the neuron level pruning removes the neurons that have a high chanceof being inactivated.
correspondingly the related connections weights are also eliminated.
model quantization.
in dnn weights are stored in the bit floating point format.
to compress the model thequantization technique converts the weights from bit intolow bit e.g.
bit integer .
iii.
o verview we first introduce the three phase design of our empirical study then present data selection metrics datasets and models and evaluation measures that are studied in our work.
the secure life cycle of deep learning is composed of some key stages from the requirement analysis and data label pair collection to the maintenance and evolution of thedeep learning model.
this paper studies the effect of the datacollection i.e.
active learning on the model developmentand deployment i.e.
the model quality .
figure gives anoverview of our study which consists of three phases effec tiveness analysis of model training with different data selectionmetrics adversarial robustness analysis of the trained model and performance analysis after model deployment.
overall we compare the models trained using the entire dataset and asubset of data selected by a specific data selection metric.
a. study design more specifically in the first phase data collection we compare the effectiveness of each data selection metric for training a model.
using different metrics e.g.
entropy mar gin we iteratively select a subset of training data and train themodel then observe the convergence trend of the test accuracy.for comparison we also train two baseline models using entiretraining data and randomly selected data respectively.
in the second phase model development we evaluate the robustness of the trained model.
in our study we applymultiple metrics e.g.
empirical robustness clever score to compare the robustness of models trained with the selectedand the model trained by the entire training data.
in the third phase we focus on the model performance test accuracy in the model deployment phase.
in general a dnnmodel usually consists of a huge number of parameters e.g.
a vgg16 model requires about 258mb of hard disk memory.before deploying such a large dnn model into the hardware e.g.
mobile devices one has to consider the performanceincluding the required memory and inference speed.
we adopttwo well known techniques model quantization and modelpruning to optimize a trained model.
then we evaluate theaccuracy of the optimized models.
besides we study theimpact of training data size on the robustness of models andthe accuracy of optimized models.
finally based on the results of our empirical study we provide some practical guidelines for the usage of activelearning on different tasks and summarize some potentialresearch directions.b.
datasets and dnn models we conduct experiments with two popular image datasets mnist and cifar and three widely used textdatasets imdb tagmynews and yahoo!
answers .
mnist includes class grayscale images of hand written digits.
the dataset includes and trainingand test data respectively.
cifar is a collection of class color images e.g.
airplane bird .
the dataset consistsof and training and test data respectively.
imdbis a dataset including movie reviews widely used for textsentiment analysis binary classification .
both the trainingand test sets include text reviews.
tagmynews providesnews headlines text in categories e.g.
sport business .we randomly collect data for training and datafor testing.
yahoo!
answers consists of text data of topiccategories e.g.
society culture and science mathemat ics .
we obtain this data from directly with trainingdata and test data.
for each dataset we employ two dnn architectures to reduce the model dependent influence on the results.
formnist we use two well known convolutional neural net works lenet and lenet and for cifar we selecttwo models nin and vgg16 both of which achievehigh accuracy.
for imdb tagmynews and yahoo!
answers we use two types of rnns lstm and gru derived from abase model .
our companion website presents all detailsabout the models and training parameters .
c. data selection metrics various data selection metrics have been proposed and verified to reduce the labeling effort.
note that the data selection metric is also known as the acquisition function inthe ml community.
we include data selection metrics fromboth the ml community metrics and the se community 6metrics .
we first introduce the ones from the ml community.
entropy considers the uncertainty of data using the prediction output.
this metric is based on the shannon entropyof the prediction probability arg max x x parenleftbiggn summationdisplay i 1pi x l o gpi x parenrightbigg margin computes a score for each data by the difference between its top prediction probabilities margin x p k x pj x wherek a r g m a x i n pi x andj a r g m a x i n k pi x .
the training data with low scores will be selected for training.
k center firstly divides data into kgroups via some unsupervised machine learning methods e.g.
k means clus tering then selects the center of each group if not enough consider the data that are close to the center .
these selecteddata are regarded as the representative of the entire group.
expected gradient length egl assumes that the model has no knowledge of the true label of data in advance.
!
!
!
!
fig.
.
overview of experiment design for each data it computes the expectation of the gradients by assigning all the labels to x. the data that have great expectations are selected.
egl can be presented as follows egl x n summationdisplay i 1pi x triangleinvxj x i where .
is the euclidean norm.
triangleinvxj x i is the gradient of the lossjgivenxand label i. bayesian active learning by disagreement bald applies dropout to select the most uncertain data arg max x x parenleftbigg count parenleftbig mode parenleftbig y1 x ... yt x parenrightbig parenrightbig t parenrightbigg wheretis the number of applying dropout to the model.
entropy dropout and margin dropout apply dropout and calculate the average score of entropy and margin overall dropout models e.g.
entropy dropout is defined as arg max x xt summationtext i 1entropy parenleftbig xi parenrightbig t whereentropy parenleftbig xi parenrightbig is the entropy of xat thei th time.
adversarial active learning adversarial al leverages an adversarial attack deepfool to facilitate selectingdata.
concretely the model predicts labels for each data thendeepfool introduces perturbations to each data until reachesan adversarial example.
finally the data requiring smallerperturbations will be selected.
intuitively such data are closeto the decision boundary of the model.
next we introduce the metrics from the se community.
neuron coverage nc selects data that have the largest neuron coverage arg max x x neu neu neurons activate neu x neurons whereactivate neu x indicates that the neuron neu is activated by x namely the output of neu is greater than a predefined threshold.
we highlight that this is a variant of theoriginal metric to fit active learning.
the original nc aims atselecting data to cover all the neurons however in practice just a few data are enough to reach coverage .namely after a few selection steps all the neurons have beenactivated at least once and the marginal increase of coveragewill be .
thus we apply this variant to fit active learning.
k multisection neuron coverage kmnc improves nc by splits the lower and upper bounds of a neuron s outputintoksections.
instead of using the coverage of neurons it considers the coverage of sections.
similar to nc the datawith high coverage will be selected.
multiple boundary clustering and prioritization mcp is an extension of margin.
first it divides data intovarious boundary areas based on the top predicted classes e.g.
for data with classes there are p permutations of pairwise classes.
next mcp selects data fromeach area based on margin.
deepgini selects the most uncertainty data by arg max x x parenleftbigg n summationdisplay i pi x parenrightbigg likelihood based surprise adequacy lsa and distancebased surprise adequacy dsa measure the surprise adequacy by the dissimilarity between a test data and thetraining set.
the difference between lsa and dsa is thatlsa uses kernel density estimation to estimate the surpriseadequacy while dsa uses euclidean distance.
both select datawith the largest surprise adequacy.
we remark that although the initial purpose of these se metrics is not for active learning their underlying selectioncriteria share similarities with the criteria that active learningmetrics rely on.
for example both entropy active learningmetric and deepgini se metric metrics select the uncertaindata based on the output probabilities.
therefore we believe 921that it is necessary to consider them in our study.
besides no existing research has revealed that whether these se metricsfit in active learning or how they perform.
d. evaluation metrics effectiveness.
to evaluate the effectiveness of a selection metric we use random selection as the baseline and calculatethe difference of accuracy between the models trained usingrandom selection and this metric.
the difference is defined by diff steps summationdisplay i parenleftbig acci ts accitr parenrightbig wheresteps is the total number of training steps.
acci tsis the test accuracy of the model trained by a selection metric andacci tris by random selection.
diff shows the degree of a metric outperforming random selection.
adversarial robustness.
the robustness of dnns refers to the ability to cope with adversarial examples.
the robustness of dnns can be evaluated in multiple ways.
we introducetwo popular methods of robustness estimation empirical ro bustness and clever score .
empirical robustness this method quantifies the robustness of a dnn model through the success rate of crafting adversarial examples byan attack.
in practice given a dnn and a test set s a n attack attempts to craft an adversarial example based on eachtest data.
the attack success rate is the ratio of adversarialexamples successfully generated asr x x s y x prime negationslash y s recall that yx primeandyare the predicted label of x primeand true label of x respectively.
a small asr indicates a robust dnn.
clever score the cross lipschitz extreme value fornetwork robustness clever score calculates the lowerbound for crafting an adversarial example given an input which is the least amount of perturbation required to fool adnn model.
a great clever score indicates a robust dnn.
iv .
i mplementation and configuration experimental environment.
this project is implementedbased on keras and tensorflow frameworks.
werun all experiments on a high performance computer clusterexcept the model pruning and quantization.
each cluster noderuns a .
ghz intel xeon gold cpu with an nvidiatesla v100 16g sxm2 gpu.
for the model pruning andquantization we conduct the experiments on a macbook prolaptop with macos big sur .
.
with a 2ghz ghz quad core intel core i5 cpu with 16gb ram.
active learning.
we initialize an empty labeled pool and an unlabeled pool with all the unlabeled data.
besides weinitialize the with random weights.
next in each trainingstep a fixed number step size of data are selected from theunlabeled pool by a specific metric.
then the selected dataare merged into to labeled pool after annotation.
the dnn isupdated by retraining using all the data in the labeled pool.
thetable i configurations of active learning dataset model step size stop point entire training size mnist lenet lenet cifar nin vgg16 imdb lstm gru tagmynews lstm gru yahoo!answers lstm gru table ii configurations of adversarial attacks .for the definition of the parameters epsilon1 c dis please refer to section ii.
datasetfgsm jsma c w clever epsilon1 c dis mnist .
.
.
.
.
.
11l l l infcifar .
.
.
.
.
.
.
.
.
procedure terminates when the size of the labeled pool reachesa threshold stop point .
table i lists the detailed settings.
notethat previous works have different parameter settings we balance these settings to set up our experiments.we implement data selection metrics based on .
robustness.
we use two public libraries foolbox for empirical robustness evaluation and art for cleverscore calculation.
in empirical robustness we apply threeattack methods i.e.
fgsm jsma and c w for the imageclassification task and conduct three groups of experimentswith different parameters as in .
for the text classificationtask we use pwws and dwb with the default setting in to attack the text related models.
by default onlythe correctly classified data undertake the attacks.
in cleverscore the setting of canddisfollows the configuration in .
one difference is that we use test data to calculate theclever score while used .
the detailed informationis shown in table ii.
note that the setting of clever worksboth for the image and text classification tasks.
model compression in model quantization we apply two lightweight frameworks coreml and tensorflowlite to transform dnns into different bit level versions.
forcoreml we use three levels bit bit and bit.
fortensorflowlite we apply bit and bit level quantizationsin our models since it only supports these two levels.
in modelpruning for both the weight and neuron level we prune adnn into six compress versions with different degrees from10 to at intervals using the implementations by .
as a reminder model pruning is conducted afterthe model is well trained.
last but not least to reduce the influence of randomness we repeat each experiment three times and compute the averageresults.
in total we trained and evaluated more than 2000models in this study.
the source code can be found on .
v. e xperimental results in this section we present the results and answer each research question mentioned in section i. in the remainingparts we remark that te ts and random represent 922table iii effectiveness with respect to random selection baseline .
the results above the baseline are highlighted in gray .
mnist cifar imdb tagmynews yahoo!answersmetriclenet lenet nin vgg16 lstm gru lstm gru lstm gruaverage entropy .
.
.
.
.
.
.
.
.
.
.
margin .
.
.
.
.
.
.
.
.
k center .
.
.
.
.
.
.
.
.
.
.
egl .
.
.
.
.
.
.
.
.
.
.
bald .
.
.
.
.
.
.
.
.
.
.
entropy dropout .
.
.
.
.
.
.
.
.
.
.
margin dropout .
.
.
.
.
.
.
.
.
adversarial al .
.
.
.
.
nc .
.
.
.
.
.
.
.
.
.
.
kmnc .
.
.
.
.
.
.
.
.
.
.
mcp .
.
.
.
.
.
.
.
.
deepgini .
.
.
.
.
.
.
.
.
lsa .
.
.
.
.
.
.
.
.
.
.
dsa .
.
.
.
.
.
.
.
.
.
.
training using the entire dataset the subset selected by data selection metrics and randomly selected data respectively.note that due to the page limitation we only illustrate a part ofthe results in this paper and the complete results are providedas supplementary material .
the conclusions we drawbelow generalize to all studied datasets subjects and dnns.
a. rq1 effectiveness of data selection metrics first we show in figure the performance of using different data selection metrics to train a dnn that achieves the same test accuracy as the fully trained model.
for comparison a horizontal dashed line in each sub figure represents theaccuracy of the fully trained model.
overall most metricsmanage to produce models with the same accuracy as the fullytrained model by using only to of training data.
next table iii offers a more detailed comparison of these metrics showing their effectiveness taking random selectionas a baseline measured using equation .
surprisingly onlythree metrics margin margin dropout mcp significantlyoutperform the baseline in all cases.
on the contrary in mostcases out of egl is worse than random selection.
thereare some metrics like lsa and dsa which outperform thebaseline on the image classification task but underperform thebaseline on the text classification task e.g.
on yahoo lstmand yahoo gru and the difference reaches up to .
.
weconjecture that lsa and dsa tend to select data based onsimilarity or distance.
however different from image data the two texts data sentence or document might have a bigdifference in the hidden space even if they are in the samecategory.
in this case the inter output of the model againstthese two data can be completely different which makes lsaand dsa select the wrong data.
specifically considering the image classification task we observe that in addition to margin margin dropout and mcp two other metrics lsa and dsa can always outperformthe baseline.
what s more on lenet and lenet halfof the metrics are worse than the random baseline.
espe cially entropy entropy dropout nc and deepgini get highnegative differences e.g.
.
for entropy on lenet which means these four metrics are much worse than randomselection.
turn to sub figures a and b in the first fewtraining steps where the big difference comes from thesefour metrics achieve much less test accuracy than the others also random selection .
one explanation is that entropy entropy dropout and deepgini try to find the most uncertaindata.
however such uncertain data are hard to be learned bymodels that are not well trained.
on the other hand for the text classification task the output probability based metrics e.g.
margin and bald performbetter than the coverage and surprise adequacy based metrics.extremely lsa performs worse than the baseline on out of6 models.
besides on model yahoo lstm the most efficient metric margin is much better than the worst one kmnc with a .
test accuracy gap.
these results reflect thatboth coverage and surprise adequacy based metrics are notsuitable for the text classification task.
answer to rq1 the nature of the classification task significantly affects the effectiveness of multiple data selectionmetrics e.g.
lsa and dsa .
therefore the limitation ofactive learning experiments to a single target task evenwith multiple datasets constitutes a critical threat to externalvalidity.
some metrics like margin margin dropout andmcp consistently perform well across all labeling budgets models and tasks.
b. rq2 adversarial robustness we then study the robustness of models trained by different data selection metrics against adversarial attacks.
figure illustrates the empirical robustness of the models againstvarious attacks.
once again we have to distinguish the twotypes of tasks since the results of different metrics are greatlybiased on the tasks.
for the image classification task the temodels are usually more robust than the ts models.
however for the text classification task the ts models are in somecases more robust than the te models ad vice versa .
weconjecture that two factors may affect the robustness of themodels i the number of data used for training and ii thetraining process.
in active learning the early selected data aretrained more times during incremental learning.
thus the mostinformative data according to the data selection metrics havea larger influence on the model weights and in turn impact itsrobustness.
on the other hand since the selected data can berepresentative of the entire dataset to some extent the differ ence between the ts and te models is small.
taking vgg16as an example the difference varies from .
to .
in fgsm from .
to .
in c w and from .
to .
in jsma.
another observation that reinforces ourhypothesis regarding the importance of the training processis that none of the metrics performs consistently better thanrandom selection.
we investigate this hypothesis in the rq4experiments where we consider different labeling budgets.
table iv lists the clever score of different models.
as for the text classification task models trained with activelearning either yield a small improvement less than .58clever score or offer inconsistent benefit either increasingor decreasing the clever score depending on the consideredmetric and norm distance .
besides none of the data selectionmetrics improves over the random selection even the threemetrics which performed better in terms of effectiveness viz.
data size0.
.
.
.
.0test accuracydsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp adversarial al kmnc deepgini random a mnist lenet data size0.
.
.
.
.0test accuracydsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp adversarial al kmnc deepgini random b mnist lenet data size0.
.
.
.
.
.
.
.9test accuracydsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp adversarial al kmnc deepgini random c cifar nin0 data size0.
.
.
.
.
.
.90test accuracydsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp adversarial al kmnc deepgini random d cifar vgg160 data size0.
.
.
.
.
.
.
.85test accuracydsa lsa entropy bald k center entropy dropout egl nc kmnc random e imdb lstm data size0.
.
.
.
.
.
.
.85test accuracydsa lsa entropy bald k center entropy dropout egl nc kmnc random f imdb gru0 data size0.
.
.
.
.
.
.
.9test accuracydsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp kmnc deepgini random g tagmynews lstm0 data size0.
.
.
.
.
.
.
.9test accuracydsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp kmnc deepgini random h tagmynews gru0240 1d a d t s iz0e80e20e.0e30e40e50e60e70e9tz a tdccurdcy1s a lsa ena ropy bal1 k cznazr md rgs n ena ropy dropoua mdrgs n dropoua egl nc mcp kmnc 1zzpgs ns rdndom i yahoo lstm0240 1d a d t s iz0e80e20e.0e30e40e50e60e70e9tz a td ccurd cy1s a lsa ena ropy bal1 k cznazr md rgs n ena ropy dropoua mdrgs n dropoua egl nc mcp kmnc 1zzpgs ns rdndom j yahoo gru fig.
.
evolution of the test accuracy y axis achieved by different data selection metrics given the number x axis of training data.
fgsm .
fgsm .
fgsm .03cw .
cw .
cw .3jsm a .
jsm a .
jsm a .
entropy bald k center margin entropy dropout margin dropout egl nc mcp adversarial al kmnc deepgini lsa dsa random te67.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6065707580859095success rate a cifar vgg16 teentropybaldk centermarginentropy dropout margin dropout egl ncmcpkmncdeepgini lsa dsarandom lstm pwws lstm dwb gru pwws gru dwb82.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5success rate b tagmynews fig.
.
adversarial attack success rate of vgg16 and tagmynews.
the number in each cell represents the success rate and the color gives a straightforward visual comparison of different values.
the most robust model is framed by a red rectangle.
the lower the success rate the better robustness.
margin margin dropout and mcp .
overall these results corroborate our previous findings.
that is for the imageclassification task active learning yields less robust models.
table iv theclever score of cifar vgg16 and tagmynews lstm .
t he results that are better than the te model are highlighted in gray .the higher score the better robustness .
cifar vgg16 tagmynews lstm mericl l l inf l l l inf etropy .
.
.
.
.
.
margin .
.
.
.
.
.
k center .
.
.
.
.
.
egl .
.
.
.
.
.
bald .
.
.
.
.
.
etropy dropout .
.
.
.
.
.
margin dropout .
.
.
.
.
.
adversarial al .
.
.
nc .
.
.
.
.
.
kmnc .
.
.
.
.
.
mcp .
.
.
.
.
.
deepgini .
.
.
.
.
.
lsa .
.
.
.
.
.
dsa .
.
.
.
.
.
random .
.
.
.
.
.
te .
.
.
.
.
.6684answer to rq2 for the image classification task models trained with active learning can have lower robustness thanmodels trained with the entire dataset and the difference canbe up to .
clever score and .
attack successrate.
therefore experimental studies should involve evaluationmetrics beyond clean accuracy.
for the text classificationtask the results are inconsistent across attacks empiricalrobustness and distance norm clever score indicatingthat other factors are at play when it comes to robustness e.g.
the training process.
c. rq3 test accuracy after model compression we compare the test accuracy of a model produced by different data selection metrics before and after compression.
table v shows the results of vgg16 image classification and tagmynews lstm text classification by quantization.
on vgg16 the accuracy of the bit compressed models drops significantly by at least .
no matter it was fullytrained or with active learning.
by contrast on tagmynews lstm the accuracy decay using bit is relatively small lessthan .
.
the reason could be that the quantization 924process has less impact on the lstm layer or the text data is less sensitive to the precision of weights.
in both cases the compressed models achieve the same accuracy asthe original models with bit expect margin and entropy dropout on tagmynews lstm .
specifically compared withrandom selection on both vgg16 and tagmynews lstm no metric always outperforms the baseline.
comparing withthe te model we found that for the image classification task the compressed te model always maintains higher with a gapup to .
test accuracy.
however for the text classificationtask the ts model sometimes loses more test accuracy thanthe ts models after quantization.
surprisingly with bitquantization the te model gets the largest accuracy decay .
.
table v the change of test accuracy of cifar vgg16 and tagmynews lstm before and after model quantization .the best and worst results are highlighted in gray and orange respectively .
cifar vgg16 tagmynews lstm coreml tflite coreml tflite bit bit bit bit bit bit bit bit bit entropy .
.
.
.
.
.
margin .
.
.
.
.
.
.
.
k center .
.
.
.
.
.
.
egl .
.
.
.
.
.
.
bald .
.
.
.
.
.
.
etropy dropout .
.
.
.
.
.
.
.
margin dropout .
.
.
.
.
.
.
adversarial al .
.
.
.
nc .
.
.
.
.
.
.
kmnc .
.
.
.
.
.
.
mcp .
.
.
.
.
.
deepgini .
.
.
.
.
.
lsa .
.
.
.
.
.
.
dsa .
.
.
.
.
.
random .
.
.
.
.
.
.
te .
.
.
.
.
figure depicts the result by weight and neuron level pruning.
in general with pruning more weights and neurons thetest accuracy decreases gradually up to .
and .
on vgg16 respectively.
however on tagmynews lstm theaccuracy changes negligibly by increasing or decreasing up to0.
.
the reason might be that these two pruning methods canonly affect the convolutional layer and the dense layer whileour lstm models only contain one dense layer to output thefinal prediction probability.
looking into vgg16 the neuron level pruning affects the accuracy more than the weight levelfor all the data selection metrics which suggests that inpractical applications the engineers should consider more theweight level pruning than the neuron level to minimize theaccuracy loss.
among these data selection metrics deepginiand nc are always better than random selection.
besides for the image classification task the te model outperformsall ts models with a gap up to .
.
however for thetext classification task the te model has no advantage ofmaintaining test accuracy over ts models after pruning.
answer to rq3 model compression inconsistently affects the performance of the models trained with active learning andno data selection metric provides satisfactory results acrossall tasks.
for the image classification task after compression the fully trained models hold higher test accuracy than themodels trained with active learning and the gap can be up to0 percentage05101520253035accuracy decreasedsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp adversarial al kmnc deepgini random te a vgg16 pruning weight0 percentage01020304050607080accuracy decreasedsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp adversarial al kmnc deepgini random te b vgg16 pruning neuron percenta ge0.
.
.
.
.
.4accuracy decreasedsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp kmnc deepgini random te c tagmynews lstm pruning weight0 percenta ge0.
.
.
.
.
.15accuracy decreasedsa lsa entropy bald k center margin entropy dropout margin dropout egl nc mcp kmnc deepgini random te d tagmynews lstm pruning neuron fig.
.
the change of test accuracy y axis after model pruning with different degrees x axis i.e.
the proportion of weights and neurons being pruned.
.
quantization and .
pruning .
on the contrary for the text classification task the fully trained models haveno advantage of test accuracy over the models trained withactive learning after model compression.
d. rq4 impact of training data size from sections v b and v c we found that the data selection metrics tend to produce less robust models and introduce greater changes of accuracy after model compression.
wefurther extend our experiments to investigate the impact of thedata size on the quality e.g.
adversarial robustness and testaccuracy after compression of models.
figure shows theresults on vgg16 and tagmynews lstm.
for adversarialattacks we use jsma .
for vgg16 and pwws fortagmynews lstm respectively.
for model compression weemploy the bit quantization by coreml.
on both models weshow the results by two data selection metrics the predictionprobability based entropy and neuron coverage nc .
figure a and figure b show that ts models become more robust with more training data added especially themodels could be more robust than the fully trained modelsin the end.
according to the results we can see that toachieve similar robustness with fully trained models more than60 of training data are required for the image classificationtask while only of training data is enough for the textclassification task.
this corroborates our hypothesis see rq2 that the training process used by active learning which makesdata selected earlier go through more training iterations thandata selected later can yield more robust models comparedto a traditional training process involving all data from thebeginning.
this finding also opens the perspective of usingsuch an active learning process in conjunction with commonmethods to improve robustness such as adversarial training.
figure c and figure d show the test accuracy decay after model compression.
for the text classification task thedifference is negligible throughout the increase in data size.
data size5560657075success rate nc entropy a vgg16 robustness02500 data size787980818283848586success rate nc entropy b tagmynews lstm robustness data size0102030405060accuracy decreasenc entropy c vgg16 quantization02500 data size0.
.
.
.
.
.
.
.6accuracy decreasenc entropy d tagmynews lstm quantization fig.
.
impact of training data size x axis on the performance y axis success rate or accuracy decrease of model.
the horizontal dashed line shows the attack success rate or change of test accuracy of a fully trained model.
for the image classification task the decay of accuracy keeps stably lower than until the data size reaches wherethe accuracy decreases significantly by up to .
.
theseresults reveal that the data size is not a key factor in the testaccuracy decay on model compression.
answer to rq4 increasing the labeling budget of active learning mitigates the loss in robustness compared to a fullytrained model.
for example using at least training datafor the image classification task respectively for the textclassification task yields models with the same robustness asthe fully trained model.
however training with more data doesnot change our previous conclusions regarding the inconsistenteffect of model compression.
e. discussion we highlight our novel findings first then discuss some practical guidance and research directions accordingly.
novel findings and user guidance.
finding previous studies were limited to test accuracy the image classification task and did not compare metrics from both the ml and secommunities.
we reveal that the benefits of active learning arehighly task dependent.
some data selection metrics lsa anddsa are highly affected by the nature of classification tasks while some margin margin dropout and mcp can achieveconsistently high performance.
guidance engineers need tochoose data selection metrics according to specific tasks.
finding the limitation of active learning also exists in theadversarial robustness and model compression however theevaluation was missing in the literature.
we found that theactive learning process has some potential but notable impacton these indicators e.g.
for the image classification task thefully trained model is more robust than the model trainedby active learning.
guidance engineers are recommended toconsider all these indicators when using active learning.
research directions.
since no existing data selection metric can perform well on all the considered objectives an interesting research direction is to design data selectionmetrics that can optimize multiple qualities such as accu racy robustness and accuracy after compression.
regardingrobustness an adequate metric should also effectively integratewith an adversarial training process minimizing the amountof data to form which adversarial examples are generated toincrease the model robustness.
our study focuses on two types of classification tasks image and text .
recently dlsystems have been applied in some se related tasks such assource code function prediction and automatic program repair.exploring how existing data selection metrics perform on thesetasks is a potential research direction.
besides proposing asource code oriented data selection metric for this kind ofsystem could be another contribution.
f .
threats to v alidity first threats to validity may lie in the selected datasets and dnn models.
regarding the datasets we employ five popular datasets across both image and text classification tasks in ourstudy.
as for the dnns we consider in total ten architectures two for each dataset to alleviate the model dependent issue.though the models we use perform well on the chosen tasks an interesting direction of future work is to repeat the study onmore complex model architectures such as transformer basedmodels for text classification and observe whether thetrends remain.
second the parameter configuration may induce a threat to validity.
regarding the parameters in active learning there isno universal rule for choosing the best settings.
for instance previous studies utilize different settings oflabeling budget and stop strategy.
we mitigate this threat tovalidity in two ways we took the best and most commonpractices from the literature to design our active learningprocess and settings and our research questions concernthe specific impact of some parameters e.g.
rq4 studies theimpact of the stop criteria .
for robustness evaluation wefollow a recent study to set a range of perturbation sizesfor different adversarial attacks.
as for model compression our study involves multiple settings to reduce the potentialbias of the results.
third due to the space limitation we only report the results of two datasets covering the image and text classificationtasks.
nonetheless we remark that the reported conclusionis generalized to all the datasets and dnns.
for example for rq2 in total out of settings the fully trainedimage related models are more robust than the actively trainedmodels which is consistent with our finding.
besides wereport all our complete results on our project site .
vi.
related work we review related works in three aspects data selection in dl systems empirical study on active learning and empiricalstudy for dl systems.
data selection in dl systems.
we have already witnessed the success of data selection in se problems such as system atic literature review defect prediction based on static 926code attributes software cost modeling and software fault prediction .
meanwhile in the ml community manydata selection metrics have been proposed mainly for activelearning.
recently ren et al.
surveyed the active learning metrics and categorized them into uncertainty based deep bayesian active learning density based and other methods .
on the other hand the secommunity has proposed some dl testing criteria as well asdata selection metrics.
inspired by the program coverage peiet al.
proposed the basic neuron coverage criterion which can be used as a data selection metric.
then deepgauge introduced other dl test criteria e.g.
kmnc neuronboundary coverage.
kim et al.
proposed surprise guided testing metrics based on the similarity between the trainingdata and test data.
moreover some prediction probabilitybased data selection metrics are also proposed.most recently wang et al.
proposed a robustness oriented data selection metric however their metric can only selectdata that are generated by adversarial attacks it is out ofour consideration.
in our work we comprehensively studiedalmost all these metrics not only on their effectiveness but alsothe potential limitations.
empirical study on active learning.
active learning has been widely studied over recent years and some empiricalstudies of active learning have also been conducted fromdifferent domains.
ramirez loaiza et al.
utilized several performance measures e.g.
accuracy f1 score and auc toevaluate active learning baselines.
however all these measuresare based on the correctness of classification while our studyincludes more evaluation methods such as the adversarialrobustness and the performance change by model compression.pereira santos et al.
conducted a large scale empirical study of active learning for three machine learning algorithms c4.
svm and 5nn.
similarly the evaluation is limitedto the classification correctness quantified by the area underthe learning curve alc .
in addition some works targetspecific tasks by active learning.
settles et al.
analyzed active learning for sequence labeling tasks such as informationextraction and document segmentation.
for these specificproblems the evaluation mainly lies in the learning curveand runtime.
yu et al.
empirically studied existing active learning techniques for literature reviews then proposed anovel one that outperforms the existing metrics concerning therecall vs. studies reviewed curve.
chen et al.
studied the behavior of active learning for the word sense disambiguationtask.
however they only considered two basic data selectionmetrics entropy and margin and the evaluation only involvesaccuracy.
heilbron1 et al.
explored active learning for the action localization task which also only considered a verylimited number of data selection metrics and comparedthe performance by alc based on the correctness.
manabu studied active learning with support vector machines svms for natural language processing.
in their study theyonly compared their proposed metrics with random selectionbased on accuracy.
besides the finding is mainly that svmactive learning is suitable for japanese word segmentation.to sum up compared with previous empirical studies our study focuses on the application of active learning in deeplearning systems and is the first one that studied both imageclassification and text classification tasks.
more importantly our work is the only one that analyzes the impacts of ac tive learning on two important testing aspects during dnndeployment the adversarial robustness and the performanceof dnns after model compression.
moreover to the best ofour knowledge our study is the first that evaluates the dataselection metrics proposed by the se community for activelearning.
empirical study for dl systems.
recently multiple empirical research studies focus on exploring the dl issues thatare hard to be solved in theory.
guo et al.
studied the performance difference between different dl frameworks aswell as the model changes after model migration.
zhang et al.
and chen et al.
studied the challenges in the deployment phase of dl systems.
both of them revealed thatdeveloping a dl system is harder than developing softwaresystems.
ma et al.
performed a comparison study on different data selection metrics.
they investigated the abilityof each metric to identify misclassified input and improve thetest accuracy by retraining.
what s more zhang et al.
conducted a comparative study about the capability of differentuncertainty metrics in distinguishing adversarial examples andbenign examples.
different from these works our empiricalstudy focuses on exploring the limitations of active learning especially the potential limitations of the model trained byactive learning compared to the fully trained model.
vii.
conclusion in this paper we conducted a comprehensive empirical study to explore the limitations of active learning.
in total more than models for image classification and textclassification tasks have been trained and systematically eval uated.
the results reveal that when using active learning totrain a model different data selection metrics yield modelsof significantly different quality in accuracy and robustness .for the image classification task a model trained with activelearning can achieve competitive test accuracy but suffers fromrobustness loss and are less to compression.
however thesedownsides rarely occur in text classification models.
also wefurther studied the relationship between the data budget andthe quality of a trained model.
we found that the robustnessof the model increases with the amount of training data andultimately reaches the robustness of the fully trained model.based on these findings we provided some practical guidanceas well as research directions.
we believe that our work couldgive engineers and researchers some valuable insights into thewhole secure life cycle of deep learning especially in the datacollection and model evolution steps.
a cknowledgments this work is supported by the luxembourg national research funds fnr through core project c18 is stellar letraon.
927references k. he x. zhang s. ren and j. sun deep residual learning for image recognition in proceedings of the ieee conference on computer vision and pattern recognition pp.
.
m. johnson m. schuster q. v .
le m. krikun y .
wu z. chen n. thorat f. vi egas m. wattenberg g. corrado et al.
google s multilingual neural machine translation system enabling zero shot translation transactions of the association for computational linguistics vol.
pp.
.
y .
tian k. pei s. jana and b. ray deeptest automated testing of deep neural network driven autonomous cars in proceedings of the 40th international conference on software engineering.
new york ny usa association for computing machinery pp.
.
j. kober j. a. bagnell and j. peters reinforcement learning in robotics a survey the international journal of robotics research vol.
no.
pp.
.
j. deng w. dong r. socher l. j. li k. li and l. fei fei imagenet a large scale hierarchical image database in ieee conference on computer vision and pattern recognition.
ieee pp.
.
y .
roh g. heo and s. e. whang a survey on data collection for machine learning a big data ai integration perspective ieee transactions on knowledge and data engineering pp.
.
b. settles active learning synthesis lectures on artificial intelligence and machine learning vol.
no.
pp.
.
d. wang and y .
shang a new active labeling method for deep learning in international joint conference on neural networks ijcnn .
beijing china ieee pp.
.
y .
gal r. islam and z. ghahramani deep bayesian active learning with image data in international conference on machine learning.
pmlr pp.
.
o. sener and s. savarese active learning for convolutional neural networks a core set approach in international conference on learning representations.
vancouver bc canada openreview.net .
m. ducoffe and f. precioso adversarial active learning for deep networks a margin based approach arxiv preprint arxiv .
.
x. xie l. ma f. juefei xu m. xue h. chen y .
liu j. zhao b. li j. yin and s. see deephunter a coverage guided fuzz testingframework for deep neural networks in proceedings of the 28th acm sigsoft international symposium on software testing and analysis pp.
.
x. xie l. ma h. wang y .
li y .
liu and x. li diffchaser detecting disagreements for deep neural networks.
in ijcai pp.
.
x. du x. xie y .
li l. ma y .
liu and j. zhao deepstellar model based quantitative analysis of stateful deep learning systems inproceedings of the 27th acm joint meeting on european softwareengineering conference and symposium on the f oundations of softwareengineering pp.
.
l. ma f. juefei xu f. zhang j. sun m. xue b. li c. chen t. su l. li y .
liu j. zhao and y .
wang deepgauge multi granularity testing criteria for deep learning systems in proceedings of the 33rd acm ieee international conference on automated softwareengineering pp.
.
k. pei y .
cao j. yang and s. jana deepxplore automated whitebox testing of deep learning systems commun.
acm vol.
no.
pp.
.
.
available j. kim r. feldt and s. yoo guiding deep learning system testing using surprise adequacy in ieee acm 41st international conference on software engineering icse .
ieee pp.
.
w. shen y .
li l. chen y .
han y .
zhou and b. xu multipleboundary clustering and prioritization to promote neural network retrain ing in ieee acm international conference on automated software engineering ase .
melbourne australia ieee pp.
.
y .
feng q. shi x. gao j. wan c. fang and z. chen deepgini prioritizing massive tests to enhance the robustness of deep neuralbetworks in proceedings of the 29th acm sigsoft international symposium on software testing and analysis ser.
issta .
new york ny usa association for computing machinery p. .
.
available w. ma m. papadakis a. tsakmalis m. cordy and y .
l. traon test selection for deep learning systems acm trans.
softw.eng.
methodol.
vol.
no.
jan. .
.
available x. gao r. k. saha m. r. prasad and a. roychoudhury fuzz testing based data augmentation to improve robustness of deep neural net works in ieee acm 42nd international conference on software engineering icse .
ieee pp.
.
l. yang y .
zhang j. chen s. zhang and d. z. chen suggestive annotation a deep active learning framework for biomedical imagesegmentation in international conference on medical image computing and computer assisted intervention springer.
cham springer international publishing pp.
.
m. xu j. liu y .
liu f. x. lin y .
liu and x. liu a first look at deep learning apps on smartphones in the world wide web conference.
new york ny usa association for computing machinery pp.
.
t. w. weng h. zhang p. y .
chen j. yi d. su y .
gao c. j. hsieh and l. daniel evaluating the robustness of neural networks an ex treme value theory approach in international conference on learning representations.
vancouver bc canada openreview.net .
s. yoo and m. harman regression testing minimization selection and prioritization a survey software testing v erification and reliability vol.
no.
pp.
.
w. afzal and r. torkar towards benchmarking feature subset selection methods for software fault prediction in computational intelligence and quantitative software engineering.
springer pp.
.
t. menzies z. milton b. turhan b. cukic y .
jiang and a. bener defect prediction from static code features current results limitations new approaches automated software engineering vol.
no.
pp.
.
t. menzies j. greenwald and a. frank data mining static code attributes to learn defect predictors ieee transactions on software engineering vol.
no.
pp.
.
k. ren t. zheng z. qin and x. liu adversarial attacks and defenses in deep learning engineering vol.
no.
pp.
.
i. j. goodfellow j. shlens and c. szegedy explaining and harnessing adversarial examples .
n. papernot p. mcdaniel s. jha m. fredrikson z. b. celik and a. swami the limitations of deep learning in adversarial settings inieee european symposium on security and privacy euros p .
ieee pp.
.
n. carlini and d. wagner towards evaluating the robustness of neural networks in ieee symposium on security and privacy.
ieee pp.
.
s. ren y .
deng k. he and w. che generating natural language adversarial examples through probability weighted word saliency inproceedings of the 57th annual meeting of the association for com putational linguistics.
florence italy association for computationallinguistics july pp.
.
c. fellbaum wordnet in theory and applications of ontology computer applications.
springer pp.
.
j. gao j. lanchantin m. l. soffa and y .
qi black box generation of adversarial text sequences to evade deep learning classifiers in ieee security and privacy workshops spw may pp.
.
s. han j. pool j. tran and w. dally learning both weights and connections for efficient neural network advances in neural information processing systems vol.
pp.
.
h. hu r. peng y .
w. tai and c. k. tang network trimming a datadriven neuron pruning approach towards efficient deep architectures arxiv preprint arxiv .
.
l. ma f. juefei xu m. xue q. hu s. chen b. li y .
liu j. zhao j. yin and s. see secure deep learning engineering a software qualityassurance perspective arxiv preprint arxiv .
.
y .
lecun l. bottou y .
bengio and p. haffner gradient based learningapplied to document recognition proceedings of the ieee vol.
no.
pp.
november .
a. krizhevsky learning multiple layers of features from tiny images university of toronto toronto tech.
rep. .
a. maas r. e. daly p. t. pham d. huang a. y .
ng and c. potts learning word vectors for sentiment analysis in proceedings of the 49th annual meeting of the association for computational linguistics human language technologies pp.
.
d. vitale p. ferragina and u. scaiella classification of short texts by deploying topical annotations in european conference on information retrieval.
berlin heidelberg springer pp.
.
l. a. adamic j. zhang e. bakshy and m. s. ackerman knowledge sharing and yahoo answers everyone knows something in proceedings of the 17th international conference on world wide web pp.
.
y .
lecun et al.
lenet convolutional neural networks url lecun.
com exdb lenet vol.
no.
p. .
m. lin q. chen and s. yan network in network arxiv preprint arxiv .
.
k. simonyan and a. zisserman very deep convolutional networks for large scale image recognition in international conference on learning representations san diego ca usa .
f. chollet keras code examples .
.
available lstm imdb active learning empirical study homepage .
.
available b. settles and m. craven an analysis of active learning strategies for sequence labeling tasks in proceedings of the conference on empirical methods in natural language processing.
usa association forcomputational linguistics pp.
.
a. siddhant and z. c. lipton deep bayesian active learning for natural language processing results of a large scale empirical study inproceedings of the conference on empirical methods in natural language processing.
brussels belgium association forcomputational linguistics october pp.
.
a. odena c. olsson d. andersen and i. goodfellow tensorfuzz debugging neural networks with coverage guided fuzzing in proceedings of the 36th international conference on machine learning ser.
proceedings of machine learning research vol.
.
pmlr june2019 pp.
.
s. m. moosavi dezfooli a. fawzi and p. frossard deepfool a simple and accurate method to fool deep neural networks in proceedings of the ieee conference on computer vision and pattern recognition pp.
.
f. chollet et al.
keras .
m. abadi p. barham j. chen z. chen a. davis j. dean m. devin s. ghemawat g. irving m. isard et al.
tensorflow a system for largescale machine learning in 12th usenix symposium on operating systems design and implementation osdi ser.
osdi .
usa usenix association pp.
.
w. h. beluch t. genewein a. n urnberger and j. m. k ohler the power of ensembles for active learning in image classification inproceedings of the ieee conference on computer vision and patternrecognition pp.
.
j. rauber w. brendel and m. bethge foolbox a python toolbox to benchmark the robustness of machine learning models arxiv preprint arxiv .
.
m. i. nicolae m. sinn m. n. tran b. buesser a. rawat m. wistuba v .
zantedeschi n. baracaldo b. chen h. ludwig i. molloy andb.
edwards art adversarial robustness toolbox v1.
.
corr vol.
.
.
.
available y .
dong p. zhang j. wang s. liu j. sun j. hao x. wang l. wang j. s. dong and d. ting there is limited correlation betweencoverage and robustness for deep neural networks arxiv preprint arxiv .
.
m. thakkar beginning machine learning in ios coreml framework 1st ed.
apress .
m. abadi a. agarwal p. barham e. brevdo z. chen c. citro g. s. corrado a. davis j. dean m. devin s. ghemawat i. goodfellow a. harp g. irving m. isard y .
jia r. jozefowicz l. kaiser m. kudlur j. levenberg d. man e r. monga s. moore d. murray c. olah m. schuster j. shlens b. steiner i. sutskever k. talwar p. tucker v .
vanhoucke v .
vasudevan f. vi egas o. vinyals p. warden m. wattenberg m. wicke y .
yu and x. zheng tensorflow large scale machine learning on heterogeneous systems software available from tensorflow.org.
.
available p. li p. zhong k. mao d. wang x. yang y .
liu j. x. yin and s. see act an attentive convolutional transformer for efficient textclassification in proceedings of the aaai conference on artificial intelligence vol.
no.
pp.
.
z. yu n. a. kraft and t. menzies finding better active learners for faster literature reviews empirical software engineering vol.
no.
pp.
.
z. chen t. menzies d. port and d. boehm finding the right data forsoftware cost modeling ieee software vol.
no.
pp.
.
h. lu and b. cukic an adaptive approach with active learning in software fault prediction in proceedings of the 8th international conference on predictive models in software engineering ser.
promise .
new york ny usa association for computing machinery p. .
p. ren y .
xiao x. chang p. y .
huang z. li x. chen and x. wang a survey of deep active learning arxiv preprint arxiv .
.
n. asghar p. poupart x. jiang and h. li deep active learning for dialogue generation arxiv preprint arxiv .
.
y .
geifman and r. el yaniv deep active learning over the long tail arxiv preprint arxiv .
.
m. fang y .
li and t. cohn learning how to active learn a deep reinforcement learning approach arxiv preprint arxiv .
.
b. yang j. t. sun t. wang and z. chen effective multi label active learning for text classification in proceedings of the 15th acm sigkdd international conference on knowledge discovery and datamining pp.
.
z. wang h. you j. chen y .
zhang x. dong and w. zhang prioritizing test inputs for deep neural networks via mutation analysis inieee acm 43nd international conference on software engineering icse .
j. wang j. chen y .
sun x. ma d. wang j. sun and p. cheng robot robustness oriented testing for deep learning systems arxiv preprint arxiv .
.
m. e. ramirez loaiza m. sharma g. kumar and m. bilgic active learning an empirical study of common baselines data mining and knowledge discovery vol.
no.
pp.
.
d. pereira santos r. b. c. prud encio and a. c. de carvalho empirical investigation of active learning strategies neurocomputing vol.
pp.
.
j. chen a. schein l. ungar and m. palmer an empirical study of the behavior of active learning for word sense disambiguation inproceedings of the human language technology conference of thenaacl main conference pp.
.
f. c. heilbron j. y .
lee h. jin and b. ghanem what do i annotate next?
an empirical study of active learning for action localization inproceedings of the european conference on computer vision eccv pp.
.
m. sassano an empirical study of active learning with support vector machines forjapanese word segmentation in proceedings of the 40th annual meeting of the association for computational linguistics pp.
.
q. guo s. chen x. xie l. ma q. hu h. liu y .
liu j. zhao and x. li an empirical study towards characterizing deep learning de velopment and deployment across different frameworks and platforms in34th ieee acm international conference on automated software engineering ase .
ieee pp.
.
t. zhang c. gao l. ma m. lyu and m. kim an empirical study of common challenges in developing deep learning applications ininternational symposium on software reliability engineering issre .berlin germany ieee pp.
.
z. chen y .
cao y .
liu h. wang t. xie and x. liu a comprehensive study on challenges in deploying deep learning based software inproceedings of the 28th acm joint meeting on european softwareengineering conference and symposium on the f oundations of softwareengineering pp.
.
x. zhang x. xie l. ma x. du q. hu y .
liu j. zhao and m. sun towards characterizing adversarial defects of deep learningsoftware from the lens of uncertainty in ieee acm 42nd international conference on software engineering icse ieee.
new york ny usa association for computing machinery pp.
.