infercode self supervised learning of code representations by predicting subtrees nghi d. q. bui yijun yuy lingxiao jiang school of computing information systems singapore management univerity fdqnbui.
lxjiang g smu.edu.sg yschool of computing communications the open university uk fy.yug open.ac.uk abstract learning code representations has found many uses in software engineering such as code classification code search comment generation and bug prediction etc.
although representations of code in tokens syntax trees dependency graphs paths in trees or the combinations of their variants have been proposed existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks and as such the code representations may not be suitable for other tasks.
even though some techniques generate representations from unlabeled code they are far from being satisfactory when applied to the downstream tasks.
to overcome the limitation this paper proposes infercode which adapts the self supervised learning idea from natural language processing to the abstract syntax trees asts of code.
the novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of asts.
with infercode subtrees in asts are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction and the trained representations are no longer tied to any specific downstream tasks or code units.
we have trained an instance of infercode model using treebased convolutional neural network tbcnn as the encoder of a large set of java code.
this pre trained model can then be applied to downstream unsupervised tasks such as code clustering code clone detection cross language code search or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction.
compared to prior techniques applied to the same downstream tasks such as code2vec code2seq astnn using our pre trained infercode model higher performance is achieved with a significant margin for most of the tasks including those involving different programming languages.
the implementation of infercode and the trained embeddings are available at the link i. i ntroduction learning code representations a.k.a.
embeddings and building a prediction model for programs have been found useful in many software engineering tasks such as classifying program functionality code search code comment generation predicting bugs translating programs etc.
while offering promising performance for the tasks the prior learning techniques have two major limitations that hinder their performance and generalizability.
most code representation models are trained through semi supervised learning.
humans need to manually label the data for a specific downstream task then engineer featuresof intermediate representations and train the models specifically for the task.
such labelling feature engineering and training efforts are specific to one particular task and may not be easily transferred to other tasks.
even though there are techniques aiming to produce code representations that are transferable to different tasks their trained code representations are only for some fixed units of code such as tokens statements and functions and are not flexible to produce embeddings for varying code units.
such techniques may miss useful information across different kinds of code units and the trained representations may not perform well for various downstream tasks either.
some other techniques based on graph embeddings share a similar drawback and in addition need the overheads of graph construction which may introduce inaccurate information in the graphs.
such limitations have been illustrated in a recent study kang et al.
show that the pre trained code2vec representation does not perform well for other tasks when it was trained specifically for the method name prediction task.
towards addressing the limitations the aim of this paper is to develop a new technique for learning code representations and it should be trainable without any manual human labeling flexible in producing embeddings for any code unit that can be parsed into syntax trees and general enough so that its trained representations for code can perform well for various downstream tasks.
we have two pillars that support the realization of our aim.
one is the large amount of source code available on public code hosting platforms such as github bitbucket gitlab.
although the code often lacks accurate labels for downstream tasks the syntax of the code itself can be checked relatively easily by parsers.
it is desirable to leverage such unlabeled data to pretrain the code representations reusable for building various program prediction models for downstream tasks.
the second pillar is supported by the advances of selfsupervised learning in machine learning .
such techniques enable the training of neural networks without the need for human labels.
usually a self supervised learning technique reformulates an unsupervised learning problem as a supervised one by automatically generating virtual labels from existing unlabeled data .
the self supervised task also known as a pretext task guides us to a supervised loss function.
ieee acm 43rd international conference on software engineering icse .
ieee while minimizing the loss function for the pretext task the technique also produces intermediate representations for the data corresponding to the virtual labels.
because the pretext task can be trained using any data it is expected that such representations can carry good information about the diverse data and be beneficial to a variety of downstream tasks.
this notion of self supervised learning is very suitable for our aim.
little effort has been invested in the literature to exploit the uses of self supervised learning for code representation learning.
although some recent work such as presents a self supervised learning paradigm for program repair it is designed specifically for this specific task.
our key idea is thus to train a pretext task suitable for any source code.
unlike self supervised learning in natural language processing and visual learning areas that use words or object regions as labels we utilize the fact that it is relatively easy to obtain the abstract syntax tree ast of any syntactically valid code snippets via parsers and it is also easy to identify all the subtrees in asts and automatically use each subtree as the label for the pretext task to predict the probability of that subtree appearing in a particular ast1.
fig.
illustrates this intuition with an example.
the two code snippets implement the same functionality i.e.
bubble sort.
if we view these two code snippets as two asts there are many similar subtrees between them.
for example the subtree that represents the conditional expression arr arr of the left snippets is similar to arr arr although the textual information is quite different.
this means that if one can exploit such information there is no longer the need for labels to build a representation learning model for source code.
unlike the recent uses of neural document embedding models e.g.
doc2vec for source code e.g.
our technique learns subtrees in asts without the overheads and losses of accuracy in constructing customized graphs from code tokens and node types although we are also inspired by the same idea of doc2vec.
we also provide an alternative to graph based or execution traces based embedding techniques as we believe asts are more readily available for all kinds of programming languages and may have contained all the code information although some are hidden .
based on the key idea we propose infercode a selfsupervised learning technique for source code by predicting syntax subtrees.
as far as we know we are the first to apply the notation of self supervised learning to syntax subtrees and can produce code representations for any syntactically valid code snippet without the need of human labelling infercode can serve as an encoder that maps any parsable code snippet into a vector representation embedding and 1an underlying assumption is that for such trained representations to capture code meanings code snippets with the same semantics should involve some syntactically similar code elements.
even though two pieces of code implementing the same functionality can be syntactically different there could still be some fine grained elements in the code or other pieces of code that use these two that are syntactically similar especially when the code base is large.
fig.
.
example of two code snippets that implement bubble sort in java that share similar fine grained code elements.
this vector can be used for various downstream tasks such as code clustering clone detection and code search.
infercode can serve as a pre trained model and its weights can be reused in downstream training of the models for supervised learning tasks which can speed up the training and alleviate the issue of lacking data for a particular task.
we implement infercode on top of the asts produced by srcml and efficient parsers such as fast .
it provides a combined vocabulary of ast node types for multiple programming languages e.g.
java c c c objective c which implies that our infercode can be polyglot producing code representations suitable for tasks involving different languages such as cross language code search as long as the asts for a code snippet can be recognized by the parser.
we have trained an instance of infercode based on a large set of java code and evaluated the usefulness of the pretrained code representations in five downstream tasks three of which are unsupervised code clustering code clone detection via similarity measurement cross language code search two are supervised code classification and method name prediction .
for the three unsupervised tasks we utilize the vectors produce by infercode and different vector similarity metrics to achieve the goal of each task for code clustering our results using infercode outperform the best baseline code2vec by in term of adjusted rand index for code clone detection our results outperform the best baseline code2vec by in term of f1 score for cross language code search our results outperform the best baseline clir on on average for multiple languages setting in term of mean reciprocal rank.
for the two supervised tasks we utilize the weights of the pre trained model from infercode to fine tune the specific prediction model for each task our results using the finetuning process increases the performance of tbcnn for code classification by in term of accuracy which is comparable to astnn the state of the art model for code classification and increase the performance tbcnn for method name prediction by which is comparable to code2seq a state ofthe art model for method name prediction.
ii.
r elated work self supervised learning has made great progress recently for visual data gidaris et al.
proposed a method to generate different viewpoints of an image by a number of rotations on certain degrees at random and formulate the learning part as a multi class classification problem over the rotations.
this pretext task drives the model 1187to learn semantic concepts of objects as the parameters of the cnn image encoder zhang et al.
proposed to use colorization as the pretext task by giving colours to a grayscale input image in order to map this image to a distribution over quantized color value outputs.
there has been tremendous effort in exploring selfsupervised learning in natural language processing nlp research .
word2vec is a form of selfsupervised learning which aims to learn good representation of words by taking a small chunk of the text of certain window size.
doc2vec shares the same principle with word2vec which aims to use a document to predict the words inside it so that similar documents will have similar embeddings skipthought vectors builds a statistical language model by predicting the neighbouring sentences of a centering sentence bert advances the language models by masking the words in a text randomly in order to predict them.
deep code learning models there has been a huge interest in applying deep learning techniques to software engineering tasks such as program functionality classification bug localization function name prediction code clone detection program refactoring program translation and code synthesis .
allamanis et al.
extend asts to graphs by adding a variety of code dependencies as edges among the tree nodes intended to represent code semantics and apply gated graph neural networks ggnn to learn the graphs from code code2vec code2seq and astnn are designed based on splitting asts into smaller ones either as a bag of path contexts or as flattened subtrees representing individual statements.
they use various kinds of recurrent neural networks rnns to learn such code representations.
unfortunately there is little effort in designing the source code model with unlabelled data.
yasunaga and liang presents a self supervised learning paradigm for program repair surveys on code embeddings present evidence to show that there is a strong need to alleviate the demands of labelled data and encourage the community to invest more into the methods for learning source code with unlabelled data.
our approach differs from existing ways to reuse the pretrained code learning model kang et al.
reuse the token embeddings from code2vec for downstream tasks only to find lower performance than simpler word embedding methods like word2vec.
in contrast we use the weights of the pretrained model and the code vector vproduced by the encoder instead of the token embeddings.
iii.
p reliminaries a. source code representation learning source code representation learning usually contains the following two phases representing a code snippet into an intermediate representation ir such as token streams asts ast paths or graphs and designing a neural network suitable to process such intermediate representations.
such a neural network can also be called an encoder which receives the code ir and maps it into a code vector embedding v usually a combination of various kinds of code elements then vcan be fed into the next layer s of a learning system and trained for an objective function of the specific task of the learning system.
for example in code2vec vis a combination of different ast paths.
in ggnn or tbcnn vis a combination of ast nodes.
a trained model either on supervised learning or self supervised learning task can produce v. in our work we will evaluate how the vtrained on a self supervised learning objective function over a large set of unlabelled data can be made useful for different downstream se tasks.
b. neural document embedding models doc2vec is an extension to word2vec .
doc2vec uses an instance of the skip gram model called paragraph vector which is a distributed bag of words interchangeably referred as doc2vec skip gram that is capable of learning the representations of a sequence words of arbitrary lengths such as sentences paragraphs and even whole documents.
more specifically given a set of documents fd1 d2 dngand a sequence of words f w ij gsampled from the document di skip gram learns a d dimensional embeddings of the documentdiand each word wijsampled i.e.
vi vij2rd respectively.
the model works by considering a word wijto be occurring in the context of document diand tries to maximize the following log likelihood function p jlog pr wijjdi where the probability pr wijjdi is defined asexp vi vij p w2vexp vi w wherevis the vocabulary of all the words across all documents.
in this paper we consider asts analogous to documents and subtrees in the asts analogous to words in the documents and adapt the idea of document embedding to learn the embeddings of asts of any size by using an encoder for the ast of any parsable code snippets.
c. self supervised learning formulation the goal of self supervised learning is to train an encoder esuch thatecan map an object into a vector representation embedding .
in our case the embedding vis for the ast representation tof a code snippet c. training the encoder eis to learn its parameters or weights so that eis able to produce the embeddings for the code snippets such that the vectors for the snippets having similar syntactical and semantic information will be close in the vector space.
in visual learning convolutional neural networks cnns are usually chosen as the encoder for images.
in nlp recurrent neural networks or recently bert is typically used as the encoder for text sequences.
in our case we choose tree based cnn as the source code encoder as it has been successfully used before and justified further in section viii.
given a dataset x for each data xiin x there is a corresponding pseudo label piautomatically generated for a predefined pretext task without involving any human annotation.
given a set of ntraining data d fpign i the aim is to minimize the loss function loss d npn i 1loss xi pi we can easily identify subtrees in asts as the pseudo labels p 1188fig.
.
a doc2vec s skipgram model given a document d it samples cwords and considers them as co occurring in the same context of dto learn d s representation b infercode given an ast t it samples ssubtrees from tand uses them as the context to learn t s representation.
automatically without human annotations so that our learning technique can be self supervised.
iv.
a pproach details a. overview figure presents a high level view of our infercode approach as an analogy to doc2vec by treating an entire ast as a document and treating its subtrees as words in the document.
given a set of asts ft1 t2 t ng and a set of all subtrees f t ij gofti we represent ti tijbyd dimensional embedding vectors vi vij2rd respectively.
by considering a subtreetij2tito be occurring in the context of the ast ti we aim to maximize the following logarithmic likelihood p jlog pr tijjti .
unlike doc2vec infercode does not query the embedding vectors directly from an embedding matrix for the whole documents instead we first encode the entire ast to obtain the vi then use it to predict the subtrees.
the steps of our technique are as follows for each ast in our dataset we identify a set of subtrees and all of the subtrees are accumulated into a vocabulary of subtrees section iv b we feed an ast into a tree based cnn tbcnn encoder to produce a code vector vi.
then viis used to predict the subtrees identified in the previous step after the encoder has been trained we can use it as the pretrained model for downstream tasks.
b. process to identify subtrees fig.
.
example to generate subtrees from a code snippet by traversing an ast every visited node satisfying a certain condition e.g.
of the type expr leads to a subtree rooted at the visited node.
in our experiments we chose to select the subtrees whose root node is of the types fexpr stmt decl stmt expr conditiong we consider these relatively fine grained code elements because they are usually meaningful yet small enough to be considered as frequent words in the vocabulary of subtrees from a large code base.
such small code elements often have similar meaning when their syntactical structure is similar even though their textual appearance may be different due to different identifier names such as int n arr.length versus int m x.length .
in addition we also consider the nodes that represent for a single keyword such as if for while .
noted that these nodes can be seen as the subtrees with size .
we do not consider coarse grained subtrees such as the whole if while for statements as those subtrees are often too big so that each of them as an individual vocabulary word may appear too infrequent in the code base for the encoder to learn a meaningful representation for it directly syntactical differences among the big subtrees do not necessarily mean the corresponding code has different meanings while the encoder may have harder time to recognize the semantic similarity among them.
figure shows a sample bubble sort code snippet written in java and the identified subtrees on the right hand side.
this snippet is parsed into an ast and certain subtrees are identified automatically.
for example the statement int n arr.length contains an expression arr.length .
both int n arr.length andarr.length are identified.
c. learning source code representation once we have the subtrees we can use them to learn the source code encoder under a self supervision mechanism.
here we choose tbcnn as the source code encoder.
there are two major differences between our implementation of tbcnn and the original design in we include the textual information into the node initialization embedding instead of using only the type information and we replace the dynamic max pooling with an attention mechanism to combine node embeddings.
figure shows an overview of the workflow of the tbcnn with the modifications we made.
there are three steps to learn the weights of the encoder which are described as follows learning nodes representation this step is to learn the representation of the node of the input ast t. the information of the tree will propagate from bottom to top i.e.
a parent node will accumulate the information of its descendant in the ast.
after the accumulation step each node will contain the information of its descendants.
aggregating nodes information since we want to represent the ast representation of the code snippet into a 1189fig.
.
workflow of tree based convolutional neural network with modifications including the token information to initialize the node vector and using the attention mechanism to aggregate node s information fixed dimension vector v we need to combine all the node embeddings into one fixed single embedding.
we use the attention layer for this purpose.
predicting subtrees once having the vc we use it to predict the subtrees extracted from t. intuitively this process is similar to eq.
iii b where the task is to predict the probability of a subtree given the embedding vc.
learning nodes representation with tbcnn we briefly introduce the tree based convolutional neural networks tbcnn for processing ast inputs.
a treet v e x consists of a set of nodes v a set of node features x and a set of edges e. an edge in a tree connects a node and its children.
each node in an ast also contains its corresponding texts or tokens and its type e.g.
operator types statement types function types etc.
from the underlying code.
initially we annotate each node v2vwith ad dimensional real valued vector xv2rd representing the features of the node.
we associate every node vwith a hidden state vector hv initialized from the feature embedding xv.
in the node is initialized only with the type embedding.
in our case we initialize the node with a fusion of the embeddings of its texts and through a linear layer.
the embedding matrices for the texts and types are learnable in the whole model training pipeline formally defined as wtypeandwtoken respectively.
in tbcnn a convolution window over an ast is emulated via a binary tree where the weight matrix for each node is a weighted sum of three fixed matrices wt wl wr2rd d each of which is the weight for the top left and right node respectively and a bias term b2rdhence for a convolutional window of depth din the original ast with k 2d 1nodes including the parent nodes belong to that window with vectors where xi2rd the convolutional output yof that window can be defined as y tanh pk i t iwt l iwl r iwr xi b where t i l i r iare weights calculated corresponding to the depth and the position of the nodes.
attention mechanism to aggregate nodes after the nodes representation has been learned we need an aggregation method to combine all the nodes in to one fixed embeddingthat represent for the code snippet.
mou et al.
use max pooling to combine the nodes.
however max pooling may discard a lot of important information so we replace it with the attention mechanism to aggregate nodes.
formally an attention vector a2rdis initialised randomly and learned simultaneously with updates of the networks.
given nnode state vectors f h1 hng the attention weight iof each hiis computed as the normalised inner product between the node state vector and the global attention vector i exp hit a pn j 1exp hjt a .
the exponents in this equation are used to make the attention weights positive and they are divided by their sum to have a max value of as done by a standard softmax function.
the aggregated code vector v2rdrepresents the whole code snippet.
it is a linear combination of the node state vectorsf h1 hngweighted by their attention scores v nx i i hi predicting subtrees from the process to extract the subtrees we have a vocabulary of all subtrees from our training dataset.
the embeddings of subtrees are learn able parameters formally defined as wsubtrees2rjlj d wherelis the set of subtrees extracted from the training corpus.
the embedding of subtrees iis rowiofwsubtrees.
the predicted distribution of the model q l is computed as the softmax normalized dot product between the code vector vand each of the subtree embeddings for l i2l q li exp vt wsubtrees i p lj2lexp vt wsubtrees i whereq li is the normalized dot product between the vector ofliand the code vector v i.e.
the probability that a subtrees liappears in a given code snippet c. this is aligned with eq.
iii b in doc2vec to predict the likelihood of a word given a document.
finally we need to learn these parameters of infercode wtype wtoken wt wl wr2rd d a2 rd wsubtrees2rjlj d. d. usage of the model after training we have presented the pipeline to train infercode by predicting subtrees as the labels.
note that in self supervised learning one does not usually care about the performance of the pretext task.
instead we care about the weights that have been learned and the ability of the model to generate the embeddings.
the trained tbcnn encoder of infercode can be used to produce an embedding vector vfor any parsable code snippet by parsing the code into an ast and feeding the ast through the encoding step presented in figure to get the vector.
the weights in the trained model can also be used for the prediction models in downstream supervised learning tasks to save training costs and potentially improve their prediction accuracy.
we illustrate the usages in next sections.
1190v.
u secases in this section we briefly describe how infercode can be adapted into different downstream tasks.
a. code embedding vectors for unsupervised tasks code clustering the task is to put similar code snippets automatically into the same groups without any supervision.
given the code vectors vproduced by the pre trained infercode for any code snippets we can realize the task by defining a similarity metric based on euclidean distance and applying a clustering algorithm such as k means .
code clone detection there are supervised and unsupervised approaches to detect clones.
while deep learning methods are applied to detect code clones they require labelled data to train a supervised learning model .
as such one needs human annotators to mark the pairs of snippets as clones limiting the ability to detect clones by large amount of the data one can collect.
to alleviate the need of labelled pairwise data to train supervised clone detectors we opt to use the unsupervised approach based on a good similarity measurement for a pair of code snippets we measure the similarity of between the two vectors by using the cosine similarity when the cosine similarity between the vectors are higher than a certain threshold we treat the pair as clones.
in this work we choose .
as the threshold.
cross language code to code search code to code search is useful for developers to find other code in a large code base that is similar to a given code query.
for example a developer working on a task to migrate a sorting algorithm implemented in java to another language e.g.
c might want to see if there exists an implementation of the same sorting algorithm in c instead of rewriting the code in c from scratch.
existing code to code search engine such as krugle facoy aroma only consider the searching problem within one programming language.
considering the more challenging cross language search use case our pretrained infercode model can be more useful.
the backbone of infercode is asts and we used the asts from an efficient parser for srcml representations because it is a combined vocabulary for the ast node types in five mainstream languages java c c c and objective c .
our pretrained model can receive srcml ast structure of any code snippets within these languages.
given a code snippet in one language as a query we aim to retrieve other code snippets that are functionally similar to the given code snippet in other programming languages.
since all code snippets can be represented in the form of vector representations this problem can be formalized as the nearest neighbor query in the vector space.
b. fine tuning for supervised learning tasks a paradigm to make use of large amount of unlabelled data isself supervised pretraining followed by a supervised finetuning which reuses parts or all of a trained neural network on a certain task and continue to train it or simply fig.
.
code features are learned through the training process of tbcnn encoder to solve a predefined pretext task.
after finishing the training the learned parameters serve as a pre trained model and can be transferred to other downstream tasks by fine tuning.
the performance on these downstream tasks is used to evaluate the quality of the learned features.
using the embedding output for other tasks.
such fine tuning processes usually have the benefits of speeding up the training as one does not need to train the model from randomly initialized weights and improving the generalizability of the downstream model even when only small datasets have labels.
as shown in figure the tbcnn encoder of infercode serves as a pretrained model in which the weights resulted from the self supervised learning are transferred to initialize the model of the downstream supervised learning task.
code classification here we use code classification as a downstream task to demonstrate the usefulness of the finetuning process.
this task is to given a piece of code classify the functionality class it belongs to.
method name prediction we use method name prediction as the second downstream task.
this task is to given a piece of code without its function header predict a meaningful name that reflects the functionality of the code.
vi.
e mpirical evaluation in this section we evaluate infercode on the five use cases presented in section v. we want to see to what degree the pretrained model is applicable to different use cases even when the cases involve multiple programming languages.
for the training phase we reuse the java large dataset that has been used in code2vec and code2seq .
this dataset contains a large number of java projects collected from github million files .
for the testing phase we use different datasets for each of the task as the test data .
we parse all the files into asts using fast .
then we identify all the subtrees to form a vocabulary of subtrees.
having the asts and the subtrees as the pseudo labels we train the infercode model by using the softmax cross entropy as the objective loss function and choose adam as the optimizer with an initial learning rate of on an nvidia tesla p100 gpu.
a. code clustering datasets metrics and baselines we use two datasets for this task.
the first is the oj dataset that contains c code snippets known to belong to classes .
the second is the sorting algorithm sa dataset used in which consists of classes of sorting algorithm written in 1191java each algorithm has approximately code snippets.
our clustering task here is to cluster all the code snippets without class labels according to the similarity among the code vectors for the oj dataset we use k means k to cluster the code into clusters for the sa dataset we use k means k to cluster the code.
then we use the class labels in the datasets to check if the clusters are formed appropriately.
we use the adjusted rand index as the metric to evaluate the clustering results.
here we present the definition of rand index.
let cbe the ground truth class assignment andkbe the number of clusters assigned by a clustering algorithm.
let abe the number of pairs of elements that are in the same set in cand the same set in k andbas the number of pairs of elements that are in different sets in cand different sets ink.
rand index for two datasets can be defined as ri a b nsamples where the combinatorial number nsamples is the total number of possible pairs in the dataset without ordering .
however the riscore does not guarantee that random label assignments will get a value close to zero esp.
if the number of clusters is in the same order of magnitude as the number of samples .
to counter this effect adjusted rand index is defined by discounting the expected riof random labelling as followed ari ri e max ri e for the baselines if we treat source code as text the selfsupervised learning techniques in nlp can also be applied for code.
as such we include two well known baselines from nlp word2vec and doc2vec .
we also include another baseline from a state of the art method to learn sentence representation.
this method uses a sequential denoising auto encoder sae method to encode the text into an embedding and reconstruct the text from such embedding.
we also compare with two baselines for code modeling code2vec and code2seq .
code2vec works by training a path encoder on bag of paths extracted from the ast.
the path encoder will encode the paths into an embedding v then use vto predict the method name.
code2seq shares a similar principle but vis used to generate a textual summary of code.
in either case we use the path encoders of code2vec and code2seq to produce the code vectors and also perform the same clustering process as infercode.
results table i shows the results of code clustering using different models.
infercode performs the best for both datasets.
the nlp methods however underperform other code learning methods.
this is reasonable because both code2vec and code2seq capture structural information from code while nlp methods treat code as text sequences.
we will provide a deeper analysis of the clusters by providing visualizations of the vectors produced by different methods see section vii a .
b. code clone detection datasets metrics and baselines we use two datasets in two languages.
one is the oj dataset again that contains c c programs.
the other is the bigclonebench a java dataset that has been widely used to benchmark codetable i results of code clustering in adjusted rand index ari modelperformance ari oj dataset c sa dataset java word2vec .
.
doc2vec .
.
sae .
.
code2vec .
.
code2seq .
infercode .
.
clone detection techniques which consists of projects from projects covering functionalities and including true clone pairs and false clone pairs.
for the oj dataset we followed the process in zhang et al.
to construct a set of code pairs for clone detection based on pairwise similarity measurement so called ojclone we choose programs from each of the first programming problems in oj.
it would produce a total of .
million clone pairs and .
million non clone pairs which are extremely timeconsuming for comparison.
so that we randomly select samples clone pairs and non clone pairs for measuring the performance of various clone detectors.
we use the well known precision recall and f1 scores.
since the task is unsupervised in this paper we compare infercode only with unsupervised clone detectors that do not require labeled data although the pretrained infercode can also be applied to supervised clone detection .
the baselines include deckard sourcerercc dlc and a detector using the code vectors extracted from code2vec and the same cosine similarity threshold used for infercode.
results table ii shows the overall precision recall and f1 for infercode and other baselines.
the detector based on infercode has the highest recall except for sourcerercc whose precision is relatively low .
overall in terms of f1 it outperforms other unsupervised clone detectors.
note that we do not compare with techniques such as oreo ccd astnn because they use supervised learning techniques to build clone classifiers .
we believe that the code embeddings or the weights from the pretrained infercode can be used for training supervised clone classifiers too and with further improvement on self supervised learning techniques such as improving the encoder the auto identified labels and the loss function the performance of unsupervised code clone detection may also get close to supervised ones.
we leave these evaluations for future work.
table ii results of code clone detection in precision recall and f1 methodsbigclonebench java ojclone c p r f1 p r f1 deckard .
.
.
.
.
.
dlc .
.
.
.
.
.
sourcerercc .
.
.
.
.
.
code2vec .
.
.
.
.
.
infercode .
.
.
.
.
.
1192c.
cross language code to code search datasets metrics and baselines given the implementation of an algorithm in one language this task is to search for other implementations of the same algorithm written in other languages.
so we need a dataset that contains multiple implementations of algorithms in different languages.
we construct such a codebase by searching from the rosetta code2 and other code from github we collect code in java c c c from rosetta code which results in around samples then we collect random program files from github for each of the languages and mix them with the samples.
for instance for java we collect a large set of java projects from github that have at least stars.
there is a possibility that the collected github projects contain implementations of the algorithms in the rosetta code.
so we perform a simple text filtering to exclude all the files that contain a token of any of the algorithm name.
let us take algorithms as examples bubble sort singly linked list traversal yinyang3 we exclude any file that contains any of these tokens fbubble sort singly linked list traversal yin yang g. then for the remaining java files we sample a subset of files and mix them with the java implementations of the algorithms from the rosetta dataset.
we do the same for c c c and obtain in total about files in our search code base.
with the constructed code base we perform the evaluation for cross language search as follows for each of the code files from rosetta code say a bubble sort implementation written in java we use it as the query to retrieve other files containing top k similar code.
here we choose k in this evaluation.
the ideal query results should only return a list of code snippets that are from rosetta code but implement the same bubble sort algorithm in c c and c other results would be considered as false positives.
since our assumption is that there is only one relevant result for the query we use the well known mean reciprocal rank mrr as the metric to evaluate the actual query results.
this task can be formulated as the information retrieval ir problem and the neural ir techniques are widely applied recently for textual data we include word2vec doc2vec clir a cross lingual information retrieval system for text.
we also follow sachdev et al.
to include elasticsearch a fuzzy text search baseline.
although there are recent methods designed specifically for code to code search such as facoy and aroma they are designed only for monolingual code search thus we do not compare with them directly.
results table iii shows the results for infercode and other baselines.
the performance of infercode is the best among all the models.
elasticsearch on the other hand performs the worst this is expected because elasticsearch is a simple fuzz text search technique not designed to capture structural information of code.
3these are taken from the names of the algorithms at acmeism rosettacodedata tree master tasktable iii results of cross language code to code search in mean reciprocal rank mrr approachperformance mrr java c c c elasticsearch .
.
.
.
word2vec .
.
.
.
doc2vec .
.
.
.
clir .
.
.
.
infercode .
.
.
.
d. fine tuning for supervised learning tasks datasets metrics and baselines a code classification we again use the oj dataset for this task.
we split this dataset into three parts for training testing and validation by the ratio of .
out of the training data we feed x to the neural model where x .
we then initialize the neural model either randomly or with the weights from the pre trained infercode.
therefore we have four settings for training the supervised model for comparison fine tuning the tbcnn encoder with or of the labeled training data respectively and the randomly initialized model.
using only or is to demonstrate that given a pre trained model one only needs a small amount of labeled data to achieve reasonably good performance for the downstream task.
we use the accuracy metric widely used for classification tasks.
as the baselines we include the astnn trained from scratch which is a state of the art model for code classification on the oj dataset and textcnn and bilstm trained with of the training data which are widely used for text classification.
b method name prediction we use the java small dataset widely used as a benchmark for method name prediction and has been used in code2vec and code2seq .
this dataset has already been split into three parts namely training testing and validation.
we perform the same evaluation protocol as the code classification task by fine tuning the model with and of the labeled training data in contrast to random initialization of the model without fine tuning.
to predict the method name we follow code2vec to use the code vector vto predict the embedding of a method name from a lookup table see section .
in code2vec .
we measure prediction performance using precision p recall r and f1 scores over the sub words in generated names following the metrics used by alon et al.
.
for example a predicted name result compute is considered as an exact match of the ground truth name computeresult predicted compute has full precision but only recall and predicted compute model result has full recall but only precision.
results table iv shows the results for code classification.
fine tuning on of the training data gets comparable results with the nlp baselines.
fine tuning on of the training data gets comparable with astnn a state of the art model for code classification on the oj dataset.
1193table iv results of code classification in accuracy with fine tuning ft on the ojdataset approach ft ft ft supervised infercode .
.
.
textcnn .
bi lstm .
astnn .
table v result of method name prediction in f1with fine tuning ft on the java small dataset approach ft ft ft supervised infercode .
.
.
.
code2vec .
code2seq .
table v shows the results for method name prediction.
we get a comparable result with code2seq when fine tuning with labeled data.
e. summary infercode outperforms most of the baselines across five tasks including three unsupervised ones code clustering code clone detection via similarity measurement cross language code to code search and two supervised ones code classification and method name prediction .
note that this does not mean that the tbcnn encoder in infercode is better than astnn code2vec or code2seq as those neural models can be used as the encoder in infercode too.
it only means that pre training a model on large unlabeled data using self supervised learning to predict subtrees can produce more transferable models while maintaining the performance of such models for various code learning tasks.
the performance of the self supervised learning models may be improved further with different encoders.
we leave those explorations for future work.
vii.
a nalysis this section analyses the effects of various parameters on the performance of different tasks.
a. cluster visualization to help understand why the vectors produced by infercode are better than the vectors produced by others we visualize the vectors of the programs from the oj dataset that have been used for the code clustering.
we choose the embeddings produced by doc2vec code2vec and infercode for the first classes of the oj dataset then we use t sne to reduce the dimension of the vectors into two dimensional space and visualize.
as shown in figure the vectors produced by infercode group similar code snippets into the same cluster with clearer boundaries and the boundaries among clusters produced by doc2vec and code2vec are less clear which makes it more difficult for the k means algorithm to cluster the snippets correctly.
this is aligned with the performance of the code clustering task table i .
also we observe that some points marked in the same color e.g.
red are somewhat far away from each other even in the vectors from infercode while they are supposed to be close according to the ground truth.
this could indicate further improvement to infercode can be made in future work.
b. effect of textual information in tbcnn the original tbcnn in mou et al.
does not include textual information in ast nodes to initialize the node embedding.
in our implementation we include the textual information by fusing it with the node type information through a linear layer.
to help understand the effect of such a fusion process we perform an ablation study by training infercode with different initialization information on the java large dataset and perform the evaluations on the three unsupervised tasks code clustering cc code clone detection ccd and cross language code to code search clcs with the same settings for each of the tasks in section vi.
table vi shows the results of this study.
using only type or token information will result in worse performance for all three tasks.
table vi effects of different initialization methods task dataset metricinitial information type token combine cc oj ari .
.
.
ccd bigclonebench p .
.
.
clcs rosetta stone mrr .
.
.
c. alternative choices to the pretext task labels there are a few alternatives when we use subtrees as the pseudo labels for the pretext task in infercode.
one can easily replace the subtrees with tokens so that the code vector vcan predict the tokens of the code snippets similar to doc2vec or one can use all the method names as the pseudo labels and train the vto predict the names similar to code2vec .
in this section we perform an ablation study to measure how different types of labels can affect performance.
as shown in table vii the performance using the subtrees as the labels is the best while using tokens as the labels result in the worst performance.
although using the method name can result in reasonable performance it is still worse than using the subtrees.
an explanation for this is that by predicting method names the model is forced to learn some incorrect patterns due to similar names in the code base that actually refer to different code.
for example jiang et al.
found that a large number code snippets contain similar method names but the actual implementations of the method bodies are different but their code vectors would be forced to predict the similar method names thus these vectors will be close in the vector space despite that they should not be.
this is a potential reason to make the model trained by predicting method names a worse choice for pretext task than using subtrees.
viii.
d iscussion a. choice of encoder in this section we want to discuss our choice on the decoder.
we choose tbcnn because of its ability to capture structural 1194fig.
.
visualization of the code vectors of the programs from classes in the oj dataset produced by infercode code2vec and doc2vec table vii effects of different ways to set up labels of the pretext task task dataset metriclabel token method name subtree cc oj ari .
.
.
ccd bigclonebench p .
.
.
clcs rosetta stone mrr .
.
.
features of code that lie in asts and the modification we made to tbcnn can also capture textual information into the model.
there are many neural network designs that can be used as a replacement of the tbcnn encoder such as astnn code2vec or ggnn however most of them especially the graph based models are unable to scale and generalize for different programming languages.
for example we can use the path encoder of code2vec to encode the ast paths into the code vector vand infer the subtrees.
ggnn is similar one can pre train the ggnn over a selfsupervised learning task.
although the graph representation proposed by narayanan et al.
allamanis et al.
has been shown to work well on tasks such as supervised clone detection code summarization variable name prediction etc.
choosing the suitable edges to be included in the graph representations for such tasks can be time consuming and not generalizable.
lambdanet is another graph based model that also contains semantic edges designed specifically for the type prediction task.
as such it is not straightforward to transfer a pre trained graph learning model through different code learning tasks and it is not easy to scale the graph representation of code into multiple languages.
similar reasons can also be applied for path based models such as code2vec and code2seq or execution trace based models .
on the other hand tbcnn is designed to receive the ast directly with minimal engineering effort to process it.
ast is relatively easy to produce accurately for most programming languages given their grammars thus building a tree based learning model on top of asts implies that we can have a model that is easier to generalize across languages which is the advantage to choose tree based models over others.
note that this is not to say that other models do not perform well on the code learning tasks they can still perform well when training data and time are specially utilized and they may be used together with each other as the encoder in the self supervised learning framework to improve the performance for various tasks further.
we leave all the exciting explorations for future work.b.
assumption on predicting similar subtrees with opposite meaning infercode works on the basis of the key assumption that code snippets containing similar subtrees have the same meanings.
there are instances where code snippets can have the opposite meaning even if they have the same subtree e.g.
a b vs. b a .
this issue is addressed by modifying the tbcnn to encode the information of the tokens.
note that the original tbcnn mou et al.
only encodes the node type information.
with this change the tbcnn can distinguish both syntactic and semantic information better than the original version as implied by the results shown in table vi.
ix.
c onclusions we have proposed infercode a self supervised learning technique for source code learning on unlabeled data.
along with the document embedding principle that similar documents contain similar words our working intuition is that similar asts should have similar subtrees to predict using a code embedding learnt from the asts.
we first train a tree based cnn on large scale datasets then reuse it as a pre trained model for the infercode encoder to map any ast into an embedding vector for downstream tasks such as code clustering code clone detection or code to code search.
evaluation of these tasks shows that the embeddings produced by the infercode encoder outperform the other baselines with significant margins.
furthermore the weights of the self supervised pretrained model can be used for subsequent supervised finetuning which outperforms the supervised models trained from a scratch.
in the future we will explore other choices of the encoder and adapt infercode to other se tasks such as bug localization defect prediction variable name prediction etc.