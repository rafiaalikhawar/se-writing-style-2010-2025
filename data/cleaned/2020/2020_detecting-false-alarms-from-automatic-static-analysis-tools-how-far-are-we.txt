detecting false alarms from automatic static analysis tools how far are we?
hong jin kang singapore management university singapore singapore hjkang.
phdcs.smu.edu.sgkhai loong aw singapore management university singapore singapore klaw.
scis.smu.edu.sgdavid lo singapore management university singapore singapore davidlo smu.edu.sg abstract automaticstaticanalysistools asats suchasfindbugs havea highfalsealarmrate.thelargenumberoffalsealarmsproduced posesabarriertoadoption.researchershaveproposedtheuseof machinelearningtoprunefalsealarmsandpresentonlyactionable warnings to developers.
the state of the art study has identified a set of golden features based on metrics computed over the characteristicsandhistoryofthefile code andwarning.recentstudies showthatmachinelearningusingthesefeaturesisextremelyeffective and that they achieve almost perfect performance.
weperformadetailedanalysistobetterunderstandthestrong performance of the golden features .
we found that several studies used an experimental procedure that results in data leakage anddataduplication whicharesubtleissueswithsignificantimplications.firstly theground truthlabelshaveleakedintofeatures that measure the proportion of actionable warnings in a given context.
secondly many warnings in the testing dataset appear in the trainingdataset.next wedemonstratelimitationsinthewarning oracle that determines the ground truth labels a heuristic comparingwarningsinagivenrevisiontoareferencerevisioninthefuture.
weshowthechoiceofreferencerevisioninfluencesthewarning distribution.
moreover the heuristic produces labels that do not agree with human oracles.
hence the strong performance of these techniques previously seen is overoptimistic of their true perfor mance if adopted in practice.
our results convey several lessons and provide guidelines for evaluating false alarm detectors.
ccs concepts software and its engineering software defect analysis.
keywords static analysis false alarms data leakage data duplication acm reference format hongjinkang khailoongaw anddavidlo.
.detectingfalsealarms from automatic static analysis tools how far are we?.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
introduction it has been years since findbugs was introduced to detect bugs in java programs.
along with other automatic static analysis tools asats findbugsaimstodetectincorrectcodeby matchingcodeagainstbugpatterns forexample patternsof codethatmaydereferenceanullpointer.sincethen manyprojects have adopted these tools as they help in detecting bugs at low cost.
however these tools do not guarantee that the warningsare real bugs.
many developers do not perceive the warnings by asats to be relevant due to the high incidence of effective false alarms .
prior work has suggested that the false positive ratemayrangeupto91 .whiletheoverapproximationofstatic analysismaycausefalsealarms falsealarmsdonotonlyreferto errorsfromanalysisoroverapproximation butincludewarnings that developers did not act on .
developers may not act on a warning if they do not think the warning represents a bug or believe that a fix is too risky.
toaddressthehighrateoffalsealarms manyresearchers have proposed techniques to prune false alarms and identify actionablewarnings whicharethewarningsthatdeveloperswould fix.
these approaches consider different aspects of a warning reported by findbugs in a project includingfactorsaboutthesourcecode repositoryhistory filecharacteristics andhistoricaldataaboutfixestofindbugs warnings within the project.
wang et al.
completed a systematic evaluation of the features that have been proposed in the literature and identified golden features which are themost important features for detecting actionable findbugs warn ings.
using these features subsequent studies show that anymachinelearningtechnique e.g.svm performseffectivelyand that the use of a small number of training instances can train effective models.
in these studies performances of up to recall and precision and .
auc can be achieved.
a perfect predictor has a recall precision and auc of suggesting that machine learning techniques using the golden features are almost perfect.
althoughthegoldenfeatureshavebeenshowntoperformwell we do not know why they are effective.
therefore in this work we seek to get a deeper understanding of the golden features.
we find a few issues first the ground truth label was leaked into the featuresmeasuringtheproportionofactionablewarningsinagivencontext.second warningsinthetestdatawereusedfortraining.tounderstandtheirimpact weaddressedthetwoflawsandfoundthat the performance of the golden features declines.
our results show thattheuseofthegoldenfeaturesdonotsubstantiallyoutperform a strawman baseline that predicts all warnings are actionable.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hong jin kang khai loong aw and david lo next we investigate the warning oracle used to obtain groundtruth labels when constructing the dataset.
to evaluate any proposedapproach alargedatasetshouldbebuilt whereeachwarning is accurately labeled as either an actionable warning or a false alarm.
many studies use a heuristic which we term the closed warning heuristic as the warning oracle to determine the actionabilityofawarning checkingifthesamewarningisreported inareference revision a revision chronologicallyafterthe testing revision.ifthefileisstillpresentandthewarningisnotreportedin thereferencerevision thenthewarningis closedandisassumed to be fixed.
it is therefore assumed to be actionable.
conversely a warningthatremained openisafalsealarm.a revisionmadea few yearsafterthesimulatedtimeoftheexperimentalsettingisusedas thereferencerevision.prior studies selectedreference revisionsset2yearsafterthetestingrevision.however noprior work has investigated the robustness of the heuristic.
there are several desirable qualities of a warning oracle.
firstly it should allow the construction of a sufficiently large dataset.
secondly it should be reliable the labels should be robust to minor changesintheoracle.thirdly itshouldgeneratelabelsthathuman annotators and developers of projects using asats agree with.
an advantageoftheclosed warningheuristicisthatitenablestheconstructionofalargedataset.however ourexperimentsdemonstrate the lack of consistency in the labels given changes in the choice of thereferencerevision.thismayallowdifferentconclusionstobe reached from the experiments.
our experiments also uncover that theoracledoesnotalwaysproducelabelsthathumanannotators or developers agree with.
these limitations show that alone the heuristicdonotalwaysproducetrustworthylabels.afterremoving unconfirmed actionable warnings the effectiveness of the golden features svm improves indicating the importance of clean data.
finally we highlight lessons learned from our experiments.
our resultsshowtheneedtocarefullydesignanexperimentalprocedure toassessfutureapproaches comparingthemagainstappropriate baselines.
ourwork points outopen challengesin thedesign ofa warning oracle for the construction of a benchmark.
based on the lessons learned we outline several guidelines for future work.
we make the following contributions we analyze the reasons for the strong performance fromtheuseofthe goldenfeatures observedinpriorstudies.
contrary to prior work we find that machine learning techniques are not almost perfect and that there is still much room for improvement for future work in this area.
westudythewarningoracle the closed warningheuristic that assigns labels to warnings used in previous studies.
we show that the heuristic may not be sufficiently robust.
we discuss the lessons learned and their implications.
im portantly we highlight the need for community effort inbuilding an accurate benchmark and suggest that future studies compare new approaches with strawman baselines.
therestofthepaperisstructuredasfollows.section2covers thebackgroundofourwork.section3presentsthedesignofthe study.section4analyzesthegoldenfeatures.section5investigates theclosed warningheuristic.section6discusseslessonslearned fromourstudy.section7presentsrelatedwork.finally section8 concludes the paper.
background .
automatic static analysis tools manyresearchers haveproposedautomaticstatic analysistools asats suchasfindbugs todetectpossiblebugsduringthe software development process.
research has shown these tools are useful and succeed in detecting bugs that developers are interested inatlowcost.comparedtoprogramverificationorsoftwaretesting thesetoolsrelyonbugpatternswrittenbytheauthorsofthestaticanalysistools matchingcodethatmaybebuggy.findbugsincludes over bug patterns that match a range of possible bugs suchas class casts that are impossible null pointer dereferences and incorrect synchronization.
studies have also shown that asats are able to detect real bugs .
indeed static analysis tools are adopted by large companies and open source projects to detect bugs.
developersmayrunthemduringlocaldevelopment usethemin continuousintegration andduringcodereviewtodetectbuggy code to catch bugs early .
projects may configure the tools forexample tosuppressfalsealarmsbyconfiguringa filter file to exclude specific warnings .
developerslargelyperceiveasatstoberelevant andthemajority of practitioners have used or heard of asats .
still these tools are characterized by the large amounts of false alarms that they produce and among other reasons this has led to resistance in adopting them in many software projects .
.
distinguishing between actionable warnings and false alarms tominimizetheoverheadofinspectingfalsealarms researchers haveproposedapproachesbasedonmachinelearningtorankor classify the warnings.
a large number of features have been designed over the past years for example based on software metrics e.g.sizeof thefile numberof commentsinthe code source codehistory e.g.numberof linesofcoderecentlyaddedtoafile and characteristics and history of the warnings e.g.
the number of revisions where the warning has been opened .
researchershaveevaluatedtheirproposedtoolsthroughdatasets of warnings produced by findbugs .
recently wang et al.
performed a systematic analysis of the features proposed in the literature.
from features they identified goldenfeatures whicharethefeaturesthatachieveeffectiveperformance.
the features are listed in table .
these features include metricssuchasthecode to commentsratio andthenumber of lines added in the past .
of note are several features of the warning combination feature type.
we will refer to three of these features warning context in method warning context in file andwarning context for warning type as thewarningcontext features.
we refer to another two features the defect likelihood for warning pattern anddiscretization of defect likelihood asthedefect likelihood features.thesefeaturesare variousmeasuresoftheproportionofactionablewarningswithina populationofwarnings buildingontopoftheinsightthatwarnings withinthepopulationsharethesamelabel e.g.ifawarningwas previouslyfixedinafile itismorelikelythattheotherwarnings in the same file will be fixed too.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
detecting false alarms from automatic static analysis tools how far are we?
icse may pittsburgh pa usa table the golden features studied in prior work .
a warning context is defined as the difference of the number of actionable warnings and false alarms divided by the total number of warnings reported in a given method file or for a warning pattern.
we provide more descriptions of each feature in our replication package .
feature type feature warning context in method warning context in file warning combination warning context for warning type defect likelihood for warning pattern discretization of defect likelihood average lifetime for warning type comment code ratio method depth code characteristics file depth methods in file classes in package warning pattern warning characteristics warning type warning priority package file age file history file creation developers code analysis parameter signature method visibility code history loc added in file last revisions loc added in package past month warning history warning lifetime by revision further research on the golden features of wang et al.
showed the lack of influence of the choice of machine learningmodel on effectiveness.
they suggested that a linear svm was optimal since it requires a lower cost of training.
in contrast while a deep learning approach achieves similar levels of effectiveness it has a longer training time.
their analysis suggested that the detection of false alarms is an intrinsically easy problem.
a differentstudy demonstratedthat withthegoldenfeatures only a small proportion of the dataset has to be labelled to train an effective classifier.
the golden features are a subject of our study.
in section we analyze them in detail.
closed warning heuristic.
the procedure to construct and label the ground truth dataset can be visualized in figure .
to assess an approach that detects false alarms a dataset of findbugs warnings is collected.
while some researchers construct a labelleddatasetthroughmanuallabellingofthewarningsinasingle revision other researchers collect a dataset through an automatic ground truth data collection process .
data for a testingrevision andatleastone trainingrevision setchronologically beforethetestingrevision iscollected.thissimulatesreal world use of the tool in which training is done on the history of the project and then used at the time of the testing revision.
usingthe closed warningheuristic asthewarningoracle each warninginagivenrevisioniscomparedagainsta referencerevision figure1 thedatasetcompriseswarningscreatedbeforethetraining and testing revisions.
the labels of each warningaredeterminedbytheclosed warningheuristic ifawarningis closed at the reference revision and the file has not beendeleted then it is actionable.
setinthefutureofthetestrevision.priorstudiesselectedareference revisionset2yearsafterthetestrevision.ifaspecificwarningis nolongerpresentinthereferencerevision i.e.
a closedwarning theheuristicassumesthatthewarningisactionable.ifthewarning is present in both the given and reference revision i.e.
an open warning thentheheuristicassumesthatitisafalsealarm.ifthe file that contains the code with the warning has been deleted then thewarningislabelled unknown andisremovedfromthedataset.
in other words according to the the closed warning heuristic aclosed warning is always actionable as long as the file has not beendeleted andanopenwarningisalwaysunactionable.other thandetectingactionablewarnings researchershaveappliedthe heuristictoidentifybug fixingcommitsforminingpatterns .
the heuristic is a subject of our study and we assess its robustness and its level of agreement with human oracles in section .
study design .
research questions rq1.
why do the golden features work?
this research question seeks to understand the golden features.
while previous studies have highlighted their strong results there has not been an in depth analysis of their practicality.
we study the golden featuresandthedatasetusedintheexperimentsbywangetal.
and yang et al.
.
we investigate the aspects of the features and dataset that allow accurate predictions by the best performing machinelearningmodel ansvmusingthegoldenfeatures.we replicate the results of the previous studies and validate the predictivepowerofthegoldenfeatures.tounderstandtheimportance ofdifferentfeatures weuselime tonarrowourfocusdownto thefeaturesthatcontributethemosttothepredictions.afterwards we switch to increasingly simpler classifiers and analyze the experimental data to better understand why the choice of classifiers did not influence the results in prior studies.
rq2.howsuitableistheclosed warningheuristicasawarning oracle?
this research question concerns the suitability of the closed warning heuristic as a warning oracle.
a good oracle should be robust and its judgments should agree with the analysis of ahuman annotator.
we investigate the robustness of the heuristic checking the consistency of labels under different choices of the referencerevision.whilepreviousstudiesuseda2 yearsinterval authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hong jin kang khai loong aw and david lo between the test revision and reference revision we investigate if different conclusions can be reached with a different time interval.
next wecomputetheproportionofclosedwarningsthathuman annotatorslabelledactionable andtheproportionofopenwarnings that project developers suppressed as false alarms.
.
evaluation setting toanalyzetheperformanceofmachinelearningapproachesthat identify actionable findbugs warnings we use the same metrics as prior studies .
a true positive tp is an actionable findbugswarningcorrectlypredictedtobeactionable.afalseposi tive fp isanunactionablefindbugswarningincorrectlypredicted tobeactionable.notethatweusetheterm falsealarm toreferto unactionablefindbugswarning.a falsepositive therefore refers toafalsealarmthatisincorrectlydeterminedtobeanactionable warning.afalsenegative fn isanactionablewarningincorrectly predicted to be a false alarm.
a true negative is an unactionable warning correctly predicted to be a false alarm.
we compute precision and recall as follows precision tp tp fp recall tp tp fn finally we compute and present f1 the harmonic mean of precision and recall.
f1 is known to capture the trade off betweenprecision and recall and is used in place of accuracy given an imbalanced dataset.
f1 is computed as follows f1 precision recall precision recall the area under the receiver operator characteristics curve auc is a measure of the predictive power of a machine learningapproachtodistinguishbetweentrueandfalsealarms.ranging between worst discrimination and perfect discrimination auc is the area under the curve of the true positive rate against thefalsepositive rate andrecommendedover accuracywhenthe data is imbalanced.
a strawman classifier that always outputs a single label has an auc of .
.
our dataset consists of projects that were studied by yang et al.
andwangetal.
.similartopreviousstudies weuseonetrainingrevisionandonetestingrevision.weusethe same testing revision as previous studies .
we train one model for each project.
analysis of the golden features to answer the first research question we investigate the performanceofthegoldenfeaturesbyfirstusingthesamedatasetusedby yang et al.
.
the dataset includes two revisions from projects.
the testing revisions are the revisions of the projects on january and the training revision is a revision of the projects up to6 months before the testing revision.
in total warning instanceswereobtained byrunningfindbugsoverthe trainingand testingrevision.onaverage .
ofthewarningsinthedataset were actionable.
table shows the breakdown of the warnings.
wesuccessfullyreplicatetheperformanceobservedintheexperimentsofyangetal.
andwangetal.
obtaininghighauc values of upto .
.
an averagef1 of .
wasobtained with f1 ranging from .
to .
.
table shows our experimental results.table the number of training testing instances and the percentage of actionable warnings act.
in the dataset.
the testing revision is the last revision checked into the main branch on .
project training with duplicates w o duplicates testing act.
testing act.
ant cassandra commons derby jmeter lucene maven tomcat phoenix yangetal.
foundthatthedatasetwasintrinsicallyeasyas the data was inherently low dimensional.
to further analyze their findings weusedtoolsfromthefieldofexplainableai inparticular lime to identify the most important features contributing to eachprediction.limeisanexplanationtechniquethatidentifiesthemostimportantfeaturesthatcontributedtoanindividualprediction.
toidentifythemostimportantfeatures wesampled50predictions made by the golden features svm and used lime to identifythe top features contributing to the predictions.
we found thattwo features warning context of file andwarning context of package appeared in the top features of every prediction.
warning context and defect likelihood features.
on analyzing the source code of the feature extractor developed by wang et al.
we found a subtle data leak in the implementation of the warning context and defect likelihood features.
these features utilizefindingsfrompreviousstudies thatfoundthatthewarnings within a population e.g.
warnings in the same file tend to be homogenous if one warning is a false alarm then the other warnings in the same population tend to be false alarms as well.
includingwarning context of file andwarning context of package there are another features computed similarly warning context of warning type defect likelihood for warning pattern discretization of defect likelihood for warning pattern .
at a high level the warning context features are computed as follows wactionable relevant wfalse alarm relevant wrelevant wrelevantreferstothesetofwarningsrelevanttothefeaturetype.
for example wrelevantofwarning context of file considers the warnings that are reported in a given file while wrelevantofwarningcontextofwarningtype considersallwarningsforthegiven category of patterns e.g.
style internationalization .
note that a warning pattern refers to a specific bug pattern in findbugs e.g.
es comparing strings with eq andawarningtype isacategoryofpatterns.the defectlikelihoodforwarningpattern featurecomputestheproportionofwarningsthatwere actionable out of all warnings with the given bug pattern p d p wactionable relevant wrelevant authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
detecting false alarms from automatic static analysis tools how far are we?
icse may pittsburgh pa usa table effectiveness of an svm using the golden features after removing the leaked features and removing the duplicate warnings between the training and testing dataset.
the numbers in parentheses are the f1 obtained by the baseline classifier that predicts all warnings are actionable.
project all golden features leaked features data duplication leak duplication f1 auc f1auc f1auc f1auc ant .
.
.
.
.
.
cassandra .
.
.
.
.
.
.
.
.
.
.
.
commons .
.
.
.
.
.
.
.
.
.
.
.
derby .
.
.
.
.
.
.
.
.
.
.
.
jmeter .
.
.
.
.
.
.
.
.
.
.
.
lucene solr .
.
.
.
.
.
.
.
.
.
.
.
maven .
.
.
.
.
.
.
.
.
.
.
.
tomcat .
.
.
.
.
.
.
.
.
.
.
.
phoenix .
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
thediscretizationofdefectlikelihoodforwarningtype feature computed for each type category tof bug patterns is a measureofthedifferenceindefectlikelihoodfromthedefectlikelihoodof tfor each bug pattern in the category t summationtext.
p t d p d t the five warning context and defect likelihood features require information about the actionability of each warning in the population of warnings considered.
a data leakage occurs when the classifierutilizesinformationthatisunavailableatthetimeofits predictions .
as shown in figure while the ratio of actionable warnings are computed over the warnings reported in the past the black line in figure the closed warning heuristic to determine the ground truth label of a warning the red lines in figure and figure is utilized to determine if these warnings were actionable.tocomputethewarningcontextofagivenwarning wt in the testing revision the labels of all warnings in the population ofwarnings e.g.allwarningsinthesamefile including wt ar e obtained based on comparison to the reference revision.
sincetheground truthlabelisalsoobtainedbasedoncomparisontothereferencerevision theground truthlabelis inadvertently leakedintothecomputationofthewarningcontext.thisisnota realistic assumption in practice at test time the ground truth label of the warning context of wtis the target of the prediction.
while checkingifawarningwillbeclosed2yearsinthefutureispossible withinanexperiment thereisnowaytocheckifthewarningswill beclosed2yearsintothefutureinpractice.table3showsthelargedropinf1 fromanaverageof0.88to0.
whenthesefivefeatures are dropped.
we refer to these features as leaked features.
baseline using data leakage.
data leakage leads to an experimental setting that overestimates the effectiveness of the classifier understudy .intable4 weshowthatabaselineequivalent to the golden features can be developed using only the five leaked features.
using just the leaked features with an svm we construct a baseline that achieves performance comparable to the use of the goldenfeatures.ansvmusingtheleakedfeatureshasaprecision of0.
about0.10lowerthanthegoldenfeaturessvm however they achieve identical recall of .
which results in an f1 of .
just .
lower than the golden features.
this indicates that the figure the warning context and defect likelihood fea tures use labels derived through the closed warning heuris tic using information from the reference revision chrono logically in the future of the test revision.
in a realistic set ting this information will not be present at test time.
strongperformanceof thegoldenfeaturesin theexperimentsdepends largely on the leaked features and is an optimistic estimate of their effectiveness.
thecomputationofthe warningcontext anddefectlikelihood features caused data leakage as it used labels determined by comparison against the reference revision chronologically in the future of the testing time.
data duplication.
next we progressively selected simpler machine learning models and surprisingly found that a k nearest neighbors knn classifierperformseffectively.inparticular we found asurprising trend where thelower values of kled tobetter results.theresultsoftheexperimentwhereweiterativelylowered kto consider in the prediction are shown in table .
surprisingly aknnclassifierwithk i.e.
onlyoneneighboris consideredto makea prediction produces thebest result obtained a precision of .
a recall of .
with an f1 of .
.
with k the classifier was selecting a single most similar warning in the trainingdataset.intypicalusageofknn alowvalueof kmaycause theclassifiertobeinfluencedbynoiseandoutliers whichmakes the strong results surprising.
to analyze the results further we observed that the number of training and testing instances were similar and we investigated the data carefully.
we found that many testing instances appeared in both the training and testing dataset.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hong jin kang khai loong aw and david lo table average precision prec.
recall and f1 of various approaches on the original dataset by yang et al.
technique prec.recall f1 golden features svm .
.
.
leaked features .
.
.
data duplication .
.
.
data duplication and leaked features .
.
.
reimplemented leaked features .
.
.
golden features knn with k .
.
.
golden features knn with k .
.
.
golden features knn with k .
.
.
golden features knn with k .
.
.
only leaked features svm .
.
.
repeat label from training dataset .
.
.
thedataduplication wascausedbythedata collectionprocess inwhichallwarningsproducedbyfindbugsforboththetraining and test revisions were included in the training and testing dataset.
saywehaveawarningatthetrainingrevision determinedtobe open and therefore unactionable by the closed warning heuristic.
inotherwords thewarningremainedopenintheperiodbeforethe trainingrevisiontothereferencerevision.then thewarningwouldcertainlybeopenedatthetestingrevision whichischronologically beforethereferencerevisionbutafterthetrainingrevision.likewise if we have a warning only closed after the testing revision butwasopenduringthetestingrevision thenthesamewarning would be present at both the training and testing revision with the same actionable label.
consequently a large number of warnings appear in both the training and testing dataset.
this contributes to an unrealistic experimental setting.
baseline using duplicated data.
data duplication creates an artificial experimental setting that inflates performance metrics .
to confirm that the data duplication contributes to the ease of the task weconstructaweakbaseline adummyclassifier thatleverages the duplication of testing data in the training dataset.
given a warning from the testing dataset the classifier heuristically identifies the same warning from the training dataset by searching for a training warning based on the class name e.g.
booleanutils and bug patternname e.g.
es comparing strings with eq .
iftherearemultiplewarningswiththesameclassnameandbug pattern name a randomtraining instance isselected fromamong them.
the classifier then outputs the label of the training instance.
if there is no training instance with the same class and bug pattern type then the classifier defaults to predicting that the warning is a false alarm which is the majority class label.
table shows the comparison of various approaches includingthebaseline approaches onthedataset.thedummy classifier achievesstrongperformance achievingaprecisionof0.
arecallof0.
andanf1of0.
.whilethedummyclassifierunderperformsthemodelusingtheleakedfeatures itoutperformsthegoldenfeatures svm without the leaked features.
this indicates that using justtwoattributes theclassnameand thebugpatternofthe warning is enough to obtain strong performance on a dataset with data duplication.
therefore we conclude that the data duplication figure we reimplemented the leaked features.
the reim plemented features use only information represented bythe blue dashed lines available at the present i.e.
eitherthe training or test revision to determine if a warning i.e.
created before the training or test revision has been closed.under this setting no information from the reference revi sion is used for making predictions.
between the training and testing dataset contributes to the strong performance observed in previous studies.
the experimental results are summarized in table .
with both the leaked features and duplicated data the average f1 was .
.
afterthedataleakagefeaturesareremoved f1decreasedto0.
.
after removing the duplicated data f1 decreases further to .
.
the average project s auc decreased from .
to .
.
in comparison using a strawman baseline that predicts that every warning is actionable produces an f1 of .
with an auc of .
.
allwarningsreportedbyfindbugsonboththetrainingandtesting revisionswereincludedinthedatasets.warningsreportedatthe trainingrevisionmaystillbereportedatthetestingrevision leading to data duplication between the training and testing dataset.
experimentsunderamorerealisticsetting.
tobetterunderstandtheperformanceofthegoldenfeaturessvm werananother experiment where the two issues of data leakage and data dupli cation have been fixed.
first we deduplicated the test data from thetrainingdataset.insteadofincludingallwarningsinthetesting revision we only consider new warnings introduced between the time after the training revision and before the testing revision.
figure3showsourprocedure.ascomparedtothepreviousdataset constructionprocessinfigure1 onlythewarningscreatedafter the training revision and before the testing revision are used for testing.thisbetterreflectsreal worldconditionswhereallwarnings prior to usage are used for training but none of the testing datainvolveswarningsthathavealreadybeenclassified.intotal thenumberofwarningsinthetesting revisionsdecreasedfroma total of to after deduplication.
without the duplicated dataandwithoutusingtheleakedfeatures theaveragef1drops from .
to .
as seen in table .
next wereimplementedtheleakedfeaturestoinvestigatethe effectiveness of golden features svm.
to prevent data leakage we modifiedthedefinitionoftheleakedfeatures.figure3visualizesthe computation of the warning context and defect likelihood features.
instead of considering all warnings we consider only warnings thatwere introducedinthe 1yearduration beforethetraining or testing revision.
instead of using the reference revision we usethe given revision i.e.
either the training or testing revision todetermine if the warning was closed.
a warning is closed at a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
detecting false alarms from automatic static analysis tools how far are we?
icse may pittsburgh pa usa givenrevisioniffindbugsdoesnotreportit.inotherwords forthe trainingrevision onlythewarningscreatedwithinthepastyear before the training revision are considered.
for testing only the warnings created within one year before the testing revision are considered.
a time interval of year was selected in contrast to the study by wang et al.
which used time intervals of up to months.unlikewangetal.
forthetestingrevision weconsider onlywarningscreatedafterthetrainingrevisiontopreventdata duplication.consequently wefoundfewernewlycreatedwarnings intheshorttimeintervalbetweenthetrainingandtestingrevisions.
notethat afterreimplementing thewarning contextanddefect likelihoodfeatures wecouldnotruntheexperimentsfortheproject phoenix as we facedmany difficulties building old versions ofthe project.
moreover their revision history did not go back beyond3 years required for computing the warning context and defect likelihood features for the training revision.
this limitation is not presentforwangetal.
astheycomputethefeaturesbycheckingifthegivenwarningisclosedinthereferencerevision setin thefutureofthetestrevision causingdataleakage .assuch we omit phoenix for the rest of the experiments.
table shows the performance of the golden features svm using the reimplemented features.
without the leaked features the golden features svm achieves an f1 of .
.
even with the reimplementation of the leaked features the golden features svm underperforms the strawman baseline which predicts all warnings are actionable with an f1 of .
.
however it has an auc of .
greaterthan0.
indicatingthatthegoldenfeaturesarebetterthan random and have some predictive power.
answer torq1 afterremovingthedataleakageanddataduplication our experimental results indicate that the golden features svmunderperformsthestrawmanbaseline althoughitsauc .
suggests that the golden features have some predictive power.
analysis of the closed warning heuristic next giventhatthequalityandrealismofthedatasetheavilyinfluencestheevaluationofthegoldenfeaturessvm weperform adeeperanalysisoftheconstructionoftheground truthdataset.
in previous studies the warning oracle is the closedwarningheuristic awarningisheuristicallydeterminedtobeactionableifitwasclosed i.e.
reportedbyfindbugsinarevisionbut wasnotreportedbyfindbugsinthereferencerevision andthefilewasnotdeleted andisafalsealarmifitwasopen i.e.reportedby findbugs on both the training test and reference revision .
inthefirstpartofouranalysis weinvestigatetheconsistencyin the warning oracle given a change in the reference revision.
next wecheckifthewarningoracleproduceslabelsthathumanusers wouldagreewith.todoso wefirstdetermineifhumanannotators considerclosedwarningsasactionablewarnings.inaddition we matchopenwarningsagainstfindbugsfilterfilesinprojectswhere developers have configured the filters for suppressing false alarms.
finally weobserveifcleanerdataincreasestheeffectivenessofthe golden features svm.
.
choosing a different reference revision we perform a series of experiments to determine how the time interval between the test revision and the selected reference revision influences the ground truth label of the warnings.
we hypothesize that the longer the time interval between the test and reference revision thegreatertheproportionofclosedwarnings.basedon theclosed warningheuristic thiswouldcausemorewarningsto belabelledactionable.ifso thelackofconsistencyinlabelsshould call the robustness of the heuristic into question.
if many bugs are fixed only after many years then an open warning at any given time may in fact be actionable.
besides that if changing the referencerevisionleadsustoadifferentconclusionaboutthegolden featuressvm thenitlimitsthelevelofconfidencethatresearchers can have in the experimental results.
in our experiments we use three reference revisions set two three and four years after the test revision.
by switching the referencerevision weobservechangesintheaverageactionabilityratio.
while the actionability ratio remained consistent for the out of projects the actionability ratio increased by over for the other 4projects asseenintable5.overall theaverageactionabilityratio increased by when varying the time interval between the test andreferencerevisionfrom2to4years.consideringallprojects we performed a wilcoxon signed rank test and found that the change inactionabilityratioisstatisticallysignificant p value .
.
.
in terms of the effectiveness of the golden features svm its average f1 increased from .
to .
as seen in table .
con sidering all projects the golden features svm underperformedthe strawman baseline.
our experiments showed some variation ofthegoldenfeaturessvm seffectivenessgivenachangeinthe referencerevision.forinstance thegoldenfeaturessvmachieved alowf1of0.06inderbywhenthetimeintervalbetweenthetest andreferencerevisionwas2years buthadahighf1of0.72witha time interval of years.
by changing reference revisions the problem exhibits different characteristics.
using a reference revision years after the test revision actionablewarningswouldbethemajorityclass whilethey weretheminorityclasswhenusingtheotherreferencerevisions.
of8projectshaveanaucthatflippedfromonesideof0.5tothe other e.g.
thegolden features svm sauc is under0.
on derby givena2 yearsinterval buttheaucincreasesabove0.5givena years interval .
in short different conclusions about the task and the effectiveness of the golden features may be reached.
changing the reference revision may affect the distribution of the actionablewarnings whichmayimpacttheconclusionsreached from experiments on the effectiveness of the golden features svm.
.
unconfirmed actionable warnings next weinvestigateifclosedwarningsaretrulyactionablewarnings.awarningcouldbeclosedduetoseveralreasons.codecontaining the warning could be deleted or modified while implement inganewfeature andthewarningmayonlybeclosedincidentally.
tofurtherunderstandthecharacteristicsofclosedwarnings and to determine how likely is a closed warning an actionable warning we sampled warnings which is more than the statistically representative sample size of warnings that were closed.
two authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hong jin kang khai loong aw and david lo table the number of training testing instances and the percentage of actionable warnings act.
in the dataset when varying the reference revision.
the numbers in parentheses are the f1 obtained by the baseline classifier that predicts all warnings are actionable.
the testing revision is the last revision checked in to the main branch before .
project testing years years years instances act.
f1aucact.
f1aucact.
f1auc ant .
.
.
.
.
.
.
cassandra .
.
.
.
.
.
.
.
.
commons .
.
.
.
.
.
.
.
.
derby .
.
.
.
.
.
.
.
.
jmeter .
.
.
.
.
.
.
.
.
lucene .
.
.
.
.
.
.
.
.
maven .
.
.
.
.
.
.
.
.
tomcat .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
figure example of code that findbugs reports a warning on.
findbugs warns against using new long recommending the more efficient long.valueof to instantiate a longobject.
authorsofthisstudyindependentlyanalyzedeachwarningtodetermine if they were removed for a bug fix.
if the warning was closedduetocodechangesunrelatedtothewarning thenwedo not consider the warning as actionable.
if the code containing the warning was modified such that it was not easily discernible if thewarningwasclosedwiththeintentionoffixingthewarning then weconsider it unknown .if the originalversion of thecode had any comments indicating that findbugs reported a false alarm e.g.
explaining the reason that a seemingly buggy behavior was expected behavior then we consider the warning a false alarm.
when the labels differed between the annotators they discussed the disagreements to reach a consensus.
we computed cohen skappa to measure the inter annotator agreement and obtained a value of .
which is considered as strong agreement .
finally afterlabelling oftheheuristically closedwarnings were considered as false alarms.
another warnings werecategorizedas unknown .lastly warningswere still considered actionable after labelling.
foranexampleofawarninglabelled unknown figure4shows a fragment of code where findbugs complains about the use of the longconstructor indicating that long.valueof would be more efficient.
even though the warning is removed in the reference revision theentirefunctionalityofthecodefragmentwaschanged asshowninfigure5.insuchcases welabelthewarningas un known instead of actionable or a false alarm as there is no evidencethatthewarningwasfixedorignored.weconsiderthat figure the warning from figure is removed through achangeinfunctionality unrelatedtothewarningotherwise.
the warning was removed incidentally and that the annotators are unable to accurately label the warning.
whiletheclosed warningheuristicconsideredthatawarning couldberemovedthroughthedeletionofafile itdoesnotconsiderothercaseswhereawarningcouldbeincidentallyremovedthroughcodemodificationthatdoesnotfixthebugindicatedbythewarning ourresultsindicatethatmoreinformationshouldbeconsidered and that the heuristic may not be sufficiently robust.
only of closed warnings were labelled actionable by human annotators implyingthatmanyclosedwarningsarenotactionable.
many closed warnings were only closed incidentally.
.
unconfirmed false alarms our findings from section .
indicate the possibility that some actionablewarningswouldonlybeclosedgivenalongertimeinterval between thetest revision and the referencerevision.
this may reflect real world conditions where developers may not prioritize reports from asats and may take a long time before inspectingthem.thus openwarningsmaybeactionablewarningsthatthedevelopers would fix with enough time.
we run an experimentto understand this effect focusing on projects that have shown evidence of using findbugs.
in this experimental setup we remove open warnings that are not confirmed by the project developers to be false alarms.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
detecting false alarms from automatic static analysis tools how far are we?
icse may pittsburgh pa usa table number of open warnings in each project matched bytheirfindbugsfilterfile.ifawarningwasfiltered itindicates that the project s developers consider it a false alarm.
project open warnings filtered filtered jmeter tomcat commons lang flink hadoop jenkins kudu kafka morphia undertow xmlgraphics fop average mean average median someprojects whichusefindbugsintheirdevelopmentprocess configureafindbugsfilterfile forindicatingfalsealarms.the filter file allows developers to suppress warnings of specific bugpatterns on the indicated files.
developers may add warnings to thefindbugsfilterfileafterinspectingthe warningsandidentifying false alarms.
on projects that have created and maintained a findbugsfilterfile weassumethatadeveloperwouldeitherfixthebuggycodeorupdatethefilterfileafterinspectingawarning.ifso thenanopenwarningthatisnotmatchedbythefindbugsfilterfilemaynotbeafalsealarm buthasnotbeeninspectedbyadeveloper.
theseopenwarningscouldbefalsealarms buttheymayalsobe warnings that developers would act on after inspecting them.
if an open warning matches the filter then it has been confirmed by the developers to be a false alarm.
to investigate the proportion of open warnings that are confirmed to be false alarms by project developers we identified projects jmeter tomcat commons lang thathavealreadyconfigured the findbugs filter file from wang et al.
s dataset used in the preceding experiments.
next we searched github for ma ture projects that showed evidence of using findbugs and have configured afindbugsfilter file.using thegithub searchapi we looked for xml files containing the term findbugsfilter which is a keyword used in findbugs filter files in projects that were not forks filteringoutprojectswithlessthan100starsorhadlessthan lines in the findbugs filter file.
we obtained projects.
the statistics of the warnings reported by findbugs on the projects are displayed in table .
on average of the open warnings amedianof18 arematchedbythefindbugsfilterconfigured by the developers although the proportion varies for each project.
our results suggest that the majority of open warnings remain uninspected by developers.
on average only of open warnings have been explicitly in dicated by developers to be false alarms suggesting that only a minorityofopenwarningsarefalsealarms.whiletherestofthe open warnings could be false alarms they could also be actionable warnings that have not been inspected yet.table effectiveness of the golden features svm after removing unconfirmed actionable warnings and false alarms.
act.
refers to the proportion of actionable warnings.
the numbers in parentheses are the f1 of the dummy baseline which predicts that all warnings are actionable.
dataset act.
f1auc original dataset .
.
.
.
unconfirmed actionable warnings .
.
.
.
projects using findbugs .
.
.
.
unconfirmed false alarms .
.
.
.
next weinvestigatetheimpactoftheunconfirmedactionable warnings and false alarms on the golden features svm.
we hypothesize that cleaning up the data will improve its effectiveness.
to study the impact of unconfirmed actionable warnings we used the dataset of warnings from the projects by wang et al.
and yang et al.
.
these projects were the same projects studiedearlierinsection5.
.weconstructadatasetofwarnings with only the warnings confirmed by the human annotators to be actionable warnings.
we randomly sampled a subset of open warnings to retain a similar actionability ratio.
forevaluatingtheeffectofunconfirmedfalsealarms weusedthe warningsfromtheprojectsthatusedfindbugs fromsection5.
.
however weomit4projects jmeter tomcat hadoop morphia where less than of open warnings matched the filter file as thelowpercentagemayindicatethatthefindbugsfilterfilesare not kept up to date in these projects.
from the other projects only openwarningsthatmatchthefilterfileareincluded.wesampleda subset of closed warnings to retain a similar actionability ratio.
the outcome of our experiment is shown in table .
removing unconfirmed actionable warnings led to an increased auc from .54to0.
andanincreasedf1from0.39to0.
.thisoutperforms the strawman baseline which has an f1 of .
suggesting that cleaner data may increase the effectiveness of the golden features svm.however removingunconfirmedfalsealarmsdidnothelp.
the results may indicate that cleaner data may help and removing unconfirmedactionablewarnings whichistheminorityclass may have a positive effect on the effectiveness of a classifier.
answer to rq2 the closed warningheuristic may not bean appropriatewarningoracle.itlacksconsistencywithrespecttothe choice of reference revision which may affect the findings reached from the experimental results.
moreover the heuristic conflatesclosed warnings for actionable warnings and open warnings for falsealarms.wefindhavingcleanerdatabyremovingunconfirmed actionable warnings can boost the performance of the golden features svm.
discussion .
lessons learned todetectactionablewarnings thegoldenfeaturesaloneare not a silver bullet.
our results indicate that the performance of the golden features svm is not almost perfect with only marginal improvements over a strawman baseline that always predicts that authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hong jin kang khai loong aw and david lo a warning is actionable.
our study motivates the need for more work.futureworkshouldexploremorefeaturesandtechniques includingpre processingmethods e.g.smote andothermachine learning methods e.g.
semi supervised learning .
all that glitters is not gold it is essential to qualitatively analyze and understand the reasons for seemingly strong performance.
despiteachievingexcellentperformance thegolden features have subtle bugs related to data leakage and data dupli cation.
this emphasizes the importance of a deeper analysis of experimentalresults andbothquantitativeandqualitativeanalysis areessential.wecallfortheneedformorereplicationstudies as suchworkscanhighlightopportunitiesandchallengesforfuture work.ourworkreemphasizestheneedtocomparebothexisting and newly proposed techniques to simple baselines .
theclosed warningheuristicforgeneratinglabelsallows a large dataset to be built but is not enough for building abenchmark.
ourworkshedslightonthelimitationsoftheclosedwarningheuristic suggestingthatitmaynotbesufficientlyaccurate warningsmaybeclosedincidentally andactionablewarnings may stay open for years before they are closed.
as a benchmark is essential for charting research direction theconstructionofarepresentativedatasetisimportant.several studies have proposed similar processes relying on the closedwarning heuristic to build a ground truth dataset while others have relied on manual labelling .
heuristicsenablesautomation allowingforadatasetofagreater scale.
however heuristics may not be robust enough.
on the other hand solely labelling warnings through manual analysis is not scalableandmaybesubjecttoanannotator sbias.wesuggestthat datasets proposed in the future should rely on bothheuristics and manuallabelling apartfromitsgreaterscale theclosed warning heuristicenablesrichinformationtobegatheredfromtheactivities ofthedeveloperstohelpthemanuallabellingprocess.forexample code commits provide richer information such as the commit message simplifying the task for human annotators.
in contrast prior studies have relied on annotators who inspected only the source code that warnings are reported on.
our experimentssuggestusingtheclosed warningheuristic followed by manual labelling is promising the annotators had a strongagreement cohen s kappa .
while no strong agreement in manual labelling has been demonstrated in prior work.
agoodbenchmarkrequiresscaleandshouldbelabelledbymany annotators.
fieldssuch ascode clone detectionhave createdlarge benchmarks through community effort .
this motivates the need for community effort to build a benchmark for actionable warningdetectiontoo.asaderivativeofthisempiricalstudy we have labelled closed warnings usable as a starting point.
.
threats to validity apossiblethreatto internalvalidity istheincorrectimplementation of our code.
to mitigate this we reused existing data and code whenever possible including the dataset by wang et al.
and yang et al.
and the feature extractor by wang et al.
.
our code and data are available .
threatsto constructvalidity arerelatedtotheappropriateness of the evaluation metrics.
we considered the evaluation metricsusedinpriorstudies andalsocomputedf1 whichhave been used in many classification tasks .
f1 captures the tradeoffbetweenprecisionandrecall andisamoreappropriate measure on an imbalanced dataset.
threatsto externalvalidity concernthegeneralizabilityofour findings.
there are several threats to external validity including the choice of projects and techniques used in our experiments.
onethreattoexternalvalidityisthechoiceofprojectsstudiedin thispaper.westudiednineprojectsusedinpreviousstudies and we considered another set of projects that actively use findbugs.
all considered projects were large mature projects.
another threat to external validity is the choice of the approach used as an actionable warning detector.
our analysis focuses onthe use of the golden features svm which had the best median performanceinexperimentsinpriorstudiesandwasthesuggested model .
other approaches using different features may achieve stronger performance.
our analysis regarding unconfirmed actionable warnings and false alarms also relies on human oracles configuration filter files written by developers manual labelling by human annotators that may not be perfectly accurate.
moreover these oracles will produce more accurate labels for warnings that are easier to label e.g.
shorterandwell documentedcode warningtypesthatareeasierto reasonabout .this mayskewthedistributionoflabels andwarningsinthedatasets.tomitigatesomeoftheabovethreats multiple annotators labeled the warnings independently and we report the inter rater reliability.
we achieved a strong agreement cohen s kappa .
.tomitigatethethreatofunmaintainedfindbugsfilter files we selected only popular projects that have filter files with at least lines.
anotherthreatisthefocusonfindbugsand javaprojects.our analysismaynotgeneralizetowarningsofotherasats suchas infer .
this threat is mitigated as findbugs detects a wide range of bug patterns including bugs patterns shared by other asats andthefeaturesarenotlanguage specific.moreover weusedthe same dataset as prior studies .
findbugs is among the most commonly used asats having been downloaded over a million times.
related work in section we discussed the studies related to asats as well astheapproachesthatusemachinelearningtodetectactionable warnings.
we discuss other related studies in this section.
many studies have performed retrospectives of the state of theartforvarioussoftwareengineeringtasks.somepapers studythelimitationsofexistingtools andothers assesstheapplicabilityofthetoolswhenappliedtosituationswith a setting different from the original experiments.
our study not only uncovers limitations of the golden features but investigates the performance of the golden features under different settings a different warning oracle in our study .
other studies have shown the need to carefully consider data usedinexperiments .similartoallamanisetal.
we show thatdata duplication may causeoverly optimistic experimental results.
similar to kalliamvakou et al.
we suggest that researchers should be careful about interpreting automati cally mined data.
kochhar et al.
investigated multiple types authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
detecting false alarms from automatic static analysis tools how far are we?
icse may pittsburgh pa usa ofbiasthataffectdatasetsusedtoevaluatebuglocalizationtechniques and similar to our work find that prior experimentalresultswereimpactedbybiasinthedatasets.ourworkis similar to the work of tu et al.
in highlighting the problem of data leakage where information from the future is used by aclassifierandleadtooveroptimisticexperimentalresults.ouranalysis indicates that there may be delays before developers inspect staticanalysis warnings.
related to this zheng et al.
found that the statusofmanyissuesinbugzillamayonlybechangedafterlarge delays.
these delays have implications for heuristics that are used toautomaticallyinferlabelsfromhistoricaldata inourcase ifa warning is actionable .
sheppardetal.
hadpreviouslydiscusseddataqualityina commonlyuseddatasetfordefectprediction.whilebothourstudy as well as sheppard et al.
raise the problem of data duplication the duplicatedinstancesinthedatasetanalyzedinthispaperreferto thesamewarningsandlabelsoccurringinbothtrainingandtesting dataset.incontrast sheppardetal.referstoduplicatedcasesthat occurnaturally similarfeaturesbelongingtodifferentinstances e.g.
software modules .
conclusion and future work in this study we show that the problem of detecting actionable warningsfromautomaticstaticanalysistoolsisfarfromsolved.in prior work the strong performance of the golden features were contributedbydataleakageanddataduplicationissues whichwere subtle and difficult to detect.
ourstudyhighlightsthe need fordeeperstudyofthewarning oracle to determine ground truth labels.
by changing the reference revision different conclusions about performance of the golden featurescanbereached.furthermore theoracleproducelabelsthat human annotators and developers of projects using static analysis toolsmaynotagreewith.ourexperimentsshowthatthegolden features svm had improved performance on cleaner data.
ourstudyindicatesopportunitiesandchallengesforfuturework.
it highlights the need for community effort to build a large andreliable benchmark and to compare newly proposed approaches with strawman baselines.
a replication package is provided at