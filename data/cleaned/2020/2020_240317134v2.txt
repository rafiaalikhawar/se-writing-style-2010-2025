repairagent an autonomous llm based agent for program repair islem bouzenia university of stuttgart germany fibouzenia esi.dzpremkumar devanbu uc davis usa ptdevanbu ucdavis.edumichael pradel university of stuttgart germany michael binaervarianz.de abstract automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience.
this paper introduces repairagent the first work to address the program repair challenge through an autonomous agent based on a large language model llm .
unlike existing deep learning based approaches which prompt a model with a fixed prompt or in a fixed feedback loop our work treats the llm as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools.
repairagent freely interleaves gathering information about the bug gathering repair ingredients and validating fixes while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts.
key contributions that enable repairagent include a set of tools that are useful for program repair a dynamically updated prompt format that allows the llm to interact with these tools and a finite state machine that guides the agent in invoking the tools.
our evaluation on the popular defects4j dataset demonstrates repairagent s effectiveness in autonomously repairing bugs including bugs not fixed by prior techniques.
interacting with the llm imposes an average cost of tokens per bug which under the current pricing of openai s gpt .
model translates to cents per bug.
to the best of our knowledge this work is the first to present an autonomous llm based agent for program repair paving the way for future agentbased techniques in software engineering.
i. i ntroduction software bugs lead to system failures security vulnerabilities and compromised user experience.
fixing bugs is a critical task in software development but if done manually demands considerable time and effort.
automated program repair apr promises to dramatically reduce this effort by addressing the critical need for effective and efficient bug resolution in an automated manner.
researchers and practitioners have explored various approaches to address the challenge of automatically fixing bugs including techniques based on manually designed and semi automatically extracted fix patterns based on symbolic constraints and various machine learningbased approaches .the current state of the art in apr predominantly revolves around large language models llms .
the first generation of llm based repair uses a one time interaction with the model where the model receives a prompt containing the buggy code and produces a fixed version .
the second and current generation of llm based repair introduces iterative approaches which query the llm repeatedly based on feedback obtained from previous fix attempts .
a key limitation of current iterative llm based repair techniques is that their hard coded feedback loops do not allow the model to gather information about the bug or existing code that may provide ingredients to fix the bug.
instead these approaches fix the code context that is provided in the prompt typically to the buggy code and sometimes also details about the test cases that fail .
the feedback loop then executes the tests on different variants of the buggy code and adds any compilation errors test failures or other output to the prompt of the next iteration.
however this approach fundamentally differs from the way human developers fix bugs which typically involves a temporal interleaving of gathering information to understand the bug searching code that may be helpful for fixing the bug and experimenting with candidate fixes .
this paper presents repairagent the first autonomous llm based agent for automated program repair.
our approach treats the llm as an autonomous agent capable of planning and executing actions to achieve the goal of fixing a bug.
we equip the llm with a set of bug repair specific tools that the models can invoke to interact with the code base in a way similar to a human developer.
for example repairagent has tools to extract information about the bug by reading specific lines of code to gather repair ingredients by searching the code base and to propose and validate fixes by applying a patch and executing test cases.
importantly we do not hard code how and when to use these tools but instead let the llm autonomously decide which tool to invoke next based on previously gathered informationarxiv .17134v2 oct 2024and feedback from previous fix attempts.
our approach is enabled by three key components.
first a general purpose llm such as gpt .
which we query repeatedly with a dynamically updated prompt.
we contribute a novel prompt format that guides the llm through the bug repair process and that gets updated based on the commands invoked by the llm and the results of the previous command executions.
second a set of tools that the llm can invoke to interact with the code base.
we present a set of tools designed to cover different steps a human developer would take when fixing a bug such as reading specific lines of code searching the code base and applying a patch.
third a middleware that orchestrates the communication between the llm and the tools.
we present novel techniques for guiding tool invocations through a finite state machine and for heuristically interpreting possibly incorrect llm outputs.
the iterative loop of repairagent continues until the agent declares to have found a suitable fix or until exhausting a budget of iterations.
to evaluate the effectiveness of our approach we apply it to all bugs in the defects4j dataset a widely used benchmark for evaluating program repair techniques.
repairagent successfully fixes bugs including and bugs of defects4j v1.
and v2.
respectively.
the correctly fixed bugs include bugs that require fixing more than one line showing that repairagent is capable of fixing complex bugs.
compared to state of the art techniques repairagent successfully fixes bugs not fixed by prior work.
measuring the costs imposed by interacting with the llm we find that repairagent imposes an average cost of tokens per bug which under the current pricing of openai s gpt .
model translates to cents per bug.
an additional evaluation on a set of recent bugs shows that repairagent is able to achieve similar performance on single line bugs while being a bit worse on multi line and multi file bugs mainly due to a higher complexity of bugs in gitbug java dataset.
we believe from these results that repairagent is not much affected by potential data leakage of defects4j.
overall our results show that our agent based approach establishes a new state of the art in program repair.
in summary this paper contributes the following an autonomous llm based agent for program repair.
a dynamically updated prompt format that guides the llm through the bug repair process.
a set of tools that enable a llm to to perform steps a human developer would take when fixing a bug.
a middleware that orchestrates the communication between the llm and the tools.
empirical evidence that repairagent establishes a new state of the art by successfully fixing bugs including bugs not fixed by prior work.
we will release the implementation of repairagent as open source to foster future work.
to the best of our knowledge there currently is no published work on an autonomous llm based agent for any code generation task.
we envision repairagent to pave the way for future agent based techniques in software engineering.
ii.
b ackground on llm b ased autonomous agents by virtue of being trained on vast amounts of web knowledge including natural language and source code llms have demonstrated remarkable abilities in achieving human level performance for various tasks .
a promising way of using these abilities are llm based agents by which we mean llm based techniques with two properties the llm autonomously plans and executes a sequence of actions to achieve a goal as opposed to responding to a hard coded query or being queried in a hard coded algorithm.
the actions performed by the llm include invocations of external tools that enable the llm to interact with its environment .
in the context of software engineering and automated repair in particular the tools could be tools usually used by developers e.g.
as part of an integrated development environment ide .
the basic idea is to query the llm with a prompt that contains the current state of the world the goal to be achieved and a set of actions that could be performed next.
the model than decides which action to perform and the feedback from performing the action is integrated into the next prompt.
recent surveys provide a comprehensive overview of llm based autonomous agents and of llm agents equipped with tools invoked via apis .
the potential of such agents for software engineering currently is not well explored which this paper aims to address for the challenging task of automated program repair.
iii.
a pproach a. overview figure gives an overview of the repairagent approach which consists of three components an llm agent left a set of tools right and a middleware that orchestrates the communication between the two middle .
given a bug to fix the middleware initializes the llm agent with a prompt that contains task information and instructions on how to perform it by using the provided tools arrow .
the llm responds by suggesting a call to one of the available tools arrow which the middleware parses and then executes arrow .
the output of the tool arrow is then integrated into the prompt for the next invocation of the llm and the process continues iteratively until the bug is fixed or a predefined budget is exhausted.llm agent tools middleware user read code search code base run tests state and discard hypotheses write a patch invoke command command to execute next query agent with dynamic prompt raw tool outputguide agent via a state machine parse and refine llm output store and summarize tool outputrepairagent fixbugfig.
overview of repairagent.
table i sections of the dynamically updated prompt.
prompt section nature role static goals static guidelines static state description dynamic available tools dynamic gathered information dynamic specification of output format static last executed command and result dynamic b. terminology repairagent proceeds in multiple iterations or cycles definition cycle .acycle represents one round of interaction with the llm agent which consists of the following steps query the agent post process the response execute the command suggested by the agent update the dynamic prompt based on the command s output in each cycle the approach queries the llm once.
the input to the model is updated based on commands calls to tools invoked by the llm and their results in previous cycles.
we call the model input a dynamic prompt definition dynamic prompt .the dynamic prompt is a sequence of text sections p where each section siis one of the following where si c refers to a section during a cycle c astatic section which remains the same across all cycles i.e.
si c si c for all c c .
adynamic section which may differ across cycles i.e.
there may exist c c withsi c si c .
c. dynamic prompting of the repair agent the repair agent is an llm trained on natural language and source code such as gpt .
.
repairagent queries the llm with a dynamic prompt that consists of a sequence of static and dynamic sections as listed in table i. we describe each section in detail in the following.
role this section of the prompt defines the agent s area of expertise which is to resolve bugs in java code and outlines the agent s primary objective understanding and fixing bugs.
the prompt emphasizes that the agent s decision making process is autonomous and should not rely on user assistance.
goals we define five goals for the agent to pursue which remain the same across all cycles locate the bug execute tests and use fault localization techniques to pinpoint the bug s location.
skip this goal when fault localization information is already provided in the prompt.
gather information about the bug analyze the lines of code associated with the bug to understand the bug.
suggest simple fixes to the bug start by suggesting simple fixes.
suggest complex fixes if simple fixes prove ineffective explore and propose more complex ones.
iterate over the previous goals continue to gather information and to suggest fixes until finding a fix.
guidelines we provide a set of guidelines.
first we inform the model that there are diverse kinds of bugs ranging from single line issues to multi line bugs that may entail changing removing or adding lines.
based on the observation that many bugs can be fixed by relatively simple recurring fix patterns we provide a list of recurring fix patterns.
the list is based on the patterns described in prior work on single statement bugs in java .
for each pattern we provide a short natural language description and an example of buggy and fixed code.
second we instruct the model to insert comments above the modified code which serves two purposes.
on the one hand the comments allow the model to explain its reasoning which has been shown to enhance the reasoning abilities of llms .
on the other hand commenting will ultimately help human developers in understanding the nature of the edits.
third we instruct the model to conclude its reasoning with a clearly defined next step that can be translated into a call to a tool.
finally we describe that there is a limited budget of tool invocations highlighting the importance of efficiency in selecting the next steps.
specifically we specify a maximum number of cycles by default .try to fix the bugcollect information to fix the bugunderstand the bugrun tests extract tests run fault localization express hypothesissearch code base find similar api calls generate method body read range get classes and methods extract method done write fix read rangediscard hypothesisdiscard hypothesis write fixcollect more information goal accomplishedfig.
state machine to guide selection of tools.
state description to guide the llm agent toward using the available tools in an effective and meaningful way we define a finite state machine that constrains which tools are available at a given point in time.
the motivation is that we observed the llm agent to frequently get lost in aimless exploration in earlier experiments without such guidance.
figure shows the finite state machine which we design to mimic the states a human developer would go through when fixing a bug.
each state is associated with a set of tools available to the agent which are described in section iii d. importantly the agent is free to transition between states at any point in time by using tools.
that is despite providing guidance the state machine does not enforce a strict order of tool invocations.
the state description section of the prompt informs the agent about its current state understand the bug the agent starts in this state where it can collect information related to the failing test cases and the bug s location.
once the agent has an understanding of the bug it formulates a hypothesis to describe the nature of the bug and the reason behind it.
throughout the repair process the agent may refute earlier hypotheses and express new ones.
after expressing a hypothesis the agent will automatically switch to the next state.
collect information to fix the bug in this state the agent collects information that help suggest a fix for the bug expressed by the hypothesis e.g.
by searching for specific repair ingredients or by reading possibly relevant code.
once the agent has gathered enough information to attempt a fix it can transition to the next state.
try to fix the bug in this state the agent tries to fix the bug based on its current hypothesis and theinterface response thoughts string command name string args record string any fig.
json format of the response of the model.
collected information.
each fix attempt modifies the code base and is validated by executing the test cases.
if necessarily the agent can go back to one of the previous states to establish a new hypothesis or to gather additional information.
in addition to the three above states repairagent has a final state done which the agent can reach by calling a specific command that indicates the success of repair.
available tools this section of the prompt describes a set of tools that the agent can call at the current state.
each tool has a name a description and a set of typed arguments section iii d .
gathered information a key ability of the repair agent is to gather information about the bug and the code base which serves as the basis for deciding which commands to invoke next.
to make this information available to the agent we maintain a prompt section that lists the information gathered by the different tool invocations.
intuitively this section of the prompt serves as a memory for the agent allowing it to recall information from previous cycles.
the gathered information is structured into different subsections where each subsection contains the outputs produced by a specific tool.
specification of output format given the dynamic prompt the llm agent provides one response per cycle.
to enable the middleware to parse the response we specify the expected output format figure .
the thoughts field provides a textual description of the agent s reasoning when deciding about the next command.
asking the agent to express its thoughts increases the transparency and interpretability of the approach provides a way to debug potential issues in the agent s decision making process and helps improve the reasoning abilities of llms .
the command field specifies the next command to be executed consisting of the name of the tool to invoke and the set of arguments.
for example figure shows a response of the llm agent.
the model expresses the need to collect more information to understand the bug and suggests a command that searches the code base with a list of keywords.
last executed command and result this section of the prompt contains the last command tool name and arguments that was executed if any and the output it produced.
the rationale is to remind the agent of the last step it took and to make it aware of any problems thoughts with the hypothesis in mind it s time to gather more information to formulate potential fixes for the bug.
i should focus on understanding the context around the condition if x prev to come up with effective solutions.
command name search code base args key words addnumber codeconsumer if x prev fig.
example of a response of the repair agent.
that occurred during the execution of the command.
furthermore we remind the agent how many cycles have already been executed and how many cycles are left.
d. tools for the agent to use a key novelty in our approach is to let an llm agent autonomously decide which tools to invoke to fix a bug.
the tools we provide to the agent table ii are inspired by the tools that developers use in their ides.
reading and extracting code a prerequisite for fixing a bug is to read and understand relevant parts of the code base.
instead of hard coding the context provided to the llm we let the agent decide which parts of the code to read based on four tools.
the read range tool allows the agent to extract a range of lines from a specific file which is useful to obtain a focused view of a particular section of code.
to obtain an overview of the code structure thegetclasses and methods tool retrieves all class and method names within a given file.
by invoking the extract method tool the agent can retrieve the implementation s of methods that match a given method name within a given file.
finally we offer the extract tests tool which extracts the code of test cases that resulted in failure.
the tool is crucial to understand details of failing tests such as input values and the expected output.
search and generate code motivated by the fact that human developers commonly search for code we present tools that allow the agent to search for specific code snippets.
these tools are useful for the agent to better understand the context of a bug and to gather repair ingredients i.e.
code fragments that could become part of a fix.
the search code base tool enables the agent to locate instances of particular keywords within the entire code base.
for example the agent can use this tool to find occurrences of variables methods and classes.
given a set of keywords the tool performs an approximate matching against allsource code files in the project.
specifically the tool splits each keyword into subtokens based on camel case underscores and periods and then searches for each subtoken in the code.
for example searching forquicksortarray yields matches for sortarray quicksort arrayquicksort and other related variations.
the output of the tool is a nested dictionary organized by file names classes and method names that provides the keywords that match a method s content.
another search tool find similar apicalls allows the agent to identify and extract usages of a method which is useful to fix incorrect method calls.
without such a tool llms tend to hallucinate method calls that do not exist in the code base .
given a code snippet that contains a method call the tool extracts the name of the called method and then searches for calls to methods with the same name.
the agent can restrict the search to a specific file or search the entire code base.
in addition to searching for existing code repairagent offers a tool that generates new code by invoking another llm.
the tool is inspired by the success of llm based code completion tools such as copilot which human developers increasingly use when fixing bugs.
given the code preceding a method and the signature of the method the generate method body tool asks an llm to generate the body of the method.
the query to the code generating llm is independent of the dynamic prompt used by the overall repairagent approach and may use a different model.
in our evaluation we use the same llm for both the repair agent and as the codegenerating llm of this tool.
the tool limits the given code context to 12k tokens and sets a limit of 4k tokens for the generated code.
testing and patching the next category of tools is related to running tests and applying patches.
the run tests tool allows the agent to execute the test suite of the project.
it produces a report that indicates whether the tests passed or failed.
in case of test failures the tool cleans the output of the test runner e.g.
by removing entries of the stack trace that are outside of the current project.
the rationale is that llms have a limited prompt size and that irrelevant information may confuse the model.
the run fault localization tool retrieves fault localization information which is useful to understand which parts of the code are likely to contain the bug.
repairagent offers two variants of this tool either it provides perfect fault localization information or it invokes an existing fault localization tool such as gzoltar to calculate fault localization scores.
in case of perfect fault localization the tool provides all the file s and line s that need to be edited to fix the bug.
as common in the field of program repair we assume perfect fault localization as the default.
once the agent has gathered sufficient information totable ii repair related tools invoked by repairagent.
tool description read and extract code read range read a range of lines in a file.
getclasses and methods get the names of all classes and methods in a file.
extract method given a method name extract method implementations from a file.
extract tests given the failure report from junit or ant extract the code of failing test cases.
search and generate code search code base scans all java files within a project for a list of keywords.
find similar api calls given a code snippet that calls an api search for similar api calls in the project.
generate method body ask an llm gpt3.
by default to generate the body of a method based on code proceeding the method.
testing and patching run tests run the test suite of a project.
run fault localization retrieve pre existing localization information or run a fault localization tool.
write fix apply a patch to the code base and execute the test suite of the project.
changes are reverted automatically if tests fail.
moves the agent into the try to fix the bug state.
control express hypothesis express a hypothesis about the bug.
moves the agent into the collect information to fix the bug state.
collect more information move the agent back to the collect information to fix the bug state.
discard hypothesis discard the current hypothesis about the bug and move back to the understand the bug state.
goal accomplished declare that the goal has been accomplished and exiting the repair process.
file path jfree data time week.java insertions line number new lines ...new lines to insert... n ...more new lines... n deletions modifications line number modified line if dataset null n file path org jfree data time day.java insertions deletions modifications fig.
example of patch given to the write fixtool.
fix the bug it can apply a patch to the code base using the write fixtool.
repairagent aims at repairing arbitrarily complex bugs including multi line and even multi file bugs.
the write fixtool expects a patch in a specific json format which indicates the insertions deletions and modifications to be made in each file.
figure shows an example of a patch in this format.
given a patch the tool applies the changes to the code base and runsthe test suite.
if the tests fail the write fixreverts the changes giving the agent a clean code base to try another fix.
motivated by the observation that some fix attempts are almost correct the write fixtool requests the llm to sample multiple variants of the suggested fix.
by default repairagent samples variants at max.
given the generated variants the approach removes duplicates and launches tests for every variant.
control the final set of tools do not directly correspond to a tool a human developer may use but rather allow the agent to move between states figure .
the express hypothesis tool empowers the agent to articulate a hypothesis regarding the nature of the bug and to transition to the collect information to fix the bug state.
inversely the discard hypothesis tool allows the agent to discard a hypothesis that is no longer viable which leads back to the understand the bug state.
together the two commands enforce a structured approach to hypothesis formulation aligning with work on scientific debugging .
in case the agent has tried multiple fixes without success the collect more information tool allows the agent to revert to the collect information to fix the bug state.
finally once the agent has found at least one fix that passes all tests it can invoke the goal accomplished tool which terminates repairagent.
e. middleware the middleware component plays a crucial role in repairagent orchestrating the communication between the llm agent and the tools.
it performs the steps in definition as described in the following.
parsing and refining llm output at the beginning of each cycle the middleware queries the llmwith the current prompt.
ideally the response adheres perfectly to the expected format figure .
in practice the llm may produce responses that deviate from the expected format e.g.
due to hallucinations or syntactic errors.
for example the llm may provide a path argument while the tool expects a file path argument.
repairagent tries to heuristically rectify such issues by mapping the output to the expected format in three steps.
first it tries to map the tool mentioned in the response to one of the available tools.
specifically the approach checks if the predicted tool name npredicted is a substring of the name of any available tool nactual or vice versa and if yes considers nactual to be the desired tool.
in case the above matching fails the approach checks if the levenshtein distance between npredicted and any nactual is below a threshold .
by default .
second the approach tries to map the argument names provided in the response to the tool s arguments following the same logic as above.
third the approach handles invalid argument values by heuristically mapping or replacing them e.g.
by replacing a predicted file path with a valid one.
if the heuristics fail or produce multiple possible tool invocations the middleware informs the llm about the issue via the last executed command and result prompt section and enters a new cycle.
in addition to rectifying minor mistakes in the response the middleware also checks for repeated invocations of the same tool with the same arguments.
if the agent suggests the exact same command as in a previous cycle the middleware informs the agent about the repetition and enters a new cycle.
calling the tool given a valid command from the llm the middleware calls the corresponding tool.
to prevent tool executions to interfere with the host environment or repairagent itself the middleware executes the command in an isolated environment.
updating the prompt given the output of the tool the middleware updates all dynamic sections of the prompt for the next cycle.
in particular it updates the state description and the available tools appends the tool s output to the gathered information and replaces the section that shows the last executed command.
iv.
i mplementation we use python .
as our primary programming language.
docker is used to containerize and isolate command executions for enhanced reliability and reproducibility.
repairagent is built on top of the autogpt framework and gpt .
from openai.
to parse and interact with java code we use antlr.
v. e valuation to evaluate our approach we aim to answer the following research questions rq1 how effective is repairagent at fixing real world bugs?
rq2 what are the costs of the approach?
rq3 what is the influence and importance of the different components of repairagent?
rq4 how does the llm agent use the available tools?
a. experimental setup a datasets we apply repairagent to bugs in the defects4j dataset .
we use the entire defects4j dataset which consists of real world bugs from java projects including bugs from projects in defects4jv1.
as well as another bugs and projects added in defects4jv2.
evaluating on the entire dataset allows us to assess the generalization capabilities of repairagent to different projects and bugs without restricting the evaluation e.g.
based on the number of lines hunks or files that need to be fixed.
to assess the generalizability of our results and the potential influence of data leakage we also evaluate repairagent on bugs from a newer dataset gitbugjava .
all bugs in this dataset were discovered and fixed in i.e.
after the cut off date of the gpt .
version that we use in our evaluation january .
gitbug java contains bugs from projects.
due to budget constraints we randomly sample of these bugs sampling at least one and at most two bugs per project.
the random sample consists of single line bugs multi line and multi file.
b baselines we compare with three existing repair techniques chatrepair iter and selfapr .
chatrepair and iter are two very recent approaches and have been shown to be the current state of the art.
all three baseline approaches follow an iterative approach that incorporates feedback from previous patch attempts.
unlike repairagent the baselines do not use an autonomous llm based agent.
we compare against the baselines based on patches provided by the authors of the respective approaches.
c metrics of success similar to past work we report both the number of plausible and correct patches.
a fix is plausible if it passes all test cases but is not necessarily correct.
to determine whether a fix is correct we automatically check whether it syntactically matches the developer created fix.
if this is not the case we manually determine whether the repairagent generated fix is semantically consistent with the developer created fix.
if and only if either of the two checks succeeds we consider the fix to be correct .
b. rq1 effectiveness overall results table iii summarizes the effectiveness of repairagent in fixing the bugs in defects4j.
the approach generates plausible fixes fortable iii results on defects4j.
project bugs plausible correct chatrepair iter selfapr chart cli closure codec collections compress csv gson jacksoncore jacksondatabind jacksonxml jsoup jxpath lang math mockito time defects4jv1.
defects4jv2 total table iv distribution of fixes by location type bug type repairagent chatrepair iter selfapr single line multi line multi file bugs.
while not necessarily correct plausible fixes pass all test cases and may still provide developers a hint about what should be changed.
repairagent generates correct fixes for bugs where are exactly as fixed by the developers and are semantically consistent with the developer provided patches.
being able to fix bugs from different projects shows that the approach can generalize to code bases of multiple domains.
furthermore repairagent creates fixes for bugs of different levels of complexity.
specifically as shown in table iv the approach fixes single line bugs multi line single file bugs and multi file bugs.
comparison with prior work the right hand side of table iii compares repairagent with the baseline approaches chatrepair iter and selfapr.
previous to this work chatrepair had established a new state of the art in apr by fixing bugs in defects4j.
repairagent achieves a comparable record by fixing a total of bugs.
our work particularly excels in defects4jv2 where repairagent fixes bugs while chatrepair only fixes bugs.
to further compare the sets of fixed bugs figure shows the overlaps between different approaches.
as often observed in the field of apr different approaches complement each other to some extent.
in particular repairagent fixes bugs that were not fixed by any of the three baselines.
out of these bugs are single line are multi line and one is a multi file bug.
comparing the complexity of the bug fixes as shown on the right hand side of table iv fig.
intersection of the set fixes with related work.
if cfa !
null for node finallynode cfa.finallymap.get parent cfa.createedge fromnode branch.uncond finallynode cfa.createedge fromnode branch.on ex finallynode fig.
closure bug fixed by repairagent.
repairagent is particularly more effective compared to other tools for bugs that require more than a single line fix.
we attribute this result to the repairagent s ability to autonomously retrieve suitable repair ingredients and the ability to edit an arbitrary number of lines and files.
examples figure is a bug fixed exclusively by repairagent where the agent uses the find similar apicalls tool to search for calls similar tocfa.createedge fromnode branch.uncond finallynode .
it returns a call from another file which passes branch.on ex to the method call instead ofbranch.uncond .
this field name is then used as a repair ingredient by the agent.
in another example fixed only by repairagent figure repairagent benefitted from the generate method body tool to generate a missing if statement which led to suggesting a correct fix afterwards.
these examples illustrate the clever and proper usage of available tools by the agent.
they also show these tools to be useful for finding repair ingredients that previous work fails to consider.
generalization and external validity to assess the generalization capabilities of repairagent we evaluate the approach on gitbug java with results shown in table v. overall repairagent finds plausible fixes and correct fixes.
the table shows that the approach is particularly effective for single line bugs where it correctly fixes out of bugs.
in contrast the approach struggles with the multi line and multi file bugs where it finds only correct fixes.
this result can at least partially be attributed to the fact that the gitbug java dataset contains more complex bugs than defects4j.
the mean number of added and removed lines per groundseparator sep separator elementpairs.get if sep.iafterparser null sep.iafterprinter null periodformatter f toformatter elementpairs.sublist size notprinter notparser sep sep.finish f.getprinter f.getparser return new periodformatter sep sep fig.
time bug fixed by repairagent.
table v results on gitbug java bug type bugs plausible fixes correct fixes single line multi line multi file total truth bug fix are .
and .
respectively for defects4j but .
and .
for gitbug java.
likewise the mean number of modified tokens is for defects4j but for gitbug java.
we conclude that repairagent generalizes well to new projects and bugs and is not strongly affected by potential data leakage e.g defects4j .
c. rq2 costs of the approach we measure three kinds of costs imposed by repairagent i time taken to fix a bug.
ii the number of tokens consumed by queries to the llm which is relevant both for commercial models such as the gpt .
used here and for self hosted models where the number of tokens determines the computational costs.
iii the monetary costs associated with the token consumption based on openai s pricing as of march .
our findings are summarized in figure .
the median time taken to address a bug is seconds with minimal variation between fixed and unfixed bugs.
surprisingly fixed bugs do not consistently exhibit lower repair times.
this is due to repairagent s autonomous nature where the repair process continues until the goal accomplished command is invoked or the cycles budget is exhausted.
the figure shows several outliers where bug fixing attempt takes multiple hours.
repairagent spends of the total time in tool executions mostly running tests.
analyzing the costs imposed by the llm we find a median consumption of approximately tokens equating to around cents us dollars .
the number of tokens consumed by fixed bugs is clearly lower than by unfixed bugs .
this difference is because the agent continues to extract additional information for not yet fixed bugs saturating the prompt with operations such as reading more lines of code.table vi different configurations of repairagent.
project plausible correct cost sl ml mf no search tools no state machine single cycle memory realistic localization repairagent default comparison to prior work we compare the time and monetary costs of repairagent with other work based on what is reported in the respective papers.
the monetary costs of chatrepair is reported as cents per bug based on the same model gpt .
as in our work.
adjusting for the change in pricing between the two evaluations the costs of chatrepair would be cents per bug under today s pricing i.e.
about the same cost as repairagent.
the monetary costs of iter and selfapr are not reported as these approaches use self trained models.
however the authors of iter report a median bug fixing time of .
hours per bug which is much higher than the median time of seconds for repairagent.
while the comparison may be biased due to different hardware and software configurations it suggests that repairagent is more efficient in terms of time costs.
we mainly attribute this difference to the number of patches that need to be validated e.g.
average of patches generated by repairagent vs. patches generated by iter .
d. rq3 ablation study to better understand the impact of different components and configurations of repairagent we perform the ablation studies summarized in table vi.
due to budget limitations the ablations are done on a randomly selected set of bugs of the entire defects4j same for all configurations out of which the full repairagent approach fixes bugs.
we report the number of plausible and correct patches costs in us dollars and a breakdown of correct fixes into single line sl multi line ml and multi file mf bugs.
importance of search tools without the search tools repairagent fixes half of the bugs fixed by default.
the absence of search tools also causes the agent to read long sequences of code more frequently which saturates the prompt quickly and doubles the costs.
importance of state machine without guidance by the state machine figure the agent also fixes fewer bugs and has higher costs.
the main reason for the reduced effectiveness is that the agent does not follow a structured approach to fixing the bug.
for example in many cases the agent directly starts with suggesting a fix often wrong without collecting any information.
importance of long term memory the third row of the table shows a variant of repairagent that keeps a time.
b tokens money consumption.
fig.
distribution of cost metrics per bug time number of token and monetary costs .
new information for a single cycle only instead of accumulating all gathered information.
again the bug fixing effectiveness suffers significantly.
the reasons are that the agent repeats the same commands after a few cycles e.g.
to ask for the same information again and it uses wrong file names and functions names.
having a long term memory helps the agent to keep useful information for future cycles avoiding repeated queries.
impact of fault localization finally we evaluate repairagent with realistic fault localization based on the spectrum based gzoltar technique .
in total repairagent fixes bugs for dollars which is a drop in fixing capability and a increase in costs.
these results were achieved without giving repairagent more cycles which would have helped otherwise since the agent spends extra time on localizing the bug.
e. rq4 usage of tools by the agent this research question aims at better understanding the approach by analyzing how the agent uses the available tools.
on average repairagent makes tool invocations per bug which also corresponds to the number of cycles.
figure shows the frequency of tool invocations where we distinguish between fixed i.e.
correct and unfixed i.e.
plausible only or completely unfixed bugs.
the agent uses the full range of tools with the most frequently called tool being write fix average of calls for fixed bugs and calls for unfixed bugs .
around of write fixinvocations in unfixed bugs produce plausible patches compared to in fixed bugs.
the least used tool is run tests which is used so infrequently because the initially provided information about the bug already provides information about any failing test cases and because the write fixtool automatically invokes the test suite.
fig.
frequency of tool invocations average per bug .
vi.
d iscussion a. qualitative insights the following describes qualitative insights gained from inspecting repairagent s logs.
understanding the bugs repairagent s ability to actively retrieve information that helps understand a bug allowed to fix a new set of bugs without higher costs.
particularly we observe four kinds of information to be useful i the code of failing test cases and the initial execution results which we provide in the prompt of the first cycle ii code snippets retrieved by searching for similar code e.g.
using the find similar apicalls tool iii details about the code structure such as the classes and methods in a file and iv feedback obtainedby applying a fix which triggers the test execution and reveals any test cases that still fail.
unfixed bugs and fix complexity as shown in table iv repairagent clearly outperforms prior work on multi line bugs but fails to fix some of the simpler single line bugs fixed by e.g.
chatrepair .
we observe that the agent sometimes suggests complex fixes for bugs that only require simple modifications.
a possible remedy could be to initially limit the complexity of candidate fixes nudging the agent toward trying simple fixes first.
for multi line multi file bugs we observe that repairagent often edits only a subset of the required locations.
future work could explore human in the loop approaches where a partial fix found by an agent could give a developer a head start.
b. threats to validity and limitations while repairagent shows promising results we acknowledge several potential threats to validity and inherent limitations i data leakage gpt .
may have seen parts of the java projects we evaluate on during training.
our closest competitor chatrepair also uses gpt .
and thus faces the same risk.
moreover the experiment on gitbug java suggests that repairagent is effective also on bugs guaranteed to not be part of the training data.
ii missing test cases defects4j has at least one failing test case for each bug which may not be the case for real world usage scenarios.
it will be interesting to evaluate repairagent on bugs with no a priori available error revealing test cases in future work.
iii fault localization inaccurate or imprecise fault localization could lead to suboptimal repair suggestions or incorrect diagnoses.
iv non deterministic output of llms the inherently non deterministic nature of llms may result in different outcomes between two consecutive runs of repairagent.
the large number of bugs we evaluate on mitigates this risk.
moreover the logs of interactions with the llm are available for further analysis.
vii.
r elated work a non learning based program repair automated program repair has received significant attention.
some approaches address it as a search problem based on manually designed code mutation rules and fix patterns .
alternatively transformation rules can be derived semi automatically from human written patches .
other approaches use symbolic constraints to derive fixes integrate repair into a static analysis that identifies bugs or replace buggy code with similar code from the same project .
apr has been successfully deployed in industrial contexts .
beyond functional bugs several techniques target other kinds of problems such as syntax errors performance bugs vulnerabilities type errors common issues in deep learning code and build errors .
b learning based program repair while early work uses machine learning to rank and select candidate fixes more recent work uses machine learning to generate fixes.
approaches include neural machine translation models that map buggy code into fixed code models that predict tree transformations neural architectures for specific kinds of bugs and repair specific training regimes .
we refer to a recent survey for a more comprehensive discussion .
unlike the above work repairagent and the work discussed below use a general purpose llm instead of training a task specific model.
llms have motivated researchers to apply them to program repair e.g.
in studies that explore prompts and in a technique that prompts the model with error messages .
these approaches perform a one time interaction with the model where the model receives a prompt with code and produces a fix.
the most recent techniques introduce iterative approaches which query the llm repeatedly based on feedback obtained from previous fix attempts .
repairagent also queries the model multiple times but fundamentally differs by pursuing an agent based approach.
section v empirically compares repairagent to the most closely related iterative approaches .
c llms for code generation and code editing beyond program repair llms have been applied to a variety of other code generation and code editing tasks including code completion fuzzing generating and improving unit tests multi step code editing .
unlike our work none of these approaches uses an agent based approach.
d llm based agents the idea to let llm agents autonomously plan and perform complex tasks is relatively new and has been applied to tasks outside of software engineering .
to the best of our knowledge our work is the first to apply an llm based agent to program repair or any other code generation problem in software engineering.
copra is an agent based approach for formal theorem proving .
after an initial version of this paper was made publicly available other llmbased agents for software engineering tasks have been proposed showing the potential of this kind of approach.
repairagent is inspired by prior work on augmenting llms with tools invoked via apis and with the ability to generate and execute code .
our key contribution in applying these ideas to a software engineering task is to define tools that are useful for program repair and a prompt format that allows the llm to interact with these tools.viii.
c onclusion this paper presents a pioneering technique for bug repair based on an autonomous agent powered by large language models llms .
through extensive experimentation we validate the effectiveness and potential of our approach.
further exploration and refinement of autonomous agent based techniques will help generalize to more difficult and diverse types of bugs if equipped with the right tools.