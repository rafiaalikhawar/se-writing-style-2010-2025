near duplicate detectioninwebapp modelinference ra hulkrishnayandrapally university of british columbia vancouver bc canada rahulyk ece.ubc.caandrea stocco universit della svizzera italiana lugano switzerland andrea.stocco usi.chali mesbah university ofbritish columbia vancouver bc canada amesbah ece.ubc.ca abstract automatedweb testingtechniques infer modelsfroma given web app which are usedfor test generation.
from a testing viewpoint such an inferred model should contain the minimal set of states that are distinct yet adequately cover the app s main functionalities.
in practice models inferred automatically are affected by near duplicates i.e.
replicas of the same functional webpage differing only by small insignificant changes.
we present the first study of near duplicate detection algorithms used in within app model inference.
we first characterize functional near duplicates by classifying a random sample of state pairs from kpairs of webpagesobtainedfromover6 000websites intothreecategories namelyclone near duplicate anddistinct.wesystematicallycompute thresholds that define the boundaries of these categories for each detection technique.
we then use these thresholds to evaluate10near duplicatedetectiontechniquesfromthreedifferentdomains namely information retrieval web testing and computer vision on nine open source web apps.
our study highlights the challenges posedinautomaticallyinferring a modelforanygiven webapp.ourfindingsshowthatevenwiththebestthresholds no algorithmisabletoaccuratelydetectallfunctionalnear duplicates within apps withoutsacrificing coverage.
ccs concepts software and its engineering software testing and debugging.
keywords near duplicatedetection reverseengineering model basedtesting acmreference format rahulkrishna yandrapally andrea stocco and ali mesbah.
.
nearduplicate detection in web app model inference .
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
.
introduction automatedtechniquessuchaswebappcrawlersarewidelyusedto reverse engineer state basedmodelsasaviablevehicleforvarious thisworktook placewhile andrea stocco wasapost doc atubc.
permission to make digital or hard copies of all or part of this w ork for personal or classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed for profit or commercial advantage and that copies bear this notice and the full citationonthefirstpage.copyrightsforcomponents of thisworkowned byothersthan the author s must be honored.
abstracting with credit is permitted.
to copy otherwise orrepublish to post onserversorto redistributeto lists requirespriorspecific permissionand orafee.
request permissionsfrompermissions acm.org.
icse may seoul republic ofkorea copyrightheld by the owner author s .
publicationrightslicensed to acm.
acm isbn978 ... .
and testing tasks such as automatedtest generation.
the stateinsuchmodelsrepresentsthedynamicwebpageoftheapp as representedbythedocumentobjectmodel dom inthebrowser.
crawlersare capableofefficiently exploring a largestatespaceof any given web app.
however an adequate model should contain only the minimal set of distinctstates that represent the web app functionalities while discarding insignificant states that do not contributetoexposingnewfunctionalitytotheenduser.instances of such states are pages only differing by small cosmetic changes which are also referred to as near duplicates in the literature .todiscardsuchnear duplicatewebpages crawlershave adopted state abstraction functions over the dom as a proxy for the similarity of webpages.
the downside of these abstractionsis that minimal changes tothedom can resultin duplicate states in the model even if such dom changes are not reflected on the final ui visually and therefore might not be representative of a new webpage functionality.
from an end to end e2e testing perspective clone and near duplicate states in web appmodelsnegativelyimpacttheiraccuracyandcompleteness undermining thequalityof thetest suites generated from such models interms ofsize runtime and coverage.
clone and near duplicate detection acrossdifferent web apps hasbeenanactiveresearch topicinmany fields .in information retrieval the content of a webpage has been the primaryfocus becausethepurposeofwebsearchengines istoindex and retrieve information from webpages through search queries.
computer vision techniques have been employed to detect visually similar webpages for instance in phishing detection .
otherapproachesleverage stateabstractionsbasedonthesimilarity of urls textual content and the dom .
detecting near duplicate pages is a challenging problem as there is no generally accepted definition of near duplicate states and there is no unifiedstandardagainstwhichatechniquecanbeassessed .
asecondchallengepertainstotheselectionofsimilarity thresholds that such techniques need as input to determine when two pages are similar.
these thresholds are usually educated guesses as no systematic means have been proposed so far to estimate them automatically.
in this work we are interested in detecting distinct states in web app modelsinthecontext of functional e2eweb testing.
our aim is to study the nature of duplicate states occurring withina web app and provide a systematic approach to selecting thresholdsforinferringanoptimalmodel i.e.
havingthelowestnumber of near duplicatestates.tothisend weevaluatethecapabilityof near duplicate detection algorithms in identifying clone nearduplicate and distinct web app states.
we adopt techniques from three different domains information retrieval web testing and computervision wherethetextualcontent thedomtree andthe ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea r ahulkrishnayandrapally andreastocco and alimesbah visualscreenshotofthepageareusedtomeasurethesimilaritybetween states.
our goal is to assess whether textual structural or visual features are related with semantic properties of webpages and provide meaningful means to understanding their degree of functional relatedness from ane2etestingperspective.
to select the similarity thresholds for fine tuning such techniques we first crawled kwebsites randomly selected from alexa s top million urls.
we retrieved kpairs of states belonging to the same application and computed the similarity distance betweenthese pairs using each near duplicate algorithm.
we then manually classified krandom state pairs into three categories of clone near duplicate or distinct.
we used our empirical data of distances to choose thresholds for each algorithm through statistical and optimization search methods.
we evaluated their accuracy in automatically classifying clones and near duplicates in the remaining unlabelled portion of the dataset.
further we evaluated these configured algorithms on a subject set of nine unseen web apps for which manual ground truthmodelswere previouslycreated.
our work makes thefollowingnovel contributions thefirststudyof10differentnear duplicatedetectiontechniques appliedinthecontext ofweb appmodelinference.
aclassificationofdifferentcategoriesofnear duplicatesoccurringwithina web app.
systematic ways of threshold selection for near duplicate detection as well as an empirical evaluation of their effectiveness in testmodels.
thetoolsetcomprisingthe10near duplicatedetectionalgorithms which is availablefor download .
a dataset of kmanually classified pairs of webpages of which kpairsarerandomlysampledfrom6kwebsites and .
kfrom nine real size web apps.ourdataset can be used by others to conduct similar near duplicate detectionstudies and is also availablefordownloadpublicly .
our results show that even with the best thresholds no algorithm is able to accurately detect all functional near duplicates within apps.
in practice existing near duplicate detectiontechniquesare notdesigned tofindfunctionalsimilarity in a way that human testers regularly assess while testing web apps.
for certain types of near duplicates we observed that the model deteriorates over time as the crawl progresses.
for instance although rted was able to achieve a high accuracy f1 score of .
initially the final produced model had only an f1of .
.
this deterioration is due to the accumulation of numerous near duplicates to the model which decreases precision.
our results underline the need for further research in devising techniques geared specifically toward web test models i.e.
that candistinguishbetweendifferent typesofnear duplicatessuchas thosefoundin ourstudy.
redundancies in web app models in practice web testing is often performedin an end to end e2e fashion by verifying the correctness of the web app state in response to user events and interactions with the gui e.g.
clicks andformssubmissions .thistaskisperformedeithermanuallyby figure1 exampleof near duplicateweb pages.
t esters or by writing test scripts with test automation tools such as selenium .
automated techniques on the other hand generate web test cases from models that are inferred through reverse engineering techniques.
a popular method to model construction for modern web apps is automated state exploration also known as web app crawling .
such techniques dynamically analyze the web app under test by automatically firing events and checking the webpage for changes.
when new state changes are detected the model is updated to reflect the event causing the new state.
generatedmodelscanberepresented invariousformatssuchas uml statediagrams finitestatemachines fsm orstate flowgraphs sfg .
to avoid redundancies in the model states that are identical or highly similar to previously encountered states should be discarded.
for instance let us consider figure a web app in which thehomepageshowsalistofphones.whentheuserclicksonany of the phones in that list she is redirected to another web page displayingthedetailedcharacteristicsoftheselectedphone.from a functional testing viewpoint however a page containing a list of phones is conceptually the same as one listing the same phones plusoneextra phone.
theproblemofdetectingalreadyvisitedstatescanbecastasan equivalenceproblem giventwowebpagestates p1andp2explored by the crawler a state abstraction function determines whether p1 p2.moreformally definition state abstraction function .a state abstraction function saf aisapair dist t wheredistisasimilarityfunction andtis a threshold defined over the values of dist.
given two web pages p1 p2 adetermineswhetherthe distancebetween p1andp2 fallsbelow t. a dist p1 p2 t true dist p1 p2 t false otherwise in practice ais defined based on the similarity of some abstracted notion of the web pages such as their urls textual content dom structure or screenshot image.
however the amount andnatureofchanges occurringinawebpagewithrespecttothe functionality of the app is not always directly proportionalto the amount of changes in the dom tree textual content or visual aspectsof thepage.
let us consider using a crawler equipped with a saf based on dom content similarity on our sample web app of figure .
this saf is less tolerant to content textual changes occurringin web pages.
therefore each page displaying a new phone s 187near duplicate detection in web app model inference i cse may seoul republic of korea table near duplicatedetectionalgorithms includedinour study.
domain algorithm input description d istanceoutput informationretrieval w ebsearch simhash dom content bitfingerprintingtechnique which uses features extractedfrom the webpagecontenthamming distance of two64 bitdigests malware detection tlsh dom content locality sensitive bit hashing scheme that is robust to minor variations of theinputhamming distance of two256 bitdigests webtesting rted dom tree minimum costsequenceofnodeeditoperationsthattransformone dom treeinto anothertreeedit distance value normalized bythesum of nodes in thetwotrees levenshtein dom string minimum number of single character edits required to transform one string into anotheredit distance value normalized by the sum of the string lengths string equality baseline dom string string equality comparison boolean value computer vision imagehashing phash screenshot bit perceptual hash that represent the lowest frequencies of pixel brightness to which discrete cosine transform dct is applied to retrieveabrightness matrixhamming distance of two128 bitdigests block mean screenshot bit perceptual hash obtained by dividing the image into nonoverlappingblocks whichareencryptedwithasecretkeyandnormalized median valueis calculatedhamming distance of two256 bitdigests whole imagecomparison histogram screenshot color distribution of adigital image 2distance between twocolor histograms pdiff screenshot adopts a human like concept of similarity that uses spatial luminance andcolor sensitivitynumber ofdifferentpixels normalized bythemaximum number of pixels in thetwoimages structural similarity ssim screenshot simulates thehighsensitivityofhumanvisualsystemtostructural distortions while compensating for non structural distortionsnormalized structural distortion value featuredetection sift screenshot computes local feature vectors and image descriptors which are invarianttogeometric affine transformationslikescaling rotationnumber of different key points normalized bythe maximum number of key pointsin both images characteristics might be considered a different state and man y functionally similar occurrences of already modelled pages i.e.
near duplicates would be included in the model.
if we use this inflated model to generate test cases the overall functional coverage does not change when the generated tests exercise the phone details page multiple times thus potentially wasting precious testingtimeand resources.
on theother hand another better saf for instance based on thedomtreesimilaritywithaproperthresholdvalue wouldconsider all such phone detail pages as the same providing a more concise model for the web application of our example.
however ahigh thresholdvaluemightcauseotherrelevant functionalityto beabstractedawayas well resulting inanincompletemodel.
near duplicate detection techniques have been studied for reducingtheoccurrenceofredundantsimilarpages acrosswebapps e.g.
inweb search engines orphishing detection .anunderstanding of whether such techniques apply also in detecting functional near duplicates withinthe same web app is missing in theliterature.despiteitsprevalenceandimportance thisproblem isunderstudied becauseitishardtodefineanotionofequivalence for two arbitrary webpages.
moreover in the general case decidingaprioriwhichabstractionfunctionandwhichthresholdwould work best for a given web app is a challenging task as it requires substantial domain specific knowledgeof theweb appunder test.
near duplicate algorithms inthiswork westudy10near duplicatedetectionalgorithmsfrom threedifferentdomains namely informationretrieval webtesting and computer vision.
table presents the techniques along with thedomaintheybelongto theinputtypes ashortdescription and their distance output.
.
information retrieval near duplicate detection has been applied to index the massive volume of web pages continuously retrieved by web crawlers for search engines.
the overall goal is to select only a relevant set of pagesbasedontheprovidedusersearchstring.inthissetting performance is the most important factor therefore hashing mechanisms have beenadoptedduetotheir design simplicityand speed of comparison.
as an input the web page content is typically the primaryfocuswhendesigning algorithms usedin thisdomain.
we chose two content hashing algorithms from this domain simhash a popularand effective web page fingerprinting technique adopted by google to index web pages and trend locality sensitive hash tlsh a hashing technique for fingerprinting source code employed for malware detection .
.
web testing in the web testing domain researchers have studied dom based abstractions to compare webpages during the crawling of the applicationundertest.theassumptionisthattwowebpagessharing similaritiesamongtheirdomsarelikelytorepresentpageshaving analogousfunctionalities henceitisworthwhiletoconsiderthem the same.
the dom can be treated either as a tree like structure oras a simplestringofcharacters.
we chose three different similarity algorithms over the dom that have been employed as state abstraction functions in prior web testing research tree edit distance with the rtedalgorithm levenshtein distance overthestring representedbythedom and stringequalitybetweentwodom strings which weuseas baseline.
188icse may seoul republic of korea r ahulkrishnayandrapally andreastocco and alimesbah .
computer vision imagesimilarityisoneofthemaintopicsincomputervision.many techniques have been proposed and studied at different levels of granularity rangingfromlow levelpixelmatchinguptohigh level feature based matching.
thesetechniques are appliedinindexing and searching summarization object detection and tracking facial recognition and also copyright image detection.
we consider different classes of image based algorithms.
image hashing techniques map visually identical or nearly identical images to the same or similar digest called image hash.
we chose two image hashing algorithms block mean hash and perceptual hash phash which have been used in multimedia security for image retrieval authentication indexing and copy detection.
whole image matching techniques focus instead on individual pixels composing the image.
color histogram and perceptual diff pdiff have been successfully applied in previous web testing work for detecting cross browser incompatibilities .
a downside of thosetechniques is that they are affected by changes in coordinates of web elements common in responsive web layouts.structural similarity techniques quantify image quality degradation.
for instance structural similarity index ssim hasbeenshowntobeeffectiveduetothehighlystructurednature of web apps guis .
lastly feature detection techniques have been widely employed for near duplicate image detection.
for instance scale invariant feature transform sift has been appliedtoaid web test repair and phishing detection .
to the best of our knowledge this work is the first to consider and evaluate visual image similarity as a near duplicate detection techniqueforweb applicationcrawling.
empirical studydesign thegoal of ourstudyis todetermine how existing near duplicate detectiontechniques canbeemployedtoobtainanoptimalmodel of a web applicationthat canbeusedfore2etesting.
rq1 whattypeoffunctional near duplicatesexist withinapps?
rq2 how well can functional near duplicatesbedetected?
rq3 what is the impact of near duplicates and detection techniquesininferringa web app model?
first in section we randomly sample within app state pairs from a dataset created by crawling krandomly selected urls.
we characterize the changes occurring between states within an app and identify how they lead to different classes of functional near duplicates rq .
we label these pairs as either clones near duplicates or distinct states and compute the distance between them for all the ten near duplicate techniques described insection3.
in section using these labelled pairs we compute statistical andoptimalthresholdstofine tuneeachnearduplicatetechnique.
through this we aim to determine whether such randomly sampled distances from a large dataset can be used to automatically classify state pairsinunseen webappsand detectnear duplicates rq2 .
in section we determine the best near duplicate detection techniques and application specific thresholds to infer web app models for nine open source web apps covering the differentnear duplicate categories.
finally we analyze these models to determine how different kinds of near duplicates impact model inference rq .
rq near duplicates in web apps inordertodeterminewhatkinds offunctionalnear duplicates occur within apps we first create a dataset of within app state pairs andtheircalculateddistances foreachnear duplicatedetectionalgorithm.then wemanuallycharacterizethenatureofdifferences betweenpairs ofpages and classify themin a randomsample.
.
datasetcreation first we crawl randomly selected website urls from the top one million as provided by alexa 1a popular website that ranks sites based on their global popularityfor a weekusingc r.sc a.sc w.sc l.sc j.sc a.sc x.sc an event driven crawler for exploring highly dynamic web apps.
we configured c r.sc a.sc w.sc l.sc j.sc a.sc x.sc to run using the chrome browser with itsdefaultsimplestateabstractionfunction namelystringequality see table1 and a runtimelimitof fiveminutes for each crawl.
to account for network communication errors and the tool s explorationlimitations e.g.
onsitesthatrequirelogincredentials wefilteredoutsitesforwhichthecrawlmodelsobtainedcontained less than states.
after this filtering stage we retained different sites accounting for states from the original web crawls.
we then created all possible pair wise combinations of states withineach crawl which wecall state pairs .
.
.
computing distances.
we computed the distance for each state pairusingeachofthe10algorithmspresentedintable1.we discarded thestate pairs for which thedistance couldnotbecomputed correctly such as the case of dom based tree edit distance ofmalformedhtmltrees.thefinaldataset called ds contained sites and states from which state pairs with properlycomputeddistances were obtained.
.
.
distance normalization.
the raw distances which quantify the difference between two given pages have different output spaces based onthe page characteristic used by thetechnique.
as an example given a state pair of web pages pdiff outputs the number of perceptually different pixels between their screenshots whereas blockhash returns the hamming distance between image hashes.
for the sake of comprehensibility we normalized all distances computed by each algorithm as described in the distance output column of table but we never compareoutputsof different techniques.
.
classification of changes togainabetterunderstandingofwhatchanges withinwebpages characterize near duplicates we classify the differences of the state pairs in our dataset from the point of view of a human testerwho is interested in functionalitycoverage.
.
.
procedure.
manually examining state pairs is a time consumingtask requiringfamiliarity withthefunctionalityofthe application.
therefore we randomly sampled a set called rs of state pairs from our final dataset of state pairs 189near duplicate detection in web app model inference i cse may seoul republic of korea which allows us to have a confidence level of with a margin of error in deriving a representative statistic.
for each state pair pi pj s the authors of the paper visually analyzed in isolation the screenshot images and the original web pages where necessary of the two web app states from a functional testing perspective to obtain a set dof differences.
each difference in dis defined as pi pj ei ej where ei ej is a pair of non identical web elements in which ei piand ej pj.
finally each author assigned a descriptive label to each detecteddifference.
.
.
differencecategorization.
afterenumeratingalldifferences across the state pairs in rs the authors reviewed them together to resolve conflicts and reached consensus on equivalence classes ofdifferences.
ourstudyrevealed thefollowingcategories.
definition2 unrelated u .givenadifference ei ej neither ofeiorejarerelated toany functionalityoffered bytheweb app.
examples of these differences include changes in background images or gui widgets related to advertisement see red ovals in figure 2a .
definition3 duplicated d .givenadifference ei ej eiand ejreplaceeachotherintheoriginalpages piandpjwithoutadding any new functionalitytoeither page.
two distinct subcategoriesof duplicateddifferences emerged replacement d1 d1 ei ejmeaning the difference represents a functionality or content that is equivalent.
for instance in figure 2b thered ovals highlight equivalent content.
addition d2 d2 ei e i pi e i ej e i ej d1meaning the non empty exin has a duplicate e y.altin thesamepage andthereforeitsadditiondoesnotaffectthe overall functionality of the page.
for example in figure 2c theovalidentifies a duplicationofanexistingfunctionality.
definition4 new n .givenadifference ei ej represents a new functionalityora semantically different content i.e.
ei ej n ei nexistse i p1s.te i ej e i ej d .
for example the search box in figure is absent in phone descriptionpages and is anexampleof new functionality.
.
.
state pair classification.
following the classification of differences described above we classified state pairs from a functional point of view in three distinct categories defined as follows.
definition functional clone cl .given two web pages p1 andp2 the state pair p1 p2 is a functional clone cl if there are no semantic functional or perceptible differences between them defined as cl p1 p2 .
definition6 functionaldistinct di .giventwowebpages p1 andp2 p1isfunctionallydistinctfrom p2ifthereisanysemanticor functional difference between thetwopages di ei e2 n. definition functional near duplicate nd .given two web pages p1andp2 p1is a functional near duplicate of p2if the changes between the states do not change the overall functionality beingexposed nd ne ationslash cl nexists e1 e2 n .
a n ear duplicate nd backgroundimage changes b n ear duplicate nd dynamicdata c n ear duplicate nd duplicatedfunctionality figure2 different subclassesof near duplicatestate pairs.
we further observed three fine grained subclasses of near duplicates inourdataset.
cosmetic nd1 when changes related to the aesthetics of the webpage such as advertisements or background images occur which leave the functionalities unaltered see figure 2a nd1 p1 p2 e1 e2 u dynamicdata nd2 when both states of the pair are generated from the same template and populated with dynamic data according to a user query or app business logic see figure2b nd2 p1 p2 e1 e2 d1 u duplication nd3 when there are additional web elements in a pagethefunctionalityandsemantics ofcontent ofwhichis entirely represented within the other page see figure 2c nd3 e1 e2 d2 p1 p2 following these definitions we manually labelled the state pairs in rs and found clones near duplicates nd1 219nd2 11nd3 and 284distinct pairs.
rq classification ofstate pairs .
subject systems to address rq and later rq we need to infer models with different algorithms and thresholds numerous times which requires web apps withdeterministic behaviours.
190icse may seoul republic of korea r ahulkrishnayandrapally andreastocco and alimesbah table subjectsetwith manual classification bins states pairs clonesnear duplicates distinctnd2n d 3total addressbook p etclinic claroline dimeshift pagekit phoenix ppma mrbs mantisbt total to this aim we selected nine open source web apps u sed in previous research of web testing as subjects claroline v. .
.
addressbook v. .
.
ppma v. .
.
mrbs v. .
.
and mantisbt v. .
.
are open source php based applications while dimeshift commit 261166d pagekit v. .
.
phoenix v. .
.
and petclinic commit 6010d5 are web apps that cover popular javascript frameworks backbone.js vue.js phoenix react andangularjs respectively.
notethattheseninesubjectappsarenotpartofthedataset ds.
.
manualclassification ground truth we set out to create manually labelled models for each subject which wecanuseas ground truthsforcomparisonoftechniques.
first we use c r.sc a.sc w.sc l.sc j.sc a.sc x.sc to create a master crawl model with default depth first exploration strategy default state abstraction functionbasedondomstringequality andamaximumtimebudget of one hour which allow us tocapturea large portionof each app s statespace.
next we created state pairs from the states in each model as follows.
the authors of this paper manually classified each statepair into a clone near duplicate with subcategories or distinct category following the same procedure described in section .
.
in addition we also assigned each state to a binthat represents a part of the application s state space devoted to a certain functionality.
as such each bin is a logical container for all dynamically generatedconcretewebpagesuponcrawling e.g.
allwebpagesrelated tologin .
we consider the first concrete instance of a bin b to be acoverage of bby that crawl model.
additional concrete instancesofabinareconsideredclonesornear duplicatesofthebin b. table2showsthemastercrawlcharacteristicsforeachwebapp as well as our classification outcome.
in the rest of the paper we refertotheninemastercrawlswithmanuallyclassified97.
kstatepairs of the nine apps as subject set ss and to our manual classification and identified bins as ground truth .our classification of thesubject setdidnotfindanynear duplicatesofcategory nd1in ssas the subjects did not feature unrelated changes u such as advertisements commonly found in other kind of websites.
mantisbt has the most bins representing a state space five times bigger than that of phoenix which has the smallest number of bins .
addressbook pagekit and phoenix have a high numbertable averagewebpage characteristics state domand screenshot across thetwo datasets dom i m.sc a.sc g.sc e.sc tree source content pixels nodes length length dataset d s subjects ss of near duplicates of category n d3 differently from the other six.
to study how different near duplicate categories impact web app model inference we group these three subjects referring to them asnd3 apps and theother sixas nd2 apps .
table3compares thesubjectswebpage characteristicsinterms of dom size complexity and image size to ds.
for example the content of a web page in dson an average is almost eight times thatoftheweb pages in ss.
.
threshold based classification we aim to evaluate the effectiveness of the near duplicate detection algorithms in classifying a given pair as either clone nearduplicate ordistinct.essentially thisisamulti classclassification problem which we propose to solve using a classification function .
function takes as inputs a near duplicate detection algorithmfand computes the distance between two given states in a state pair p1 p2 classifying thepairtoacategoryaccording toa threshold pair tc tn as follows p1 p2 f tc tn cl f p1 p2 tc d f p1 p2 tn nd otherwise toevaluate weneed to find appropriatethreshold values for each algorithmthat maximizetheclassificationscores.
.
.
threshold determination.
we employ two different approaches namely statistical andoptimization tofind a suitable threshold pair tc tn for each algorithm.
in the statistical approach we follow a data based approach in which we use the distance distributions of different classes figure .
in the optimization approach instead we determine the thresholds that maximize the classification score on a given labelled set a commonly adopted strategy in machine learning for hyper parameters selectionofpredictive models .
definition statistical threshold pair stc stn .threshold stcis the 3rd quartile q3 of the distances calculated by a techniqueonagiven setofclonestate pairs whereas threshold stnis themediandistanceona given set of near duplicatestate pairs.
definition optimal threshold pair oc on .given a labelled set of clones near duplicates and distinct state pairs the optimal thresholds ocandonare retrieved by a bayesian optimizationsearchthatmaximizestheaverage f1classificationscore for over allthreeclasses.
figure shows the distribution of distance values among the threeclasses foreachconsideredalgorithm.asthebox plotsshow 191near duplicate detection in web app model inference i cse may seoul republic of korea figure3 normalizeddistancedistributionoflabelledpair sinthedataset ds.withineachbox plot fromlefttoright clone near duplicateanddistinctpairs.
table4 estimatedstatistical st andoptimal o thresholds for clone c andnear duplicate n bounds in dataset ds stc dsstn ds oc ds on ds tlsh .
.
.
.
l evenshtein .
.
.
.
rted .
.
.
.
simhash .
.
.
.
blockhash .
.
.
.
hyst .52e .29e .15e .49e pdiff .
.
.
.
phash .
.
.
.
sift .
.
.
.
ssim .
.
.
.
aclearseparationbetweendistancevaluesamongclasseseme rged uponstatisticalanalysis despitesomeoverlapscausedbyoutliers whichmotivatesusingthisdatatodeterminestatisticalthresholds onrs.forinstance clones left mostplotforalltechniques have low distances whereas distinct pairs have high distance scores.
near duplicates as expected lie in between those two categories forall10techniquesconsidered inourstudy.weusequartiledata for choosing thresholds since prior work has shown that the median value is a better estimator of the central tendency than mean insuch cases.
we refer to thefourthresholds stc ds stn ds oc ds on ds asuniversal thresholds as the state pairs in dsrepresent a large set of randomlyselected real worldwebpages see section5.
.
.
.
classificationaccuracy.
toaddressrq weevaluatethealgorithms by comparing the effectiveness of section .
.
with corresponding state pair inputs.
we evaluate the effectiveness of using the f1measure which is the harmonic mean of precision pr ratio of correctly classified pairs to total number of classified pairsineachclass andrecall re ratioofcorrectlyclassifiedpairs totheactualnumber ofpairs thatbelongtotheclass .
sincewehavemorethantwoclasses wetreatitasamulti class classificationproblem andobtaintheaverage f1overthescoresof allthreeclasses cl nd d .however thedatasetsareunbalanced i.e.
the ratio of state pairs of the classes are not equal hence we employ macro averaging to avoid favouring classes with higher representation .
we calculate the f1score of each algorithm using withtheuniversal thresholds seetable4 ontwodisjoint inputs a manually labelled random sample of state pairs ts fromthedataset ds and2 the .5klabelledpairsfrom ss.table5 f1measureforstatisticalandoptimalthresholdsets algorithms tatistical optimal all stc ds stn ds oc ds on ds ts ss a vgts ss avgts ss avg tlsh .
.
.
.
.
.
.
.
.
levenshtein .
.
.
.
.
.
.
.
.
rted .
.
.
.
.
.
.
.
.
simhash .
.
.
.
.
.
.
.
.
blockhash .
.
.
.
.
.
.
.
.
hyst .
.
.
.
.
.
.
.
.
pdiff .
.
.
.
.
.
.
.
.
phash .
.
.
.
.
.
.
.
.
sift .
.
.
.
.
.
.
.
.
ssim .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
r andom .
.
.
.
.
.
.
.
.
while the scores on t scan validate these thresholds scores onssassess the viability of discovering universal thresholds for anear duplicate detectionalgorithmfor unseen web apps.
.
.
findings rq .table5showsthe f1classificationscoresfor alltechniques onthetwolabelledsets tsandss.as abaseline tocomparethetechniques weusea stratified random classifier that classifies each state pair randomly based on proportions of classes inthelabelledset.
allevaluated techniquesperformbetteron tsthansswhen universal thresholds are used on average .
this result is not surprisingas tsis sampledfrom ds as well as rsfromwhich wederived thesethresholds.
ss ontheotherhand iscompletely disjoint and different from ds .
although statistical andoptimalthresholdshavesimilaroverall averagef1scores .
.
itisimportanttonoticethatoptimal thresholds perform worse than statistical thresholds on ss contrarytoexpectation.
thesetwofindingsessentiallyindicatethat thedistancethresholdsforoptimalclassification ofstate pairs canvary basedonthe characteristics of theparticularweb app.the thresholds obtained from a labelled data such as rsare therefore not necessarily applicableforarandomunseenwebapp.hence universalthresholds thatcan classify any givenstate pairmaynot befeasible .
amongst the techniques simhash has the lowest average f1 score .
onss almost worse than the random baseline.
192icse may seoul republic of korea r ahulkrishnayandrapally andreastocco and alimesbah the results concur with findings of a previous study which pointstothefactthatthealgorithmispooratdistinguishingstates that belongtothesame app.
on average five out of top six techniques belong to the computervisiondomain.
pdiffisthebestwithaclassification f1score of0.
better thanthe baseline and better than levenshtein and tlsh the best techniques in dom and ir categories respectively.
on average most visual techniques outperform dom and ir techniques with the exception of phashand color histogram .
onss pdiffagain outperforms all techniques whileblockhash andssim both visual are the only other techniques thathave an f1scoreofmorethan .
.
rq impactoninferred models with rq we evaluate the impact of the near duplicate detection algorithms inautomatedweb appmodelinference.
rq3.
howcanclassificationthresholdsbeappliedtostateabstractionfunctions safs ?
rq3.
candomainknowledgebeemployedtoimprovetheobtained models?
rq3.
how doesefficiencyof safsimpactthe obtainedmodels?
specifically weevaluatethequalityofcrawlmodelsinferredusingeachofthenear duplicatedetectionalgorithmsas stateabstract function saf seedefinition1 alongwiththedeterminedthresholds.c r.sc a.sc w.sc l.sc j.sc a.sc x.sc already includes all dom based algorithms describedinsection3.
weaddedthecomputervisionand informationretrievalnear duplicatealgorithmswithin c r.sc a.sc w.sc l.sc j.sc a.sc x.sc assafs.
morespecifically weintegratedtheimplementationsofpdiff sift and ssim from the open source computer vision library opencv and thepubliclyavailable versions oftlsh2andsimhash.
sinceweneedtorunandanalyzemanycrawlsessions i.e.
nine apps algorithms different threshold sets we limit the crawl session with a maximumruntime offive minutes.
model quality.
we measure the quality of a generated model through its f1score the harmonic mean of prandre.
lower precision pr denotes a greater redundancy inthemodeland is computed as the ratio of unique states bins covered by the model to the total number of states in the model.
recall re quantifies the applicationstatecoverage achieved in themodeland is computed asthenumberof binscovered bythemodeltothetotalnumberof binsidentifiedbyhumans forthecorrespondingapp inthe ground truth seesection6.
.
the recall reof a crawl model is highly dependent on the ability of the saf to reliably distinguish the distinct state pairs and itsprecision pronitsabilitytoexcludenear duplicatesandclones of states already present from the model.
crawlers however typically expect one single similarity threshold for deciding if a state is new to be added to the model i.e.
they do not distinguish between clone near duplicate.
therefore we frame the problem of findingoptimalthresholdsforasafasmaximizingthe f1scoreof itsdistinct pairdetection .
distinctpair pr re f1 onexisting datasets ts ss a verage pr re f 1p r re f 1pr re f on ds0.
.
.
.
.
.
.
.
.
stn ds0.
.
.
.
.
.
.
.
.
.
thresholds for safs rq .
beforeweemploythenear duplicatetechniques assafsincrawling and evaluate the generated models which is a manual and timeconsumingprocess we assess thetechniques and the universalthresholds based on the f1score of the distinct pair detection which indicates theapplicabilityof thetechniques as safs.
findings rq .
.in the distinct state pair detectionscores from rq2shownintable6 scores on tsallowustoassess theability ofatechniquetodistinguish distinctstate pairsin thewild while ssletsussimulateeach techniqueasasaf ongenerated models captured in our subject set .
in contrast to the rq 2results where boththethresholdsetshadbetteraverage classification f1onts compared to ss table shows that statistical threshold had better distinct state pair detection f1of .
on ssthan .
in ts.
optimalthreshold on ds whichishigher stricterthan stn ds in terms of actual threshold value as shown in table has a poor recallonss compared to ts .
also in tsstatistical threshold has the highest recall but by sacrificing precision the optimal threshold emerges with a better overall f1score through a25 betterprecisionon ts.thesamethreshold however could notimprove precisionin ssbuthas lower recall.
as we optimized our threshold to be stricter to fit the distribution inds we ended up misclassifying distinct pairs to be nearduplicates in ssbecause of the differences in the distributions betweenthetwodata sets.aswepointedoutinrq theseresults showtheinfeasibilityoffindinguniversalthresholdsasthedistances for state pairs are highly influenced by the intrinsic characteristics ofthe web apptheybelongto .
.
usingapplication knowledge rq .
these results for universal thresholds prompted us to investigate whetherhavingknowledgeofthewebappcharacteristicshelpsin selecting better thresholds to improve the detection rates of the techniques.
we use the manually labelled models see section .
in the subject set ss for each app to represent application knowledge.
in order to use this application knowledge we apply the near duplicate threshold definitions in definition and definition to each subject in ssto derive stn ssandon ss respectively.
in addition to these two thresholds through initial experiments we have observed that category nd3 near duplicates overlap with distinct di pairs and it is not possible to design a threshold that can distinguish them.
we therefore created a new threshold definition that sacrifices the precision of distinct pair detection by allowing misclassification ofnd3near duplicates as diforbetterrecall re .
193near duplicate detection in web app model inference i cse may seoul republic of korea table inferredmodel f1score universal app specificstn ds on ds avg stn ss stn3 ss on ss avg addressbook .
.
.
.
.
.
.
p etclinic .
.
.
.
.
.
.
claroline .
.
.
.
.
.
.
dimeshift .
.
.
.
.
.
.
pagekit .
.
.
.
.
.
.
phoenix .
.
.
.
.
.
.
ppma .
.
.
.
.
.
.
mrbs .
.
.
.
.
.
.
mantisbt .
.
.
.
.
.
.
average .
.
.
.
.
.
.
n d2 apps .
.
.
.
.
.
.
nd3 apps .
.
.
.
.
.
.
table8 inferredmodel f1f or each algorithm for selectedthresholdsthresholds apps tlsh simhash levenshtein rted blockhash phash hystpdiffsiftssimaverageallfiveall .
.
.
.
.
.
.
.
.
.
.
nd2 .
.
.
.
.
.
.
.
.
.
.
nd3 .
.
.
.
.
.
.
.
.
.
.34on ssall .
.
.
.
.
.
.
.
.
.
.
nd2 .
.
.
.
.
.
.
.
.
.
.
nd3 .
.
.
.
.
.
.
.
.
.
.40stn3 ssall .
.
.
.
.
.
.
.
.
.
.
nd2 .
.
.
.
.
.
.
.
.
.
.
nd3 .
.
.
.
.
.
.
.
.
.
.
definition .
s tn3is defined as the medianof the data distributionofmanuallylabellednear duplicates nd1 nd2 .inother words stn3isstncomputedafter excluding nd3near duplicates.
we refer to these thresholds obtained by applying application knowledgein ssforeachalgorithmas app specificthresholds .we crawled each of our subjects with two universal and three appspecific thresholds with each technique as a saf separately and assess thequalityof thegenerated models.
findings rq .
.table shows the average f1of crawls for all algorithms for each threshold.
overall as expected the universal optimal near duplicate threshold on dshas the worst score of0.
only half of the .
scored by the best threshold on ss theoptimalthreshold derived withapplicationknowledge.
onaverage app specific thresholds improve the model quality by comparedtouniversal thresholdsunderliningtheneedto consider appcharacteristicstochoosethresholds .fornd3 apps itcanbeseen thatstn3 ssderivedusingthestatisticaldefinition10significantly improves the f1score over the stn ss showing that threshold design needs to consider fine grained near duplicate categories prevalent intheapp undertest.
applicationknowledgeimproves generatedmodels.table8shows theaverage f1scores foreach algorithmfor five minute crawls on our subjects.
rtedconsistently outperforms other techniques with an f1score of .
averaged over all five thresholds.
it is better than levenshtein the next best algorithm.
the results for visual techniques in table8 are contrary to our expectation given that in rq they convincingly outperformed the dom and ir techniques in state pair classification using .
apart from being slow compared to dom based algorithms as shown in table visual techniques rely on characteristics that cannot directly capture differences corresponding to web elements e.g.
sift keypoints .
techniques such as rted which use a dom characteristic on the other hand can reliably capture differences in individual web elements between given two web pages essential to be able to classify states similar to a human tester.
in ir techniques simhash is not able to distinguish even two completely different states in our subject set as already seen in rq2.
tlsh on the other hand fails to calculate digests for app states of our subjects due to lack of enough complexity as shown intable3 thecontentinoursubjectsis1 9thofthecontentsize inthewild.therefore weexcludesimhashandtlshfromfurther analysis.
.
impact of efficiency rq .
ananalysis ofvisitedstatesperminuteor speedofthealgorithms shown in table seems to suggest that faster algorithms such as rted states per minute could explore more states in a given crawl time and improve its rewheras slower algorithms such as pdiff whichcouldonlyexplore fourstates perminuteonanaverageare ata clear disadvantage.
table shows that for all remaining eight techniques with the exceptionofsift on ssfornd2 appsand stn3 ssfornd3 apps is thebestthreshold configuration.
table shows the statistics of the min crawls for each technique with their best threshold configuration.
coverage re data suggeststhat5minuteswasnotenoughtocoveralloftheappstatespace.
therefore we experiment with a longer crawl time i.e.
minutes.giventheexponentialnatureofincreaseinmanualeffort to analyze larger crawl models we limit this experiment to the best performing techniques tuned with thresholds from the best minute crawls presented in table .we select the top fourtechniques based on f1scores however as discussed before since the slower algorithms were placed at a disadvantage in the minute crawls wealsoincludepdiffandssimthatproducedmodelswith thebestprecision pr scoresof0.91and0.
respectively12 and betterthanrtedwhich has thebest f1scoreof .
.
findings rq .
.averagef1scores shown in table for minute crawls indicate that when tuned correctly and given enough time histogram blockhash rted and levenshtein can all perform well on nd2 apps meaning that they managed to discard near duplicates of type nd2reasonably well.
however it is surprising to see that pdiff and ssim score higher than all of them on nd3 apps.
thus we decided to analyze how f1has changed over the minutes for nd3 apps as opposed to the nd2 apps.
194icse may seoul republic of korea r ahulkrishnayandrapally andreastocco and alimesbah table techniques speedandinferred model re pr f1 for best5 minutecrawlslevenshtein rted blockhash phash hyst pdiff sift ssim speed recall .
.
.
.
.
.
.
.
precision .
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
table inferredmodel f1f or minutecrawls apps blockhash hyst levenshtein pdiff rted ssim all .
.
.
.
.
.
nd2 .
.
.
.
.
.
nd3 .
.
.
.
.
.
a plot of f1o f the model over its states percentage for rted crawls is shown in figure .
the figure highlights that for nd3 apps the model deteriorates as states being added are near duplicates mostly of type nd3 while the models of nd2 apps seem to stabilize as nd2near duplicates are being detected and discarded.
during the manual analyses of models we observed that the nd3near duplicates are dynamically created typically through user interactions that result in addition removal of web elements whose functionality already exists in the state e.g.
addition deletion of new rows in a table .
not only is this newly created state a near duplicate that will eat into precious testing time but each time the crawler revisits this state it may invoke the same creation path adding even more duplicates resultingina never ending loop.
efficiency may negatively impact the generated model in timelimitedcrawlsfor nd3apps.
given that rted is the best algorithm and was fine tuned to producebestmodelforeachapplication thissurprisingrevelation points to the limitation of existing crawlers and threshold based safs and shows that threshold based crawling may never produce an accurate and complete model of modernweb apps with dynamic nd3near duplicates .wethereforethinkthatfuturesafsshouldincorporatecharacteristicsthatrepresentfunctionalityandcrawlers should bedesigned to utilizenear duplicate detectionto establish the natureof duplicationinstead of quantifying thecomputeddifferences to actively guide the exploration to discover newer functionality.
threats tovalidity externalvalidity threatsconcernthegeneralizationofourfindings.
we considered only nine web apps and experiments with other subjectsystemsarenecessarytofullyconfirmthegeneralizability of our results and corroborate our findings.
we tried to mitigate figure4 normalized f1o ver statesin model during minutecrawlsof rted this threat selecting real world web apps with different sizes pertaining to different domains and adoptedin previous web testing work .another threat concerns theselectionof thresholds for near duplicate detection techniques whose results may notgeneralize tootheralgorithms.wemitigatedthis threat byselecting techniques from three different domains web testing computervisionandinformationretrieval.
internalvalidity threats concern uncontrolled factors that may have affected our results.
a possible threat is represented by the manually created ground truth whichwasunavoidablebecausenoautomatedmethodcould provideus withtheideal classificationof web pages.
tominimize thisthreat theauthorsofthispapercreated inisolation aground truth.
then the two established a discussion to produce a single ground truthforeach web app.
for reproducibility of the results we made our tool datasets andusedsubjectsystemsavailable alongwithrequiredinstructions.
related work a large body of research has addressed the analysis of web sites structurevia clustering for clone detectionand duplicateremoval ofweb pages .
henzinger performed an evaluation of two near duplicate detectionalgorithmsbasedonshingling ona largedataset of1.6b web pages.
manku et al.
followed up on the work using simhash to detect near duplicates for web information retrieval data extraction plagiarism and spam detection with promising results.
fetterly et al.
study the evolution of near duplicate web pages over time and concludethat near duplicates have little variability over time and two pages that have been found to be near duplicates of one another will continue to be so for the foreseeablefuture.
our study is different from the above work as we aim to detectnear duplicates withinweb apps and not across different web apps.regarding detectionof within app near duplicates calefato et al.
proposea method to identify near duplicates as well as functional clone web pages based on a manual visual inspection of the gui.
crescenzi et al.
propose a structural abstraction for web pages as well as a clustering algorithm that groups web pagesbasedonthisabstraction.diluccaetal.
evaluatethe levenshtein distanceandthetagfrequencymethodsfordetecting near duplicate web pages.
eyk et al.
apply simhash and broders near duplicatedetectionwithin crawljax .
195near duplicate detection in web app model inference i cse may seoul republic of korea in mobiletesting research researchers used mobile gui widget hierarchies in order to design optimal state abstractions.
ourstudydidnotconsidersuchtechniquesastheyarenotdirectly applicableforweb applications.
tothebestofourknowledge ourworkis thefirstonetostudy different near duplication detection algorithms from different fields as safs in a web crawler.
this paper is the first to propose a systematic categorization of near duplicates in web apps from a functional e2e testing perspective and to study the impact of near duplicate detection on generated web application models and web testing.
moreover our paper is the first to discuss selection of thresholds for near duplicate detection an important firststep.
conclusions and futurework automaticallyassertingtheequalityoftwocomplexwebpagesisa difficultproblem whichthestateabstractionfunctionofacrawler needs to solve at runtime during the exploration.
the problem is further complicated by the presence of near duplicates that need tobedetectedandmappedtothelogicalpagesinordertoproduce meaningful crawlmodels.
we study ten existing near duplicate detection techniques from three different domains and compare their effectiveness as state abstraction functions in a crawler.
our results show that near duplicates characterized by dynamic data as categorized in the study are detectable when application knowledge is employed.
however near duplicates characterized by duplication of web elements that are often a by product of state exploration cannot behandled bythreshold based modelinference.
future work includes devising novel types of abstraction functions incorporating both page structural and visual characteristics in a single hybrid solution to detect different kinds of nearduplicates.