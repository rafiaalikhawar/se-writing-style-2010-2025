arxiv .05283v2 may 2025software development life cycle perspective a survey of benchmarks for code large language models and agents kaixin wang xi an jiaotong university china tianlin li nanyang technological university singapore xiaoyu zhang nanyang technological university singapore chong wang nanyang technological university singapore weisong sun nanyang technological university singapore yang liu nanyang technological university singapore bin shi xi an jiaotong university china code large language models codellms and agents have shown great promise in tackling complex software engineering tasks.
compared to traditional software engineering methods codellms and agents offer stronger abilities and can flexibly process inputs and outputs in both natural and code.
benchmarking plays a crucial role in evaluating the capabilities of codellms and agents guiding their development and deployment.
however despite their growing significance there remains a lack of comprehensive reviews of benchmarks for codellms and agents.
to bridge this gap this paper provides a comprehensive review of existing benchmarks for codellms and agents studying and analyzing benchmarks from relevant papers covering the different phases of the software development life cycle sdlc .
our findings reveal a notable imbalance in the coverage of current benchmarks with approximately focused on the software development phase in sdlc while requirements engineering and software design phases receive minimal attention at only and respectively.
additionally python emerges as the dominant programming language across the reviewed benchmarks.
finally this paper highlights the challenges of current research and proposes future directions aiming to narrow the gap between the theoretical capabilities of codellms and agents and their application in real world scenarios.
ccs concepts software and its engineering software development techniques general and reference surveys and overviews computing methodologies artificial intelligence .
additional key words and phrases code large language models software development life cycle benchmarks software engineering introduction large language models llms have ushered in a transformative era in software engineering se particularly through specialized variants known as code large language models codellms .
with the capabilities to handle tasks such as code generation translation and completion these models have outperformed traditional se methods and have been widely used in academia and industry .
for example github copilot powered by gpt has been shown to improve developer productivity significantly.
developers using copilot code faster and are more able to maintain workflow on repetitive tasks .
similarly codex a model with billion parameters achieved a .
success rate in solving complex python programming challenges outperforming traditional models.
corresponding author authors addresses kaixin wang kxwang stu.xjtu.edu.cn xi an jiaotong university xian china tianlin li tianlin001 e.ntu.edu.sg nanyang technological university singapore singapore xiaoyu zhang joshingrain gmail.com nanyang technological university singapore singapore chong wang chong.wang ntu.edu.sg nanyang technological university singapore singapore weisong sun weisong.sun ntu.edu.sg nanyang technological university singapore singapore yang liu yangliu ntu.edu.sg nanyang technological university singapore singapore bin shi shibin xjtu.edu.cn xi an jiaotong university xian china.
manuscript submitted to acm wang et al.
i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 fig.
.
taxonomy of codellm benchmarks across the sdlc.
this figure categorizes existing codellm benchmarks according to different phases of sdlc.
the numbers indicate how many benchmarks correspond to each phase or task.
entries marked with an asterisk denote benchmarks that can be used to evaluate the capabilities of codellm agents.
the blank spaces indicate that there are no relevant benchmarks for the current task.
furthermore researchers develop a series of agents that build upon codellms and se tools.
these codellm based agents demonstrate superior performance in addressing se challenges while significantly enhancing development efficiency.
for example cursor an ai powered code editor built on codellm technology integrates contextual code understanding capabilities directly within the development environment enabling developers to generate refactor and debug code through natural language instructions.
these advances highlight the growing influence of codellm and agents in research and real world applications and their potential to revolutionize the software development process.
codellms and agents possess two distinctive capabilities that are reshaping the traditional se paradigm.
first they can effectively process inputs and outputs that combine natural language nl and programming language pl manuscript submitted to acmsoftware development life cycle perspective a survey of benchmarks for code large language models and agents demonstrating exceptional performance on complex tasks such as bug fixing and code optimization.
for instance developers can leverage natural language instructions to conveniently guide code generation and debugging processes substantially reducing development effort and associated costs.
second codellms possess relatively comprehensive code comprehension and reasoning capabilities enabling them to handle diverse programming scenarios and multiple se tasks.
existing studies demonstrate that developers can efficiently accomplish more complex se tasks including generating an entire repository .
thus traditional se benchmarks are insufficient for evaluating the versatile applications of codellms and agents.
recently researchers have made notable contributions by proposing various benchmarks to more comprehensively evaluate codellms and agents in se tasks.
for example swe bench is designed to evaluate the ability of codellms and agents to automatically resolve github issues.
however despite the rapid emergence of numerous benchmarks a natural question remains are these benchmarks sufficient to evaluate the full capabilities of codellms and agents?
and what further improvements are needed in the future?
answering this question is particularly challenging as it remains unclear whether the application scenarios of codellms and agents have been fully explored let alone whether the existing benchmarks are sufficiently comprehensive.
as codellms and agents are expected to increasingly replace human effort across various se activities this paper seeks to address the question by systematically mapping existing benchmarks to specific phases of the software development life cycle sdlc as illustrated in fig.
which is commonly regarded as a comprehensive process covering as many se activities as possible.
this mapping reveals which phases have received adequate research attention and where significant gaps remain.
through analysis of current benchmark limitations and statistical benchmark usage this paper proposes challenges and potential directions for future benchmark development aimed at enabling a comprehensive evaluation of codellms and agents across different sdlc phases.
.
motivation the main motivations for this survey are as follows compared to traditional se methods codellms demonstrate superior performance on complex tasks and enhanced cross task generalization capabilities.
this fundamental advancement necessitates specialized benchmarks that exceed conventional evaluation approaches.
while traditional se benchmarks typically assess performance on isolated tasks codellm evaluation demands a comprehensive assessment across multiple tasks and progressively complex scenarios.
therefore a comprehensive study of benchmarks for codellms is highly valuable for understanding llm capabilities and accelerating the development of more sophisticated code models.
benchmarks serve as essential instruments for evaluating models and techniques across specified tasks.
highquality benchmarks establish standardized and equitable evaluation metrics facilitating comparative analysis of diverse models on identical tasks and elucidating their respective strengths and limitations.
these standardized frameworks not only guide targeted model improvements but also provide a foundational structure for industry practitioners to select and optimize appropriate solutions.
however the rapid advancement of artificial intelligence capabilities has created a significant gap as existing evaluation benchmarks struggle to keep pace with these developments.
this misalignment between evaluation approaches and model capabilities potentially undermines accurate assessment of state of the art models particularly in the se domain.
in light of this conducting comprehensive analyses of benchmarks for codellms and agents and understanding their limitations and manuscript submitted to acm4 wang et al.
table .
comparison between existing surveys and our work.
surveyobjectstudy of benchmarksdlc phases codellm agentrequirements engineeringdesignsoftware developmenttestingsoftware maintenance zhang et al.
gonzalez et al.
hou et al.
yang et al.
zheng et al.
zheng et al.
ours challenges becomes imperative for ensuring reliable assessment and appropriate application of these increasingly powerful models and agents in real world se scenarios.
although existing surveys have examined benchmarks and models on se tasks they still have limitations.
we have compared related surveys in table and observed that previous studies have either focused exclusively on traditional machine learning models without involving advanced codellms or lacked comprehensive benchmark analysis .
even recent review has studied the codellm specific benchmarks but it still limit the scope to specific tasks on the development and testing phases leaving significant gaps in analyzing benchmarks across various requirements throughout sdlc.
worse still existing reviews generally neglect the emergence of powerful codellm based agents and their corresponding evaluation benchmarks thereby failing to analyze the methodological limitations and technical challenges these advanced systems present for assessment.
given these realities it becomes imperative to establish a comprehensive survey of benchmarks for codellms and agents from the perspective of sdlc.
.
review questions this survey provides a comprehensive analysis of benchmarks for codellms across various phases of the sdlc identifies their limitations and proposes directions for future research.
specifically this survey addresses the following research questions rq1 do existing benchmarks comprehensively cover the practical requirements of the sdlc?
answered in rq2 how are current benchmarks distributed in terms of i use frequency ii release year and iii the programming languages they target?
answered in rq3 what are the key future directions in the evaluation of codellms and agents?
answered in .
collection strategy to ensure a comprehensive and systematic collection of research at the intersection of large language models llms and software engineering we implemented a rigorous approach for literature collection.
our process comprised four primary stages as illustrated in fig.
.
the details are as follows keywords summary first we conducted preliminary literature reading and summarized different keyword combinations code large language model code llms ai4se llm4se ai for se code agents manuscript submitted to acmsoftware development life cycle perspective a survey of benchmarks for code large language models and agents i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 fig.
.
research process for literature collection.
this process includes keywords summary publication search publication filtering and snowball expansion.
benchmarks programming agents benchmarks llm agents for software engineering ai for requirements engineering ai for software design ai for software development ai for software testing ai for software maintenance programming benchmarks code benchmarks code generation code understanding code completion automated programming automated testing .
publication search we performed automated literature searches using predefined keywords across four major academic databases ieee xplore acm digital library elsevier science direct and springer.
these databases provided broad coverage of core research in llms and software engineering.
in total we retrieved relevant research papers.
publication filtering we performed a manual selection of the retrieved articles based on the following criteria resulting in articles.
research on the application of llms in software engineering.
priority is given to papers accepted by influential journals e.g.
tosem tse ijcn jmlr tdsc tochi and conferences e.g.
icse ase fse acl icml ijcai issta neurips pldi sigkdd s p chi .
the selection of publications on arxiv is based on two key criteria namely the quality of the paper and the reputation of the author.
papers published after .
based on the commonly accepted definition of llms we selected literature published from onwards.
all papers that propose code related benchmarks are directly included in the scope of this survey.
snowball expansion considering that keyword based searches have the potential to overlook relevant studies we employed both forward and backward snowball sampling techniques starting from the initial literature set to include additional relevant papers.
through this process we added an additional papers ensuring the thoroughness and completeness of the research.
in the end we collected a total of papers that met the criteria.
we conducted a strict quality assessment of the included papers.
specifically each paper was independently reviewed by two co authors with backgrounds in both se and llms.
if there were differences in their assessments a third co author was involved in the discussion to reach an agreement.
this review process aimed to ensure that all the studies included in the review were of reliable quality and high relevance.
the remainder of the paper is organized as follows introduces the background of the large language models for software engineering llm4se and sdlc.
outlines the benchmarks corresponding to each phase of the sdlc.
provides a statistical analysis of the collected benchmarks.
summarizes the key findings of our survey and discusses potential future research directions and concludes this paper.
manuscript submitted to acm6 wang et al.
i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 i255 fig.
.
the phases and their core tasks within the sdlc.
background this section provides an overview of the background on sdlc and llm4se establishing the foundation for the subsequent sections.
.
llms for software engineering llm4se software engineering se is a discipline focused on the design development testing implementation and maintenance of software systems.
in recent years the emergence of artificial intelligence in software engineering ai4se has made remarkable advancements in the field .
ai4se provides intelligent solutions for software engineering tasks thereby enhancing developer productivity and software quality.
similar to the developments in the field of nlp ai4se is undergoing a historic transformation from statistical models and recurrent neural networks rnns to pre trained transformers and llms .
llms demonstrate significant potential to revolutionize software engineering practices by enhancing productivity code quality and innovation .
these models assist developers in writing code more efficiently debugging applications and comprehending complex codebases .
across various code related tasks llms have demonstrated outstanding performance including code generation vulnerability detection and program repair .
more recently researchers have developed llm based agents for se tasks that integrate planning capabilities tool usage and feedback mechanisms to autonomously complete complex development workflows .
these advancements have accelerated the rapid evolution of llm4se establishing it as one of the most dynamic research directions in the field.
to assess the capabilities of llms in code related tasks several benchmarks have been established such as humaneval and mbpp .
however existing evaluation frameworks primarily focus on traditional tasks such as code generation understanding and repair without comprehensively covering all phases of the sdlc.
the coverage of current benchmarks across the entire sdlc remains underexplored within the community and systematic studies evaluating llms capabilities across various phases of the full life cycle are lacking.
to address this gap we classify the benchmark datasets according to the distinct phases of the sdlc aiming to uncover tasks inadequately addressed in existing research and highlight overlooked code capabilities while simultaneously providing direction for future research.
.
software development life cycle sdlc in the early phases of software development where sdlc is absent the process is often disorganized and inefficient.
the lack of clear phase divisions and formal standards frequently results in a range of issues including poor project management unclear progress tracking unstable code quality and vague requirements.
with the introduction of sdlc the software development process has been significantly improved.
the sdlc offers a structured systematic approach manuscript submitted to acmsoftware development life cycle perspective a survey of benchmarks for code large language models and agents to software development with each phase having defined goals and tasks.
by segmenting the software development process into distinct phases teams can more effectively manage project progress allocate resources and control risks ultimately improving both software quality and customer satisfaction.
the sdlc provides a framework for the entire software development process as shown in fig.
.
a typical sdlc includes key phases such as requirements engineering design software development testing and software maintenance each with its specific goals and tasks.
the requirements engineering phase primarily focuses on gathering analyzing and documenting detailed requirements.
the design phase aims to translate the requirements into a software architecture blueprint covering both high level system architecture and detailed design for each submodule.
the development phase focuses on selecting appropriate programming languages and development tools based on the design documents and implementing system functionalities.
the testing phase aims to validate whether the software meets the requirements functions correctly and performs as expected while generating test and defect reports.
following testing and deployment the software enters the maintenance phase ensuring stability and performance during use alongside necessary bug fixes and updates .
research on the use of llms throughout the sdlc is vital for understanding their impact and potential in software engineering.
however the integration of llms into the sdlc is still in its early phases with current research primarily focusing on specific software development tasks.
comprehensive research covering the entire sdlc remains a significant gap in the field.
benchmark taxonomy by the sdlc to address rq1 we conduct an in depth analysis of the benchmarks across all software development life cycle phases.
we categorize these benchmarks according to their primary evaluation objectives and then analyze their distinctive characteristics strengths and limitations.
the sdlc consists of five phases requirements engineering software design software development testing and software maintenance.
in the following analysis we first identify the core tasks within each phase and then thoroughly study the relevant benchmarks.
.
requirement engineering table .
benchmarks for requirements engineering tasks.
tasks benchmarks requirements elicitation pure jain s cpsbench requirements classification promise requirements modeling ferrari s requirements specification krishna s requirement validation reinpold s requesta rsde bench requirements management the essence of requirements engineering is to achieve a precise understanding and formal definition of the requirements established by the users or stakeholders.
llms are increasingly being applied in the field of requirements engineering including tasks such as requirements gathering analysis specification and verification .
in the following subsections we provide a detailed overview of the key tasks in requirements engineering and their corresponding benchmarks as shown in table .
manuscript submitted to acm8 wang et al.
.
.
requirements elicitation.
requirements elicitation is a pivotal task in requirements engineering involving the systematic collection organization and specification of customer needs to ensure high quality product development.
this process is widely recognized as the most critical component within the requirements engineering phase .
pure contains publicly available natural language requirements documents with around sentences.
it is one of the earliest and most widely used benchmarks for requirements elicitation.
building on this jain et al.
extend the research to software engineering contracts where complex legal language makes requirement extraction more challenging.
they create a dataset of expired contracts across industries offering a new perspective for evaluating llms in domain specific contexts.
cpsbench further expands the focus to cyber physical systems cps introducing real world industrial requirements documents and tutorial cases.
each sample includes requirement descriptions entities system interactions and a problem diagram providing a structured context rich evaluation setting.
.
.
requirements classification.
requirement classification involves categorizing the requirements into functional and non functional requirements.
the promise dataset is frequently utilized for evaluating performance in requirements classification.
it comprises requirements including functional requirements and non functional requirements.
.
.
requirements modeling.
requirement modeling involves the process of transforming the requirements into formal models using modeling languages such as uml and sysml .ferrari s benchmark consists of industrial requirement documents from various application domains with requirement models manually constructed based on these documents.
this dataset can be used to evaluate the performance of llms in requirement modeling tasks.
.
.
requirements specification.
requirements specification is the process of converting the analyzed requirements into the software requirements specification which provides detailed descriptions of the system s functional performance and interface requirements.
krishna s benchmark adopts a manually created software requirements specification that adheres to ieee specifications .
this benchmark undergoes rigorous review by a team of software engineering experts and includes the problem background stakeholder information functional and non functional requirements and use case analysis.
.
.
requirement validation.
the purpose of requirements validation is to ensure that the requirements accurately reflect the stakeholders needs .
reinpold benchmark focuses on requirements validation in the smart grid domain covering system specifications and requirements descriptions.
in contrast rsde bench focuses on the fields of web and game development.
it includes coding tasks and requirement instances and utilizes unit tests for automated evaluation.
additionally requesta conducts requirements validation from the question answer qa perspective.
this dataset includes qa pairs covering common validation issues.
manuscript submitted to acmsoftware development life cycle perspective a survey of benchmarks for code large language models and agents table .
benchmarks for software design tasks.
tasks benchmarks architectural design detailed designui design rico scaprepo and screenrepo screen2words redraw algorithm design clrs database design summary requirement engineering benchmark coverage across requirement engineering tasks is still incomplete.
existing benchmarks primarily focus on requirement elicitation and validation while key areas such as modeling specification classification and especially requirement management remain sparse or entirely absent.
the challenges in constructing benchmarks for this phase include a lack of standardization and limited generalizability .
first most datasets are collected independently and are not open source which restricts reproducibility and fair comparison across models and studies.
second some benchmarks rely on specific cases or proprietary datasets which limits their generalizability and may not fully capture the diversity of real world requirements engineering scenarios.
.
software design the software design phase is the process of transforming the collected requirements into a software architecture blueprint .
this phase includes architecture design and detailed design such as user interface ui design algorithm design and database design.
the related tasks and benchmarks are shown in table .
.
.
architectural design.
architectural design is the process of creating the overall structure and organization of a software system s components interactions and relationships.
unfortunately we do not identify relevant benchmarks.
.
.
detailed design.
detailed design is the process of refining the software architecture into a more detailed representation.
this task includes designing the user interface ui algorithms and databases.
ui design.
ui design refers to the process of designing ui for software applications or websites.
current ui design benchmarks predominantly focus on the mobile development domain.
rico is the first large scale graphical user interface gui dataset specifically designed for mobile application ui analysis.
it provides resources such as screenshots view hierarchies and semantic annotations covering application domains.
in contrast scaprepo and screenrepo focus on the gui description and retrieval.
the former builds a database pairing screenshots with descriptions while the latter creates a resource library containing screenshots for gui retrieval.
screen2words emphasizes semantic interaction of ui pages rather than visual appeal.
it is the first large scale manually annotated android screen functional description dataset.
in contrast redraw focuses more on understanding components.
it collects application interface and component annotation information from non game applications on google play.
algorithm design.
algorithm design is the process of solving a problem by creating a series of steps or rules.
the clrs benchmark is designed to assess the computational reasoning abilities of llms.
the benchmark includes fundamental algorithms selected from a classic textbook covering key topics such as sorting searching dynamic programming and others.
manuscript submitted to acm10 wang et al.
database design.
database design refers to the design of efficient and secure systems for data storage and retrieval.
however no benchmark related to database design have been identified.
summary software design benchmark coverage is imbalanced across design tasks.
current benchmarks focus heavily on ui design while architectural and database design tasks lack dedicated benchmarks.
the diversity of ui design tasks is limited and there are still limitations in algorithm design.
existing algorithm design benchmarks mainly focus on specific algorithm tasks lacking cross domain applicability.
moreover ui design benchmarks are predominantly limited to mobile application domain and there is a lack of a comprehensive framework to assess the capabilities of codellms across diverse design scenarios.
.
software development software development is the core phase of sdlc where software design is translated into executable code.
in the following subsections we provide a detailed overview of the key tasks in software development and their corresponding benchmarks as summarized in table .
.
.
code generation.
the objective of code generation is to automatically generate code based on natural language descriptions.
in this field benchmarks are categorized into various levels according to the granularity of the code generation tasks such as function level class level and repository level benchmarks.
furthermore there are benchmarks focused on specific areas including competitive programming multilingual code generation domain specific tasks and others.
function level benchmarks.
function level benchmarking focuses on the implementation of individual functions making it suitable for small scale code generation task.
humaneval contains python programming problems written by humans each accompanied by an average of .
test cases.
humaneval substantially increases the number of test cases for each problem with an average of .
test cases improving the coverage of edge cases and boundary conditions.
humaneval et further introduces approximately test cases.
in contrast mbpp focuses on basic programming tasks and includes approximately problems with relatively low complexity.
mbpp extends the average number of test cases to .
resulting in a fold improvement in test rigor.
mbpp et also introduces approximately additional test cases to strengthen boundary condition evaluation.
additionally bigcodebench targets high complexity scenarios focusing on evaluating the model s ability to handle long prompts and invoke specific libraries making it an important supplement to function level benchmarks.
class level benchmarks.
class level benchmarks focus on the design of classes and the implementation of their attributes and methods specifically for object oriented programming.
classeval includes class level code generation examples used to assess the performance of llms in class level code generation tasks.
repository level benchmarks.
to simulate real world development more accurately repository level benchmarks have emerged in recent years.
early benchmarks such as conala and concode are not designed for llms and lack test cases limiting their applicability.
conala focuses on python while concode is designed for java.
codereval is the first benchmark to focus on context dependent non independent function generation scenarios.
it extracts complete task contexts from real open source projects and supports both java and python.
evocodebench addresses data leakage by introducing dynamic dataset updates.
humanevo simulates project evolution over time manuscript submitted to acmsoftware development life cycle perspective a survey of benchmarks for code large language models and agents table .
benchmarks for software development tasks.
tasks benchmarks code generationfunction levelhumaneval humaneval humaneval et mbpp mbpp mbpp et bigcodebench class level classeval repository levelconcode codereval evocodebench humanevo conala pragmaticcode fea bench deveval codeagentbench competitionsapps codecontests livecodebench leetcode codeforces multimodalmmcode plot2code spider2 v web2code design2code matplotbench babelbench multilingualhumaneval x mathqa x mbxp multipl t multilingual humaneval r benchmark rtl repo verilogeval chibench fveval multi domainds roboscript pathbench cobra robotouille epic kitchens biocoder mlagentbench ml bench deep bench othersjuice exec csn race lessleak bench domaineval studenteval text to sqlsingle table wikisql variants of spiderspider spider syn spider dk spider realistic cspider complex sql query bird kaggledbqa multi turn dialogue sparc cosql others ehrsql termite code completionrepository level repobench repoeval cross file crosscodeeval code understanding and reasoningqainfibench codeqa cs1qa cosqa cosqa dqabench reasoning cruxeval cruxeval x codemmlu reval understanding codeapex speceval faun eval code translation polyhumaneval mceval codetransocean multipl e code summarization code nn deepcom tl codesum type inference lambdanet idbench code retrieval deepcs codesearchnet non functionalrecode redcode rmcbench advbench hmcorp d c8 ecco effibench gec mercury evalperf enamel faircoder socialbias bench zhang s codearena galeras by incorporating version histories.
pragmaticcode focuses on environment consistency.
it uses java repositories to build tasks and is equipped with complete dependency configurations supporting actual compilation and execution processes.
fea bench focuses on incremental feature development.
its tasks are derived from pull requests across code repositories and aim to evaluate the model s ability to edit code in multi component codebases.
deveval is suitable for cross domain code generation.
it includes manually annotated samples from repositories.
finally codeagentbench focuses on evaluating ai agents for repository level code generation.
its data is sourced from manuscript submitted to acm12 wang et al.
open source python repositories and includes documentation dependencies installation scripts and test suites to simulate real development environments.
competitive code generation benchmarks.
several benchmarks for code generation are derived from well known competitive programming platforms.
apps contains python programming problems covering three difficulty levels beginner interview and competition.
in contrast codecontests focuses on high difficulty competitive programming tasks.
it evaluates algorithmic reasoning and data structure using problems collected from the codeforces platform.
livecodebench emphasizes reasoning and problem solving in complex algorithmic tasks.
to avoid data leakage the benchmark is continuously updated with problems from platforms such as leetcode atcoder and codeforces.
guo et al.
propose a leetcode competition benchmark that includes recent contest problems from july to january .
each problem is accompanied by test cases.
codeforces supports multiple programming languages and solution types collecting various programming problems from the codeforces platform.
multilingual benchmarks.
to support a broader range of programming language evaluations researchers have proposed several multilingual benchmarks.
humaneval x extends humaneval to five languages python c java javascript and go.
multipl t focuses on low resource programming languages.
r benchmark is the first benchmark designed for the r programming language.
recent studies also explore code generation for hardware description languages.
for verilog representative benchmarks include rtl repo verilogeval and chibench .
for systemverilog fveval broadens code generation evaluation in hardware design scenarios.
multimodal benchmarks.
multimodal large language models mllms have driven the emergence of new programming paradigms and several benchmarks have been proposed to evaluate their capabilities.
mmcode is the first benchmark focused on image to code generation covering a variety of tasks ranging from basic programming to mathematical problem solving.
plot2code focuses on the reconstruction of scientific data visualization charts and evaluates the model s ability to generate reproducible python code from images.
similarly matplotbench also focuses on data visualization techniques but it differs by handling the multimodal conversion from natural language and structured data to graphical outputs.
spider2 v focuses on more complex real world enterprise systems emphasizing mllms and agents ability to handle multiple tasks simultaneously.
web2code focuses on web development and evaluates the model s ability to generate html code from webpage screenshots.
it also introduces a qa module to assess the model s understanding of semantics.
design2code focuses on web page reconstruction evaluating the model s ability to generate html and css from web pages and ensuring visual consistency with the original page.
finally babelbench is one of the most comprehensive multimodal benchmarks focusing on evaluating the model s generalization ability and cross task adaptability.
it includes assessments of perception reasoning and code generation across image text and tabular data.
multi domain benchmarks.
as code generation extends into specialized domains benchmarks for various fields have emerged.
ds1000 focuses on the field of data science aiming to evaluate the model s performance in data analysis tasks.
biocoder focuses on bioinformatics and includes tasks such as parsing biological data formats and using bioinformatics tool apis.
mlagentbench assesses the ability of llm agents to conduct machine learning experiments including file operations code execution and result analysis.
it covers subdomains such as image classification and time series forecasting.
building on this mlbench consists of two components ml llm bench and ml agent bench.
ml llm bench evaluates the ability to generate python and bash scripts while ml agent bench measures agent performance in tasks like environment setup data acquisition and cross file information retrieval.
deepbench focuses on code generation in deep learning workflows.
it covers the full pipeline from preprocessing to inference and supports tasks such as classification regression and recommendation across different data modalities manuscript submitted to acmsoftware development life cycle perspective a survey of benchmarks for code large language models and agents including tabular data images and text.
in robotics several benchmarks including roboscript pathbench cobra robotouille and the widely used epic kitchens dataset enable evaluation of control path planning and manipulation tasks.
these benchmarks support research on code generation in embodied ai.
other benchmarks.
some benchmarks focus on specific aspects of code generation.
juice is the first to explore interactive code generation.
exec csn evaluates code executability and introduces a sandboxing process using llms to ensure that generated code can run.
race assesses code quality from four key perspectives readability maintainability correctness and runtime efficiency.
lessleak bench focuses on detecting and mitigating data leakage issues in codellm benchmarks.
domaineval focuses on cross domain generalization covering topics across six application areas.
studenteval is designed for educational use and includes basic programming prompts.
.
.
text to sql.
llms have become a key factor in driving the development of text to sql tasks.
we have classified the existing benchmarks into the following categories single table benchmarks spider and its variants complex sql benchmarks multi turn dialogue benchmarks and other related benchmarks.
single table benchmarks.
wikisql contains over tables from wikipedia.
however these queries are relatively simple primarily limited to single table queries which restricts their effectiveness in real world applications.
spider and its variants.
spider is the most widely used benchmark for text to sql tasks.
it contains natural language questions and complex sql queries spanning domains and relational databases.
despite its large scale many queries include overly specific