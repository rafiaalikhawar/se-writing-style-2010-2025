rlcoder reinforcement learning for repository level code completion yanlin wang1 yanli wang1 daya guo1 jiachi chen1 ruikai zhang2 yuchi ma2 zibin zheng1 1sun yat sen university zhuhai china wangylin36 chenjch86 zhzibin mail.sysu.edu.cn wangyli58 guody5 mail2.sysu.edu.cn 2huawei cloud computing technologies co. ltd. shenzhen china zhangruikai1 mayuchi1 huawei.com abstract repository level code completion aims to generate code for unfinished code snippets within the context of a specified repository.
existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length.
however traditional lexical based retrieval methods like bm25 struggle to capture code semantics while model based retrieval methods face challenges due to the lack of labeled data for training.
therefore we propose rlcoder a novel reinforcement learning framework which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data.
specifically we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context and provide feedback to update the retriever parameters.
this iterative process enables the retriever to learn from its successes and failures gradually improving its ability to retrieve relevant and high quality content.
considering that not all situations require information beyond code files and not all retrieved context is helpful for generation we also introduce a stop signal mechanism allowing the retriever to decide when to retrieve and which candidates to retain autonomously.
extensive experimental results demonstrate that rlcoder consistently outperforms state of the art methods on crosscodeeval and repoeval achieving .
em improvement over previous methods.
moreover experiments show that our framework can generalize across different programming languages and further improve previous methods like repocoder.
index terms repository level code completion reinforcement learning perplexity stop signal mechanism i. i ntroduction with the advancement of large language models for code code llms code completion has emerged as one of the most important features in integrated development environments ides .
however due to the vast size of code repositories and the limitations of context length in models repository level code completion which involves generating code suggestions within the context of an entire repository cannot practically leverage the entire repository directly as context .
therefore previous works typically employ a retrieval augmented generation rag strategy.
in this approach the unfinished code in the current file serves as a query to retrieve code candidates from the entire repository providing cross file context.
these candidates are then concatenated with the unfinished code before being fed into code llms.
to retrieve relevant code snippets from other files corresponding authorvarious retrievers are adopted.
repofuse uses lexicalbased method bm25 as the retriever to retrieve code snippets that are textually similar with the unfinished code.
repocoder and repohyper use the model based approach that encodes code candidates and unfinished code into vectors and employs dense retrieval to find similar codes.
although these efforts have shown promising performance in repository level code generation we have identified the following problems in retrieval.
p1 labeled data dependency.
lexical based methods such as bm25 cannot capture code semantics while model based methods are capable of understanding code semantics but are hampered by the lack of ground truth candidate data for training.
this labeled data is hard to obtain as it requires significant effort in data parsing and expert labeling limiting its generalizability.
p2 candidate construction issue.
previous methods of code candidate construction mainly employ the fixed window strategy or dependency parsing .
however the fixed window strategy may disrupt the continuity of the code.
methods based on dependency parsing can only focus on limited context in the dependency graph and can not be applied to complex scenarios.
p3 non selective retrieval.
previous studies typically directly retrieve several candidates to serve as the context for generation neglecting when to retrieve and which candidates to retain.
unnecessary candidates can detract from the performance in completion scenarios that do not require repository context.
in this paper we propose rlc oder a reinforcement learning framework for repository level code completion to address the aforementioned problems.
firstly we propose a codebase construction pipeline with a simple yet effective splitaggregate strategy.
this approach allows better code continuity of the candidates which we refer to as natural candidates addressing p2 .
secondly during the training stage we diverge from supervised learning methods that depend on labeled data.
instead we train a retriever named rlretriever that learns what to retrieve based on feedback from a specifically designed evaluator without needing labeled data addressing p1 .
specifically we iteratively evaluate the usefulness ofarxiv .19487v1 jul 2024retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context and provide feedback to update the retriever parameters which enables the retriever to learn from its successes and failures gradually improving its ability to retrieve relevant and high quality content.
moreover to mitigate hallucinations often observed in repository level code completions typically due to incorrect identifier or api usage we design a weighted perplexity ppl mechanism that allocates higher weights to certain important tokens in perplexity calculation.
furthermore considering that not all candidates retrieved are useful for generation we introduce a stop signal mechanism to evaluate the usefulness of candidates allowing the retriever to decide when to retrieve and which candidates to retain autonomously addressing p3 .
finally in the inference stage given an unfinished code as input rlcoder retrieves natural candidates from the codebase retains the useful candidates and then feeds them along with the unfinished code into the generator a backbone llm for target code generation.
we evaluate rlcoder with extensive experiments with several llms on crosscodeeval and repoeval .
experimental results show that our framework achieves .
improvement of exact match compared with previous methods.
furthermore rlcoder demonstrates high generalizability showing effectiveness across various llms and programming languages.
additionally experiments show that rlcoder can be integrated into previous methods such as repocoder to enhance code completion performance further.
our main contributions are we propose rlcoder a reinforcement learning framework for repository level code completion.
to our knowledge we are the first to train the retriever without labeled data for repository level code completion.
besides we design a mechanism that uses the weighted perplexity of the target code as the reward to further enhance performance.
we introduce a simple yet effective split aggregate candidate construction strategy based on human programming habits.
this method avoids the disruption of code continuity and outperforms fixed window candidates indicated by the experimental results.
we propose a stop signal mechanism to evaluate the usefulness of candidates and discard useless candidates for more effective code completion.
we perform an extensive evaluation of rlcoder.
experimental results show that rlcoder outperforms the state of the art methods and demonstrates generalizability and applicability.
we provide the code and data at ii.
b ackground a. retrieval augmented generation retrieval augmented generation rag is an approach that enhances the quality of generation by retrieving from external knowledge bases.
this method includes three keycomponents retriever generator and augmentation techniques .
the retriever is used to find relevant information from a large scale dataset or knowledge base including pertinent documents facts or text snippets that are relevant to the input query or prompt.
the retrieved information is fed into thegenerator which integrates this external knowledge into the generation stage.
augmentation techniques focus on how retrieved information is integrated into the generation process.
to formalize the rag process consider a scenario where we want to generate code based on a query qand a set of retrieved candidates c1 c2 ... c n .
the process can be described by the following formula code generate q retrieve q c1 c2 ... c n where the retrieve function selects the most relevant candidates based on the query qfrom the candidate set c1 c2 ... c n and the generate function then takes the query and the retrieved candidates to generate the target code.
in recent years researchers have conducted a substantial amount of research related to rag highlighting its promising potential for future applications .
many studies have utilized rag for code related research .
in repository level code completion due to the massive amount of code in the repository and limited context of generator it is impractical to use the entire repository as the context for generation.
therefore most current methods employ the rag method to retrieve suitable candidates from the repository for generation .
in regex pattern in style styles.style out style styles.style none initializes the class .converter instance.
self.
escape start len escape char if escape char is not none else escape start class int is the offset used to skip the escapecharacter.
self.
expand tuples bool expand tuples expand tuples class bool is whether to convert tuples into a sequence of parameters.
class converter object the class .converter class is the base class for implementing the conversion from one in style parameter to another out style parameter.
def init self escape char optional expand tuples bool code continuity is disrupted!candidate candidate candidate fig.
.
using fixed window candidates may disrupt the continuity of code semantics resulting in the definition of functions being split across different code snippets.
b. repository level code completion traditional code completion usually focused on generating code with in file context.
with the development of llms repository level code completion is gradually gaining attention as it better reflects real world neo4j python driver src neo4j data.py ... defkeys self t.list return the keys of the record.
returns list of key names returnlist self.
keys defvalues self keys k t.list return the values of the record optionally filtering to include only certain values by index or key.
param keys indexes or keys of the items to include if none are provided all values will be included returns list of values query unfinished code ifkeys return forkeyinkeys else returnlist self generation without retrieval ifnotkeys returnlist self.
values return forkeyinkeys neo4j python driver src neo4j sync work result.py ... def values self keys tresultkey t.list return the remainder of the result asa listof values lists.
param keys fields to returnforeach remaining record.
optionally filtering to include only certain values by index orkey.
returns listof values lists raises resultconsumederror ifthe transaction fromwhich this result was obtained has been closed orthe result has been explicitly consumed.
retrieved context generation with retrieval attribute does not exist!
fig.
.
due to the limitations of llms inappropriate retrieval can mislead generation resulting in attempts to call an non existent attribute.
scenarios .
to formalize repository level code completion we conceptualize the process as selecting the most relevant snippets candidates from a code repository and generating code based on the query.
this can be encapsulated in a formula as follows code generate q retrieve q codebase where qrepresents the query or the prompt for code completion.
codebase symbolizes code snippets from the code repository.
retrieve q codebase is the function that selects the most relevant code snippets candidates from the repository based on the query q. generate q candidates is the generation function that generates the target code based on the query q and the selected candidates.
previous work highlights the importance of integrating both the in file and cross file context in repository level code completion .
this implies that the model needs to understand not only the local context but also third party libraries and global modules .
fusing analogy context and rationale context can greatly ensure the integrity of the retrieval codebase .
iterative retrieval and generation method involves concatenating the results generated from the previous iteration with the prior context to form a query.
this query is then used for the subsequent round of retrieval and generation.
additionally agents that assist in code completion through invoking tools or collaborating with each other is also a remarkable approach.
rlretriever evaluatorf x weighted perplexity rewardunfinished code ...retrivedcodescodebase traininggithubrepos unfinished codecodebase inferencecurrentrepo generatorrlretriever ...retrivedcodestargetcode fig.
.
overview of rlcoder.
limitations there are still some issues that need to be addressed for current repository level code completion methods.
first the lack of labeled data limits the generalizability of many learning based approaches.
for example cocomic can only be used in trained repository and struggles to expand to other languages and repositories.
repohyper uses a subset of the benchmark as training data and sets the gold candidate as label.
second previous works mostly adopted fixed window candidates or candidates based on dependency parsing .
methods based on dependency parsing only consider the nodes in the dependency parse graph neglecting other code in the repository.
this can lead to omitting many potentially useful code pieces during retrieval.
methods using fixed window candidates as shown in figure may split the signature of init function into two different candidates.
this situation may lead to the retriever fetching the required code snippet without capturing the full parameter list.
as a result this partial information could confuse the generator leading to incorrect function calls.
third current work lacks an evaluation of the necessity for candidates.
as illustrated in figure the task can be correctly completed using only the in file preceding context.
however if the context retrieved is blindly used it may mislead the generation results.
in this case there is a function definition for keys in the unfinished code which calls the keys attribute.
the code snippet retrieved happens to have a function definition for values leading the model to mistakenly believe there is a corresponding values attribute defined thus calling a non existent values attribute during code generation.
iii.
m ethodology a. overview in this section we introduce rlcoder a reinforcement learning framework for repository level code completion.
the overview of rlcoder is shown in figure comprising two stages training and inference.
in the training stage the major objective is to train the retriever rlretriever the key component of our framework.
first to train rlretriever we construct data from repositories collected from github andfilter code repositoriesdependency analysis code clusters candidates target code fig.
.
data construction pipeline.
obtain unfinished code target code and candidate codebase.
then rlretriever will retrieve from the candidate codebase using unfinished code as the query.
finally the retrieved code candidates will be evaluated by the evaluator and obtain the weight perplexity reward to update the parameters of rlretriever.
through repeated iterations rlretriever enhances its retrieval capability via continuous feedback and learning.
in the inference stage given unfinished code and the current repository context we first construct codebase from current repository.
then we use the rlretriever trained in the training stage to retrieve from the codebase using the unfinished code.
finally we use the retrieved codes as context to concatenate with the unfinished code and feed them into the generator for target code generation.
b. data construction repository level code completion tasks typically refer to generating partial code within the given repository code context generally including a line an api or a part of a function body .
to ensure the retriever we train meets the requirements of repository level code retrieval we need to simulate such scenarios.
figure shows the pipeline of our data construction process which includes the following steps repository filtering dependency analysis target code selection and candidate construction.
repository selection we randomly select largescale python and java repositories from github that were created before march and meet the following requirements have cross file dependencies for constructing our training dataset and not included in well known benchmarks such as crosscodeeval and repoeval which are used in our evaluation.
this filtering process aims at preventing potential data leakage ensuring the reliability of the evaluation.
dependency analysis to ensure our training data contains a substantial quantity of cross file context dependencies we implement a dependency analysis for each repository.
the methodology is outlined in algorithm .
specifically to get the code files that are related to each other we analyze theimport statements within code files and construct a dependency graph that represents the relationships between these files.
based on whether dependencies exist between code files we categorize them into clusters of interdependent code files.
through this process we obtain python file clusters and java file clusters.
we eliminate any cluster that contains only a single file.
for clusters comprising multiple files we employ a topological sorting based on the in degree and out degree of files.
this means that aside fromalgorithm dependency analysis and clustering.
require set of code files f ensure clusters of interdependent code files clusters g constructdependencygraph f clusters identifyclusters g for all cluster inclusters do ifsize cluster then clusters clusters cluster else sortedcluster topologicalsort cluster update cluster inclusters withsortedcluster end if end for return clusters the first file each file in the cluster will contain code segments that depend on one or more of other files.
target code selection within the clusters of interdependent code files we designate files other than the first file as the ones to be completed.
we select a random position within these files excluding the beginning and end to ensure ample context for the code to be completed.
this position serves as the starting point for the target code segment that needs completion.
to formalize this process we define the target code segment to be masked and completed as ctarget starting from position pstart with length l where pstart is chosen randomly within the constraints mentioned above.
the selection of pstart can be expressed as pstart random pmin pmax where pmin andpmax define the permissible range within the file excluding the very beginning and ending segments to ensure sufficient context.
the length lof the target code ctarget is also determined randomly with the constraint that the entire segment ctarget must lie within the boundary of the code file ctarget c after identifying ctarget we mask this segment within the file to simulate an unfinished code scenario that needs completion.
upon masking ctarget the segment designated for completion we intentionally exclude the file containing the masked code when assembling candidates.
to formalize this concept we define a binary selection function for candidate files as s fi where firepresents a candidate file s fi 0ifctarget fi 1otherwise candidate construction unlike previous works that utilized fixed window candidates or candidates parsed from dependencies we propose a simple yet effective split aggregate candidate construction strategy inspired by human programming habits.
we term these candidates as natural candidates.
specifically programmers often write code with continuous semantic information together using blank lines asseparators to facilitate readability.
as shown in the left part of figure code and its corresponding comments are usually not separated by blank lines.
in fact blank lines are usually used to separate code snippets with different semantics and usage.
this practice naturally forms continuous code segments.
the splitaggregate strategy is outlined in algorithm .
specifically we divide the code in a file into several mini blocks based on blank lines and then aggregate these mini blocks into candidates by a certain length.
during aggregation the miniblocks are concatenated to form candidates in such a way that the length of any candidate does not exceed a preset threshold value t. algorithm split aggregate strategy require code file f threshold t ensure candidate set c blocks splitintoblocks f c for all block inblocks do iflinecount block t then aggregate block while linecount aggregate t andblock last blocks do block next block aggregate aggregate block end while c c createcandidate aggregate else c c splitblock block t end if end for return c c. reinforcement learning based rlretriever training design of reward for reinforcement learning reward is feedback from the external environment that assists a model in learning specific capabilities based on the feedback.
in the scenario of repository level code completion the most intuitive indicator of reward is whether the generated code can be executed to obtain the expected results.
however obtaining feedback through actual execution is difficult.
on the one hand it s challenging to set up the execution environment for repository code.
even if the execution environment is established execution can be time consuming and there may be a lack of corresponding test cases to evaluate the accuracy of the execution results.
in the context of repository level code completion our primary aim is to identify the optimal candidate cfrom a set of possibilities that maximizes the likelihood of accurately generating the target code sequence ygiven the contextual information x. this objective can be formally articulated as max cp y x c it is evident that this maximization is equivalent to minimizing the negative log likelihood min c logp y x c perplexity ppl a standard measure for evaluating the predictive performance of probabilistic models is defined as the exponential of the average negative log likelihood nll over a sequence.
minimizing nll thereby directly corresponds to minimizing the perplexity of the target code sequence y min cppl y x c e npn i 1logp yi x c y i where yirepresents the i th token in the target code sequence y. in the domain of code completion the first few tokens generated play a pivotal role in shaping the entire output.
considering this we give more attention to the first few tokens.
besides errors in repository level code completion often occur due to hallucinations caused by a lack of understanding of the entire repository such as generating incorrect or non existent apis.
therefore we assign a higher focus on the identifier tokens.
to further refine the model s focus we introduce a weighted variant ppl w ppl w y x c e 1pn i 1wipn i 1wi logp yi x c y i the weight wifor each token of the target code is determined by a function that considers the token s position in the sequence and whether it is an identifier which can be represented as wi wfirst ifi k wapi ifyi apis otherwise where the first ktokens are assigned by a weight wfirst to reflect their significant impact on the overall quality of the generated code.
if the i th token is part of an api or an identifier it is assigned a weight wapito acknowledge the importance of accurate and contextually appropriate identifiers in code completion.
we define the reward for choosing a particular candidate ci cjfrom the set of all candidates cas follows r ci 1ifppl w ci ppl w cj cj c 0otherwise where ppl w ci denotes the weighted perplexity of the target code given candidate ci serving as an abbreviation for ppl w y x ci .
the reward r ci equivalently referred to as reward ci x c in the formulations is assigned a value of if the candidate ciexhibits a ppl that is equal to or lower than that of any other candidate in the set c. conversely a reward of is allocated to ciif it fails to meet this criterion.
building on the concept of this reward mechanism we further define our objective function l as an aggregation of the logarithmic probabilities of choosing each candidate class converter object the class .converter class is the base class for implementing theconversion from one in styleparameter to another out style parameter.
def init self escape char optional expand tuples bool in regex pattern in style styles.style out style styles.style none initializes the class .converter instance.
self.
escape start len escape char if escape char is not none else escape start class int is the offse used to skip the escapecharacter.
self.
expand tuples bool expand tuples expand tuples class bool is whether toconvert tuples into asequence of parameters.
class converter object the class .converter class is the base class for implementing theconversion from one in styleparameter to another out style parameter.
def init self escape char optional expand tuples bool in regex pattern in style styles.style out style styles.style none initializes the class .converter instance.
self.
escape start len escape char if escape char is not none else escape start class int is the offse used to skip the escapecharacter.
self.
expand tuples bool expand tuples expand tuples class bool is whether toconvert tuples into asequence of parameters.
rawcodeminiblockscandidates splitaggregateclass converter object the class .converter class is the base class for implementing theconversion from one in styleparameter to another out style parameter.
def init self escape char optional expand tuples bool in regex pattern in style styles.style out style styles.style none initializes the class .converter instance.
self.
escape start len escape char if escape char is not none else escape start class int is the offse used to skip the escapecharacter.
self.
expand tuples bool expand tuples expand tuples class bool is whether toconvert tuples into asequence of parameters.
candidatesfig.
.
candidate construction strategy.
...top k candidates...stop signalretaindisgard fig.
.
illustration of the stop signal mechanism.
weighted by the corresponding reward.
this is mathematically represented as l nx i reward ci x c logp ci x c where nis the total number of candidates in set c and p ci x c denotes the probability of selecting candidate ci given the context xand the set of candidates c. stop signal mechanism for candidates selection previous works often overlook when to retrieve and which candidates to retain after retrieval.
specifically after obtaining the top kcandidates traditional methods simply truncate this list to the top icandidates determined by a predefined context length.
however this method overlooks the fact that not every retrieved candidate contributes positively to the generation process and some may even have a negative impact.
therefore discerning which candidates to retain is crucial for code completion performance.
as shown in figure we design a stop signal mechanism.
for each repository this mechanism introduces an empty candidate serving as the stop signal into the candidate codebase.
within the candidate list retrieved by rlretriever we only retain those appearing before the stop signal.
if the stop signal appears at the beginning of the list it suggests that the generation task is likely to be a task that does not require cross file context as illustrated in figure .
learning from reward as depicted in figure rlretriever is fine tuned through a dynamic learning process where it receives rewards from the evaluator to update its parameters.
this iterative learning process enables rlretriever to progressively improve its retrieval results leading to the selection of code candidates with progressively higher quality.
d. code completion with rlcoder as shown in the lower half of figure during the inference stage given unfinished code and the current repository context we first construct the candidate codebase from the current repository using the split aggregate method.
then we retrieve code candidates using the unfinished code with the trained rlretriever.
inherently the stop signal strategy mentioned in section iii c2 is embedded in the retrieved results to retain only the useful candidates.
finally the unfinished code together with these selected candidates is provided as input to the generator for code completion.
iv.
e xperimental setup a. baselines for rlcoder to evaluate the effectiveness of rlcoder we compare it with the rawrag method and repocoder framework.
besides we use a popular dense retriever unixcoder in these experiments.
rawrag refers to the standard retrieval and generation approach in the repository level code completion task.
for the unfinished code to generate rawrag uses the left context of unfinished code as the query to find the relevant code in the repository to build prompts for generation.
repocoder is the state of the art framework for repository level code completion.
it uses an iterative retrieval and generation approach to generate target code.table i benchmark statistics .
benchmark category samples avg.
lines avg.
tokens crosscodeevalpython .
.
java .
.
repoevalpython line .
.
python api .
.
for rlretriever to evaluate the effectiveness of rlretriever we compare it with the following commonly used retrieval methods in rag noretrieval stands for direct generation with unfinished code without retrieval.
bm25 calculates scores for code candidates based on the frequency of query terms in each candidate.
it adjusts for candidate length and the average candidate length across the entire database to prevent bias towards longer candidates.
unixcoder is a dense retriever that encodes both the query and the code snippets into dense vector spaces.
this encoding facilitates the identification and retrieval of semantically relevant code snippets from a large corpus based on the similarity of vector representations.
unixcoder sft is a retriever that we trained using supervised fine tuning of unixcoder.
due to the lack of labeled data we use the candidate with the lowest perplexity of target code as the label to fine tune the retriever.
b. benchmarks we evaluate rlcoder on widely used benchmarks for code completion crosscodeeval and repoeval.
crosscodeeval is a diverse and multilingual code completion benchmark and we use the python and java parts of it.
repoeval is a benchmark proposed simultaneously with repocoder .
the benchmark consists of the latest repositories that cover the line level api level and function level completion tasks.
we use the line level and api level tasks among it for evaluation.
table i shows the statistics of the benchmarks.
samples stands for the number of samples in a benchmark avg.
lines andavg.
tokens stands for the average numbers of lines and tokens of the target code snippets in the benchmark respectively.
tokens are tokenized by the tokenizer of deepseekcoder 1b.
c. evaluation metrics we measure the performance of our approach using the widely used metrics exact match em andedit similarity es .
these metrics are widely used in previous code completion studies .
em assesses the precision of code completion by checking if the generated code matches the expected code exactly.
it treats the entire code snippet as a single unit.
es measures the similarity between the epoch121416182022em language python javafig.
.
performance trajectory curve during the training process.
generated code and the expected code by calculating the edit distance.
it reflects the number of edits needed to transform the generated code into the expected code.
d. experimental details all experiments are conducted on a machine with two tesla a100 gpus each with gb memory.
in the training stage we use the parameters of unixcoder to initialize rlretriever and use deepseekcoder 1b as the evaluator.
the batch size is and the learning rate is 5e .
we train the model for epochs with samples per epoch and perform early stopping.
in the inference and evaluation stage we use five different backbone models as the generators.
v. e valuation results in this section we report and analyze the experimental results to answer the following research questions rqs rq1 how effective is rlcoder in repository level code completion?
rq2 how effective is rlretriever compared to other retrieval methods?
rq3 does each component of rlcoder contribute to its performance?
rq4 how is the generalizability of rlcoder?
a. rq1 effectiveness of rlcoder to evaluate the effectiveness of rlcoder we compare it with the rawrag framework and repocoder with five backbone llms i.e.
codellama7b startcoder 7b starcoder2 7b deepseekcoder1b and deepseekcoder 7b.
we evalaute the performance on crosscodeeval and repoeval benchmarks.
from the experimental results shown in table ii we can find that the proposed rlcoder demonstrates effectiveness on all backbone language models across the four evaluated datasets except for rlcoder deepseekcoder 7b evaluated on repoeval api where its performance is on par with its corresponding best baseline repocoder deepseekcoder 7b .
we can also observe that among all models rlcoder deepseekcoder 7b achieves thetable ii performance of different models .
the superscripts in percentage denote the improvement ratios of rlc oder over the corresponding best baseline .
modelcrosscodeeval python crosscodeeval java repoeval line repoeval api em es em es em es em es rawrag codellama 7b .
.
.
.
.
.
.
.
repocoder codellama 7b .
.
.
.
.
.
.
.
rlcoder codellama 7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rawrag starcoder 7b .
.
.
.
.
.
.
.
repocoder starcoder 7b .
.
.
.
.
.
.
.
rlcoder starcoder 7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rawrag starcoder2 7b .
.
.
.
.
.
.
.
repocoder starcoder2 7b .
.
.
.
.
.
.
.
rlcoder starcoder2 7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rawrag deepseekcoder 1b .
.
.
.
.
.
.
.
repocoder deepseekcoder 1b .
.
.
.
.
.
.
.
rlcoder deepseekcoder 1b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rawrag deepseekcoder 7b .
.
.
.
.
.
.
.
repocoder deepseekcoder 7b .
.
.
.
.
.
.
.
rlcoder deepseekcoder 7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
best performance with em score of .
improving its corresponding best baseline repocoder deepseekcoder 7b by .
on crosscodeeval python and .
on repoeval line.
furthermore to investigate the efficacy of our training process we plot the performance trajectory curve across the training epochs on crosscodeeval as illustrated in figure .
the result shows that the em score gradually increases with each epoch until stabilizing indicating the effectiveness of our training process.
rq1 summary our approach significantly outperforms current state of the art methods for all backbone llms improving the crosscodeeval benchmark by .
and repoeval .
.
the performance trajectory further demonstrates the efficacy of our training process.
b. rq2 effectiveness of rlretriever to assess the effectiveness of rlretriever the key module of rlcoder we conduct a comparative study.
we evaluate rlcoder equipped with different retrieval methods described in section iv a including noretrieval bm25 unixcoder and our enhanced model unixcoder sft.
table iii shows the experimental results on the crosscodeeval and repoeval benchmarks.
the results yield the following findings our proposed rlretriever consistently outperforms comparative baseline methods under all metrics in both benchmarks underscoring its superior performance.
all retrieval based methods i.e.
bm25 unixcoder unixcoder sft and rlretriever perform better than noretrieval showing the inherent value of the retrieval process itself.
both unixcoder sft and rlretriever show better performance than unixcoder indicating that retrieval training can enhance the performance.
notably our reinforcement learning based training method exhibits better performance over supervised fine tuning.rq2 summary rlretriever consistently outperforms other retrieval methods.
furthermore the results affirms the significance of the retrieval and training processes particularly highlighting the advantages of our reinforcement learning based training approach.
c. rq3 contributions of each component to understand the contributions of each component to rlcoder we conduct an ablation study on rlcoder.
specifically we remove each component of rlcoder each time and study the performance of the ablated model.
the experimental results are shown in table iv.
w o rl means using retriever without reinforcement learning.
w o wp means utilizing unweighted perplexity of the target code as the reward.
w o nc means using fixed window candidates instead of our natural candidates w o ss means using retriever without the stop signal mechanism.
from table iv we can see that the performance of the model drops after removing any one component indicating that each component contributes to the effectiveness of rlcoder.
especially the performance drops the most significantly for rlcoder w o rl indicating that the reinforcement learning mechanism is the most important component in rlcoder.
we observe that unweighted perplexity and no stop mechanism can be beneficial to es performance in some cases but still harm em performance.
in fact es mainly considers the similarity between two pieces of code.
the introduction of the stop signal and weighted perplexity can both affect code similarity.
the stop signal reduces useless but similar candidates while weighted perplexity emphasizes api tokens more rather than all tokens.
this can potentially reduce the similarity between generated and target code.
although these strategies weaken similarity they improve code correctness.
so removing the stop signal and weighted perplexity decreases in em across all benchmarks.table iii experimental results of rlc oder equipped with different retrieval methods .
the backbone llm used is deepseekcoder 7b.
t he superscripts in percentage denote the improvement ratios of our retrieval model rlr etriever over the corresponding best baseline retriever .
retrieval methodcrosscodeeval python crosscodeeval java repoeval line repoeval api em es em es em es em es noretrieval .
.
.
.
.
.
.
.
bm25 .
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
unixcoder sft .
.
.
.
.
.
.
.
rlretreiver .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iv ablation study results on cross codeeval and repoeval.
modelcrosscodeeval python crosscodeeval java repoeval line repoeval api em es em es em es em es rlcoder .
.
.
.
.
.
.
.
w o rl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o wp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o nc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o ss .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table v additional ablation study for the stop signal mechanism .
note that contrary to em and es lower ppl scores correspond to better performance .
model em es ppl rawrag codellama 7b .
.
.
rlcoder codellama 7b .
.
.
w o stop signal .
.
.
.
.
.
rawrag starcoder 7b .
.
.
rlcoder starcoder 7b .
.
.
w o stop signal .
.
.
.
.
.
rawrag starcoder2 7b .
.
.
rlcoder starcoder2 7b .
.
.
w o stop signal .
.
.
.
.
.
rawrag deepseekcoder 1b .
.
.
rlcoder deepseekcoder 1b .
.
.
w o stop signal .
.
.
.
.
.
rawrag deepseekcoder 7b .
.
.
rlcoder deepseekcoder 7b .
.
.
w o stop signal .
.
.
.
.
.
since crosscodeeval and repoeval are specifically curated to evaluate code completion ability in scenarios requiring cross file context the improvement brought by the stop signal is expected to be minor.
to further evaluate the practical effectiveness of the stop signal mechanism we construct a new dataset githubeval that construct code completion targets at random positions within repositories thus incorporating instances that may not necessitate cross file context.
following the same construction procedure as the training dataset we obtain samples for evaluation.
the results shown in table v indicate that the stop signal is an important component in rlcoder without which the em scores drop significantly by an average of .
.rq3 summary reinforcement learning mechanism is the most important component in rlcoder.
other components of rlcoder also contributes to its superior performance with the stop signal mechanism showing further enhancements in scenarios involving target code completion that both require and do not require cross file context.
d. rq4 generalizability of rlcoder to explore the generalizability of rlcoder we conduct an evaluation with a setting different from the training stage.
specifically we train a new retriever by fusing rlcoder and repocoder.
then we evaluate the performance of the fused model.
as shown in table vi we find that repocoder trained using the framework of rlcoder significantly outperforms the original repocoder method.
specifically the improvement rates in em for python and java are .
and .
on crosscodeeval respectively.
this result indicates that our training framework can be integrated into other models to further improve their performance.
note that when comparing repocoder w rlcoder to rlcoder they have comparable performance with repocoder w rlcoder slightly better .
however considering that repocoder requires multiple iterations of retrieval and generation we opt for rlcoder which accomplishes code completion in a single round as the default setting in this work.
rq4 summary the training pipeline of rlcoder shows generalizability on all datasets in applying to other frameworks.
e. case study we illustrate the effectiveness of rlcoder through a case study presented in figure .
the left hand side shows the incomplete code groundtruth code and the gold candidatetable vi experimental results of repocoder integrated with rlc oder .
methodcrosscodeeval python crosscodeeval java repoeval line repoeval api em es em es em es em es rawrag .
.
.
.
.
.
.
.
rlcoder .
.
.
.
.
.
.
.
repocoder .
.
.
.
.
.
.
.
w rlcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
similarity filter.pyfrom typing import listfrom common.constants import constantsfrom common.utils import utilsrate 2class similarityfilter def init self detect data list algorithm type str anomaly duration int self.algorithm type algorithm type self.detect data self.minus data detect data self.anomaly duration anomaly duration def run self check if the current data is similar to the historical data.
return true if the current data is similar to the historical data.
agg list utils.
utils.py staticmethoddef agg diff fe calc input data list agg length int list ... diff func callable none lambda a b np.sum a np.sum b diff for i in range len input data agg length post input data pre input data diff.append diff func post pre return diffgold candidateaggregate data self.detect data self.anomaly duration rawragunixcoderagg diff fe calc self.detect data constants.agg length rawragunixcoder sftget agg list self.detect data self.anomaly duration agg diff fe calc self.detect data self.anomaly duration rlcoderquery incompletecode code to generate diff outlier detector.pyclass diffoutlierdetector def init self detect data list algorithm type str self.algorithm type algorithm type self.detect data self.minus data detect data self.default point self.alarm last time self.tk delta .
self.default duration output self.real duration def run self detect an anomaly using the previous difference.bad candidate 3rd candidate1st candidatenot found 1st candidateagg diff fe calc self.detect data self.anomaly duration groundtruth repocodernot found not found fig.
.
case study.
an example sources from crosscodeeval with the task idbeing project ccpython .
the highlight token represents the identical tokens exists in both the query and the bad candidate.
we labelled for this case.
we can see that rlcoder ranks the gold candidate as the first candidate and generates the correct code.
a possible reason that other methods fail to retrieve the correct code is that these methods rely on the surfacelevel similarity between the query and the candidate.
the bad candidate despite sharing several tokens with the query as highlighted in the figure does not contribute meaningfully to the correct code completion.
in contrast to repocoder which iteratively uses generated code for retrieval and generation but still depends heavily on query candidate similarity rlcoder leverages the perplexity of generating target code from a given candidate.
this approach enables rlretriever to bypass candidates that are seemingly useful but actually useless focusing instead on those more likely to aid in accurate code generation.
this strategic prioritization explains rlcoder s success in both retrieving the gold candidate and generating the correct target code.
vi.
r elated work a. code completion code completion as one of the most important tasks in modern ides has attracted the attention of many researchers .
traditional studies use rule based methods or code examples for code completion.
in recent years deeplearning based methods have been explored to improve the performance of code completion.
recent studies found that code search can enhance code completion performance .
with the development of large language models many researchers have introduced llms into code completion .
equipped with llms many studies have employed rag for code completion generation .
for example redcoder enhances code generation and summarization by integrating relevant past work using dense retrieval techniques.
to enhance private library code generation apicoder was proposed to employ api documentation to train models to better generate these libraries.
docprompting introduces a method to enhance code generation by using code documentation to address the challenge of generating code for unseen functions and libraries.
acecoder improves code generation by integrating example retrieval and guided generation.
recode improves neural code generation by incorporating subtree retrieval from existing code examples.
knn tranx improves code generation from natural language by using syntax aware retrieval reducing noise and computational time.b.
repository level code completion repository level code completion which leverages the broader context of an entire code repository has become a focal point for research in the field of code completion and many studies have attempted to improve repository level code completion performance .
cocomic and repohyper enhance code completion capabilities through dependency analysis and learning based methods but they encounter the problem of difficulty in obtaining training data and poor generalizability.
codeplan repofuse anda3 codegen employ static code analysis to obtain relevant candidates.
repocoder and de hallucinator adopt an approach through iterative retrieval and generation.
codeagent and toolgen explore tool invocation to help code completion.
although these efforts show promising performance rlcoder differs from them in that it does not require labeled data to train and uses a novel stop signal mechanism to know when to retrieve and which candidates to retain.
vii.
c onclusion in this paper we propose rlcoder a novel reinforcement learning framework for repository level code completion.
we enable the retriever to iteratively learn by obtaining feedback from the evaluator.
besides unlike using fixed window candidates or candidates parsing from dependency we introduce a simple yet effective split aggregate candidate construction method based on human programming habits.
moreover we propose the stop signal to avoid using useless cross file context.
experimental results indicate that rlcoder achieves state of the art performance on repository level code completion and demonstrates good generalizability and applicability to further enhancing existing methods.