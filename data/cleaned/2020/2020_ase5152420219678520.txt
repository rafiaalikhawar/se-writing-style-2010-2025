subtle bugs everywhere generating documentation for data wrangling code chenyang y ang peking universityshurui zhou university of torontojin l.c.
guo mcgill universitychristian k astner carnegie mellon university abstract data scientists reportedly spend a significant amount of their time in their daily routines on data wrangling i.e.
cleaning data and extracting features.
however data wranglingcode is often repetitive and error prone to write.
moreover it iseasy to introduce subtle bugs when reusing and adopting existingcode which results in reduced model quality.
t o support datascientists with data wrangling we present a technique to generatedocumentation for data wrangling code.
we use programsynthesis techniques to automatically summarize data transfor mations and test case selection techniques to purposefullyselect representative examples from the data based on executioninformation collected with tailored dynamic program analysis.we demonstrate that a jupyterlab extension with our techniquecan provide on demand documentation for many cells in popularnotebooks and find in a user study that users with our pluginare faster and more effective at finding realistic bugs in datawrangling code.
index t erms computational notebook data wrangling code comprehension code summarization i. i ntroduction it has been reported that data scientists spend a significant amount of time and effort on data cleaning and feature engineering the early stages in data science pipelines collectively called data wrangling in the literature.
typical data wrangling steps include removing irrelevant columns converting types filling missing values and extracting andnormalizing important features from raw data.
data wranglingcode is often dense repetitive error prone and generally notwell supported in the commonly used computational notebookenvironments.
importantly data wrangling code often contains subtle problems that may not be exposed until later stages if at all.
inour evaluation we found dozens of instances of suspiciousbehavior such as computations that use the wrong source thatare not persisted or that inconsistently transform part of thedata.
although they do not crash the program they clearly vio late the code s apparent intention often specified in commentsand markdown cells thus we consider them as bugs.
unfor tunately as tests are very rare in data science code in note books these bugs remain undetected even for many highly upvoted notebooks on popular data science sites like kaggle.
in this work we propose to automatically generate concise summaries for the data wrangling code and purposefully select representative examples to help users understand the impact of the code on their data.
this form of automated documentation is useful for multiple scenarios that require code understanding debugging data wrangling code is often concise sequencing multiple nontrivial data transformations asin our example in fig.
1a but also usually not welltested .
data scientists currently mostly rely on codereading and inserting print statements to look for potentialproblems.
reuse data scientists heavily reuse code through copying and editing code snippets often within a notebook from other notebooks from tutorials or from stackover flow .
at the same time reusing data wrangling codecan be challenging and error prone especially if thereused code needs to be adapted for the data scientist sown data.
maintenance data science code in notebooks is often not well documented yet data science codeneeds to be maintained and evolve with changes indata and models especially when adopted in productionsettings .
to avoid mistakes in maintenance tasksand degrading model quality over time understandingexisting data wrangling code and assumptions it makesis essential.
our work is inspired by past work on code summarization to automatically create summaries of code fragments that couldserve as documentation for various tasks.
however whileexisting code summarization work tries to characterizewhat a code fragment does generally for all possible inputs our approach summarizes what effect code has on specific input data in the form of a dataframe highlighting represen tative changes to rows and columns of tabular data.
giventhe data centric nature of data wrangling code understandingthe effect that data wrangling code has on the data often isthe immediate concern for data scientists.
to the best of ourknowledge this is a novel view on summarization tailored fordebugging reuse and maintenance tasks of data scientists.
moreover our approach generates the documentation on demand for the code and data at hand to help with program comprehension.
this is achieved by instrumenting data sciencecode to collect runtime data and select data access pathsand branches executed at runtime using program synthesis techniques to generate short descriptive summaries and using techniques inspired by test suite minimization to select and organize examples.
we integrate our tool w rangle doc i n jupyterlab a commonly used notebook environment.
we evaluated our approach and tool in two ways.
first we conducted a case study with kaggle notebooks to evaluate 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee data pd.read csv .
data.csv x load some other data that s not relevant for the next cell first change varies with device to nan 4def to nan item ifitem varies with device return np.nan else return item data data .map to nan convert size num data.size.replace r regex true .
astype float factor data.size.str.extract r expand false factor factor.replace .
fillna data num factor.astype int fill nan data .fillna data .mean inplace true some training code reading combined targets data data.drop target inplace true axis clf randomforestclassifier n estimators max features sqrt clf clf.fit data targets a three notebook cells loading tabular data transforming the size column converting k and m to numbers and replacing varies withdevice by mean value and learning a model from the data.
whilethis code is fairly linear and relies heavily on common apis itencodes nontrivial transformations compactly that are not alwayseasy to understand.
b w rangle doc interface documentation of the second cell data flow into or out of the cell concise summary of changes highlighting changed columns meta information type unique range for columns selected examples.
fig.
excerpt of real data wrangling code from a kaggle competition and corresponding generated documentation with wrangle doc.
due to case sensitivity in regular expressions values with a k are not transformed correctly as easily visible in the generated summary.
correctness and runtime overhead and additionally explore the kind of documentation we can generate for commonnotebooks.
second we conducted a human subject study toevaluate whether w rangle doc improves data scientists efficiency in tasks to debug notebooks.
through the two studies we provide evidence that our approach is both practical andeffective for common data wrangling code.
overall we make the following contributions an approach to summarize data transformations for datawrangling code based on program synthesis.
an approach to purposefully select rows to illustratechanges in data wrangling code inspired by test suiteminimization techniques.
a prototype implementation as a jupyterlab plugin.
empirical evidence showing that our approach can accu rately generate summaries for nontrivial real world datascience code with acceptable overhead.
a user study finding that our approach improves data sci entists efficiency when debugging data wrangling code.
we share the tool and our supplementary material on github.
d esign motiv a tions many prior studies explored practices of data scientists in notebooks and challenges that they face.
with the surginginterest in machine learning notebooks are a very populartool for learning data science and for production data scienceprojects used by data scientists withwidely varying programming skills and software engineeringbackground.
data science work is highly exploratory and iter ative with heavy use of copy and paste from othernotebooks and online examples .
while researchers foundwide range of challenges including reproducibility collaborative editing and reliability wefocus on challenges regarding comprehension and debugging.
data wrangling code can be challenging to understand although it is typically linear and structured in short cells data wrangling code can be dense and make nontrivial trans formations with powerful apis as in our example .
to better capture how data scientists approach understanding data wrangling code we conducted a small informal ex periment in which we gave four volunteers with data scienceexperience a notebook and two tasks that required programcomprehension.
specifically we asked them to modify the 305notebook to accommodate changes to the input dataframe and to look for possible improvements of model performance allwhile thinking aloud .
we observed two main strategies that our participants used to understand data wrangling code.
on the one hand theyfrequently reasoned statically about the code inspecting the code line by line without running it.
in this process theyoften left the notebook to look up the api documentationand code examples as needed for the numerous functions inthe used data science libraries such as extract and replace and their various arguments in our example.
on the otherhand they also reasoned dynamically by observing executions.
our participants frequently injected print statements at the beginning and the end of cells or in a new cell to inspect datasamples typically the first few rows and manually comparethem before and after the data wrangling steps.
we sawthat dynamic reasoning quickly became overwhelming andtedious with large amounts of data especially if data triggeringproblematic behavior is not part of the first few rows.
inour example the first five rows of the rowscontained sizes ending with the letter m and containing thevalue of v aried with device but not sizes ending in k which makes the incorrect transformations of k ending rowsdifficult to spot.
existing tools are limited notebook environments are evolving and various new tools are proposed by practitionersand researchers .
for example more recent notebookenvironments now provide code completion features and canshow api documentation in tooltips the ide pycharm andjupyterlab extensions integrate a traditional debugger allstandard features in ides.
several extensions like pandas profiling help inspect data stored in variables.
y et tool support for understanding data wrangling code is still limited and does not well support the activities weobserved.
classic tools like debuggers if available at all do not provide a good match for data centric linear andoften exploratory notebook code where a single line canapply transformations to thousands of rows at once and actualcomputations are performed deep in libraries often in nativecode .
tools for exploring data in variables are useful forunderstanding data at one point in time but do not help inunderstanding complex transformations within a cell.
data wrangling code is frequently buggy several researchers have pointed out code quality problems in note books .
notebooks almost never include anytesting code and practitioners report testing as a commonpain point .
the commonly used data wrangling apisare large and can be easily misunderstood .
due to thedynamic and error forgiving nature of python and the pandaslibrary design buggy code often does not crash with anexception but continues to execute with wrong values whichcould subsequently reduce model accuracy.
it is generally easy to introduce mistakes in data wrangling code which became very obvious when we inspected exam ples of documentation generated with our tool on popularnotebooks some among the most upvoted notebooks onapi misuse attempting to remove na values from column not table df df.joined.dropna .map lambda x x. split .split loc called twice resulting in assignment to temporary column only df.loc .loc df title .loc .map map means astype is not an in place operation df .astype str .astype int typos reading from wrong table should be df2 df2 df1 .apply int data modelling problems converting money to numbers e.g.
10k .
ignoring decimals thus converting .4k to .
df df .replace regex value df df .astype str .
astype float fig.
examples of subtle bugs in data wrangling code ranging from data cleaning stage e.g.
normalizing the col umn reviews to integers to feature engineering stage e.g.
extracting new feature join year from the joined column .
kaggle .
without actively looking for bugs it is not alwaysclear what the code intends to do we found many exampleswith subtle problems in data wrangling code.
for example there is a subtle bug in our example in fig.
1a where the code tries to convert k to and m to in download counts a capitalized k in line14 results in converting k to instead of .
the codeexecutes without exception but produces wrong results e.g.
.
for .0k rather than the intended .
.
theproblem could have been found easily if one could observeexample transformations with k .
in fig.
we illustrate three kinds of problems in data wrangling code that we found repeatedly across popular notebooks in our evaluation described later in section v a api misuse is common where a function call looks plausible but does not have the intended effect on theinput data e.g.
dropna does not remove the entire row of a table if applied to a single column .
this commonlyresults in computations that are not persisted and have noeffect on the data used later.
simple typos in variable names column names and regular expressions are the source of many other problems often leading to wrong computations.
finally multiple problems relate to incorrect modeling of data often stemming from wrong assumptions aboutthe different kinds of data in the dataset thus missingrare cases.
all the above problems can be difficult to locate without aclear and thorough understanding of the api specifications how they are used in the data wrangling code and the impact 306on the specific instances from the input dataset.
iii.
s olution overview before we describe the technical details of how we generate documentation let us illustrate the kind of documentation we generate with w rangle doc from a notebook user s perspective.
in a nutshell we summarize the changes a codefragment typically a notebook cell performs on dataframesand show them in a side panel through a jupyterlab extension as illustrated in fig.
1b for our running example.
ourdocumentation includes the following pieces of information we identify the dataframes tabular variables that flow in and out of the code fragment to identify which importantvariables are read and written in the code fragment.
to avoidinformation overload we deliberately include only variablesthat are later used in the notebook again but not temporaryvariables.
in our running example the dataframe data is changed for subsequent cells whereas we omit temporaryvariables num and factor from our documentation.
we provide a concise summary of the transformations for each changed dataframe using a domain specific language dsl we designed.
the summary describes which columnsof the dataframe were added removed or changed howcolumns were changed and whether rows were removed.
thesummary intentionally uses somewhat generic function nameslike str transform to indicate that strings were manipulated without describing the details of that transformation whichcan be found in the code.
these summaries provide a quickoverview of what the code does helping to ensure thatthe static understanding of apis aligns with the observedexecution.
it is particularly effective at highlighting data notwritten bugs where the summary would clearly indicate thatno data was changed.
for example data scientists can easilyspot all api misuse bugs in fig.
when they encounter the unexpected summary of no changes for their transformationcode.
similarly the typos bug in fig.
can also be surfaced as the summary review count int merge reviews would show that different items are merged whereas the code intendsto convert strings to integers without merging.
we show sample data from the modified dataframes specifically comparing a dataframe s values before andafter the cell the summary highlights which columns havebeen modified highlights changes to column data and metadata including types cardinality and range of values .
this direct before after comparison highlights thechanges that would usually require manual comparison of twodataframes hence reducing the manual efforts of comparingthe output of print statements in dynamic debugging.
finally where classic print statements would simply show the first few rows of long dataframes our documentationpurposefully groups rows that take the same path at branchingdecisions in transformation code showing one example eachand highlighting the number of other rows that take thesame path.
grouping rows by transformation decisions drawsattention to paths that may not occur in the first few rows making it easier to spot potential problems.
for example this input notebookpython scriptdef use analysis run time informationrun instrumented script patterng synthesis clustering examplesdocumentation instrumenting tracing code summary synthesizer example selectorexecution pathsdynamic values fig.
approach overview.
makes the bug in fig.
that does not transform k to 1000obvious even though it occurs only in percent of all rows andnot in any early ones.
our approach enables the data scientiststo examine those rare examples and corner cases effectively.
as we will show the above forms of documentation support effective static and dynamic reasoning which is the foundationof various debugging reuse and maintenance tasks and theyhelp surface subtle bugs in data wrangling code.
iv .
w rangle doc s ynthesizing summaries and selecting examples wrangle doc generates documentation on demand with two components summary synthesizer and example selector.
both collect information by analyzing and instrumenting anotebook s code and observing it during its execution seefig.
for an overview.
the summary synthesizer gathers static and run time information about access to dataframesand columns and runtime values of dataframes before andafter a cell to synthesize summary patterns fig.
1b .
the example selector traces branching decisions during data transformations to cluster rows in a dataframe that share thesame execution paths fig.
1b .
a. summary synthesis the goal of synthesizing summaries is to derive a concise description of how data is transformed by a fragment of datawrangling code typically a notebook cell.
to avoid distractingusers with implementation details which may use nontrivialapi sequences external libraries and custom code we syn thesize summaries that describe the relationship between databefore and after the code fragment.
through instrumentation we collect data of all variables with emphasis on tabular datain dataframes before and after the target code from whichwe synthesize summaries that explain the differences such asadded columns removed rows or changed values.
a synthesis approach as in all summary generation there is an obvious tradeoff between providing concise sum maries e.g.
dataframe x was changed and detailed summaries e.g.
column y was added with values computed by removing all dashes from column z replacing k at the endof the string by and then converting the result into anumber .
summaries at either extreme are rarely useful too df createcol df col col modifycol df col col modify column removecol df col removeduplicaterows df row col removenullrows df row col removerows df row col remove some rows rearrangecols df col col change col. order rearrangerows df row row change row order concatrows df df concat dataframes by rows dataframe compute unspecified dataframe computation col fillna col fill null values merge col merge items to reduce cardinality category col convert columns to category type float col str col int col bool col datetime64 col type conversion encode col encode columns in consecutive ints one hot encoding col encode columns in ints type convert col other type conversion str transform col unspecified string transf.
num transform col unspecified numerical transf.
compute col ref unspecified col. computation col ref col ref dataframe.col fig.
the dsl for data transformation.
cost per expression is indicated as subscript.
concise summaries do not convey much information whereas too detailed summaries might just paraphrase the code andprovide little benefit over reading the code directly.
our summary synthesis aims to find a balance by describing data wrangling code as expressions formed from an extensiblegrammar of transformation patterns which describe the impact of the transformations concisely and unambiguously.
forexample pattern fillna describes that null values are filled but not any details about how the new values are computed patternstr transform describes that a column with string values has been manipulated but not how pattern one hot encoding describes that a binary column was created for distinct valuesin a source column.
we include generic catch all summariesfor transformations we cannot further explain such as compute for generating data from unknown processes.
while the pattern language is easily extensible we start with themost used patterns shown in fig.
which we derived bymanually summarizing and grouping common transformationpatterns in dozens of sampled notebooks from github.
ourdsl supports abstracting multiple transformation patterns inone combined expression.
the operation of applying float to the size column in data after str transform in the running exampling in fig.
1a will be synthesized as modifycol data size float str transform data.size cf.
fig.
1b .
during the summary synthesis the synthesis engine searches for the expressions in the pattern language that matchthe input output examples created by the target code.
becausethere could exist multiple matches for the same input outputpair we design a cost function to select the best matched ex pression.
concretely we assign a higher cost to more genericpatterns see fig.
and consider the cost of an expression asthe sum of the cost of all patterns used in that expression.
thisway our synthesis engine favors concise but concrete patterns.
.decl apply depth number pattern pattern .decl var depth number type coltype nafilled boolean carddrop boolean onehot boolean encode boolean 4var depth type true carddrop any any apply depth fillna var depth type false carddrop fig.
we describe propagation of knowledge about rules in terms of datalog relations and illustrate the propagation rulefor fillna .
in line there are two datalog facts apply and var .
we use var to track a column s attributes e.g.
whether its missing values are filled and we use apply to represent application of a pattern.
the rule in line propagates var through a pattern fillna .
the attribute nafilled is set to true while onehot and encode are set to any because any priorinformation about these is lost in the possible transformations.
while this kind of search is standard for program synthesis the novelty of our approach is in encoding summary generationas a synthesis problem not in the synthesis algorithm itself.
b synthesis implementation for the synthesis we follow a standard top down enumerative search to explore ex pressions of a given grammar as shown in algorithm .the synthesis engine incrementally enumerates expressionsallowed by the grammar by substituting non terminals withnew expressions in order of increasing costs the synthesisprunes the search space for partial expressions i.e.
containholes for some nonterminals that cannot explain the input output difference it returns the expression that explains theinput output difference with the lowest cost as all expressionsare maintained in a priority queue.
the synthesis problem canbe encoded in standard synthesis engines like rosette but for simplicity we implemented our own.
to validate whether a partial expression can explain the input output pair we compute and check the result of anexpression against the output.
aside from parts that can beeasily checked concretely such as the removal of columnsand rows many patterns express generic computations thatcan match many input output examples e.g.
fillna .
here we reject infeasible expressions similar to how static analysesidentify problematic computations we track abstractfacts across patterns and each pattern has a transfer functionthat can generate or kill facts.
throughout these patterns wetrack each column s type if known whether it has missingvalues its length its cardinality and whether it looks like acommon encoding pattern.
this way we can check that fillna actually filled missing values without having to know concretefilled values or having to worry about interactions with otherpatterns.
in fig.
we show an example rule for propagatingfacts when applying fillna .
details of the other validationrules can be found in the supplementary material.
c data gathering and optimization to collect the values of all variables before and after the target data wranglingcode we instrument the user s code to store values into files atruntime.
to target the instrumentation we use a simple static 308algorithm enumeration based synthesis algorithm input e input output example pair c cost function for transformation patterns s a set of accessed columns function synthesis e c s q priority queue of patterns ordered by c top compute whileqis not empty do cur q.dequeue ifc cur c top then break prune because elements left have higher cost ifcur is fully resolved then ifvalidate e cur then top cur else newpatterns extend cur s forpinnewpatterns do ifisfeasible e p then q.add p returntop def use analysis adopted from prior work to record only values of variables that flow into or out of the target code.
in addition we collect the sequence of access paths for read and write access to columns of dataframes by dynamicallyintercepting such access to limit the columns over which tosearch as part of the synthesized expressions.
furthermore when synthesizing column expressions we consider onlysource columns for which we observed read access beforewrite access to the target column.
in our current implementation we consider each cell as a separate segment for which we gather data and synthesizesummaries but it is also possible to consider each line asa segment if more granular documentation is desired.
forexample users could interactively indicate what to consideras a segment or a tool could automatically cluster lines orcells .
d presenting results in the user interface we present transformations for all changed dataframes.
as exemplifiedin fig.
1b we translate the synthesized expressions into astructured format separated by affected column.
in addition we highlight parts of the transformation patterns in a tableof example data showing but crossing out removed columns highlighting changed columns and a few patterns such as type conversions as part of the table metadata .
b. example selection as discussed data scientists often inspect the data before and after a cell with temporary print statements but they maymiss issues that are not visible in the first few rows.
our insightis to group rows that are affected by the same transformationpaths highlighting examples from each path to reveal data thatfit unusual processing conditions which may be of particularinterest in understanding and debugging tasks.
inspired by test suite minimization techniques that select a small number of test cases from a larger pool to trigger differ ent execution paths we group rows in a dataframe bythe branching decisions taken in the data wrangling code thusidentifying a minimum number of rows that trigger differentexecution paths.
conceptually our strategy can be consideredas inspecting path coverage within data wrangling code whereeach row is treated as a test input to the code.
if a function is applied to every row separately we can treat each row as separate test input and track branching decisionswithin the supplied function.
however most transformationsin data science code are applied to entire tables or vectors atonce through functions like replace fillna and map possibly parallelized.
also many decisions in common functions likesplit and replace happen deep in libraries or native code.
for simplicity and manageable overhead we instrument commonlibrary functions on tables and vectors each producing a vectorof decision outcomes for every instrumented function thathas internal branching decisions.
for example for replace we record whether the search term has been replaced and for split we record the number of splits.
only for user defined functionssupplied to map orapply functions we collect branching decisions with a tracer as a traditional test coverage tool does.
using the vector that represents the branching decision of every decision point for each row we group all rows that took the same path through all decisions.
for our motivatingexample we have branching decisions corresponding to the if statement in the function supplied to map and in the calls to replace extract and fillna.
in this case the input data takes three distinct paths shown in fig.
1b .
presenting results when displaying the examples for the target data wrangling code we show only the first row fromeach group while indicating how many additional rows thereare in each group and provide a button to show more examples fig.
1b .
in a tooltip we also show the branching decisions taken in each group.
this presentation highlights the mostcommon transformations and also directs the user s attentionto uncommon transformations where the subtle bugs oftenoccur in data science code.
c. implementation we implemented our synthesis and example selection engine as an extension to the popular jupyterlab environment called w rangle doc.
the goal of this prototype implementation is to demonstrate feasibility and allow experimentation with a best effort approach though a fully featured industry readyimplementation would likely need to extend this.
specifically our dsl patterns are derived transformations observed in a convenience sample of notebooks on github.while all transformations can be explained with the generic compute pattern we are likely missing more specific pat terns that may be useful in some settings.
for example selection we instrumented common library functions thatwe observed in the same sample including str .replace pandas.series.fillna and pandas.dataframe.apply.
we also instrumented if expression that may lead to different brancheswith the same line number.
we do not claim that instrumen tation is exhaustive in our prototype but the implementationcan be easily extended with support for more functions.
309table i characteristics of our subject notebooks per dataset reporting averages for code length number of codeand text cells and number of named and lambda functions.
dataset loc code cells text cells funct.
lambda titanic .
.
google play .
.2fifa19 .
.0airbnb .
.
all .
.
v. e mpirical study of popular notebooks we evaluate our approach both regarding technical capabilities on popular notebooks this section and how it helps users debug notebooks in action next section .
we start by exploring to what degree w rangle doc can generate documentation in data wrangling cells of a set ofnotebooks and how accurate that documentation is how often are the synthesized summaries correct for data wranglingcells?
rq1 next we report statistics about patterns and branching characteristics observed to capture to what degree data wranglingcode performs nontrivial transformations for which documen tation is likely useful what are typical characteristics of explanations for data wrangling code?
rq2 finally we measure the overhead introduced by our instrumentation and the computational effort required to synthesizethe documentation to capture to what degree w rangle doc can be used in an interactive setting how much overhead does wrangle doc introduce?
rq3 a. notebook selection to answer the research questions we apply our approach to a set of notebooks that were not used during our tool sdevelopment.
there are several challenges with assembling acorpus for our study first while millions of public notebooksare shared on sites like github they are often from classprojects of low quality and challenging to reproduce e.g.
dueto missing data or library dependencies .
in addition manynotebooks start with an already cleaned dataset and focus moreon modeling than data cleaning and feature engineering.
hencewe decided to curate a small but diverse corpus of reproduciblepopular notebooks with significant data wrangling code.
we decided to sample high quality notebooks from popular kaggle competitions that provide tabular data in a raw format.kaggle is the largest social platform for data science com petitions where users can upload datasets and correspondingchallenges and others can submit and rate solutions in theform of notebooks.
popular challenges often have thousands ofsubmitted solutions.
we select multiple solutions per competi tion as they share the same setup.
specifically we select fourpopular competitions titanic google play store fifa andairbnb .
from each competition we choose the top25 notebooks based on most votes after filtering non pythonnotebooks task irrelevant notebooks not solving the modelingtask typically tutorials for python libraries and notebooks wecould not reproduce due to missing or outdated dependencies.we sampled each competition with careful consideration thetitanic dataset is usually used for educational purposes manysolutions are written as educational notebooks for data sciencelearners.
google play and fifa are selected based on theirpopularity representative of trending notebooks on kaggle.finally we selected airbnb as a challenge that uses multiplelarger datasets which provides a better approximation ofproduction setting.
notebooks in our corpus have many characteristics that are similar to those found in other large scale studies of publiclyavailable notebooks as shown in tab.
i they aretypically long and split into many cells rarely abstract codeinto functions or lambda expressions though they containmore text cells than most public notebooks.
b. summary correctness rq1 first we analyze correctness of the synthesized summaries for the sampled notebooks.
we are not aware of an automated procedure that could test correctness other than how wevalidate patterns during the synthesis process in the first place hence we manually judge whether the summary corresponds to the actual transformation in the cell.
to gain confidence inthe manual judgement multiple authors independently analyzea subset of cells to establish inter rater reliability.
research design we proceed in four steps.
first we identified all cells from the sampled notebooks that createor modify any dataframes by monitoring state changes ofscript execution.
we found such cells writing to 1998dataframes.
second we synthesized documentation for allthe cells and prepared a user interface to show thatdocumentation including tool tips explaining each involvedpattern.
third to establish a reliable judgement process wecreated a rubric on how to evaluate correctness of the syn thesized summary shared in supplementary material .
usingthe rubric four authors independently judged the correctnessof randomly selected dataframes and their evaluationachieved excellent inter rater agreement .
according tofree marginal kappa .
finally having established relia bility of judgement another randomly sampled dataframeswere judged by a single author.
analyzing out of 1998dataframes gives us a margin of error of less than percentat a confidence level.
results w rangle doc created correct summaries for of the inspected dataframes that were created orchanged in sample code.
in six out of the eight incorrectcases w rangle doc did not create a summary even though data was changed due to limitations of our current def useanalysis it can not catch modifications to a dataframethrough indirect