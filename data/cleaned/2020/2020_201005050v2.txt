symbolic parallel adaptive importance sampling for probabilistic program analysis yicheng luo university college london imperial college london united kingdom yicheng.luo.
ucl.ac.ukantonio filieri imperial college london united kingdom a.filieri imperial.ac.ukyuan zhou artificial intelligence research center dii china yuaanzhou outlook.com abstract probabilistic software analysis aims at quantifying the probability of a target event occurring during the execution of a program processing uncertain incoming data or written itself using probabilistic programming constructs.
recent techniques combine symbolic execution with model counting or solution space quantification methods to obtain accurate estimates of the occurrence probability of rare target events such as failures in a mission critical system.
however they face several scalability and applicability limitations when analyzing software processing with high dimensional and correlated multivariate input distributions.
in this paper we present symbolic parallel adaptive importance sampling sympais a new inference method tailored to analyze path conditions generated from the symbolic execution of programs with high dimensional correlated input distributions.
sympais combines results from importance sampling and constraint solving to produce accurate estimates of the satisfaction probability for a broad class of constraints that cannot be analyzed by current solution space quantification methods.
we demonstrate sympais s generality and performance compared with state of the art alternatives on a set of problems from different application domains.
ccs concepts mathematics of computing metropolis hastings algorithm software and its engineering software verification and validation .
keywords symbolic execution probabilistic analysis probabilistic programming importance sampling markov chain monte carlo introduction probabilistic software analysis methods extend classic static analysis techniques to consider the effects of probabilistic uncertainty whether explicitly embedded within the code as in probabilistic programs or externalized in a probabilistic input distribution .
analogously to their classic counterparts these analyses aim at inferring the probability of a target event to occur during execution e.g.
reaching a program state or triggering an exception.
for the probabilistic analysis of programs written in a generalpurpose programming language probabilistic symbolic execution pse exploits established symbolic execution engines for the language e.g.
to extract constraints on probabilistic input or program variables that lead to the occurrence of work done when the author is with imperial college londonthe target event.
the probability of satisfying any such constraints is then computed via model counting or inferred via solution space quantification methods depending on the types of the variable and the characteristic of the constraints and the probability distributions.
variations of pse include incomplete analyses inferring probability bounds from a finite sample of program paths executed symbolically methods for non deterministic programs and data structures with applications to reliability security and performance analysis .
while pse can solve more general inference problem the overhead of symbolic execution is typically justified when the probability of the target event is very low rare event or high accuracy standards are required e.g.
for the certification purposes of safety critical systems.
a core element of pse is to compute the probability of certain variables satisfying a constraint under the given input probability distribution.
in this paper we focus on estimating the probability of satisfying numerical constraints over floating point variables.
for limited classes of constraints and input distributions analytical solutions or numerical integration can be computed .
however these methods become inapplicable for more complex classes of constraints or intractable for high dimensional problems.
monte carlo methods provide a more general and scalable alternative for these estimation problems.
these methods estimate the probability of constraint satisfaction by drawing samples from the input distribution and estimating the satisfaction probability as the ratio of samples that satisfy the constraints.
nonetheless while theoretically insensitive to the dimensionality of problems care must be taken to apply direct monte carlo methods in quantifying the probability of rare events i.e.
when the probability of satisfying required constraints is extremely small.
to improve the accuracy of the estimation in the presence of low satisfaction probability recent work uses interval constraint propagation and branch and bound techniques to partition the input domain of a program into sub regions that contain only no orin part solutions to a constraint.
this step analytically eliminates uncertainty about the regions containing only ornosolutions requiring estimation to be performed only for the remaining regions.
the local estimates computed within these regions are then composed using a stratified sampling scheme the probability mass from the input distribution enclosed within each region serves as the weight of the local estimate effectively bounding the uncertainty that it propagates through the composition.
however the performance of this method degrades exponentially with the dimensionality of the input domain and it requires an analytical form for the cumulative distribution function of the input distribution toarxiv .05050v2 jun 2021yicheng luo antonio filieri and yuan zhou compute the probability mass enclosed within each region.
since the cumulative distribution function of most correlated distributions is not expressible in analytical form the numerical programs that can be currently analyzed with pse are restricted to those with independent inputs.
in this paper we propose symbolic parallel adaptive importance sampling sympais a new inference method for the estimation of the satisfaction probability of numerical constraints that exploits adaptive importance sampling to allow the analysis of programs processing high dimensional correlated inputs.
sympais does not require the computability of the input cumulative density functions overcoming the limitations of current state of the art alternatives relying on stratified sampling.
we further incorporate results from constraint solving and interval constraint propagation to optimize the accuracy and convergence rate of the inference process allowing it to scale to handle higherdimensional and more general input distributions.
we implemented sympais in a python prototype and evaluated its performance on a set of benchmark problems from different domains.
background this section recalls program analysis and mathematical results required to ground our contribution and details the limitations of the state of the art we aim to tackle.
.
probabilistic symbolic execution probabilistic symbolic execution pse is a static analysis technique aiming at quantifying the probability of a target event occurring during execution.
it uses a symbolic execution engine to extract conditions on the values of inputs or specified random variables that lead to the occurrence of the target event.
it then computes the probability of such constraints being satisfied given a probability distribution over the inputs or specified random variables.
these constraints are called path conditions because they uniquely identify the execution path induced by an input satisfying them .
probabilistic profile 2altitude gaussian 3obstacle x obstacle y gaussian program 7if altitude ... if math .
pow obstacle x math .
pow obstacle y callsupervisor ... else callsupervisor listing example code snippet for an example safety monitor of an autopilot navigation system.
consider the simplified example in listing adapted from using a java like syntax and hypothetical random distributions for the input variables.
the snippet represents part of the safety controller for a flying vehicle whose purpose is to detect environmental conditions excessive altitude or collision distance of an obstacle that may compromise the crew s safety and call for a supervisor s intervention.
the purpose of the analysis is to estimate the probability of invoking callsupervisor at any pointin the code.
safety critical applications may require this probability to be very small e.g.
and to be estimated with high accuracy.
the symbolic execution of the snippet where random variables are marked as symbolic would return the following two path conditions pcs corresponding to the possible invocations of callsupervisor pc0 altitude andpc1 altitude pow obstacle x pow obstacle y .
the probability of satisfying a path condition pccan be computed based on the distributions assigned to the symbolic variables as in equation for simplicity in the remainder of the paper we assume a probability distribution is specified for every symbolic variable or vector of symbolic variables ppc pr x pc x1pc x p x dx nn i 11pc x i pdmc where x i p x where 1pc x denotes the indicator function which returns if x pc that is xsatisfiespc and otherwise.
for clarity we will use p x to denote the truncated distribution satisfying the constraints i.e.
p x 1pc x p x .
because analytical solutions to the integral are in general intractable or infeasible monte carlo methods are used to approximateppc as formalized in equation .
when the samples x i are generated independently from their distribution p x equation describes a direct monte carlo dmc integration also referred to as hit or miss which is an unbiased estimate of the desired probability and its variance pdmc pdmc nis a measure of the estimator convergence which can be used to compute a probabilistic accuracy bound i.e.
the probability of the estimate deviating from the actual unknown probability by more than a positive accuracy .
since the path conditions are disjoint i.e.
x pci x pcj i j an unbiased estimator for the probability of the target event to occur through any execution path is ppc i ppc i over all the pcireaching the target event.
specialized model counting or solution space quantification methods to solve the integral in equation for pse application have been proposed for linear integer constraints arbitrary numerical constraints string constraints bounded data structures .
in this work we focus on the probabilistic analysis of program processing numerical random variables.
.
compositional solution space quantification borges et al .
proposed a compositional monte carlo method to estimate the probability of satisfying a path condition over nonlinear numerical constraints with arbitrary small estimation variance we will refer to this method as qcoral .
the integrand function in equation is an indicator function returning 1for variable assignments satisfying a path condition pc and 0otherwise.
such function is typically ill conditioned for standard quadrature methods and may suffer from the curse of dimensionality when the number of symbolic variables grows the ill conditioning and discontinuities of the integrand may also lead to high variance for monte carlo estimators and particular care should be placed whensymbolic parallel adaptive importance sampling for probabilistic program analysis dealing with low probability constraints.
qcoral combines insights from program analysis interval constraint propagation and stratified sampling to mitigate the complexity of the integration problem and reduce the variance of its estimates.
constraint slicing and compositionality.
as already recalled the path conditions of a program are mutually exclusive.
therefore the probability estimates of a set of path conditions leading to a target event can be added algebraically the mean of the sum being the sum of the means while the variance of the sum can be bounded from the variance of the individual summands .
a second level of compositionality is achieved in qcoral within individual path conditions via constraint slicing .
a path condition is the conjunction of atomic constraints on the symbolic variables.
two variables depend directly on each other if they appear in an atomic constraint.
the reflexive and transitive closure of this dependency relation induces a partition of the atomic constraints that groups together all and only the constraints predicating on transitively dependent variables .
because each group of independent constraints predicate on a separate subset of the program variables its satisfaction probability can be estimated independently from the other groups.
the satisfaction probability of the path conditions is then computed using the product rule to compose the estimates of each independent group .
besides enabling independent estimation processes to run in parallel constraint slicing can potentially reduce a highdimensional integration to the composition of low dimensional ones on independent subsets of the symbolic variables in turn leading to shorter estimation time and higher accuracy for a fixed sampling budget .
interval constraint propagation and stratified sampling.
to further reduce the variance of the probability estimates of each independent constraint qcoral uses interval constraint propagation and branch and bound methods to find a disjoint union ofn dimensional boxes that reliably encloses all the solutions of a constraint where nis the number of variables in the constraint.
regions of the input domain outside the boxes are guaranteed to contain no solutions of the constraint .
a box is classified as either an inner box which contains only solutions or an outer box which may contain both solutions and non solutions.
boxes are formally the conjunction of interval constraints bounding each of the nvariables between a lower and an upper bound n i 0lbi xi ubi.
because the boxes are disjoint the probabilities of satisfying a constraintcfrom values sampled from each box can be composed viastratified sampling as the weighted sum of the local estimates weighted by the cumulative probability mass enclosed within the corresponding box .
however since the inner boxes contain only solutions the probability of satisfying cfrom values sampled from an inner box is always no actual sampling required and consequently no estimation variance to propagate.
sampling and variance propagation is instead required only for the outer boxes as per equation xc o i 1p oi xc oi i j 1p ii vc o i 1p oi vc oi whereoandiare the sets of outer and inner boxes respectively.
p is the cumulative probability mass in a box and xcand vc represent the mean and variance of the direct monte carlo estimates for constraint c. for independent input variables as assumed in qcoral the cumulative probability mass enclosed in a box is the product of cdf ubi cdf lbi for all the variables xidefining the box.
sampling from within a box is possible if the distribution of a variable xican be truncated within the interval .
x 202y x figure left solution space of x2 y2 .
right inner and outer boxes produced by realpaver in pink and gray respectively.
stratified sampling with interval constraint propagation can lead to a significant variance reduction in the aggregated estimate by reducing the uncertainty only to the regions of the domain enclosed within the outer boxes potentially avoiding sampling from large regions of the domain that can be analytically determined as including only or no solutions.
because boxes can be iteratively refined up to arbitrary accuracy there is a trade off between the target size and consequently weight of the boxes and their number since each outer box requires a monte carlo estimation process .
example.
consider the constraint x2 y2 1from the example in listing variable names abbreviated .
performing interval constraint propagation with realpaver an interval constraint solver supporting conjunctive nonlinear inequality constraints used in qcoral with the initial input domain x y we obtained the outer and inner boxes depicted in figure in gray and pink respectively.
in figure a large region of the domain falls outside the boxes since it contains no solutions.
hence the probability of satisfying the constraint for values in this region is .
similarly the probability of satisfying the constraint with inputs from an inner box is .
therefore uncertainty is bounded within the outer boxes and estimation proceeds sampling from their truncated distributions and aggregating the result via stratified sampling.
.
limitations of qcoral qcoral can produce scalable and accurate estimates for the satisfaction probability for constraints that have low dimensionality or can be reduced to low dimensional subproblems via constraints slicing are amenable to scalable and effective interval constraint propagation and whose input distribution have cdfs in analytical form and allows efficient sampling from their truncated distributions.
these constraints typically do not hold for high dimensional and correlated input distributions .yicheng luo antonio filieri and yuan zhou constraint slicing assumes that all the inputs are probabilistically independent with dependencies among variables arising only from computational operations e.g.if x y ... .
support for correlated variables requires changing the dependency relation to include also all correlated variable pairs.
this may reduce the effectiveness of constraint slicing in reducing the dimensionality of the integration problems.
interval constraint propagation contributes to reducing estimation variance by pruning out large portions of the input domain that do not contain solutions of a constraint and producing smallsize outer boxes to bound the variance propagated from in box local estimates.
however the complexity of this procedure grows exponentially with the dimensionality of the problem rendering it ineffective when after constraint slicing the number of variables appearing in an independent constraint is still large e.g.
due to correlated inputs that cannot be separate.
the effectiveness of interval constraint propagation for nonlinear non convex constraints also varies significantly for different formulations of the constraint e.g.
x2vs.x x and may require manual tuning for optimal results .
stratified sampling requires analytical solutions of the input cdfs as well as the ability to sample from truncated distributions.
both requirements are generally unsatisfiable for correlated input variables whose cdf cannot be computed in closed form.
the lack of an analytical cdf would require a separate monte carlo estimation problem to quantify the probability mass enclosed within each box and an analysis of how the corresponding uncertainty propagates through the stratified sampling and the composition operators of qcoral .
additionally sampling from a truncated distribution typically relies on the computation of both the cdf and the inverse cdf of the original distribution which is inefficient without an analytical form of these functions.
in summary the main variance reduction strategies of qcoral based on interval constraint propagation and stratified sampling are not applicable for all but trivial correlated input distributions.
constraint slicing can be extended with probabilistic dependencies among input variables but this results in smaller dimensionality reduction with exponential impact on interval constraint propagation even when the cdfs of correlated inputs can be computed analytically.
.
importance sampling the indicator function in equation return only within the regions of the input domain satisfying a constraint e.g.
only within the circle in figure .
when this region encloses only a small probability mass direct monte carlo methods sampling from the input distribution p x may struggle to generate enough samples that satisfy the constraint and therefore fail to estimate the quantity of interestppc.
we discussed before how qcoral uses interval constraint propagation and stratified sampling to prune out regions of the domain that contain no solutions sampling within narrower boxes containing a larger portion of solutions.
an alternative method to improve statistical inference in this problem is importance sampling is .
instead of sampling from the input distribution p x is generates samples from a different proposal distribution q x that overweighs the important regions of the domain i.e.
the regions containing solutions in our case.because the samples are generated from a different distribution thanp x the computed statistics need to be re normalized as in equation ppc x1pc x p x dx x1pc x p x q x q x dx nn i 11pc x i p x i q x i pis where x i q x .
while any distribution q x 0over the entire domain guarantees the estimate will eventually converge to the correct value an optimal choice of q x determines the convergence rate of the process and its practical efficiency.
in our context of estimating the probability of satisfying path conditions pc the optimal proposal distributionq x isexactly the truncated normalized distribution p x satisfyingpc q x ppcp x 1pc x .
in general it is infeasible to sample from q x as it requires the calculation of ppcwhich is exactly our target.
fortunately as we will demonstrate in section .
a proposal distribution found via adaptive refinement can allow us to achieve near optimal performance.
in this paper we propose a new inference method to estimate the satisfaction probability of numerical constraints on high dimensional correlated input distributions.
our method does not require analytical cdfs and can replace qcoral s variance reduction strategies to analyze constraints where these are not applicable.
our method combines results from constraint solving and adaptive estimation to produce near optimal proposal distributions aiming at computing high accuracy estimates suitable for the analysis of low probability constraints.
sympais symbolic parallel adaptive importance sampling in this section we introduce our new solution space quantification method for probabilistic program analysis symbolic parallel adaptive importance sampling sympais .
programpcspcspcsp x symbolic execution assumptionsinferencepath conditionsprobabilistic profileconstraint solverx1 ... xnmcmcq x ppc figure overview of sympais.
figure gives an overview of sympais s workflow.
following the probabilistic symbolic execution approach the path conditions leading to the occurrence of a target event are extracted using a symbolic execution engine.
for simplicity we assume that each vector of symbolic variables is associated with a probability distribution either provided explicitly by the user or extracted from the code where it has been specified via convenient random generators.symbolic parallel adaptive importance sampling for probabilistic program analysis independent variables are associated with univariate probability distributions.
vectors of correlated variables are associated with multivariate probability distributions.
for example in listing altitude is associated with a univariate gaussian distribution with location and scale while obstacle x obstacle y are distributed as a bivariate gaussian with location and covariance matrix .
the path conditions are assumed to have been sliced as in where the dependency relation is augmented with pairwise dependency between the correlated variables besides the dependencies induced by the program control and data flows.
the probability of satisfying each independent constraint is quantified in the inference phase which is the focus of this work.
algorithm symbolic parallel interacting markov adaptive sampling sympais givenc p x symex p domain knowledge p x p x 1c x xinit models c constraint solver initialize the proposal distribution qn 0forn ... n .
fort ... t do run pi mais for titerations updatenproposal distributions q1 n tusing mcmc drawmsamples from each proposal distribution x m n t qn t x form m andn n. compute importance sampling weights w m n t p x m n t n n j 1qj t x m n t end for obtain pi mais esitmate ppimais t n mt t 1n n 1m m 1w m n t v2 pimais t n mt t 1n n 1m m w2 m n t ppimais return ppimais v2 pimais the main steps of sympais are summarized in algorithm .
sympais takes as input a constraint c which may be a path condition ofpor an independent portion of the path condition after constraint slicing cis assumed to be the conjunction of inequalities or equalities on numerical functions of the inputs.
in addition to the constraint c sympais requires specifying a probability distributionp x over the symbolic variables in the program.
such distribution can be provided by the user or specified in the code via convenient random generators.
for simplicity we refer to the probability distribution over all of the symbolic variables as the input distribution.
overview.
the core part of sympais is the adaptive importance sampling process implemented in the for loop at line .
the goal of this process is to iteratively refine an importance sampling proposal that maximizes sample efficiency i.e.
it is very likely to generate sample points within the solution space of the input constraint c.due to the wide range of possible constraint forms e.g.
linear nonlinear non convex and of different types of distributions optimal proposal distributions cannot be obtained analytically.
it is instead approximated via a hierarchical probability distribution whose parameters are iteratively refined via a markov chain monte carlo mcmc algorithm to best approximate the intractable optimal proposal.
mcmc algorithms generate sequences of samples that when the process converges to its steady state the samples are distributed according to a target distribution whose analytical form may be unknown or from which it is not possible or intractably complex to sample directly.
the mcmc samples can thus be used to iteratively estimate the parameters of the proposal distribution towards approximating the optimal distribution.
the algorithm returns the estimate of the satisfaction probability as well as the estimator variance.
the latter may be used to reason about the dispersion of the estimate e.g.
constructing confidence intervals to decide if more sampling is desirable.
notice however that similarly toqcoral the estimator variance is centered around the estimate formula in algorithm .
this requires enough samples to have been collected for the estimate to stabilize first in order for the variance to represent the estimator dispersion around it.
in the remaining of the section section .
we will formally define the adaptive importance sampling strategy of sympais and the mcmc methods it adopts for the adaptive refinement of the proposal distribution.
results from constraint solving will be brought in to mitigate the complexity of the estimation process and accelerate its convergence.
a set of optimizations to improve the practical performance of the methods will be discussed in section .
while implementation details and an experimental evaluation will be reported in section .
running example.
to illustrate the different features of sympais we will use a dimensional nonlinear and non convex constraint torus which is defined in equation x2 y2 r z2 r2 with the constant parameters r r .
at first we will associate to each of the three variables an independent univariate gaussian distribution i.e.
x y z n .
.
we will later generalize the method to correlated inputs.
.
sympais adaptive importance sampling as recalled in section .
importance sampling is methods aim at constructing a proposal distribution q x that increases the likelihood of generating samples that satisfy a constraint c. this allows us to focus the estimation problem to the regions of the input domain that satisfy c while avoiding the need to find a stratification of the input domain and computing the inverse cdfs for the distribution truncation that prevent the use of qcoral for high dimensional and correlated input distributions.
the choice of a proposal distribution q x largely affects the efficiency of is.
however it is usually difficult to obtain an analytical form for the theoretically optimal q x or to sample from.
instead we use an adaptive scheme to iteratively refine a proposal distribution to approximate q x .
.
.
adaptive proposal refinement.
to construct and refine the is proposal distribution sympais adapts the parallel interactingyicheng luo antonio filieri and yuan zhou x 505yp x xq x figure left input distribution p x for torus projected on the x y plane.
right the optimal importance sampling proposalq x .
the intensity of the blue shadowing is proportional to the probability density.
the solution space lays between the dashed circles.
markov adaptive sampling pi mais schema defined in .
the proposal distribution in pi mais is a hierarchical model parameterized bynsub proposals q1 ... qn.
to sample from the proposal distribution we first choose a sub proposal qiand then draw samplesxfromqi.
together the sub proposals form a mixture distribution.
pi mais adapts the sub proposals to the target distribution by running parallel chain mcmc line so that it can form efficient proposal distributions for target distributions that are multimodal and non linear1.
in this paper we use sub proposals parameterized by gaussian distributions qi x n x i .
the probability density function pdf for the proposal distribution is then given by a gaussian q nn i 1n i where the mean vectors i n i 1are adapted by running nparallel mcmc samplers so that the proposal distribution approximates more accurately q x .
at each step t a sampler produces a set of samples xn t .
the proposal distribution at step tis qt nn n 1qn t nn n 1n xn t .
when the refinement process stabilizes the estimate for the probability of satisfying the constraint cgiven the input distribution p x i.e.
the solution of the integration problem in equation is ppimais t1 n1 mt t 1n n 1m m 1wt n m wt n m p x m n t qt x m n t where x m n tare samples drawn from qn t x .
please refer to for the convergence proofs of the pi mais scheme.
to update the proposal distribution we implemented two mcmc samplers in sympais random walk metropolis hastings rwmh and hamiltonian monte carlo hmc .
the former provides a general procedure that only requires the ability to evaluate the density of the constrained input distribution p x p x 1cfor a given 1we provide an executable notebook with more details on pi mais in the open source code .value x. the latter requires the p x to be differentiable and exploits the gradient information to achieve higher efficiency in many cases especially for higher dimensional problems.
for space reason in the remainder of this section we will mostly focus on rwmh while additional details on our hmc implementation are provided in .
random walk metropolis hasting for adaptation.
mcmc methods generate a sequence of samples where each sample is via a probabilistic transition from its predecessor.
randomwalk metropolis hasting rwmh is an mcmc algorithm where the next sample x is generated from its predecessor xfrom a proposal distribution or proposal kernel x x .
the newly proposed sample x is accepted and added to the sequence randomly with probability min p x x x p x x x otherwise the new sample is rejected and xis retained.
for rwmh within sympais we use x x n x x i.e.
the next candidate sample is generated by adding a white gaussian noise with covariance to the current sample x. after the generation process converges at steady state a value xshould appear in the sequence with a frequency proportional to its probability in the target distribution p x .
becausep x is zero outside the solution space of the constraint c all the samples that do not satisfy cwill be rejected.
high rejection rate slows down the convergence and can be mitigated by tuning using a different proposal kernel or switching to more sophisticated methods to generate the next sample such as a hamiltonian proposal.
.
.
sympais estimation process.
each of the parallel mcmc processes used to refine the importance sampling proposal requires an initial value xinitto start from.
in theory any point from the input domain can be chosen to start the mcmc processes.
however principled choices of xinitcan speed up the converge of the markov chain to a steady state and reduce the sample rejection rate.
the choice of the initial points is particularly important when the constrained distribution p x is multimodal i.e.
its density function has two or more peaks either because the original input distribution p x is itself multimodal or because the restriction to the solution space of cinduces multiple modes in p x .
we will discuss the problem of multiple modes and sympais s mitigation strategies in section .
while focus here on sympais s use of constraint solving to initialize the mcmc processes.
in statistical inference literature the initial sample of an mcmc process is typically randomly assigned by a value within the input domain.
however if the satisfaction probability of the constraint cis small randomly generating a value of xthat satisfies the constraint may require a large number of attempts.
instead we use aconstraint solver z3 in this work to generate one or more models for the constraint cto seed the mcmc processes.
figure demonstrates visually the evolution of the proposal distribution through the iterations of the sympais loop line towards the optimal proposal distribution depicted in figure .
we use a projection of the torus constraint on the x y plane again as an example.
at the beginning t the process is initialized with a solution produced by z3.
the importance sampling proposal distribution is concentrated around that point where darker shadows of blue represent higher probability density.
at iteration t the proposal distribution translated towards the inner border of the solution space where the constrained input distribution p x hassymbolic parallel adaptive importance sampling for probabilistic program analysis 505yt t x 505yt xt .
figure graphical illustration of learning the adaptive proposal in sympais.
a higher density.
the white dots represent the samples xn tfrom the mcmc processes that are also used to refine the mean vectors of the importance sampling proposal qn t. proceeding through the iterative refinement at iteration t the proposal distribution approximates the optimal proposal very closely.
the red line in the rightmost subfigure shows a trajectory a sequence of values generated by one of the mcmc processes which touches portions of the solution space approximately proportionally to their density in the optimal distribution.
the accurate approximation of the optimal proposal distribution allows sympais to effectively sweep the solution space of cand estimate its satisfaction probability.
correlated input distributions.
the adaptive importance sampling strategy as well as the mcmc processes described in this section do not require the input distributions to be independent.
correlated distributions such as the bivariate gaussian in listing can be seamlessly processed by sympais.
the requirement for rwmc is the ability to evaluate the pdf of the distribution while hmc requires its differentiability.
computing the cdf and its inverse as required for qcoral s stratified sampling does instead involve an integration problem that usually has no analytical solution and requires a separate monte carlo integration.
sympais thus complements qcoral to allow the probabilistic analysis of a broader range of programs.
we will demonstrate applications of sympais to correlated input distributions in section .
.
optimizations the target distribution of sympais is p x p x 1pc x where the indicator function zeroes the input distribution s density outside the solution space of c. however the mcmc processes are not aware of the geometry or location of the solution space of c. the volume and shape of the solution space may affect the rejection rate of the processes how often the random walk reaches non solution points and may induce multiple modes in p x even ifp x is unimodal.
intuitively each mode is a peak in the density functionofp x which behaves as an attractor for the mcmc processes requiring a longer time to converge to covering all the modes.
example.
figure shows how the optimal proposal distribution q x for a unimodal correlated input distribution p x we used a student s t distribution with degrees of freedom for the plot degenerates into a bimodal optimal proposal when constrained within the solution space of c torus constraint projected to the x y plane .
an mcmc process initialized in the neighborhood of one of the mode may take a long time before jumping in the neighborhood of the other mode having to traverse a low probability path across the non convex solution space.
x 505yp x xq x figure left unimodal correlated input distribution p x .
right optimal bi modal proposal distribution q x .
while a complete characterization of c s geometry is intractable in this section we propose three heuristic optimizations that may mitigate the impact of ill geometries of the solution space on sympais adaptive importance sampling.
.
.
diverse initial solutions.
for an effective importance sampling the adaptive proposal distribution should capture all the modes ofq x .
running multiple mcmc processes in parallel increases the chances of at least any of them covering each mode.
in the statistical inference literature each chain is typically initialized with an independent random sample from the input distribution to maximize the chances of reaching all the modes on the whole.
however as discussed before if the satisfaction probability of cis small it is unlikely to randomly generate valid solutions and even less likely to also cover multiple modes.
initializing all the chains with a feasible solution generated by the constraint solver may result in the mcmc processes exploring in a finite time only the mode closest to the initial solution.
we observed this phenomenon in particular for rwmh but multimodal distributions require longer convergence times also with hmc.
the top row in figure shows the evolution of the adaptive importance sampling proposal q x of sympais initialized with a single solution from the constraint solver.
this time the optimal proposal distribution q x is the one on the right hand side of figure .
after t 100iterations q x still fails to converge to q x with most of the samples still generated around one of the two modalities which have instead the same density in q x .
to mitigate this problem sympais tries to generate multiple diverse solutions of cto increase the probability of obtaining at least one in the neighborhood of each mode.
we explored two different approaches for this purpose.
in principle initial solutions diversity can be achieved using an optimizing solver where eachyicheng luo antonio filieri and yuan zhou 505single solution yt t t 505diverse solution y x 505re sample y x x figure convergence of the adaptive proposal distribution with different initialization strategies.
solution is chosen to maximize the distance from all the previous ones.
this method is general and flexible e.g.
it allows customizing the distance function based on domain specific information but computationally heavy for non linear non convex constraints.
an alternative more scalable method relies on interval constraint propagation and branch and bound algorithms to single out regions of the input domain that satisfy c. in our implementation we use realpaver s depth first search mode with a coarse accuracy 2in this example but the configuration can be tuned based on the length of the domain dimensions to generate several boxes that contain solutions of c. this mode differs from the standard paving used in qcoral because it does not require the computed boxes enclosing all the solutions but potentially only a subset of them making it more scalable even for higher dimensional problems.
for each box sympais calculates the center and if it satisfies c adds it to the mcmc initialization points.
an example of the sympais adaptive proposal distribution using this heuristic is shown in the middle row of figure .
the initial solutions cover with different concentrations several parts of the solution space of c. while the initial iterations result in a very spread proposal q x contrary to the highly concentrated optimal proposal the diversity of the initial points allows the adaptive refinement to converge to covering both optimal proposal s modes.
while the heuristics of using depth first interval constraint propagation may fail to produce solutions at the center of its boxes or to refine the boxes enough for it to happen when applicable it is an efficient alternative to solving heavier optimization problems.
.
.
re sampling.
the diversification of the initial solutions described in the previous section uses only information about the constraint to generate diverse initial points.
however the constraint solver is not aware of the underlying input distribution and may generate many solutions that are far from the modes of q x .when thenparallel mcmc processes are initialized with solutions taken uniformly at random from those generated by constraint solver many of these solutions are likely to be far from the modes therefore requiring longer warmup of the markov chains to move towards the modes.
this can be observed in the wide spread of the proposalq x fort 10in the middle row of figure .
to reduce the number of samples used for warmup we sample the initial solutions proportionally to their likelihood in the input distribution.
let xi f i 1be the initial solutions found by the constraint solver.
we sample ninitial points x i n i 1from q x f i 1wi xi x wi p xi f j 1p xj to be the initial states for the nparallel mcmc chains.
the initial solutions seeding the mcmc chains now reflect both the location of the solutions space from the constraint solver and the distribution of the input probability across the solution space from the resampling.
an example of the effects on the convergence speed of sympais adaptation is shown in the bottom row of figure .
.
.
truncated kernel for rwmh.
the proposal kernel for rwmh defined in the previous section generates the next sample by adding gaussian noise to the current one x x n x x .
while arbitrarily concentrated around xby the value of this kernel may propose new samples far away from the solution space of c which would then be rejected.
a way to reduce the rejection rate is to replace the kernel with a gaussian noise truncated within a smaller region of the input domain that contains the solution space of c. we obtain this region in the form of the smallest n dimensional box that contains c. such a box can be efficiently computed when an interval contractor function is defined for c .
efficient interval contractors are implemented for a broad class of numerical constraints e.g.
and can be used to compute the smallest box bthat encloses the solutions space of c. the gaussian proposal kernel of rwmh can then also be replaced by the truncated gaussian proposal kernel b x x nb x x to increase the probability of generate candidate samples x that are still solutions of c. notice that this optimization does not require us to truncate the input distribution p x as required by qcoral .
instead it truncates the uncorrelated gaussian distribution of the proposal kernel of rwmh which can be done efficiently.
notably the rejection rate does not go to zero because bmay be a coarse bounding of the solution space of c which may also be non convex.
nonetheless it usually increases the probability of sampling solutions of c. evaluation in this section we report an experimental evaluation of sympais.
we include direct monte carlo dmc estimation as a baseline qcoral for the uncorrelated input distributions and sympais and sympais h where we configure sympais to use the rwmh and the hmc algorithms for the mcmc samples respectively.
we include two geometrical microbenchmarks to expose the features of sympais and six benchmarks from path conditions extracted from a relu neural network and subjects from qcoral .
because thesymbolic parallel adaptive importance sampling for probabilistic program analysis d10 1raedmc qcoral sympais sympais h .
.
.
no.
of samples figure rae and convergence rates for spheres of different dimensionality.
dependability of monte carlo estimators variance depends on the convergence of their estimates cf.
section overview we use the relative absolute error rae to compare the estimates against the ground truth instead of performing statistical tests on the variance that are particularly challenging with the rare events considered in this paper.
details about the experimental environment and set up are available in the extended version .
code is available at .
.
geometrical microbenchmarks .
.
sphere.
the first constraint we consider the d dimensional sphere c x c where x d rdis the input domain and c rdis the center of the sphere.
we use p x n i i.e.
uncorrelated gaussian as the input distribution and set c .
despite its simplicity this problem illustrates the challenges faced by direct monte carlo methods as well as qcoral in high dimensionality problems where c s satisfaction probability is small.
specifically as dincreases the probability of the event happening decreases which makes estimation by dmc increasingly challenging.
moreover the increase in dalso leads to coarser paving of d dimensional boxes which reduces the effectiveness of variance reduction via stratified sampling.
the rae results are illustrated in figure left .
as expected dmc achieves the worst performance throughout all tests.
for lowdimensional problems d qcoral is the most efficient while its performance deteriorates significantly when the dincreases and realpaver fails to prune out large portions of the domain that contain no solutions.
sympais s performance is comparable to qcoral in low dimensions but up to one order of magnitude more accurate when the dimensionality grows d .
figure right shows the convergence rate of rae for different methods over sample size for d .
sympais achieves the final rae of dmc with of the sampling budget and the final rae ofqcoral with .
simpais h only marginally outperforms sympais for d .
the improvement in sample efficiency becomes more significant for d .
.
.
torus.
torus is a three dimensional constraint introduced in section as a running example.
we evaluate the different methods for both independent and correlated inputs.
independent inputs.
we first consider the uncorrelated input distributionp x n .5i with input domain x r3.figure left shows the rae performance of the four methods.
while performing marginally better than the baseline dmc qcoral achieve poor performance on this non convex subject because realpaver fails to effectively prune out the inner empty region of the domain within the torus effectively reducing qcoral to a dmc sampling over most of the input domain.
realpaver can be fine tuned for a torus constraints by using different consistency configurations see for instructions on the matter .
however this may require human ingenuity to select and tune the correct settings.
finally we observed the performance of realpaver varies for equivalent formulations of the constraint e.g.
x2vsx xor reformulating the constraint without sqrt .
we conjecture that using different interval constraint propagation algorithms or clever simplifications of the constraint may improve the performance of qcoral for this problem.
both variations of sympais achieve an order of magnitude lower rae.
dmc qcoralsympaissympais h0.
.
.10raeindependent dmc sympaissympais h0.
.
.10correlatedtorus figure rae comparison for torus.
correlated inputs.
consider the correlated distribution p x y z t2 .
n x .
n x .
wheret2denotes a student s t distribution with degrees of freedom.
similarly to the situation illustrated in figure the distribution constrained within the solution space of torus is bi modal.
in this case the input distribution is correlated with yandzprobabilistically dependent on x. correlated and potentially multimodal input distributions are commonly used to describe real world inputs arising from physical phenomena.
more recently the success of deep learning has encouraged incorporating deep neural networks for generative modeling of high dimensional data distributions trained from observed data.
for these distributions the pdfs are often tractable while cdfs often are not.
this in turn makes the stratified sampling and truncations of the input distribution intractable for qcoral .
on the other hand sympais can handle these problems because it only requires evaluating the pdf of the input distribution not its cdf.
since qcoral cannot handle correlated inputs figure right shows how both sympais and sympais h outperforms the baseline dmc by nearly one order of magnitude.
as discussed in section .
seeding the mcmc processes with re sampled diverse solutions from the constraint solver improves sympais h s convergence for multimodal distributions as in this experiment.yicheng luo antonio filieri and yuan zhou path constraint id10 1raeacas xu dmc qcoral sympais sympais h figure rae comparison for acas xu activation patterns.
.
acas xu acas xu is a benchmark neural network implementing a safety critical collision avoidance system for unmanned aircraft control.
its inputs are readings from a set of sensors including distance from the other vehicle angle of the other vehicles relative to ownship direction heading angle of other vehicle speed of ownship and speed of the other vehicle.
the outputs of the networks are either clear of conflict no risk of collision between ownship and the other vehicle or one of four possible collision avoidance maneuvers the ownship can take to avoid a collision.
the us federal aviation administration is experimenting with an implementation of acas xu to evaluate its safety for replacing the current rule based system .
this subject is has been used to benchmark several verification methods e.g.
including performing a probabilistic robustness analysis that computes bounds on the probability of the network producing inconsistent decisions for small perturbations on the inputs .
a central component of the analysis method in consists of computing reliable bounds for the probability of satisfying a constraint that corresponds to a unique activation pattern of the network when white noise is added to an initial sensor reading.
for this experiment we extract the constraints corresponding to six random activation patterns and estimate their satisfaction probability with different methods.
consider a neural network with one hidden layer of mneurons that receives input x rd.
the neural network computes the output as z w0x b0 a relu z y wt 1a b1 where relu is the rectified linear unit defined as relu x max x evaluated component wise on x.w0 w1 b0andb1are the pre trained weights of the neural network.
a hidden unit aiis active if the constraint zi 0is satisfied and inactive otherwise i.e.
zi .
an activation pattern is defined as the conjunction of the activation constraints of the hidden units ai m i .
we select the network with one hidden layer of five neurons for analysis the selected network generates possible combinations of activation patterns and we select randomly six activation patterns for analysis.
we use n to model the distribution of each input dimension xiof the neural network and additionally impose a domain of rfor each dimension.
the bounded input and independent constraints allow the use ofqcoral as well.
however neural networks tend to generate highdimensional problems because they establish control dependencies among all their inputs which prevents effective constraint slicing.
figure reports the rae achieved by the different estimation methods for each of the six randomly sampled activation patterns.
for this experiment we used a single initial solution computed by z3 to initialize the mcmc chains of sympais and sympais h. being the conjunction of relu activations the constraints produced by acas xu are convex intersection of half planes and do not induce multiple modes on the constrained input distribution.
already using a single initial solution sympais converged to better estimates than dmc and qcoral with the same sampling budget.
.
volcomp finally in this section we experiment with a set of constraints from the benchmark volcomp also used to evaluate qcoral .
we picked the first five path conditions for each of the subjects named in figure from the public qcoral replication package.
because most of the input variables in these subjects are computationally independent constraint slicing would reduce to constraints with dimensionality we instead skip slicing and evaluate the original constraints having between and variables.
the constraints are linear with convex solution spaces.
in this situation realpaver can produce a tight approximation of the solution space with significant benefits for qcoral s stratified sampling efficiency.
the input cdf can be computed analytically independent truncated gaussian from qcoral s replication package normal .
among the different experiments in these subjects represent a sweet spot for the stratified sampling method of qcoral and are included here as sympais s worst case comparison scenario.
our current implementation of the hmc kernel proposal does not support jit compilable truncated distributions.
thus we run only sympais with rwmh for this set of subjects.
figure shows the rae of the different methods.
the ground truth is computed with mathematica.
for all the subjects in this experiment except cart of the input domain enclosed within realpaver s boxes contains only solutions of the constraint.
for all the subjects except framingham qcoral and sympais produce comparable rae.
a deeper inspection of framingham showed that most of the constraint is effectively the intersection of boxes which are identified as inner boxes by realpaver and require no further sampling for probability estimation equation .
the experiment demonstrates that while the adaptive importance sampling strategy of sympais is designed to estimate the satisfaction probability of high dimensional constraints with multimodal correlated input distributions it can match the performance of stratified sampling for most simpler problems where stratified sampling can be applied.
the analysis of the same constraints with correlated inputs would instead not be possible with stratified sampling.
related work probabilistic symbolic execution relies on symbolic execution to extract the path conditions characterizing the inputs that lead to the occurrence of a target event the probability of satisfying the path condition constraints given an input distribution is then quantified using model counting or solution space quantification methods.symbolic parallel adaptive importance sampling for probabilistic program analysis 1raecart 01234carton path constraint idckd epi 01234ckd epi simple 01234framingham dmc qcoral sympais figure rae achieved by the different methods on volcomp subjects.
pse has been applied in several domains including reliability security and performance analyses with variations implemented for nondeterministic and probabilistic programs.
quantification methods have been proposed for uniform or discretized distributions over linear integer constraints string constraints bounded data structured and numerical constraints over continuous input distributions .
in the probabilistic programming literature chaganty et al .
proposed breaking a probabilistic program with branches and loops into small programs focused on only some execution paths and use pre image analysis to perform efficient importance sampling.
differently from chaganty et al .
we use pi mais and mcmc processes to further adapt the proposal distributions for the analysis of individual path conditions.
nori et al .
similarly uses the idea of pre image analysis to design a proposal distribution that generates samples that are less likely to be rejected in mcmc.
these analyses complement our approach and can potentially be incorporated to improve our mcmc scheme.
recent work by zhou et al.
motivates the decomposition into subproblems by considering universal probabilistic programs with stochastic support i.e.
depending on the values of the samples the program may take on different control flow paths and the number of random variables evaluated along each path varies as a result.
this makes designing a proposal distribution for efficient mcmc difficult.
zhou et al.
approaches this issue by decomposing the problem into small straight line programs slps for which the support is fixed and posterior inference is tractable.
however differently from pse approaches slps are execution paths sampled via a specialized mcmc algorithm which adds an additional degree of uncertainty to the results of probabilistic analysis and is not suitable for the analysis of rare events.
conclusions and future work we introduced sympais a new inference method for estimating the satisfaction probability of numerical constraints on highdimensional correlated input distributions.
sympais combines a sample efficient importance sampling scheme with constraint solvers to extend the applicability of probabilistic symbolic execution to a broader class of programs processing correlated inputs that cannot be analyzed with existing methods.
while we currently implemented only rwmh and hmc kernel as adaptive proposals sympais can be extended with additional kernels to improve its performance on different classes of constraints.
finally it is also worth investigating the integration of kernels and parametric importance sampling proposals for discrete distributions aiming atsupporting integer input variables that cannot be analyzed with our current algorithms.