detectingnumericalbugsin neuralnetwork architectures yuhaozhang key laboratory of highconfidence softwaretechnologies moe department of computer science and technology peking university beijing pr china yuhaoz cs.wisc.eduluyao ren key laboratory of highconfidence softwaretechnologies moe department of computer science and technology peking university beijing pr china rly pku.edu.cnliqianchen key laboratory of software engineeringforcomplexsystems collegeof computer national universityof defensetechnology changsha pr china lqchen nudt.edu.cn yingfeixiong key laboratory of highconfidence softwaretechnologies moe department of computer science and technology peking university beijing pr china xiongyf pku.edu.cnshing chi cheung department of computer science and engineering the hong kong university ofscience and technology hong kong pr china scc cse.ust.hktaoxie key laboratory of highconfidence softwaretechnologies moe department of computer science and technology peking university beijing pr china taoxie pku.edu.cn abstract detectingbugsindeeplearningsoftwareatthearchitecturelevel provides additional benefits that detecting bugs at the model level does not provide.
this paper makes the first attempt to conduct staticanalysisfordetectingnumericalbugsatthearchitecturelevel.
weproposeastaticanalysisapproachfordetectingnumericalbugs in neural architectures based on abstract interpretation.
our approach mainlycomprisestwokindsofabstractiontechniques i.e.
one for tensors and one for numerical values.
moreover to scale up while maintaining adequate detection precision we propose two abstraction techniques tensor partitioning and elementwise affine relation analysis to abstract tensors and numerical values respectively.
we realize the combination scheme of tensor partitioningandaffinerelationanalysis togetherwithintervalanalysis as debar and evaluate it on two datasets neural architectures withknownbugs collectedfromexistingstudies andreal world neural architectures.
the evaluation results show that debar outperforms other tensor and numerical abstraction techniques on accuracy without losing scalability.
debar successfully detects all known numerical bugs with no false positives within .7 2. seconds per architecture.
on the real world architectures debar reports warnings within .6 135. seconds per architecture where 299warnings are true positives.
corresponding author permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 8 13 virtual event usa associationfor computing machinery.
acm isbn ... .
concepts software and its engineering formal software verification computingmethodologies neuralnetworks .
keywords neuralnetwork static analysis numerical bugs acmreference format yuhaozhang luyaoren liqianchen yingfeixiong shing chicheung and tao xie.
.
detecting numerical bugs in neural network architectures.inproceedingsofthe28thacmjointeuropeansoftwareengineering conferenceandsymposium on the foundationsof softwareengineering esec fse november 8 13 virtual event usa.
acm new york ny usa 12pages.
introduction the use of deep neural networks dnns within software systems which are named as dl software systems is increasingly popular supporting critical classification tasks such as self driving facialrecognition andmedicaldiagnosis.constructionofsuchsystemsrequirestrainingadnnmodelbasedonaneuralarchitecture scriptedbyadeeplearning dl program1.toeasethedevelopment of dl programs the developers popularly adopt various dl frameworks such as tensorflow.
a neural architecture i.e.
a network oftensorswithasetofparameters iscapturedbyacomputation graphconfigured todoone learning task.
when theseparameters are concretely bound after training based on the given training dataset the architecture prescribes a dl model which has been trainedfor aclassification task.
to avoid unexpected or incorrect behaviors in dl software systems it is necessary to detect bugs in their neural architectures.
although various approaches have been proposed to test or verify dl models these approaches do not address the needs of two types of stakeholders architecture vendors who design and publish neural architectures to be 1a dl program may specify multiple neural architectures each responsible for an assigned learning task.
to ease the presentation we assume that each dl program performsasingletaskwithaneuralarchitectureunlessotherwisestatedinthispaper.
826esec fse november8 virtualevent usa y. zhang l.ren l.chen y. xiong s.cheung t. xie used by other users and developers who use neural architectures to train and deploy a model based on the developers own training dataset.
architecture vendorsneed to provide qualityassurance for their neural architecture.
it is inadequate for the vendors to verifythe architecture withspecific instantiated models whichare dataset dependent.
bugs in a neural architecture may manifest themselves into failuresafterdevelopershavetrainedamodelforhours days orevenweeks causinggreatlossintimeandcomputation resources .thelosscanbepreventedifthesebugscanbe detectedearlyatthearchitecturelevelbeforemodeltraining.
failurescanalsooccurwhendevelopersofadlmodelneed toretraintheirmodelsuponupdatesontrainingdata.these updatescanfrequentlyhappenduringsoftwaresystemdevelopment and deployment e.g.
when the new feedback data iscollectedfrom users .
failures in dl models can be caused by a bug in the dl architecture low qualitytraining data incorrect parameter settings orotherissues.itisnoteasyforthedevelopersto localizethe bug.
inthispaper wepresentthefirstattempttoconductstaticanalysisforbugdetectionatthearchitecturelevel.specifically wetarget numerical bugs an important category of bugs known to have catastrophic consequences.
numerical bugsare challenging to detect often caused by complex component interactions and difficult to be spottedoutduringcode review.
a neural architecture can contain numerical bugs that cause serious consequences.
numerical bugs in a neural architecture manifest themselves as numerical errors in the form of nan inf or crashes during training or inference.
for example when a non zero number is divided by zero the result is inf indicating thatitisaninfinitenumber whenzeroisdividedbyzero theresult is nan indicating that it is not a number.
when a numerical error occurs during training the model trained using the buggy neuralarchitecturebecomesinvalid.anumericalbugthatmanifests only when making inference is even more devastating it can crash thesoftwaresystemorcauseunexpectedsystembehaviorswhen certaininputsare encounteredduringreal systemusage .
detecting numerical bugs via testing is either too challenging atthearchitecturelevelortoolateatthemodellevelasrevealed inpreviousempirical studies .testing anarchitecture ischallengingaswecannotexecutethearchitecture.testingthe trained models is too late to discover the bugs occurring at the training time as statedearlier.
todetect numerical bugsat the architecture level in this paper we propose to use static analysis because static analysis is able tocoverthelargecombinatorialspaceimposedbythenumerous parameters and possible inputs of a neural architecture.
we propose a static analysis approach for detecting numerical bugs in neural architectures based on abstract interpretation which mainly comprises two kinds of abstraction techniques i.e.
one for tensors and one for numerical values.
we study three tensor abstractiontechniques arrayexpansion arraysmashing andtensorpartitioning aswellastwonumericalabstractiontechniques interval abstraction and affine relation analysis.
among these techniques arrayexpansion arraysmashing andintervalabstraction are adapted from existing abstraction techniques for imperative programs .inaddition toachievescalabilitywhilemaintaining adequate precision we propose tensor partitioning to partition tensorsandinfernumerical informationoverpartitions basedon our insight many elements of a tensor are subject to the same operations.inparticular representing concrete tensorelements inapartitionasoneabstractelementunderappropriateabstract interpretation can reduce analysis effort by orders of magnitude.
motivated bythisinsight tensorpartitioning initiallyabstractsall elementsinatensorasoneabstractelementanditerativelysplits eachabstractelementintosmalleroneswhenitsconcreteelements go through different operations.
each abstract element represents onepartitionofthetensor associatedwithanumericalintervalthat indicates the range of its concrete elements.
moreover for the sake of precision besides interval analysis we conduct affine relation analysistoinfertheelementwiseaffineequalityrelationsamong abstract elements representing partitions.
we evaluate the scalability and accuracy of our approach on twodatasets asetof9architectureswithknownnumericalbugs collectedbyexistingstudies andasetof48largereal world neuralarchitectures.
in our evaluation we designcomparative experiments to study three tensor abstraction techniques and two numerical abstraction techniques.
we specifically name the implementation of the combination scheme of tensor partitioning and affine relation analysis together with interval abstraction as debar beingreleasedasopensourcecode2.intermsofscalability the evaluation results show that array expansion is unscalable and ittimesoutin33architectureswithatimebudgetof30minutes while other techniques are scalable and can successfully analyze allarchitecturesin3minutes.intermsofaccuracy debarcould achieve .
accuracy with almost the same time performance compared to array smashing .
accuracy and sole interval abstraction .
accuracy .theseresultsdemonstratetheeffectiveness of tensor partitioning and affine relation analysis together withintervalabstraction.
in summary this paper makes three main contributions astudyofastaticanalysisapproachfornumericalbugdetection in neuralarchitectures with three abstraction techniques for abstracting tensors and two for abstracting numericalvalues.
twoabstractiontechniquesdesignedforanalyzingneural architectures tensor partitioning for abstracting tensors and elementwise affine relation analysis for inferring numericalrelations among tensorpartitions .
anevaluationon9buggyarchitecturesin48real worldneuralarchitectures demonstratingtheeffectivenessofdebar.
overview in this section we explain how our approach detects numerical bugs with an example in listing .
the example is modified from a 827detectingnumerical bugs in neural network architectures esec fse november8 virtualevent usa real worldcodesnippetthatcreatesrectanglesandcalculatesthe reciprocals oftheirareas3.
input center shape tensor whose elements in offset shape tensor whose elements in create rectangles.
6bottomleft center offset 7topright center offset 8rectangle tf.concat axis calculate the reciprocal of their areas.
11bottom left top right tf.split rectangle num or size splits axis 12width right left 13height top bottom 14area width height 15scale tf.reciprocal area listing acodesnippet ofamotivatingexample the program consists of two parts.
the first part defines rectangles each by a central point and an offset vector.
input variablecenterrepresents central points where each point is a elementvectorof32 bitfloats.similarly offsetrepresents100 offset vectors.
then from the central points and the offsets the bottom left points and the top right points are calculated lines 6 7 and are concatenated into a tensor to create rectangles line .
the second part calculates the reciprocals of the areas.
first from rectangle the bottom left top and right coordinates are extracted line each being a shape tensor.
the areas of rectangles are then calculated lines 12 14 followed by the calculation of their reciprocals line .
this program contains a numerical bug that when any offset vector has an element of zero the corresponding area becomes zero and the value of scale becomes nan.
to capture the bug an ideal way is to statically consider all possiblevaluesof centerandoffset andcheckwhetheranyof thevalueswouldresult inazero area.abstract interpretation isaneffectivesolutiontostaticallyconsiderallpossiblevaluesof variables.
itanalyzestheoriginalprogram viaanabstractdomain where eachabstractvaluerepresentsaset ofconcretevalues.to apply abstract interpretation to our problem the key is how to abstractaneuralarchitecture.givenaneuralarchitecture weneed to consider mainly two aspects.
first the numerical values and arithmetic computations need to be abstracted.
second the tensors need to be abstracted.
we first discuss three abstraction techniques adaptedfromexistingworkforanalyzingimperativeprograms and thendescribetwonewtechniquesthatweproposeforanalyzing neuralarchitectures.
.
interval abstraction interval abstraction is a popular abstract interpretation technique for abstracting numerical values where each scalar variable vis represented by an interval indicating the lower bound and theupperbound.theseintervalsarethencalculatedbymapping the standard operations into interval arithmetic.
as a result the following showsthe calculationsperformedbythe analysis.
7708b9df7 research object detection box coders faster rcnn box coder.py l80center allelements have offset allelements have bottomleft allelements have topright allelements have rectangle the first two elements in each row have and the last two elements in each rowhave bottom left allelements have top right allelements have width height allelements have area allelements have to detect numerical bugs one can predefine the safe conditions for various operations e.g.
by restricting the argument not to take a zero value when calling reciprocal .
since zero is included in the interval of any element in area a potential numerical bug is detected.
thefirsttypeofimprecision isintroducedbyintervalabstraction.
in the preceding example we can conclude from the interval ofoffsetthat the elements in areaare within interval which is smaller than the inferred interval .
since both bottomleft andtopright arecalculatedfrom center theeffect ofcenteris nullified when calculating widthandheight.
however such information is lost after the values have been abstracted intointervals.the imprecision mayleadto false alarms.
consider a situation that the elements in offsetcontain values within interval where no numerical error shouldbe triggered.
when analyzing using the interval abstraction we would get for all elements in area leading to a false alarm of numerical bugs.
.
arrayexpansion array expansion is a basic technique for abstractingan array in an imperative program to an abstract domain the elements in the array are one to one mapped to the abstract domain and noabstractionisperformed.mappingarrayexpansionfortensor abstraction with interval abstraction we can also directly map the elements inatensorone to oneto ranges inthe abstract domain.
scalability is the main problem of array expansion.
the reason isthatweneedtorecordanintervalforeachelementinatensor and in the motivating example we need to record intervals forcenterand intervals for offset substantially affecting scalability.asshownbyourevaluationlater analyses using array expansion time out for most real world models with a time budget of30 minutes.
.
arraysmashing arraysmashing isanalternativeabstractiontechniquethatuses oneabstractelementtorepresentallelementsinatensor.inthis way the number ofabstract elementsisgreatly reduced.mapping arraysmashingfortensorabstractionwithintervalabstraction we use one range to cover allelements inatensor.
rectangle bottom left top right width height area the second type of imprecision is introduced by array smashing.
in the preceding example the intervals of center offset bottomleft andtopright remainthesame.nevertheless array 828esec fse november8 virtualevent usa y. zhang l.ren l.chen y. xiong s.cheung t. xie smashing can get the interval of areaas which is less precise than what array expansion gets.
in fact when using array smashing a warning would be reported for any input intervals as the difference between bottomleft andtopright disappears when they are concatenatedinto rectangle .
.
tensorpartitioningandaffine relation analysis to scale up while maintaining adequate precision we propose two techniques for abstracting tensors and numerical values respectively.
the first technique is tensor partitioning which allows a tensortobesplitintomultiplepartitions whereeachpartitionis abstracted as a summary variable and we maintain interval ranges forsuchsummaryvariables.thesecondtechniqueis affinerelation analysis whichmaintainsaffinerelationsbetweenpartitionsand makesuse ofthis relation to achieve more precise analysis.
specifically to support tensor partitioning we maintain the set ofindexesforeachpartition.weuse iatodenotetheindexranges of the partition a and use to denote all included indexes in a dimension.forexample thefollowingshowsthecalculationprocess of the preceding example in tensor partitioning where the names in uppercase represent partitions.
tensor centerhas one partition cincluding all its concrete elements.
to support affine relationanalysis weintroduceasymbolicsummaryvariable ato denote each partition a i.e.
a a whose name is in lowercase.
we use a to denote its corresponding interval.
with these summary variables we maintain affine equality relations among thesesummary variables.
inthemotivatingexample thepartition ccorrespondstoasymbolic summary variable c whose interval range is c .
similarly offsetalso has one partition ocorresponding to an expression o where the interval ois .
next bottomleft is calculatedfrom centerandoffset.sinceboth centerandoffset haveonepartition bottomleft alsohasonepartition bl which correspondstoexpression c othatiscalculatedfrom c o .
followingthisprocess wecancalculatethepartitionsandmaintain theiraffine equalityrelations.
.center ic c c .
c .offset io o o .
o .bottomleft ibl bl c o c o .topright itr tr c o c o .rectangle ir1 r1 bl c o .
ir2 r2 tr c o .bottom ib b r1 c o .left il l r1 c o .top it t r2 c o .right ir r r2 c o .width iw w r l 2o .height ir h t b 2o .area ia a a .
a 2 o 2 o thecalculationisdissimilartointervalabstractionwitharray smashingintwowaysfortheexample.thefirstdifferenceincalculation occurs at lines and .
since rectangle is concatenated from two tensors we keep rectangle as two partitions r1andr2 each corresponding to an argument.
in this way we overcome thesecondtypeofimprecisionbroughtbyarraysmashingwhile keeping the number of abstract elements small.
when splitting rectangle intobottom left top andwidth we can get the preciseintervalsforthesetensorsfromthecorrespondingpartitions inrectangle .
the second difference in calculation occurs at line .
when calculating width we make use of the affine equality relations amongpartitionsof randl andthuswecanpreciselyinferthat width 2orather than onlyan imprecise interval for width.
similarly theintervalfor heightisalsoprecise.inthisway weovercomethefirsttypeofimprecisionduetointervalabstraction.finally wecalculate areabasedon widthandheight.sincetheoperation of 2o 2ois no longer linear we cannot get any affine equality relationsfor area.hence weintroduceasummaryvariable afor the whole area and compute its interval range.
then for areawe getan interval which isthe ground truth interval rangeof area.
approach inthissection wefirstintroducethepreliminariesofabstractinterpretationandtwobasicnumericalabstractdomains theinterval abstract domain and the affine equality abstractdomain.
we then describe our abstractionfor neural architectures using tensorpartitioning forabstractingtensors andnumericalabstractions i.e.
combining intervals with affine equalities for abstracting numerical values .
we then show how to abstract tensor operations under two abstraction techniques tensor partitioning and affine relation analysis as well as interval analysis designed for neural architectures.wealsodiscusstheinitialintervalsforinputrangesand parameterranges.
.
preliminaries .
.
basicsofabstractinterpretation.
inabstractinterpretation concrete properties are described in the concrete domain cwith a partialorder andabstractpropertiesaredescribedintheabstract domainawithapartialorder .wesaythatthecorrespondence betweenconcretepropertiesandabstractpropertiesisagaloisconnection c a with an abstraction function c a andaconcretizationfunction a csatisfying c c a a. c a c a .
to infer the value range for variables in a dl program we need tocomputethepossiblesetsofvaluesthateachvariablecantake.
wedefinetheconcretedomain cofnvariablesas p rn wherean elementis asetof n elementvectors denotingthepossiblevalues thatnvariablescantake.thepartialorderin cisthesubsetrelation over sets.
.
.
abstractdomainofintervals.
theabstractdomainofintervals aiisdefinedas ai ... l u rn .
an element inaican be seen as a pair of two vectors l u where denotes the lower bound and upper bound of the values that thei th variable may take.
given two elements a1 a2 ai 829detectingnumerical bugs in neural network architectures esec fse november8 virtualevent usa we saya1 a2if both have nintervals and each interval in a1is a sub intervalofthe intervalinthe corresponding position in a2.
the abstraction function iof an element c cis defined as i c lc uc ... where lc i minx c xi uc i maxx c xi i n. theconcretizationfunction iofanelement a aiisdefinedas i a x rn i .xi la i ua i where la i ua i isthe intervalrange ofthe i th variable of a. itiseasytoseethattheconcretedomain c andtheinterval abstractdomain ai i formthegalois connection.more details can be foundinthe publication bycousotandcousot .
.
.
abstract domain of affine equalities.
as discussed in section2 we alsomaintain affine relations among variables ina dl program inthe form of summationdisplay.
i 0 ixi 0 wherexi s are variables and i s are constant coefficients inferred automaticallyduringthe analysis.
the abstract domainofaffine equalities aeisdefinedas ae a b a rm n b rm m where amatrix aand acolumn vector bdefine the affine space of nvariables.
an element in aeconstrains variables x rnby an equation ax bdescribing the possible set of values that xcan take.furthermore tohaveacanonicalform werequire a b to be inthe reducedrowechelonform .
the abstractionfunction eofan element c cisdefinedas e c a b a b isinreducedrowechelonform and ax bholdsfor all xinc ifcisthe wholespace otherwise .
the concretization function eof an element a e a b ae isdefinedas e a b x rn ax b .
theconcretedomain c andtheaffine equalityabstractdomain ae e form the galoisconnection c e e ae e .
thedetailsaboutthedomainoperations includingmeet join inclusion test etc.
of the affine equality abstract domain can be foundinthepublication bykarr .wedo notneedawidening operation for thedomain of affine equalities becausethe lattice of affineequalitieshasfiniteheight andthenumberofaffineequalities while analyzing a program is decreasing until reaching the dimensionofthe affine spaceinthe program.
.
abstractionforneuralarchitectures we use tensor partitioning and interval abstraction with affine equalityrelation to abstract tensorsinneuralarchitectures.
.
.
tensorpartitioning.
asdiscussedinsection weintroducea new granularity of array abstraction named tensorpartitioning which is a form of array partitioning also named array segmentation but tailored for tensor operations we partition atensoraintoasetofdisjointpartitions a1 a2 ... an where each partition aiis a sub tensor of a. the number of partitions ofais denoted as na.
the set of array indexes of the cells from partition aiiscontinuous in aanddefinedbycartesianproducts of index intervals for all dimensions denoted as iai.
note that the indexes in iaiare indexes of the corresponding elements in tensora whilewesometimesuse iai.shapetorepresenttheindexes of the corresponding elements in sub tensor ai whereai.shape denotes a tuple of integers giving the size of the sub tensor ai along each dimension.
in our motivating example r2is a partitionofrectangle andir2 r2.shape ir2.shape .forclarity weintroduceanotionof partitioning positions for each dimension to denote the indexes where we partition the tensor inthat dimension.
index iis a partitioning position for a tensor ain dimension piff the element a and the elementa in dimension pbelong to different partitions.
it is worth mentioning that the partitioning positions are easier to infer for dnn implementations than for regular programs e.g.
c programs because the shapes of sub tensors are usually determined syntactically often specified by parameters of tensor operations so that we know the exact boundary of each partition e.g.
tf.concatandtf.splitinour motivatingexample.
afterpartitioning foreachpartition ai weintroduceanabstract summary variable aito subsume all the elements in ai denoted as ai ai.
note that in this paper we use lowercase letters todenotethesummaryvariablesofpartitions sub tensors while uppercaseletters to denote tensors.toperformstaticanalysis we maintainnumericalrelationsamongsummaryvariablesofpartitions withdetails describedinthe nextsubsection.
.
.
intervalabstractionwithaffineequalityrelation.
wecombinetheintervalabstractionandaffineequalityrelationabstraction as our numerical abstraction to infer the value range for scalar variables in the dl programs and also for those auxiliary abstract summaryvariablesintroducedbytensorpartitioning.wecoulduse relational numericalabstractdomains such as polyhedra toinfer inequality relations.
however because many tensor operations induce affine equality relations in this paper we consider only the affineequalityrelationsamongvariables.inaddition affineequality relations are cheap to infer and thus are fit to analyze large dnns.
furthermore because reluoperations are widely used in dnn implementations foreachvariable a weintroduce arelutodenote the resulting variable of relu a i.e.
arelu max a .
considering the way of using reluoperations in dnn implementations in this paper we maintain only the affine equality relations between avariable bandthereluresultofanothervariable aofthesame shape while a bmay be the summary variables of partitions from differenttensors inthe form of b arelu whichcanalsobeexpressedintheformofeq .additionally the reluoperation has aproperty relu a relu a a a r. 830esec fse november8 virtualevent usa y. zhang l.ren l.chen y. xiong s.cheung t. xie we can utilize this property for better analysis precision by addingan additionalequality arelu a relu a wherea reludenotes the result of relu a adding an additionalequality crelu a relu for every equalityinthe form of c a. .
.
abstractdomain forneuralarchitectures.
definition .
.
the abstract domain for tensor partitioning and intervalabstraction withaffine equalityrelationatieis defined as atie p a i a e a i ai a e ae wherep a1 ... an is the set of the disjoint partitions of the tensors and a i a eare the numerical abstract elements over the n summary variables corresponding to the partitions pin the interval domainaiand theaffineequalitydomain ae respectively.
definition3.
.
theconcretizationfunction tieofanelement a p a i a e atieis defined as tie a braceleftbigg ap a1 ... an j1 ... jn j. a1 ... an i a i e a e bracerightbigg whereaisthe tensor constructedby its partitions p a1 ... an and j j1 ... jn i .ji iai.shape .
.
abstractingtensoroperations wenextshowhowtoconstructabstractoperationsbasedontensor partitioning and affine relation analysis together with interval analysis for analyzingthree commontensor operationsin neural architectures.
we provide the construction for other operations in our debar open sourcecode.
toeasethepresentation weillustrateourapproachusingvectors one dimensional tensors and matrices two dimensional tensors .
our approach isgeneralizable to multi dimensional tensors.
.
.
additionandsubtraction.
tensoradditionandsubtraction intheformof c a b taketwoinputtensors a bwiththesame shape to calculatethe resultingtensor c. since the input tensors aandbmay not be partitioned in the same way weshould first align thepartitions of aand thoseof b such that i .iai ibiwherenadenotes the number of partitions of aandbafter aligned.
to align the partitions of aand those ofb for each dimension we take the set union of the two partitioningpositionsasthenewsetofpartitioningpositionsfor aandb.then weputthealignedsetofpartitioningpositionsas that for the resultingtensor c such that cisalignedwith a b. after that for each partition ci we compute the interval range for its summary variable by ci ai bi .
furthermore for tensor addition and subtraction we maintain the elementwise affine equality relations among partitions of a b c. for eachpartition ci we have ci ai bii ... nc figure concatenating tensors horizontally whichmeans j i ci.shape .ci ai bi i ... nc .
forexample supposethatboththeone dimensionaltensors a andbare partitionedintotwopartitions and ia1 ia2 ib1 ib2 a1 a1 a1 a2 a2 a2 b1 b1 b1 b2 b2 b2 .
after aligningpartitionsof aandb we have ia1 ia2 ia3 ib1 ib2 ib3 a1 a1 a1 a2 a2 a2 a3 a3 a3 b1 b1 b1 b2 b2 b2 b3 b3 b3 .
afterc a b forc we have ic1 ic2 ic3 c1 c1 c1 c2 c2 c2 c3 c3 c3 ci ai bii .
.
.
concatenate.
indnnimplementations tensors canbeconcatenated along certain dimension p. an assignment statement c concatenate a b p denotes that two input tensors aandb areconcatenatedtoformanoutputtensor calongdimension p.for example figure 1showsthattwotwo dimensionaltensors aandb areconcatenatedtoformatensor calongdimension0 rows .to handle the concatenate operation first we need to align partitions ofaandbalongallotherdimensionsexceptdimension p.toalign the partitions in each dimension except dimension p we use the set union of partitioning positions of aandbas the new set of partitioningpositionsfor a bandalsothatfor c.indimension p wedonotchangethepartitioningpositionsof aandb whilethe set of partitioning positions of cconsists of a s partitioning positions in dimension p the size of ain dimension p denoted as n representingtheboundarybetween aandb b spartitioning positions indimension p plusn.letna nbdenotethenumber ofpartitionsof a bafter alignment respectively.
forsimplicityofpresentation hereweassumethattwo dimensional tensorsa bareconcatenatedtoformatensor calongdimension i.e.
p .thenforeachpartition ci wecalculateitsinterval range by ci braceleftbigg ai ifi ... na bi na ifi na ... na nb .
831detectingnumerical bugs in neural network architectures esec fse november8 virtualevent usa figure splitting tensors vertically wealsomaintaintheelementwiseaffineequalityrelationsbetween ciandai wheni na as well as relations between ciandbi wheni na as ci aii ... na ci bi nai na ... na nb whichmeans j k i ci.shape .ci ai i na j k i ci.shape .ci bi na na i na nb.
for the example showninfigure suppose ia1 ia2 ib1 ib2 a1 a1 a2 a2 b1 b b2 b where we temporarily use b ito denote the summary variablesfor bihere.afteraligningthe partitionsof aandb we have ia1 ia2 ib1 ib2 ib3 ib4 a1 a1 a2 a2 b1 b1 b1 b b2 b2 b2 b b3 b3 b3 b b4 b4 b4 b .
then after c concatenate a b forc we have ic1 ic2 ic3 ic4 ic5 ic6 c1 c1 c1 a1 c2 c2 c2 a2 c3 c3 c3 b1 c4 c4 c4 b2 c5 c5 c5 b3 c6 c6 c6 b4 ci aii ci bi 2i .
.
.
split.
atensorcanbesplitintosub tensorsalongacertaindimension.moreclearly astatement b ... b n split a n p denotes that ais split along dimension pintonsmaller tensors which are stored in b ... b n .
the statement requires that n evenlydivides a.shape i.e.
thenumberofelementsindimensionpina .forexample figure 2showsthatatwo dimensional tensorais split along dimension rows into sub tensors b1 andb2.tohandlethe splitoperation first weusethefollowing set as the new set of partitioning positions of ain dimension p a.shape n a.shape n ... n a.shape n and align the partitions of awith respect to the new set of partitioning positions.
let nadenote the number of partitions of aafter alignment.then wekeepthesetofpartitioningpositionsof aasthatof b j for all dimensions except p. in dimension p we use the emptysetasthesetofpartitioningpositionsof b j .inotherwords we do not partition b j indimension p. forsimplicityofpresentation hereweassumethattwo dimensional tensorsaare split into sub tensors b ... b n along dimension0 i.e.
p .then consideringanoutputsub tensor b j for eachofits partitions b j i we calculateits intervalrange by b j i ai wherei j na n i.wealsomaintaintheelementwiseaffine equalityrelationsbetween b j iandai wherei j na n i b j i ai i ... na n whichmeans k m b j i.shape .b j i ai i ... na n .
for example consider b b split a in figure and suppose that before this statement ais partitioned into the following twopartitions ia1 ia2 a1 a a2 a where we temporarily use a itodenote thesummaryvariablesfor aihere.after aligningthe partitionsof a we have ia1 ia2 ia3 ia4 a1 a1 a1 a a2 a2 a2 a a3 a3 a3 a a4 a4 a4 a .
then after b b split a forb b we have ib ib ib ib b b b a1 b b b a2 b b b a3 b b b a4 b j i ai i j wherei j i. .
input ranges andparameterranges in previous sections we initialize the intervals as full ranges of the respective types e.g.
for floats.
however intherealworld theinputmayfallintoonlyasmallrange.
forexample anrgbvalueofanimagefallsinto .especially inmanyapplications inputsarenormalizedintoasmallrange e.g.
after the preprocessing step.
similarly the parameters of a neuralnetworkmayalsofallintoasmallrange.forexample many neuralarchitecturesareinitializedwithaweightinitializationfunction which reflects the desired upper bounds and lower bounds of the parameters.
assuming full ranges for these inputs and parametersmayleadtounnecessaryfalsepositives andthusourapproach also allows the usertospecifyinput rangesandparameterranges anduses the user providedranges to initialize the intervals.
evaluation our evaluation aims to answer the following research questions for assessing effectiveness of debar rq1 and studying the techniques indebar rq2andrq3 832esec fse november8 virtualevent usa y. zhang l.ren l.chen y. xiong s.cheung t. xie rq1 is debar effective indetecting numerical bugs?
rq2 how effective are the three tensor abstraction techniques?
rq3 howeffectivearethe twonumericalabstractiontechniques?
.
datasets we collect two datasets for the evaluation.
the first dataset is a set of9buggyarchitecturescollectedbyexistingstudies.thebuggy architecturescomefromtwostudies 8architectureswerecollected byapreviousempiricalstudy ontensorflowbugsand1architecturewasobtainedfromastudyconductedtoevaluatetensorfuzz .
as most of the architectures in the first dataset are small we collect the second dataset which contains architectures from alargecollectionofresearchprojectsintherepositoryoftensorflow research models4.
the whole collection contains projects implemented in tensorflow by researchers and developers for differenttasksinvariousdomains includingcomputervision natural language processing speech recognition and adversarial machine learning.
we first filter out the projects that are not related to specific neural architecturessuch as api frameworks andoptimizers.
we further filter out the projects of which the computation graph cannot be generated due to incomplete documentation or complicated configuration.
as a result projects remain after filtering and some of them contain more than one neural architecture.
overall our second dataset contains a great diversity of neural architectures such as convolutional neural network cnn recurrentneuralnetwork rnn generativeadversarialnetwork gan andhiddenmarkovmodel hmm .notethatwehaveno knowledge about whether the architectures in this dataset contain numerical bugswhen collecting the dataset.
foreveryarchitectureinthetwodatasets weextractthecomputation graph via a tensorflow api.
each extracted computation graph is represented by a protocol buffer file5 which provides the operations nodes and the data flow relations edges .
we make computationgraphs publiclyavailable6.
columns1 4intable 1provideanoverviewofthetwodatasets.
column2providesanestimationofthelinesofcodeinthecorresponding dl programs.
column shows the number of operations in the computation graphs and textsumhas the highest number of operations .
moreover column shows the number of parameters trainable weights in the dnn architectures and lm 1bhas the largestnumber ofparameters .04g .
.
setups ofinput range andparameter range in ourevaluation we conservativelyprovidetheinputranges.
as described in section .
we get the initial input ranges from the physical meaning of inputs and derive input range information from the preprocessing programs typically written for the training 7708b9df7 research we setthe inputranges to .
wedeterminetheparameterrangeswiththeweightinitialization functions.
if the parameters are initialized to zero we set their rangesasdefault values .wealsoprovidesomeheuristics foruninitializedparameters setting varianceto andsetting countandstepto .otherwise weset the parameterranges to .
we provide the setups for each architecture in our debar open sourcecode.
.
unsafe operationsto check by investigating the dataset in a previous empirical study we collect a list of unsafe operations shown in table .
these operationshavethemostfrequentoccurrencesandhaveahighpotential to cause numerical errors.
in this paper we use our static analysis approach based onabstract interpretationtocheckwhether these operations can cause numerical errors.
specifically after performingouranalysis wecangettheintervalrangefortheparameter xof the operations denoted as .
then we check the unsafe constraints listed in table .
if the unsafe constraints for an operation are satisfiable our checker issues an alarm for indicating that the operation may cause numerical errors.
otherwise the operation is safe.
in table mfandmf respectively denote the largest non infinity floating point number and the smallest non zero positive floating point number that the used floating point format e.g.
bit bit can represent exactly.
indnnimplementations therearemanyoperations e.g.
the multiplication that may lead to numerical bugs.
our approach s current implementation checks onlythose operationslisted intable2 but can be easily extendedto otheroperations.
.
measurements our approach checks every operation that may lead to a numerical error and determines whether a warning should be reported.
to measuretheeffectivenessofourapproach wetreatitasaclassifier thatclassifieswhethereachoperationisbuggy andevaluatesits effectiveness using the number of true false positives negatives and accuracy.
more concretely true false positives refer to the warningsthatare not indeedbugs true false negativesreferto thosecorrect buggy operationswherenowarningisreported and accuracyiscalculatedusingthefollowingformula where tp fp referstotrue falsepositiveand tn fnreferstotrue falsenegative.
accuracy tp tn tp tn fp fn forthefirstdataset werefertouserpatchestodeterminewhether awarning isabug.for the seconddataset 204truepositivesareconfirmedbyexecutingthearchitectureunderanalysisusingthedesignedinputsandparameters to trigger the numerical errors.
52truepositivesareconfirmedbythedeveloper provided fixes notmergedyet inthe issuediscussion.
true positives are confirmed when two authors of this paperseparatelydoreasoningoneachcomputationgraph andbothauthorsconcludethateachwarningistruepositive.
833detectingnumerical bugs in neural network architectures esec fse november8 virtualevent usa table datasetoverview andresults name loc ops params tpdebar array smashing soleintervalabstraction tn fp acc time tn fp acc time tn fp acc time tensorfuzz 178k .
.
.
.
.
.
github ips .05m .
.
.
.
.
.
github ips .6k .
.
.
.
.
.
github ips .6k .
.
.
.
.
.
stackoverflow ips .28m .
.
.
.
.
.
stackoverflow ips .28m .
.
.
.
.
.
stackoverflow ips .28m .
.
.
.
.
.
stackoverflow ips 407k .
.
.
.
.
.
stackoverflow ips .85k .
.
.
.
.
.
ssd mobile net v1 .3m .
.
.
.
.
.
ssd inception v2 100m .
.
.
.
.
.
ssd mobile net v2 .3m .
.
.
.
.
.
faster rcnn resnet 50 .4m .
.
.
.
.
.
deep speech .131k .
.
.
.
.
.
deeplab .1m .
.
.
.
.
.
autoencoder mnae 944k .
.
.
.
.
.
autoencoder vae .41m .
.
.
.
.
.
attention ocr .74m .
.
.
.
.
.
textsum .5m .
.
.
.
.
.
shake shake 32 .85m .
.
.
.
.
.
shake shake 96 .4m .
.
.
.
.
.
shake shake 112 .3m .
.
.
.
.
.
pyramid net .6m .
.
.
.
.
.
sbn .21m .
.
.
.
.
.
sbnrebar .21m .
.
.
.
.
.
sbndynamicrebar .61m .
.
.
.
.
.
sbngumbel .98m .
.
.
.
.
.
audioset 216m .
.
.
.
.
.
learning to remember .30m .
.
.
.
.
.
neural gpu1 .68m .
.
.
.
.
.
neural gpu2 .35m .
.
.
.
.
.
ptn 145m .
.
.
.
.
.
namignizer 652k .
.
.
.
.
.
feelvos .0m .
.
.
.
.
.
fivo srnn 357k .
.
.
.
.
.
fivo vrnn 365m .
.
.
.
.
.
fivo ghmm .
.
.
.
.
.
dcb var bnn .0k .
.
.
.
.
.
dcb neural ban .0k .
.
.
.
.
.
dcb bb alpha nn .0k .
.
.
.
.
.
dcb rms bnn .0k .
.
.
.
.
.
adversarial crypto .14k .
.
.
.
.
.
sentiment analysis .39m .
.
.
.
.
.
next frame prediction .70m .
.
.
.
.
.
minigo .4k .
.
.
.
.
.
compression entropy coder .0k .
.
.
.
.
.
lfads 928k .
.
.
.
.
.
lm 1b .04g .
.
.
.
.
.
swivel .0k .
.
.
.
.
.
skip thought 377m .
.
.
.
.
.
video prediction .6m .
.
.
.
.
.
gan mnist .7m .
.
.
.
.
.
gan cifar .3m .
.
.
.
.
.
gan image compression .5m .
.
.
.
.
.
vid2depth .6m .
.
.
.
.
.
domain adaptation .01m .
.
.
.
.
.
delf .10m .
.
.
.
.
.
total .
.
.
.
.
.
since our approach does not have false negatives by design we omit this column inreportingour evaluation results.
.
implementation andhardwareplatform wehaveimplementedourdebartoolinpython.allofourmeasurementsare performedonaserverrunningubuntu16.
.6ltswith a geforce gtx ti gpu and i7 8700k cpu running at .70ghz.
.
rq1 effectiveness ofdebar .
.
setup.
toanswerrq1 weinvokedebaronthetwodatasets andcheckthenumberoftrue falsepositives negatives accuracy andexecutiontime inseconds .
834esec fse november8 virtualevent usa y. zhang l.ren l.chen y. xiong s.cheung t. xie table operationsto check operations unsafeconstraints exp x expm1 x log mf x log x x mf log1p x x mf realdiv y x x mf x mf reciprocal x x mf x mf sqrt x x mf rsqrt x x mf .
.
results.
columns 5 9 of table 1show the results.
we make the following observations aboutdebar.
it detects all knownnumerical errorson the architectures inthe firstdataset withzerofalse positive.
it detects previously unknown operations that may lead to numerical errors in the real world architectures from the second dataset.
note that a numerical bug can trigger multiplenumericalerrorsatdifferentoperations e.g.
failingto normalizeaninputtensorthatisusedinmultipleoperations.
it correctly classifies operations with only false positives achievingaccuracyof93.
.
it is scalable to handle the real world architectures all of which are analyzed in minutes and the average time is .1seconds.
tounderstandwhydebargeneratessomefalsepositives we investigatethe false positives fps and find the following reasons.
someoperationsdependonanargumenttoindexthetensor elementsfortheoperations.forexample function gather returnselementsinatensorbasedonanargumentthatspecifiestheelements indexes.sincewedonotknowbeforehand which indexes are subject to an operation we merge the intervals atallpossibleindexes leadingtoimprecision.
fpsbelong to this category.
ouraffinerelationanalysisworksononlylinearexpressions.
when a non linear operation is used we create a new abstract element leading to imprecision.
fps belong to this category.
fpsbelong to both ofthe preceding twocategories.
forwhileloopsin rnns wedonotusetensorpartitioning and elementwise affine equality relations but use the classic kleene iteration together with the widening operator in the interval abstract domain leading to imprecision.
fps belong to this category.
the tensorflow api used to extract computation graphs fails to analyze the shapes ofsometensors leadingto 1fp.
.
rq2 studyon tensorabstraction .
.
setup.
tostudythreetensorabstractiontechniques wecompare array smashing and array expansion with tensor partitioning by fixing the numerical abstraction as affine relation analysis used togetherwithintervalabstraction .
.
.
results.
columns 10 13 of table 1show the results of array smashing andtable 3shows the resultsof array expansion.sincetable results ofarrayexpansion namearray expansion tn fp acc time tensorfuzz .
github ips .
github ips .
stackoverflow .
stackoverflow .
autoencoder mnae .
.
autoencoder vae .
.
sbn .
.
sbnrebar .
.
sbngumbel .
.
learning to remember .
.
neural gpu1 .
.
neural gpu2 .
.
namignizer .
.
fivo srnn .
.
fivo ghmm .
.
dcb var bnn .
.
dcb neural ban .
.
dcb bb alpha nn .
.
dcb rms bnn .
.
adversarial crypto .
.
sentiment analysis .
.
mingo .
.
compression entropy coder .
.
arrayexpansiontimesouton33ofthesubjectswithatimebudgetof 30minutes wereportonlytheresultsontheremaining24subjects.
from the tables we make the following observations.
compared to array smashing debar even runs faster indicatingthattheoverheadoftensorpartitioningissonegligible that the overhead is dominated bythe random error of execution time and debar successfully eliminates more false positives improving the total accuracy from .
to .
.
comparedtoarrayexpansion theanalysisofdebarruns seconds to hundreds of seconds faster and does not lose any accuracy on all the subjects that array expansion can analyze within the time budget of minutes.
these observations confirm that tensorpartitioning is more effective thanthe othertwotensorabstractiontechniques.
.
rq3 studyon numericalabstraction .
.
setup.
tostudytwonumericalabstractiontechniques we compare sole interval abstraction and affine relation analysis with interval abstraction by fixing the tensor abstraction as tensor partitioning.
.
.
results.
columns14 17oftable 1showtheresultsofdebarandsoleintervalabstraction.debarhasanegligibleoverhead .
on average and eliminates false positives improving the 835detectingnumerical bugs in neural network architectures esec fse november8 virtualevent usa accuracy from .
to .
in total.
these observations indicatethattheaffinerelationanalysisiseffectiveandsubstantially contributesto the overalleffectiveness.
.
threatsto validity the threat to internal validity mainly lies in the implementation of our approach whether our implementation correctly captures our approach.toalleviatethethreat wehavemanuallycheckedallthe warningsreportedbyourapproach andanalyzedthereasonsfor the false positives validating the implementationto someextent.
thethreattoexternalvalidityliesintherepresentativenessof the subjects.
in particular the proportion of false warnings among all warnings heavily depends on the number of numerical bugs in the subjects and maynot be generalizable.
on the otherhand the accuracyismoregeneralizableandthuswechooseaccuracyaspart ofthe metrics inour evaluation.
thethreattoconstructvalidityismainlythatwehavedefinedthe input range and parameter range and realusers may set different ranges from us.
to alleviate this threat we take a very conservative approach such that real users are likely to set only smaller ranges rather than larger.
to further understand the effect of these ranges we conduct two additional experiments to understand the effect of removing these ranges.
we find that after removing all the input ranges the accuracy drops .
percentage points and after removing all the parameter ranges the accuracy drops .
percentage points.
the results suggest that the ranges do affect the accuracyof thedebar tool.
however the effect is relativelysmall and ourconclusionstill holds ingeneral even ifdifferentranges are specified.
related work staticanalysisfortensorflowprograms.
ariadne can detecterrorsatcodecreationtimefortensorflowprogramsinpython byapplyingastaticanalysisframework wala .unlikedebar ariadne cannot detect bugs at the architecture level.
moreover ariadne targets to infer the shapes of tensors and builds a type systemforanalyzingtensorshapes.sinceariadnedoesnottrack the tensorvalues itcannotbe appliedto detectnumerical bugs.
static analysis of dl models.
multiple approaches have been proposed to statically analyze dl models.
reluplex uses satisfiability modulo theory smt to verify properties of dnns.
dutta etal.
proposed an output range analysis fordnnsusing a local gradient search and mixed integer linear programming milp .
lomuscio et al.
used linear programming to analyze the reachabilityofdnns.arecentstudy showsthattheseapproaches cannotscaleuptolargedlmodelsduetothescalabilityissueof existing constraintsolvers.
otherworkbuiltstaticanalyzersbasedonabstractinterpretation.
gehr et al.
proposed ai2 which deploys abstract interpretation zonotope domain to prove safety propertiesofdnns.singh et al.
proposed deeppoly for using floating point polyhedra and affine transformations to improve the scalability and precision of analysis.
singh et al.
proposed refinezero for using the zonotope domain and milp to further improve the precision of analysis.li etal.
usedthesymbolic propagationtechniqueto improveprecision.unlikedebar theseapproachesaimtoanalyzeneural network models precisely so these approaches all adapt the arrayexpansionstrategy whereeachelementisinstantiatedasa scalar variable.
as shown by our evaluation this strategy is not efficient enough to identify numerical bugs before training.
on the other hand debar incorporates a novel tensor abstraction maps tensorpartitionstoabstractelements anddiscoverslinearequality relations between partitions.
testing dl models.
quite some previous work on testing dl modelsfocuses ontestcoverage criteria for dl models 29 31 .forexample basedoncoveragecriteria odenaetal.
proposed tensorfuzz for using coverage guided fuzzing to test dl models.
such previous work focuses on dl models and does not detectnumerical bugsbefore training.
adversarial examples are often viewed as revealing vulnerabilities of dl models and many approaches focus on finding adversarial examples.
popular adversarial attack approaches such as fgsm c w andpgd usegradient basedtechniquestogenerate adversarialexamplesguidedbyobjectivefunctionswhoseinputs are theparameters.
these approachescannot be easilyadapted to testdlarchitecturesastheobjectivefunctionscannotbecomputed withoutparameters.
testing dllibraries.
cradle was proposed to detect and locatebugsindeeplearninglibraries.incontrast ourdebarapproach targets at dl architectures instead of dl libraries.
array analysis in imperative programs.
our approach is inspired by the existing approaches of abstract interpretation for analyzingarrays in particular arraypartitioning .compared with the existing approachof array partitioning we are the first to generalize this approach from arrays to tensors employ an affine relation analysis for capturing affine equality relations among partitions and design abstract tensor operations for dl architectures such as the abstract operation for relu.
conclusion wehaveproposedastaticanalysisapproachtodetectnumerical bugs in neural architectures.
we specially designed tensor partitioning and affine relation analysis used together with interval abstraction over partitions for our approach and implemented them as debar.
we evaluated our approachon two datasets with various settings on tensor abstraction and numerical abstraction techniques and the results show that debar is effective to detect numerical bugs in real world neural architectures and two specially designed abstraction techniques are essential for improvingthe scalability andaccuracyof detecting numerical bugs.