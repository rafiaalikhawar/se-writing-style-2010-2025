automatic extraction of opinion based q a from online developer chats preetha chatterjee university of delaware newark de usa preethac udel.edukostadin damevski virginia commonwealth university richmond v a usa kdamevski vcu.edulori pollock university of delaware newark de usa pollock udel.edu abstract virtual conversational assistants designed specifically for software engineers could have a huge impact on the time it takes for software engineers to get help.
research efforts are focusing on virtual assistants that support specific software development tasks such as bug repair and pair programming.
in this paper we study the use of online chat platforms as a resource towards collecting developer opinions that could potentially help in building opinion q a systems as a specialized instance of virtual assistants and chatbots for software engineers.
opinion q a has a stronger presence in chats than in other developer communications thus mining them can provide a valuable resource for developers in quickly getting insight about a specific development topic e.g.
what is the best java library for parsing json?
.
we address the problem of opinion q a extraction by developing automatic identification of opinion asking questions and extraction of participants answers from public online developer chats.
we evaluate our automatic approaches on chats spanning six programming communities and two platforms.
our results show that a heuristic approach to opinion asking questions works well .
precision and a deep learning approach customized to the software domain outperforms heuristics based machine learning based and deep learning for answer extraction in community question answering.
index terms opinion question answering system public chats opinion asking question answer extraction i. i ntroduction recognizing the increasing capabilities of virtual assistants that use conversational artificial intelligence ai e.g.
chatbots voice assistants some researchers in software engineering are working towards the development of virtual assistants to help programmers.
they have conducted studies to gain insights into the design of a programmer conversational agent proposed techniques to automatically detect speech acts in conversations about bug repair to aid the assistant in mimicking different conversation types and designed virtual assistants for api usage .
while early versions of conversational assistants were focused on short task oriented dialogs e.g.
playing music or asking for facts more sophisticated virtual assistants deliver coherent and engaging interactions by understanding dialog nuances such as user intent e.g.
asking for opinion vs knowledge .
they integrate specialized instances dedicated to a single task including dialog management knowledge retrieval opinion mining and question answering .
to build virtual assistants for software engineers we need to providesimilar specialized instances based on the available information from software engineers daily conversations.
recent studies indicate that online chat services such as irc slack and gitter are increasingly popular platforms for software engineering conversations including both factual and opinion information sharing and now playing a significant role in software development activities .
these conversations potentially provide rich data for building virtual assistants for software engineers but little research has explored this potential.
in this paper we leverage the availability of opinionasking questions in developer chat platforms to explore the feasibility of building opinion providing virtual assistants for software engineers.
opinion question answering opinion qa systems aim to find answers to subjective questions from user generated content such as online forums product reviews and discussion groups.
one type of virtual assistant that can benefit from opinions are conversational search assistants csas .
csas support information seekers who struggle forming good queries for exploratory search e.g.
seeking opinions recommendations on api tools or resources by eliciting the actual need from the user through conversation.
studies indicate developers conducting web searches or querying q a sites for relevant questions often find it difficult to formulate good queries .
wizard of oz studies have explicitly shown the need for opinions within csas .
a key result of our paper is the availability of opinions on chat platforms which would enable the creation of a sizable opinion q a corpus that could actually be used by csas.
the opinion q a corpus generated from chats by our technique can be used in a few different ways to build a csa matching queries questions asked to the csa with questions from the corpus and retrieving the answers summarizing related groups of opinion q a to generate e.g.
using a gan an aggregate response for a specific software engineering topic.
opinion extraction efforts in software engineering have focused on api related opinions and developer emotions from q a forums developer sentiments from commit logs developer intentions from emails and issue reports and detecting software requirements and feature requests from app reviews .
these studies suggest that beyond reducing developers effort of manual searches ieee acm 43rd international conference on software engineering icse .
ieee on the web and facilitating information gathering mining of opinions could help in increasing developer productivity improving code efficiency and building better recommendation systems .
findings from our previous exploratory study of slack conversations suggests that developer chats include opinion expression during human conversations.
our current study in section ii of developer chat conversations selected from six programming communities showed that of the chat conversations start with a question that asks for opinions e.g.
which one is the best orm that is efficient for large datasets?
what do you think about the onyx platform?
.
this finding shows much higher prevalence of questions asking for opinions in chats than the .
found in emails and .
found in issue reports .
thus we investigate the problem of opinion q a extraction from public developer chats in this paper.
we decompose the problem of opinion q a extraction into two subproblems identifying questions where the questioner asks for opinions from other chat participants which we call posing an opinionasking question and extracting answers to those opinionasking questions within the containing conversation.
researchers extracting opinions from software related documents have focused on identifying sentences containing opinions using lexical patterns and sentiment analysis techniques .
however these techniques are not directly applicable to identifying opinion asking questions in chats for several reasons.
chat communities differ in format with no formal structure and informal conversation style.
the natural language text in chats could follow different syntactic patterns and contain incomplete sentences which could potentially inhibit automatic mining of opinions.
outside the software engineering domain researchers have addressed the problem of answer extraction from community question answering cqa forums by using deep neural network models and syntactic tree structures .
compared to cqa forums chats contain rapid exchanges of messages between two or more developers in short bursts .
a question asked at the start of a conversation may be followed by a series of clarification or follow up questions and their answers before the answers to the original question are given.
moreover along with the answers conversations sometimes contain noisy and unrelated information.
therefore to determine the semantic relation between a question and answer understanding the context of discussion is crucial.
we are the first to extract opinion q a from developer chats which could be used to support se virtual assistants as well as chatbots programmer api recommendation automatic faq generation and in understanding developer behavior and collaboration.
our automatic opinion q a extraction takes a chat conversation as input and automatically identifies whether the conversation starts with an opinion asking question and if so extracts one or more opinion answers from the conversation.
the major contributions of this paper are for opinion asking question identification we designed a set of heuristics learned from the results from ourpreliminary chat analysis to determine if the leading question in a chat conversation asks for opinions.
for automatic answer extraction we built upon related work on non se artifacts to create a deep learning approach customized to the se domain.
we compare against heuristics machine learning combining features and a deep learning technique based on the context of the discussion in community question answering.
this answer extraction model could potentially be leveraged to extract answers from other types of questions in chats.
we evaluated our techniques on developer conversations from six different programming communities on two different platforms slack and irc.
our evaluation results show that we can automatically identify opinion asking questions and extract their corresponding answers within a chat conversation with a precision of .
and .
respectively.
we publish the dataset and source code1to facilitate the replication of our study and its application in other contexts.
ii.
o pinion asking questions in developer online communications since developer chats constitute a subclass of developer online communications we began by investigating whether we could gain insights from work by others on analyzing the opinion asking questions in other kinds of developer online discussions emails issue reports q a forums .
emails.
the most closely related work by di sorbo et al.
proposed an approach to classify email sentences according to developers intentions feature request opinion asking problem discovery solution proposal information seeking and information giving .
their taxonomy of intentions and associated linguistic patterns have also been applied to analyze user feedback in app reviews .
in their taxonomy di sorbo et al.
define opinion asking as requiring someone to explicitly express his her point of view about something e.g.
what do you think about creating a single webpage for all the services?
.
they claim that sentences belonging to opinion asking may emphasize discussion elements useful for developers activities and thus make it reasonable to distinguish them from more general information requests such as information seeking .
of their manually labelled sentences from mailing lists of qt and ubuntu only sentences .
were classified as opinion asking suggesting that opinion asking questions are infrequent in developer emails.
issue reports.
to investigate the comprehensiveness and generalizability of di sorbo et al.
s taxonomy huang et al.
manually labelled sentences from issue reports of four projects tensorflow docker bootstrap vs code in github.
consistent with di sorbo et al.
s findings huang et al.
reported that only .
sentences were classified as opinion asking .
given this low percentage and that 1replication package 1261table i example of opinion asking question and answers on slack python dev channel ques hello everyone i ve requirement where dataset is large like millions records i want to use django rest framework in order to provide that data.
question which one is the best orm which is efficient for large datasets?
.
django orm .
sql alchemy ans sqlalchemy is more performant especially if you re using core.
ans yea you can mix sqlalchemy and django orm.
it s all just python at the end of the day.
however if you swap out one for the other you lose all the benefits of the django orm and how it integrates with the framework.
ans if performance is a factor than use sqlalchemy core to work on this large data set if deadlines are more of a factor that use django orm since you re already use drf.
just make sure to use eager loading on relationships where you can to optimize queries.
opinion asking could be a sub category of information seeking they merged these two categories in their study.
to broaden their search of opinions huang et al.
introduced a new category aspect evaluation defined as express opinions or evaluations on a specific aspect e.g.
i think bs3 s new theme looks good it s a little flat style.
but i think it s cleaner than my old test and i prefer a non js solution personally.?
.
they classified sentences as aspect evaluation .
comparing the two definitions and their results it is evident that although opinions are expressed widely in issue reports questions asking for others opinions are rare.
chats.
chatterjee et al.
s results showing potentially more opinions in slack developer chats motivated us to perform a manual study to systematically analyze the occurrence of opinion asking questions and their answers in developer chats.
dataset to create a representative analysis dataset we identified chat groups that primarily discuss software development topics and have a substantial number of participants.
we selected three programming communities with an active presence on slack.
within those selected communities we focused on public channels that follow a q a format i.e.
a conversation typically starts with a question and is followed by a discussion potentially containing multiple answers or no answers.
our analysis dataset of slack developer conversations consists of conversations from slack pythondev help channel from clojurians clojure from elmlang beginners and from elmlang general all chosen randomly from the dataset released by chatterjee et al.
.
procedure using the definition of opinion asking sentences proposed by di sorbo et al.
two annotators authors of this paper independently identified conversations starting with an opinion asking question.
we also investigated if those questions were answered by others in a conversation.
the authors annotated a shared set of conversations which indicates that the sample size is sufficient to compute the agreement measure with high confidence .
we computed cohen s kappa inter rater agreement between the annotators and found an agreement of .
which is considered to be sufficient .
.
the annotators further discussed their annotations iteratively until all disagreements were resolved.
observations we observed that out of our developer conversations conversations start with an opinion asking question.
there are a total of answers to those opinion asking questions since each conversation could contain no or multiple answers.
table i shows an opinion asking question ques and its answers ans extracted from a conversation on python dev channel.
each of the answers contain sufficient information as a standalone response and thus could be paired with the question to form separate q a pairs.
given that conversations are composed of a sequence of utterances by each of the people participating in the conversation in a back and forth manner the q a pairs are pairs of utterances.
summary compared to other developer communications conversations starting with opinion asking questions in developer chats are much more frequent.
thus chats may serve as a better resource to mine for opinion q a systems.
iii.
a utomatically extracting opinion q a from developer chats figure describes the overview of our approach chateo to automatically extract opinion q a from software developer chat s. our approach takes a developer chat history as input and extracts opinion q a pairs using the three major steps individual conversations are extracted from the interleaved chat history using conversation disentanglement.
conversations starting with an opinion asking question are identified by applying textual heuristics.
possibly multiple available answers to the opinion asking question within the conversation are identified using a deep learning based approach.
a. conversation disentanglement since utterances in chats form a stream conversations often interleave such that a single conversation thread is entangled with other conversations.
hence to ease individual conversation analysis we separate or disentangle the conversation threads in each chat log.
the disentanglement problem has been widely addressed by researchers in the context of irc and similar chat platforms .
we used the best available disentanglement approaches proposed for slack and irc chat logs respectively in this paper.
slack chat logs we used a subset of the publicly available disentangled slack chat dataset2released by chatterjee et al.
since their modified disentanglement algorithm customized for slack developer chats achieved a microaveraged f measure of .
.
irc chat logs we used kummerfeld et al.
s technique a feed forward neural network model for conversation disentanglement trained on 77k manually annotated irc utterances and achieving .
precision and .
recall.
in the disentangled conversations each utterance contains a unique conversation id and metadata including timestamp and author information.
conversation answer extraction from a conver sation chat logs disentanglement heuristics based identification of opinion asking questionsextracted opinion asking q a questi on utterancesbilstm q a pairsse customized word embeddi ngs cnn answ er predictiondeveloper chat logs conver sations conver sations with opinion asking questi ons figure overview of chateo automatic extraction of opinion q a from developer chats b. heuristics based identification of opinion asking question di sorbo et al.
claim that developers tend to use recurrent linguistic patterns within discussions about development issues.
thus using natural language parsing they defined five linguistic patterns to identify opinion asking questions in developer emails.
first to investigate the generalizability of di sorbo et al.
s linguistic patterns we used their replication package of deca3to identify opinion asking questions in developer chats.
we found that out of conversations in our manual analysis dataset only questions were identified as opinion asking questions by deca.
hence we conducted a deeper analysis of opinion asking questions in our manual dataset in section ii to identify additional linguistic patterns that represent opinion asking questions in developer chats.
we followed a qualitative content analysis procedure where the same two annotators authors of this paper first independently analyzed developer conversations to understand the structure and characteristics of opinion asking questions in chats.
the utterances of the first speaker in each conversation were manually analyzed to categorize if they were asking for an opinion.
when an opinion asking question was manually identified the annotator identified the parts of the utterance that contributed to that decision and identified part of speech tags and recurrent keywords towards potential linguistic patterns.
consider the question in table i. first the annotator selects the text that represents an opinion asking question in this case which one is the best orm which is efficient for large datasets?
.
orm is noted as a noun referring to the library and best as an adjective related to the opinion about orm.
thus a proposed linguistic pattern to consider is which one verb tobe verb phrase ?
.
throughout the annotation process the annotators wrote memos to facilitate the analysis recording observations on types of opinions asked observed linguistic patterns of opinion asking questions and researcher reflections.
the two annotators then met and discussed their observations on common types of questions asking for opinions which resulted in a set of preliminary patterns for identifying opinion asking ii most common pattern for opinion asking question identification in chats.
pattern code pany adj description question starting with any and followed by a positive adjective and target noun.
rule any verb phrase definitions good better best right optimal ... project api tutorial library ... example any good examples or things i need to be looking at?
questions in developer chats.
using the preliminary patterns the two authors then independently coded the rest of the opinion asking questions in our manual analysis dataset after which they met to further analyze their annotations and discuss disagreements.
thus the analysis was performed in an iterative approach comprised of multiple sessions which helped in generalizing the hypotheses and revising the linguistic patterns of opinion asking questions.
our manual analysis findings from slack conversations showed that an opinion asking question in a developer chat is a question occurring primarily at the beginning of a conversation and could exhibit any of these characteristics expects subjective answers i.e.
opinions about apis libraries examples resources e.g.
is this a bad style?
what do you think?
asks for which path to take among several paths e.g.
should i use x instead of y?
asks for an alternative solution other than questioner s current solution e.g.
is there a better way?
thus we extended di sorbo et al.
s linguistic pattern set for identifying opinion asking questions by adding additional linguistic patterns.
table ii shows the most common pattern p any adj in our dataset with its description and example question.
most of the patterns utilize a combination of keywords and part of speech tagging.
the annotators curated sets of keywords in several categories e.g.
related to nouns verbs and adjectives respectively.
the complete set of patterns and keywords list are available in our replication package.
1263c.
answer selection from a conversation we build upon work by zhou et al.
who designed r cnn a deep learning architecture for answer selection in community question answering cqa .
since r cnn was designed for application in cqa for the non se domain4 we customize for application in chats for software engineering and then compare to the non customized r cnn in our evaluation.
we chose to build on r cnn because other answer extraction models only model the semantic relevance between questions and answers.
in contrast r cnn models the semantic links between successive candidate answers in a discussion thread in addition to the semantic relevance between question and answer.
since developer chats often contain short and rapid exchanges of messages between participants understanding the context of the discussion is crucial to determine the semantic relation between question and answer.
hence we adapt r cnn to extract the relevant answer s to an opinion asking question based on the context of the discussion in a conversation.
zhou et al.
regarded the problem of answer selection as an answer sequence labeling task.
first they apply two convolution neural networks cnns to summarize the meaning of the question and a candidate answer and then generate the joint representation of a q a pair.
the learned joint representation is then used as input to long short term memory lstm to learn the answer sequence of a question for labeling the matching quality of each answer.
to design chateo we make the following adaptations to account for both the se domain content and specifically software related chats.
first we preprocess the text apply a software specific word embedding model and use those embeddings as input to a cnn to learn joint representation of a q a pair.
we use textcnn since text in chat utterances are much shorter compared to cqa post .
the representations from the cnn are then passed as input to bidirectional lstm bilstm instead of lstm to improve prediction of the answers from a sequence of utterances in a conversation.
we detail chateo answer extraction as follows preprocessing to help chateo with the semantics of the chat text the textual content in the disentangled conversations is preprocessed.
we replace url user mentions emojis and code with specific tokens url username emoji and code respectively.
to handle the informal style of communication in chats we use a manual set of common phrase expansions e.g.
you ve to you have .
we then convert the text to lowercase.
se customized word embeddings text in developer chats and other software development communication can differ from regular english text found in wikipedia news articles etc.
in terms of vocabulary and semantics.
hence we trained custom glove vectors on the most recent stack overflow data dump as of june to more precisely capture word semantics in the context of developer communications.
to train glove vectors we performed standard tokenization and on each stack overflow post s title and text and trimmed extremely rarely occurring words vocabulary minimum threshold of posts window size of words .
our word embedding model thus consists of words where each word is represented by a dimensional word vector.
we applied this custom word embedding model to each word in each utterance of a conversation.
convolutional neural networks in natural language analysis tasks a sentence is encoded before it is further processed in neural networks.
we leverage the sentence encoding technique from textcnn .
textcnn also used for other dialog analysis tasks is a classical technique for sentence modeling which uses a shallow convolution neural network cnn that combines n gram information to model compact text representation.
since an utterance in a chat is typically short words on average we take each utterance as a sentence and apply word embedding multiple convolution and max pooling operations.
the input for textcnn is the distributed representation of an utterance created by mapping each word index into its pre trained embeddings.
each utterance is padded to the same length nwith zero vectors.
let zj2rddenote the d dimensional embedding for the jth word in an utterance.
thus an utterance of length ncan be represented by z1 n z1 z2 ...z n where is the concatenation operator.
to gather local information convolution is achieved by applying a fixed length sliding window kernel wm2rh d on each word position isuch that n h 1convolutional units in the mth layer are generated by cm i wm zi i h bm i ... n h where his the size of convolution kernel is the activation function and bmis the bias factor for the mth layer.
the convolution layer is followed by a max pooling layer which can select the most effective information with the highest value.
the flattened output vectors for each kernel after max pooling are concatenated as the final output.
bidirectional lstm the task of identifying answers in a conversation requires capturing the context and flow of information among the utterances inside a conversation.
hence we use bidirectional long short term memory bilstm where the utterances of a conversation are considered as sequential data.
the input to our bilstm is a sequence of utterance representations created by textcnn.
variations of lstm widely used by researchers for answer extraction tasks are capable of modeling semantic links between continuous text to perform answer sequence learning.
lstm uses a gate mechanism to filter relevant information and capture long term dependencies.
an lstm cell comprises of input gate i forget gate f cell state c and output gate o .
the outputs of lstm at each time step ht can be computed by the following equations 664it ft ct ot3 tanh w xt ht b ct ct it ct ft 1264ht ot tanh ct where xtis the ith element in the input sequence wis the weight matrix of lstm cells bis the bias term denotes sigmoid activation function and tanh denotes hyperbolic tangent activation function denotes element wise multiplication.
bilstm processes a sequence on two opposite directions forward and backward and generates two independent sequences of lstm output vectors.
hence the output of a bilstm at each time step is the concatenation of the two output vectors from both directions h where h and hdenote the outputs of two lstm layers respectively and is the concatenation operator.
iv .
e va l uat i o n study we designed our evaluation to analyze the effectiveness of the pattern based identification of opinion asking questions rq1 and of our answer extraction technique rq2 .
a. metrics we use measures that are widely used for evaluation in machine learning and classification.
to analyze whether the automatically identified instances are indeed opinion asking questions and their answers we use precision the ratio of true positives over the sum of true and false positives.
to evaluate how often a technique fails to identify an opinionasking question or its answer we use recall the ratio of true positives over the sum of true positives and false negatives.
f measure combines these measures by harmonic mean.
b. evaluation datasets we established several requirements for dataset creation to reduce bias and threats to validity.
to curate a representative analysis dataset we identified chat groups that primarily discuss software development topics and had a substantial number of participants.
to ensure the generalizability of our techniques we chose two separate chat platforms slack and irc which are currently the most popular chat platforms used by software developers.
we selected six popular programming communities with an active presence on slack or irc.
we believe the communities are representative of public softwarerelated chats in general we observed that the structure and intent of conversations are similar across all communities.
to collect conversations on slack we downloaded the developer chat conversations dataset released by chatterjee et al.
.
to gather conversations on irc we scraped publicly available online chat logs5.
after disentanglement we discarded single utterance conversations and then created two separate evaluation datasets one for opinion asking question identification and a subset with only chats that start with an opinion asking question for answer extraction.
we created our evaluation datasets by randomly selecting a representative portion of the conversations from each of the six programming communities.
iii shows the characteristics of the collected chat logs and our evaluation datasets where oaconv gives the number of conversations that we identified as starting with an opinionasking question using the heuristics described in section iii b per community.
the question identification evaluation dataset consists of a total of conversations utterances and users.
our question extraction technique is heuristics based requiring conversations that do not start with an opinion asking question.
thus we randomly chose from our 45k chat logs for a reasonable human constructed goldset.
the evaluation dataset for answer extraction consists of a total of conversations utterances and users.
our machine learning based answer extraction requires conversations starting with a question and a dataset large enough for training.
thus conversations starting with opinion asking questions were used.
rq1.
how effective is chateo in identifying opinionasking questions in developer chats?
gold set creation we recruited human judges with years experience in programming and in using both chat platforms slack and irc but no knowledge of our techniques.
they were provided a set of conversations where each utterance includes the unique conversation id anonymized name of the speaker the utterance timestamp and the utterance text.
using di sorbo et al.
s definition of opinion asking questions i.e.
requiring someone to explicitly express his her point of view about something e.g.
what do you think about creating a single webpage for all the services?
the human judges were asked to annotate only the utterances of the first speaker of each conversation with value for opinion asking question or otherwise .
the judges annotated a shared set of conversations of which they identified instances i.e.
utterances of the first speaker of each conversation containing opinion asking questions angularjs c opengl python clojurians elm .
we computed cohen s kappa interrater agreement between the judges and found an agreement of .
which is considered to be sufficient .
while the sample size of conversations is sufficient to compute the agreement measure with high confidence .
the two judges then iteratively discussed and resolved all conflicts to create the final gold set.
comparison techniques researchers have used sentiment analysis techniques and lexical patterns to extract opinions from software related documents.
thus we selected two different approaches i.e.
pattern matching approaches and sentiment analysis as comparison techniques to chateo.
we evaluated three well known sentiment analysis techniques sentistrength se corenlp and nltk with their default settings.
since opinions could have positive negative polarities for the purpose of evaluation we consider a leading question in a conversation identified with either positive or negative sentiment as opinion asking.
deca is a pattern based technique that uses natural language 1265table iii evaluation dataset samples oa question identification created from chat logs conv samples answer extraction created from chat logs oaconv source durationchat logssamples oa question identification answer extraction conversations oaconv conversations utterances users conversations utterances users angularjs irc apr2014 jun2020 c irc oct2018 jun2020 opengl irc jul2005 jun2020 pythondev slack jul2017 jun2019 clojurians slack jul2017 jun2019 elmlang slack jul2017 jun2019 total table iv opinion asking question identification results technique p r f example fp example tp sentistrength se .
.
.
...i m having a weird issue with my ng cli based angular app...is there any potential issue because my model is called class and or the method is called toclass ?anyone know a good cross platform gui library that preferably supports cmake?
i d rather not use qt if i don t have to because i don t want to use the qt moc corenlp .
.
.30why does read string return but read string throws an exception?any suggestions for cmake test running strategies?
or how to organize tests expected to succeed or fail?
nltk .
.
.30hi is there a way to expose a class and a function using c style declaration void foo myclass bar does anyone know of a good way to sandbox the loading of namespaces?
deca .
.
.
..seems lambda with auto argument type provide separate templated methods per used type am i right?..
chateo .
.
.
can someone tell me why does this give an error ?what is the best way in your opinion to convert input file xml etc.
to pdf with precision to mm?
parsing to classify the content of development emails according to their purpose.
we used their tool to investigate the use of their linguistic patterns to identify opinion asking questions in developer chats.
we do not compare with huang et al.
s cnnbased classifier of intention categories since they merged opinion asking with the information seeking category.
results table iv presents precision p recall r f measure f and examples of false positives fp and true positives tp for chateo and our comparison techniques for opinionasking question identification on our evaluation dataset.
chateo achieves a precision recall and f measure of .
.
and .
respectively.
results in table iv indicate that chateo achieves an overall better precision except deca and f measure compared to all the comparison techniques.
with high precision when chateo identifies an opinionasking question the chance of it being correct is higher than that identified by other techniques.
we aim for higher precision with possible lower recall in identifying opinionasking questions since that could potentially contribute to the next module of chateo i.e.
extracting answers to opinionasking questions.
some of the opinion asking instances that chateo wasn t able to recognize lacked presence of recurrent linguistic patterns such as how does angular fit in with traditional mvc frameworks like .net mvc and ruby on rails?
do people generally still use an mvc framework or just write a web api?
.
some fns also resulted from instances where the opinion asking questions were continuation of a separate question such as is there a canvas library where i can use getimagedata to work with the typed array data?
or is this where i should use ports?
.
we observe that the sentiment analysis tools show a high recall at the expense of low precision with an exception of sentistrengthse which exhibits lower values for both precision and recall.
the example fp column in table iv indicates that sentiment analysis tools are often unable to catch the nuances of se specific keywords such as expose exception .
another example what is the preferred way to distribute python programs?
which chateo is able to correctly identify as opinion asking is labelled as neutral by all the sentiment analysis tools.
the same happens for the instance how do i filter items in a list when displaying them with ngfor?
should i use a filter pipe or should i use ngif in the template?
.
chateo is able to recognize that this is asking for opinions on what path to take among two options while the sentiment analysis tools classify this as neutral.
note that this just indicates that these tools are limited in the context of identifying opinion asking questions but could be indeed useful for other tasks e.g.
assessing developer emotions .
deca identified only one instance to be opinionasking which is a true positive hence the precision is .
.
apart from this it was not able to classify any other instance as opinion asking hence the low recall .
.
on analyzing deca s classification results we observe that out of instances in the gold set it could not assign any intention category to instances.
this is possibly due to the informal communication style in chats which is considerably different than emails.
since an utterance could contain more than one sentence deca often assigned multiple categories e.g.
information seeking feature request problem discovery to each instance.
the most frequent intention category observed was information seeking .
during the development phase we explored additional linguistic patterns but they yielded more fps.
this is a limitation of using linguistic patterns as they are restrictive when expressing words that have different meaning in different contexts.
chateo opinion asking question identification significantly outperforms an existing pattern based technique that was designed for emails as well as sentiment analysis tools in terms of f measure.
rq2.
how effective is chateo in identifying answer s to opinion asking questions in a developer conversation?
gold set creation similar to rq1 we recruited human 1266judges with years experience in programming and in using both chat platforms slack and irc but no knowledge of our techniques.
the gold set creation for answer annotation was conducted in two phases as follows phase annotation the human judges were provided a set of conversations with annotation instructions as follows mark each utterance in the conversation that provides information or advice good or bad that contributes to addressing the opinion asking question in a way that is understandable meaningful interpretable when read as a standalone response to the marked opinion asking question i.e.
the answer should provide information that is understandable without reading the entire conversation .
such utterances should not represent answer s to followup questions in a conversation.
an answer to an opinionasking question could also be a yes no response.
there could be more than one answer provided to the opinion asking question in a conversation.
phase validation the purpose of phase was two fold measure validity of phase annotations and evaluate if an answer would match an opinion asking question out of conversational context such that the q a pair could be useful as part of a q a system.
therefore for phase2 annotations we ensured that the annotators read only the provided question and answers and not the entire conversations from which they were extracted.
the phase annotations from the first annotator were used to generate a set of question and answers which were used for phase2 annotations by the second annotator and vice versa.
for each utterance provided as an answer to an opinion asking question the annotators were asked to indicate yes no if the utterance represents an answer based on the guidelines in phase .
additionally if the annotation value was no the annotators were asked to state the reason.
the judges annotated a total of conversations of which they identified a total of answers to opinion asking questions angularjs c opengl python clojurians elm .
we found that the first annotator considered .
of annotations of the second annotator as valid while the second annotator considered .
annotations of the first annotator as valid.
we also noticed that the majority of disagreements were due to the answer utterances containing incomplete or inadequate information to answer the marked opinion asking question when removed from conversational context e.g.
and then you can replace your calls to f with logargs2 f without touching the function... and human annotation errors such as marking an utterance as an answer when it just points to other channels.
comparison techniques since we believe this is the first effort to automatically extract answers to opinion asking questions from developer chats we chose to evaluate against heuristicbased and feature based machine learning classification as well as the original r cnn deep learning based technique on which we built chateo.
heuristic based he intuitively the answer to an opinion asking question might be found based on its location in the conversation the relation between its content and the question or the presence of sentiment in the answer.
we investigated each of these possibilities separately and in combination.
location based on the intuition that a question might be answered immediately after it is asked during a conversation we compare against the naive approach of identifying the next utterance after the leading opinion asking question as an answer.
content traditional q a systems have often aimed to extract answers based on semantic matching between question and answers .
thus to model content based semantic relation between question and answer we compare the average word embedding of the question and answer texts.
using our word embedding model described in iii c2 we extract utterances with considerable similarity .
to the opinion asking question as answers.
sentiment previous researchers have leveraged sentiment analysis to extract relevant answers in non factoid q a systems .
thus based on the intuition that the answer to an opinion asking question might exhibit sentiment we use corenlp to extract utterances bearing sentiment positive or negative as answers.
we explored other sentiment analysis tools e.g.
sentistrength se nltk however we do not discuss them since they yielded inferior results.
machine learning based ml we combine location content sentiment attributes as features of a machine learning ml based classifier.
we explored several popular ml algorithms e.g.
support vector machines svm random forest using the weka toolkit and observed that they yielded nearly similar results.
we report the results for svm.
deep learning based dl we present the results for both r cnn and chateo implemented as follows.
rcnn we implemented r cnn for developer chats using open source neural network library keras .
rcnn used word embeddings pre trained on their corpus.
similarly we trained custom glove vectors on our chat corpus for our comparison.
chateo we also implemented chateo using keras .
we used grid search to perform hyper parameter tuning.
first to obtain sufficient semantic information at the utterance level we use three convolution filters of size and with twice the average length of an utterance feature maps for each filter.
the pool sizes of convolution are respectively.
then a bilstm layer with units for each direction is used to capture the contextual information in a conversation.
finally we use a linear layer with sigmoid activation function to predict the probability scores of binary classes answer and nonanswer .
we use binary cross entropy as the loss function and adam optimization algorithm for gradient descent.
to avoid over fitting we apply a dropout of .
on the textcnn embeddings i.e.
units will be randomly omitted to prevent complex co adaptations on the training 1267table v answer extraction results on held out test set he heuristic based ml machine learning based dl deep learning based techniqueangularjs c opengl python clojurians elm overall p r f p r f p r f p r f p r f p r f p r f helocation l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
content c .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sentiment s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ml l c s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dlr cnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
chateo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rfdwlrq rqwhqw 6hqwlphqw kdw qj 2shq wkrq ormxuldqv opprecision rfdwlrq rqwhqw 6hqwlphqw kdw qj 2shq wkrq ormxuldqv oprecall figure evaluation measures of answer extraction data.
additionally we use a recurrent dropout of .
in the lstm units.
we also use early stopping i.e.
if the performance of the classifier did not improve for epochs the training process is stopped.
chateo answer extraction takes approximately minutes to train and test conversations on a system with .
ghz intel core i5 processor and 8gb ddr3 ram.
evaluation process evaluation of chateo and the comparison techniques was conducted using the evaluation dataset of conversations described in table iii.
we created a test set of conversations adhering to commonly recognized traintest ratio of .
we ensured that it contains similar number of instances from each programming community conversations from angular js and python conversations from c opengl clojurians and elm.
results table v presents precision p recall r and fmeasure f for chateo and our comparison techniques for automatic answer extraction on our held out test set of conversations which contains a total of answers.
the best results for p r f across all techniques are highlighted in bold.
overall table v shows that chateo achieves the highest overall precision recall and f measure of all techniques with .
.
and .
respectively.
overall chateo identified answers in the test set out of which are true positives.
r cnn and the ml based classifier perform next best andsimilar to each other in precision and f measure.
the better performance of chateo suggests that capturing contextual information through bilstm can benefit answer extraction in chat messages and that using a domain specific word embedding model trained on software related texts accompanied with hyper parameter tuning is essential to adapting deep learning models for software related applications.
figure shows that the performance of chateo is consistent precision across all communities except clojure.
one possible reason for this is answers are often provided in this chat community in the form of code snippets along with little to no natural language text which makes it difficult for chateo to model the semantic links.
chateo s overall recall of .
indicates that it is difficult to identify all relevant answers in chats even with complex dialog modeling.
in fact the recall of chateo is lower than the heuristic based technique location for c and opengl.
upon deeper analysis we observed that these two communities contain less answers one answer per opinion asking question on average compared to the other communities and the answer often resides in the utterance occurring immediately after the first speaker.
the location heuristic exhibits significantly better performance than content or sentiment heuristics.
of the conversations have at least one answer occurring immediately after the utterances of the first speaker.
neither content or sentiment is a strong indicator of answers with precision of .
and f measure of .
and .
respectively.
these heuristics cannot distinguish the intent of a response.
consider q hi clojure intermediate here.
what is the best way to read big endian binary file?
r do you need to make something that parses the binary contents does it suffice to just get the bytes in an array?
.
both content andsentiment marked this response as an answer without being able to understand that this is a follow up question.
combining the heuristics as features to svm the precision and thus f measure improved slightly over the location heuristic with .
increase in precision and .
in f measure.
as expected location as a feature shows the highest information gain.
we investigated several classifier parameters e.g.
kernal and regularization parameters but observed that the classification task was not very sensitive to parameter choices as they had little discernible effect on the effectiveness metrics in most cases .
.
since our dataset is imbalanced with considerably low ratio of answers to other utterances we explored over sampling smote techniques.
no significant improvements occurred.
ml based classification may be improved with more features and feature engineering.
1268chateo answer extraction shows improvement over heuristics based ml based and existing deep learningbased techniques in terms of precision recall and fmeasure.
v. t hreats to validity construct validity a threat to construct validity might arise from the manual annotations for creating the gold sets.
to limit this threat we ensured that our annotators have considerable experience in programming and in using both chat platforms and that they followed a consistent annotation procedure piloted in advance.
we also observed high values of cohen s kappa coefficient which measures the inter rater agreement for opinion asking questions.
for answer annotations we conducted a two phase annotation procedure to ensure the validity of the selected answers.
internal validity errors in the automatically disentangled conversations could pose a threat to internal validity affecting misclassification.
we mitigated this threat by humans without knowledge of our techniques manually discarding poorly disentangled conversations from our dataset.
in all stages of the pipeline of chateo we aimed for higher precision over recall as the quality of information is more important than missing instances chat datasets are large with many opinions so our achieved recall is sufficient to extract a significant number of opinion q a. other potential threats could be related to evaluation bias or errors in our scripts.
to reduce these threats we ensured that the instances in our development set do not overlap with our train or test sets.
we also wrote unit tests and performed code reviews.
external validity to ensure generalizability of our approach we selected the subjects of our study from the two most popular software developer chat communities slack and irc.
we selected statistically representative samples from six active communities which represent a broad set of topics related to each programming language.
however our study s results may not transfer to other chat platforms or developer communications.
scaling to larger datasets might also lead to different evaluation results.
our technique of identifying opinion asking questions could be made more generalizable by augmenting the set of identified patterns and vocabulary terms.
vi.
r elated work mining opinions in se.
in addition to the related work discussed in section ii significant work has focused on mining opinions from developer forums.
uddin and khomh designed opiner which uses keyword matching along with a customized sentiment orientation algorithm to summarize api reviews.
lin et al.
used patterns to identify and classify opinions on apis from stack overflow.
zhang et al.
identifies negative opinions about apis from forums.
huang et al.
proposed an automatic approach to distill and aggregate comparative opinions of comparable technologies from q a websites.
ren et al.
discovered and summarized controversial criticized answers in stack overflow based on judgment sentiment and opinion.
novielli et al.
investigated the role of affective lexicon on the questions posted in stack overflow.
researchers have also analyzed opinions in developer emails commit logs and app reviews.
xiong et al.
studied assumptions in oss development mailing lists.
sinha et al.
analyzed developer sentiment in github commit logs.
opinions in app reviews have been mined to help app developers gather information about user requirements ideas for improvements and user sentiments about specific features.
to the best of our knowledge our work is the first to extract opinion q a from developer chats.
extracting q a from online communications.
outside the se domain researchers have proposed techniques to identify q a pairs in online communications e.g.
yahoo answers .
shrestha et al.
used machine learning approaches to automatically detect q a pairs in emails.
cong et al.
detected q a pairs from forum threads by using sequential pattern mining to detect questions and a graph based propagation method to detect answers in the same thread.
recently researchers have focused on answer selection a major subtask of q a extraction which aims to select the most relevant answers from a candidate answer set.
typical approaches for answer selection model the semantic matching between question and answers .
these approaches have the advantage of sharing parameters thus making the model smaller and easier to train.
however they often fail to capture the semantic correlations embedded in the response sequence of a question.
to overcome such drawbacks zhou et al.
designed a recurrent architecture that models the semantic relations between successive responses as well as the question and answer.
xiang et al.
investigated an attention mechanism and context modeling to aid the learning of deterministic information for answer selection.
wang et al.
proposed a bilateral multi perspective matching model in which q a pairs are matched on multiple levels of granularity at each time step.
our model belongs to the same framework which captures the contextual information of conversations in extracting answers from developer chats.
most of the these techniques for q a extraction were designed for general online communications and not specifically for software forums.
gottipati et al.
used hidden markov models to infer semantic tags e.g.
question answer clarifying question of posts in the software forum threads.
hen et al.
used topic modeling and text similarity measures to automatically extract faqs from software development discussions mailing lists online forums .
analyzing developer chats.
wood et al.
created a supervised classifier to automatically detect speech acts in developer q a bug repair conversations.
shi et al.
use deep siamese network to identify feature request from chat conversations.
alkadhi et al.
showed that machine learning can be leveraged to detect rationale in irc messages.
chowdhury and hindle exploit stack overflow discussions and youtube video comments to automatically filter off1269topic discussions in irc chats.
romero et al.
developed a chatbot that detects a troubleshooting question asked on gitter and provides possible answers retrieved from querying similar stack overflow posts.
compared to their work we are automatically identifying opinion asking questions and their answers provided by developers in chat forums.
chatterjee et al.
s exploratory study on slack developer chats suggested that developers share opinions and interesting insights on apis programming tools and best practices via conversations.
other studies have focused on learning developer behaviors and how chat communities are used by development teams across the globe .
vii.
c onclusions and future work in this paper we present and evaluate chateo which automatically identifies opinion asking questions from chats using a pattern based technique and extracts participants answers using a deep learning based architecture.
this research provides a significant contribution to using software developers public chat forums for building opinion q a systems a specialized instance of virtual assistants and chatbots for software engineers.
chateo opinion asking question identification significantly outperforms existing sentiment analysis tools and a pattern based technique that was designed for emails .
chateo answer extraction shows improvement over heuristicsbased ml based and an existing deep learning based technique designed for cqa .
our replication package can be used to verify our results .
our immediate next steps focus on investigating machine learning based techniques for opinion asking question identification and attention based lstm network for answer extraction.
we will also expand to a larger and more diverse developer chat communication dataset.
the q a pairs extracted using chateo could also be leveraged to generate faqs provide tool support for recommendation systems and in understanding developer behavior and collaboration asking and sharing opinions .
acknowledgment we acknowledge the support of the national science foundation under grants and .