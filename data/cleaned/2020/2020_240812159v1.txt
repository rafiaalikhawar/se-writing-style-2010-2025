search based llms for code optimization shuzheng gao1 cuiyun gao2 wenchao gu1 michael r. lyu1 1department of computer science and engineering the chinese university of hong kong china 2school of computer science and technology harbin institute of technology shenzhen china szgao23 cse.cuhk.edu.hk gaocuiyun hit.edu.cn wcgu cse.cuhk.edu.hk lyu cse.cuhk.edu.hk abstract the code written by developers usually suffers from efficiency problems and contain various performance bugs.
these inefficiencies necessitate the research of automated refactoring methods for code optimization.
early research in code optimization employs rule based methods and focuses on specific inefficiency issues which are labor intensive and suffer from the low coverage issue.
recent work regards the task as a sequence generation problem and resorts to deep learning dl techniques such as large language models llms .
these methods typically prompt llms to directly generate optimized code.
although these methods show state of the art performance such one step generation paradigm is hard to achieve an optimal solution.
first complex optimization methods such as combinatorial ones are hard to be captured by llms.
second the one step generation paradigm poses challenge in precisely infusing the knowledge required for effective code optimization within llms resulting in under optimized code.
to address these problems we propose to model this task from the search perspective and propose a search based llms framework named sbllm that enables iterative refinement and discovery of improved optimization methods.
sbllm synergistically integrate llms with evolutionary search and consists of three key components an execution based representative sample selection part that evaluates the fitness of each existing optimized code and prioritizes promising ones to pilot the generation of improved code an adaptive optimization pattern retrieval part that infuses targeted optimization patterns into the model for guiding llms towards rectifying and progressively enhancing their optimization methods and a genetic operatorinspired chain of thought prompting part that aids llms in combining different optimization methods and generating improved optimization methods.
our evaluation of sbllm on a dataset of python and c code demonstrates its effectiveness in improving code efficiency.
specifically the results indicate that sbllm can improve program execution efficiency by up to .
and consistently outperform all baseline methods by .
.
and .
.
with different llms in terms of top speedup rate on python and c respectively.
i. i ntroduction as referred in the iso iec software quality guidelines the computational efficiency of the software is a critical cornerstone of system performance and user satisfaction .
inefficient code snippets can induce increased system latency computational resources waste and lead to poor user experience which is referred to as performance bugs .
existing studies have demonstrated that these inefficiencies are widely existed in software and are hard to detect and repair .
consequently the task of code optimization corresponding author.
the author is also affiliated with peng cheng laboratory.which aims to automatically refactor the code simplify its complexity and enhance performance metrics has attracted researchers attention in recent years.
early research in code optimization primarily focuses on rule based methods which mainly target specific types of inefficiencies such as software misconfigurations and loop inefficiencies .
these methods heavily rely on pre defined rules created by experts which are labor intensive and suffer from the low coverage problem .
recent advancements in deep learning dl such as large language models llms have inspired a burgeoning body of research.
these techniques learn code optimization patterns from data broadening the array of inefficiency types that can be addressed.
for example rapgen introduces a retrieval augmented generation approach with llms to generate the optimized code in a zeroshot manner surpassing the performance of previous finetuned small sized neural models .
another recent work pie establishes a benchmark with test cases and evaluates the performance of various prompting methods such as incontext learning icl and chain of thought cot .
despite the success of llm based approaches the adopted one step generation paradigm tends to largely limit the performance of code optimization for two main reasons.
first it is challenging for llms to capture the complex optimization methods in one attempt.
code optimization involves a range of optimization levels from incremental improvements like removing some unnecessary computation to more substantial optimizations that reshape the whole algorithm.
moreover optimization patterns are combinatorial in nature implying that a single snippet may consist of multiple segments with the potential for different optimization.
second it is difficult to precisely integrate the essential knowledge required for effective code optimization into llms.
although some studies have attempted to improve llms by retrieving similar code snippets these approaches only consider the input similarity and ignores the characteristic of code optimization such as its combinatorial nature which may result in the generation of under optimized code.
to mitigate the above challenges we propose to model the code optimization task from the search perspective instead of the typical generation viewpoint.
specifically the task of refactoring a given code snippet to be more efficient can be formulated as a search problem where the objective is to find the most efficient optimization method among a vast set of potential methods created by extensive code transformationarxiv .12159v1 aug a fitness estimationacc .
sp .2sample acc .
sp .19sample acc .
sp .12sample current optimized code astssamples set for number in numbers for j in range... if ... f b itness based re ranking adaptive optimization pattern retrieval improved optimized code of s genetic operator inspired chain of thought promptingtexisting optimization techniquesslow code st optimized code of st.... go cotseen samples for number in numbers if number in seen data samples for i in numbers if i in seen execution based representative sample selectionfitness scoresrepresentative samples similar patterndifferent pattern based pattern retrieval training dataset pattern base... b representative sample a fine grained pattern parsing input placeholder reasoning specification genetic operatorincorporated instructions samples for i in range len numbers for j in range... if ... fig.
the overview of sbllm.
methods and their possible combinations.
in search based software engineering significant efforts have been devoted to advancing the discovery of optimal solutions in the large search space for various tasks .
these methods iteratively identify limitations in existing solutions to update their strategy to propose better ones aiming to progressively move towards the optimal solution .
considering that a single generation by llms can be regarded as a one step exploration in the search space the search based approaches can benefit llms by incorporating them into the iterative refinement process.
in this work we introduce a search based llm s for code optimization named sbllm.
sbllm synergistically combines llms and evolutionary search comprising three main components execution based representative sample selection where we leverage execution feedback to evaluate the fitness of each sample and prioritize representative ones with effective and distinct optimization methods to pilot further optimization.
adaptive optimization pattern retrieval where we propose an adaptive retrieval mechanism to infuse domain knowledge in llms and guide llms to rectify and progressively enhance their optimization methods.
genetic operator inspired chain of thought go cot prompting where we introduce a cot prompt with crossover and mutation operations aiding llms in developing improved optimized code.
to evaluate the effectiveness of sbllm we conduct experiments on a widely used benchmark dataset containing both python and c code.
we compare sbllm against four representative prompting methods on four popular open source and closed source llms including codellama gemini chatgpt and gpt .
the experimental results demonstrate the effectiveness of sbllm in improving code efficiency.
our approach achieves a significant boost in program execution efficiency surpassing the performance of all baseline methods by .
.
and .
.
with different llms in terms of top speedup rate metric on python and c respectively we summarize our contributions as follows.
to the best of our knowledge we are the first to explore the code optimization task from a search perspective and propose to enhance llms with search based methods forthe task.
we propose a novel framework sbllm for effectively guiding llms towards identifying efficient optimization methods in the vast search space by integrating execution based representative sample selection adaptive optimization pattern retrieval and genetic operatorinspired chain of though prompt.
extensive experiments demonstrate the effectiveness of sbllm in improving code efficiency compared with baseline methods across different llms.
ii.
p roposed framework a. overview fig.
presents the overview of the proposed framework sbllm.
sbllm follows the evolutionary search paradigm that first generates initial solutions and then iteratively selects the fittest candidates while breeding new ones until termination criteria are met.
to start sbllm acquires the initial seed optimized code of the given slow code stusing existing optimization techniques.
in the execution based representative sample selection part sbllm evaluates the fitness of current optimized code and selects the representative samples that contain distinct and effective optimization methods by a re ranking mechanism.
in the adaptive optimization pattern retrieval part sbllm retrieves code optimization patterns from the pattern base based on both the slow code and the selected representative samples aiming to guide llms towards rectifying and progressively enhancing their optimization methods.
in the genetic operator inspired chain of thought prompting part sbllm constructs a prompt that leverages crossover and mutation operations to facilitate llms in combining existing optimization methods and developing improved optimized code for st. the above procedure can be conducted multiple iterations until it no longer yields further optimization in the code efficiency or reaches the maximum iteration.
b. execution based representative sample selection to enable llms iteratively refine the optimized code we propose to evaluate the fitness of each sample and provide the selected representative ones to llms.
as shown in fig.
the execution based representative sample selection module 2algorithm sample selection and pattern retrieval input the slow code need to optimize st existing predictions with their execution information e the number of selected representative samples ns the training data t output selected representative samples rs retrieved patterns p initialize initialize three lists correct list incorrect list 1function sample selection and pattern retrieval execution based representative sample selection e sort e key speedup rate order descend foralle edo ife.acc and abstract e.code not in correct list then correct list.append e.code else if e.acc 1then incorrect list.append e.code e.dis end end iflen correct list n sthen forallea incorrect list do foralleb incorrect list do ea.dis editdistance abstract ea abstract eb end end incorrect list sort incorrect list key dis order ascend end rs correct list incorrect list adaptive optimization pattern retrieval input score bm25 abstract st t.sa simscore difscore foralle rsdo ds df getdiff abstract st abstract e.code score opt bm25 ds t.ds bm25 df t.df simscore score opt difscore max score opt score opt end simtemp arg max simscore input score diftemp arg max difscore input score p simtemp diftemp 31return rs p mainly contains two steps including a fitness estimation and b fitness based re ranking.
fitness estimation.
in order to focus llms search directions towards the most efficient optimization method we propose to quantitatively assesses the fitness of each sample based on its accuracy and speedup rate.
sbllm evaluates current optimized code on a set of public test cases.
in alignment with the previous work two separate test cases are used in the prediction and evaluation process respectively including public test cases and private test cases.
this setup avoids the leakage of the test case information.
fitness based re ranking.
based on the collected fitness information sbllm first prioritizes samples that are both correct and have a high speedup rate.
specifically as shown in algorithm sbllm sorts all the code snippets based on their speedup rate and divides them into correct and incorrect groups according to the accuracy line .
to consider the combinations of optimization patterns we then propose to select distinct samples from the correct group as representative samples.
as shown in line to line sbllm abstracts the correct code based on the asts abstract syntax trees and ensures that only one sample with identical abstractions can be chosen.
considering that incorrect code can alsoprovide hints to llms for avoiding the same errors we involve incorrect code as representative samples.
specifically sbllm calculates the edit distance between the abstract code and prioritizes the incorrect code with distance sum.
the prioritized incorrect code tails the selected correct code.
the topnssamples are retained as the selected representative samples rs while the remaining samples are discarded.
c. adaptive optimization pattern retrieval the adaptive optimization pattern retrieval module aims at providing llms with effective optimization patterns to facilitate the generation of improved optimized code.
the retrieved optimization patterns are expected to provide hints to llms towards generating correct and more efficient code.
we propose to involve both the input slow code stand selected representative samples rs to adaptively retrieve effective optimization patterns for llms.
specifically the retrieval part considers both the optimization methods that are semantically similar to rsfor rectifying potential errors and those different from rsfor drawing inspiration from unexploited optimization methods.
as shown in fig.
the adaptive optimization pattern retrieval module mainly contains two steps including a fine grained pattern parsing and b representative samplebased pattern retrieval.
fine grained pattern parsing.
to facilitate accurate retrieval of similar and different patterns sbllm first parses each optimization pair in the training dataset and extracts patterns with fine grained optimization information to construct the pattern base.
each optimization pair consists of a nonoptimized code snippet sand its optimized version f. to preserve the general optimization information and eliminate project specific influences sbllm parses them into asts and obtains their abstracted code saandfa.
then it identifies the fine grained optimization information with the difflib package by isolating the abstracted deleted statements ds from sa as well as the abstracted added statements dffrom fa.
based on the above process we obtain the pattern base with fine grained optimization information including sa fa ds and df which is then used to assist the retrieval process.
representative sample based pattern retrieval.
the representative sample based pattern retrieval method aims at guiding llms in refining the current optimized code rsin two ways.
specifically as shown in fig.
current optimized code may contain incorrect and insufficient optimizations.
to address these issues we propose to adaptively retrieve separate patterns that are semantically similar to and different from rs respectively.
first as shown in fig.
a the current optimized code intends to use the eratosthenes sieve algorithm to reduce the time complexity.
however as llms do not learn this optimization method well the optimized code contains incorrect initialization which will lead to an out of index error.
the similar pattern presented in fig.
a 2shows a correct implementation of this algorithm that can provide hints for llms to rectify the errors.
however the pattern is hard to be retrieved based solely on its semantic similarity current optimized code def isprime x ... while i root x if x i return false def generateprimes n sieve ... while p p n ... for i in range p p n p sieve false the similar pattern for var in range num int var num num ... def prime table num ... var num for num in range num int var num num incorrect initailization low input similarity refined code def generateprimes n sieve n ... while p p n ... for i in range p p n p sieve false a a python example showing how the similar pattern helps rectify errors in current optimized code.
current optimized code if s.size n for int i len i n i s.pop back for int j j j s.push back .
cout s cout s ... the different pattern for int var var var var var ... str.pop back ... string str str .substr num num refined code if s.size n cout s.substr n ... low input similarity b a c example showing how the different pattern helps find the unexploited optimization method.
fig.
examples for illustrating the two kinds of retrieved patterns in the adaptive optimization pattern retrieval module.
to the input part due to the relatively low similarity degree.
to effectively capture similar patterns sbllm leverages rs and retrieves the pattern that not only exhibits similar abstracted slow code with sabut also possesses similar optimized parts i.e.
abstracted deleted statements dsand abstracted added statements df to those in rs.
specifically as shown in algorithm sbllm first measures the input similarity input score between the abstracted code of stand the abstracted code snippets sain the pattern base by the bm25 lines .
subsequently for each representative sample in rs sbllm extracts its abstracted deleted statements dsand abstracted added statements df and calculates the similarity score score optof the optimized part with each pattern in the pattern base lines .
then the score is added to the simscore line .
finally sbllm integrates simscore .
analyze the original code and the optimizations applied in the existing versions genetic operator incorporated instructions task description instructions you will be provided with a code snippet its existing optimization versions along with their performance and two code transformation patterns.
please follow the instructions step by step to improve code efficiency .
.
identify any additional optimization opportunities .
explain your optimization methods and provide a new optimized code snippetthathavenotbeenutilized.
.
input placeholder slow code current code patterns different patternreasoning specification desired output format .
analyze the original code and the optimizations applied in the existing versions answer .
identify any additional optimization opportunities that have not been utilized answer .
explain your optimization methods and provide a new optimized code snippet optimization points n code n mutate crossover generatiomutation nfig.
the illustration of the go cot prompt.
contents in will be substituted by the corresponding data.
the complete prompt can be found in our github repository .
withinput score and the pattern with the highest score is selected as a similar pattern line .
second as illustrated in fig.
b the current optimized code presents insufficient optimization as it only focuses on the optimization of the second for loop while overlooking the optimization of the pop back statement.
correspondingly the different pattern in fig.
b 2presents an effective optimization method by employing the substr api.
however this pattern displays low semantic similarity to the input part and is hard to be retrieved.
to this end sbllm retrieves the pattern that exhibits similar abstracted slow code sabut employs different optimization methods compared to those present in rs.
similarly sbllm also first calculates the similarity score input score of the input part andscore optof the optimized part based on rs.
then the value of score optis inverted and added to difscore to prioritize patterns exhibiting lower similarity of the optimized part line .
ultimately the pattern with the highest score of the sum of difscore andinput score is selected as the different pattern line .
these two retrieved patterns pare then integrated into the prompt to guide the generation of new optimized code.
d. genetic operator inspired chain of thought prompting the part aims at guiding llms in integrating different optimization methods from representative samples rs and retrieved patterns p and subsequently generating refined optimized code.
specifically we propose to aid llms with the evolutionary algorithm s generic operators and introduce the genetic operator inspired chain of thought go cot prompt.
genetic operators are inspired by biological evolution principles and comprise crossover and mutation to synthesize 4algorithm the evolutionary optimization process input the slow code need to optimize st maximum iteration number i output re ranked optimized code sol 1function evolutionary optimization obtain initialization solutions sol forst foralli do select representation samples rsifromsol and retrieve patterns ifrsi rsi 1andrsihave correctly optimized stthen break end generate new code nc sol rsisnc end re rank sol using the selection part in algorithm 12return sol new solutions.
these operators facilitate the combination of advantageous traits promote exploration and generate superior solutions.
specifically we construct the go cot prompt as depicted in fig.
.
the prompt consists of three main components including genetic operator incorporated instructions reasoning specification and input placeholder.
thegenetic operator incorporated instructions component illustrates the task requirement and outlines the instructions to be executed by llms.
llms are instructed to follow three sequential steps to generate a new code snippet.
the first two steps involve combining the advantages observed in the selected representative samples and referring to the retrieved patterns to identify unexploited optimization methods which correspond to the crossover and mutation operators in the evolutionary algorithm respectively.
based on these two genetic operators in the third step llms are required to conclude the optimization methods and generate a new optimized code.
the reasoning specification component aims to standardize the format of the output content.
llms are instructed to follow the given reasoning format step by step and produce the result accordingly.
the input placeholder includes the code that llms need to optimize along with the representative samples rsand the retrieved patterns p. with the instructions and provided information in the prompt llms can learn to follow the reasoning strategy and generate a new optimized code step by step.
e. evolutionary optimization based on the aforementioned processes sbllm iteratively generates improved optimized code.
as shown in algorithm at the end of each iteration the newly generated code will then be integrated with the representative samples in the prompt for the next iteration generation.
the iterative refinement process continues until it reaches the convergence condition.
specifically if the code has been correctly optimized and the representative samples rs remain unchanged with those in the last iteration we terminate the process.
this is based on the intuition that the prompt in this iteration will be the same as the one in the last iteration and llms are less likely to generate better code.iii.
experimental setup a. research questions in the evaluation we focus on the following four research questions rq1 how effective is sbllm in improving code efficiency?
rq2 what is the fine grained performance of code generated by sbllm across different optimization levels?
rq3 what are the contributions of different modules in sbllm?
rq4 what is the impact of different hyper parameters on the performance of sbllm?
to study rq1 we conduct a comprehensive evaluation of sbllm by comparing with four representative baseline methods across four popular llms aiming to provide a thorough assessment across language models with different parameter sizes and capabilities.
for rq2 we delve into the analysis of the proportion of generated code across different accuracy and speedup rate levels including cases where the code is not correct correct but not faster than slow code faster than slow code but not faster than human reference and faster than human reference.
additionally we investigate how much code generated by sbllm could be faster than reference code derived by human developers.
for rq3 we remove different parts in sbllm to assess their individual contributions.
for rq4 we explore the influence of various hyperparameters by varying the number of representative samples in the prompt and the number of maximum iterations.
b. datasets in this work we evaluate sbllm on the widely used pie dataset which contains two programming languages i.e.
python and c .
the two popular programming languages are critical for code optimization evaluation.
python is a dynamic language known for its slow execution speed .
in contrast c is a statically typed compiled language renowned for its high performance especially when leveraging the o3 optimization option.
by applying the proposed optimization we can validate whether the optimized methods generated by sbllm are trivial and can be achieved through compiler optimization techniques.
the pie dataset is derived from codenet which is curated from an online judge system and contains programming problems.
here we craft the public and private test cases for each problem by using the input output examples in the program description in codenet and the test cases provided by alphacode respectively.
on average for each problem we obtain .
public test cases to obtain feedback for sbllm and .
private test cases to evaluate the correctness and efficiency of generated code.
each entry in pie contains a triplet problem id slow code fast code written by the same programmer.
the python subset of the pie dataset comprises training samples valid samples and test samples while the c comprises training samples valid samples and test samples.
5c.
baselines to provide a comprehensive evaluation we experiment on four popular llms and compare sbllm with four representative prompt methods with details as below.
for llms we evaluate the performance of sbllm on both open source and closed source models including codellama gemini chatgpt and gpt .
codellama is a family of open source large scale code language models developed by meta.
we use the 34b instruct tuned version i.e.
codellama34b instruct hf for experiments.
chatgpt and gpt4 are two popular llms developed by openai which show versatile abilities across different fields such as code generation.
they are closed source model and we access it through apis i.e.
gpt .
turbo and gpt 1106preview .
gemini is a recent powerful closed source llm developed by google which shows comparable ability with gpt .
we also access it based on its official api i.e.
geminipro .
as for the prompt methods we follow previous work and involve four representative methods including instruction prompting in context learning icl retrieval augment generation rag and chain of though cot .
instruction prompting directly prompts llms to generate optimized code without providing other information.
in context learning adds some examples input output pair before the query sample to help the model understand this task.
following prior work we randomly sample four pairs from the training set to create the examples for icl.
as for retrieval augment generation rag method instead of random selection it retrieves different samples from the training set for different query samples.
specifically we employ bm25 to select the code from the training set with the highest similarity to the query sample.
lastly in the chain of thought cot prompt we follow and employ prompts that instruct the llm to first explain how to optimize the program before producing the optimized code.
we use the same examples as icl for cot and manually craft the explanations to aid the llm in reasoning through cot.
the detailed prompts for all baseline methods are provided in our replication package .
d. metrics to evaluate the correctness and efficiency of optimized code we follow previous work and measure the following metrics percent optimized opt opt denotes the fraction of code in the test set that demonstrate improvement through a given method.
a program must be at least faster and correct i.e.
pass all test cases to contribute i.e.
t s t o t o anda o where t anda represent the execution time and accuracy and oandsdenote the optimized and slow code respectively.
speedup rate sp sp measures the improvement in running time.
we first calculate the speedup rate of each generated code and then report the average results on the whole test set.
if a generated code is either incorrect or slower than the original slow code we assign a speedup of .
tothat example as the worst case scenario assumes the original program has a speedup of .
.
formally sp is calculated as follows sp nx i 1t si t oi ifa oi t oi t si else where nis the size of test set.
e. implementation details for the hyperparameters of all llms following the previous work we set the temperature to .
for all experiments and generate five results by random sampling.
for all baseline methods the optimized code will be re ranked based on the output probability predicted by the llms.
for sbllm after the whole optimization process we re rank the optimized code obtained in the last iteration using the selection method in algorithm .
as for the hyper parameters of sbllm we set the number of selected representative samples nsto three and the maximum iteration number to four.
the impact of different numbers is discussed in section iv d. for gpt due to the cost limitation we randomly sample data in the test set for experiment.
following previous work we execute each slow and generated program times and report the average execution results excluding the first run.
for the execution environment we execute python programs with python .
.
and compile all c programs with gcc version .
.
and c as well as the o3 optimization flag.
all experiments are conducted on a linux server bit ubuntu .
with one core intel xeon platinum cpu .20ghz four nvidia a100 40gb gpus and 1tb ram.
iv.
e xperimental results a. rq1 comparison with baselines to assess the effectiveness of sbllm in improving code efficiency we compare sbllm with four representative prompt methods on four popular llms.
table i presents the performance of sbllm along with baseline methods on python and c .
for each metric we denote the performance using the top k metric where k represents the number of generated code snippets considered.
comparison of the opt metric.
as shown in table i sbllm demonstrates considerable improvements over the baseline methods across both languages and all llms.
for example when compared to the strongest baseline method cot sbllm achieves an average improvement of .
and .
in opt on python and c respectively.
these results demonstrate the effectiveness of sbllm in identifying efficient optimization methods within the vast search space.
besides by comparing the improvements across different llms we observe that sbllm achieves higher enhancements on more powerful llms such as chatgpt and gpt4.
for instance on python sbllm enhances the opt of codellama and gpt by .
and .
respectively.
this discrepancy can be attributed to the limited instructionfollowing capability and context size of codellama which might make it not fully comprehend the instructions and 6table i the performance of sbllm along with the baselines on two programming languages in terms of top .
under each metric the best performance is marked as gray.
denotes statistical significance in comparison to the base models i.e.
wilcoxon test with p value .
.
approachpython c o3 opt k sp k opt k sp k top top top top top top top top top top top top codellamainstruction .
.
.
.
.
.
.
.
.
.
.
.
icl .
.
.
.
.
.
.
.
.
.
.
.
rag .
.
.
.
.
.
.
.
.
.
.
.
cot .
.
.
.
.
.
.
.
.
.
.
.
sbllm .
.
.
.
.
.
.
.
.
.
.
.
geminiinstruction .
.
.
.
.
.
.
.
.
.
.
.
icl .
.
.
.
.
.
.
.
.
.
.
.
rag .
.
.
.
.
.
.
.
.
.
.
.
cot .
.
.
.
.
.
.
.
.
.
.
.
sbllm .
.
.
.
.
.
.
.
.
.
.
.
chatgptinstruction .
.
.
.
.
.
.
.
.
.
.
.
icl .
.
.
.
.
.
.
.
.
.
.
.
rag .
.
.
.
.
.
.
.
.
.
.
.
cot .
.
.
.
.
.
.
.
.
.
.
.
sbllm .
.
.
.
.
.
.
.
.
.
.
.
gpt 4instruction .
.
.
.
.
.
.
.
.
.
.
.
icl .
.
.
.
.
.
.
.
.
.
.
.
rag .
.
.
.
.
.
.
.
.
.
.
.
cot .
.
.
.
.
.
.
.
.
.
.
.
sbllm .
.
.
.
.
.
.
.
.
.
.
.
sbllminstruct cotrag icl a python.
sbllminstruct cotrag icl b c o3 .
fig.
venn diagram of optimized code provided by sbllm and baseline methods on chatgpt.
input information provided by sbllm.
in contrast powerful llms such as gpt exhibit better understanding of the gocot prompt enabling them to generate superior optimization methods step by step.
in addition to measuring the number of optimized code generated by each approach we also follow previous work in program repair and investigate the overlap of optimized code among different methods.
fig.
illustrates the unique optimized code of the top predictions achieved by sbllm and four baseline methods on chatgpt represented through venn diagrams.
notably sbllm identifies and unique optimized code in python and c respectively while the other approaches only yield 18and0 7unique optimized code across the two languages.
this indicates that the contribution of the iterative refinement process cannot be replaced by the combination of existing prompting approaches.
comparison of the sp metric.
as for the speedup rate by analyzing the top prediction results in table i we observe that sbllm can boost program execution efficiency by up to .
and .
outperforming all baselines by .
.
and .
.
on python and c respectively.
the improvement of sbllm over baselines is even more significant for top predictions where sbllm surpasses cot on gpt by .
and .
on two languages respectively which can be attributed to our execution based representative selection method in sbllm.
these results demonstrate that sbllm can effectively guide the llms progressively moving towards the better optimization method and generating more efficient code.
answer to rq1 sbllm successfully optimizes the most code snippets compared to all baselines across different llms.
it boosts program execution efficiency by up to .
and .
outperforming the top speedup rate of all baselines by .
.
and .
.
on python and c respectively b. rq2 fine grained performance analysis in this rq we study the fine grained proportion of generated code across different levels of accuracy and speedup rates.
to achieve this we classify the generated code snippets instruction icl rag cot sbllm ncno nhfh a the proportion of different optimization level on python.
instruction icl rag cot sbllm ncno nhfh b the proportion of different optimization level on c o3 .
fig.
the proportion of top prediction based on chatgpt with different optimization levels on python and c respectively.
nc no lh and fh indicate the code is not correct correct but not optimized optimized but lower than human reference and faster than human reference respectively.
into four distinct levels not correct nc correct but not optimized no optimized but not faster than the human reference nh and faster than the human reference fh .
following the opt definition we consider code that is at least faster than the human reference as falling into the fh category.
the experimental results are depicted in fig.
.
due to space limitations we only present the top results of each method on chatgpt while the results for other models can be found in our github repository .
as shown in fig.
we can observe that sbllm consistently achieves the relatively lower nc rate and the highest nh and fh rate compared with other methods on both programming languages.
specifically concerning the nc rate baseline methods generate at least .
incorrect code for python.
in contrast sbllm reduces this rate by .
.
as for the fh rate .
and .
of the code generated by sbllm outperforms the human reference on python and c respectively.
this represents an obvious improvement over the strongest baseline method named cot which only achieves fh rates of .
and .
on these two programming languages.
furthermore we also investigate the overlap of fh code snippets generated by different methods.
as shown in figure we find that sbllm achieves and unique improvements on python and c respectively surpassing the performance of other methods on both languages.
these findings suggest that the iterative refinement process in sbllm facilitates the generation of correct and efficient code solutions.
answer to rq2 sbllm excels in generating code with the lowest error rate in python and achieves the highest rate of efficiency surpassing human written reference across both programming languages.
c. rq3 ablation study to investigate the individual contribution of different components in sbllm we conduct an ablation study and present the results in table ii.
due to the space limitation we only present the results on chatgpt with results for other llms presented on our github repository .
sbllminstruct cotrag icl a python.
sbllminstruct cotrag icl b c o3 .
fig.
venn diagram of code faster than human reference provided by sbllm and baseline methods on chatgpt.
sample selection.
to assess the impact of the executionbased representative sample selection component we replace it with a direct selection method that simply selects samples with the highest speedup rate.
as shown in table ii removing our selection strategy results in a noticeable performance decrease.
specifically the top opt drops by .
and .
on python and c respectively which demonstrates the benefits of our sample selection criteria.
pattern retrieval.
in this experiment we evaluate the effectiveness of the adaptive optimization pattern retrieval part by removing the two patterns utilized in sbllm respectively.
when the similar pattern is eliminated the performance of sbllm suffers from an obvious decline i.e.
.
on the top speedup rate of python which demonstrates the importance of providing llms with a similar optimization pattern to refine their optimization methods.
similarly removing the different patterns leads to a substantial drop of sp metric on c as the dissimilarity pattern could assist llms in drawing inspiration from unexploited optimization methods and generating more efficient code.
go cot.
to evaluate the contribution of go cot we remove its crossover and mutation based genetic instructions and reasoning specification part.
instead we solely use input placeholder to query llms to generate optimized code.
the results in table ii demonstrate a substantial performance 8table ii ablation study.
under each metric the best performance is marked as gray.
approachpython c o3 opt k sp k opt k sp k top top top top top top top top top top top top w o selection .
.
.
.
.
.
.
.
.
.
.
.
w o sim pattern .
.
.
.
.
.
.
.
.
.
.
.
w o dif pattern .
.
.
.
.
.
.
.
.
.
.
.
w o go cot .
.
.
.
.
.
.
.
.
.
.
.
w o all .
.
.
.
.
.
.
.
.
.
.
.
sbllm .
.
.
.
.
.
.
.
.
.
.
.
5top top top 5speedup rate a nson python.
5top top top 5speedup rate b nson c o3 .
5top top top 5speedup rate c iteration number on python.
5top top top 5speedup rate d iteration number on c o3 .
fig.
parameter analysis.
decrease of .
and .
on the top speedup metric for python and c respectively.
this indicates that go cot is effective in aiding llms to combine different optimization methods and generate improved optimized code.
all.
in this experiment we remove all the aforementioned components to evaluate the performance of a direct combination of llms and evolutionary search.
from table ii we can find that removing all of the above components lead to worse results e.g.
a drop of .
and .
on the top opt on python and c respectively.
this suggests that simply combining llms with evolutionary search or increasing the generation number can not yield promising results.
answer to rq3 all components in sbllm contribute to the performance.
removing the execution based representative sample selection adaptive optimization pattern retrieval or go cot leads to substantial performance decreases.
d. rq4 parameter analysis in this section we investigate the influence of two parameters namely the number of selected examples ns and the maximum number of iterations on the performance of sbllm.
similar to section iv c the results presented in this optimal optimizationsearch space directly generated optimization methods 1st iterationsearch space 2nd iteration3rd iteration a existing work directly generates optimization methods and stop here.
b sbllm effectively searches towards the optimal optimization method.fig.
an illustration of the difference of existing work and sbllm from the search perspective.
section are also based on chatgpt and the remaining results can be found in the github repository .
number of selected representative samples.
we study the effect of nsby varying it from to .
as depicted in fig.
a and b for both python and c sbllm exhibits optimal performance when nsis set to .
choosing larger or smaller values does not yield improved results.
we suggest that it is because insufficient selected samples may result in inadequate information while an excessive number of samples may introduce redundancy which may hinder the model s ability to effectively utilize the provided information .
number of iterations.
we evaluate the performance of sbllm across different iterations by setting the maximum iteration to five.
the corresponding results are presented in fig.
c and d .
from the results we can observe that sbllm demonstrates increasingly better performance with each iteration on python while on c it achieves the peak at the fourth iteration since excessively large iterations may introduce the risk of overfiting on public test cases.
consequently we set the maximum iteration as four.
answer to rq4 sbllm achieves the best performance with three representative samples.
for the number of iterations the performance of sbllm improves with more iterations initially but excessively large iteration numbers may cause performance degradation.
v. d iscussion a. what makes sbllm work?
sbllm synergistically integrates evolutionary search into llms.
the first advantage of sbllm is its novel optimized code generated by cot cnt while not n n cnt optimized code generated by sbllm cnt n n .bit length reference optimized code for in range a np.ndarray a a answer len a a 1time 72ms time 330mstime 337msfig.
a case showing that sbllm can optimize codes with efficient apis and surpass human reference.
slow code ... int a2 a for int i i 1e7 i b2 true a2 a ... optimized code in iteration ... int a2 a b for int i i b i b2 true a2 a2 a b ... optimized code in iteration ... int a2 a b for int i i 1e7 i b2 true if a2 b c ... return a2 a ... optimized code in iteration ... int a2 a b for int i i b i b2 true if a2 c ... return a2 a2 a b ... fig.
a case showing that llms can follow the crossover instruction in go cot and combine different optimization methods.
the complete case can be found in our github repository.
paradigm which benefits llms by incorporating them into the iterative refinement process.
as shown in fig.
a previous llm based work directly generates the optimized code.
however due to the complex optimization methods they are hard to directly yield the optimal solution within such an expansive search space.
in contrast sbllm integrates the search based method which enables selecting effective optimization methods and combining them into improved ones.
as depicted in fig.
b based on this iterative refinement process sbllm guides llms towards the optimal optimization method stepby step.
sbllm could provide llms with effective optimization patterns.
the second advantage of sbllm lies in its provided effective optimization patterns.
llms heavily rely on general knowledge acquired during the pre training phase.
therefore without explicit external hints it is hard for them to identify errors in existing optimized code snippets or discover unexploited optimization methods .
sbllm assists llms by providing the optimization patterns as hints thereby guiding them in generating correct and more efficient code.
as shown in fig.
the cot prompt with chatgpt fails to achieve optimal optimization for the slow code due to their limited code optimization knowledge.
sbllm achieves better performance by leveraging the retrieved pattern that includes the api bit length which enables it to even outperform slow code ... for int i i m i cin p s if s ac is ac for int i i m i if s ac !ac check ac check ac if s wa !ac check is ac wa ... optimized code in iteration ... unordered map int pair bool bool problems for int i i m i cin p s if s ac problems .first true ac if problems .second wa else if s wa problems .second true ... optimized code in iteration ... unordered map int pair bool bool problems for int i i m i cin p s if problems .first continue if s ac problems .first true ac if problems .second wa else if s wa problems .second true ...fig.
a case showing that go cot can follow the mutation instruction in go cot and achieve further improvement.
the complete case can be found in our github repository.
the reference code derived by human developers.
sbllm could aid llms in generating improved optimized code.
another advantage of sbllm is the gocot which guides llms in effectively leveraging existing optimized code to generate improved ones.
as shown in fig.
the original code repeatedly adds a and takes the modulus b. although the optimized code snippets generated by llms may not directly achieve the optimal optimization method they may contain various optimization techniques that when combined lead to more efficient code.
as illustrated in fig.
2and these two code snippets improve efficiency by reducing the number of iterations and incorporating an early termination check respectively.
in the next iteration sbllm follows the crossover instruction in the go cot prompt and combines them to achieve better performance as depicted in fig.
.
additionally as shown in fig.
the slow code reads competition problem submissions tracks accepted ac and wrong attempt wa statuses using arrays and outputs the counts of ac and wa.
in the first optimized version this code replaces arrays with anunordered map to efficiently track ac and wa statuses and updates counts during input processing.
then as shown in fig.
sbllm follows the mutation instruction in the go cot prompt and further optimizes the code in the next 10iteration by skipping the processing for problems that have already been accepted.
b. threats to validity we identify three main threats to the validity of our study the selection of languages.
in this paper we conduct experiments on the pie dataset containing two widely used programming languages python and c .
while there are other datasets that contain different programming languages such as the c dataset in deepdev perf we don t use this dataset since it does not contain test cases.
in the future we will create test cases for this dataset and conduct experiments.
the selection of llms.
another threat is the baselines we utilized in our evaluation.
we evaluate sbllm on four popular and representative llms and prompting methods.
while there are other existing llms our proposed sbllm is model agnostic and can be easily generalized to different llms.
furthermore our method focuses on how to further boost the initial results directly generated by llms.
therefore our research is orthogonal to the work that solely generates code such as fine tuning and one step prompting methods.
in future work we plan to further evaluate the effectiveness of sbllm on other llms.
potential data leakage.
in this paper we conduct experiments utilizing the apis of chatgpt gpt and gemini.
however as these models are closed source their training data are not publicly accessible giving rise to concerns regarding potential data leakage.
however our experiments reveal that directly prompting the llms to optimize the code cannot yield promising results.
therefore we believe that the optimization code generated by sbllm is not simply from memorizing the training data.
vi.
r elated work a. code optimization early research in code optimization techniques tends to employ rule based methods and focus on specific inefficiency types such as software misconfigurations and loop inefficienciess .
recently deep learning based methods are introduced and achieve promising results.
deepdevperf is a pre trained model that suggests performance improvements in c code.
chen et al.
introduce a variational auto encoder that identifies effective code edits for performance.
rapgen leverages openai codex to fix c code inefficiencies issue in zero shot.
it surpasses deepdevperf in precision without extra training.
supersonic develops a seq2seq model for code optimization using diffbased output representation.
pie is a recent benchmarks that explore using llms for improving code performance.
it evaluates various prompting methods and shows that these methods can significantly speed up code execution.
different from the above work that typically focuses on directly generating optimized code sbllm aims at iterative refining and boosting the initial results directly generated by llms in a search based manner.b.
large language models for software engineering recently the advent of llms has significantly advanced various software engineering tasks .
a lot of research is dedicated to effectively harnessing the capability of llms by fine tuning or prompt engineering for software engineering tasks .
for example wizardcoder fine tunes llms with complex instructions for code generation.
xia et al.
leverage llms for program repair by using the cloze stype prediction.
typegen prompts llms with static analysis results and cot prompts for type inference.
chatrepair iteratively evaluates programs on test cases and feeds the error messages to llms for further patch generation.
self edit utilizes compiler error messages to enhance the correctness of code generation.
c. sbse and large language models codamosa leverages llms to provide example test cases for under covered functions when search based testing hits a coverage stall.
tawosi et al.
use available searchbased methods to optimize the number and combination of few shot examples for llms in the story point estimation task.
brownlee et al.
introduce a method that employs llms as mutation operators for genetic improvement.
similarly kang and yoo propose to leverage the capabilities of llms in code comprehension and generation for creating objectivetailored mutants.
dakhama et al.
improve search based fuzzing by using chatgpt to parameterise c programs for gem5 testing.
vii.
c onclusion and future work in this paper we propose sbllm a search based llms framework for code optimization.
sbllm synergistically integrates llms with evolutionary search encompassing three components an execution based representative sample selection mechanism an adaptive optimization pattern retrieval method and a genetic operator inspired chain of thought prompting method.
extensive experiments on four popular llms show that sbllm can effectively guide llms towards identifying efficient optimization methods in the vast search space.
in the future we plan to apply our search based llms framework to other tasks in software engineering such as program repair.
our source code and detailed experimental results are available at .