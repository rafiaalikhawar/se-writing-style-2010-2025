towards the use of the readily available tests from the release pipeline as performance tests.
are we there yet?
zishuo ding jinfu chen and weiyi shang department of computer science and software engineering concordia university montreal canada zi ding fu chen shang encs.concordia.ca abstract performance is one of the important aspects of software quality.
performanceissuesexistwidelyinsoftwaresystems andtheprocess of fixing the performance issues is an essential step in the release cycle of software systems.
although performance testing is widelyadoptedinpractice itisstillexpensiveandtime consuming.
inparticular theperformancetestingisusuallyconductedafterthe systemisbuiltinadedicatedtestingenvironment.thechallengesofperformancetestingmakeitdifficulttofitintothecommondevops process in software development.
on the other hand there exist a large number of tests readily available that are executed regularly within the release pipeline during software development.
in this paper weperformanexploratorystudytodeterminewhethersuch readily available tests are capableof serving as performance tests.
inparticular wewouldliketoseewhethertheperformanceofthese testscandemonstrateperformanceimprovementsobtainedfrom fixing real life performance issues.
we collect performance issuesfrom hadoopandcassandra andevaluatetheperformance of the readily available tests from the commits before and after the performance issue fixes.
we find that most of the improvements from the fixes to performance issues can be demonstrated using thereadilyavailabletestsinthereleasepipeline.however onlya verysmallportionofthetestscanbeusedfordemonstratingthe improvements.
by manually examining the tests we identify eight reasons that a test cannot demonstrate performance improvements eventhoughitcoversthechangedsourcecodeoftheissuefix.finally we build random forest classifiers determining the important metrics influencing the readily available tests not being able to demonstrate performance improvements from issue fixes.
we find that the test code itself and the source code covered by the test are importantfactors whilethe factorsrelated to the code changes in the performance issues fixes have a low importance.
practitioners may focus on designing and improving the tests instead of finetuning tests for different performance issues fixes.
our findings canbeusedasaguidelineforpractitionerstoreducetheamount of effort spent on leveraging and designing tests that run in the release pipeline for performance assurance activities.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn ... .
performancetesting performanceissues software performance acm reference format zishuo ding jinfuchen and weiyi shang.
.
towards the use ofthe readilyavailabletestsfromthereleasepipelineasperformancetests.are we there yet?.
in 42nd international conference on software engineering icse may23 seoul republicofkorea.
acm newyork ny usa pages.
introduction performance is one of the most important aspects of software quality.
performance can directly affect the user experience of largescale systems such as amazon ebay and google .
a prior studyfindsthatfieldissuesreportedinsuchsystemsaremoreassociated with the performance of the system instead of functional issues .
performanceissuesexistwidelyinsoftwaresystems andare difficulttoavoidduringthesoftwaredevelopmentprocesses .
theperformanceissueshavevariouseffectsonthesystem.some leadtohighresource likecpuormemory utilization andsome cancausealongresponsetimetouserrequests.anexampleperfor manceissueexcerptfrom hadoopissuetrackingsystem 1describes thatwhen networktopology callsadd orremove itcalls tostring forlog.debug whichrequiresextraresources.asindicatedinthe issue report the tostring method is used for logging messages which can lead to the unnecessary slowdown of the operation and extra resource utilization.
performance testing is challenging.
it is often an expensive and time consumingprocess .performancetestsoftenneedtorun with carefully designed sophisticated test plans on top of the supportofspecialsoftware likejmeter andareexecutedforalong period of time days .
on the other hand such performance teststypicallyexercisetheentiresystemasawholeinsteadofan optimized targetedtherapy .inparticular suchlong runningand un targeted performance testing is difficult to fit into the widely adopteddevopsprocess whenreleasesarefrequentandcontain smallerchangesbetween two releases.
ontheotherhand thereexistalargenumberofteststhataretypically executed regularly during every build in the release pipeline ofsoftwaredevelopment .forinstance inarecentreleaseof cassandra more than tests are executed by default in a regular build process during the release pipeline while more than tests are executed in a recent release of hadoop2.
prior studies find thatsuchtestsareoftencomplex coveringvariousscenariosofthe ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea zishuo ding jinfu chen and weiyi shang usageofthesoftware .moreimportantly thesetestsare readily available and are executed by default on a regular basis.
due to the expensive performance testing as well as the wide availability and maturity of tests that run in the release pipeline recent research has been advocating the use of such tests in performance assurance activities .
however there exists littleknowledgeabouttowhatextentcanthetestsintherelease pipeline behave as performance tests.
therefore in this paper we studytheuseofthereadilyavailabletestsinthereleasepipeline of two open source projects i.e.
hadoopandcassandra as performancetests.weidentify127performanceissuesthatare fixed in the two subject systems and the snapshots of the source codebeforeandafterthefixofeachperformanceissue.byevaluating the performance of the tests with the snapshots of the source code we aim to answer the following research questions3 rq can the readily available tests from the release pipeline demonstrate performance improvements from performance issuesfixes?
most of the performance improvements after an issue fix can be demonstrated by at least one test.
however for each performanceissue onlyaverysmall .
and20.
portion ofthetestscandemonstratetheperformance improvements.
rq whatarethereasonsthatsometestsinthereleasepipeline cannotbe used as performance tests?
weidentifyeightreasonsthatatestfromthereleasepipeline cannot demonstrate performance improvements from a performance issue fixes.
the reasons can be used as a guideline for practitionersto design micro performance tests.
rq what are the important factors for a test to be useful as a performance test?
webuildclassifierstomodelwhetheratestcandemonstrate the performance improvements of a particular performance issue.byexploringtheimportantfactorsinourclassifiers we find that the factors related to the test itself and the coveredsourcecodeofthetestareimportantintheclassifiers.
ontheotherhand thefactorsrelatedtothecodechanges intheperformanceissuefixeshavealowimportance.our results imply that practitioners may focus on designing and selecting tests instead of optimizing tests especially for differentperformance issues.
ourfindingsdemonstratethecapabilityandthechallengesof using the readily available tests from the release pipeline in performance assurance activities.
our paper calls for future research that assists in designing and selecting tests that can be used in various e.g.
functional and non functional scenarios for the development of software systems.
paper organization.
the restof this paperis organized asfollows section presents the prior research that is related to thispaper.
section presents our approach for collecting the perfor mance datafrom the readily available tests and manual labelling with test the performance metrics.
section presents our three research questions and our results to answer the three research questions.section5presentsthethreatstothevalidityofourstudy.
finally section concludes this paper.
3thedatafromourstudyissharedat related work inthis section wediscussthe priorresearchthat isrelatedtothis paper.
empiricalstudieson performance issues empiricalstudiesareconductedinordertogainadeepunderstanding of the nature of performance issues.
jin et al.
conducted an empirical study on real world performance issues that are collected from five representative software projects.
za man et al.
study a random sample of performance and non performanceissuesfrom mozillafirefox andgooglechrome.
huang et al.
study randomly selected real world performance regression issues from three open source systems.
based on the study results prior research found that that it is difficult to reproduceperformanceissuesandmoretimeisspentondiscussing performanceissuesthanother kindsofissues .therefore automated approaches are designed in order to assist in detecting performance issues and prioritizing performance tests basedonthestudyresults.priorresearchillustratestheimportance of addressing performance issues in practice.
our work can be adoptedbypracticesintandemwiththepriorresearchonthetopic of performance issues.performanceissues detection priorresearchbuildspredictivemodelsinordertopredictperformance issues .
lim et al.
formulate the performance issueidentificationasahiddenmarkovrandomfieldbasedclusteringproblem.
xiongetal.
leverage statisticalmodelsto model thesystemperformanceinthecloud.luoetal.
proposearecommendationsystem calledperfimpacttoidentifycodechangesthat maypotentiallycauseperformanceregressions.suchapproaches are appliedwith anew versionof the softwarein orderto detect performanceissue.however suchpriorresearchonperformanceis suemodelingdependsonalargeamountofperformancedatawith complex modeling techniques.
such approaches although proven tobeeffective aredifficulttoadoptinpractice duetotheirextra overheadandtherequiredresources.moreover suchapproaches are often conducted at the last stage of the release.
leveraging these approaches to detect every performance issue is difficult and impractical.
therefore our findings in this paper may complement existingapproachesinordertodetectperformanceissuefixesmore frequently during the rapid development processes.micro scaleperformance tests extensive prior research has proposed automated techniques to design executeandanalyzelarge scaleperformancetesting .
duetothecomplexityandtheresourcesneededforsuchlarge scale performance testing in recent years research has been conducted inordertostudyanddesignperformancetestinginasmallscale micro scaleperformance test .
leitner et al.
conduct a study on open source java projects to understand the state of art of performance testing.
similarly stefan et al.
conduct a study on the practices of using performance unit testing frameworks including caliper contiperf japex jmh junitperf.bothstudiesshowthatmostoftheperformance tests are smoke tests and the projects often use junitto testtheperformancecombinedwithfunctionaltest whileonlyfew open source projects use any performance unit testing framework.
1436towards the use of the readily available tests from the release pipeline as performance tests.
are we there yet?
icse may seoul republic of korea subject systemscollecting performance issues identifying the issue fixing commit identifying associated teststests execution performance data collection performance issue fixing commitissues reports labelling the performance issue with performance metrics associated testslabelled performance issueperformance data version control repository issue tracking system commit before performance issue fixesfor each issue figure an overview of our case study setup and performance data collection.
thesepriorpapersmotivateourworkinordertosupportamore flexible and low friction performance testing practice.
approachesaredesignedtoimprovetheexistingmicro performance testing.
bulej et al.
present a statistic approach to express performancerequirementsonunittesting.inaddition hork etal.
propose an approach to use performance unit tests to increase performanceawareness.
the priorresearch on micro performance testing motivates the needofknowingtheeffectivenessofthereadilyavailabletestsin performance assurance scenarios.
our findings can complement prior research in order to advance the practice of testing system performance in a targeted manner.
case study setup inthissection wefirstpresentthesubjectsystemsofourstudyandthecollectionofperformanceissuesfromthesubjectsystems.then we present our approach and experiment to collect performance dataandwealsopresenttheexperimentalenvironment.figure1 shows an overview of these steps.
.
subject systems webaseourstudyontwoopen sourceprojects hadoopandcassandra.hadoopis a distributed data processing system.
cassandra isafreeandopen sourcedistributednosqldatabasemanagement system.
we choose hadoopandcassandra since they are highly concernedwiththeirperformanceandhavebeenstudiedinprior research in mining performance data .
.
collecting performance issues we first collect the performance issues in the two subject systems.
we follow an approach similar to the one used in prior studies for performance issues collection.
in order to ensure that thereexists a performance improvement after the issue fixes we only focusontheissuereportsthathavethetype bugandarelabeled asresolvedorfixed.
weusekeywordsastheheuristicstoidentifyperformanceissue reports.
we start by using the keywords that are used in priorresearch .
in order to avoid missing performance issues we expandour listofkeywordsbyusing wordembedding.weadopt a word2vec model trained over 15gb of textual data from stack overflow posts to identify the words that are semantically relatedtotheexistinglistofkeywords.examplesoftheuncommon words that related to performance issues include sluggish and laggy which may not be used in previous research but can help collect performance issue reports.
by expanding the list of keywords we gathered a total of and issue reports in hadoopandcassandra respectively4.
intuitively not all issue reports are indeed related to performance issues.therefore thefirstandlastauthorsmanuallyexamineeveryissuereportindependentlytoconfirmthattheissuereportisrelated to a performance issue.
the two authors achieve an agreement of73.
.afterwards thetwoauthorsdiscusseachdisagreement to reach consensus.
when the consensus cannot be reached a third author examines the issue report and makes a final decision.
finally we collect and performance related issue reports in hadoopandcassandra respectively.theamountofissuereportsis comparableto prior study on performance issues .
.
labelling performance issues with performance metrics eachperformanceissuehasitscorrespondingperformancemetrics that can be measured and used to demonstrate the symptom of the performanceissueandtheimprovementafterfixes.forexample issuehadoop hasadescriptionof ...distributedfilesystem liststatusisveryslowwhenlistingadirectorywithasizeof ... .basedonthedescription weknowthattheperformance issue can be observed by measuring elapsed time of the execution andtheelapsedtimeshoulddecreaseaftertheissueisfixed.the first two authors manually label all of the collected performance issues with their correspondingperformance metrics.
in total we identify five performance metrics in our labelling of the performance issues in our subject systems i.e.
elapsed time cpu usage 4thetimeperiod ofthedata collectionis fromthestart dateof eachprojectto theday we collected the issues september .
1437icse may seoul republic of korea zishuo ding jinfu chen and weiyi shang memory usage i o read and i o write.
for hadoop and issues are labeled with elapsed time cpu usage memoryusage i o read and i o write respectively.
and issues from cassandra are labeled with elapsed time cpu usage memoryusage i oreadandi owrite respectively.notethatanissue report can have performance issues with multiple performance metrics.
the two authors have an agreement of .
on the la belling and a similar approach as the last step is followed when labelling disagreement occurs.
.
evaluating the fixes of performance issue inthissubsection wepresenthowdowestudytheuseofthereadily availabletestsfromthereleasepipelinetoevaluateperformance.
we first identify the performance issue fixing commits in order to identify the two snapshots of the source code i.e.
before and after fixingeachperformanceissue.wethenpresenttheselectionand execution of the associated tests that cover the issue fixing source code.
finally we present theperformance evaluation for each test inordertostudywhethereachtestcandemonstrateaperformance improvement for the performance issue fixes.
.
.
identifying performance issue fixing commits.
weclonethe gitversion controlrepositories ofour subject systems and use git logtoextractallthecodecommitstogetherwiththecorresponding commit messages.
the commit messages typically contain an issue id indicatingtheissuethateachcommitaddresses.withthisinformation we collect all the associated commits for each collected performance issue.
we note that there may exist multiple commits for fixing one issue.
one reason is that an issue may be too complex to fix in one commit.therefore developersmaydividethefixofanissueinto severalcommits.inaddition developersmighthavethoughtthat theissueisfixed whileactuallyisfoundnotfixed reopened and fixed in a later commit.
in these cases we consider the chronologicallastcommitsastheissuefixingcommits.wealsoexclude the commits that do not have any code changes.
finally if an issue idisnotcontainedinanycommitmessage weremovetheissue from our study.
afterthisstep 46issuesarefilteredout.andthen wecancollect two snapshotsof source codefor eachperformance issue i.e.
one before issue fixing and one after issue fixing.
we checkout both snapshotsof the source code for each performance issue.
.
.
executing associated tests.
both of our subject systems have alargenumberofteststhatareavailableinthereleasepipeline.we firstsearch forall testsbasedon theirbuildfiles.
hadoophasfour differentsub modules.weselectthetestsbyeachsub moduleto minimizethelargeamountofirrelevantteststosavecomputational resources.
for cassandra we include all the retrieved tests.
intuitively not all tests execute the source code that is changed by the performance issue fixes.
hence for each performance issue weidentifytheteststhatexecutethesourcecodethatischanged by the fixes impacted tests and the tests that do not un impacted tests .
we leverage code coverage tools to identify the executed lines in the source code for each test.
different code coverage tools areusedinthesubjectsystems.inparticular cobertura andjacoco are used for cassandra.
hadoopdepends on atlassian clover tocalculate code coverage.
since atlassian clover needs licenses to execute and all support was discontinued at april we turn toopenclover which is an open sourced version of atlassian clover to measurethe code coverage in hadoop.
if atest executes theaddedormodifiedlinesinthesourcecodebetweentwoversions beforeandaftertheperformanceissuesfixes weconsiderthetest impacted.inaddition fordeletedlinesofcode weconsideratest covering the code if the test executes the lines before and after the deleted lines.
by this we identify issues that have the impacted tests.
afterwards we run every test both impacted and un impacted individuallytoevaluateperformancethatisassociatedwitheach test.inparticular thetestsforeachperformanceissueareexecutedononevirtualmachinewith8gbmemoryand16corescpuhosted by google compute engine gce .
each test is independently executedwith30repetitionstominimizenoise.priorresearchstudiestheuseofcloudenvironmentonperformanceevaluationand showsthesuccessfuluseofsuchanumberofrepetitions .note that we also exclude the commits and the issues where the project fails to build and run.
in total we spent more than machine hoursforexecutingall thetestsforthe127performanceissuesin our subject systems.
.
.
evaluating the performance of each test.
to evaluate the performancethatisassociatedwitheachtest wecollectthefiveperformance metrics including the elapsed time cpu usage memory usage i o read and i o write as the labelling of performance issues.
we use psutil python system and process utilities for monitoring the cpu usage memory usage i o read and i o write oftheprocessthatexecutesthetests.
psutilhasbeenusedwidely in prior research on software performance .
we use test summaryreportsgeneratedvia ant maven andjunittomeasure the elapsed time of each test.
after this step we have collected performance data for all the tests both impacted and un impacted that are associated with two versions of source code before and after each performance issue fix of each performance issue.
we thenuse thisdatato answer our research questions.
case study results in this section we aim to answer the following research questions rq1 can the readily available tests from the release pipeline demonstrate performance improvements from performance issues fixes?
motivation.
performance issue reports are often used as a great source of knowledge in system performance assurance activities in prior research .
the certainty of having performance improvements thedescriptionofthereportsandtheavailablepatches make performance issues a great subject for prior research on software performance.
this research question concerns whether the performance of the readily available tests from the release pipeline can demonstrate performance improvements from performance issuefixes.ifnot thereadilyavailabletestswouldnotbecapable ofservingasperformancetestsforotherperformanceassurance activitieswitheven higher difficulty.
1438towards the use of the readily available tests from the release pipeline as performance tests.
are we there yet?
icse may seoul republic of korea approach.
analyzingperformanceevaluationresults.
foreach test we leverage statistical tests on the performance evaluation resultstodeterminewhethertheperformanceofthetesthaschanged after fixing the performance issue.
in particular for each performance issue we first select only the tests that are impacted by the performance issue fixes.
afterwards we check the label of the performancemetrics e.g.
elapsedtime seesection3.
thatare associatedwiththesymptomsoftheperformanceissues.wewould like to determine whether the corresponding performance metrics have different statistical significance values before and after the performance issues fixes.
duetothenon normalityoftheperformancedata weuse mannwhitneyutest asdoespriorwork .ournullhypothesis and alternative hypothesis are given below h0 the two performance result i.e.
test and control group thesametestbeforeandafterperformanceissuefixes are equal.
h1 the two compared tests do not have the same performance.
and we run the test at the level of significance i.e.
.
.
that is if the p valueof thetest is notgreater than .
i.e.
p value .
we would reject the null hypothesis in favour of the alternative hypothesis.
in other words there exists a statistically significant performance change between the performance metrics and thechangeis unlikely by chance.
however astatisticalsignificancetestdoesnotcontaintheinformation about the size of the effect and when the performance data points under study are formed by a great number of items thestatisticallysignificantdifferencesaremorefrequently observed .therefore wefurtheradopttheeffectsizeasa complementofthestatisticalsignificancetest.consideringthenonnormalityofourdatapoints weutilize cliff sdelta whichdoes not require any assumptions about the shape or spread of the two distributions .theeffectsizeisassessedusingthethresholds provided in prior research filteringfalse positiveresults.
toavoidthefalsepositives and eliminate the influence of the negligible or small changes of the performance we only consider the performance changes that have alargeeffectsize.inshort iftheperformancemetricofanimpacted testischanged inparticularimproved e.g.
lowercpuusage after the performance issue fixes with statistically significant difference and large effect size and the performance metric is also labelled for the performance issue we consider the test to be capable of verifying the performance issues fixes.
in order to further avoid false positive results we would like to understand the patterns of false positive results and use such patternstofilteroutourdata.inordertoidentifythemostobvious false positives wecheckthelargesttenperformancechanges in effectsizes c.f.
section4 intheun impactedtests nomodification committed on the source code covered by the tests in each subject system.wemanualstudyonthepossiblecausesofthefalsepositivechangesthatresideinthesourcecode.wefindtworeasons some functionaltestscontainrandomoperations whichcan lead tothe unstable performance and frequent i o operations.
therefore we do not consider the results of a test if the test is corresponding to either of these two reasons.finally we manually examine all the cases of each performance issue c.f.
section to ensurethat thetests indeeddemonstrate a performance improvement after a performance issue fix.
results.mostperformancefixes improvementscanbedemonstrated by at least one readily available test.
we find that for 56outof60oftheperformanceissuesin hadoopand46outof67 performance issues in cassandra at least one test from the release pipeline can be used to demonstrate performance improvements withalltheirassociatedperformancemetrics.inaddition forseven additional performance issues in cassandra performance improvements withpart ofthe performance metrics canbedemonstrated.
forexample thecommit 9afc209fixestheissue cassandra whichdescribesanendlessloopinthesourcecode.basedonthe report there should be improvements on both elapsed time and cpuusagefromtheissuefix.amongalltheimpactedtests elapsed timeandcpuusageareindeedimprovedsignificantlywithlarge effect size in three tests.
such results show the potential capability ofthereadilyavailabletestsfromthereleasepipelinetoserveas performance tests.
onlyasmallportionofthetestsfromthereleasepipeline canbeusedtodemonstrateperformanceimprovements.
figure2showsthepercentageofteststhatcanorcannotbeusedto demonstrate the improvements from performance issues fixes.
the resultsshowthatitwouldbechallengingforpractitionertodirectly use the readily available test in the release pipeline as performance tests.
in particular on average only .
and .
of the tests incassandra andhadoop respectively can demonstrate performance improvements for all associated performance metrics.
.
and .
of the tests in cassandra andhadoop respectively can demonstrateperformanceimprovementswithpartoftheassociated performance metrics.
on the other hand .
and .
of thetestsincassandra andhadoop respectively cannotdemonstrate any performance improvement even though these tests all executethechangedsourcecodefortheissuefixes.forexample tofixissuecassandra 25testsareimpactedbythecodechange while onlytwotestscandemonstratetheperformanceimprovementfrom the issue fix.
due to the large number of total available tests in the releasepipeline practitionersmaybeoverwhelmedbytheinflux of performance results from the tests in the release pipeline and thedifficultyof selecting the useful ones.
ononehand mostofperformanceimprovementsfromperformanceissuefixescanbedemonstratedusingthereadilyavailable tests in the release pipeline.
on the other hand it is challenging to use these tests in practice since only a very small portion of thetestscan demonstratetheimprovements.
rq2 what are the reasons that some tests in the release pipeline cannot be used as performance tests?
motivation.
in the last research question we find that many of the readily available tests in the release pipeline cannot demonstrate a performance improvement from the performance issue fixes even thoughthechangedsourcecodefortheissuefixesisexecutedby thesetests.therefore inthisresearchquestion wewouldliketo understandthereasonthatthesetestscannotserveasperformance 1439icse may seoul republic of korea zishuo ding jinfu chen and weiyi shang figure the percentage of tests that can or cannot be used todemonstrateperformanceimprovementsfromissuefixes foreach issue.
tests.thefindingsofthisresearchquestioncanassistpractitioners in avoiding the use of certain tests in performance assurance activitiesand in improving tests to serve as performance tests.
approach.
we follow a four step open coding approach to analyze the reasons that can cause a test to not be able to demonstrate performanceimprovements eventhoughthetestisimpactedby the issuefix.
based on the results from rq1 we collect all the impacted tests for the performance issues i.e.
the tests that cover the changed source code of the corresponding issue fix but do not demonstrate performanceimprovementsontheperformancemetricsoftheissue.
two authors independently examine each test to uncover reasons of not being able to demonstrate performance improvements.
in particular theauthorsexaminethefollowinginformationthatis associatedwitheachtest theperformanceissuereport which containsthehigh levelinformationfortheissues description the test code which contains the low level information of the tests andthechangedpartsofthecommittedfilesand3 thesourcecode covered by the test which tells us which lines have been executed by the tests.
step .the first two authors independently generate categories of reasons that a test cannot demonstrate performance improvements.inparticular eachauthoriterativelyinvestigatesallthetests to identify the reasons until no more new reasons can be found.
theoutcomeofthefirststepisthedifferentcategoryofreasonsby eachof the two authors.
step2.intuitively thetwoauthorswouldnotgenerateidentical categories.hence thetwoauthorsmeetanddiscusstheircategories.
the goal is to generate final categories of reasons that both of the two authors agree on.
the two authors discuss each of their generatedcategoriesofreasonsandreachconsensusonthefinal categories.
step .the two authors use the agreed categories from the second step.
the two authors independently put each test into one category.
step .finally thetwoauthorsexaminetheresultswherethe twoauthorsdonotagree.thetwoauthorsdiscusstheirrationaletotrytoreachconsensus.ifconsensuscannotbemade thethirdauthorwillexaminethecorrespondingtesttomakethefinaldecision.
the two authors have an agreement of .
.
results.weidentifyeightpossiblereasonsthatatestcannot be used to demonstrate performanceimprovements.
we discuss each reason in detail with examples in the rest of this rq.
too light workload tests .
we find that some performance issues can only be triggered with a rather large data size.however functional tests may not be written with such a large data size as input making it impossible to demonstrate the issue fixes.forexample theissuereportedin cassandra canbe triggered with a very large number of sstables.i ti sfi x e di nt h e commit 2b62df2.
however the impacted tests do not have a large enoughamount of sstablesasinput toreproduce theperformance issue.
not enough repetition tests .
some performance issues havearathersmalleffect whilebecomingimpactfulwithalarge number of repetitions.
for such performance issues the tests of ten can detect the performance improvements but only with asmallormediumeffect size which are not considered in our experiments to minimize noise.
however with more repetitions the effectcanincrease.forexample inthereportofperformanceissue cassandra developers mention that the method convertfromdiskformat usingsplitis slow only when being tested with morethan1 000keys.althoughatest randompartitionertest covers the code changed by the issue fix the method convertfromdiskformatiscalledonlyonceinthetestandtheelapsedtimeisslightly improvedwithasmalleffectsize.basedonthedescriptionofthe issuereport ifthereweremorerepetitionsaroundthismethod the performance improvement would be demonstrated by the test.
race conditions tests .
the race condition related performance issuescan onlyhappen when given a certain set of circumstances.
for example the commit 6158c64fixed the deadlock issue in the streaming code.
with the description provided in the report cassandra we find that we need a specific execution conditionto trigger the deadlock.
limited line coverage of the performance related codes 24tests .
wenoticethatdevelopersmaychangealargeamount ofsourcecodetofixperformanceissues butthetestonlycovers a small portion of the committed changes.
in this situation the performance of the test can be misleading since it does not tell the full picture of the issue fixes.
for example the commit 67ccdab fixedaperformanceissueinthestreamingcode.byusingthe gitdiff command weknowthatthereare10fileschangedwith437line additionsand243linedeletions.however amongthesechanges onlyonelineiscoveredbythetest sessioninfotest.moreover the covered lineisa refactoringoperation renamevariable andthe performance sensitive operations are never performed by the tests to demonstrate the performance improvement.
partial branch coverage tests .
if the performance issue iscausedbythecodeinsidethe ifstatement andwithoutthe100 coverage of the conditions the code snippets cannot be tested and thus the tests cannot demonstrate the fix to the performance issue.
a representative example can be found in the fixing process of issuecassandra .theperformanceissueiscausedbythe 1440towards the use of the readily available tests from the release pipeline as performance tests.
are we there yet?
icse may seoul republic of korea echoedrow function while this function cannot be invoked as it lies insidethe ifstatementwithouta branch coverage.
indirectperformanceinfluence 1test .
inthissituation the behaviorofperformanceissuerelatedcodeisbasedonthereturn value of another function.
therefore covering the fix locations of the issue may not be useful to demonstrate the fix to the performance issue.
for example in the fixing process of the issue cassandra whilebenchmarking cql3secondaryindexes developers noticed substantial performance degradation as the volume of indexed data increases.
the issue is caused by the page size selection whichisreturnedbyanotherfunction.wenoticethatthe testscancovertheuseofthereturnvaluewhilemissingitscaller.
therefore the tests cannot demonstrate the performance changes as expected.
frequent access of external resources tests .
frequent accessoperationsofexternalresourcesmayintroducenoiseintothe performance evaluation of the tests.
we find tests that may have frequenti ooperations includingtables creation deletion update anddatainsertionandselection or2 frequentmemoryoperations liketheflushoperations.forexample test defstestcoversthefix in commit 3ad3e73 for theissue cassandra .
however the test cannot demonstrate the improvement due to the noise from its largenumber of flush operations.
idle during execution tests .
some tests may proactively wait for a period of time introducing an idle time that is much longer than the actual execution time which reduces the observed performanceimprovementafterissuefixes.forexample inthecommit 3ad3e73 that fixes issue cassandra test cleanuptest contains a second thread.sleep operation with a total .685s elapsed test time.
in this case the elapsed time is dominated by the sleep time hiding the performance improvement after the issue fixes.
weidentifyeightpossiblereasonsthatatestinareleasepipeline cannot serve as a performance test.
the reasons can be used as a guidelineforpractitionerstoavoidandimprovetheuseofcertain testsfrom the release pipeline.
rq3 what are theimportant factors for a test to be useful as a performance test?
motivation.
priorresearchhasstudiedtheuseofmicro scaleperformancetestsinperformanceevaluation .however thefindingsinourpriorresearchquestionsillustratethechallenges and show the reasons why we cannot directly adopt those tests in performanceevaluation.onthe otherhand thereexisttestsfrom the release pipeline that successfully demonstrate performance improvements.byunderstandingthecharacteristicsofteststhat areabletodemonstrateperformanceimprovements wemaygain a better understanding of these tests and thus can provide more generalguidancetoadeveloperforwritingnewteststhatrunin the release pipeline for performance assurance activities.
approach.
to answer this research question we adopt random forest an ensemble learning method as it is one of the most used machine learning algorithms for its performance and has been adopted in various software engineering research .
we build a binary classifier to identify whether a test can be used to demonstrateperformance improvements.step raw data collection.
in rq1 we have identified the impacted tests of each performance issue and whether the test can demonstrate performance improvements.
however the ability of a test to serve as a performance test may vary among differentperformancemetrics.forexample atestthatcansuccessfully demonstrate memory usage improvement may not be able to show the improvement with elapsed time.
therefore in this step we separate the data based on each performance metric i.e.
we build one classifier for each performance metric.
for example to collect the raw data of elapsed time for project cassandra we first only take all the performance issues that are manually labelled withelapsed time.
then we collect the impacted tests of each performanceissue.foreachimpactedtest weusetheresultsshownin rq1 to determine whether the test can demonstrate a performance improvement.
the results in rq1 are considered the ground truthdatafor our classifier.
step2 metricsextraction.
tobuildclassifiers weextractmetricsfortherawdatacollectedfromthepreviousstep.theeffectiveness of a test can be associated with many metrics.
in this work we extract metrics from three aspects of the tests test code which contains the information about the test itself.
source code covered by the test where we canfind the test coverage rate and the characteristics of covered source code.
sourcecodeimpactedbytheissuefix whichmeasuresthe characteristics of committed changes of the source code whilefixingtheperformance issue.
theintuitionbehindtheselectionofthethreeaspectsisstraightforward as we are running the test to evaluate the performance of the covered source code and the performance improvements from issuesfixesshould be caused by the committed changes.
inspired by the work on defect prediction and theprior findingson performanceissues andperformance regressions weextractmetricsfromeachofthethree aspects.
some metrics exists in multiple aspects.
the details of the metricsare shown in table .
step training and testing random forest classifiers.
in this step we build random forest classifiers to model whether a test can demonstrate the performance improvements or not.
in particular webuildfiveclassifiers eachpredictingforoneperformance metric i.e.
elapsed time cpu usage memory usage i o read and i o write .
for each classifier we use a fold crossvalidationimplementationin scikit learn6withrandomshuffle .
wefitaclassifier onthetrainingdata andusethevalidation data totesttheclassifier.forourbinaryclassificationproblem weuse the area under the receiver operating characteristic roc curve auc as a performance measurement .
auc ranges in value from0to1 showingthecapacityoftheclassifierondistinguishing between classes.
a higher auc means a better classifier at predicting.finally wehave10 10modelsandcorrespondingauc values.inthisstudy weusetherandomforestimplementation7 stratifiedkfold.html randomforestclassifier.html 1441icse may seoul republic of korea zishuo ding jinfu chen and weiyi shang table an overview of our extracted metrics to build random forest classifiers.
tsf category metrics level description complexity and sizefanout method number of unique methods that are called by the code snippet.
fanin method number of unique methods that call the methods of the code snippet.
cyclomaticcomplexity method mccabe cyclomatic complexity of the code snippet.
sloc file number of source code lines in the code snippet.
codeelementssize method code elements divided by size.
diffusion entropy commit distribution of modified code across files in one commit.
historydevelopercount commit number of developers that changed the code snippet.
timeinterval file average time interval between the last and the current change of the code snippet.
human factordevelopercommitcount file average number of commits of the developers who modified the code snippet.
recentdevelopercommitcount file average number ofcommits made inlast months ofthe developers who modifiedthe code snippet.
code elementscondition method number of condition statements of the code snippet.
loop method number of loop statements of the code snippet.
exceptionhandling method number of try catch statements of the code snippet.
synchronization method number of synchronization statements of the code snippet.
finalstatic method number of final or static statements of the code snippet.
expensivevariableparameter method number of expensive parameters variables of the code snippet.
externalcall method number of external function call of the code snippet.
control method number of control statements of the code snippet.
code changecodechurn file total sum of lines added into and deleted from the code snippet across all the commit history.
lineadded file total sum of lines added into the the code snippet across all the commit history.
linedeleted file total sum of lines deleted from the code snippet across all the commit history.
coverage criterialinecoverage file line coverage ratio of the test.
brahchcoverage file branchcoverage ratio of the test.
note t s and f in the heading are abbreviations for the three aspect of metrics test code source code covered by the test and source code impacted by the issue fix.
means that the metricis calculated for the corresponding aspect.
androc auc score8functionin scikit learn totrainandevaluate our classifiers.
note that for i o read of project hadoop w e onlyhave13and 345functional teststhat canand cannotdemonstrate performance improvements.
the dataset is small for training aclassifier resultinginthemisleadingconclusions.therefore we do not train our classifier for i o read with hadoop.
step4 determiningimportanceofeachgroupofmetrics.
inthisstep weexaminetheimportanceofeachgroupofmetrics.
inparticular weextractthreegroupsofmetrics i.e.
fiximpacted source code test code and test covered source code.
we removeeach group of metrics from our data and rebuild the classifiers.
afterwards we measure the auc values of each classifier and compare with the auc values of the original classifiers with all metrics.
the more the auc values decrease the more important the group of metrics are.
step determining the importance of each metric.
to evaluate the importance of each metric on our random forest classifiers we adopt the mean decrease impurity mdi also called gini importance .
in a tree algorithm it calculates each metric simportanceasthesumoverthenumberofsplitsthatinclude themetric proportionally to the number of samples it splits.
for our random forest the importance is averaged over all trees oftheensemble.weusethefunction feature importances ofthe scikit learn9 in python to compute the metrics importance values.
afterwerepeatthe10 foldcross validationfor10times each metric has importance scores.
we then perform scott knotteffect size difference esd test on the metrics importance.
html randomforestclassifier.html sklearn.ensemble.randomforestclassifier.feature importances the scott knott esd test uses hierarchical clustering analysis to partitiondifferent metricsinto distinctgroups.
withthisanalysis each metric has a rank.
in this study we use the sk esdfunction of thescottknottesdpackage10in r .
finally to examine the direction of the relationship between each metric and the likelihood of a test being successful on demonstratingperformanceimprovements wemeasurethecorrelation betweeneachmetricandthetargets classesusingaspearmanrank correlation rho .apositivespearmanrankcorrelationindicates thatthemetricsharesapositiverelationshipwiththelikelihoodofa testbeingsuccessfulondemonstratingperformanceimprovements whereas a negative correlation indicates an inverse relationship.
results.ourrandomforest classifiersachieve high auc values considerablyoutperformingarandomclassifier.
forproject cassandra table2showsthat ourrandomforestclassifiersachieve anaverageaucof0.
.
.
.
and0.73forelapsedtime cpu usage memory usage i o read and i o write respectively.similarly for project hadoop our classifiers achieve an average aucof .
.
.
.
forelapsedtime cpuusage memory usage andi owrite respectively.theseresultsindicatethatourrandomforestclassifiersoutperformrandomclassifierswhendeterminewhetheratestcanbeusedfordemonstratingperformance improvements.
by analyzing the results we find that the higher auc value of elapsed time than the cpu usage memory usage i o read and i o write classifiers may be due to the larger number of available tests that can be used to demonstrate improvements in elapsed time over other performance metrics.
in addition we findthattheaucvaluesofalltheclassifiersarestable especially the models from the elapsed time.
the stable auc values of our classifierssuggestthatourclassifiersachievestableperformancein 1442towards the use of the readily available tests from the release pipeline as performance tests.
are we there yet?
icse may seoul republic of korea table an average of auc and auc changes after removing some metrics.
and means there is a decrease increase andno change of auc.
all metricsmetricswithout source code impacted by the issuefixmetricswithout test codemetricswithout source code covered by the testcassandraauc auc change auc change auc change elapsed time .
.
.
.
.
.
.
cpu usage .
.
.
.
.
.
.
memory usage .
.
.
.
.
.
.
i o read .
.
.
.
.
.
.
i o write .
.
.
.
.
.03hadoopelapsed time .
.
.
.
.
.
cpu usage .
.
.
.
.
memory usage .
.
.
.
.
.
i o write .
.
.
.
.
.
determining the effectiveness of using these readily available tests in the release pipeline in performance assurance activities.
the metrics extracted from the source code covered by the testplayanimportantroleintheusefulnessofatest.
table2 showsthatfor cassandra themetricsfromthesourcecodecovered by the tests always have a strong influence on the auc values among the classifiers for all performance metrics.
table presents thetopthreemostimportantmetricstotheclassifiers.tohavea betterunderstandingofthesemetrics wealsopresenttheirmetrics importancemeasuredusingmdi thedirection i.e.
thesignof of the relationship between these metrics and the likelihood of a test being successful on demonstrating performance improvements.
by examiningtable3 wefindthatfor cassandra themetricsfromthe source code covered by the test always have the largest mdi for all classifiers.
the linecoverage andbranchcoverage metrics lie in the toptworanksacrossalltheclassifiers.theresultsalsoshowthat thesetwometricshaveapositiveimpactontheunitusage i oread and i o writeperformance metrics.
it indicates that a test tends to successfully demonstrate a performance improvement from a performance issue fix if the test has a relatively higher line or branch coverage.
these findings confirm the results in our preliminarymanual study in rq2 i.e.
the tests with a lower line or branchcoverage have difficulty triggering the performance issues thus cannotdemonstratetheimprovementsfromtheperformanceissues fixes.
thisfinding suggests the importanceof coverage criteriain developing performance tests.
the metrics of the test itself play an important role in the usefulness of a test.
shown in table for hadoop the metrics related to the test code have a large influence on all the classifiers.
byexaminingthetopthreemostimportantmetricstotheclassifiers see table the sizeandtimeinterval metrics from test code and are also important on whether a test can demonstrating perfor mance improvements.
for project cassandra table shows that slocmetricofthetestcoderanksfirstinthei owriteclassifier.
thisslocmetric is also one of the top three important metrics in theelapsedtime cpuusage memoryusage andi oreadclassifiers.
theslocmetric has a positive impact in all the five performance metrics.it indicatesthat atest tendstosuccessfully demonstrate performanceimprov ements ifithasarelativelyhighersourcelines of code.
meanwhile for project hadoop the metric timeintervalalso lie in the top three most important metrics.
the negative sign indicatesthatif atest codeis updatedlong timeago itmay result in a low likelihood demonstrating the performance improvements.
finally for hadoop theimportanceofthemetric relativeexpensivevariableparameter from test code indicates that a readily available test tends to successfully demonstrate the performance improvementsfromperformanceissues especiallymemoryissues ifithas a relatively higher call of expensive variables in the test.
themetricsofthechangedsourcecodebyaperformanceissuefixdonotoftenplayanimportantroleintheusefulnessof a test.
we find that for hadoopthe average auc our random forest classifiers do not change when the metrics extracted from the source code impacted by the issue fix category see table .
in addition none of the metrics that are related to the source codeimpacted by the issue fix lies in the top three important metrics oftheclassifiers.thesefindingssuggestthatdevelopersmaypay more attention to the test code and the source code covered bythetest.somepractitionersmayliketofinetunethetestsforeveryperformance issuefix.
however ourresults suggestthat such fine tuning may not be cost effective since the characteristics from the changed source code of a performance issue do not typicallyplay an important role in whether the test can demonstrate the performance improvements from performance issue fixes.
metricsrelatedtothetestitselfandthesourcecodecoveredby thetestareimportantintheclassifiers.ontheotherhand the metricsrelatedtothecodechangesintheperformanceissuesfixes havealowimportance.practitionersshouldfocusondesigning and improving the tests instead of optimizing tests for different performance issue fixes.
threats to validity thissection discusses the threats to the validity of our study.
externalvalidity.
duetothelargeamountoftimeandcomputing resources for execution to identify performance improvements and thecodecoverageoftests ourevaluationisconductedontwoopensource software systems i.e.
hadoopandcassandra.
although our studyonlyfocuseson127performanceissues thescaleofourstudy iscomparabletopriorresearchonperformanceissues .our findingsmightnotbegeneralizabletoothersystems.futurestudies 1443icse may seoul republic of korea zishuo ding jinfu chen and weiyi shang table average rank of the top three influential metrics and the spearman rank correlation .
note a or sign of indicates a positive or an inverse relationship of the metric with the likelihood that a functional being able to demonstrate the performance improvements.
the largermdithata metric has the more influentialthemetricis.
cassandra rank aspect metrics mdi sd elapsed time s linecoverage .
.
s brahchcoverage .
.
t relativeexceptionhandling .
.
cpu usage s linecoverage .
.
s brahchcoverage .
.
t timeinterval .
.
memory s brahchcoverage .
.
s linecoverage .
.
t timeinterval .
.
i o read s branchcoverage .
.
s linecoverage .
.
t timeinterval .
.
i o write s brahchcoverage .
.
s linecoverage .
.
t sloc .
.
t relativeexpensivevariableparameter .
.
t timeinterval .
.
hadoop rank category metrics mdi sd elapsed time s lineadded .
.
s timeinterval .
.
s linedeleted .
.
cpu usage t timeinterval .
.
t relativeexceptionhandling .
.
t relativeexpensivevariableparameter .
.
memory t relativeexpensivevariableparameter .
.
s timeinterval .
.
t timeinterval .
.
i o write s lineadded .
.
t timeinterval .
.
t relativeexpensivevariableparameter .
.
note t and s in the aspects are abbreviations for the two aspect of metrics testcode and source code covered by the test.
can apply our approach on other systems such as commercial closed source systems.internalvalidity.
our issue report selection in the jiratracking system may be biased by the keyword definition.
although we use a manual identification process to verify whether the filtered issue reports are related to performance we may still miss performance issuethatdonotcontainanyofourlistedkeywords.ourapproach requires performance metrics to measure performance of available tests.inparticular weonlystudyfiveperformancemetricswhile there may be others if other people study and label other performance issues.
future studies can include more performance issues and metricsto complement the findings of our study.
the manuallabellingandmanualstudyresultsmaybesubjectivetothetwoauthors.
more user studies and surveys on practitioners may address thisthreat.
we use software metrics based on the findings from prior researchandalsoextractnewmetricshighlyrelatedtotest.wechoose ourpredictionmodel randomforest basedonitswidespreaduse inpriorsoftwareengineeringresearch andsinceittypically providesahighaccuracyinthemodeling.theremayexistother metrics and machine learning models that can be leveraged in our study wherefutureresearchcanexploretocomplementourfindings.construct validity.
thereexistotherperformanceassuranceactivities suchaperformanceregressiondetection .ourstudy choosestouseperformanceissuesbecauseoftheknowledgeand quality of issue reports and the certainty in performance improvements.
future research can complement our study by using readily availabletests in other performance assuranceactivities as performancetests.
therealwaysexistsnoisewhenmonitoringperformance .
in order to minimize such noise for each readily available test we repeattheexecution30timesindependently.thenweuseastatistically rigorous approach to measuring performance improvements.
further studies may opt to increase the number of repeated executionstofurtherminimizethethreatbasedontheirtimeandresource budget.ourapproachisbasedonthesystemperformancethatis recorded by psutil .
further studies may evaluate our approach by varying such performance monitoring tools i.e.
pidstat.
in our context we evaluate the performance of tests in a google cloud platform performance evaluation environment.
although weminimizethenoiseintheenvironmenttoavoidbias suchan environment is not exactly the same as in field environment of the users.tominimizethethreat weonlyconsidertheperformance improvements that have large effect size.
in addition with the advancing of devops more operational data will become available for future mining software repository research.
research based on field datafrom the real users can address this threat.
conclusion in this paper we evaluate the performance of readily available tests in the release pipeline and then examine whether these tests can be used as performance tests in particular to demonstrate the performance improvements from performance issues fixes.
by performing an exploratory study on a total of performance issues in two open source projects i.e.
hadoopandcassandra w e find that most of improvements from performance issues can be demonstratedusingthereadilyavailabletestsinthereleasepipeline.
moreover through a manualstudy we identify eight reasons that may lead a test not being able to demonstrate the performance improvements.finally webuildrandomforestclassifierstoidentify themostimportantmetricsthatinfluencethetests capabilityon demonstratingperformance improvements.
to summarize this paper makes the following contributions tothebest ofourknowledge our workisthefirst tostudy theuseofreadilyavailabletestsinperformanceassurance activities.
1444towards the use of the readily available tests from the release pipeline as performance tests.
are we there yet?
icse may seoul republic of korea weuncovereightreasonswhyareadilyavailabletestcannot be used as a performance test.
we find that a test itself and the source code covered by the test are the important factors for tests to be able to serve as performance tests.
our findings shed light on the opportunities and challenges in leveraging the readily available tests in performance assurance activities.
practitioners can use our uncovered reasons and factors as guidelines to design and improve tests that run in the release pipeline forperformance assurance activities.