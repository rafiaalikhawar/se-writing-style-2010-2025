on the relationship between codeverifiability and understandability kobi feldman jcfeldman wm.edu collegeof william mary williamsburg virginia usamartinkellogg martin.kellogg njit.edu new jersey institute of technology newark new jersey usaoscarchaparro oscarch wm.edu collegeof william mary williamsburg virginia usa abstract proponents of software veri f ication have argued that simplercode is easier to verify that is that veri f ication tools issue fewer false positives and require less human intervention when analyzing simplercode.weempiricallyvalidatethisassumptionbycomparing the number of warnings produced by four state of the art veri f icationtoolson211snippetsofjavacodewith20metricsofcode comprehensibilityfrom human subjectsinsix prior studies.
our experiments based on a statistical meta analysis show that in aggregate there is a small correlation u1d45f .
between understandabilityandveri f iability.theresultssupporttheclaim thateasy to verifycodeisofteneasiertounderstandthancodethat requires more e ffort to verify.
our work has implications for the users and designers of veri f ication tools and for future attempts to automaticallymeasurecodecomprehensibility veri f icationtools may have ancillary bene f its to understandability and measuring understandabilitymayrequirereasoningaboutsemantic notjust syntactic code properties.
ccs concepts softwareanditsengineering formalsoftwareveri f ication general andreference empirical studies .
keywords veri f ication staticanalysis code comprehension meta analysis acmreference format kobi feldman martin kellogg and oscar chaparro.
.
on the relationship between code veri f iability and understandability.
in proceedings of the31stacmjointeuropeansoftwareengineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.
introduction programmersmustdeeplyunderstandsourcecodeinordertoimplement new features f ix bugs refactor review code and do other essential software engineering activities .
however permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forpro f itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe f irstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspeci f icpermission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn .
studies have estimated developers spend oftheirtime understanding code.
complexity is a major reason why code can be hard to understand algorithms may be written in convoluted ways or be composed of numerous interacting code structures and dependencies.
there are two major sources of complexity in code essential complexity which is needed for the code to work and accidental complexity which could be removed while retaining the code s semantics .
whether the complexity is essential or accidental understandingcomplexcodedemandshighcognitive effort from developers .
researchershaveproposedmanymetricstoapproximatecode complexity using vocabulary size e.g.
halstead s complexity program execution paths e.g.
mccabe s cyclomatic complexity program data f low e.g.
beyer sdepdegree etc.thesesyntacticmetricsareintended to alert developers about complex code so they can refactor or simplify it to remove accidental complexity or to predict developers cognitive load when understanding code .
however recent studies have found that some of these metrics e.g.
mccabe s eitherweaklyordonotcorrelateatallwithcode understandability as perceived by developers or measured by their behavior and brain activity .
other studies have demonstratedthatcertaincodestructures e.g.
ifvsforloops f latvsnested constructs orrepetitive codepatterns leadto higherorlowerunderstandinge ffort a.k.a.
codeunderstandability orcomprehensibility whichdivergesfromthesimplisticway metrics e.g.
mccabe s measurecodecomplexity .
in this paper we investigate the relationship between understandabilityand codeveri f iability howeasyorharditisforadevelopertouseaveri f icationtooltoprovesafetypropertiesaboutthe code suchastheabsenceofnullpointerviolationsorout of bounds array accesses.
our research is motivated by the common assumptioninthesoftwareveri f icationcommunitythat simplercodeisboth easiertoverifybyveri f icationtoolsandeasiertounderstandbydevelopers.forexample thecheckerframework usermanualstates this assumption explicitly in its advice about unexpected warnings rewrite your code to be simpler for the checker to analyze this islikelytomakeiteasierforpeopletounderstand too .the documentation of the openjml veri f ication tool says success in checking the consistency of the speci f ications and the code will depend on... the complexity and style in which the code and speci f ications are written .
this assumption is widely held by veri f ication expertsbut has never been validatedempirically.
theintuitionbehindthisassumptionisthataveri f iercanhandle acertainamountofcodecomplexitybefore itissues awarning.if esec fse december3 san francisco ca usa kobifeldman martinkellogg andoscar chaparro itispossibletoremovethewarningbychangingthecode thenthe complexity thatcaused itmustbeaccidental rather than essential andthereforeremovingthewarningreducestheoverallcomplexity of the code.
for example consider accessing a possibly null pointerinajava likelanguage.asimplenullcheckmightusean if statement.
a more complexvariant withthe samesemantics might dereference the pointer within a trystatement and use a catch statement to intercept the resulting exception if the pointer is null.
thesecond moreconvolutedvariant withitssigni f icantaccidental complexity might not be veri f ied a null pointer dereference does occur but it is intercepted before it crashes the program.
a veri f ier would need to model exceptional control f low to avoid a false positivewarning.
alternatively averi f ier mightwarnabout code that makes unstated assumptions.
for example by dereferencing a possibly null pointer without checking it f irst the code assumesthatthepointerhasalreadybeenchecked.averi f iermight warn that this is unsafe unless a human provides a speci f ication that the pointer is non null.
in that case the veri f ier can verify the dereference but then must check that the value assigned to the variablereallyisnon null ateach assignment.a warningbecause ofamissingspeci f icationcanalsoindicatecomplexitythatahuman might need to reason about to understand the code the human might need to determine why it is safe to dereference the pointer.
our goal is to empirically validate the purported relationship betweenveri f iabilityandunderstandability andthereforeeither con f irmorrefutetheassumptionthateasy to understandcodeis easy to verify and vice versa .
to do so we need to measure veri f iability.
veri f iers analyze source code to prove the absence of particular classes of defects e.g.
null dereferences using soundanalyses.
a soundveri f ier can f ind all defects of awell de f ined class in the code.however mostinterestingpropertiesofprogramsareundecidable soallsoundveri f iersproducefalsepositivewarnings that is theyconservativelyissueawarningwhentheycannotproducea proof.
the user of the veri f ier must sort the true positive warnings thatcorrespondtorealbugsfromfalsepositivewarningsduetothe veri f ier simprecisionorduetotheneedtostatecodeassumptionsas speci f ications.thecombinationoffalsepositivewarningsandwarningsaboutunstated assumptions which we refertoas falsepositives or warnings forbrevity isagoodproxyforveri f iabilitybecausethefewersuchwarningsinagivenpieceofcode thelesswork adeveloper using the veri f ier willneedto do to verifythat code.
withthatinmind wehypothesize thatacorrelationexistsbetweena code snippet s comprehensibility as judged by humans and itsveri f iability as measured byfalse positivewarnings .
weconductedanempiricalstudytovalidatethishypothesis the f irst time that this common assumption in the veri f ication community is tested empirically.
our study compares the number of warnings produced by three state of the art sound static code veri f iers and one industrial strength unsound static analysis tool based on a sound core with 18k measurements code understandability proxies collected from humans in six prior studies for java code snippets.
suchmeasurements comefrom20metricsinfourcategories human judged ratings program output correctness comprehension time and physiological i.e.
brainactivity metrics.
weusedastatisticalmeta analysistechnique toexamine the correlation between veri f iability and these understandabilitymetricsinaggregate.giventhesmallsamplesizesoftheoriginal studies and the danger of multiple comparisons this meta analysis technique permits us to draw methodologically sound conclusions aboutthe overalltrends.
we found a small correlation between veri f iability and the proxies for understandability in aggregate u1d45f .
individually of metrics were correlated with veri f iability.
this trend suggests thatmoreoftenthannot codethatiseasiertoverifyiseasierfor humansto understand.
oneimplicationofthisresultisthatourresultsprovideevidence for a relationship between the semantics of a piece of code and its understandability which may explain in part the apparent ineffectiveness of prior syntactic approaches.
this implies that the veri f iability of a code snippet as measured automatically by the warnings issued by extant veri f ication tools might be a useful input to models of code understandability .
another implicationisthatwhenusingaveri f icationtool developersshould considermaking changes to the code to make it easier to verify automatically so is more likely than not to make the code easierforahumantounderstand.ifadevelopermakesachangeto removeafalsepositivewithoutchangingthecode ssemantics any complexity they remove must have been accidental.
this means that veri f ication tools provide a secondary bene f it beyond their guaranteesoftheabsenceoferrors codethatcanbeeasilyveri f ied should be easier for future developers to improve andextend.
in summary the main contributionsof this paper are empiricalevidenceofacorrelationbetweencodeunderstandabilityandveri f iabilityderivedviameta analysis supporting thecommon assumptionthateasier to verifycode iseasier for humans to understand and vice versa .
the results have implicationsforthedesignanddeploymentofveri f ication toolsandfordevelopingmoreaccurateautomatedmetrics ofcode comprehensibility and anonlinereplicationpackage thatenablesveri f ication andreplication ofour results andfuture research.
empirical studydesign our goal is to assess the correlation between human based code comprehensibility metrics and code veri f iability i.e.
how many warnings static code veri f ication tools issue.
intuitively our goal is to check if code that is easy to verify is also easy for humans to understand.tothatend weformulatetheseresearchquestions rqs rq1how human basedcodecomprehensibility metrics correlate withtool basedcode veri f iability?
rq2how do human based code comprehensibility metrics correlatewithtool basedcode veri f iabilityinaggregate ?
rq3whatistheimpactofeachveri f icationtoolontheaggregated correlation results?
rq4do different kinds of comprehensibility metrics correlate betterorworse withtool basedveri f iability?
rq1andrq2encodeour hypothesis thatacorrelationexists between tool based code veri f iability and human based code comprehensibility.
rq1asks whether individual metrics correlate with veri f iability.however duetothelimitationsofpriorstudies sample sizes for the metrics considered individually are too small to draw 212on the relationshipbetweencode verifiability andunderstandability esec fse december3 san francisco ca usa table datasets dss of code snippets and understandability measurements metrics used in our study.
the metrics types are c forcorrectness r forratings t fortime and p forphysiological.
ds snippets ncloc participants understandability task understandability metrics meas.
csalgorithms students determineprog.outputc correct output rating levelcorrectnessscoreforprogram output r output di fficulty leveldi fficultyscoreforprogram output t time to give output seconds toread program andanswer a question csalgorithms students determineprog.outputp brain deact 31ant deactivation of brainarea ba31ant p brain deact 31post deactivation of brainarea ba31post p brain deact 32 deactivation of brainarea ba32 t time to understand seconds tounderstand program within seconds 100ossmethods 121students rateprog.readability r readability level levelscoreforreadability easetounderstand ossmethods students and developersrateunderst.
answer qsr binary understandability program understandabilityscore c correct verif questions of correctanswers toveri f icationquestions t time to understand seconds tounderstand program ossmethods 104students rateread.
complete prog.c gap accuracy accuracyscorefor f illinginprogram blanks r readability level ba levelavg.
scoreforreadabilityb a codecompletion r readability level before levelscoreforreadabilitybeforecodecompletion t time to read complete avg.
seconds torate readabilityandcomplete code f csalgorithms students determineprog.outputp brain deact 31 deactivation of brainarea ba31 p brain deact 32 deactivation of brainarea ba32 r complexity level scoreforprogram complexity c perc correct output of subjects who correctlygaveprogram output t time to understand seconds tounderstand program within seconds reliableconclusions.therefore rq2askswhetherthereisapatterntotheanswersto rq1thatsummarizestheoverallcorrelation trend.
we answer rq2statistically by combining the results of the individualmetrics targetedby rq1withameta analysis.
rq3asks whether veri f iability measured using only one tool s warnings correlates with comprehensibility.
intuitively we aim tocheck ifthepatternsofcorrelations aresimilaracrosstools indicating generalizability and whether any particular tool dominates theresults.
to answer thesecondpart we usea leaveone out ablationanalysis droppingeachtoolindividually.
rq4 asks whether there is any di fference in correlation between veri f iability and di fferent proxies for code comprehensibility.
based on prior work we focus on four metric categories correctness rating time andphysiological .together theanswersto rq3and rq4helpusexplainour rq1andrq2results theyexplorethe tool s andmetric s responsible for the observedcorrelations.
toanswerourrqs we f irstcompiledasetofhuman basedcode comprehensibilitymeasurementsfrompriorstudies section .
.
then we de f ined our metric for code veri f iability section .
and executedfourveri f ication basedtoolsonthesamecodesnippets to measure how often each snippet cannot be veri f ied sections .
and2.
.
next we conducted an analysis of the warnings produced by the veri f iers to ensure that they met our de f inition of false positive section .
.
finally we correlated the comprehensibility metrics with the number of warnings produced by the tools and analyzed the correlation results using a meta analysis section .
.
we did a correlation study rather than try to establish causation because that would require expensive controlled experiments with humansubjects.sincecorrelationcannotexistwithoutcausation it is practical to re use existing studies and establish correlation f irst beforeattemptingacausationstudy whichweleaveasfuturework.
.
codeandunderstandability datasets we used existing datasets dss from six prior understandability studies whicharesummarizedintable .each study used a di fferent set of code snippets and proxy metrics tomeasureunderstandabilityusingdi fferentgroupsofhumansubjects whoperformedspeci f ic understandabilitytasks see table .
toselectthesedatasets weleveragedthesystematicliterature review conducted by mu oz et al.
who found ten studies that measuredcodeunderstandabilitywithpubliclyavailabledata.from these ten studies we selected the f ive studies whose snippets were writteninjava sincetheveri f iersweconsideronlyworkonjava code seesection .
.toidentifythedatasetsandfacilitatereplication we use the same nomenclature asmu oz et al.
s ds1 ds2 ds3 ds6 and ds9.
since those f ive studies were conducted before weperformedaliteraturesearchofadditionalcomprehensibilitystudiesfrom2020toearly2023andfoundtheone bypeitek et al.
dataset fordsf whoalsousedjava snippets.
in total we used 18k understandability measurements see the meas.
columnintable for211javacodesnippets collectedfrom human subjects using metrics.
the snippet programs are to non comment blank loc or ncloc ncloc on avg.
with di fferent complexity levels as reported by the methodology of their respective studies .
datasets and derive from open source software projects oss e.g.
hibernate jfreechart antlr spring weka and the other datasets are implementations of algorithms from 1st year programming courses e.g.
reversing an array .
the originalstudiesselectedshortcodesnippetstocontrolforpotential cofoundingfactors that maya ffectunderstandability .
weselectedtheunderstandabilitymetricsusedinthemeta study conducted by mu oz et al.
see table 1for the metrics their type andabriefdescriptionofthem ourreplicationpackagehas fulldescriptions .wealsousedmu oz etal.
scategorization of the metrics.
correctness metrics marked with a cin table1 measurethecorrectnessoftheprogramoutputgivenbytheparticipants.
time t metrics measure the time that participants took toread understand orcompleteasnippet.
rating r metricsindicate the subjective rating given by the participants about their understanding of the code snippet or code readability using likert 213esec fse december3 san francisco ca usa kobifeldman martinkellogg andoscar chaparro scales.physiological p metrics measure the concentration level of the participants during program understanding via deactivation measurementsofbrainareas e.g.
brodmannarea31orba31 .
study participants were mostly cs undergraduate graduate students with intermediate to high programming experience as reported in the original papers .
only ds6 s study includedprofessional developers see table .
weusedalltheavailabledatafromthesixoriginalstudies.their measurements come in aggregate or individual form e.g.
the physiological measurements in ds2 and dsf are provided persnippetaveragedacrossparticipants whiletheds3measurementscomeforeachparticipantandsnippet .somestudiesalso includedanunevennumbersofparticipantspersnippet duetodifferentmethodologicaldecisions.forexample ds9 sstudyincluded arandomassignmentofparticipantstooneofsixsequencesof f ive snippets .
in ds6 s study six snippets were understood by eightparticipantsandtheremaining44snippetswere understood bynine.forthesereasons eachdataset snumberofmeasurements see the meas.
column in table is not always divisible by the number ofsnippetsandparticipants.
.
proxyforcodeveri f iability we de f inecode veri f iabilityas theeffortthat a developer incurs when using a veri f ication tool to prove safety properties about a snippet of code.
since measuring this e ffort is infeasible without runningastudywithveri f icationtoolusers weuse falsepositive countsas an automatable proxyfor veri f iability.
wede f inea falsepositive asaveri f ierwarningthatindicates the veri f ier is unable to prove that a code snippet is correct due to aweakness intheveri f ier i.e.
undecidablilty ordueto a missingcodespeci f ication whichahumanshouldprovide.ine ffect a false positive warning represents a fact about the code that the veri f ier needs but cannotprove withthe snippet scode only.
our de f inition of false positive di ffers from the typical one whenevaluatingtheprecisionandrecallofaveri f ier.inthatcontext itisassumedthatcorrectspeci f icationsareexplicitandavailable and falsepositive meansafactthattheveri f iercannotprove even withaspeci f ication.conversely inourcontext wewantaproxy for thedifficultyof verifying a snippet.
that di fficulty includes bothwritingspeci f icationsandsuppressingfalsepositivewarnings so it is sensible to include both in our proxy for veri f iability.
in otherwords thefewerwarningsaveri f ierissues thelessworka developer using the veri f ier mustdoto verifyacode snippet.
weconsideredtwootherproxiesforveri f iability numberoffalse positives after writing speci f ications and number of facts veri f ied aboutthecode.wediscardedtheformerproxybecausethefullcontextofhowthesnippetsareintendedtobeusedisnotavailableand becausewritingspeci f icationsiserror andbias prone.thelatter wasdiscardedbecausenoneoftheveri f iersprovidetheproxydirectly andbecauseapproximatingwhetherornotaveri f ierneedsto evencheckafactisundecidableforsomepropertiesconsideredbya veri f ier e.g.
determining what is considered a resource in the code byaresourceleakveri f ier soaprecise count isimpossible.
.
veri f ication tools we usedthe following criteriato selectveri f ication tools eachtoolmustbebasedonasoundcore i.e.
theunderlying techniquemustgenerateaproof.
eachtoolmustbe activelymaintained.
eachtoolmustfail to verifyat leastone snippet.
eachtoolmustrun mostly automatically.
eachtoolmusttarget java.
criterion requires that each tool be veri f ication based.
our hypothesis impliesthat the process ofveri f icationcanexposecode complexity that is our purpose in running veri f iers is not to expose bugs inthe code but to observe when the tools produce false positive warnings due to code complexity .
therefore each tool must perform veri f ication under the hood i.e.
must attempt to constructaproof forourresultstobemeaningful.thiscriterion excludes non veri f ication static analysis tools such as findbugs whichuseunsoundheuristics.exploringwhetherthosetoolscorrelate with comprehensibility is future work.
however criterion doesnotrequire the tool to be sound merelythat it be based ona sound core.
we permit soundiness and intentionally unsound tools because practical veri f ication tools commonly only make guarantees aboutthe absenceof defectsundercertainconditions.
criteria 2through are practical concerns.
criterion2 requires the veri f ier to be state of the art so that our results are useful to thecommunity.criterion3requireseachveri f iertoissueatleast onewarning fortoolsthatverifyapropertythatisirrelevantto the snippets and so do not issue any warnings we cannot do a correlationanalysis.criterion4excludesproofassistantsandother tools that require extensive manual e ffort.
criterion restricts the scopeofthestudy wefocusedonjavacodeandveri f iers.wemade thischoicebecause veri f iersareusuallylanguage dependent manypriorcodecomprehensibilitystudiesonhumansubjectsused java e.g.
10studiesinmu oz etal.
andnootherlanguage hasmorethan2 10studies and javahasreceivedsigni f icant attentionfromtheprogramveri f icationcommunityduetoitsprevalence in practice.
we discuss the threats to validity that this and otherchoicescause insection .
.
.
selected verification tools.
by applying the criteria de f ined above we selectedfourveri f ication tools infer is anunsound industrial static analysis tool based on asoundcoreofseparationlogic andbi abduction .separation logic enables reasoning about mutations to program state independently making it scalable bi abduction is an inference procedure that automates separation logic reasoning.
infer is unsound by design despite internally using a sound separation logic based analysis it uses heuristics to prune all but the most likely bugs from its output because it is tailored for deployment in industrial settings.inferwarnsaboutpossiblenulldereferences dataraces andresourceleaks.we usedinfer version1.
.
.
thecheckerframework isacollectionofpluggabletypecheckers whichenhanceahostlanguage stypesystemtotrack an additional code property such as whether each object might be null.
the checker framework includes many pluggable typecheckers.
we used the nine that satisfy criterion which prevent programming mistakes related to nullness interning objectconstruction resourceleaks arraybounds signaturestrings formatstrings regularexpressions andoptionals .
we usedchecker framework version3.
.
.
214on the relationshipbetweencode verifiability andunderstandability esec fse december3 san francisco ca usa table numberofsnippets eachtoolwarns on andthetotal numberofwarnings perdataset.
snippets warned on totalwarnings tool dataset f all f all infer checkerfr.
jatyc openjml all tools thejava typestate checker jatyc is a typestate analysis .atypestate analysisextendsatypesystemtoalsotrack states forexample atypestatesystemmighttrackthata fileis f irst closed then open then eventually closed.
currently maintained typestate basedjavastaticanalysistoolsincludejatyc atypestateveri f ier andrapid anunsoundstaticanalysistoolbased onasoundcorethatpermitsfalsenegativeswhenveri f icationisexpensive .wechosetousejatycratherthanrapidfortworeasons.
first jatycshipswithspeci f icationsforgeneralprogrammingmistakes but rapid focuses on mistakes arising from mis uses of cloud apis the snippets in our study do not interact with cloud apis.
second jatyc is open source but rapid is closed source.
jatycwarnsaboutpossiblenulldereferences incompleteprotocols on objects that have a de f ined lifecycle such as sockets or f iles andaboutviolationsofitsownershipdiscipline whichissimilarin spiritto rust s .
we usedjatyc commit b438683.
openjml convertsveri f icationconditionstosmtformulae and dispatches those formulae to an external satis f iability solver.
openjml veri f ies speci f ications expressed in the java modeling language jml itisthelatestinaseriesoftoolsverifyingjml speci f ications by reduction to smt going back to esc java .
openjmlveri f iestheabsenceofacollectionofacommonprogrammingerrors includingout of boundsarrayaccesses nullpointer dereferences integer over and under f lows and others.
we used openjml0.
.
alpha withthe defaultsolver z3 v. .
.
.
.
.
verificationtoolsconsideredbutnotused.
weconsidered and discarded three other veri f iers jayhorn which fails criterion cognicrypt which fails criterion and java pathfinder whichfails criterion4.
.
snippetpreparationandtoolexecution we acquired the snippets from prior work but had to make some modi f ications to prepare them for tool execution.
ds3 included commented out snippets which we uncommented.
to make the snippets compilable we created stubs for the classes methodcalls etc.theyusewithoutmodifyingthesnippetsthemselves.
since the the snippetsthemselves didnot change theirunderlying measuredcodecomprehensibilitydidnotchangeeither intheoriginalstudies thesnippetswereprovidedtothehumans in isolation.
at the same time our modi f ications would change the programs state if the snippets were to be executed.
we performed a manual analysis of tool warnings to ensure our modi f ications did notcausespuriouswarnings seesection .
.wecreatedscripts to execute the veri f iers on the snippets and display all veri f ication failures for each tool.
table 2shows descriptive statistics of the warnings issuedbyeachtooloneachdataset.
.
codecorrectnessandwarningvalidation weassumedthateverywarningissuedbyaveri f ieraboutasnippet is a false positive according to the de f inition we presented in section2.
.ineffect thismeansthatwearetreatingthesnippetsas if they are correct.
for example if a snippet dereferences a pointer withoutanullcheck weassumethatpointerisnon null ifasnippet accessesanarraywithoutcheckingabound weassumethatthe bound was checked elsewhere in the program etc.each veri f ier warning therefore represents some fact that the veri f ier needs butcannotprovewiththesnippet scodeonly.weconsiderthese reasonable assumptions because no context about the snippets is available or was presented to the human subjects in the prior studies and the snippets are likely to be correct as researchers showedthemto humansinprior comprehensibilitystudies.
however tocheckthatthese assumptionsdonot skewour correlation analysis we manually validated whether a representative sampleoftoolwarningswereindeedfalsepositives accordingto ourde f initionfromsection .
.afterexecutingtheveri f iers one author examined a representative subset of the warnings a sample of of at con f idence level and of error margin andrecordedthecauseofeach.asecondauthorexaminedthe f irst author s assessment and both authors discussed the cases where the assessment was incorrect or needed more details reaching consensus in case of disagreements of which there were .
of these 344warnings nonewere realbugs inthesensethattheyareguaranteed to make the code fail when executed.
many do represent potential bugs that is code that does not check boundary conditionssuchasnullability however thesewarningscouldberemoved by writing a speci f ication for the relevant veri f ier indicating the assumptionsmade by the snippet.
this means these warnings are allfalse positives according to our de f inition from section .
.
inthesample themostcommonreasonsforaveri f iertowarn were violation sofjatyc srust likerulesformutability and violationsoftheveri f iers assumptionsaboutnullability.other common causes were possible integer over and under f lows too large or too small array indexes and unsafe casts.
our analysis of warnings for each snippet indicates a fairly uniform distribution of warning types over the datasets.
our replication package provides our detailedanalysisofwarnings .
.
correlation andanalysis methods .
.
aggregation.
we aggregated the comprehensibility measurementsandthenumberoftoolwarningsforeachcodesnippetinthe datasets.theresultingpairsofcomprehensibilityandveri f iability valuesper snippetwere correlatedfor setsof snippets.
215esec fse december3 san francisco ca usa kobifeldman martinkellogg andoscar chaparro speci f ically we averaged the individual code comprehensibility measurements per snippet for each metric.
for example for eachsnippetinds1weaveragedthe time to give output measurements collected from the participants in the correspondingstudy .followingmu oz etal.
weaverageddiscrete measurements which mostly come from likert scale responses in the original studies.
for example the metric output di fficulty from ds1 is the perceived di fficulty in determining program outputusing a discretescale.
while there is noclearindication of whetherlikertscalesrepresentordinalorcontinuousintervals we observed that the likert items in the original datasets representdiscretevaluesoncontinuousscales soitisreasonable to average these values to obtain one measurement per snippet.
allphysiologicalmeasurementsgivenbytheoriginalstudiesare averagedacrossallparticipants whounderstoodagiven snippet.
regardingcodeveri f iability wesummedupthenumberofwarnings from the veri f ication tools for each snippet.
we considered averaging rather than summing up.
however since the correlation coefficientthatweused seebelow isrobusttodatascaling i.e.
the averageisessentiallyascaledsum imbalancesinthenumberof warnings from each tool do not change the correlation results.
further forrq3 we performed an ablation experiment to investigate possible e ffectsofwarning imbalances oncorrelation.
.
.
statisticalmethods.
weusedkendall s u1d70f u1d70f u1d70f tocorrelatethe individualcomprehensibilitymetricsandthetoolwarningsbecause it does not assume the data to be normally distributed and have alinearrelationship itisrobusttooutliers and it has been used in prior comprehensibility studies .
as in previous studies we follow cohen s guidelines and interpretthecorrelationstrengthas nonewhen0 u1d70f .
small when0.
u1d70f .
mediumwhen0.
u1d70f .
andlarge when0.
u1d70f .
toanswer rq1 we f irststatedtheexpectedcorrelation aseitherpositiveornegative betweeneachcomprehensibilitymetric and code veri f iability that would support our hypothesis.
for some metrics suchas correct output rating inds1 a negativecorrelation indicates support for the hypothesis if humans can deduce the correct output moreoften the hypothesis predicts a lowernumber ofwarningsfromtheveri f iers.apositiveexpectedcorrelation such asfortime to understand inds6 indicates thathighervaluesin thatmetricsupportthehypothesis e.g.
ifhumanstake longerto understand a snippet our hypothesis predicts that morewarnings willbeissuedonthatsnippet.wecomputedthecorrelation and its strength between the comprehensibility metrics and code veri f iability and compared the observed correlations with the expected ones to checkif the results validate orrefuteour hypothesis.
toanswer rq2 weperformedastatisticalmeta analysis oftherq1correlationresults.ameta analysisisappropriatefor answering rq2because it combines individual correlation results that come from di fferent metrics as a single aggregated correlationresult .indisciplineslikemedicine ameta analysisis used to combine the results of independent scienti f ic studies on closely related research questions e.g.
establishing the e ffect of atreatmentforadisease where eachstudyreportsquantitative results e.g.
a measured e ffect size of the treatment with somedegreeoferror .themeta analysisstatisticallyderivesanestimateoftheunknowncommontruth e.g.
thetruee ffectsizeof the treatment accounting for the errors of the individual studies.
typically a meta analysis follows the random e ffects model to account for variations in study designs e.g.
different human populations .intuitively arandom e ffects basedmeta analysis estimatesthetruee ffectsizeastheweightedaverageofthee ffect sizesoftheindividualstudies wheretheweightsareestimated viastatisticalmethods e.g.
sidikandjonkman s .
since the comprehensibility measurements come from di fferent studies with di fferent designs i.e.
with different goals comprehensibility interpretations and metrics code snippets human subjects etc.
arandom e ffects meta analysisis appropriate to estimatean aggregated correlation.in our case however we f irst combine the resultsoftheindividualcorrelationanalyses i.e.
foreachmetric for each dataset into a single aggregated correlation per dataset to avoidthe unit of analysis problem see .
.
.thisproblem arises in meta analysis when there are inputs that are not independent i.e.
are themselves correlated typically because they representmultiplemeasurementsobtainedonthesamepopulation.
because most of our datasets include multiple metrics that were derived from the same subjects and snippets and therefore are related e.g.
readability level ba andreadability level before from ds9dependononeanother ana veapplicationofmeta analysis that treated each metric as independent would over weight studies with multiple metrics because it would double count their statistical power i.e.
multiply the statistical power of the study by the number of metrics it contains .
we con f irmed that most of the combinationsofmetricswithinasinglestudyshowedmediumor largecorrelations 28combinationsaremediumorlargecorrelations of those are large so the unit of analysis problem could seriously skewour results.
dealingwiththeunit of analysisprobleminmeta analysesof smallnumbersofstudies asinour case with multiplecorrelated metricsisanopenprobleminstatisticalmethodsresearch.weconsidered the recently proposed correlated and hierarchical e ffects che model butdiscoveredthat forourdata itwashighly sensitivetothechoiceofthe rhoparameter whichrepresentsan assumptionabouthowmuchvariancethereisbetweenthedi fferent metricsineachstudy .sincewewantedtobeconservativeinour choiceofstatisticalmethod wechosethe bruteforce aggregation approachsuggestedby whichtradesstatisticalprecisionfor simplicity and conservatism it combines the correlation results of the various metrics ineach study into a single estimateof correlation whichguaranteesthatnostatisticalpowerisderivedfromthe presence ofmultiple metrics on thesamepopulation even if such power might be warranted .
though it also has a rhoparameter the results for our data are insensitive to the choice of rho with extremelyhighandlowvaluesof rhogivingnearly identicalresults.
allmeta analysesinsection 3userho .
.
to perform the random e ffects meta analysis we followed a standard procedure for data preparation and analysis .
first we transformed kendall s u1d70fvalues into pearson s u1d45f u1d45f u1d45fvalues .
then wetransformedthe u1d45fvaluestobeapproximatelynormallydistributed usingfisher sscaling.next wenormalizedthesignsofthe individualmetriccorrelations i.e.
the u1d45fvalues sothatanegative 216on the relationshipbetweencode verifiability andunderstandability esec fse december3 san francisco ca usa correlation supports our hypothesis the choice of negative is arbitrary choosing positive leads to the same results with the opposite sign wemultipliedby 1thecorrelationvalueformetricswhere apositivecorrelation wouldsupportthehypothesis.
thisstrategy hasbeenusedinotherdisciplineswhencombiningdi fferentmetrics whosesignshaveoppositeinterpretations e.g.
in .weusedr s dmetarpackage version .
.
toaggregatethe correlations of the metrics from each study and the r s metaforpackage version3.
torunthemeta analysisandgenerateforestplots tovisualizethepearson s u1d45fvalues theirestimatedcon f idenceintervals theestimatedweightsfortheaggregatedcorrelation and additionalmeta analysisresults e.g.
p valuesandheterogeneity .
toanswer rq3 weappliedthesamemethodologyas rq2for each individual tool s warnings i.e.
no aggregation was used .
we also performed a leave one tool out ablation experiment to checkifanysingletoolwasdominatingtheoverallmeta analysis results.
to answer rq4 we repeated the same methodology for onlythemetricsineachmetriccategory time correctness rating andphysiological i.e.
weperformedfourmeta analyses onefor eachmetric group.
whileweprovidethe u1d45d valuesofallofthesestatisticalanalyses weemphasizethattheyshouldbeinterpretedwithcautiongiventhe relativelysmallsamplesizes and for rq1 thatfactthat20metrics areconsidered .forexample ds2onlycontains12snippets which means only data points were used for correlation for its metrics.
we also used a meta analysis rq2 rq4 because interpreting the individualmetricresults rq1 todrawgeneralconclusionsforour hypothesiscanbemisleading .ourmeta analysisalsoobviates the need for statistical correction to avoid multiple comparisons suchasholm bonferroni s themeta analysisaggregatesall of the results and informs us of the overall trend.
we use the same interpretation guidelines for pearson s u1d45fvalues that we used for kendall s u1d70f smallwhen0.
u1d45f .
etc.
.
studyresults and discussion we present and discuss the results of our study in this section.
scripts and data that generate these results are available in our replication package .
.
rq1 individual correlation results table3summarizestheresultsofeachmetric scorrelation based on kendall s u1d70f with the total number of warnings from all tools.
weprovidedescriptivestatisticsabouttheseresults butweemphasize that these results should be interpreted with caution votecounting e.g.
checking the number of statistically signi f icant metrics ineach direction canlead to misleading conclusions .
weavoiddirectlydrawingconclusionsfromtheseresultsandinstead we investigate the aggregated trend of all metrics with a meta analysisinsection .
.
table3showsthatfor13ofthe20 metrics thedirectionof thecorrelationsupportsourhypothesis.for4metrics thereis no correlation andfortheremaining3metrics thecorrelationisin theoppositedirectionthanexpected.table 3indicatesthestrength ofthecorrelationintherightmostcolumn.ofthemetricswherewe found amediumor higher correlation are in the direction that supportsourhypothesis.fortheother5metricsthatsupportourtable3 correlationresultsbasedonkendall s u1d70f u1d70f u1d70f k. s u1d70f u1d70f u1d70f for each dataset ds and metric.
a metric falls into one type correctness time rating physiological.theexpectedcorrelationdirection exp.cor.
ifour hypothesisiscorrect is either positive or negative.
we assess u1d70f u1d70f u1d70f s direction strength compared to the expected correlation exp?
means no correlation y y means expected and measured correlations match thus supporting our hypothesis and n n means theydonotmatch.capitallettersindarkercolors y n mean amedium or higher correlation.
lowercase letters and lighter colors y n mean a smallcorrelation.
u1d70f u1d70f u1d70f s signi f icanceistested at the u1d45d .
u1d45d .
u1d45d .
u1d45d .
u1d45d .
u1d45d .01levels .
ds metric type exp.cor.
k. s u1d70f u1d70f u1d70fexp?
1correct output rating c negative .
y output di fficulty r negative .
y time to give output t positive .
y 2brain deact 31ant p negative .
y brain deact 31post p negative .
y brain deact 32 p negative .
y time to understand t positive .
y 3readability level r negative .
y 6binary understand r negative .
correct verif questions c negative .
time to understand t positive .
9gap accuracy c negative .
y readability level ba r negative .
readability level before r negative .
n time to read complete t positive .
n fbrain deact 31 p negative .
y brain deact 32 p negative .
y complexity level r positive .
y perc correct out c negative .
y time to understand t positive .
n hypothesis and the metrics that do not their correlation is small.
whilewecannotdirectlydrawconclusionsfromtheseresultsabout theoveralltrend theyaresuggestive.weexaminetheaggregate trend rigorouslywithameta analysisinsection .
.
withregardtometriccategories 4correctness c metrics rating r metrics 5time t metrics and5 5physiological p metrics correlate with veri f iability.
all metrics that anti correlate withveri f iabilityareconcentratedintheratingandtimecategories.
these two metric categories are the most subjective ratings are opinions and some time metrics require the human subjects to signaltheexperimenterwhentheycompletethetask.theseresults suggest that there may be a relationship between metric categories and the correlation with veri f iability we further investigate the differences between metric categoriesinsection .
.
.
rq2 aggregatecorrelation results becausedirectinterpretationoftable 3isdifficultduetothedi fferent samplesizesofthevariousstudies weperformedameta analysis tounderstandtheoveralltrend see f ig.
.asnotedinsection .
.
217esec fse december3 san francisco ca usa kobifeldman martinkellogg andoscar chaparro re model .
.
.
pearson s r negative correlation supports our hypothesis f96321 .
.
.
.
.
.
.
.
.
.
.
.
.
dataset estimate of snippets weight p .
heterogeneity test q .
df p .
figure results of the random e ffects meta analysis of the metrics in table after aggregating the results by dataset to avoid theunit of analysisproblem section .
.
.
the results are presented by dataset rather than by metric to avoid unit of analysiserrors .
the forest plot in f ig.1displays the observed correlation person s u1d45f u1d45f u1d45fvalue obtained from the kendall s u1d70fvalue as described insection .
.
andthe95 con f idenceinterval estimate ci as well as the estimated weight for each dataset weight .
this information is shown numerically and graphically in f ig.
.
eachbox ssizeisthedataset s estimatedweight alargerbox size meansa largerweight andthe box smiddlepointrepresents the correlationwithrespecttothedashedverticallineatzero.there is a negative correlation ifthe box isto theleft ofthe verticalline positiveifitistotheright allmetricshavebeennormalizedsothat the expected correlation is negative that is a negative correlation supports our hypothesis .
the horizontal lines visualize the con f idence intervals for each dataset.
at the bottom the plot shows theaggregatedcorrelation ontheright andrelatedinformation calculated by the meta analysis.
the diamond at the bottom of the plotvisualizestheaggregatedcorrelation thewidthofthediamond represents the con f idenceinterval.
figure1shows a small aggregated correlation supporting an affirmative answer to rq2 u1d45f .
with a ci that contains negligible small medium correlations u1d45f .46to u1d45f .
u1d45d .
.
we interpret these results overall as support for the hypothesisthat tool basedveri f iability andhumans abilityto understand code are correlatedto someextent.
the heterogeneity of the considered studies is non negligible u1d43c2 u1d444 u1d451 u1d453 u1d444 .
.
.
not shown in f ig.
indicating that .
of the correlation variation i.e.
variance we observeisduetothestudiesmeasuringdi fferentfactorsratherthan due to chance.
this result validates our choice of a random e ffects modelfor the meta analysis.
the plots in f ig.1show wide con f idence intervals for all the datasets except ds and ds6 which indicates relatively high variability in the correlations.
this indicates that most of these studieswere under powered for our purpose the number of snippets considered was not high enough to give the meta analysis much con f idenceinthecorrelationresults.themeta analysiscorrespondingly givesthelargestweightstothetwodatasetswiththemostsnippets about35 weighttods3 with100snippets andabout25 weight to ds6 with snippets .
future work should explore running understandability experiments with larger numbers of snippets whichwouldenable usto gain further con f idenceinour results.
.
rq3 correlation results by tool toanswer rq3 werepeatedtheanalysesusedtoanswer rq1and rq2independentlyforeachtool i.e.
nowarningaggregation .we alsorepeatedtheanalysisina leave one out ablationexperiment.
wereportonlythesummaryresultsforeachtool i.e.
theresults ofthemeta analyses forspacereasons forestplotssimilarto f ig.
as well as the individual correlation results on each tool metric combination are available inour replication package .
repeating the meta analysis on only the warnings produced by eachtoolindividuallygavesimilarresultstothemeta analysisin f ig.
exceptforinfer.thecheckerframeworkresultssupportsour hypothesismorestronglythantheoverallmeta analysis u1d45f .
ci of u1d45d .
.
the results of openjml and jatyc support the hypothesis more weakly than the overall results u1d45f .
with a ci of u1d45d .16for openjml and u1d45f .17with a ci of u1d45d .14for jatyc .
inferhastoofewwarningstodrawmeaningfulconclusionsfrom its results u1d45f .09witha95 ciof u1d45d .
.
from theseresults we conclude that while sometools support the hypothesis less strongly than the overall meta analysis all the toolsbutinfershowthesametrend.theseresultssupporttheoverallmeta analysisresults rq2 thecorrelationmeasuredfor3of the studied tools suggests that the correlation between veri f iability and understandability indeed exists in small magnitude no tool shows a markedly di fferent trend except infer whose trend is not meaningfuldueto its small warning count.
we were alsoconcerned thatasingle tool might be dominating theoverallresults.tomitigatethisthreat weperformedanablation study by repeating the meta analysis on warning data aggregated from each combination of three tools i.e.
excluding thewarnings ofonetoolonly .overall theresultsareextremelysimilarforeach combination of tools to the overall results the results without infer are in fact nearly identical with u1d45fvalues ranging from .
to .
ci lowerbounds ranging from .46to .
and ci upperbounds ranging from .01to0.
and u1d45dvalues from .
to0.
.we concludefromthisablationexperimentthatnosingle tooldominatesthe rq2results.
taken together the results in this section show that the correlationfoundfor rq2isnotentirelydrivenbyanytool theoverall resultsremainsimilar ifslightlyweaker foreverytoolindividually exceptinferandforeachcombinationofthreetools i.e.
without eachtool .we interprettheseresults to mean thatthecorrelation existsregardlessofthespeci f icveri f ierinuse meaningthatour results apply to veri f ication ingeneral.
218on the relationshipbetweencode verifiability andunderstandability esec fse december3 san francisco ca usa .
rq4 correlation results by metrictype in section .
we observed that thecorrectnessandphysiological metriccategoriesappearedtosupportourhypothesismorestrongly than the rating and time categories.
to test this observation we repeatedour meta analysesfor eachofthe fourmetric categories.
the results refute the idea that these categories are a major in f luenceontheresults.thecorrectness rating andtimemetrics show overall results similar to f ig.
but with wider con f idence intervals u1d45f .28with ci of u1d45d .20for correctness u1d45f .25with ci of u1d45d .11for rating and u1d45f .22with95 ciof u1d45d .23fortime.
the results for the physiological metrics show that they have a minimal impact u1d45f .32but with a huge ci of .
these results especially for the physiological metrics are likely due to the smaller sample sizes created by considering only one metrictype e.g.
therearephysiologicalmetricsinonlytwodatasets ds2 and dsf with total snippets between them.
the dataset with the most weight in the overall results ds3 only has rating metrics whichreducesthemeta analysis con f idenceintheother types.finally theheterogeneityforthethreemetriccategorieswith usefulresults i.e.
notphysiological ishigherthanintheoverall results with u1d43c2of and for correctness rating and time metrics respectively .
.
robustnessexperiments we ran additional experiments to probe the robustness of the f indingsfor the rqsandmitigate somethreatsto validity.
.
.
handling code comments in dataset .
ds9 s original study had3versionsofeachofits10snippets withthreetypesofcode comments good bad andnocomments .theresultspresented elsewhere in this section used the no comments nc version of ds9 because none of the four veri f iers use comments as partoftheirlogic.however thischoicemightbesourceofpossible bias soweanalyzedhowthecorrelationresultswouldchangeif wehadusedthe goodcomments gc or badcomments bc versionsofthedataset.notethatbecausenoneoftheveri f ierstake comments intoaccount theirwarningsare exactlythesame the only differences are inthe comprehensibilitymeasurements.
table4shows how the correlation results di ffer for the three versions of ds9.
a signi f icant difference is observed in the two readability metrics when the comments are bad these metrics are anti correlatedwithveri f iability thatis humansratedthesnippets onwhichthetoolsissuedmorewarningsas morereadable .weseea similarphenomenonforthetimemetrics butitoccursonlyforthe good ratherthanbad comments.toexplainthisphenomenon we comparedthedistributionofthemetricsacrosscommentcategories and analyzed the scatter plots of the data used for correlation.
our analysis revealed that such disparity in correlation stems from a combination of outliers found in the human measurements likelyduetodatacollectionimprecisionsintheoriginalstudy and the low number of data points in ds9.
for example we found that bad comment code was rated more readable by a few participantsthancodewithnocomments eventhoughthesnippets weresemanticallythesame.theseunusualmeasurementsledto outliersthathadaconsiderableimpactonthecorrelationresults across comment categories because ofthe small number ofdatatable4 correlationresults kendall s u1d70f u1d70f u1d70f ondifferentversions of ds9 no nc bad bc and good comments gc .
a indicates statistical signi f icanceat the u1d45d .
u1d45d .
u1d45d .01level.
metric exp.cor.
nc bc gc gap accuracy negative .
.
.
readability level ba negative .
.
.
readability level before negative .
.
.
time to read complete positive .
.
.
table correlation results kendall s u1d70f u1d70f u1d70f for openjml for each timeout handling approach ignoretimeouts under estimate the warnings hidden by timeouts overestimate the warnings hidden by timeouts.
u1d70f u1d70f u1d70f s signi f icance is tested at the u1d45d .
u1d45d .
u1d45d .
and u1d45d .
u1d45d .
u1d45d .01levels .
approach ds metric ignore under over 3readability level .
.
.
6binary understand .
.
.
correct verif .
.
.
time to understand .
.
.
points .thee ffectofthisphenomenonontheoverallresultsislow because ds9 is given very low weight .
lowest among all datasets bythe meta analysisdueto its small sample size.
.
.
handling openjml timeouts.
openjml uses an smt solver underthehood.thoughmodernsmtsolversreturnresultsquickly for most queries using sophisticated heuristics some queries do lead to exponential run time making it necessary to set a timeoutwhenanalyzingacollection ofsnippets.weuseda60minute timeout which led to snippets in ds6 and snippets inds3timingout andzerointheotherdatasets .weconsidered threeapproachesinourcorrelationanalysis tohandletimeouts ignoresnippetscontainingtimeoutsentirely counteachtimeout aszerowarnings butdocountanyotherwarningsissuedinthe snippetbefore timingout or counteachsnippetthat timedout asthemaximumwarningcountinthedataset.alltheresultsfor rq1 rq4 were produced by following approach .the reasonwe choseapproach 3overapproach 2isthat timeoutstypicallyoccur on the most complicated smt queries which might hide many warnings.
therefore approach underestimates the warning count thatano timeoutrunofopenjmlwouldencounter whileapproach 3overestimates thewarningcountinano timeoutrun.were ran thecorrelation analysisunderallthreeconditions.the resultsare in table5and do not show any signi f icant differences between the strategies for timeouts the overall direction and strength of the correlationsaresimilar andtheabsolutesizeofthedi fferencesis small meaningthat the impact onthe meta analysisisnegligible.
.
results discussionandimplications .
.
program semantics and understandability.
veri f ication warningcounts indirectly encode program semantics ratherthansyntacticpropertiesofthe code.veri f icationtoolsaretryingto prove semantic properties checking syntactic properties is decidable so it is not the target of veri f iers which f ind approximate solutions 219esec fse december3 san francisco ca usa kobifeldman martinkellogg andoscar chaparro to undecidable semantic problems e.g.
using smt solvers .
our results suggest that there might be complexity caused by semantics and veri f iers are well suited to reasoning about that kind of complexity.
previous work using decidable syntactic metrics for complexity certainlycouldnotcapture semantics sinceany non trival semantic propertyofa program is undecidable see section .
on one side the measured correlation between veri f iability and understandability increases our con f idence that there is a semantic componenttohumancodeunderstanding.ontheotherside the smallcorrelationwemeasuredindicatesthatthereareotherfactors to code understanding beyond justprogram semantics.
neitherof theseconclusionsareparticularlysurprising butprogramunderstandabilityresearchhassofarmostlyfocusedonthenon semantic components such as variable names or syntactic metrics see section5 .ourworkmotivatestheneedforfuturestudiesthatinvestigate thesemantic component of code understanding in particular the speci f ic semantic factors make code simple or complex and how theyimpactunderstandability.fortunately ourworkalsooffersapathforward theveri f icationcommunityhasalreadybuilt manytoolsthatattempttoverifysemanticproperties i.e.
veri f iers whichgivesusanopportunitytoleveragethoseexistingtoolstoimprove ourunderstandingof codecomplexity and understandability.
.
.
incorporating verifiability into comprehensibility models.
most prior attempts to design automated metrics or models that measure or predict code understandability have used syntactic featuresthatdonotaccountforprogramsemantics .rather theyusedsyntacticfeaturessuchascodebranching vocabulary size and executions paths among other proxies that attempt to capture code complexity see section .
many of these features have shown to be poor predictors of code understandability .webelieveoneofthereasonsforthis is because they do not capture complexity arising from program semantics.
based on the link we found between veri f iability and comprehensibility wehypothesizethatsemanticcodeproperties wouldleadto more accuratemodels ofunderstandability.
future work should validate this hypothesis by incorporating veri f iability into models that predict human based comprehensibility andmeasuringitsimpactonpredictionperformance.
ifthelinkbetweenveri f iabilityandcomprehensibilityexists asour resultssuggest veri f iabilityinformationshouldcomplementthe syntactic features of these models.
veri f iability can be captured by adaptingexistingveri f icationtoolsorbyleveragingtoolwarning data.forexample wecouldprovidethenumberofwarningsatool produces onasnippetas an inputfeature to thesemodels.
.
.
reducingfalsepositives toincrease codecomprehensibility.
developers could use the warning count of veri f iers to know when code might be complex i.e.
when it might need to be refactored toreduce accidental complexity.whilecoding thedevelopercan monitor the warning count of veri f iers on a code snippet e.g.
a method they are writing or updating knowing the code is correct.
if this count increases they could assess potential complex parts of the method and come up with changes to the method that wouldbe semantically equivalent e.g.
replacing recursion which istraditionallyhardforveri f ierstoreasonabout withaloop .this auxiliary bene f it of using veri f ication tools has not been studied intheliterature andmightrepresentanopportunitytomakeveri f iers more appealingto everydaydevelopers.
this usage scenario poses a research opportunity too what if wecouldautomaticallydetermineandsuggesttothedevelopera semantically equivalentrefactoringthatiseasiertoverify?such a refactoring would change the code to perform the same task butwouldcausea veri f iertoissuefewerwarnings.
themeasured correlationbetweenveri f iabilityandunderstandabilitywouldmean more oftenthan not thatapplyingsuch a refactoring might make thecodeeasiertounderstand.sinceitisunclearifsuchrefactoring is possible more research should be conducted.
however if it is possibleandthedeveloperisawareofthecorrelation weanticipate they would be more willing to use veri f iers in their everyday codingtasks and accepttherefactoringsuggestion.onepossible issue with this approach is that our correlation includes warnings causedbymissingspeci f ications thereislargeexistingliterature onspeci f icationinference e.g.
thatcouldbeleveraged to focusonly onfalse positives when speci f ications are explicit.
.
.
codeverifiabilityvs.understandabilityvs.complexity.
our study found a correlation between code understandability and veri f iability yet it did not f ind whether one of the two causes the other i.e.
correlationdoesnotimplycausation .furtherresearch isneededtodeterminewhetheronecausestheother orwhether thereareotherfactorsthatcauseboth.however basedonourresultsanddiscussion wehypothesizethat codecomplexity causes both humans and veri f ication tools to struggle to understand code.
future studiesshould investigate this andotherpossible causes.
limitationsand threats to validity ourstudyshowsa correlation betweenveri f iabilityandunderstandability butdonotshowone causestheother.so ourresultsmustbe interpretedcarefully furtherworkisneededtodeterminecausality.
regardingthreats toexternal validity the correlation we found may not generalize beyond the speci f ic conditions of our study.
the snippets are all java code so the results may not generalize to otherlanguages.weonlyusedafewveri f iers aswewerelimited by parcity of practical tools that can analyze the snippets.
while limitationsorbugsinindividualtoolscouldskewourresults we mitigated this threat by re running the experiments individually for eachtooland withanablationexperiment section .
which demonstratedthatnosingletooldominatestheresults.thesnippets are small compared to full programs the comprehensibility of larger programs may di ffer.
further datasets are snippets from introductory cs courses rather than real life programs but this is mitigatedbythe otherthree datasets of open sourcesnippets.
anotherthreatisthatsubjectsinthepriorstudiesweremostly students.
only ds6 used professional software engineers and only participants the other were students so our results may not applyto more experienced programmers.future work should conduct understandabilitystudieswithprofessional engineers.
beyondthedatasetsandtools therearethreatstointernaland construct validity.
we assumed the snippets are correct as written andthateachveri f ierwarningthereforerepresentseitherafalse positive or a speci f ication that a human would need to write to verify the code.
the presence of a bug would make a snippet seem harder to verify in our analysis because every veri f ier would 220on the relationshipbetweencode verifiability andunderstandability esec fse december3 san francisco ca usa warn about it even if the snippet is easy for humans to understand skewing the results.
we mitigated this threat by manually examining a representative subset of the warnings as described in section2.
we didnot observe any bugsinthe snippets.
related work codecomplexitymetrics.
researchershaveproposedmanymetrics for code complexity though the concept is not easyto de f ine due to di fferentinterpretations .
most metrics rely on simple syntactic properties such as code sizeorbranchingpaths .thesemetricsareusedtodetect complexcodesodeveloperscan simplifyitduringsoftwareevolution .
the motivation is that complex code is harder to understand which may have important repercussions on developer e ffort and software quality e.g.
bugs introduced due to misunderstoodcode .ourcorrelationresultsimplythatcodethat is easier to verify might also be simple and easier to understand by humans we believe the underlying mechanism might be that simple code f its into the expected code patterns of a veri f ication technique.ourresultsalsosuggestthatacomplexitymetricthat aims to capture human understandability should consider not only syntactic information aboutthe code but alsoits semantics.
empiricalvalidationofcomplexitymetrics.
scalabrino et al.
collected code understandability measurements from developersandstudentsonopen sourcecode.theycorrelatedtheirmeasurementswith 121syntacticcomplexitymetrics e.g.
cyclomatic complexity loc etc.
anddeveloper relatedproperties e.g.
code author s experience and background .
they found small correlationsforonlyafewmetrics butamodeltrainedoncombinationsof metrics performed better.
another study found similar results .
researchershaveexploredthelimitationsofclassicalcomplexity metrics .forexample ajami etal.
foundthat differentcodeconstructs e.g.
ifsvs.forloops havedi fferenteffects onhowdeveloperscomprehendcode implyingthatmetricssuchas cyclomatic complexity which weights code constructs equally fail tocaptureunderstandability .recentworkhasproposednew metricssuchascognitivecomplexity cog whichassigns different weights to di fferent code constructs.
mu oz et al.
conductedacorrelationmeta analysisbetweencogandhuman understandability.
they found that time and rating metrics have a modest correlation with cog while correctness and physiological metrics have no correlation.
they did not take into account the unit of analysisproblem intheirmeta analyses.
weextendpriorworkwithempiricalevidenceofthecorrelation betweenveri f iabilityandhumanunderstandability.tothebestof our knowledge we are the f irst to investigate this empirically.
studyingcodeunderstandability.
researchershavestudied code understandability and factors a ffecting it via controlled experimentsanduserstudies .preciselyde f iningunderstandabilityisdi fficult sosomestudies use it interchangeably with readability a di fferent yet related concept .
measurements include the time to read understand or complete code the correctness of output given by the participants perceivedcodecomplexity readabilityorunderstandability and recently physiologicalmeasuresfromfmriscanners biometrics sensors or eye tracking devices .ourstudyutilizesthesehuman basedmeasurementsofunderstandabilityto assesstheircorrelation withveri f iability.
factors that a ffect understandability include code constructs andpatterns identi f ierqualityandstyle comments informationgatheringtasks comprehension tools code reading behavior authorship high levelcomprehensionstrategies programmer experience andtheuseofcomplexitymetrics .our workinvestigatesanew factorimpactingunderstandability code veri f iability.ourresultssuggestthereisacorrelationbetweenthese variables yetfuture studiesare neededto assesscausality.
studies of veri f ication and static analysis tools.
a study conducted to evaluate a code readability model is closely relatedtoours.themodelwasfoundtocorrelatemoderatelywith snippetsonwhichfindbugs issuedwarnings.
unlikethetools inourstudy findbugsis notaveri f icationtool itusesheuristicsto f lagpossibly buggycode .wecorrelatedveri f iabilitywithhuman understandability the earlier study correlated findbugs warnings with an automated readability modeltrained on human judgments.
though veri f ication and static analysis tools are becoming more common in industry studies of their use and the challenges developers face in deploying them suggest that falsepositivesremainaprobleminpractice .ourworkgives anewperspectivetheproblemoffalsepositives.wehaveshown that the presence of false positives from veri f iers correlates with moredifficult to understandcode.wehopethatthisperspective encouragesdeveloperstoviewfalsepositivesasopportunitiesto improve theircode ratherthanas barriersto f inding defects .
conclusionsand futurework ourempiricalstudyonthecorrelationbetweentool basedveri f iabilityandhuman basedmetricsofcodeunderstandingsuggests thereisa connection between whether a tool can verify a code snippet and how easy it is for a human to understand.
though our results are suggestive our meta analysis shows that extant studies onhumancodeunderstandabilitylacksu fficientpowertoenableus todrawastrongerconclusion somorestudiesofunderstandability preferably including many more snippets of code are needed.
further our work has shown only a correlation establishing acausal link between veri f iability and understandability perhaps through amutual cause such as complexity remainsfuture work.
veri f iabilityisapromisingalternativethatcomplementstraditional code complexity metrics and future work could combine measuresof tool basedveri f iabilitywith moderncomplexitymetrics such as cognitive complexity that seem to capture di fferent aspectsofhumanunderstandabilityintoauni f ied automaticmodel.
ourresultsarealsopromisingsupportfortheprospectofincreased adoption of veri f iers our results o ffer a new perspective on the classic problem of false positives since they suggest that false positivesfromveri f iersareopportunitiestoidentifypotentiallymore complex code andmake itmore understandable byhumans.