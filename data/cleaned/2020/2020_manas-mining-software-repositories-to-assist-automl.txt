manas mining software repositories to assist automl giang nguyen gnguyen iastate.edu dept.
of computer science iowa state university ames ia usamd johirul islam mislam iastate.edu dept.
of computer science iowa state university ames ia usa rangeet pan rangeet iastate.edu dept.
of computer science iowa state university ames ia usahridesh rajan hridesh iastate.edu dept.
of computer science iowa state university ames ia usa abstract todaydeeplearningiswidelyusedforbuildingsoftware.asoftware engineering problem with deep learning is that finding an appropriate convolutional neural network cnn model for the task can be a challenge for developers.
recent work on automl morepreciselyneuralarchitecturesearch nas embodiedbytools likeauto keras aimstosolvethisproblembyessentiallyviewing it as a search problem where the starting point is a default cnn model and mutationofthis cnnmodelallows explorationofthe space ofcnn modelsto finda cnnmodel thatwill workbest for theproblem.theseworkshavehadsignificantsuccessinproducing high accuracy cnn models.
there are two problems however.
first nascanbeverycostly oftentakingseveralhourstocomplete.
second cnnmodelsproducedbynascanbeverycomplexthat makes it harder to understand them and costlier to train them.
we propose a novel approach for nas where instead of starting from a default cnn model the initial model is selected from a repos itory of models extracted from github.
the intuition being that developers solving a similar problem may have developed a betterstartingpointcomparedtothedefaultmodel.wealsoanalyze commonlayerpatternsofcnnmodelsinthewildtounderstand changes that the developers make to improve their models.
our approachusescommonlyoccurringchangesasmutationoperators in nas.
we have extended auto keras to implement our approach.
ourevaluationusing8topvotedproblemsfrom kagglefortasks including image classification and image regression shows that giventhesamesearchtime withoutlossofaccuracy manasproduces models with .
to99.
fewer number of parameters thanauto keras models.
benchmarked on gpu manas models train30.
to641.
faster than auto keras models.
ccs concepts software and its engineering search based software engineering computingmethodologies machinelearning .
this work was done when md johirul islam was at iowa state university.
this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
deep learning automl mining software repositories msr acm reference format giang nguyen md johirul islam rangeet pan and hridesh rajan.
.
manas mining software repositories to assist automl.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm newyork ny usa 13pages.
.
introduction an increasingly larger number of software systems today are includingdeeplearning.deeplearningusesaconvolutionalneural network cnn model essentiallyagraphwithweightededgesand nodes that are weighted functions to convert inputs to the output.
as more software systems incorporate deep learning more softwaredevelopershavetodesignandtraincnnmodels.designinga cnn model is very difficult and developers often struggle leading to bugs.
model bugs are frequent bugs in cnn programs .
recentworkonneuralarchitecturesearch nas aimstosolve thisproblem .nastechniquesstartfromacollectionofdefault cnn models and search for a suitable model for the problem.
the searchspaceisdefinedbythecollectionofdefaultmodelsandacollection of mutation operators that are used to modify cnn models to create new candidates.
nas techniques have been implemented inindustrial strengthtoolssuchas auto keras .nastechniques facetwoproblems.first nascanbeverycostly e.g.
auto keras takes hours on high performance machines to search for models with reasonable accuracy .
the accuracy drops rapidly if the search time is reduced.
second cnn models produced by nas canbeverycomplexthatmakesithardertounderstandthemfor maintenance and costlier to train and retrain them.
weintroduce manas miningassistedneuralarchitecturesearch toalleviatethelimitationsofnas.thefundamentalintuitionbehindmanasisthatminingandusingthehand developedmodels that are available in open source repositories as default models or starting point of search can help nas leverage human developer efforts.
manasapplies this intuition in two ways.
first handdevelopedmodelsareminedtosearchforabetterstartingpointfor nas.second thechangepatternsofthehand developedmodels are mined to identify more suitable mutation operators for nas.
wehaverealized manasbyextending auto keras thestate ofthe artnasframework.
auto keras isopensourceandoutperforms state of the art methods like seas nasbot making it a suitablebaseline .somekeytechnicalcontributionsin manas ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa giang nguyen md johirul islam rangeet pan and hridesh rajan a problem b solution figure an example from stackoverflow showing the necessity to change model architecture includemodelmatching atechniqueformatchingtheproblemthat the developer intends to solve with the hand developed models minedfromrepositories modeladaptation atechniqueforadapting theminedmodeltotheproblemcontext modeltransformation a techniqueforadaptingtheminedmodelforfurtherimprovingmetric values and training adaptation that leverages mined parameter values from the repositories to change the optimizer.
to evaluate manas we use the top problems from diverse domainsobtainedfromkaggleformachinelearningtasksincluding imageclassificationandimageregression.ourevaluationshows that given the same amount of searching time manasgenerates simpler neural architectures than auto keras without losing accuracy.
in terms of the models size manas models have .
to .
fewernumbersofparametersthan auto keras models.we observed up to .
faster training s peed when training models produced by manasas compared to those produced by auto keras .
our main contributions are the following we have proposed a novel approach for nas that leverages software repository mining.
wehaveproposedmethodstoidentifythesuitablemodels by analyzing data characteristics and adapting models.
wehaveutilizedthecommonpatternstotransformmined modelstoimprovetheperformanceofthesemodelsinterms of error rate mse model complexity and training time.
wehaveimplementedtheseideasinasotanasframework auto keras .our artifact is available here .
the paper is organized as follows presents a motivating example 3presentspreliminariesandproblemstatement 4describesthe manasapproachfornas 6describesthelimitations and threats to validity of manas describes related work and concludes.
motivation deep learning has received much attention in both academia and industry.
therefore many deep learning libraries and tools are created for supporting a large number of deep learning developers.
althoughtheselibrariesandtoolsmakedeeplearningmoreaccessible therearestillmanychallenges.oneofthechallengingproblems is constructing an appropriate cnn model architecture which also has been shown as a frequent bug in cnn programs by islametal.
.forinstance figure1ashowsaquery posted onstackoverflow whereadeveloperisunabletofindanappropriatecnn model for their purpose.
in particular the question discusses thedifficultythattheresnetarchitecturedoesnotgivetheresult asthedeveloperexpected.inresponse anexpertsuggestschanging the cnn model.
figure 1b shows the solution of the expert for the question of the developer.
the expert suggested that the developer should add dropout layers to minimize overfitting.
neuralarchitecturesearch nas aimstosolvethisproblem .
nas takes the training data as an input to automatically define the neural network for that data.
moreover nas is able to tune the hyperparameter of the searched neural network.
there are both commercialandopen sourcerealizationsofnas.forexample a developer can pay about per hour to use google s automl.
auto keras is an automl system using nas created as an open source alternative.
auto keras returns outstanding results comparedwithstate of the arthandmademodelsoncifar10 mnist andfashion datasets.
auto keras isshownto outperform state of the art methods like seas nasbot .
nashastwolimitations.first itcanbeverycostly.forexample auto keras consumes more gpu computation time compared to using handmade model.
second nas often produces complexmodelsthatarehardtounderstandandtime consumingto train.toillustrate weused auto keras onanotherdataset which isblood cell collected from kaggle.
the model created by auto keras for theblood cell problem has more than .
million learnable parameters and more than weight layers.
the searched cnn models are constructed based on the architecture of the large default cnn models thus the models produced by auto keras are oftenreally large.smaller cnnmodelstrain fasterandsave more energy .hanetal.haveshownthatreducingthenumberof parametersofdeeplearningmodelscanreducethetrainingtimeby to4 andenergycomsumptionby3 to7 .therestofthis workdescribesourapproach manasthataddressestheselimitations.
asanexample for bloodcell dataset manasproducesamodelthat decreases the error rate by .
the model depth by .
the model width by .
and incr eases the training s peedby56.
compared with auto keras s model.
the model produced by manas has layers and .
million parameters neurons whereas the model produced by auto keras has layers .
million parameters.
1369manas mining software repositories to assist automl icse may pittsburgh pa usa ds qyih qyjigq o 0rgho 0lqlqj qyjigig ds qyih rjg ej !
!
giy ds qyih !
giy j d hi rjg ej !
djqzqvig dkj ds qyi rjg ej 1hig h ihj !
giyh !q giyh djqzqvigh dhjg ej ikg y ijq gx qjpkd ikg y gepqjiejkgi i gep !
giy g dj jq gdswdwlrq vwhp kg ykg y qsxw pigjig !
giy oredob3rro5hox odwwhq djqzqvigh 0rgho 7udqvirupdwlrq 6riwpd ikg y gepqjiejkgi i gep ihj in kyj !
giy0g gzig !
giyh ursrxw 0g q q o g dj jq djqzqvig kooihjig in kyj !
giyh0rgho 0dwfklqj !
giy qyjigq o !
giy ykhjigq o !
jepq o dkj p iy !
jepq o dkj qvi !
jepq o kjdkj s 2xwsxw 2xwsxw !ij q gz jq q q kyj giy oo o hulydwlrq !
giyh figure an overview of manas.
two inputs are mined models from repository left top and user s initial file middle top .
preliminaries and problem statement .
preliminaries nas wedefinenaslikeitwasdefinedin auto keras .givenasearch spacesand the input data dsplit into dtrainanddval the goal is to find an optimal neural architecture n s which achieves the lowest value of the cost function in dataset d. search space scovers all the neural architectures created from default neural networks.
n argmin n scost n prime prime dval prime argmin l n prime dtrain wherecostandlare the cost function and loss function is the learned parameter of initial architectures n prime.
.
problem statement this work aims to utilize the neural networks from open source repositories to optimize the neural architecture search.
we extract the data characteristics from both the input dataset and mined neural networks todetermine better starting points initial models fornas.insteadofusingconcretedefaultneuralnetworks thegoal is to find optimal initial architecture n for nas for each different inputdataset.similartoequation2 weobtainlearnedparameter of new initial architectures n argmin l n dtrain the optimal initial architectures n support nas to find out theoptimalneuralnetworkfaster.inotherwords withthesame amountofsearchingtimeandinputdataset newinitialarchitectures help nas to find out a neural network with lower error compared to the original nas argmincost n dval argmincost n prime prime dval manas figure shows the overview of manas.manashas five major components that we describe below.
1to initialize manasfor nas the model database must be populated by mining models from open source repositories.
currently manascollectshigh qualitymodelsfrom github byextractingpythonfilesfrom kerasprojects.theseprojects are selected using certain filtering criteria to ensure code quality.then apiusageisusedtofilterpythonfilestothose that contain models.
finally both the models and the values for optimizer are extracted to store in the model database.
this database is constructed once and should be updated periodically as new models are added frequently.
2the data characteristics extracted from both the input data andminedmodelsareusedtoselectthesuitableinitialmodels for nas from the database.
3model matching matches the data characteristics extracted from an input dataset and the mined models to obtain a good starting point.
it selects the models which have the closestdatacharacteristicswiththeinputdatasetbyusing themodelclusteringapproach.incasetherearetoomany initialmodels themodelfilteringapproachwillbeapplied to reduce the number of models.
4the selected models are transformed by the model transformation approach based on related state of the art papers andcommonlayerapipatternsofminedmodels.thetransformation can enhance the performance of the models in terms of errors and training s peed.
.
model mining inordertoextractcnnmodelsandtheiroptimizersfromsource code repositories we build a source code analyzer based on the control flow graph cfg .
figure shows the overview of model mining process.
1370icse may pittsburgh pa usa giang nguyen md johirul islam rangeet pan and hridesh rajan 7rs uhsrvlwrulhv surjudpv lw xe 4xhu surjudpv 6wdu rxqw wkrq xe hu .hudv nh zrug wkrq odqjxdjh uhdwhg jh0hwd gdwd ri lw xe uhsrvlwrulhv prghov prghov prghov qfrpsohwh prghov 6xssruw e 0dqdv 1rq gxsolfdwh prghov .hudv prghov wudfwlqj prghov figure model mining process.
.
.
meta data collection.
we collect the meta data of github repositories by using a githubapi query1.
meta data contains basic information of a repository such as authors repository s name etc.
the query allows us to obtain the meta data of the githubrepositorieswiththreefilteringcriteriaincludingpython programming language containing keraskeyword and created datebetween2015 01and2020 .fromthemeta data we obtaingithuburls of the top repositories with the most starcounttoensurethequalityofthemodels .theurlshelp us to access the repositories to collect the cnn models.
.
.
keras programs collection.
we obtain python programs from collected repositories however only programs usekerasapi.
in particular we use cfg to analyze their import statements of python programs to only collect which one imports kerasapi.
.
.
models extraction.
in this work since manasonly works withkerascnn we will explain how we extract cnn from keras programs.
we use cfg to extract a model from a deep learning program.wemanuallycreatealistoffunctioncallsusedtobuild neuralnetworksof kerasbasedon keras documentation .then cfg examines every api call to collect the functions contained in thelistandtheirconnections.thecollectedfunctionsrepresentthe layers in the models.
the connections between functions represent the layer connections in the models.
the reason for using cfg istocollectcompletemodelsfromprogramseveniftheycontain branches.
for example the cfg contains a convolutional block followed by a dense block in the if branch and a skip connection inthe else branch.ifaconvolutionalblockcombineswithadense block or a convolutional block combines with a skip connection to be a complete model we will extract those parts in the branches to collectthecompletemodel.cfgisusedtohandlethesituationthat thereisaloopcontainingapartofamodelinthecnnprograms.if thenumberofiterationsisavailabletoextractthatpartcompletely we will collect it to complete the model.
if there is a method call in the program containing a part of the model we will collect it tocompletethemodel.fortheothercases whenthepartsinthe branches cannot complete the model or cannot be extracted we will ignore them.
for instance a part of the model in a loop whose numbers of iterations are unavailable cannot be extracted.
after thisstep wecollected38 808modelsfrom42 kerasprograms.
thenumberofmodelsislessthanthenumberof kerasprograms yyyy mm dd because many programsdo not contain models.
we assume that a programcontainsamodelifithasatleastanapiusedtobuilda model.
rather than mining only neural architectures we also mine their optimizers thatdeeplearning developers carefullyselectafter spendingmanualeffortsonretrainingtheirmodels.then wheneveramodelisselectedasaninitialmodel manastrainsthemodel withitsoptimizerinsteadofthedefaultone.optimizersarealgorithmsdecidinghowtheparametersofthemodelschange.every optimizer has strengths and weaknesses thus it is necessary to chooseasuitableoptimizer.whilemodelsarethedecisivefactor to the performance of manas we note that it is wasteful if we cannotfullyusethesemodels.inotherwords goodmodelswith wrongoptimizerscannotproduceagoodperformance.toobtain the optimizer we use the same process of extracting cnn models bycreatingalistoffunctionsrelatedtotheoptimizer.afterthat cfg analysis is used to obtain the api call which contains the optimizer.
.
.
incomplete models detection.
a complete model includes an input layer hidden layers and an output layer .
the input layer is the first convolutional layer of the neural network which is distinguished from the other convolutional layers based on its parameters.inparticular onlytheapicallrepresentingtheinput layercontains input shape parameter.theoutputlayer isthelast linearlayerof thecnn.hiddenlayersarethe layersbetweenthe inputlayerandtheoutputlayer includingconvolutionallayers activationlayers andfullyconnectedlayers linearlayers .therefore ifanextractedmodeldoesnothaveconvolutionallayers activation layers orlinearlayers wewillconsiderthatthemodelisincomplete.
by removing incomplete models there are models left.
.
.
supported by manas.
asmanascurrently supports a few kindsoflayersthataretheconvolutionallayer thelinearlayer the batch normalization layer the concatenate layer the add layer the maxpoolinglayer thedropoutlayer thesoftmaxlayer therelu layer the flatten layer and the global pooling layer we filter out the models containing unsupported layers.
after this step we have models left.
.
.
model duplication detection.
we obtain 793models after removing the duplicate ones.
we consider if two models have the same abstract neural network ann there will be a duplicate model.
we store the extracted cnn models in a database as an abstract neural network which is an abstract representation of the neural network.
this representation has the structure as a network where the nodes are api calls and the connections are the order between api calls.
we use this representation to adapt models and their optimizers into manas.
from each node we obtain the name of layer and its parameters which can be converted into an api call.
figure presents an example of ann built from an mined model.
notice that if an activation parameter is implemented as an argument inside a layer we consider that the activation is a separate layer.
1371manas mining software repositories to assist automl icse may pittsburgh pa usa 1conv2d kernel size activation relu input shape 3maxpooling2d pool size 4conv2d activation relu 6maxpooling2d pool size 7dropout .
8flatten 9dense activation softmax 12sgd lr .
decay 1e a extracted model1 func conv2d input shape arg2 kernel size func relu func maxpool2d pool size func conv2d arg1 arg2 func relu func maxpool2d pool size func dropout arg1 .
func flatten func linear arg1 arg2 func softmax func sgd lr .
decay 1e b abstract neural network figure building an abstraction of neural network from extracted model .
data characteristics derivation weanalyzestheinputdataandinput outputlayersofthemined modelstoextracttheirdatacharacteristics.takinganimageclassificationasanexample weobtaintheinputdatacharacteristics including the input size and the output channel from the image data.thedatacharacteristicsextractedfromaminedmodelalso aretheinputsizeandtheoutputchannelareobtainedbyanalyzing the first convolutional layer and the last linear layer of a cnn model whicharetheinputlayerandtheoutputlayerthemodel respectively.
example1.
line1isanapicallrepresentingfortheinputlayerof a cnn.
we extract the valueof the argument input shape from the input layer to obtain the input size and the input channel which are and3 respectively.theoutputchannelisobtainedfromthe output layer .
by extracting the first argument s value of the output layer we obtain the value of the output channel.
conv2d input shape activation relu ... dense activation relu since we currently focus on image problems like image classification and image regression we use the input size the input channel and the output channel of image as data characteristics.
first the input size includes the height and the width extracted from theinput data and minedmodels.
second theinput channel representsthenumberofprimarycolorsintheimage.third the output channelis the number ofoutput categories ofthe data and models.
.
model matching modelmatchingisarankingsystemusedtofindgoodmodelsfora certain problem by using the data characteristics.
instead of using constant default models model matching finds the suitable models to uses them as new default models for nas.
.
.
model clustering.
model clustering uses the data characteristics of both the input dataset and mined neural networks to select appropriate initial architectures n for nas.
first of all manas clusters the mined models based on the data characteristics of the models and the input dataset.
secondly in a meta feature data characteristics space our approach identifies closest clusters to theinputdataset.lastly manasusesallthemodelsintheclosest cluster to the dataset as the initial architectures for nas.
formally wedeterminetheinitialarchitectures n fordataset dinequation as follows n ck n ck ck c c g ok sk ik k dist n d braceleftbigg min oif min i min sif min i min o inequation6 cisasetofclustersofneuralnetworkdetectedby clustering algorithm g means which uses a statistical test to automatically decide the number of clusters.
ok ik and skare measured as follows ik i ik ok o ok sk radicalbig w wk h hk wherei o w andharetheinputchannel thenumberofoutput classes the input width and the input height of the dataset respectively.similarly ik ok wk andhkaretheinputchannel the number of output classes the input width and the input height of amodelk respectively.theideabehindtheclusteringequations is that there are two types of the input channel including and .therefore weclassifytheneuralnetworksthathavethesame input channel first.
then we use o s as the input of g means tosplittheminedmodelsintoclusters.afterthatweidentifythe closest model nto the input dataset like equation .
the closest modelisidentifiedbasedonthepriorityof oand sthat otakes precedenceover s.wehavetriedtorunourtoolindifferentorders ofpriority oand s however thisorderofprioritygivesusthe bestresults.the closestclustertothe inputdatasetisthecluster which contains the closest model.
then we select all the models in the closest cluster to the input dataset as the initial models n for nas shown in equation .
.
.
model filtering.
if the number of initial models found by the model clusteringapproachis too large we usemodel filtering to filter some models to increase the performance of manas.
in the searching process manastrains all the default neural networks to selectthebestone.afterthatnasisappliedtotunetheselected model.
thus with a specific time budget the more time manas spends on trying the default models the less time it spends on nasformodelsearching.tofilteringneuralnetworks wedetect the equivalent architectures.
we treat each neural architecture as a graph whose trainable layers like convolutional layers or dense layers represent vertices connections between two trainable layersrepresentedges andchannelsoftheoutgoingtrainablelayers representtheweightsofedgesbetweentwovertices.weusecosine similarity to measure the similarity between two vertices.
mijis anelementofthesimilaritymatrixofvertex viofagraph gaand vertexvjof a graph gb which is measured as follows mij summationtext.1n k 1wikwjk radicalbig summationtext.1m l 1w2 il radicalbig summationtext.1k n 1w2 jl suppose that vkis the common neighbor of vertex viand vertex vj we have wikandwjkare the weights of edges vkviandvkvj 1372icse may pittsburgh pa usa giang nguyen md johirul islam rangeet pan and hridesh rajan respectively.
we also have wilorwjlare the weights of edges that incident with each of the vertices viandvj respectively.
wpq braceleftbigg weight of the edge between vpandvqin a graph g otherwise wecreateasimilaritymatrix mforeachpairofgraphsandclassify equal matrices into the same classes.
in this work we filter the modelsbyonlyusingthe neuralnetworkwhosegraphsbelongto the class having the highest number of matrices.
in other words wewillonlychoosethemostusedarchitectureinthedetermined initialarchitecturefornas.
weconsiderthemost frequentarchitecturetobemoresuitablethanothersbecausemanydeeplearning developersrepeatedlyusetheseneuralarchitectures.non trainable layerslikedropoutoractivationarenotusedasthefactorstodetect the similarities of the neural architectures because we want to utilize the various usagesof these layers to increase the performanceof manas.particularly differentmodelscan havethesame graph structure because of the differences in non trainable layers therefore agraphcanrefertomanydifferentneuralarchitectures.
.
model transformation even though the mined models are from the top star githubrepositories they are not perfect.
therefore we transform the initial architectures by modifying or adding the batch normalization bn layer the flatten layer the activation layers the global average pooling gap layer and the dropout layer to optimize nas in termsofspeed errors andthenumberofparameters.wechoose theselayerstomodifythedefaultmodelsbecauseoftworeasons.
firstly we have found many common patterns related to these layers.
secondly many recent studies have shown the effectiveness of theselayersonincreasingtheperformanceofdeeplearningmodels.
wehavecreatedasetofpre definedrulestotransformnetworks basedonrelatedstate of the artpapersandcommonfunctioncalls patternsfromminedmodels.
manasanalyzesthesearchitectures todeterminewhetherthenetworksatisfiesthepre definedrules.if theserulesaresatisfied thepre definedmodeltransformationswill be applied.
the pre defined model transformations support nas to ignore transformations offered by our approach and focus on the other transformations.
the model transformations do not work forauto keras initial neural architectures because those default models have already included these transformations.
.
.
batch normalization layer constraint.
we add a new batch normalization layer between the convolutional layer and the activation layer to increase training speed .
using bn means thatwemodifytheactivationstonormalizetheinputlayertodecreasethetrainingtime.manywell knownneuralarchitectureslike resnet densenet efficientnet usebntoincreasethe trainingspeed .
bn is also popular in optimizing nas .
example2.
accordingtofigure5a theoriginalmodelhasaconvolutionallayerconnectstoarelulayer whichisanactivationfunction layer.thus infigure5b followingthebatchnormalizationlayerconstraint manas adds a new bn layer between the convolutional layer and the relu layer like the following example.
conv2d ... tanh maxpool2d kernel stride ... conv2d ... tanh maxpool2d kernel stride ... flatten linear in out ... relu linear in out ... softmax a original model1 conv2d ... batchnorm2d ... relu maxpool2d kernel stride ... dropout2d p .
conv2d ... batchnorm2d ... relu maxpool2d kernel stride ... dropout2d p .
globalavgpool2d linear in out ... relu dropout2d p .
linear in out ... softmax b transformed model figure original cnn model vs transformed cnn model conv2d ... relu a original model1 conv2d ... batchnorm2d ... relu b transformed model figure example of batch normalization layer constraint .
.
globalaveragepoolinglayerconstraint.
weusegaptoreshape the data into the correct format for fully connected layers to prevent overfitting .
moreover we use the mined models as defaultmodelsfornas thus theoriginalinputsizeoftheinitial modelsandtheinputsizeofthedatasetcanbedifferent whichcan causeashapemismatchbug.
however using gapcansolvethis problem since it does not care about the input shape.
example3.
accordingtofigure5a theoriginalmodelusedthe flattenlayertopassthe featuremapthroughthecnn.therefore in figure5b followingtheconstraintaboutgaplayer manastransforms flatten into gap like the following example.
1flatten 2linear in out ... a original model1globalavgpool2d 2linear in out ... b transformed model figure example of global average pooling layer constraint .
.
activation layer constraint.
we investigate the patterns of usagesofactivationfunctionsusedintheminedmodel.wehave found3218 hiddenlayersused in793models where reluisused 2946times accountingfor94.
.therefore wereplacethecurrent activation layers of convolutional layers with relu.
example .
according to figure 5a the original model uses tanh for the convolutional layer.
thus following the constraint of the activation layer manas transforms tanh to relu like following example like the following example.
1373manas mining software repositories to assist automl icse may pittsburgh pa usa conv2d ... tanh a original model1 conv2d ... batchnorm2d ... relu b transformed model figure example of activation layer constraint .
.
dropout layer constraint.
we investigate the frequency of dropout layersandtheirdropratesusedintheminedmodel.
out of times that dropout is used in hidden layers the drop rate of .
is used times which accounts for .
.
out of timesthatdropoutisusedinfullyconnectedlayers thedroprate of .
is used times which accounts for .
.
thus we add dropout layers with a drop rate of .
and .
to the hidden layers and fully connected layers respectively.
example .
according to figure 5a the original model does not use the dropout layer.
therefore in figure 5b following dropout layer constraint manasaddsdropoutlayersintoaconvolutionallayerwith .
drop rate and a dropout layer into the fully connected layer with .
drop rate like the following example.
1conv2d ... 2relu 3maxpool2d kernel stride ... a original model1conv2d ... 2relu 3maxpool2d kernel stride ... 4dropout2d p .
b transformed model figure example of dropout layer constraint allthetransformationsexceptdropouttransformationareappliedsimultaneouslytotheminedmodelsbeforesearching.after the best model is selected from candidate models dropout transformation is applied to the best model.
since adding dropout does notalwaysimprovethe model serrors weutilizenastoidentify whether using dropout is good or bad.
evaluation .
experimental setup we implement manasby extending auto keras .
all experiments use python .
with 16gb gpu tesla v100.
in these experiments we use datasets for image classification and image regression whichareobtainedfrom kagglebasedonthevotecount.
the efficiency and effectiveness of manasare evaluated in three aspects.firstly weevaluatethemetricvaluesincludingerrorrateand mse that is lower the better m odel complexity and training speed ofmanasby comparing it with auto keras .
secondly we evaluate the model matching approach s efficiency and effectiveness.
lastly weevaluatetheefficiencyandeffectivenessofmodeltransformation and training adaptation approaches.
in these comparisons manasincludemodelmatching modeltransformation andnas.
thealgorithmnasusedby manasexploresthesearchspacevia morphing the neural architectures guided by the bayesian optimizationalgorithm.miningthemodelsisthemosttime consuming task which took about hours to complete mining all the models.
however we only do this one time so we do not count the time to mine the model in the comparison of manasandauto keras .
.
mined models since many models for different kaggledatasets have already been published on github it is possible that some kagglemodels of the testingdatasethavealreadybeenincludedinourminedmodels.to avoid this problem for each dataset we have examined all github repositoriesofselectedinitialmodelsofeachdataset.thereisnoinformationshowingthatthoseinitialmodelsarespecificallycreated for the input dataset.
moreover for each input dataset we have compared the initialmodel of manaswith all modelsfrom kaggle.
manas initialmodelisnotoneof kaggle smodels.moreover since mostofthemodelsfrom kagglearepublishedasjupyternotebooks we only mine the model written as python files.
weonlycollectmodelsfromthetop10 000repositorieswiththe moststarcounttoensurehigh qualitymodels.wealsodosanity checksbyremovingincompletemodelsandduplicatemodels.moreover we evaluate the mined models by training them on mnist with epochs .
the average accuracy of the mined models on mnist is .
.
.
datasets to evaluate the performance of our method we use 8different imagedatasetscollectedfrom kaggle bloodcell breastcancer flower intelimageclassification iic malaria mnist ham sign language digits sd andsign language sl .
our goal was to utilize more complex datasets compared to well known datasets such as mnist cifar10 and fashion.
most of our datasets have large sizes of images.
for example intheintelimageclassification dataset theimagesize is 150x150 while the image size of mnist is only 28x28.
secondly thenumberofimagesinourdatasetsismuchlargerthanmnist cifar10 andfashion.forinstance thebreastcancerdataset has images.
lastly the number of classification of the evaluateddatasetsisupto10classes.thesedatasetareoftenusedfor image classification task however we treat prediction targets as numericalvaluesforimageregressiontask.forexample wewill treatthepredictiontargetsofthemnistdatasetasintegersranging from0to9asnumericalvaluestobedirectlyusedastheregression targets.
we only use image datasets because auto keras only apply nasforimagedata.wedivideitintotwosubsets ofrandomly selected images are used for training and the remaining images for validation.
.
results .
.
rq1 how efficient is manas?
to evaluate the efficiency of manas we run both manasandauto keras on datasets for image classificationandimageregression.wevarythesearchtimefrom2 hours to hours which are described in figure .
table shows theerrorrate mse thedepth thenumberofparameters andthe training speed of the best models of manasandauto keras .
by the bestmodel wemeanonethathasthelowesterrorrateormseafter search timeout.
we run both manas and auto keras times for each dataset with random training and validation sets.
in the table we report the information of the lowest errors of manas and autokeras .
by comparing manasandauto keras two conclusions can be drawn.
1374icse may pittsburgh pa usa giang nguyen md johirul islam rangeet pan and hridesh rajan table manasclassification regression results dataimage classification image regression error rate depth layers param million speed epoch min msedepth layers param million speed epoch min ak mn akmn akmn ak mn ak mn akmn ak mn ak mn blood cell .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
breast cancer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
flower .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
iic .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
malaria .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnist ham .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
in the table avg ak and mn represent average auto keras andmanas respectively.
in each cell and represent a decrease percentage and an increase percentage respectively.
in each evaluation metric the bold value show the best percentage change of manascompared to auto keras .
firstofall table1showsthat manasproducesmodels which has lower error rate or mse compared with auto keras models.
thelowererrorsof manascomparedwith auto keras indicatesthat using mined models as the starting points for nas can produce bettermodels.notably manasoutperforms auto keras byachieving .
lower error rate on average.
for some problems like iic andmnist ham for image classification task the decrease of the errorrate of manascompared with auto keras is smallbecause the models created by manasandauto keras have reached the limit of error rate.
therefore a small decrease in error rate is a major improvement.
we use the error rate as the main evaluation metric to clearly point out this improvement of manastoauto keras .
secondly manasachieves lower errors with less complicated models compared to auto keras .
using mined models on nas significantly decreases the complexity of the produced models.
on average modelsgeneratedby manasare75.
lessdeepand .
less wide compared to the models generated by auto keras for image classification task.
similarly manascreates models with .
less deepand .
less widecompare to auto keras s modelsfor imageregressiontask.simplerdnnmodelstrainfasterandsave more energy than the complex ones.
han et al.have shown thatreducingthenumberofparametersofdeeplearningmodels canreducethetrainingtimeby3 to4 andenergycomsumption by to .
table also shows that on average manas m od els run faster than auto keras model247.
and66.
in image classification and image regression respectively.
.
.
rq2 howefficientaremodeltransformationandoptimizers modification?
we create an ablation study to observe the efficiency of the model transformation and modifying optimizers via mining separately.
originalmanas om representsminedmodels notransformation no optimizer nas.
transformedmanas tm representsminedmodels transformations no optimizer nas.
manas mn represents for mined models transformations optimizers nas.
auto keras ak represents nas.
weobservetheerrorvaluesof originalmanas transformedmanas manas andauto keras byexecutingthemon8datasetsfor20hours for both image classification and image regression.
to evaluate the efficiency of the model transformation we compare original manasandtransformedmanas .wecompare transformedmanasandmanastoevaluatetheefficiencyofoptimizersmodification.to evaluatethecombinationofallmethods wecompare manasand auto keras .
notice that we have applied the model transformation on autokeras models however this did not lead to any improvement becausethose defaultmodelsuse bnlayersor dropoutlayersappropriately.
figure 10b does not have the original manas series because the best models of original manas andtransformed manas are the same.
in other words the model already contains all the transformationconstraints sothereisnotransformationapplied tothe model.
similarly figure 10edoesnothave the transformed manasseries because the optimizers of the best model of transformedmanas andmanasarethesame.theselected model soptimizer of this problem is not available therefore we use the default optimizer of auto keras for this model.
from the results are shown in figure we draw four observations.
first of all model transformation increases the performance ofmanasin terms of errors and converge time.
figures 10a 10c 10f 10gshowthat transformedmanas achievelowererrorsthan original manas most of the time.
the activation layers the gap layer and the dropout layers contribute to the better errors of transformed manas compared to original manas .
for example the dropout layers can prevent overfitting which decreases the errors.
the model transformation also helps manasto converge faster.
as we can observe figures 10a 10c 10f 10g original manas has higher errors compared to transformed manas in the first 2hours of training which indicate the transformed manas converge faster thanoriginalmanas .transformedmanas hasafastconv ergespeed thanks to bn.
secondly minedoptimizershelp manastoreduceerrorsandconvergefaster.inmostofproblems thebestmodelsof manashave lowererrorsthanthebestmodelsof transformedmanas .moreover manascanconvergeeasierwiththeminedoptimizers.forinstance thefigures10d 10f 10gshowsthat transformedmanas hastrouble converging.
after hours of searching transformed manas cannot find out better models while manassucceeds.
transformed manashas trouble in converging since it does not have an suitable optimizer for those problems like manas.
thirdly auto keras mayhavelowererrorsthan manasinthefirst few hours however in the last hours manasoften finds out better models than auto keras .
for example figures 10b 10c 10d and 10mshowthat auto keras hastheerrorsatthebeginning however 1375manas mining software repositories to assist automl icse may pittsburgh pa usa a blood cell classification b breast cancer classification c flower classification d malaria classification e mnist ham classification f iic classification g sd classification h sl classification i blood cell regression j breast cancer regression k flower regression l malaria regression m mnist ham regression n iic regression o sd regression p sl regression figure error rate and mse of auto keras original manas transformed manas and manasover time as time gone we have not seen any improvement in auto keras results.thereasonfor thisproblemisthat auto keras startswith very complicatedmodels that take alot of time totrain therefore thenumber ofmodelsis searchedby auto keras aresmall which decreasesthechancetofindoutgoodmodelsof auto keras .manas starts with simple models which trains faster.
therefore manas maynotbeofftoagoodstart butintheend itstilloutperforms auto keras .
the observation shows the benefit of using simple mined models in nas.lastly figure10indicatesthat manasobtainslowererrorrates thanauto keras inalmostdifferenttimeperiods.aswecanobserve in figures 10a and 10i manasalways outperforms auto keras in termsoferrorrateinthe20 hoursofsearching.figure10showsthat thelongersearchingtimesometimesgivesworsemodels.during the searching process the nas estimates the errors of searched models and selects the best one.
however the estimation may not beaccurate leadingto anincorrectchoice.thisproblem ofnas indicatestheimportanceofusingasimplemodelasastartingpoint.
simplemodelscantrainfasterthatincreasesthechancefornas 1376icse may pittsburgh pa usa giang nguyen md johirul islam rangeet pan and hridesh rajan table efficiency of model matching datablood cellbreast cancerflower iicmalariahamsdsl taskicimicimicimicimicimicimicimicim total mc555718611465761169455 mf ineachcell ic im mm andmfrepresentimageclassification imageregression modelclustering and model filtering respectively.
the unit of all the data in the table is the number of models.
table manasvsauto keras for well known problems dataerror rate depth layers param million speed epoch min ak mnak mnak mnak mn cifar10 .
.
.
.
.
.
.
.
fashion .
.
.
.
.
.
.
.
.
mnist .
.
.
.
.
.
.
.
.
.
avg4.
.
.
.
.
.
.
.
.
.
.
in each cell avg ak and mn represent average auto keras andmanas respectively.
to search for more models.
since the estimation of nas can be incorrect thus ifweincreasethenumberofsearchedmodel we can increase the chance to obtain better models.
for example after hours of searching nas goes wrong with iicproblem when it produces worse models than before.
however when we keep searching for new models nas gradually fixes the problem to obtains a good model.
.
.
rq3 how efficient is model matching?
the goal of model matching is not only selecting good default models for manasbut also reducing the number of default models.
from table we observe the efficiency of model matching when manascan outperformauto keras inmanydifferentperspectives.table2showsthat model matching including model clustering and model filtering cansignificantlyreducethenumberofdefaultmodelsfor manas.to balancethetimethat manasspendsbetweeninitialarchitectures and nas we applied the models filtering when the number of initial neural networks is larger than since manasoften takes more than hours half of the amount of time used in our evaluations to complete training these neural networks.
we use all mined models as input for each problem which may take few days to complete training.
by using model matching we decrease the number ofdefault models by99 on average.taking sdin image classification as an example after using dnn clustering there are remaining69defaultmodels.therefore weusednnfilteringon this dataset to reduce the number of models of sdfrom models to models which eliminates .
number of the dnn model.
.
.
rq4 how efficient is manas for well known problems?
we alsoevaluate manaswiththewell knowndatasetslikefashion mnist orcifar10fortheimageclassificationtask.table3shows the error rate depth number of parameters ands peedofthebest modelsof manasandauto keras .as canbeobserved auto keras achievebettererrorratesonthesedatasetsthan manas however auto keras produces larger models to achieve these error rates whilemanasuses much smaller models to get close to the error rates ofauto keras in cifar10 and mnist problems.
particularly manas modelsare62.
shorterand89.
than auto keras models onaverage whichincreasesthetraining speedofmanas models by .
compared to auto keras models.
auto keras achieves better error rates than manassinceauto keras uses resnet and densenetas initialmodels.
theseneural networksare well tuned to achieve outstanding results on these datasets which once again shows that using good initial architecture optimizes nas.
limitations and threats to validity .
limitations inthiswork manasdirectlyderivesthedatacharacteristicsfrom the imagedataset whichis limitedto theimage classificationand imageregressionproblems.webelievethat manasisnotdirectlyapplied to other problems such as natural language processing nlp or video classification however our approach of mining models to identify a good starting point candidate should be applicable to any automl problems.
for example data characteristics of nlp problemsalsoincludetheinputshape whichareinputtimesteps and the number of features and the total number of output classes.
timestepsrepresentthemaximumlengthoftheinputsequence which could either be the number of words or the number of characters dependingon what we want.the number of featuresis the numberofdimensionswefeedateachtimestep.wecananalyze theinputlayerandoutputlayertoextractdatacharacteristicsfrom models.
we can analyze the input dataset to obtain its data characteristics.
the extracted data characteristics can be used to find better starting points for nlp problems in automl systems.
manascan only work with few kinds of layers since it only uses thelayersthat auto keras supports.thislimitationcandecreasethe performanceof manasbecauseif auto keras supportedmorelayers we can mine more kinds of models from the software repositories.
.
internal validity wehavetriedourbesttoobtaintheresultsof manasandauto keras onasmanydatasetsaspossible.becauseofthetimelimit manas is currently evaluated on datasets.
all the source code trained models datasets andevaluationdataarepublicforreproduction to mitigate these threats.
.
external validity first of all manasonly focuses on image classification regression problems.
we rely on meta features to find good starting points fornas therefore onepossiblethreatisthatmeta features data characteristics do not work for other types of problems.
however auto sklearn and auto sklearn .
mitigate this threat by showing that using meta features can increase the performance of automl systems in terms of training speed and accuracy on structureddatasets.secondly manasonlyfocusesoncnn.thus another threat is that the model transformation approach does not workforothertypesofmodels.nevertheless cambronero etal.
propose ams showing that using unspecified complementary and functionally related api components can improve the performance of automl systems for classical models such as linear regression or random forest.
the difference between manasand ams is that ams applies these transformations to search space while manas applies these transformations to default models.
1377manas mining software repositories to assist automl icse may pittsburgh pa usa related work .
neural architecture search nas is a technique for automatically finding appropriate neural architectures which can outperform most of the hand designed neuralnetworks.specifically nasneedsathetrainingdatasetas the input to create a powerful neural architecture.
there are many differentapproachesforanassystem however most ofthemhave three main components which are search space search strategy and optimization strategy.
the search space represents the search boundaryofanassystemlimitingwhatkindsofneuralnetwork canbesearchedandoptimized.forinstance baker etal.
usethe convolutional architecture with pooling linear transformations as asearchspace.aroundthesametime zoph etal.
useasimilar searchspace however theauthorsusemoreskipconnectionforthe searchspace.thesearchstrategyisusedtosearchappropriatemodels in a defined search space.
there are many approaches to search models such as reinforcement learning or evolutionaryalgorithms .thisoptimizationstrategysupports nastoguidethenetworksearchprocess.theoptimizationstrategyevaluatesasearchedmodelwithtrainingdatawithouttraining these models.
recently many methods are proposed to optimize nas .
in our work manasmines the neural networks from repositories to enhance the power of nas by supporting it to have a better starting point.
.
automl automlisaprocessforconstructinganappropriatemodelarchitecture for a specific problem automatically.
many features that automlcanprovidetodeeplearningusers suchasautomateddata augmentation automated hyperparameter tuning or automated modelselection.alotofautomlsystemshavebeencreatedlike auto weka ontopofweka auto sklearn on topofscikit learn whichsupportdeeplearningusertoautomate tuninghyperparameterandmodelselection.someotherautoml systemscansupportdeeplearninguserstoautomateoptimizingthe fullmlpipeline.forinstance tpot usesevolutionaryprogrammingtooptimizemlsoftware.
however amaindisadvantage of these systems are very slow because of the high gpu computationrequirement.recently auto keras iscreatedtohandlethis problem which has implemented network morphism t or e duce the searching time of nas.
network morphism is a technique tomorphaneuralarchitecturewithoutchangingitsfunctionality.
nevertheless even though auto keras apply network morphism technique it still takes a lot of gpu computation.
our approach uses dnn model mining and common layer patterns to enhance the performance of automl system.
.
mining software repositories cambronero et al.
proposed al a system that leverages existing machinelearning code from repositoriesto synthesize final pipelines.
alcan generate mlpipelines fora wide rangeof problems without any manual selection.
cambronero et al.
also proposedams whichautomatedgeneratesnewsearchspacefor automl systems by utilizing source code repositories.
the newsearch space is created based on an input ml pipeline which increases the performance of automl systems.
however these only operate classical machine learning models whereas manasworks with neural networks.
.
meta features auto sklearn uses38meta featuresofstructureddatasetsto findabetterstartingpoint.feurer etal.
proposesauto sklearn .
which reduces the number of meta features to three including the number of data points the number of features and the number ofclasses.thereasonsforthereductionarethatgoodmeta features are time consuming and memory consuming to generate.
we also donotknowwhichmeta featuresworkbestforwhichproblem.unlike auto sklearn and auto sklearn .
manasuses meta features to find a better starting point for nas which works for neural networks.moreover manasalsoproposesthemeta features which helps nas find a better starting point for image datasets.
conclusion we present manas a technique for nas which uses the mining technique to assist nas.
the key idea of manasis to mine modelsfromrepositories toenhancenas.inparticular weusecnn modelsminedfromsoftwarerepositoriesasthedefaultmodelof nas.
from a large number of models we use the model matching approachtofindgoodmodelsforaproblem.wealsoapplysome transformationsforthosemodelstoenhancetheirperformances.
withbetterdefaultmodels manascanincreasenas sperformance whichleadstobettercnnmodelsassearchresults.ourexperiment shows that manascan produce better cnn models in terms of the errorrateandmse themodel complexity andthetraining speed thanauto keras .futureworkwillinvolveextending manastoproblemsbeyondthosetackledinthispaper suchasvideoclassification.
we can utilize the code change patterns in ml programs to improve the results.
moreover the proposed approach of manas can be applied to other automated tools e.g.
autoaugment o f other components in ml pipeline .
the technique can also be utilized to useautoml to address theother problems in ml such as fairness bug .