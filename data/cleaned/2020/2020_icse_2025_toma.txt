answering user questions about machine learning models through standardized model cards tajkia rahman toma balreet grewal and cor paul bezemer electrical and computer engineering university of alberta edmonton canada email tajkiatoma balreet bezemer ualberta.ca abstract reusing pre trained machine learning models is becoming very popular due to model hubs such as hugging face hf .
however similar to when reusing software many issues may arise when reusing an ml model.
in many cases users resort to asking questions on discussion forums such as the hf community forum.
in this paper we study how we can reduce the community s workload in answering these questions and increase the likelihood that questions receive a quick answer.
we analyze discussions from the hf model community that contain user questions about ml models.
we focus on the effort spent handling questions the high level topics of discussions and the potential for standardizing responses in model cards based on a model card template.
our findings indicate that there is not much effort involved in responding to user questions however .
of the questions remain open without any response.
a topic analysis shows that discussions are more centered around technical details on model development and troubleshooting indicating that more input from model providers is required.
we show that .
of the questions could have been answered if the model provider followed a standard model card template for the model card.
based on our analysis we recommend that model providers add more development related details on the model s architecture algorithm data preprocessing and training code in existing documentation sub sections and add new sub sections to the template to address common questions about model usage and hardware requirements.
index terms machine learning model hubs model cards questions answers hugging face i. i ntroduction reusing pre trained machine learning models has become very popular in recent years as it reduces the cost and development complexity of training a model from scratch .
one increasingly popular way to access these pretrained models is through model hubs such as hugging face hf .
when users encounter problems with or have questions about the models they can turn to the community on these hubs for help.
for example hf provides a platform for users to communicate with the model community through hf discussions inside model repositories which are similar to issues on github.
unfortunately answering these questions is done by volunteers from the model community and often questions are left unanswered which in turn blocks the questioner from using the model.
in this paper we study what types of questions are being asked and we investigate whether they could have been answered through improved model documentation which would both reduce the questionanswering burden and the waiting time for an answer.many studies have worked on helping answer questions in question answering communities like stack overflow.
they provide suggestions for information seekers and platform designers on how to get a faster answer .
some studies improve platform design such as automatic tag recommendation highlighting content or ranking candidate answers to help users find relevant results.
to the best of our knowledge our study is among the first to use model documentation to answer user questions on machine learning ml models to improve the user experience on ml model hubs.
the purpose of model documentation is to help model users understand and effectively utilize ml models .
we study empirically how to optimize the use of model documentation based on the questions being asked about the models.
in this study we analyze user questions from hf discussions to understand questions related to ml models and analyze how we can effectively address these questions.
we examine discussion metadata to measure community effort in addressing the questions.
through topic modeling we identified the high level topics of discussions that contain questions to understand their underlying nature and investigate the possibility of addressing the questions in model documentation.
finally through manual analysis we explore how standard model documentation such as a model card template can answer questions faster than waiting for a response.
the study addresses the following research questions rq1 how much effort is spent on handling questions?
we analyzed the hf community s effort in responding to questions to understand the time involved.
our analysis indicates that it does not cost the community much effort to respond to questions.
however .
of the questions are still open of which .
are open without any response.
using model documentation to answer questions can help the community focus more on the remaining open questions.
rq2 what are the high level topics of questions containing discussions?
to understand the types of questions asked we clustered high level topics of posts.
we identified clusters encompassing topics related to various aspects of model development model usage and requests for model variants.
the findings suggest that most questions and their underlying topics can beanswered through the model documentation as they touch upon common topics.
rq3 which questions could be answered using a standard model card template?
we align questions asked with sub sections of model cards to assess how effectively a standard model card template can provide information.
we found .
of the analyzed questions could be answered through following a standard model card template showing the template s usefulness in addressing questions.
for questions that cannot be answered with a standard model card template unmapped questions we found that adding sub sections on how to use the model and its hardware requirements can address some of the common unmapped questions.
our study reveals that following a standard model card template can help address many questions.
hence we encourage model providers to adopt a standard template for their model cards.
furthermore our study sheds light on a discrepancy between the questions asked and the information provided in current model cards indicating a need for further enhancements to the template.
we suggest model providers to include more development related details in the model cards and add additional sub sections on model usage and hardware requirements to answer common user questions.
the remainder of the paper is structured as follows.
section ii provides background knowledge to increase the readability of this paper.
section iii discusses our study setup.
sections iv v and vi discuss the results of our study.
section vii discusses the implications of our study.
section viii discusses the threats to the validity of this work and section ix summarizes the related work.
section x concludes our work.
ii.
b ackground a. hf model discussions hf provides a central hub for developers to publish their models along with a wide range of datasets for model training and evaluation .
it also offers tools for building and deploying applications on top of these models.
a key new feature added in of hf is the community tab in each repository allowing community contributions through pull requests and discussions1.
the discussions enable community members to ask and answer questions about a particular model and share ideas and suggestions directly with the repository owner and the community2.
arepository owner can be an individual or an organization that owns a repository to a particular model.
an organization may have one or more team members working on its repositories referred to as organization team members .
together we refer to the repository owner andorganization team members asmodel providers .
further the community of model providers and model users are referred to as the model community .
we refer to the discussions made about a model simply as discussions throughout the paper.
hf also provides uni00000030 uni00000044 uni0000005c uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni0000002d uni00000058 uni00000051 uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni0000002d uni00000058 uni0000004f uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni00000024 uni00000058 uni0000004a uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni00000036 uni00000048 uni00000053 uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni00000032 uni00000046 uni00000057 uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni00000031 uni00000052 uni00000059 uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni00000027 uni00000048 uni00000046 uni00000003 uni00000015 uni00000013 uni00000015 uni00000015 uni0000002d uni00000044 uni00000051 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000029 uni00000048 uni00000045 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000030 uni00000044 uni00000055 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000024 uni00000053 uni00000055 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000030 uni00000044 uni0000005c uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni0000002d uni00000058 uni00000051 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni0000002d uni00000058 uni0000004f uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000024 uni00000058 uni0000004a uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000036 uni00000048 uni00000053 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000032 uni00000046 uni00000057 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000031 uni00000052 uni00000059 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni00000027 uni00000048 uni00000046 uni00000003 uni00000015 uni00000013 uni00000015 uni00000016 uni0000002d uni00000044 uni00000051 uni00000003 uni00000015 uni00000013 uni00000015 uni00000017 uni00000037 uni0000004c uni00000050 uni00000048 uni00000003 uni0000000b uni00000030 uni00000052 uni00000051 uni00000057 uni0000004b uni00000010 uni0000003c uni00000048 uni00000044 uni00000055 uni0000000c uni00000013 uni00000018 uni00000013 uni00000013 uni00000014 uni00000013 uni00000013 uni00000013 uni00000014 uni00000018 uni00000013 uni00000013 uni00000015 uni00000013 uni00000013 uni00000013 uni00000006 uni00000003 uni00000052 uni00000049 uni00000003 uni00000051 uni00000048 uni0000005a uni00000003 uni00000047 uni0000004c uni00000056 uni00000046 uni00000058 uni00000056 uni00000056 uni0000004c uni00000052 uni00000051 uni00000056 uni00000003 uni00000053 uni00000052 uni00000056 uni00000057 uni00000048 uni00000047 uni00000003 uni00000052 uni00000051 uni00000003 uni00000057 uni0000004b uni00000048 uni00000003 uni00000050 uni00000052 uni00000051 uni00000057 uni0000004bfig.
the number of new discussions over time a community discussion forum3where users developers and researchers can engage in discussions on different categories of topics that are not necessarily model specific.
our study is limited to the analysis of discussions which comes from only repositories of models and not the forum.
like the increase in the number of models with a weekly growth rate of .
the number of discussions is also increasing.
figure shows the increase of the number of new discussions in each month from may to january with an average monthly growth of discussions.
b. hf discussion components a discussion in hf is a simpler version of those found in other git hosts like issues on github1.
here we explain and name each part of a hf discussion.
figure is an example of a hf discussion4.
it consists of the following title a brief overview of the topic being discussed.
discussion status indicates whether the discussion is open or closed.
as hf discussions are based on github issues we can derive that every discussion is open unless it is either answered or not relevant cannot be answered5.
post a detailed explanation of the question or issue being addressed or discussed in the discussion.
post author the author of the post who initiates the discussion or asks the question.
post time the date and time when the discussion was posted or last updated.
response the suggestion or thought shared by other community members about the discussion.
response author the author of the response who contributes to the discussion with an answer or insight.
discussions closing an issuefig.
overview of the components of an hf discussion4 response time the date and time when the response was provided to the post.
iii.
s tudy setup our goal is to analyze questions from the hf community about models to help answer questions faster.
first we collected the list of models and their discussions from the hf community and filtered the data to ensure quality.
then we identify the discussions that contain a question using gpt .
turbo for our study.
figure outlines the steps of our data collection and preparation.
we collected our data in january .
in the remainder of this section we explain each step in detail.
a. step collecting data collect list of models we first collect the list of models from hf so that we can access the corresponding discussions.
we collect the list of models utilizing the hf hub api6 which produced a total of models.
collect model community discussions we collected the discussions for each model using the hf hub s community api7.
these discussions also include pull requests.
since we focused solely on analyzing the questions in the discussions we discarded the pull requests in total from our collected data.
out of the models we found discussions from models with the remaining models not having any discussions.
the discussions span from may to january .
b. step filtering data filter models to reduce the number of spam or toy models in our analysis we filtered the models by their number of downloads and likes.
we kept the models that have at least like and download.
the filtering process resulted in a final list of models.
hub en package reference hf api hub en package reference community2 filter discussions to identify the criteria for filtering out discussions we performed a preliminary analysis of a sample discussion.
we studied a randomly selected statistically representative sample of discussions with a confidence level and error margin from discussions discussions .
based on our analysis we listed the following criteria to filter out the data.
hidden discussions some discussions had posts made private resulting in the posts not being publicly visible.
consequently we removed discussions with hidden private posts.
short discussions some discussions were too short or incomplete to comprehend.
after examining them manually we discovered that discussions with less than characters lack meaningful content.
therefore we excluded such discussions.
non english discussions certain discussions were not in english making it difficult for us to grasp the content.
consequently we excluded these non english discussions from our analysis utilizing the xlm roberta baselanguage detection model.
we applied the filters on all discussions to get a refined list for further analysis resulting in discussions.
c. step identifying questions to identify the discussions that contain a question we utilizedgpt .
turbo due to its renowned natural language processing capabilities .
to determine how gpt performs in our case we first performed a sample analysis.
classify sample discussions using gpt we applied the filters from section iii b2 and classified the remaining sample discussions using the prompt specified in figure .
we added the 2ndand4thparagraph in the system prompt to decrease the number of false negatives gpt chooses no our agreed class is yes and false positives gpt chooses yes our agreed class is no respectively.
for each discussion we replaced the title of the discussion from the user prompt with the title of the discussion and content of the post with the content of the post.
we executed thegpt .
turbo classifier three times for each discussion with the prompt with a temperature of and considered the majority class yes or no as the final class.
evaluate performance of gpt as classifier we evaluated gpt s ability to identify if a post contains questions.
two authors manually reviewed the discussions individually and labeled them as containing a question or not to prepare the ground truth.
they achieved a high cohen s kappa coefficient of .
.
the labelling disagreements were resolved through discussion and consensus.
for discussions where disagreements could not be resolved the third author acted as a tiebreaker.
gpt .
turbo achieved a high accuracy of and a high f1 score of demonstrating its robustness and reliability in distinguishing posts with questions.
we used gpt .
turbo to classify all discussions that passed the cleaning filters.
we encountered errors classifying discussions due to their long token length or tostep .
filtering data step .
collec ng data step .
iden fying ques ons hugging face hub collect models collect discussions480 models discussionsselect sample discussions iden fy discussion filtering criteriaclassify ques on containing discussions using gptfilter models filter discussions discussionsiden fy ques on containing discussions ques on containing discussionsprompt template and other se ngs29 models filtering criteria378 discussionssample data analysis prepare ground truthevaluate gpt s performancesample data analysis accuracy f1 scorefig.
overview of our process to collect discussions containing questions system you will be presented with a discussion of a hugging face model.
you will answer the following question with yes or no for the discussion have any questions been asked in the discussion?
think sentence by sentence.
be aware that there can be no ?
mark at the end of the question.
please give your answer in the following format contains question yes no if the answer is yes return me the part of the discussion that contains the question in the following format question part part of the discussion containing the question user title of the discussion content of the post fig.
prompt template used to classify each discussion having many repetitive letters or symbols like or nin the discussions which we excluded from our analysis.
we ended up with a total of discussions that contained questions.
our collected data and analysis are available in our replication package .
iv.
rq1 h ow much effort is spent on handling questions ?
motivation the increasing use of hf models has led to an increasing number of model community discussions.
this research question aims to assess the effort invested in responding to questions to better understand the time commitment invested by hf model community members.
approach we measure the effort spent on handling questions in terms of the number of responses to each question the length of the responses the number of unique participants in each discussion and the time between posting the question and receiving the first response.
of responses per discussion fig.
distribution of number of responses per discussion we used the mann whitney u test to compare the distributions of the number of responses per discussion based on discussion status and responses from model providers.
the mann whitney u test is a non parametric statistical test used to check if two non normal distributions differ.
a pvalue below .
indicates a significant difference between the distributions.
we calculated the effect size of the statistical difference between the distributions using cliff s delta to measure the magnitude of the difference.
we used the thresholds proposed by romano et al.
to interpret the delta value d negligible if d .
small if .
d .
medium if .
d .
and large if .
d .
findings among the discussions containing questions discussions have at least one response.
a discussion with a question gets a median of one response.
figure shows that the number of responses per discussion ranges from to .
with a mann whitney u test we found that the distribution of the number of responses varies noticeably with a small effect size of .
based on the discussion status meaning that open discussions have fewer responses than closed discussions.
from figure we see that open discussions receive a median of one response whereas closed discussions receive a median of two responses.
we notice that open discussions are left unanswered or contain fewer responses compared to closed discussions.
responses to discussions containing questions have a median length of words.
figure shows that the length of responses varies from to words which were mostly made up by the contents of error log that was included in the response .
from figure we see that responses for closed1 of responses per discussionopen closeddiscussion status fig.
comparison of number of responses per discussion based on discussion status response length word count fig.
distribution of the number of words per response discussions are longer than open discussions.
open discussions receive responses that are a median of words long while closed discussions receive responses that are a median of words long.
a discussion with a question has a median of two participants the questioner and another person.
the number of participants in discussions ranges from to as shown in figure .
a mann whitney u test shows that the number of responses varies significantly with a large effect size of .63based on the participation of model providers in discussions.figure shows that discussions with at least one response from a model provider receive a median of two responses while those without typically get none.
additionally figure shows that discussions are much more likely to be closed with a model provider s participation.
the number of responses and status of the discussions with a model provider s participation suggest that the questions require specialized knowledge about the model to answer.
response length word count open closeddiscussion status fig.
comparison of the length of the responses based on discussion status of participants per discussion fig.
distribution of the number of participants per discussion of responses per discussionwith participation without participation fig.
comparison of the number of responses per discussion based on the participation of a model provider percentage of discussionsclosedopendiscussion statuswith participation without participation fig.
comparison of discussion status based on the participation of model providers in a discussion a median discussion containing a question gets a response on the same day.
the response delay ranges from to days.
among the discussions that got a response were responded to within hours making up .
of the total question containing discussions.
however many open discussions did not receive a response at all .
among the discussions that did not get a response are open discussions making up .
of the total questioncontaining discussions of which have been open for more than hours.
questions are typically handled by model providers and responses are typically short.
if a question does not get a response within hours it is unlikely to ever get a response.
.
of the question containing discussions are open of which .
are open without any response.
v. rq2 w hat are the high level topics of questions containing discussions ?
motivation to understand the types of questions users ask about models this research question identifies the highlevel topics of the posts.
we use automated topic analysis to investigate whether it is possible to address the questions in the model documentation from a high level.
knowing prevalent uni00000014 uni00000014 uni00000013 uni00000014 uni00000013 uni00000013 uni00000027 uni00000048 uni0000004f uni00000044 uni0000005c uni00000003 uni0000000b uni0000004c uni00000051 uni00000003 uni00000047 uni00000044 uni0000005c uni00000056 uni0000000c fig.
distribution of response delay per discussiondiscussion topics enables model providers to organize the documentation accordingly.
approach the goal of this research question is to determine whether the questions relate to topics that could be included in the model card.
therefore we categorize all the questions based on their topic.
we use bertopic a topic modelling technique to identify topics in the studied discussions.
bertopic has recently been gaining attention to identify discussion topics .
it provides more meaningful and diverse topics than other popular techniques like latent dirichlet allocation lda latent semantic analysis lsa or top2vec .
we prepared the discussion posts that contained questions for topic analysis by removing unnecessary elements such as code blocks images emojis and urls which do not help bertopic understand the content.
since the bertopic documentation does not recommend any data preprocessing we did not further process the discussions.
bertopic consists of five steps embedding dimensionality reduction clustering tokenization and weighting.
each step is modular and can use different algorithms.
following bertopic s best practices8 we used sentencetransformer for embedding umap for dimensionality reduction hdbscan for clustering countvectorizer for tokenization and c tf idf for topic representation.
we also used gpt .
turbo to assist in labelling the topics with short representative names based on a prompt template from the bertopic documentation.
one author further went through the representative documents of the topics which are automatically selected by bertopic manually to fine tune the labels.
the author used the gpt generated labels as a guide and adjusted the labels based on their own observations of the topic keywords and representative posts.
by setting bertopic to cluster a minimum of posts per topic we identified meaningful topics and one outlier topic.
this threshold of posts per topic was defined empirically through trial and error to produce a meaningful output.
we further grouped similar topics to simplify the analysis and clarify the main areas of interest.
we used hierarchical topic modeling9with bertopic allowing topics with a distance of less than to merge condensing the initial topics into clusters.
the distance among topics and a visualization of how they were grouped into clusters are available in our replication package .
we labelled the clusters similar to how we labelled the topics.
findings we identified clusters containing topics from posts with questions .
table i lists these clusters with their topics along with the number of posts in each topic a short label for each topic and their top keywords.
here we briefly describe each cluster based on its topics representative documents and keywords.
started best practices best practices.html started hierarchicaltopics hierarchicaltopics.htmlcluster challenges with model setup and optimization questions the issues discussed in this cluster cover various aspects of setting up and optimizing models.
users are mainly concerned with the context length and maximum input capacity of the model as well as the tokenizer files like tokenizer.json or tokenizer config.json.
another major focus is questions about different natural language support by models.
the cluster also contains questions about prompt templates and composition of prompts.
finally the discussions highlight performance challenges such as slow inference times and issues with transcribing and processing audio files.
we believe the common questions on topics like maximum context length tokenizer files multilingual support prompt template formatting and performance challenges can be addressed in the model documentation to provide users with a comprehensive guide.
some discussions also focus on the analysis of the model s behaviour with not safe for work nsfw contents.
cluster errors and issues with model loading usage and deployment questions the posts in this cluster focus on issues related to loading using and deploying models across different environments like hf transformers local machines and cloud platforms e.g.
aws sagemaker .
common problems include setup errors missing dependencies memory and vram limitations gpu compatibility issues and api errors during inference.
there are also issues with loading models through web interfaces e.g.
oobabooga .
the cluster shows the technical challenges and troubleshooting needed for model use in various environments and suggests that comprehensive documentation could help resolve some of the issues.
cluster seeking assistance with model usage questions this cluster covers details on requesting and providing guidance on the usage of the models.
some posts provide guides on training and optimizing stable diffusion an ai model for generating images.
there are also requests for assistance on topics like license details for commercial use model download errors and using and citing the model.
additionally users request help converting and exporting the model to onnx format.
the requests for help on model usage scenarios indicate a need for more detailed documentation and support resources for users to effectively utilize the models.
cluster questions about model training evaluation and fine tuning questions the posts in this cluster discuss various challenges and techniques related to the training and fine tuning of the models.
several posts inquire about the datasets used for training and evaluating models including requests for dataset descriptions and evaluation scores.
other documents focus on fine tuning the models to suit unique use cases or incorporate new datasets.
furthermore there are queries concerning the sharing and interpretation of training codes and results particularly the hyperparameters and evaluation methods employed.
finally there are technical questions regarding issues encountered during fine tuning.
the questions in this cluster highlight user interest in more details on the dataset and code used for model training and evaluation as well as fine tuning the model for their own use cases.cluster updates and requests for model versions questions this cluster predominantly discusses users questions about future plans specific model merges and comparisons among similar models.
users are requesting various versions of the models particularly in the gguf format a binary file format for quantized models highlighting the need for specific versions to overcome hardware limitations.
users also ask for a model variant with a different number of model parameters indicating the need for more parameters for a better performing model and fewer parameters for lower computation resources.
there are also questions about the differences between the two models indicating a clear need to better distinguish between models.
we also find discussions on mistral 7b a large language model created by the ai research company mistral indicating a high user interest in the model.
the question containing discussions are centred on different topics of model development model usage and requests for model variants.
regarding model development we find users asking for details about model training and evaluation and assistance for fine tuning.
regarding model usage we find users facing challenges in different phases of model loading setup inference deployment and usage in general.
our findings highlight that many of these topics can be anticipated and should be addressed better in the model documentation.
vi.
rq3 w hich questions could be answered using a standard model card template ?
motivation with the increasing number of questions posted in hf discussions one way to reduce the time model providers spend answering questions and to give answers faster is to integrate common questions answers into the model documentation.
in this research question we identify whether the current industry standard template of a model card can answer the questions asked.
by mapping questions from discussions to the standard model card template we aim to identify how useful it would be for model providers to follow the template and improvements to the template.
approach a model card is part of a model s documentation that serves the purpose of providing detailed information about the models to facilitate user understanding and increase the transparency of the model to its stakeholders .
many recent studies use the model card template of mitchell et al.
as a standard.
therefore we also adopt this template.
the model card template includes sections model details intended use factors metrics evaluation data quantitative results ethical considerations broader impacts and caveats.
some sections contain subsections to describe specific details of the section having a total of subsections for a model card.
we mapped user questions to the most relevant sub sections of the template based on what was asked.
to be more precise to map the questions to model card sub sections we manually analyzed the questions.
with a confidence level and a margin of error we randomly selected a statisticallyrepresentative sample of discussions from the total question containing discussions.
we manually recorded the questions from these posts.
out of discussions contained questions with posts having multiple questions.
thirteen posts did not include any user questions however they were marked as such either because they discussed the model s question answer capabilities or due to gpt s error in detecting questions from the post.
we then mapped each of the questions to the appropriate sub section of the model card template based on where the answer would be found in the standard model card.
initially two authors one phd student with years of professional experience one msc student completed a closed card sort on the first questions based on their experience to establish a better mutual understanding of the model card template.
then the two authors individually mapped the next questions achieving substantial agreement with cohen s kappa coefficient of .
.
after comparing and resolving disagreements through discussion the authors mapped the remaining questions reaching an inter rater agreement of .
indicating a very good agreement.
findings we could map .
of the total questions to a sub section of the model card template.
the questions were mapped to subsections found in of the sections of the model card template.
table ii shows the distribution of questions across the different sections and subsections.
most of the questions .
were related to the model details section showing a strong interest in a model s technical details.
users asked questions related to the training detail of the model to understand the model s training process including the code used training parameters and the quantization process for quantized models.
we also noticed many questions relating to the paper or other resource of the model.
repeated questions were asked about the code or script used to train or fine tune the model or how to use the model.
thus providing training or the model s usage instruction code in the model card can improve the model s reproducibility and usability.
furthermore we identified repeated questions on license in three groups whether the model can be used for commercial purposes details about the current license and if the license allows redistribution of the model in different platforms.
this shows the importance of providing clear and detailed licensing information in the model card.
easily accessible license details can help users decide if the model is suitable to use in specific cases.
additionally users repeatedly asked about the context size or number of tokens to use in the model which we mapped to the model type subsection.
the model type section also includes questions about the model s architecture and specifications showing users interest in its technical details and constraints.
we then have .
of questions the second highest from the intended use section of the template.
while theprimary intended uses subsection typically outlines scenarios for using the model we found that user ques table i list of the topics in clusters generated using bertopic.
the topics of the same cluster are grouped together topic label top keywords of posts cluster challenges with model setup and optimization maximum context length length context tokens context length max maximum input model text tokenizer file tokenizer token json tokens tokenizer model model tokenizer config tokenizer config json special error385 multilingual support language english languages chinese translation model french support multilingual translate prompt design prompt prompt format format template prompt template assistant user instruction prompts use276 multilingual audio transcription audio whisper transcription speech transcribe voice language file output model optimizing inference time electronic music inference speed time td slow faster inference time inference speed prompt for question answering question answer answering question answering context answering model prompt model answers document66 nsfw censorship analysis nsfw uncensored censored scene censorship life meaning rules oh say cluster errors and issues with model loading usage and deployment model loading error layers model layers model transformers self attn error bias py file from pretrained gpu memory allocation issue memory gpu model layers layers self attn g idx g idx model bias model model vram inference api error inference api inference api huggingface endpoint model loading error in webui webui generation webui text generation generation file py py line text line oobabooga windows241 model deployment error sagemaker deploy endpoint aws error tgi inference aws sagemaker instance ml cluster seeking assistance with model usage stable diffusion web ui tutorial diffusion stable stable diffusion image ui web ui web images automatic1111 size model license license commercial apache mit commercial use use license model commercially model license hi310 model download issues download download model model git files error locally run load access how to use model use use model example model app usage help code guide example code cite request paper request citation model paper work like cite onnx model export onnx onnx model model onnx export convert onnx version onnxruntime model js optimum cluster questions about model training evaluation and fine tuning model training dataset dataset data bits used training dataset used int set std binary sentence embeddings for semantic similarity embeddings similarity sentence score embedding model label sentences use word model evaluation and benchmarking evaluation score leaderboard scores benchmarks results accuracy model benchmark humaneval186 fine tune model on dataset fine tuning fine tuning tune fine tune tune model model model fine tuning model dataset training code sharing training code training code share share training script training script train scripts thanks geneformer perturbation error gene cell geneformer perturbation data cells dataset insilicoperturber silico outputdirectory102 lora model fusion lora loras fine lora model lcm training peft model lora fine tuning cluster updates and requests for model versions gguf model version request gguf version ggml gptq quantized quantization model bit llama gguf version different parameter version request 13b version 7b model 3b 70b 33b plans 13b version 30b model merging process merge merging merged models mergekit did model merge models did merge adapter model discrepancy investigation llama llama2 chat teacher llama 7b older 7b old years mother mistral 7b fine tuning mistral mistral 7b 7b solar v0 7b v0 mistral model 7b solar mixtral outlier total tions focused more on technical details such as whether the model output can be adapted for their specific usage scenarios.
therefore alongside describing usage scenarios the model card template should include information on how flexible the model output is for different applications.additionally we encountered questions on out of scope use cases where users wanted to understand the differences between the model and others.
we also noted posts where users sought advice on selecting models based on their requirements.
the third highest portion .
of questions are about the training data section.
users were keen to know about the source and composition of the training data which should be addressed in the datasets subsection.
additionally questions were raised about how the training datawas prepared highlighting the need for detailed information on the dataset and preprocessing methods.
this transparency is crucial for assessing the model s generalizability and reliability based on its training data.
to determine whether users asked questions despite the answers being in the model card we examined the model card version from before the questions were asked.
we found questions were asked even though the answers were in the model card.
some questions can be answered through the model tags and some are on the base model s model card.
this shows the need to make the model cards more navigable and searchable so that users can quickly find the information they need.
from the unmapped questions we found repeated ques table ii our mapping of user questions to model card template sections and subsections sections subsections of questions model detailstraining detail paper or other resource license model type citation details .
intended useprimary intended uses out of scope use cases .
training datadatasets preprocessing motivation .
quantitative analysesunitary results intersectional results .
evaluation datapreprocessing datasets .
metricsvariation approaches model performance measures .
mapped total .
of total questions tions on topics that can be added as sub sections in the model card.
we noted the context of the unmapped questions focusing on the subject of the question and grouped them accordingly.
we found a significant number of questions about how to use the model covering topics from model input to model loading model usage and model output.
questions about model input are mainly focused on prompt or instruction formats.
questions about model loading are mostly requesting instructions on how to load the model.
questions about model usage generally inquire about using the model overall with a ui or through the hf inference api across different operating systems and on a gpu.
this emphasizes the challenges users face at various stages of model use.
a sub section on how to use the model could address these concerns and improve user experience.
furthermore we found questions about the memory or hardware requirements to load deploy or fine tune the model showing user concerns about the computational resources needed for effective use deployment or fine tuning.
offering detailed information on system requirements and hardware configuration recommendations in the model card can help users prepare better for deploying or fine tuning the model in their specific computing environment.
we also found many questions about potential future updates or enhancements to the model.
most inquiries are about a quantized version of the model or the same model with different parameters .
this reflectsuser interest in the ongoing development and improvement of the model.
addressing these questions in a roadmap or future development sub section could manage user expectations.
our analysis also showed that some questions cannot be answered solely through the model card.
for instance questions were about specific errors or bugs encountered while using the model which are often user specific cases.
this supports our proposal of a comprehensive tutorial or documentation on effectively using the model alongside the model card to provide basic guidance to users.
we also encountered questions regarding model fine tuning and questions asking for training instructions to train a similar model with a different dataset.
since this process varies depending on scenarios and datasets providing technical details on the model s training process and basic fine tuning guidance would help users get started.
users ask more technical questions than what the model card currently covers.
for unmapped questions adding sub sections on using the model effectively and its hardware requirements for loading or fine tuning the model can address recurring inquiries.
we also noted questions about errors bugs and fine tuning that may not be fully addressed by the model card alone.
however providing a detailed tutorial or user guide alongside the model card can address these issues and enhance the overall user experience with the model.
vii.
d iscussion the number of discussions in hf is rapidly increasing.
although we do not find much effort involved in responding to questions in these discussions our results indicate the feasibility of answering questions through model documentation.
proactively answering common and straightforward questions in model cards can reduce the number of questions posted and allow the community to focus on more complex userspecific issues.
from the topic analysis of the hf questioncontaining discussions we find that they mainly focus on different topics of model development model usage and requests for model variants many of which could be found in the model documentation.
the analysis also suggests that the topics are primarily centred on the technical details of models which can be best addressed by model providers.
from our manual analysis we found that many questions can be answered following the standard model card template.
however the current model card template gives a general overview of the model.
we suggest adding more details related to development in the subsections related to primary intended uses paper or other resource and training detail of the model.
we also propose adding sub sections to the template to address common questions about hardware requirements and model usage.
in addition to our manual analysis the thematic analysis of the discussions containing questions revealed discussions about gpu memory allocation issues and about how to use the model supporting our proposal.bhat et al.
in their study about model documentation practice found that the model details section is the most filled section in model cards yet we still found many questions belonging to this section in our analysis indicating support of their claim that the information in model documents is often vague.
their study also revealed that model documentation is often not self contained and directs readers to additional resources.
we also found user questions where the answers were in additional resources.
we also found questions from users after getting different errors.
to address the errors and increase users technical understanding of models it may be beneficial to include supplementary documentation such as tutorials or detailed technical guides in addition to the model card.
this will increase the transparency of model usage and help model users understand how to troubleshoot any issues making the model more accessible and usable.
viii.
t hreats to validity internal validity the data filtering choices we made in section iii b could affect the internal validity of our study.
for instance we only included repositories with at least one like and one download to eliminate spam or toy repositories.
our data shows that of the models on hf have likes and have downloads.
by using one like and one download as thresholds we discarded models with discussions in total.
a random check showed that most of these were toy or spam models.
selecting higher thresholds would discard many more discussions proportionally e.g.
likes and downloads would discard approximately more discussions for models .
future research should explore other data filtering methods on the hf model hub to ensure robust results.
construct validity effort is a difficult concept to measure.
there is no single metric to capture effort hence we used four metrics as proxies to capture several aspects related to effort.
however these proxies may not fully capture the true meaning of effort.
future studies could use alternative metrics to better assess effort.
we relied on the status of discussions to determine if questions were resolved.
since there is no notion of answer or accepted answer for hf discussions and the concepts of open and closed are not well defined we derive the definitions from github issues and modified them to fit the context on hf.
future studies should consider alternative ways to determine if a question has been answered like analyzing the discussion responses.
for topic modelling bertopic can yield different results depending on the data preprocessing step and training parameters.
furthermore the cluster of topics can vary depending on bertopic s clustering parameters.
therefore we experimented with various discussion preprocessing steps training and clustering parameters.
we selected the parameters that produced the best results while minimizing interference.
in addition we labelled topics and clusters using our knowledge of model development disciplines and gpt assistance.
althoughthe labels might not be perfect we supported our choices with a manual review of representative posts to ensure they roughly describe the topics and clusters accurately.
conclusion validity we manually mapped the user questions to relevant model card sections based on our understanding.
two authors mapped the questions independently to reduce bias.
future research should explore more objective methods for mapping user questions to model card sections.
external validity we considered one model card template for our analysis which might introduce bias since other templates could offer different insights.
future research should consider using a range of templates to strengthen and generalize the findings.
additionally we focus solely on discussions in model repositories on hf.
there might be more discussions about these hf models on other platforms like github and discord which we did not consider which could affect the external validity of our study.
moreover our findings may not generalize to questions asked about models found on other model hubs.
ix.
r elated work a. question and answering q a communities numerous studies have analyzed questions in various software engineering domains such as web development mobile development game development software security and testing and many more .
roy et al.
recently conducted a systematic review of articles on community questionanswering sites cqas that use traditional machine learning and deep learning methods to explore the growing volume of cqa content.
they identified key research themes in the literature including question quality answer quality and expert identification with popular platforms being yahoo!
answers stack exchange and stack overflow so .
closest to our work yang et al.
studied github issues of open source ai repositories to investigate the problems during the process of employing ai systems to assist developers.
they identified categories of problems that developers are likely to encounter in open source ai repositories.
our study is the first to analyze user questions on ml models and their usage from hf model community discussions focusing on the effort involved in responding to questions and the topics of those questions.
supporting and improving answering several studies have focused on supporting q a to improve user experience.
wang et al.
made a set of suggestions to information seekers in the msdn visual c general forum on how to make their questions be answered faster.
wang et al.
suggested q a website designers improve their incentive systems to motivate non frequent answerers to be more active and answer questions faster.
wang et al.
proposed an automatic tag recommendation system to improve the organization of questions that helps answerers find questions on their topic of interest.
nadi and treude developed and compared four potential approaches to identify key sentences from so answers to help users decide whether an answer is relevant totheir task and context.
luong et al.
developed arseek a context specific algorithm to capture relevant information from discussions allowing developers to access useful api resources quickly.
abbasiantaeb et al.
used transformerbased language models to rank candidate answers for each question.
ahmed et al.
used cnn based and bertbased models to recommend highlighted content with different formatting styles in so.
lill et al.
proposed an approach to automatically identify repeated questions and suggest answers from previous discussions on so and discord.
in addition there are many recent studies that focus on domain specific automated question answering .
in our study we propose the use of model cards to address common user questions to improve question answering.
b. studies on the hf community there are a few recent studies that have examined the metadata of repositories in hf and its community.
osborrne et al.
analyzed various aspects of development activities in hf like interactions in model dataset and space repositories collaboration in model repositories and model adoption in spaces to understand collaborative practices in the open ai ecosystem.
casta no et al.
examined the status and evolution of the hf community by analyzing changes in popularity framework usage tag and dataset trends and key author groups.
closest to our work taraghi et al.
conducted an empirical study on the hf discussion forum and identified categories of challenges while our study focuses on understanding questions asked related to models in the hf model repositories discussions.
taraghi et al.
s finding of the most prominent category of challenges model usage understanding and training pipeline supports our proposal to add more development related content on training and a section on how to use the model in the model card.
c. model documentation analysis and improvement current studies examine the content of existing model documentation to identify gaps and scope of improvements in model documentation .
bhat et al.
and liang et al.
showed that model cards do not often contain enough information about different sections of the proposed standard template.
pepe et al.
highlighted the need for better documentation of training datasets biases and licenses in pre trained models to improve transparency and mitigate potential biases and legal issues.
oreamuno et al.
found that many models and datasets in the hf store lack comprehensive documentation either failing to meet user needs or lacking enforcement.
the study demonstrated inconsistencies in ethics and transparency related documentation for ml models and datasets indicating the need for improved practices to address ethical concerns biases and limitations.
additionally they suggested adding categories for model versioning and attribution in documentation standards.
crisan et al.
introduced a new design for model documentation the interactive model card imc to make it moreunderstandable and actionable for a wider range of stakeholders by incorporating interactive elements and human centered design principles into the model documentation process.
the authors provided guidelines for imc based on the feedback from both ai experts and non expert analysts which enhance the transparency and interpretability of model details for a diverse range of stakeholders.
tsay et al.
identified challenges in documenting ai models including the reliance on manual processes and lack of standardized practices which lead to inconsistencies and information gaps.
as a solution they extracted metadata relevant to model documentation from software repositories and created a searchable catalog using this metadata to improve model documentation.
x. c onclusion in this paper we analyze discussions about ml models on hf.
we find that while answering questions does not require much effort questions that do not receive a response in hours are unlikely to be answered at all.
also .
of the question containing discussions are open of which .
are open without a response.
a topic analysis of the question containing posts shows that most discussions are related to technical details of model development and model usage and requests for model variants.
through a manual analysis of the questions we found that .
of the questions could be answered following a standard model card template and adding sub sections on model usage and hardware requirements would cover even more frequently asked questions.
with our suggestion for model card enhancements we believe support efforts for model developers can be simplified.
the suggested improved model documentation can optimize the question answer process and decrease wait time for an answer.