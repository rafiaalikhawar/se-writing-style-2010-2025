fairway a way to build fair ml software joymallya chakraborty jchakra ncsu.edu north carolina state university raleigh usasuvodeep majumder smajumd3 ncsu.edu north carolina state university raleigh usa zhe yu zyu9 ncsu.edu north carolina state university raleigh usatim menzies timm ieee.org north carolina state university raleigh usa abstract machine learning software is increasingly being used to make decisions that affect people s lives.
but sometimes the core part of this software the learned model behaves in a biased manner that gives undue advantages to a specific group of people where those groups are determined by sex race etc.
.
this algorithmic discrimination in the ai software systems has become a matter of serious concern in the machine learning and software engineering community.
there have been works done to find algorithmic bias or ethical bias in software system.
once the bias is detected in the ai software system mitigation of bias is extremely important.
in this work we a explain how ground truth bias in training data affects machine learning model fairness and how to find that bias in ai software b propose a method fairway which combines preprocessing and in processing approach to remove ethical bias from training data and trained model.
our results show that we can find bias and mitigate bias in a learned model without much damaging the predictive performance of that model.
we propose that testing for bias and bias mitigation should be a routine part of the machine learning software development life cycle.
fairway offers much support for these two purposes.
ccs concepts software and its engineering software creation and management computing methodologies machine learning .
keywords software fairness fairness metrics bias mitigation acm reference format joymallya chakraborty suvodeep majumder zhe yu and tim menzies.
.
fairway a way to build fair ml software.
in proceedings of the 28th acm joint european software engineering conference and symposium permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
https introduction software plays an important role in many high stake applications like finance hiring admissions criminal justice.
for example software generates models that decide whether a patient gets released from hospital or not .
also software helps us to choose what products to buy which loan applications are approved which citizens get bail or sentenced to jail .
further self driving cars are run by software which may lead to damage of property or human injury .
these all are examples of software systems where the core part is machine learning model.
one problem with any machine learning ml model is they are all a form of statistical discrimination.
consider for example the discriminatory nature of decision tree learners that deliberately selects attributes to divide that data into different groups.
such discrimination becomes unacceptable and unethical when it gives certain privileged groups advantages while disadvantaging other unprivileged groups e.g.
groups divided by age gender skin color etc .
in such situations discrimination or bias is not only objectionable but illegal.
much recent se researchers presume that the construction of fairer less biased ai systems is a research problem for software engineers .
we assert that modern principles for software engineering should encompass principles for building ai ml software.
this paper mainly focuses on improving ai software to satisfy an important and specific non functional requirement fairness .
in the age of agile software development requirements gathering architectural design implementation testing verification in any step bias may get injected into software system.
so test and mitigation is now a primary concern in any se task that uses ai.
many researchers agree that fairness is a se problem worthy of se research.
for example entire conference series are now dedicated to this topic see the fairware series1 the acm fat conference fat fat is short for fairness accountability and transparency and the ieee ase explain workshop series.
nevertheless when discussing this work with colleagues we are still sometimes asked if this problem canorshould be addressed by software engineers.
we reply that oct 2020esec fse november virtual event usa joymallya chakraborty suvodeep majumder zhe yu and tim menzies se researchers canaddress bias mitigation.
as shown below technology developed within the se community can be applied to reduce ml bias.
as to whether or not this community should explore ml bias mitigation that is no longer up to us.
when users discover problems with software it is the job of the person maintaining that software i.e.
a software engineer to fix that problem.
for all these reasons this paper explores ml bias mitigation.
in the recent software engineering literature we have found some works to identify bias in machine learning software systems .
but there is no prior work done to explain the reason behind the bias and also removing the bias from the software.
we see some recent works from ml community to mitigate ml model bias.
all of these works trust the ground truth or the original labels of the training data.
but any human being or algorithm can make biased decisions and introduce biased labels.
for example white male employees were given higher priority to be selected for company leadership by human evaluators compas recidivism algorithm was found biased against black people .
if these kind of biased data is used for machine learning model training then trusting the ground truth could introduce unfair decisions in future.
so training data validation testing model for bias and bias mitigation are equally important.
this paper covers all the concerns.
the idea of fairway comes from two research directions chen et al.
mentioned that a model acquires bias from training data .
they bolstered on data collection process and training data sampling.
their work motivated us to find bias in the training data rather than model.
berk et al.
have stated that achieving fairness has a cost .
most of the bias mitigation algorithms damage the performance of the prediction model while making it fair.
this is called accuracy fairness trade off .
when trading off competing goals it is useful to apply multiobjective optimization.
while so one objective is to reduce bias or achieve fairness and another objective is to keep the performance of the model similar.
drawing inspiration from both these works we propose a new algorithm fairway which is a combination of pre processing and in processing methods.
following the motivation of chen et al we evaluate the original labels of the training data and identify biased data points which can eventually make the machine learning model biased.
then following the idea of berk et al we apply multiobjective optimization approach to keep the model performance same while making it fair.
the combination of these two approaches makes fairway a handy tool for bias detection and mitigation.
overall this paper makes the following contributions we explain how a machine learning model acquires bias from training data.
we find out the specific data points in training data which cause the bias.
thus this work includes finding bias in ai software.
we are first to combine two bias mitigation approaches preprocessing before model training and in processing while model training .
this combined method fairway performs better than each individual.
our results show that we can achieve fairness without much damaging the performance of the model.
we comment on the shortcomings of broadly used fairness metrics and how to overcome that.
we describe how concept of ethical bias depends on various applications and how we can use different fairness definitions in different domains.
our fairway replication package is publicly available on github2 and figshare .
this last point is not so much a research contribution but a systems contribution since it enables other researchers to repeat confirm and perhaps even refute improve our results.
the rest of this paper is structured as follows section provides an overview of software fairness and generates the motivation of this work.
two subsections summarize the previous works.
section explains some fairness terminology and metrics.
section describes the five datasets used in our experiment.
section describes our methodology to make fairer software.
section shows the results for six research questions.
in section we have stated the threats to validity of our work.
finally section concludes the paper.
background .
about software fairness there are many instances of a machine learning software being biased and generating arguably unfair decisions.
google s sentiment analyzer model is used to determine positive or negative sentiment.
it gives negative score to some sentences like i am a jew and i am homosexual .
google s photo tagging software mis categorizes dark skinned people as animals .
translation engines inject social biases like she is an engineer he is a nurse translates into turkish and back into english becomes he is an engineer she is a nurse .
a study was done on youtube s automaticallygenerated captions across two genders.
it is found that youtube is more accurate when automatically generating captions for videos with male than female voices .
a popular facial recognition software shows error rate of .
for dark skinned women and .
for light skinned men .
recidivism assessment models that are used by the criminal justice system have been found to be more likely to falsely label black defendants as future criminals at almost twice the rate as white defendants .
amazon scraped automated recruiting tool that showed bias against women .
in brun et al.
first commented that it is now time that software engineers should take these kinds of discrimination as a major concern and put effort to develop fair software .
a software is called fair if it does not provide any undue advantage to any specific group based on race sex or any individual.
this paper represents a method fairway which specifically tries to detect and mitigate ethical bias in a binary classification model used in many ai software.
.
previous work bias in machine learning models is a well known topic in ml community.
recently se community is also showing interest in this area.
large se industries have started putting more and more importance a way to build fair ml software esec fse november virtual event usa on ethical issues of ml model and software.
ieee the european union and microsoft recently published the ethical principles of ai.
in all three of them it is stated that an intelligent system or machine learning software must be fair when it is used in real life applications.
ibm has launched a software toolkit called ai fairness which is an extensible open source library containing techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the ai application lifecycle.
microsoft has created a research group called fate which stands for fairness accountability transparency and ethics in ai.
facebook announced they developed a tool called fairness flow that can determine whether a ml algorithm is biased or not.
ase has organized first international workshop on explainable software where issues of ethical ai were extensively discussed.
german et al.
have studied different notions of fairness in the context of code reviews .
in summary the importance of fairness in software is rising rapidly.
so far the researchers have concentrated on two specific aspects testing ai software model to find ethical bias making the model prediction fair by removing bias .
finding ethical bias angell et al.
commented that software fairness is part of software quality.
an unfair software is considered as poor quality software.
tramer and other researchers proposed several ways to measure discrimination .
galhotra et al.
created themis a testingbased tool for measuring how much a software discriminates focusing on causality in discriminatory behavior.
themis selects random values from the domain for all the attributes to determine if the system discriminates amongst the individuals.
udeshi et al.
have developed aequitas tool that automatically discovers discriminatory inputs which highlight fairness violation.
it generates test cases in two phases.
the first phase is to generate test cases by performing random sampling on the input space.
the second phase starts by taking every discriminatory input generated in the first phase as input and perturbing it to generate furthermore test cases.
both techniques themis and aequitas aim to generate more discriminatory inputs.
the researchers from ibm research ai india have proposed a new testing method for black box models .
they combined dynamic symbolic execution and local explanation to generate test cases for non interpretable models.
these all are test case generation algorithms that try to find bias in a trained model.
we did not use these methods because along with the model we also wanted to find bias in the training data.
we developed our own testing method based on the concept of situation testing .
.
removing ethical bias the prior works in this domain can be classified into three groups depending on the approach applied to remove ethical bias.
pre processing algorithms in this approach before classification data is pre processed in such a way that discrimination or bias is reduced.
kamiran et al.
proposed reweighing method that generates weights for the training examples in each group label combination differently to achieve fairness.
calmon et al.
proposed an optimized pre processing method which learnsa probabilistic transformation that edits the labels and features with individual distortion and group fairness.
in processing algorithms this is an optimization approach where the dataset is divided into three sets train validation and test set.
after learning from training data the model is optimized on the validation set and finally applied on the test set.
zhang et al.
proposed adversarial debiasing method which learns a classifier to increase accuracy and simultaneously reduce an adversary s ability to determine the protected attribute from the predictions.
this leads to generation of fair classifier because the predictions cannot carry any group discrimination information that the adversary can exploit.
kamishima et al.
developed prejudice remover technique which adds a discrimination aware regularization term to the learning objective of the classifier.
post processing algorithms this approach is to change the class labels to reduce discrimination after classification.
kamiran et al.
proposed reject option classification approach which gives favorable outcomes to unprivileged groups and unfavorable outcomes to privileged groups within a confidence band around the decision boundary with the highest uncertainty.
equalized odds post processing is a technique which particularly concentrate on the equal opportunity difference eod metric.
two most cited works in this domain are done by pleiss et al.
and hardt et al .
fairway combines both pre processing andin processing approach.
further post processing is not needed after using fairway.
changing a misclassified label requires domain knowledge based on the type of application.
that kind of knowledge can be difficult to collect since it requires access to subject matter experts .
hence post processing is not explored in this paper.
fairness terminology in this section some specified terminology from the field of fairness in machine learning are described.
this paper is limited to the binary classification models and tabular data row column format .
each dataset used has some attribute columns and a class label column.
a class label is called favorable label if its value corresponds to an outcome that gives an advantage to the receiver.
examples include being hired for a job receiving a loan.
protected attribute is an attribute that divides a population into two groups privileged unprivileged that have difference in terms of benefits received.
an example of such attribute could be sex or race .
these attributes are not universal but are specific to the application.
group fairness is the goal that based on the protected attribute privileged and unprivileged groups will be treated similarly.
individual fairness is the goal of similar individuals will receive similar outcomes.
fairness measures martin argues and we agree that bias is a systematic error .
our main concern is unwanted bias that puts privileged groups at a systematic advantage and unprivileged groups at a systematic disadvantage.
a fairness metric is a quantification of unwanted bias in models or training data .
we used two such fairness metrics in our experiment equal opportunity difference eod difference of true positive rates tpr for unprivileged and privileged groups .esec fse november virtual event usa joymallya chakraborty suvodeep majumder zhe yu and tim menzies table combined confusion matrix for privileged p and unprivileged u groups.
predicted nopredicted yespredicted nopredicted yes privileged unprivileged actual no tnpfpptnufpu actual yes fnptppfnutpu sex race012345678ratio of negative positive class figure ratio of negative and positive class for two protected attributes sex and race for adult dataset.
orange column is for privileged group male white and blue column is for unprivileged group female non white .
average odds difference aod average of difference in false positive rates fpr and true positive rates tpr for unprivileged and privileged groups .
tpr tp p tp tp fn fpr fp n fp fp tn eod tpr u tpr p aod .
eod and aod are computed using the input and output datasets to a classifier.
a value of implies that both groups have equal benefit a value lesser than implies higher benefit for the privileged group and a value greater than implies higher benefit for the unprivileged group.
in this study absolute value of these metrics have been considered.
depending upon the notion of fairness there are various fairness metrics also.
the statistical notion of fairness in binary classification mainly comes from the confusion matrix a table that is often used to describe the accuracy of a classification model.
if there are two confusion matrices for two groups privileged and unprivileged see table all the fairness metrics try to find the difference of true positive rate and false positive rate for those two groups from those two matrices .
beutel et al.
commented that all of these fairness metrics suffer from three shortcomings these metrics ignore the class distribution for privileged and unprivileged groups.
as a case study figure shows the ratio of negative low income and positive high income class for two protected attributes sex and race for adult dataset.
orange column is for privileged group sex male race white and blue column is for unprivileged group sex female race nonwhite .
the figure shows the uneven distribution of positive and negative classes for unprivileged and privileged groups.
these metrics do not consider the sampling of the data.
but incorrect sampling creates data imbalance which may lead to incorrect measurement of bias.
these metrics ignore the cost of misclassification.
for example in case of credit card approval software assigning bad credit score to an applicant who has actual good credit score is less costlier than assigning good credit score to an applicant who has actual bad credit score.
in this work several steps are taken to overcome those shortcomings.
most of the prior works have either used aod or eod we have used both of them for our study as we compared our approach with previous works .
instead of depending on only those two metrics the concept of situation testing was used to find discrimination .
in the context of binary classification situation testing is the process of verifying whether model prediction changes for same data point with changed protected attribute value .
while measuring the performance of fairway we used random sampling of data for ten times to overcome the sampling problem.
cost of misclassification is not solved because that is application specific and requires domain knowledge.
dataset description in this experiment five datasets from uc irvine machine learning repository have been used.
all the datasets are quite popular in fairness domain and used by previous se researchers .
a brief description of the datasets are given adult census income this dataset contains records of people.
the class label is yearly income .
it is a binary classification dataset where the prediction task is to determine whether a person makes over 50k a year.
there are fourteen attributes among them two are protected attributes.
compas this is a dataset containing criminal history demographics jail and prison time and compas which stands for correctional offender management profiling for alternative sanctions risk scores for defendants from broward county .
the dataset contains rows and twenty eight attributes.
among them there are two protected attributes.
german credit data this dataset contains records of people and binary class labels good credit or bad credit .
there are twenty attributes among them one is protected.
default credit there are records of default payments of people from taiwan .
binary class label is default payment yes or no .
there are twenty three attributes among them one is protected.
heart health the heart dataset from the uci ml repository contains fourteen features from adults .
the goal is to accurately predict whether or not an individual has a heart condition.fairway a way to build fair ml software esec fse november virtual event usa table description of the datasets used for the experiment.
dataset rows features protected attribute label privileged unprivileged favorable unfavorable adult census income48 14sex male race whitesex female race non whitehigh income low income compas 28sex female race caucasiansex male race not caucasiandid not reoffend reoffended german credit data1 sex male sex female good credit bad credit default credit sex male sex female default payment yes default payment no heart health age young age old not disease disease table gives an overall description of all five datasets.
these are binary classification datasets.
like most of the prior research we used logistic regression model on these datasets .
but our approach is applicable for any classification model.
the fairway method as stated above the fairway algorithm is a combination of the preprocessing and in processing approach to make machine learning software fairer.
.
why not remove the protected attributes?
this section describes one of the methods we explored before arriving at fairway.
when we think of prediction model discriminating over a protected attribute the first solution which comes to mind is that why not train the model without that protected attribute.
being novice in fairness domain we tried that for the five datasets.
two of the datasets have two protected attributes adult compas sex race and other three datasets have only one protected attribute.
we removed the protected attribute column from the train and test data so that the model has no information about that attribute.
surprisingly there was almost no change in bias metrics even after that.
brun et al.
have mentioned one reason behind this surprising result.
they mentioned that if there is high correlation between attributes of the dataset then even after removing the protected attribute the bias stays .
in amazon created a model for same day delivery service offered to prime users around the major us cities .
but the model turned out to be highly discriminatory against black neighborhood.
while training this model race attribute was not used but the model became biased against a certain race because the zipcode attribute highly correlates with race .
the training data had zipcode and the model induced race from that.
initially we also thought maybe correlation is the reason for our datasets also.
but when we checked for the correlation between attributes we found that bias is not coming from the correlation.
for the datasets we are using here the bias mainly comes from the class label.
the data have been historically captured over the years.
the classification was done by several human beings or algorithms whether credit card gets approved or a person having a disease.
human bias or algorithmic bias against certain sex or racereflected on predictions.
in some cases people of specific race or sex were unfairly treated.
thus the historical records have improper labels for some portion of data.
this is to say that even if we remove the protected attribute column bias still remains.
for removal of bias we need to find out those data points having improper labels.
finally we can summarize different ways of a model acquiring bias from training data if in the training data the class labels are related to any of the protected attributes while training a model can acquire that bias.
if there is no protected attribute but other correlated attributes which affect the decision then also model may become biased.
kamishima et al.
reported a reason for unfairness called underestimation .
it happens when a trained model is not fully converged due to the finiteness of the size of the training data set.
they defined a new metric called the underestimation index uei based on the hellinger distance to find underestimation .
according to them this occurs very rarely.
so we did not try to find uei for our datasets.
bias may come from unfair sampling of training data or unfair labeling of the training data.
for the five datasets used in this study the main reason of bias is unfair labeling of some data points.
in this work data has been randomly sampled ten times to make sure bias does not come from improper sampling.
.
removal of ambiguous biased data points depending upon the protected attribute there is a privileged group and an unprivileged group in each dataset.
which group is privileged and which group is unprivileged depend on the application.
for example in credit card applications male might be considered privileged and female as unprivileged in criminal prediction white people might be considered privileged and non white as unprivileged.
in this step we try to find and remove the data points which are responsible for creating the bias based on the protected attribute.
we call these data points the ambiguous data points.
fig.
describes the approach we applied to find out the ambiguous data points depending on the protected attribute.
we divideesec fse november virtual event usa joymallya chakraborty suvodeep majumder zhe yu and tim menzies training data privileged groupsunprivileged groups training model training model predicted labels for training datapredicted labels for training data prediction match?
yes nodata point unbiaseddata point biaseddivision based on protected attribute figure pre processing technique for bias removal from training data the training data into two groups based on the protected attribute privileged and unprivileged.
then we train two separate models on those two groups.
once we get the two trained models for all the training data points we check the prediction of these two models if the prediction matches in both cases the data point being examined is unbiased.
if two models contradict each other for a data point there is a possibility of this data point being biased this is an ambiguous data point.
we remove that data point from training data.
later we will describe why this works and how to validate.
we call this data cleaning process as bias removal from training data.
once we are done removing the probable biased data points we train a new model on the rest of the training data and make prediction using that model.
table shows the total number of rows in each dataset and the number of rows we removed.
we see that at most we lose of training data after bias removal step.
later we will show that this does not affect much the performance of the prediction model.
we remove the ambiguous bias causing data points by constructing two separate logistic regression models conditioned upon the protected attribute of the dataset.
let s assume the original data points are denoted as x where x1 x2 x3 .... x nare the attributes of the dataset and the protected attribute is denoted as s s xk where k is a number between to n and yis the model prediction.
the original dataset is further divided into subsets based on the values of a protected attribute in this case x1 x s 1andx2 x s .
we use these two subsets to build two logistic regression models such as p y s 0 1x1 2x2 .... n 1xn p y s 1x1 2x2 .... n 1xn f1 x logep y s p y s f2 x logep y s p y s next we use these logistic regression models to check for each training data point by retaining the data points where x x f1 x1 f2 x1 this results in retaining only the data points where there is no contradiction about the models outcome irrespective of data distribution conditioned upon the protected attribute thus removing the data points which add ambiguity to the model and introduce bias into the model s prediction.
table rows total number of rows dropped rows total number of rows detected as ambiguous biased datasetprotected attribute rows dropped rows of rows dropped sex .6adultrace48 .
sex .6compasrace7 .
default credit sex .
heart helth age .
german sex .
in the five datasets we used due to the pre processing step we do not lose much of training see table .
but in case of other datasets or real world scenarios if too many data points are found biased and model prediction gets damaged due to this loss then we would suggest relabeling of data points instead of removal.
in such relabeling any majority voting technique like k nn can be used.
biased data points will be assigned a new class label depending on k nearest neighbor data points.
such relabeling comes with an extra cost finding distance for all the data points so we recommend it to use only if model prediction is affected due to the removal of biased data points.
this study does not include that experiment but this could be an interesting direction for future work.
.
what if there are two protected attributes?
fig.
shows the approach we applied for one protected attribute.
but in some cases there are more than one protected attribute in a dataset.
like adult and compas datasets sex and race .
if we have two protected attributes we divide the training data based on those two attributes into four groups two privileged and two unprivileged groups .
then we apply the similar logic to find the biased data points.
we train four different models on those four groups and check their predictions match or not.
these models are not used for prediction they are used to find biased data points only.
in the two datasets we did not lose more than of training data with this approach.fairway a way to build fair ml software esec fse november virtual event usa as to handling more than two protected attributes we do not explore it here for the following reason.
with our data sets such ternary or more protection divides the data into unmanageable small regions.
future research in this area would require case studies with much larger data sets.
.
model optimization ibm has created a github repo to combine some promising prior works on fairness domain .
the results show that most of the prior methods damage the performance of the model while making it fair.
so prediction performance and fairness are competitive goals .
when there is a trade off between competing performance goals multi objective optimization is the way to explore the goal space.
in our case the goal of such optimizer would be to make the model as fair as possible while also not degrading other performance measures such as recall or false alarm.
to explore such multiobjective optimization we divided the dataset into three groups training validation and test .
during the pre processing step we removed biased data points from the training set.
after that logistic regression model is trained on the training set with the standard default parameters3.
then we used the fair flash algorithm discussed below to find out the best set of parameters to achieve optimal value of four metrics higher recall lower false alarm lower aod and lower eod on the validation set.
finally the tuned model is applied on the test set.
nair et al.
proposed flash a novel optimizer that utilizes sequential model based optimization smbo .
the concept of smbo is very simple.
it starts with what we already know about the problem and then decides what should we do next .
the first part is done by a machine learning model and the second part is done by an acquisition function.
initially a few points are randomly selected and measured.
these points along with their performance measurements are used to build a model.
then the model is used to predict the performance measurements of other unevaluated points.
this process continues until a stopping criterion is reached.
flash improves over traditional smbo as follows flash models each objective as a separate classification and regression tree cart model.
nair et al.
report that the cart algorithm can scale much better than other model constructors e.g.
gaussian process models .
flash replaces the actual evaluation of all combinations of parameters which can be a very slow process with a surrogate evaluation where the cart decision trees are used to guess the objective scores which is a very fast process .
such guesses may be inaccurate but as shown by nair et al.
such guesses can rank guesses in approximately the same order as that generated by other much slower methods .
flash was invented to solve software configuration problem and it performed faster than more traditional optimizers such as differential evolution or nsga ii .
for our work we modified flash and generated fair flash that seeks best parameters for logistic regression model with four goals higher recall lower false alarm lower aod lower eod.
algorithm shows the pseudocode of fair flash.
it has two layers one learning layer and one 3in scikit learn those details are c .
penalty l2 solver liblinear max iter .algorithm pseudocode of fair flash inspired from deffair flash pick a number of data into build pool evaluate the build pool and put the rest into rest pool while life build cart model by using build pool next point max model.predict rest pool build pool next point rest pool next point ifmodel.evaluate next point max build pool life return max build pool dataset train set validation set test set bias removalpre processor70 model training fair flash goals recall f ar aod eodoptimizer optimized model prediction figure block diagram of fairway.
for details on fair flash see algorithm .
optimization layer.
when training data arrives the estimator in the learning layer is being trained and the optimizer in optimizing layer provides better parameters to the learner to help improve the performance of estimators.
such trained learner is evaluated on the validation data afterward.
once some stopping criteria is met the generated learner is then passed to the test data for final testing.
in summary fairway consists of two parts bias removal from training data and model optimization to make trained model fair.
fig.
shows an overview of the method.
results our results are structured around six research questions.
for all the results we repeated our experiments ten times with data shuffling and we report the median.
rq1.
what is the problem with just using standard learners?
the premise of the paper is our methods offer some improvement over common practices.
to justify that we first need to show that there are open issues with standard methods.
we trained a logistic regression model with default scikit learn parameters and tested onesec fse november virtual event usa joymallya chakraborty suvodeep majumder zhe yu and tim menzies adult sexadult racecompas sexcompas race default credit sexheart health agegerman sex0.
.
.
.
.
.0recallchange of recall baseline pre processing p optimization o fairway p o adult sexadult racecompas sexcompas race default credit sexheart health agegerman sex0.
.
.
.
.
.
.6false alarmchange of false alarm adult sexadult racecompas sexcompas race default credit sexheart health agegerman sex0.
.
.
.
.
.
.
.14aodchange of aod adult sexadult racecompas sexcompas race default credit sexheart health agegerman sex0.
.
.
.
.
.
.30eodchange of eod figure performance and fairness metrics for a default state in orange b after pre processing in blue c after just optimization in green and d after performing pre processing optimization in red.
in these charts higher recalls are better while for all other scores lower values arebetter .
the five datasets.
the orange column in fig.
shows the results achieved using that model.
the recall is higher the better and false alarm aod eod are lower the better.
recall and false alarm are showing the prediction performance of the model.
the high value of fairness metrics aod eod in all five datasets signifies that model prediction is not fair means depending upon protected attribute privileged group is getting advantage over unprivileged group.
we treat this results as baseline for our experiment.
we need to make the prediction fair without much damaging the performance.
rq2.
how well does pre processing improve the results?
fairway is a two part procedure data pre processing ambiguity removal and learner optimization fair flash .
it is reasonable to verify the contribution of both parts.
accordingly rq2 tests the effects of just ambiguity removal.
before training logistic regression model training data was cleaned to remove ambiguous data points having improper labels using the approach mentioned in section .
.
table shows this step causes loss of maximum of the training data.
after that logistic regression model was trained on remaining data points and tested.the blue column in fig.
shows the results achieved using that model.
we see minor damage in recall for some cases and significant improvement in case of fairness metrics lower aod eod .
it is evident that pre processing the data before model training makes the model prediction fairer.
rq3.
how well does optimization improve the results?
moving on from rq2 the third research question is to check the effect of just optimization no pre processing .
to do that we tuned the logistic regression model parameters using fair flash to optimize the model for higher recall lower false alarm and lower fairness metrics aod eod .
then the tuned model was used for prediction.
the green column in fig.
shows the results achieved using that model.
we see that in cases of prediction performance recall false alarm it performs similar or better than pre processing but in case of fairness metrics aod eod preprocessing does better.
so optimized learner is significantly better than baseline learner but combining pre processing may perform even better.fairway a way to build fair ml software esec fse november virtual event usa table comparison of fairway with prior algorithms.
recall is higher the better.
false alarm aod and eod are lower the better.
gray cells show improvement and black cells show damage.
white cells show no change.
recall false alarm aod eodalgorithm dataset protected attributebefore after before after before after before after sex .
.
.
.
.
.
.
.04adultrace .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.07compasrace .
.
.
.
.
.
.
.03optimized preprocessing german sex .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.03adultrace .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.12compasrace .
.
.
.
.
.
.
.03reweighing pre processing german sex .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.02adultrace .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.06compasrace .
.
.
.
.
.
.
.06adversial debiasing in processing german sex .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.04adultrace .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.03compasrace .
.
.
.
.
.
.
.07reject option classification post processing german sex .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.03adultrace .
.
.
.
.
.
.
.
sex .
.
.
.
.
.
.
.21compasrace .
.
.
.
.
.
.
.13fairway pre processing in processing german sex .
.
.
.
.
.
.
.
rq4.
how well does fairway improve the results?
our fourth research question explores the effect of fairway which is a combination of pre processing and optimization.
the red column in fig.
shows the results achieved after applying fairway.
fairway is performing better than pre processing and optimization in most of the cases.
for example in case of adult dataset for the protected attribute race fairway achieves almost similar recall with optimization but much better in the other three metrics.
in case of default credit dataset for the protected attribute sex fairway is providing best results for all four metrics.
in some cases recall is slightly damaged.
but overall fairway is making the model fair without much affecting the performance.
so pre processing the data before model training and tuning the model while training both are important.
rq5.
how well does fairway perform compared to previous fairness algorithms?
we have decided to compare our approach fairway with some popular previous algorithms described in section .
.
we chose five such algorithms all from ibm aif360 which we thought could be representative of the works done before.
table shows the results for three datasets adult compas and german.
it shows the change of recall false alarm and two fairness metrics aod eod before and after the algorithms are applied.
in most of the cases fairway is performing better or the same with prior algorithms in case of reducing ethical bias aod eod .
in case of false alarm fairway has less number of black cells showing damage.
like fairway previous algorithms also slightly damage the recall metric.
in some situations this may become a matter of concern.
we see a scope of improvement here where future researchers should focus.
we have performed scott knott significance test and a12 effect size test for comparison.
for aod fairway performs better in cases and for eod in cases.
here better means result is statistically significantly better.
for the rest of the cases although having the same rank improvement is between .
also fairway wins on false alarm for all cases and keeps the same recall in cases and damages in .
and when fairway loses in recall it does not lose by much .
fairway is not just another bias mitigation approach.
it differs from prior works in several ways the first part of fairway is finding bias in training data.
so even before model training fairway shows which data points in the training data have improper biased labels and can affect prediction in future.
if labeling was done by human reviewers it leads to finding bias in human decisions.
instead of blindly trusting the ground truth of training data fairway can be used to find bias in the ground truth.
prior bias mitigation algorithms come from the core concepts of machine learning.
software practitioners having little ml knowledge may face difficulties to use these algorithms .
in case of fairway users can clearly see how two different models trained on privileged and unprivileged groups give different predictions on biased data points.
this makes fairway much comprehensible.
fair flash gives user the flexibility to choose which parameters to optimize.
in this paper logistic regressionesec fse november virtual event usa joymallya chakraborty suvodeep majumder zhe yu and tim menzies model is used.
but fair flash is easily extensible for other classification models.
so fair flash is adjustable too.
fairway is a combination of bias testing and mitigation.
this is described in rq6 .
rq6.
can fairway be used as a combined tool for detection and mitigation of bias?
in section .
it is shown that there are mainly two types of previous works done by researchers finding the bias in ai software and mitigating the bias.
as per our knowledge we are the first one to combine these two.
fairway finds the data points which have unfair labeling in the training data and remove those data points so that prediction is not affected by protected attribute.
we used situation testing to verify whether after bias removal the role of a protected attribute on the prediction changes or not.
we switched the protected attribute value for all the remaining data points e.g.
we changed male to female and female to male .
then we checked whether these changes lead to prediction changes or not.
if the prediction changes for a data point we say that it fails situation testing.
figure shows the percentage of data points failing situation testing before and after pre processing step of fairway the orange and blue columns show results before after applying fairway.
in all cases the values on the blue column are far smaller than orange column.
so fairway can find the data points responsible for bias in the training data.
now it is an engineering decision to set the threshold of what percentage of training data can be ambiguous where prediction may change depending on the protected attribute value.
fairway provides the percentage and depending on the application user can decide whether bias is present in the system or not.
so fairway can be applied as a discrimination finder tool.
if discrimination is above the tolerable threshold then fairway can be applied for removing bias from training data and optimizing model without damaging predictive performance.
so fairway can be used as a combined tool for detection and mitigation of discrimination or ethical bias.
one unique feature of fairway is it is model agnostic .
it finds bias by verifying prediction of a model and mitigates bias by cleaning training data and tuning model parameters.
so it can work for any black box model.
as fairway only works on the output space of a model it can be easily used in industrial purposes where revealing core algorithm of the underlying model is not possible.
so to summarize the results we say that we have explained the reasons of bias in the five datasets we used.
we have developed a comprehensible method fairway which can remove bias from training data and the model.
unlike prior works fairway is not just a bias mitigation approach it is a combined tool for ground truth validation bias detection and mitigation.
threats to validity sampling bias we have used five datasets from uci machine learning repository where most of prior works in fairness domain use only one or two datasets.
these are well known datasets and used by previous researchers in ml and software adult sexadult racecompas sexcompas race default credit sexheart health agegerman sex024681012 of data points failing situation testingbefore afterfigure percentage change of data points failing situation testing showing bias before and after pre processing.
fairness domain.
it is an open issue if these data sets reflect an interesting range of fairness issues for other data sets.
in future work we would explore more data sets.
evaluation bias we have used two fairness metrics eod and aod.
we have mentioned the drawbacks of fairness metrics which only consider the tpr and fpr and neglect the class distribution.
recent work has deduced a new fairness metric called conditional equality of opportunity to overcome this drawback .
conditional equality of opportunity is defined for conditioning on every feature and finding the opportunity gap for privileged and unprivileged groups.
in future work we would explore more performance criteria.
construct validity in our work we trained different models on privileged and unprivileged groups.
the datasets contained one or two protected attributes so our method is feasible.
all the prior works we have seen treated each protected attribute individually.
we have shown how to deal with two protected attributes.
in future work we would explore larger data sets with more protected attributes.
external validity fairway is limited to classification models which are very common in ai software.
we are currently working on extending it to regression models.
in future work we would extend this work to other kinds of data mining problems e.g.
to text mining or video processing systems.
conclusion we have explained how a model acquires bias from improper labels of training data and have demonstrated an approach called fairway which removes ethical bias from the training data and optimizes a trained model for fairness and performance.
we have shown that fairway is comprehensible and can be used as a combined tool for detection and mitigation of bias.
unlike some prior ml works fairway is not just a bias mitigation tool it validates ground truth labels finds bias and mitigates bias.
we have made the source code of fairway publicly available for software researchers and practitioners.
to the best of our knowledge we claim this is the first work in se domain which concentrates on mitigating ethicalfairway a way to build fair ml software esec fse november virtual event usa bias from software and making software fair using optimization methods augmented with some data pre processing.
in future we hope more and more software researchers will work on this domain and industries will consider publishing more datasets.
when that data becomes available it would be appropriate to rerun this study.