large language models for safe minimization aashish yadavally computer science dept.
university of texas at dallas texas usa aashish.yadavally utdallas.eduxiaokai rong computer science dept.
university of texas at dallas texas usa xiaokai.rong utdallas.eduphat nguyen computer science dept.
university of texas at dallas texas usa phat.nguyen2 utdallas.edutien n. nguyen computer science dept.
university of texas at dallas texas usa tien.n.nguyen utdallas.edu abstract several tasks in program analysis verification and testing are modeled as constraint solving problems utilizing smt solvers as the reasoning engine.
in this work we aim to investigate the reasoning capabilities of large language models llms toward reducing the size of an infeasible string constraint system by exploiting inter constraint interactions such that the remaining ones are still unsatisfiable.
we term this safe minimization.
motivated by preliminary observations of hallucination and error propagation in llms we design s afemin a framework leveraging an llm and smt solver in tandem to ensure a safe and correct minimization.
we test the applicability of our approach on string benchmarks from leetcode in the computation of minimal unsatisfiable subsets muses .
we observed that safeminhelps safely minimize .
of these constraints with an average minimization ratio of relative to the muses.
in addition we assess s afemin s capabilities in partially enumerating non unique muses which is baked into our approach via a sample and enumerate decoding strategy.
overall we captured .
more non unique muses than without such llm based macro reasoning.
finally we demonstrate s afemin s usefulness in detecting infeasible paths in programs.
index terms large language models constraint solving safe minimization inter constraint reasoning i. i ntroduction several tasks in the domains of analysis testing and verification are modeled as constraint satisfaction problems.
these include but are not limited to symbolic execution and automated test case generation type checking program verification security and optimization .
for these tasks program states and their transitions are expressed as logical formulae and satisfiability modulo theories smt solvers such as z3 cvc4 etc.are used as the back end reasoning engines.
for example in symbolic execution a program path is represented as a logical constraint aiding a systematic exploration of all possible execution paths.
in model checking specifications of certain properties e.g.
no null pointer is ever dereferenced are expressed in a temporal logic and the unreachability of the error state is verified.
machine learning ml has been driving advancements to improve smt s efficiency and scalability particularly on enhancing and optimizing heuristic algorithms in solvers.
ml based solvers can be classified into two categories learning aided solvers and stand alone learning based ones.
first learning aided approaches combine ml techniques with modern solvers replacing manually crafted heuristics with efficient ml based ones .
these in clude producing initialization assignments to achieve runtime reduction on local search replacing searching heuristics with reinforcement learning to reduce the number of iterations learning the distribution of the subset of clauses that is unsatisfiable called unsat cores to guide the solvers or learning the correlations among clauses by treating them as un ordered sequences with transformers to provide a learning enhanced initialization heuristic for solvers.
second stand alone learning based solvers predict satisfiability and decode value assignments with ml models.
for instance deepsat formulates a solution as a product of conditional distributions and obtains the assignments by maximizing joint probabilities.
its assertions rely on statistics.
neurosat trains an gnn for satisfiability prediction.
with the advances of large language models llms it is natural to study the extent they can aid smt solvers.
while the state of the art ml based smt solvers focus on deciding whether a constraint system is satisfiable sat or unsatisfiable unsat in this work we aim to explore the ability of llms in providing the evidential explanation of such a decision.
in particular we investigate the llm s ability in safe minimization which is defined as follows given an unsat constraint system safe minimization reduces the size of the system such that the unsat property is preserved.
in other words safe minimization reduces the size of the constraint system from ntom m n such that the remaining mconstraints are still unsat.
we refer to such subsets of the original constraint system as smaller unsatisfiable subsets suses .
a special case of suses is a minimal unsatisfiable subset mus .
an mus is defined as an unsatisfiable subset of a constraint system such that the removal of any constraint makes it satisfiable.
safe minimization to suses or muses is crucial in several tasks.
it could enhance the analysis of inconsistencies in a program which may lead to unexpected behaviors.
an unsat path constraint system contains multiple constraints representing the conditions in a program.
an mus is useful in localizing the inconsistencies among the conditions along the path due to its minimal size.
these inconsistencies could stem from bugs incorrect assumptions conflicts among program components or even design flaws and implementation errors.
localizing the source of such inconsistencies is crucial for software engineers to effectively tackle these challenges.
moreover muses can be viewed as representing the minimal reasons orexplanations of the infeasible paths.in brief we seek to answer the question is it possible to safely minimize a constraint system under the condition that no inconsistency causing constraints are eliminated ?
smt solvers have specialized decision procedures spanning theories on integer and real number arithmetic arrays and strings.
in this work we focus on string constraints involving string operations such as concatenation character and regex matching string lengths substrings etc.
the solvers on string theory are relevant for various tasks including automated testcase generation for scripting languages static analysis of security vulnerabilities in web applications against code injection caused by improper handling of untrusted strings etc.
we conducted a preliminary empirical study in safe minimization with llms on multiple smt lib string benchmarks extracted from leetcode interview questions.
we observed that llms are capable of analyzing dependencies among the constraints and can decompose an unsatisfiable set of constraints into sub groups and reason over them let us call this macro reasoning to discover the redundant constraints that can safely be eliminated.
however one key issue with them is hallucinations.
we reported two classes of hallucinations in the llm generated responses i orthographic where the constraints in generated candidate suses do not belong to the original constraint system ii rational where the reasoning used to produce the solution s or the candidate suses is flawed due to which it is incorrectly assumed to be un satisfiable.
with gpt .
we reported parsing errors in .
of the cases i.e.
the model did not adhere to the prescribed output format due to which the sus could not be retrieved.
in .
of the cases it determined satisfiable constraint subsets as unsatisfiable.
other llms e.g.
gpt gemini .
pro and claude .
sonnet have no parsing errors.
the cases where a model incorrectly determined satisfiable constraint subsets as unsatisfiable dropped to .
.
and respectively.
accordingly we incorporated the following strategies to mitigate hallucinations.
first for orthographic hallucinations we employed a parser which uses edit distance as a metric to map the constraint generated by the llm to the closest one in the input formula.
second we utilized an smt solver to verify the unsatisfiability of the candidate suses.
thus we ensure that the candidate suses are both correct andunsatisfiable .
as an application for safe minimization we used the suses produced by the llm in combination with smt solver as a verifier and input them to the smt solver cvc5 to compute the muses.
this process of computing muses is more efficient due to a reduction in search space of muses from o 2n too 2m m n .
in terms of finding muses in their constraint space we can consider this as a soft search which allows for the llm to not be penalized for imprecision.
another dimension of complexity in muses stems from the fact that there can exist multiple non unique ones for a given over constrained system.
however enumerating all muses is typically intractable with respect to completion and a formula withnconstraints can have an order of 2nmuses.
as a result traditional mus enumeration algorithms do not complete enumeration within a reasonable time.
recently there has been a shift towards avoiding such an exhaustive enumeration and computing an approximate count of the muses .
we bake such a parallelized partial mus enumeration into our approach by adopting a sample andenumerate decoding strategy within the llm.
we refer to this asself exploration and the model as explorer llm .
our idea is that the explorer llm samples combinations of different macro reasoning paths while analyzing the inter constraint relations where each path aims at explaining a unique source of unsatisfiability and potentially leads to a unique sus.
putting together the above ideas to enhance the capability of the llms we propose s afemin a learning aided solving framework that employs an llm and an smt solver in tandem to safely minimize a constraint system into its unsatisfiable subsets suses such that it encompasses all inconsistency causing constraints.
from our extensive evaluation of s afeminframework on smt lib string benchmarks we were able to safely minimize .
of the string constraints to their suses with an average reduction of relative to their respective muses across all minimized ones.
by adapting the marco algorithm which is designed to enumerate minimal unsatisfiable subsets muses by searching the constraint space we explored the computation of smaller unsatisfiable subsets suses .
our results show that safeminsignificantly outperforms both random selection and search based breadth first and depth first strategies in terms of minimization accuracy and minimization ratio.
we include several stratified analyses probing the effect of the number of constraints in the formula on s afemin s effectiveness.
we compared s afeminwhen being used with an smt solver for mus computation against using a traditional smt solver directly toward partial mus enumeration and noticed that our tool captures .
more non unique muses.
moreover we conducted an in depth analysis of the llms macro reasoning capabilities in s afemin revealing their understanding of decision procedures in smt theories as well as indicating its potential to scale to complex systems.
finally we conducted a case study on the usability of our framework in identifying the conflicting conditions toward detecting infeasible paths in a real world program.
in brief our key contributions include to the best of our knowledge this is the first study to explore the reasoning capabilities of llms on complex constraint benchmarks such as smt comp.
s afemin a novel macro reasoning approach for safely minimizing the constraint systems towards optimizing constraint solving.
all our data is publicly available .
s afeminis a useful tool for computing muses while also facilitating their parallelized partial enumeration.
ii.
i llustrations and concepts a. infeasible constraint systems in fig.
we present the conjunctive normal form cnf of a string formula extracted from the source code for an interview question related to the partition problem leetcode as documented in the smt lib benchmarks .
this constraint system is unsatisfiable and the constraint set1at s at s 2not at s at s 3length s 4length s 5at s at s 6at s at s 7not at s at s 8not at s at s 9not at s at s not length s not length s not at s at s not length s not length s not length s not length s not length s not length s at s at s not at s at s not at s at s not at s at s not at s at s not length s not length s at s at s not at s at s not at s at s not at s at s not at s at s at s at s not length s not length s not length s not length s not length s not length s fig.
.
a motivating example c1 c5 c19 c21 highlighted in orange indicates one of itsminimal unsatisfiable subsets muses .
the inconsistency is related to the string s in which constraints c1 c5 and c19 assert that the characters at indices and are all the same and c21asserts that those at indices and can not be the same.
note removing any of the constraints in the mus will no longer preserve such a contradiction thus minimal .
for a constraint system c c1 c2 ... c n over a set of variables if we can find an assignment to all variables such that each ciis satisfiable i.e.
all restrictions of every ci are met by the corresponding assignments we say that cis satisfiable .
otherwise it is considered to be unsatisfiable or infeasible .
in this work we focus on analyzing infeasible string constraint systems on the basis of the following concepts definition smaller unsatisfiable subset sus .a smaller unsatisfiable subset of an unsatisfiable constraint system cis a subset s csuch that sis still unsatisfiable.
definition safe minimization .given an unsatisfiable constraint system c the identification of a subset ssuch that it is still unsatisfiable i.e.
s is an sus is referred to as safe minimization.
otherwise if sis satisfiable it is unsafe.
definition minimal unsatisfiable subset mus .a minimal unsatisfiable subset of an unsatisfiable constraint system cis a subset m csuch that mis still unsatisfiable and cj m m cj is satisfiable.
note that the minimality in mus refers to a set minimality and not to minimum cardinality.let us illustrate safe minimization via this example.
for achieving safe minimization of the above unsat constraint system consider constraint c4in listing which asserts that the length of a string sshould be equal to .
next consider pairs of constraints c10 c11 c13 c14 c15 c16 c17 c18 c24 c25 c32 c33 c34 c35 and c36 c37 .
each of these resolves to length s length s length s length s length s length s length s and length s respectively.
furthermore c3resolves to length s in conjunction with c4.
thus including c4and eliminating the constraints c3 c10 c11 c13 c18 c24 c25 andc32 c37 would not affect the unsatisfiability of the resulting set.
consider constraint c1which asserts that the character at index in string sis the same as that at index .
accordingly constraint c2 which originally asserts that the character at index in string sis not the same as at index can be interpreted as the character at index in string snot being the same as that at index .
we can see that this is the same as constraint c8 which is repetitive.
thus c8can be eliminated without affecting the unsatisfiability of the resulting set.
using a similar analogy between constraint sets c5 c9 c12 c5 c21 c22 c5 c28 c29 c6 c20 c23 c6 c27 c30 and c21 c31 c28 the constraints c12 c22 c29 c23 c30 andc28 respectively can safely be eliminated as well.
we refer to such a reasoning on sub groups of constraints in a constraint system as macro reasoning .
we hypothesize that by exploiting the inter constraint relationships via macroreasoning we can identify redundant constraints from the input formula while preserving the inconsistency in the sus.
definition macro reasoning .macro reasoning is the reasoning on sub groups of an unsat constraint system that exploits the inter constraint relationships to identify and eliminate redundant constraints and enable safe minimization.
recently large language models llms have shown emergent behaviors with the abilities to reason in various domains e.g.
arithmetic formal logic source code etc.
moreover they have been shown to effectively produce proof steps towards proof generation .
this work aims to evaluate whether llms can be leveraged to decompose a constraint system and macro reason on groups of constraints via tractable steps of thoughts towards identifying inferred redundant constraints .
that is we model this as a soft search problem to see whether the llm can be leveraged to explore the constraint space to approximately identify the constraints that do not contribute to an inconsistency the remaining which can be encapsulated as the sus .
this contrasts with traditional exhaustive search on the entire constraint space .
for illustration fig.
displays the hasse diagram for the power set lattice for a set of constraints c .
edges represent containment relations.
starting from the bfs and dfs strategies exhaustively explore the state space by eliminating constraints from cand checking the satisfiability of each subset.
in contrast s afeminreasons about the inter constraint relations to get to i.e.
anfig.
.
hasse diagram of the power set lattice for a generic set of four constraints c .
starting from one can explore the constraint space via local search strategies such as depth first and breadthfirst search compared to s afemin s macro reasoning driven approach.
sus without exhaustive search which helps converge faster to i.e.
mus .
our evaluation results empirically demonstrate this macro reasoning capability of llms section iv .
b. halluciation mitigation we conducted a preliminary experiment to study the capability of llms in macro reasoning for safe minimization section iv .
a key limitation of llms is their hallucination i.e.
generating plausible but non sensical information.
this is concerning as the generated candidate suses cannot be trusted to be correct or even unsatisfiable.
from our results we report two classes of hallucinations in the llm generated response and leverage the following strategies for mitigation i orthographic hallucinations the constraints in candidate suses do not belong to the original constraint system.
for this type of hallucinations a parser is used in which we map the generated constraint by the llm to the closest one in the input formula via edit distances.
ii rational hallucinations the reasoning used to produce candidate suses is incorrect due to which satisfiable ones are marked unsatisfiable and vice versa.
for the rational hallucinations an smt solver is used as a verifier to validate the llm generated candidates and accept or reject them.
c. parallelized partial enumeration of multiple muses by exploring diverse reasoning paths while computing muses is an application of suses produced by our framework a constraint system might have multiple muses.
an mus for the above constraint system is highlighted in listing .
however this string formula has multiple muses including constraint sets c1 c5 c26 c28 c1 c26 c29 c1 c5 c21 c26 c31 etc.
finding all muses is typically intractable with respect to completion and a formula with nconstraints can have an order of 2nmuses.
thus applications of muses tend to relax the completeness criterion by not focusing on finding all of them but depend on the number produced within a certain time limit .
complex reasoning tasks often have multiple reasoning paths.
wang et al.
make use of this diversity viaa sample and marginalize decoding strategy in the llm where the optimal answer for a question is decided by marginalizing out the sampled reasoning paths and finding the most consistent one.
due to the nature of our task of minimizing constraint systems we formulate it as a soft search problem.
thus we propose a contrasting sample andenumerate decoding strategy.
here the sampled reasoning paths can lead to multiple sus candidates pertaining to nonunique muses.
such a design establishes a partial enumeration of muses.
we call this strategy self exploration which encompasses the varied macro reasoning perspectives of the llm within a specific constraint system.
theoretically speaking increasing the size of the sample during self exploration results in a more complete enumeration of muses.
furthermore combining all such candidate suses with independent smt verifiers as described in section ii b helps parallelize partial mus enumeration with llms.
iii.
m acro reasoning for safe minimization of unsat c onstraint systems our empirical results section v showed that gpt .
and gpt exhibit macro reasoning capabilities in safe minimization of unsat constraint systems.
however they are still limited by halluciations and partial enumeration of multiple muses.
putting together the above ideas to enhance the macro reasoning capabilitiess of llms in safe minimization we propose s afemin a learning aided solving framework that employs an llm and an smt solver in tandem to safely minimize a constraint system into the unsatisfiable subsets suses .
this section presents s afeminframework .
an infeasible constraint system c c1 c2 ... c n often contains constraints some of them contributing to the unsatisfiability of the system and others that do not.
safe minimization aims to find the subsets of c which are both unsatisfiable and have fewer constraints than the input formulae.
these subsets are referred to as the smaller unsatisfiable subsets suses s .
an sus contains all constraints that lead to unsatisfiability in an mus possibly also including a few that do not.
we posit that this approximation is desirable as theimprecision provides better correctness guarantees in the form of unsatisfiability of the candidates.
safeminis designed to have the following key phases a. self exploration the goal here is to explore the constraint space of a given constraint system and identify candidate suses.
accordingly we call this process self exploration and enable it by leveraging large language models llms along two dimensions large language model as a macro reasoning agent inter constraint relationships within muses encapsulate the underlying conflicts that lead to the unsatisfiability of a given constraint system.
the primary role that the llm serves is to capture such constraint interactions and decompose the constraint system into sub groups of constraints.
we surmize that reasoning on such sub groups facilitates the identification of the root causes for inconsistencies and subsequently thefig.
.
overview of s afeminframework you are playing the role of an smt solver on strings.
instructions given a list of constraints or clauses your goal is to identify a subset of these such that it is still unsatisfiable i.e.
containts conflicts resulting from inconsistencies or contradictions in the logical state space.
follow these thought steps to find the conflicts or contradictions step read and understand the given logical string formula to identify the variables operators and logical connectives used in the formula.
step analyze dependencies both within and between constraints.
these can arise from string operations such as concatenation length position substring extraction etc.
as well as the variables they share.
step macro reason on these constraints to find pairs or groups that can possibly be combined and resolved.
it is okay for there to be overlap among the different groups of constraints.
the motivation for this grouping however should be to find redundant constraints in the input which can be inferred from combining parts of these groups.
note that such redundant constraints can be eliminated.
step next try to identify constraints or groups of constraints which also satisfy another constraint or group of constraints.
in such a case the latter constraint or group can be eliminated.
for steps and try to find as many such groups as possible.
step try to identify pairs of constraints that directly conflict with each other.
these need to definitely be part of the output subset.
note that such conflicts or inconsistencies will arise in the given example.
think step by step.
output format after identifying all such constraints that can safely be removed output a comma separated list of all remaining clauses.
each clause should be inserted between c and c tags and the output between output and output tags.
disclaimer try to minimize the input constraints as best as possible while also ensuring the output subset s unsatisfiability.
fig.
.
system prompt for explorer llm in s afemin.
suses.
furthermore this process establishes tractable steps of thoughts for minimizing the constraint space.
in fig.
we present the prompt given to the llm for this purpose which primarily illustrates the instructions for it to pursue a chain of thought cot reasoning and decompose the input constraint system into constraint subgroups to macro reason about them and the prescribed output format for the candidate suses.
in our preliminary experiments we observed that directing the llm to output the entire list of constraints that belong to the candidate suses nudges it to recall the inferred knowledge from the macro reasoning steps as opposed to using special markers for identifying theconstraints.
in this way the llm plays the role of exploring the constraint space towards summarizing the conflicts in a constraint system in the form of their suses.
exploring diverse reasoning paths as noted earlier it is possible for a constraint system to have multiple muses i.e.
multiple sources of inconsistencies.
in this context another dimension of llm operability is the parallelized partial enumeration of such non unique muses .
to enable this process we propose a sample andenumerate decoding procedure where we first sample from the llm s decoder to generate a diverse set of reasoning paths.
while the basis of these paths is rooted in macro reasoning on the input formula as in section iii a1 each might be focused on a different source of inconsistency thus leading the llm toexplore non unique candidate suses.
subsequently we can leverage these candidates to extract non unique muses.
while self exploration is similar in spirit to self consistency decoding strategy a notable difference is that the latter follows the sample and marginalize approach wherein marginalization is used to converge to the most consistent response.
as our task is rooted in search space exploration we do not aggregate the reasoning paths in this manner but use the divergence toenumerate multiple candidates instead.
another major advantage of this design is that it naturally facilitates parallelization as the candidate suses can be independently analyzed towards computing the muses.
in theory sufficiently increasing the sample size during self exploration can result in a complete enumeration of all muses.
b. correctness assurance llms can hallucinate and the candidate suses generated by explorer llm can contain constraints that do not belong to the input constraint system or they themselves can possibly not be unsatisfiable as is required by the definition of an sus .
to mitigate hallucinations of this form we incorporate two forms of correctness verification into s afemin s design constraint validity the first class of hallucinations is orthographic as it is possible that a constraint in the candidate sus could not belong to the input constraint set altogether.
to mitigate this we incorporate a parser into our design which maps the incorrect constraint to the closest constraint in the input formula using edit distance as a heuristic.
we adopt thismetric for such an auto correction as it is lightweight.
note that this can be swapped with embedding based similarity metrics as well which is left for future work.
overall this step ensures the validity of the candidate suses.
unsatisfiability verification the next class of hallucinations is rationale specific as it is possible for the llmgenerated candidate suses which it deduces to be unsatisfiable via several macro reasoning steps is not actually so.
in this case we see two ways of ensuring correctness by using an smt solver to verify a the correctness of the macroreasoning steps b the unsatisfiability of the final generated candidate sus.
while the former ensures a robust generation where only valid macro reasoning steps are considered to infer the sus it can require multiple calls to the solver and can be prohibitively expensive.
moreover during our preliminary evaluation we also observed that the llm sometimes tends to self correct through the course of generation.
thus we chose to include an smt verifier after the fact i.e.
which checks whether the final generated candidate sus is indeed unsatisfiable ensuring the correctness of candidate suses.
overall the explorer llm for self exploration as in section iii a parser andsmt verifier for correctness verification as in section iii b work in tandem to ensure the safe minimization of unsatisfiable constraint systems enabling a parallelized enumeration of suses and muses later.
c. mathematical formulation for an unsatisfiable input constraint system c c1 c2 ... c n letp c be its power set i.e.
p c c1 c2 ... cn c1 c2 ... c1 c2 ... c n .
the explorer llm in self exploration phase takes cas input and returns multiple candidate suses k i 1si where si p c andkis its decoding sample size.
next we check k i 1siforcorrectness to obtain l i 1si l k such that si p c i.e.
from parser and siis indeed unsatisfiable i.e.
from smt verifier .
finally muses can be extracted in parallel from l i 1si.
iv.
e mpirical evaluation we conducted several experiments seeking to answer the following research questions i intrinsic evaluation rq .
effectiveness in safe minimization of string constraints i can the llms in safeminexploit interconstraint relations toward safe minimization?
and ii how well do they safely minimize string constraint systems?
ii qualitative analysis of macro reasoning rq .how accurate is the llm s macro reasoning on input string constraint systems toward safe minimization?
iii extrinsic evaluation rq .
safe minimization toward mus computation how close are the suses produced by safeminto the actual muses?
rq .
diverse reasoning paths for suses toward muses how useful is safeminin using diverse reasoning paths in enumerating different muses for a constraint system?table i data statistics .
constraints instances operations .
.
.
.
.
overall .
rq .
detection of infeasible paths in source code how useful is safeminin detecting infeasible paths?
v. e ffectiveness in safe minimization a. data collection with the first objective being to evaluate the llm s capabilities in safe minimization we used the string benchmark set in smt comp that contains quantifier free string formulae with constraints reasoning about string lengths i.e.
qfslia from qf strings division .
in particular we picked theleetcode benchmark containing string formulae obtained from several leetcode interview questions.
these cover a wide range of complex string equations inequalities regular expressions and include string functions such as str.indexof str.substr and str.at .
in table i we list the total number of instances and the mean number of string operations.
to build the ground truth we leveraged the state of theart tool cvc5 to solve these constraints.
of the we identified a total of unsatisfiable instances.
since safemintakes unsatisfiable sets of constraints as input we disregard the remaining instances.
next we simplified the unsatisfiable instances algebraically via rewriting operations to extract corresponding lists of constraints.
finally we stratified them based on the number of constraints in each formula and split them equally into validation andtestsets.
we use the former for tuning the prompt in explorer llm and report the final performance on test split.
b. experimental setup baselines in this experiment we aim to assess how well the llms in s afeminreduce the given unsatisfiable set of constraints to the corresponding sus toward safe minimization.
first we adopted chain of thought cot prompting to assess the models macro reasoning capabilities.
then we used chain of thought prompting with self exploration cot se enabling them to explore diverse reasoning paths and capture multiple suses.
we compare the performance of safeminwhen using both prompting strategies with gpt3.
gpt gemini .
pro and claude .
sonnet.
consider a constraint system c c1 c2 ... c n which has a power set lattice p c c1 c2 ... cn c1 c2 ... c1 c2 ... c n .
as the baselines for our advanced prompting in s afemin we considered state space exploration with systematic search based as well as random strategies.
first we compared the cot based approaches with a baseline that randomly selects an element from p c i.e.
table ii effectiveness evaluation on string constraints .
evaluation metrics constraints total msus dumsus d approach d d d d d d random pass .
.
.
cot w gpt .
.
.
.
cot w gpt .
.
.
cot w gemini .
pro .
.
.
cot w claude .
sonnet .
.
.
random pass .
.
.
depth first max visited .
.
.
breadth first max visited .
.
.
cot sew gpt .
.
.
.
cot sew gpt .
.
.
cot sew gemini .
pro .
.
.
cot sew claude .
sonnet .
.
.
random pass .
these are equivalent since both check the unsatisfiability of the candidate sus once.
second to benchmark our cot se we also established a baseline that randomly selects kelements from p c i.e.
random pass k where kis the number of samples produced in cot se.
finally adapting the traditional non llm based marco algorithm which employs systematic state space exploration to enumerate muses we also established baselines that eliminate the constraints in the input constraint set cby exploring p c via breadth first bfs and depth first search dfs checking the unsatisfiability of the subsets in order to enumerate the suses.
for a fair comparison with cot se which checks for unsatisfiability of kcandidate suses we limit the traversal in bfs and dfs to a maximum of knodes involving a maximum of kchecks of unsatisfiability.
in fig.
we present an illustration of such search based baselines.
also given the computational cost of the unsatisfiability checks with an smt solver we set kas in this work.
in table ii we call these baselines depth first and breadth first max visited .
metrics to intrinsically measure the models performance in macro reasoning for safe minimization we define minimization accuracy am which measures the ratio of the number of instances in which the candidate suses are unsatisfiable to the total number of instances.
if drepresents the test set and durepresents the set of instances in which the candidate suses are indeed unsatisfiable mathematically am du d .
in the case of chain of thought prompting with self exploration i.e.
cot se where there are multiple candidate suses we consider the prediction of even one unsatisfiable candidate sus as a correct instance.
while ammeasures the proportion of instances in the dataset in which the input formula is safely minimized it does not quantify the reduction in search space.
thus we measure the quality of safe minimization of the constraint system cto the sus s i.e.
c s with minimization ratio which is defined as msus c s c .
finally we define two aggregated variants of m a that on the entire test set i.e.
msus d d p dmsus and b that where candidate suses are unsatisfiable msus du du p dumsus.c.
experimental results comparative results in table ii we report the performance of different variants of the models.
interestingly when prompted without self exploration from s afemin gpt .
row performs slightly better than the naive approach row despite the latter only picking a random constraint subset fromp c as the sus.
upon further analyzing the failures we discovered that there were parsing errors in .
of the cases i.e.
the model did not adhere to the prescribed output format due to which the sus could not be retrieved.
moreover in .
of the cases it determined satisfiable constraint subsets as unsatisfiable which we were able to reject with the smt verifier.
this shows the limitations of gpt .
in following our instructions for computing the sus.
in contrast by prompting gpt with the same instructions i.e.
cot row we observed an improvement in performance by .
.
this establishes a direct comparison between both gpt variants in their ability to follow our instructions and cot steps.
such improvement is .
for gemini .
row and .
for claude .
row .
in particular there were noparsing errors with these llms and the number of cases in which the model incorrectly determined satisfiable constraint subsets as unsatisfiable dropped to .
.
and .
rows .
safeminaims to address these limitations and enhance the llm s ability to safely minimize constraint sets.
compared to the baseline with random selections we observed the improvements for s afeminwith gpt gemini .
pro and claude .
sonnet from .
.
in amand .
.
inmsus.
this improvement for s afeminwith gpt .
over that random picker is smaller as gpt .
is limited as seen.
we also compared s afeminwith the marco algorithm with systematic bfs and dfs searches.
with the traversal limited to a maximum of nodes eliminating constraints to navigate the state space results in a limited exploration depth.
accordingly we observed low minimization ratios for both baselines.
however the possibility of hitting a candidate sus which is a superset of the mus in the constraint state subspace is high.
as a result the search based baselines achieve a highat concat indexof len substr406080100 string operations correct gpt .
gpt gemini .
claude .
fig.
.
performance comparison of cot sein llms on the most occurring string operations at concat indexof len and substr .
minimization accuracy.
this trade off shows the limitation of search based strategies for safely minimizing constraint sets.
next we compare the performance of our self exploration prompting with different llms i.e.
rows .
as seen safeminimproves the performance over only cot in minimization accuracy by .
.
and when using gpt .
gpt gemini .
pro and claude .
sonnet respectively.
moreover all llms with cot seoutperform their non sevariants across all intervals when stratified on the number of constraints demonstrating the effectiveness of cot sein s afemin.
finally while s afeminwith gpt and claude .
sonnet achieve similar minimization accuracy the minimization ratio for the latter is .
higher.
performance on string operations fig.
shows the llms performance with cot seprompting on top most occurring smt2 string operations.
among the instances containing str.at string character function str.concat string concatenation function str.len string length function and str.substr substring function all llms make correct predictions in more than of the cases with gpt and claude .
showing a better understanding of these operations .
gpt .
records a particularly low performance for str.indexof returns the index of the first occurrence of the specified value minimizing only of these occurrences correctly.
this can be attributed to the complexity of the str.indexof function which often occurred in more complex constraints with other operations which gpt .
failed to resolve.
in contrast gpt correctly predicts .
of these instances while gemini .
and claude .
s correctly predict all of them.
this result exhibits the improved macro reasoning capabilities of the other llms over gpt .
.
overall with s afemin s cot seprompting strategy we record the best performance with gpt row and claude3.
sonnet row safely minimizing the input formulae to suses in of the cases .
it outperforms the other baselines in minimization accuracy by .
.
andminimization ratio by .
.
in brief we can see that safeminwith gpt or claude .
sonnet when prompted with cot se was able to navigate the intricacies of the constraints and inter constraint relations inconsistencies.
ra .
gpt and claude .
sonnet demonstrate superior macro reasoning capabilities in exploring interconstraint relationships to achieve safe minimization.
s afeminwith cot seenhances the llms macroreasoning about sources of inconsistencies enabling safe minimization of of the string constraints.
time efficiency for safe minimization we report the time for safe minimization taken by all llms for safely minimizing the input formulae when prompted with s afemin s cot sestrategy.
for gpt .
the minimum observed processing time was .
seconds while the maximum recorded time was .
seconds.
the mean processing time for gpt3.
was .
seconds with a standard deviation of .
seconds .
conversely gpt recorded minimum and maximum processing times of .
and .
seconds respectively.
the mean processing time for gpt was .
seconds with a standard deviation of .
seconds .
for claude .
sonnet the times ranged from .
seconds to .
seconds with a mean of .
seconds and a standard deviation of .
seconds.
similarly gemini .
pro recorded a minimum processing time of .
seconds and a maximum of .
seconds with a mean of .
seconds and a standard deviation of .
seconds.
while being subject to network latency these metrics provide insights into scaling to real world systems with llms.
vi.
q ualitative analysis of macro reasoning in this experiment we aim to assess the quality of macroreasoning of llms used within s afemin.
due to the effort involved in manually parsing through the llm s textual responses we randomly picked examples from each of the input formulae sub groups in section v a i.e.
... collecting a total of instances and their corresponding llm responses when prompted with cot se.
from our analysis we could overarchingly identify the following strengths in all llms macro reasoning s1 able to pick contradicting logic pairs.
s2 able to resolve intricate combinations of operations.
other llms not gpt .
s3 able to combine constraints via transitive inference.
s4 understands str.substr andstr.at operations well.
s5 able to combine and resolve combinations of operations e.g.
the length of a substring str.len andstr.substr .
we observed the following categories of inaccuracies in all llms macro reasoning on constraint sub groups w1 makes some logical errors when constraints involve not operation.
for e.g.
gpt .
resolved not a b asa b. w2 some errors in combining constraints.
for e.g.
gpt .
resolved a and not a asnot a .
w3 some errors in understanding complex constraints.
for e.g.
str.len str.substr s 1table iii qualitative study on macro reasoning of llm s. weaknesses w1w2w3w4w5w6llm gpt .
gpt claude .
sonnet only gpt .
w4 tends to forget some of the previously inferred steps.
w5 combines constraints based on incorrect reasoning.
w6 fails to localize sources of inconsistencies.
table iii displays the counts of llms for all inaccuracies listed above except gemini .
which only produced the output constraint subsets without any analyses due to which we could not identify the weaknesses .
note that these counts are not mutually exclusive i.e.
one instance can be counted multiple times across categories.
furthermore in some cases we noticed that gpt and claude .
tend to ignore the prescribed thought steps even more so than gpt .
.
however its superior performance is indicative of its adaptability during constraint resolution.
refer to for more details.
as an illustration consider the string formula in fig.
.
gpt when prompted with cot se was able to not only safely minimize it but also accurately predict the corresponding mus i.e.
c1 c4 c6 .
notably it was able to a resolve constraints involving multiple variables c1and c6 b understand and analyze constraints involving intricate concat andstr.substr operations c3 c4andc5 .
we also tested the motivating example from fig.
with gpt using cot se.
interestingly we observed that gpt4 was able to a group all constraints about length s to combine them into a single constraint length s b reason about the transitive relationships between c1 c26 and c29 to recognize that c1andc26together would contradict c29 since at s at s and at s at s would resolve to at s at s which contradicts with its negation c29 .
these reasoning steps align with our hypotheses in section ii thus confirming the llms capabilities in reasoning based safe minimization.
ra .llms demonstrate superior macro reasoning capabilities to reason about the inter constraint inconsistencies and safely minimize infeasible constraint systems with a high accuracy thus exhibiting a potential to scale.
vii.
s afe minimization toward mus c omputation in this study we aim to explore the usefulness of suses predicted by s afeminin the application of computing muses with the reduction of search space via safe minimization.
a. data collection first we leverage cvc5 to compute the muses for all constraint sets in the test set in section v a. at its core cvc5 uses either the constructed proof or an assumptionbased approach for mus extraction.
due to its lightweight1not beginword endword 2not length beginword 3endword concat str.substr beginword concat t str.substr beginword length beginword 4endword concat str.substr beginword concat o str.substr beginword length beginword 5endword concat d str.substr beginword length beginword 6beginword dot 7length beginword 8length beginword 9length beginword fig.
.
string formula safely minimized by gpt with cot seprompting.
nature we opt for the latter.
accordingly for each instance in the test set we establish c s m tuples where each corresponds to the input constraint set candidate sus from safemin and the corresponding mus computed by cvc5 respectively.
note that we can extract multiple m s for a c. in this experiment we ensure that both candscorrespond to the same m. b. experimental setup baselines we compare all llms with the bestperforming cot seprompting for s afemin in section v as used for computing muses from the corresponding suses.
metrics while section v measures the reduction in search space of the input formula it does not measure this reduction relative to the size of the mus.
to this effect we define minimization ratio asmmus c s c m .
by definition mmusranges from to .
if mmus it means that explorer llm in s afeminremoved few or no constraints from c ands c. on the contrary if mmus it indicates that explorer llm removed most of the non conflict causing constraints from c i.e.
s m .
similar to rq1 we define two aggregated variants i.e.
mmus dandmmus du.
c. experimental results in table iv we report the minimization ratios for all llms with s afemin s cot seprompting strategy.
this illustrates the usefulness of the suses generated in section v c in the context of their corresponding muses.
overall we can see that among the safely minimized suses as noted in table ii claude .
sonnet records an average minimization ratio with respect to their muses by followed by gemini .
pro at gpt at and gpt .
at .
that is the suses generated by claude .
sonnet are on average only larger than the corresponding muses.
when also including the instances for which claude .
sonnet could not safely minimize to the sus here m the average is .
for gemini .
pro and gpt these aggregated minimization ratio variants are and respectively.
the breakdown in table ii on the basis of the number of constraints further sheds light on the quality of the suses.
for instance consider the string formulae containing constraints.
gpt .
safely minimizes .
of them as noted in table ii .
the average minimization in these formulae istable iv usefulness of sus es produced by safemin effectiveness of safe minimization toward mus computation .
evaluation metrics i.e.
mmus di constraints total approach dud dud dud dud dud dud cot sew gpt .
.
.
.
.
.
.
.
.
.
.
.
.
cot sew gpt .
.
.
.
.
.
.
.
.
.
.
.
cot sew gemini .
pro .
.
.
.
.
.
.
.
.
.
.
.
cot sew claude .
sonnet .
.
.
.
.
.
.
.
.
.
.
.
.
i.e.
the corresponding mus computation involved starting from suses containing only 31constraints.
in contrast gpt safely minimized .
of the formulae for an average minimization of .
gemini .
pro .
for an average minimization of and claude .
sonnet .
for an average minimization of .
thus with claude .
sonnet the mus computation for these formulae involved suses containing only 9constraints while the average size of these muses is .
.
in terms of the search space for muses this represents a reduction from o o with gpt3.
and o o with claude .
sonnet .
furthermore for string formulae we noticed that claude .
sonnet with cot seminimizes exactly to their muses i.e.
m .
in comparison gpt gemini .
pro and gpt .
do so only and times.
upon further inspection we observed that for80.
of the instances in the0 10constraint range claude .
sonnet safely minimizes exactly to the muses.
among the and 50constraint ranges cot seminimizes to the mus via macro reasoning for and instances respectively.
ra .safeminwith claude .
sonnet helps minimize infeasible string constraint systems to suses that are only larger than the corresponding muses reducing their search space from o o and localizing exactly to the muses in .
of the cases.
viii.
d iverse reasoning paths for parallelized partial enumeration of multiple mus es by design s afeminis able to explore diverse reasoning paths to generate multiple sus candidates via parallelized partial enumeration .
thus for a decoding sample size of k in the explorer llm it can generate o k suses.
in this experiment we use claude .
sonnet as the llm within safeminand assess its ability to identify multiple suses in parallel for a given constraint set which highlights its usefulness in producing non unique muses.
a. experimental setup let us use dto denote our dataset of string formulae.
the mus computation from an input formula by using an smt solver is deterministic and exhibits a one to one correspondence.
accordingly we compare the partial enumeration of muses by s afeminalong two dimensions and establish loose lower and upper bounds for the total number of unique muses produced by the smt solver as follows our first experiment setting helps establish the benefits of using self exploration in s afemintoward partial enumeration.
specifically for the lower bound we extract muses for each of the input string formulae c m from the smt solver as well as the candidate suses k generated by s afeminfor each formula i ksi m .
thus it represents the comparison between the sets j c m j and j k i si m ij where j .. d .
the second setting acts as a benchmark for s afemin.
specifically we mimic the partial enumeration of the muses from the original constraints cby using kunique random seeds within smt solver and collect the muses i kci m i .
the basis for selecting those kseeds is to make the comparison consistent with the kcandidates suses and establish a loose upper bound for all possible unique muses having a search space of o c .
thus it represents the comparison between j k i ci m ij and j k i si m ij where j .. d .
b. experimental results first we conducted an overlapping analysis of mus enumeration using the smt solver directly on the input formula i.e.
c m and by combining gpt in cot sewith the solver i.e.
s m .
of the computed muses we can see that are common to both approaches.
furthermore safeminhelps localize additional muses that were not captured by the solver directly and misses that were deterministically computed by the solver directly.
safeminalso computes non unique muses for .
of the instances resulting in it localizing .
more muses than when the input formulae were solved directly.
that is in .
of the instances it explored different reasoning paths to successfully identify non unique sources of inconsistencies.
in the second setting with multiple runs of the smt solver on the input formula we observed that it computes a total of distinct muses.
of these were also computed by safemin and were computed by s afeminbut not the solver.
overall s afemincomputes .
of the total muses in the benchmark.
thus our findings corroborate our design thatself exploration helps the llms exploit diverse reasoning paths facilitating a parallelized enumeration of muses.
ra .the suses from s afeminhelps smt solver capture non unique muses in .
of the instances thus computing .
more muses than the original solver without s afemin and .
of the total ones.
public static byte opaques byte x byte y byte z if byte x y x y byte x y x y ... block1 else if byte x y x y x y 0x17 ... block2 else if byte x y x y x y x y x y x y x y x y x y ... block3 else ... block4 return z fig.
.
a case study on detecting infeasible paths in source code.
lines highlighted in green andredindicate feasible and infeasible paths respectively.
ix.
d etection of infeasible paths in source code in this section we illustrate via a case study an application of s afeminindetecting infeasible paths in source code .
we selected a c code snippet from a website containing source code with infeasible paths .
we converted the code into java .
the feasible paths are highlighted in green and infeasible paths in red.
after converting the paths into the smt lib format we used cvc5 to test the path constraints.
the constraint system comprises three constraints two of which as in lines and line contain conflicts and are unsatisfiable resulting in two muses.
here we extracted all possible path constraints and input them to s afemin.
it successfully minimized the constraint system and accurately identified the unsat causing constraints directly returning the muses and thereby identifying the infeasible paths .
this case study illustrates a useful scenario of s afemin in identifying the infeasible path that could help developers in fixing the issue of unreachable code.
it is able to safely minimize directly to a mus providing an explanation for developers in understanding the reason for an unfeasible path by pointing out the mus with a minimum number of constraints.
x. r elated work while not formally defined the notion of safe minimization occurs in various software engineering tasks including conditioned slicing constraint slicing symbolic execution .
however they do not leverage any contextual information.
there has been research along two dimensions to further optimize these processes.
the first dimension includes efforts to incorporate heuristicbased conflict analysis strategies in smt solvers .
to this end lagniez et al.
proposed factoring out assumptions eagerly while belov et al trimmed input formulae based on proofs.
marques silva et al.
proposed a tree search based algorithm that constructs a solution througha combination of backtracking and learning new clauses from conflicts.
these strategies are still limited and can not scale.
the next dimension includes the incorporation of learningbased components in cdcl solvers.
this includes learning to predict conflict causing variables to guide branch selection .
wang et al.
focus on the bottlenecks in clause deletion and predict what value a variable should have.
the closest to our work is satformer where shi et al.
try to directly predict the conflict causing clauses.
nonetheless these are trained on specific theories and can not generalize thus limiting their extension to complex smt theories.
in this work we propose a fundamentally different approach aiming to leverage the llm s macro reasoning capabilities.
we are encouraged by recent research on automated reasoning with llms where they have shown promising results and a potential to achieve scale.
xi.
t hreats to validity first we only evaluated s afeminon the leetcode benchmarks in string theory.
this choice is grounded in the inherent complexity of string constraints and the particular relevance of these formulae to software engineering due to being sourced from real world coding challenges.
besides they involve both string operations e.g.
concatenation substring and linear integer arithmetic operations e.g.
length based computations highlighting s afemin s potential applicability to other domains beyond string theory the results for which might be different.
second we used cvc5 as the smt solver in our experiments.
thus the results for section vii may vary with other smt solvers.
however our sampleand enumerate framework involving an llm and smt based verifier is general and can be extended to all smt solvers.
third while we tested our approach on multiple llms the results for others might vary and requires further investigation.
fourth the manual investigation in section vi might have human errors.
nonetheless we employed multiple evaluators to reach a consensus.
finally we showed s afemin s usefulness in mus computation and detecting infeasible paths.
however our approach to safe minimization could also motivate other tasks including constraint based program slicing testing etc.
xii.
c onclusion this study introduces s afemin which leverages llms to safely minimize formulae by employing macro reasoning on constraint sub groups.
by exploring various reasoning paths through a sample and enumerate strategy s afemin generates multiple candidate subsets of the constraints while retaining conflict causing ones.
when applied to searching non unique muses it brings substantial advantages reducing search space in .
of instances by an average reduction of and capturing .
more muses than traditional solvers.
we showed its usefulness in infeasible path detection.