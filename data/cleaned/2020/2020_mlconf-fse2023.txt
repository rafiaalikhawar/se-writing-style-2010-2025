can machine learning pipelines be better configured?
yibo wang northeastern university shenyang china yibowangcz outlook .comying wang northeastern university and hkust shenyang hong kong china wangying swc .neu.edu.cntingwei zhang northeastern university shenyang china qq .com yue yu national university of defense technology changsha china yuyue nudt .edu.cnshing chi cheung the hong kong university of science and technology hong kong china scc cse.ust.hkhai yu northeastern university shenyang china yuhai mail .neu.edu.cn zhiliang zhu national frontiers science center for industrial intelligence and systems optimization and key laboratory of data analytics and optimization for smart industry northeastern university shenyang china zhuzhiliang neu .com abstract amachine learning ml pipeline configures the workflow of a learning task using the apis provided by ml libraries.
however a pipeline s performance can vary significantly across different configurations of ml library versions.
misconfigured pipelines can result in inferior performance such as inefficient executions numeric errors and even crashes .
a pipeline is subject to misconfiguration if it exhibits significantly inconsistent performance upon changes in the versions of its configured libraries or the combination of these libraries.
we refer to such performance inconsistency as a pipeline configuration plc issue .
a systematic understanding of plc issues helps configure effective ml pipelines and identify misconfigured ones.
to this end we conduct the first empirical study of plc issues pervasiveness impact and root causes.
to facilitate scalable in depth analysis we develop piecer an infrastructure that automatically generates a set of pipeline variants by varying different version combinations of ml libraries and detects their performance inconsistencies.
we apply piecer to the pipelines that can be deployed out of the ml pipelines collected from multiple ml competitions atkaggle platform.
the empirical study results show that .
of the pipelines manifest significant performance inconsistencies on at least one variant.
we find that and pipelines can achieve better competition scores execution time and memory usage respectively by adopting a different configuration.
yibo wang and ying wang made equal contributions to this work.
ying wang is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
.org .
.3616352based on our findings we construct a repository containing defective apis and api combinations from library versions.
the defective api repository facilitates future studies of automated detection techniques for plc issues.
leveraging the repository we captured plc issues in real world ml pipelines.
ccs concepts software and its engineering software libraries and repositories .
keywords machine learning libraries empirical study acm reference format yibo wang ying wang tingwei zhang yue yu shing chi cheung hai yu and zhiliang zhu.
.
can machine learning pipelines be better configured?.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
.org .
.
introduction machine learning ml libraries e.g.
tensorflow and pytorch have gained much attention in both academia and industry which are used in a wide range of domains including natural language processing image processing autonomous driving etc .
these libraries provide off the shelf ml solutions optimized numerical computations efficient data structures visualization and other handy utilities to facilitate application development.
in practice the calls to ml library apis are often organized by means of an ml pipeline which automates the workflow of an ml task in nine stages as shown in figure .
to timely incorporate support for new hardware algorithms and bug fixes ml libraries keep evolving rapidly.
however different combinations of ml library versions can cause inconsistencies in pipelines performances crashes and nan bugs as demonstrated by the real life example 463esec fse december san francisco ca usa yibo wang ying wang tingwei zhang yue yu shing chi cheung hai yu and zhiliang zhu in section .
.
in this paper we refer to the performance inconsistencies of ml pipelines induced by varying ml library versions as pipeline c onfiguration plc issues .
despite the widespread use of ml pipelines there have been no prior systematic studies on plc issues.
we have little knowledge of plc issues such as their pervasiveness in real world ml pipelines their impact on ml tasks performances and their root causes.
the deficient knowledge can hinder effective deployment of ml pipelines in various ways ml library vendors lack awareness of the need to test the interactions between specific library versions that could be concurrently deployed by a pipeline.
ml pipeline developers lack advice on good and bad practices in ml pipeline configuration.
tool builders lack scientific criteria to recommend appropriate versions of ml libraries to pipeline developers when maintaining and evolving ml libraries.
to fill the knowledge gap we conduct a large scale empirical study to understand the status quo of plc issues.
to facilitate scalable analysis we develop piecer which automatically generates a set of pipeline variants by varying version combinations of ml libraries and compares three aspects of performance inconsistencies execution time memory usage and prediction performance precision andrecall .
to understand the status quo of plc issues we leverage piecer to analyze ml pipelines from various competition categories on kaggle platform.
the pipelines utilize popular ml libraries.
we investigate the pervasiveness and severity of plc issues in ml pipelines and analyze their root causes of inducing performance inconsistencies.
the main empirical findings are we identify library combinations commonly adopted by the ml pipelines in our dataset.
among the ml pipelines that we are able to deploy .
manifest significant performance inconsistencies on at least one variant.
we find that and pipelines among the deployable pipelines could achieve higher competition scores shorter execution time and lower memory usage respectively using a different configuration of the library versions available at the time of competition.
by analyzing the historical evolution data of defective apis that induce plc issues we identify four types of issue root causes.
specifically plc issues exposed in defective apis are affected by one library version.
plc issues in pipeline variants are caused by combinations of apis from different library versions.
in summary this paper makes four major contributions an infrastructure for analyzing ml library version impacts.
we developed an infrastructure piecer to automatically generate pipeline variants with different ml library version combinations and analyze their performance inconsistencies.
an empirical study on the impacts of various ml library version combinations.
leveraging piecer we conduct the first empirical study to systematically explore the impacts on different ml library version combinations and categorize the root causes of inducing performance inconsistencies.
a repository of defective apis for detecting plc issues.
based on our empirical findings we constructed a repository containing defective apis and api combinations from ml library versions which shed light on designing automated approaches to detecting plc issues.
based on the repository we captured plc issues in real world ml pipelines.
stages of ml pipelines model requirements mr istheprocess ofidentifying theappropriate mlmodels andsuitable features data collection dco istheprocess ofgathering andmeasuring information from countless different sources data cleaning dcl refers toidentifying andcorrecting errors inthedataset thatmaynegatively impact amodel data labeling dl istheprocess ofassigning ground truth labels toeach datarecord feature engineering fe helps inpreparing transforming andextracting features from rawdatatoprovide thebestinputs toamlmodel model training mt isaprocess inwhich amachine learning algorithm isfed withtraining datafrom which itcanlearn model evaluation me istheprocess ofusing different evaluation metrics to understand amachine learning model s performance model deployment md istheprocess ofdeploying amachine learning model ina liveenvironment model monitoring mm refers tothe process ofclosely tracking the performance ofmachine learning models inproduction machine learning workflows are highly non linear and contain several feedback loops.
figure description of the stages in ml pipeline preliminaries .
background ml libraries machine learning comprises several kinds of learning algorithms such as supervised learning unsupervised learning deep learning and reinforcement learning .
to provide fast access to these algorithms and facilitate the complicated data analysis process the open source community developed two types of ml libraries ml frameworks anddata science libraries .ml frameworks e.g.
scikit learn and pytorch provide ready made algorithms and allow developers to easily apply ml based solutions to applications.
data science libraries provide data visualization e.g.
matplotlib efficient data structures e.g.
pandas scientific computing functions e.g.
numpy or other utilities.
ml pipelines the term pipeline corresponds to the pipes andfilter design pattern that decomposes the software architecture into several stages with processing units filters and ordered connections pipes .
by the ml pipeline we are referring to a nine stage pipeline that goes through data oriented collection cleaning and labeling and model oriented model requirements feature engineering training evaluation deployment and monitoring stages.
figure explains each ml stage.
according to the investigation results in studies most performance issues or crashes occurred in the dco dcl fe mt and mestages of ml pipelines.
the five stages are commonly implemented based on various readymade algorithms encapsulated in ml libraries.
.
motivation since ml libraries rapidly evolve to add remove change features and fix bugs various version combinations may dramatically affect pipelines performance.
figure shows an illustrative example of a plc issue which participates in the competition mnist hosted onkaggle platform.
this pipeline aims to identify digits from a dataset of tens of thousands of handwritten images leveraging twotensorflow apis and eight keras apis to implement dcl and mtstages respectively.
interestingly when using different version combinations of tensorflow andkeras the categorization accuracy of the pipeline varies from .
to .
leading to a rise in competition ranking from thto 1st.
even worse some version combinations cause crashes due to api breaking changes.
464can machine learning pipelines be better configured?
esec fse december san francisco ca usa keras tensorflow versionsscore auc rankingtime ms memory mb .
.
.
.
crash crash crash crash .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
crash crash crash crash .
.
.
.
crash crash crash crash .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
crash crash crash crash .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure an illustrative example of a plc issue similar situations can be found in many issue reports.
in the plc issue of project sklearn a developer complained that the execution time of api sklearn .cluster.kmeans in their ml pipeline varies significantly when using different versions of numpy .
this issue attracted comments from experienced developers to diagnose the root causes and was linked to four similar performance bugs induced by ml library versions i.e.
scipy sklearn scipy and sklearn .
the above examples motivate us to conduct a large scale empirical study to investigate the impacts of different combinations of ml library versions on ml pipelines performances which can shed light on approaches for detecting or repairing plc issues.
empirical study our study aims to answer the following three research questions rq1 common ml library combinations what combinations of ml libraries do developers commonly use?
to answer rq1 we collect published ml pipelines and analyze their library usage to identify common library combinations.
rq2 impacts of different ml library version combinations do different version combinations of ml libraries affect pipelines performances?
to answer rq2 for each ml pipeline we generate a series of variants with different ml library version combinations to inspect their performance inconsistencies.
in particular we define the generation rules of ml library version combinations to systematically explore their impacts on pipelines performances.
rq3 root causes of performance inconsistencies what are the root causes of pipelines performance inconsistencies when adopting different version combinations of ml libraries?
to answer rq3 we consider the pipeline variants that induce significant performance inconsistencies crashes and nan bugs as subjects to analyze the corresponding root causes and triggering conditions.
.
piecer to scale up the impact analysis of possible ml library version combinations on pipelines performances we develop an infrastructure piecer pipelineconfiguration explo re to automate such a process.
figure shows an overall architecture of piecer .
it mainly consists of three components component constructing ml library version pool.
for a given ml pipeline piecer constructs a dependency pool dp vlib vlib vlibn to collect all the installable version candidatesvlibi v1 v2 vm of referenced ml libraries libi.
to achieve this it first parses the pipeline s dependency management script requirement.txt setup.py ormetadata to identify all the versions of direct dependencies that satisfy the specified version constraints.
for each version of direct dependency it iteratively collects the version constraints of the required transitive dependencies.
if a librarylibicorresponds to multiple version constraints piecer considers their intersection as installable version candidates vlibi.
in the cases where a library libiis specified with incompatible version constraints we remove the versions of direct dependencies that induce such dependency conflicts from dp.
component generating pipeline variants with different ml library version combinations.
to explore the impacts of library versions on a pipeline s performance piecer generates a set of pipeline variants with different version combinations of ml libraries selected from our dependency pool dp.
for a given ml pipeline it considers the newest versions of referenced ml libraries as a version combination baseline.
by defining a series of comparison rules see definitions in section .
piecer generates a set of ml library version combinations by varying the versions of concerned libraries and keeping the remaining library versions be consistent with the baseline.
according to the dependency resolution rules of python build tool pip it installs the newest library versions from pypi central repository that satisfies the corresponding version constraint.
to enable the installation of an expected version combination of ml libraries piecer customizes the dependency resolution rules of pip to select our specified library version candidates satisfying the corresponding version constraints from our dependency pool dp.
component analyzing performance inconsistencies.
for each ml pipeline piecer runs the baseline and all the variants to check their performance inconsistencies across various library version combinations.
it focuses on three aspects of performance inconsistencies execution time memory usage and evaluation metrics e.g.
precision andrecall .
specifically piecer performs two tasks measuring performance inconsistencies for each library api referenced by the pipeline it leverages measurement functions to capture the performance inconsistencies between the variant and baseline.
for example piecer adopts functions timeit .default timer tracemalloc .get traced memory andpynvml .nvml deviceget memoryinfo to measure the execution time and cpu and gpu memory usage of the changed apis respectively.
the evaluation metrics can be implemented with the aid of functions sklearn .metrics.accuracy score andsklearn .metrics.recall score etc.
for the pipelines involving deep learning models piecer retrains the model to capture the impacts of library version changes precisely.
to reduce the interference of random factors it runs each pipeline five times and averages the outcomes.
recording the problematic library version combinations compared with the baseline piecer records the library version combinations with the concerned apis that induce significant performance inconsistencies of pipelines see definitions in section .
cause program crashes or nan bugs for further analysis.
.
data collection we collected pipelines from kaggle the most popular crowdsourced platform for ml competitions.
we select kaggle pipelines 465esec fse december san francisco ca usa yibo wang ying wang tingwei zhang yue yu shing chi cheung hai yu and zhiliang zhu component constructing ml library version poolinputcomponent generating pipeline variantscomponent analyzing performance changes ml pipelinespipeline sklearn pandas numpy scipy numpy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
library ml library version pool pandas sklearn scipy .
.
.
.
.
.
.
.
.
.
.
.
numpy identifying the installable version candidates baseline variant variant variant n ...generating pipeline variants with different library version combinations defining comparison rules customizing pip s dependency resolving rules combination ...pipcombination output baseline defective library version combinationswith concerned apisvariant variant variant n...performance changes memory execution time evaluation metrics crashes nan bugs the newest versions of ml librariesapplication of empirical findings repository of defective apis api 1from lib1 api 2from lib2 lib version .
lib2 version .
lower accuracy repository of defective apis sheds light on designing automated approaches to detecting plc issues figure the overall architecture of piecer repository of defective apis is described in section as subjects to conduct our empirical study for four reasons all the ml pipelines in a competition correspond to the unified evaluation metrics for ranking which enable us to compare the outcomes across different combinations of library versions each pipeline provides its corresponding source code and dataset which facilitate our reproduction competitions in kaggle platform cover a wide range of domains and techniques which guarantee the diversity of ml tasks the small size pipelines on kaggle have few interference factors e.g.
environmental dependencies which enables our analysis for the root causes of plc issues.
we did not select open source large scale ml pipelines as subjects because they rarely provide detailed training steps in documentation complete datasets and all the required dependencies.
therefore we face challenges on deploying the complex pipelines.
however the common ml library version combinations identified in kaggle pipelines are also adopted by large scale ml pipelines see statistics in section .
as such the empirical findings identified in kaggle pipelines can scale to more complex ml based projects.
we collected the ml pipelines from kaggle competitions that satisfy three criteria.
first the competitions were hosted in the last three years from jan to jul .
the search returned competitions that involve pipelines.
second we only kept the competitions that contain more than competing pipelines i.e.
popularity .
third the pipelines should depend on more than three ml libraries including both ml frameworks and data science libraries i.e.
complexity .
with the above process we finally obtained pipelines from competitions.
in total our collected pipelines depend on ml libraries including ml frameworks and data science libraries.
among them the top five most frequently used libraries are pandas sklearn numpy tensorflow and pytorch .
figure shows the demographics of the ml libraries.
on average each ml library has downstream projects monthly downloads stars and issues and commits in their github repositories.
according to a ranked list of awesome ml libraries provided by a popular github project updated weekly the libraries cover categories including image video processing etc.
.
the above statistics demonstrate that the ml libraries used in kaggle pipelines are also widely used in large amounts of open source ml pipelines indicating their representativeness.
.
rq1 common ml library combinations study methodology.
a combination of ml libraries are typically used in certain stages of ml pipeline workflows .
to granularly identify common library combinations we analyze the library usage in pipeline workflow stages.
specifically we perform two tasks in a the ml libraries used by collected pipelines b demographics of the ml libraries libraries in pipelines downstream projects downloads stars issues commits figure description of the stages in ml pipeline table occurrences of ml library combinations in the pipelines occurrences dco dcl femt medata oriented stagesmodel oriented stages avg common data oriented stages dco dcl fe model oriented stages mt me.
avg denotes the average occurrences of ml library combinations in each stage common denotes the number of common library combinations we consider a library combination is commonly adopted by ml pipelines if its occurrences in each stage is above avg.
the investigation leveraging jedi a static analysis tool for python we located unique ml library apis invoked by our collected pipelines.
two authors of this paper assigned label of workflow stages for each ml library api.
the labels were assigned based on the code comments in ml pipelines for stating the api usage or the description of the package or sub package in which the api was declared.
for example we considered api xgboost .score worked in the mestage since its declaration was accompanied by a explanatory code comment evaluate model performance in the ml pipeline.
we did not assign labels for the apis that i did not have a description in its package or ii the two authors could not reach a consensus on the descriptions.
we employed cohen s kappa to measure the consistencies between the two authors.
if the kappa value was less than .
the authors needed to discuss to resolve disagreements.
this iterative process would stop once the kappa value was equal to or greater than .
indicating substantial agreement.
eventually we obtained 466can machine learning pipelines be better configured?
esec fse december san francisco ca usa table statistics of top common ml library combinations in single stage data co llection data cl eaning feature e ngineering model t raining model e valuationrankcombinations occurrences combinations occurrences combinations occurrences combinations occurrences combinations occurrences 1pandas numpy sklearn sklearn sklearn 2numpy pandas pytorch807 pytorch numpy tensorflow tensorflow 3pandas pytorch nltk keras pytorch sklearn tensorflow 4nltk pandas tensorflow tensorflow keras keras sklearn 5opencv numpy pandas pytorch487 pandas pandas lightgbm 519pytorch lightning sklearn11 the top common ml library combinations in multi stages are provided on our website labeled ml library apis.
the labelling process took two months for the two authors who have over two years ml development experience to analyze and perform cross validation.
we released the dataset on the website for public scrutiny.
results.
by inspecting the labeled ml library apis we observed that the pipelines from kaggle platform only involved five stages of workflows dco dcl fe mt me.
as aforementioned recent empirical studies revealed that the majority of performance issues or crashes occurred in the five stages.
in our study we focus on these core stages of pipeline workflows to identify their common combinations of library usage.
table summarizes the occurrences of ml library combinations in the single stage multi stages of pipeline workflows.
we identified and library combinations in the dco dcl and me stages respectively.
since the open source community provides various solutions to help developers extract critical features from raw data and automatically train their ml models and library combinations were identified in the feandmtstages of our collected pipelines.
our results reveal that and of library combinations were commonly adopted by the majority of ml pipelines to implement dco dcl fe mt andmestages respectively occurrences avg .
the combinations may include both ml frameworks and data science libraries.
table illustrates the top common library combinations in single stage.
for example pipelines use pandas indco stage while pipelines combines numpy pandas with pytorch for data collection.
as shown in table during the synthesis of the data oriented stages dco dcl and fe we identified the use of library combinations.
out of the combinations occurred in more than five pipelines.
we also identified ways of implementing model oriented stages mtandme using combinations of libraries.
of them were commonly used in our collected pipelines.
the top common ml library combinations used in multi stages are provided on our website.
we observed that pipelines adopts a combination of pandas numpy and sklearn for data oriented stages.
pipelines combine tensorflow with sklearn for modeling oriented stages.
answer to rq1 we identified and common ml library combinations in the dco dcl fe mt and me stages respectively.
in the data oriented stages dco dcl and fe and model oriented stages mt and me we identified and common ml library combinations respectively.
such common ml library combinations are adopted by the majority of ml pipelines.
.
rq2 impacts of different ml library version combinations study methodology.
we present the study methodology of rq2 in three aspects subject selection generation rules of library version combinations andexperiment setup .subject selection.
since each pipeline corresponds to a series of variants the comparison experiment is time consuming and would take lots of machine resources.
to guarantee the feasibility of our rq2 study we set three filter conditions to select subjects from our collected pipelines pipelines that do not use common ml library combinations in the single or multi stages are filtered out.
pipelines that involve the private datasets not published onkaggle are filtered out.
pipelines that use the private libraries could not be found from pypi or github repositories are filtered out.
we restrict the dataset size of pipelines to 100gb.
pipelines are filtered out according to this condition.
eventually we obtained pipelines involving ml frameworks and data science libraries.
the ml libraries cover .
of popular ones we presented in figure .
generation rules of library version combinations.
we deployed component ofpiecer to take each pipeline as an input to construct a dependency pool dp vlib vlib vlibn by collecting all the installable version candidates of the referenced ml libraries.
to reduce the computation complexity in our study we only considered popular versions of each ml library in dpthat are widely used by downstream users the popularity of library versions are provided on libraries.io website .
leveraging component ofpiecer we generate a set of pipeline variants with different ml library version combinations.
to this end we define two generation rules of ml library version combinations to systematically explore their impacts on pipelines performances rule varying version combinations of ml libraries in the singlestage for a given ml pipeline piecer iteratively varies the version combinations of ml libraries used in each workflow stage dco dcl fe mtandme while keeps the remaining libraries to be the newest versions in our dependency pool dp.
letl lib1 lib lib m be a ml library combination direct dependencies used in a certain workflow stage.
each librarylibi lcorresponds to a set of version candidates vlibi v1 v2 vt in our dependency pool dp.piecer determines a set of version combinations of ml libraries used in a certain workflow stage as lv vlib vlib vlibm where libi land the symbol denotes cartesian product .
rule varying version combinations of ml libraries in the multistages piecer iteratively varies the version combinations of ml libraries used in data oriented stages dco dcl and fe and model oriented stages mtandme while keeps the remaining libraries to be the newest versions in our dependency pool dp.
letlv dco lv dcl lv fe lv mt andlv me be the sets of installable version combinations of ml libraries used indco dcl fe mtand meworkflow stages respectively 467esec fse december san francisco ca usa yibo wang ying wang tingwei zhang yue yu shing chi cheung hai yu and zhiliang zhu table statistics of pipeline variants generated by piecer variants generated by rule variants generated by rule pipeline variantdco dcl fe mt medata orientedmodel orientedoverall architecture we filtered out the overlapping pipeline variants generated by rules and generated based on rule .piecer determines the ml library version combinations in the multi stages as follows dataoriented stages lv data lv dco lv dcl lv fe .
model oriented stages lv model lv mt lv me .
overall architecture lv overall lv data lv model .
table shows the statistics of pipeline variants.
in total piecer generated variants for the pipelines based on the generation rules of library version combinations.
on average each pipeline corresponds to variants.
experiment setup.
for a ml pipeline piecer considers the newest versions of referenced ml libraries as a version combination baseline.
using component we run the baseline and all its corresponding variants to capture the performance inconsistencies.
based on confidence interval analysis we consider the library version combinations induce performance inconsistencies if metric b metric v wheremetric bandmetric vdenote the execution time memory usage orcompetition scores of baseline and a pipeline variant respectively xandsare the average value and standard deviation ofmetrics v respectively ndenotes the total number of pipeline variants.
according to the parameter setting and ztable provided in approach we calculate the .
confidence interval of the mean and the zvalue is taken as .
.
note that competition scores e.g.
precision andrecall are the metrics provided by kaggle for evaluating the pipelines outcomes.
we use cohen sd to measure how far the performance inconsistent outcomes exceed the confidence interval.
according to formula we divide the outcomes into two groups performance inconsistency group and normal group.
cohen sdis used to describe the standardized mean difference between two group means.
d x1 x2 n1 s2 n2 s2 n1 n2 in this formula x1and x2are the mean values of performance inconsistency group and normal group respectively niis the sample size i.e.
number of outcomes in each group and siis the sample variance of each group.
according to approach cohen sdinterprets effect sizes in the meta analysis small effect .
.
medium effect .
.
large effect .
and higher.
in our study we consider the outcomes with large effect size as the significant performance inconsistencies induced by pipeline variants.
our study is conducted on three machines with the same configuration intel xeon silver machine with 128gb ram ubuntu .
.
lts and two nvidia tesla v100 gpus.
the comparison experiments took about eight months for three authors of this paper to deploy and run the variants of pipelines.
results.
among the pipelines .
manifest significant performance inconsistencies on at least one variant.
and variants involving pipelines induce program crashes andnan bugs respectively.
figure and table summarize the performance inconsistencies of the pipelines.
we can observe that competition score execution time memory usage pdiff pdiff pdiff pdiff pdiff pdiff pdiff pdiff pdiff pdiff pipelines pipelinesbaseline achieves better performancevariants achieve better performance 900pdiff 900ppdddddddd mmmmmmmmmmmm bb mmmmmmmmmmmm vv mmmmmmmmmmmm bbfigure performance inconsistencies baseline v.s.
variants table performance inconsistencies baseline v.s.
variants competition score execution time memory usage pipeline variants denotes that at least one pipeline variant achieves better worse performance than the baseline denotes unchanged performances.
table impacts of library versions on competition ranking pipeline variants that affect competition rankings competitionsrise fall r r r r r r r r r r 0competition 0competition 0competition 0competition 0competition 0competition 0competition 0competition 0competition 0competition r rank b rank v rank b where rank band rank vare competition rankings of the baseline and variant respectively.
for and pipelines changing the version combinations of ml libraries from the baselines i.e.
the newest versions could achieve better performances in competition scores execution time and memory usage respectively.
the variants of 468can machine learning pipelines be better configured?
esec fse december san francisco ca usa table statistics of the ml libraries whose varied versions significantly affect pipelines performances problematic apismetrics type ml libraries pipelinesdco dcl fe mt me baseline betterpandas opencv python nltk numpy torchvision autokeras catboost sklearn keras tensorflow competition score baseline worsepandas transformers opencv python numpy sklearn keras tensorflow baseline betterscipy sklearn opencv python nltk category encoders numpy pandas gensim transformers catboost xgboost tensorflow lightgbm keras spacy execution time baseline worseautokeras torchvision numpy opencv python catboost nltk pandas keras tensorflow sklearn baseline betteroptuna spacy catboost transformers numpy nltk pytorch lightgbm pandas keras xgboost sklearn tensorflow memory usage baseline worse keras numpy xgboost pandas sklearn pipeline denotes the number of pipelines that adopt the ml libraries.
pipeline manifested over of performance differences compared with the baselines.
for example cfec adopts the apis of library sklearn sklearn .neighbors.kneighborsclassifier andsklearn .naive bayes.gaussiannb inmtworkflow stage.
combining older version sklearn .
.
with other ml libraries newest versions the pipeline achieves significant improvement inexecution time andmemory usage compared with the baseline .6s .8mb v.s.
.2s .9mb .
for and pipelines the baselines outperform all the other ml library version combinations in competition scores execution time and memory usage respectively.
in pipelines the newest versions of ml libraries manifested over of performance differences compared with the average outcomes of variants.
for example tps uses the api of library catboost catboost .catboostregressor inmtworkflow stage.
the baseline significantly outperforms the other pipeline variants in execution time .0s v.s.
.6s .
as shown in table compared with baselines the variants of pipelines achieved gain on certain performance metrics with a drop of other ones.
the variants of and pipelines manifest an overall rise and fall in performance metrics respectively.
.....the.........results ...........indicate ......that..a..............significant ...............proportion ...of ................performance ...........changes .....are.....the........issues ............deserved ..........further ..............investiga .......tions .........rather ......than.............trade offs ............between ...........different ....................measurements.
in general ml pipelines with different library combinations more easily induce significant inconsistencies of execution time and memory usage .
.
of pipelines still manifest the inconsistencies of competition scores on the variants.
table illustrates ten competitions whose rankings determined by competition scores can be affected by changing the ml library versions in pipelines.
interestingly we observed that the rankings in competitions and even changed dramatically on the pipeline variants.
for example among the variants generated by piecer for the pipelines in competitions and variants can cause the rise and fall in ranking respectively.
in particular mnist moved up from thto 5thplace in the ranking after changing the versions of libraries keras .
.
and tensorflow .
.
to keras .
.
and tensorflow .
.
respectively.
piecer exposed plc issues in ml libraries involving apis.
table shows statistics of the ml libraries whose versions significantly affect pipelines performances.
we can observe that the defective ml libraries include both ml frameworks and data science libraries.
for each ml library a proportion of apis perform best in the newest versions while the other apis achieve the optimal performances in their old versions.
for example tensorflow provides and apis whose newest versions negatively affect the pipelines competition scores execution timeandmemory usage respectively.
besides the three metrics reward the apis of tensorflow in the newest versions.
among the five workflow stages mtinvolves the most library apis apis from libraries that can induce plc issues into ml pipelines.
answer to rq2 among the deployable ml pipelines .
manifest significant performance inconsistencies on at least one variant.
for and pipelines changing the version combinations of ml libraries from the baselines i.e.
the newest versions could achieve better performances in competition scores execution time and memory usage respectively.
for and pipelines the baselines outperform all the other ml library version combinations in competition scores execution time and memory usage respectively.
implication due to the version constraints between dependencies or the limitations of the installation environment various library version combinations are likely to be adopted in ml pipelines.
understanding their impacts on pipeline performances helps developers avoid plc issues.
.
rq3 root causes of performance inconsistencies study methodology.
to further analyze the root causes of plc issues we randomly sampled a out of the pipelines that induced significant performance inconsistencies after varying library versions b out of the pipelines causing crashes and c out of the pipelines inducing nan bugs.
this sampling approach guarantees that the root causes distilled from the sampled pipelines can be generalized to the whole dataset with a confidence level and a confidence interval.
specifically we performed four tasks as follows.
for each pipeline variant we identified a collection of ml library apisa a1 a2 an that induced plc issues using component ofpiecer .
leta1.libv be the ml library version that contains api ai a. in total we obtained apis from library versions in the pipeline variants.
we performed an in depth analysis on defective apis by inspecting their historical evolution data across the ml library versions including release notes issues code commits and code comments on github repositories .
to understand whether the plc issue of an apiai awas affected by a combination of library versions we divided the apis into two groups group we consider a plc issue is induced by the individual library if it satisfies the issue is exposed in api ai a among the library versions used by the pipeline variant only ai.libv is inconsistent with that in baseline.
apisai ainducing plc issues in pipeline variants fall into group .
we considered the plc issues were induced by the evolution of library ai.libv and focused on its historical data for inspection.
469esec fse december san francisco ca usa yibo wang ying wang tingwei zhang yue yu shing chi cheung hai yu and zhiliang zhu pipeline variant pipeline variant ... api with plc issues library version is inconsistent with baseline ...orderly connected by a pipeline t ype a indirect dependency interferencet ype b data flow interference... figure usage patterns of defective library version combinations group as shown in figure we divided the plc issues induced by combinations of library versions into two types type a. suppose a pipeline variant directly uses apis aiand ajfrom different ml libraries and aitransitively invokes the apis from library aj.libv .
we consider a plc issue is induced byindirect dependency interference if it satisfies the issue is exposed in api ai a among the library versions used by the pipeline variant only aj.libv is inconsistent with that in baseline.
we then consider ai.libv aj.libv is a defective library version combination.
for the defective apis in pipeline variants falling into type a we inspected their historical evolution data.
type b. type b. suppose apis aj ak auandaifrom different libraries are orderly connected by a pipeline and the outcomes of apis aj ak aucan affect the inputs of ai.
we consider a plc issue is induced by data flow interference if it satisfies the issue is exposed in api ai a among the library versions used by the pipeline variant only aj.libv is inconsistent with that in baseline aidoes not invoke apis fromaj.libv .
we then consider aj.libv ak.libv au.libv ai.libv is a defective library version combination.
we inspected the historical evolution data of defective api ajand analyzed the impacts of defective library version combination.
we followed an open coding procedure a widely used approach for qualitative research to distill and categorize the root causes of performance inconsistencies.
initially two authors of this paper who had over two years ml development experience independently analyzed the release notes issues code commits and code comments of defective apis across the library versions in variant and baseline.
after the first round of analysis and labeling the two authors gathered to compare and discuss their results in order to adjust the taxonomy with the help of a third author to resolve conflicts.
in this manner we constructed the pilot taxonomy.
this led to a more clear cut labeling strategy.
next the first two authors continued to label the root causes of the remaining apis performance inconsistencies and iteratively refined the results.
the conflicts of labeling were further discussed during meetings and resolved by the third author.
we adjusted the pilot taxonomy and obtained the final results.
for each defective api we identified all the pipeline variants that use it from the same library version to further inspect whether they have similar performance inconsistencies.
such inspections helped us understand the triggering conditions of plc issues.
results.
for the defective apis we identified the root causes of plc issues by analyzing related library release notes issues andcode commits.
we identified four common root causes that can arise in two scenarios performance impacts induced by individual library andinduced by library version combinations .
performance impacts induced by individual ml library.
plc issues of defective apis were affected by one library version.
the issues share four common root causes below.
api optimization .
.
for defective apis we found descriptions in the historical evolution data for declaring the optimization for hardware support data processing and calculations etc.
for example in nlpdt plyger the memory usage of api transformers .trainer is changed from 2mb to 91mb when downgrading library transformers to .
.
or lower versions.
in commit log 7a0adbb developers stated that they added support for gradient checkpointing in bert since transformers .
.
for optimizing memory performance.
the api optimization results in the improvement of execution time memory usage and competition scores .
on average the library versions containing non optimized apis still have monthly downloads in pypi.
default hyperparameter changes .
.
for defective apis developers added deleted or changed the values of default hyperparameters for training ml models leading to performance inconsistencies across different library versions.
the above clues can be located in the historical evolution data of libraries.
for example the release note of library sklearn .
.
states that developers change a default hyperparameter gamma in api sklearn .svm.svc .
we observed that the accuracy of mnist rose from .
to .
after upgrading sklearn to .
.
or higher versions.
the symptoms of changing default hyperparameters are manifested as the improvement of execution time memory usage competition scores or cause nan bugs .
for each api with changed default hyperparameters all the pipeline variants using it could reproduce the symptoms of performance changes in the cases that developers did not customize the arguments.
however such library versions with poor performances still have monthly downloads on average.
technical debts .
for defective apis we found declarations in their code comments stating that there are technical debts in the old library versions.
developers either considered them as experimental apis or incomplete implementations.
for example in the code comments of api sklearn .ensemble.histgradientboostingclassifier developers explained that they improved this experimental api since sklearn .
.
.
we observed that the memory usage of cfecii reduced from 105mb to 55mb after upgrading sklearn to .
.
or higher versions.
the symptoms of technical debts are manifested in the bad performance of execution time memory usage or competition scores .
by inspection for each api with technical debts all the pipeline variants adopting it could manifest the similar performance changes.
this means that such plc issues can be easily triggered if any ml pipeline uses our identified defective apis.
on average the above library versions with technical debts have monthly downloads.
api breaking changes .
.
for defective apis developers introduced breaking changes i.e.
api addition removal and signature changes during library evolution.
they 470can machine learning pipelines be better configured?
esec fse december san francisco ca usa induced crashes and nan bugs in the pipeline variants.
digging out the above api breaking changes in the popular library versions can help developers avoid compatibility issues.
performance impacts induced by ml library version combinations.
according to the definition of types a andbissues in group ....for.....the...........defective ..........library ..........version ..................combinations .....the......root.........causes ...of ...........problems .......may .......arise .......from ......one.....ml...........library .....but.....the.......plc ........issues .....are ..........actually ..........exposed ...in.....the......apis ......from .......other ............libraries ....due...to.....the...............interactions ..........between ...........libraries ...in.....the............pipelines.
by inspecting historical evolution data of the concerned defective apis we observed that the root causes can be api optimization default hyperparameter changes technical debts orapi breaking changes .
in total we identified plc issues in pipeline variants that are affected by defective api combinations from different ml library versions.
type a indirect dependency interference .
.
we located type a library version combinations involving apis from ml libraries.
pipeline variants adopting the defective library version combinations manifested the poor performance of execution time memory usage competition scores or induced crashes and nan bugs .
for example pipeline cfec directly depends on ml libraries sklearn .
.
pandas .
.
in the mtstage.
the pipeline invokes api sklearn .logisticregression which transitively depends on api pandas .rolling from library pandas .
we observed the memory usage of sklearn .logisticregression changed from 593mb to 329mb after downgrading pandas to version .
.
.
after investigation we found an issue pandas that api pandas .rolling suffered from memory bugs in pandas .
.
.
the combination of sklearn andpandasis used by pipelines on kaggle and of them invoke sklearn .linear model.logisticregression .
such defective versions are liable to induce plc issues.
type b data flow interference .
.
we located type blibrary version combinations from pipeline variants involving apis from ml libraries.
the above defective library version combinations caused inferior performance of execution time memory usage competition scores or induced crashes and nan bugs .
for example pipeline nlpdt invokes apis nltk .stem.porter.porterstemmer sklearn .extraction.
text.tfidfvectorizer andcatboost .catboost classifier from libraries nltk .
sklearn .
.
catboost .
.
.
the outcomes of former two apis can affect the inputs of catboost .catboostclassifier via data flow interactions in the pipeline while the former two apis are not invoked by the latter one.
after downgrading nltk to version .
we observed the execution time of catboost .catboostclassifier varied from 16s to 36s.
the combination of nltk sklearn andcatboost is adopted by pipelines on kaggle of them invoked the concerned three apis.
such plc issues can be easily triggered since the buggy version nltk .
has monthly downloads.
answer to rq3 by analyzing the historical evolution data of defective apis we identified four types of essential root causes of inducing plc issues.
plc issues exposed in defective apis are affected by one library version.
plc issues in pipeline variants are caused by combinations of apis from different library versions.
implication future research may focus on designing approaches to detecting and repairing plc issues by automatically mining and monitoring the evolution of defective ml library apis.
application of empirical findings repository of defective apis.
let a tuple d a lv p be a defective api combination whose invocation s potentially trigger plc issues where adenotes the signature s of api combination lvdenotes the defective library version range pdenotes the aspects of performance changes e.g.
execution time memory usage and precision .
for example nltk.stem.por ter.porterstemmer catboost.catboostclassifier catboo st .
.
nltk .
executiontime denotes an api combination of nltk.stem.porter.porterstemmer andcatboost.catboo stclassifier in libraries catboost .
.
and nltk .
respectively which achieves worse performance in execution time.
based on our empirical findings in rqs1 we construct a repository containing defective apis and api combinations involving ml libraries.
leveraging the defective api repository we can capture the plc issues in real world ml pipelines and ml libraries.
our study aims to answer the following two research questions rq4 do real world ml complex pipelines use the identified defective library apis?
rq5 can the repository of defective apis help capture real plc issues?
experiment setup.
to answer rq4 we collected large scale representative ml pipelines from github and hugging face platforms released by leading it companies including microsoft google tencent andalibaba as subjects.
the large scale industrial pipelines typically handle real world challenging tasks which are typically more complex than the specific competition tasks implemented by kaggle pipelines.
we selected hugging face platform because it is the most popular ai model repository where users can share pre trained models datasets and demos of ml projects .
to achieve this we formulated search keywords as topic machinelearning topic deep learning org microsoft org google org tencent org aalibaba .
the search returns and ml pipelines from github and hugging face repectively.
table shows the statistics of collected large scale pipelines.
on average they achieve stars on github and downloads on hugging face popular have kloc large in scale depend on ml libraries and ml library apis complex .
as we discussed in section .
we face challenges on deploying large scale pipelines due to lacking detailed training steps in documentation complete datasets or required dependencies.
for the collected ml pipelines we investigate whether they use defective apis in the problematic library version combinations identified in our repository.
to answer rq5 we collected pipelines as subjects from the competitions hosted between august and june onkaggle platform.
eventually out of pipelines have been successfully deployed.
among them and pipelines invoke .
defective apis and .
api combinations in our repository respectively.
for each pipeline using defective apis we checked if the performance inconsistencies crashes and nan bugs can be exposed on the corresponding problematic library versions.
since kaggle platform has no issue trackers we posted the validated results i.e.
performance changes across various library versions as comments on the pipelines websites to warn pipeline developers users against plc issues.
for the defective apis invoked by pipelines that achieve worse performances on ml libraries newest versions we also reported the plc issues to their issue trackers to ask library developers for validation.
471esec fse december san francisco ca usa yibo wang ying wang tingwei zhang yue yu shing chi cheung hai yu and zhiliang zhu table statistics of collected real world large scale pipelines stars downloadskloc ml libraries ml library apis min max avg min max avg min max avg min max avg github pipelines9 hugging face pipelines107 microsoft pipelines google pipelines tencent pipelines alibaba pipelines table experiment results of rq4 m1 m2 m3 m4 github pipelines hugging face pipelines sum m1 pipelines using defective ml library version combinations in our repository m2 pipelines using the defective apis combinations in our repository m3 defective library version combinations used by the pipelines m4 defective apis combinations used by the pipelines sklearn xgboost versionsscoretime ms memory mb sklearn xgboost versionsscoretime ms memory mb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure a plc issue googlebrain vpp reported by us results of rq4.
table reports the experiment results of rq4.
among the collected large scale ml pipelines .
of them depend on the identified defective ml library version combinations in our repository and of them invoke the corresponding defective apis combinations .
in total the collected large scale ml pipelines involve defective library version combinations and defective apis combinations .
in our repository.
for example api cv2.imread from library opencv is used by ml pipelines in dco stage which may induce significant performance inconsistencies in execution time.
the results indicate that our empirical findings identified in kaggle pipelines can scale to more complex industrial pipelines.
results of rq5.
table reports the statistics of our experiment results.
in total we exposed plc issues in out of real world pipelines.
.
of them were induced by one ml library version while the remaining .
of pipelines were caused by the combinations of ml library versions.
the plc issues involve defective apis and api combinations.
specifically and pipelines manifested in performance inconsistencies crashes and nan bugs respectively.
we can observe that .
pipelines invoking the defective api combination did not reveal plc issues.
the conditions of triggering such issues may be affected by many factors such as hyperparameters and dataset size which deserve further investigations in the future research.
encouragingly twenty pipeline developers shown their great interests in our detailed diagnosis info of plc issues generated bypiecer .
for example as shown in figure piecer s testing results pointed out that the execution time of pipeline googlebrainvpp changed dramatically across different version combinations of sklearn andxgboost .
developers confirmed the issue and left a comment thanks for this valuable information.
i will surely try and compare different versions.
for the defective apis that achieve worse performance on ml libraries newest versions we filed them into issue reports ongithub.
.
of them had been quickly confirmed by developers and were being fixed based on our testing results.
issues .
were acknowledged by the developers to be worthy of further investigation.
developers considered xgboost were not bugs.
they had to improve some aspects of performance at the cost of other metrics.
the remaining reports are pending probably because of inactive project maintenance.
for example we reported a plc issue to library catboost whose apicatboost .catboostclassifier in the latest version nearly consumes five times memory compared with that in versions .
.
.
developers quickly confirmed this issue and commented that i could reproduce your results and saw a jump in ram consumption from mb to mb.
the experiment results demonstrate that our defective api repository shed light on designing automated approaches to detecting plc issues on real world pipelines and ml libraries.
implications for ml pipeline developers.
our empirical study reveals the pervasiveness and seriousness of plc issues in ml pipelines.
developers can utilize piecer or our provided defective api repository to select version combinations of ml libraries when optimizing the performance of their pipelines.
for ml library vendors.
our empirical findings show that an ml library may achieve worse performance on its latest version when working with other ml libraries in the pipelines.
ml library developers should consider common library combinations and perform integration testing before releasing new versions.
for se researchers.
for se researchers future research can focus on designing promising techniques to detect and repair plc issues.
for tool builders.
tool builders can leverage our defective api repository and piecer to recommend appropriate version combinations of ml libraries to pipeline developers when maintaining evolving ml libraries.
we hope that this paper can inspire a symbiotic ecosystem where researchers tool builders and library vendors work together to assist developers combat plc issues.
threats to validity internal validity.
our empirical study involved manual effort which might introduce subjectivity and bias.
to mitigate this threat we followed an open coding scheme where two authors independently checked and cross validated all the results.
we employed cohen s kappa to measure the consistencies between the two authors.
if the kappa value was less than .
the authors needed to discuss to resolve disagreements.
to enhance transparency and accountability we made our dataset publicly accessible for scrutiny.
external validity.
the external validity concerns about the generality of our results.
to mitigate such a threat we collected high quality ml pipelines from kaggle platform as subjects to conduct our empirical study.
the pipelines depend on popular ml libraries including ml frameworks and data science libraries.
besides we also collected large scale representative ml pipelines from github and hugging face platforms released by leading it companies as subjects to investigate whether they use defective apis in the problematic library version combinations identified in our repository.
our empirical findings obtained based on such a representative dataset can be generalized to more ml pipelines.
472can machine learning pipelines be better configured?
esec fse december san francisco ca usa table results of rq5 plc issues identified in real world pipelines and ml libraries plc issues identified in real pipelines pipeline id performance impacts induced by one ml library version type a performance impacts induced by combinations of ml library versionstype b plc issues repored to ml libraries report id problematic apis plc issues were manifested in performance changes rise fall of three aspects competition scores execution time and memory usage and crashes nan bugs confirmed issues under fixing acknowledged by developers developers considered the issues were worthy of further investigation not bugs not bugs in the latest versions pending issues.
related work machine learning libraries.
there have been a lot of work focusing on the performance enhancement and quality assurance of ml frameworks.
zhang et al.
conducted an empirical study on the common bugs of ml applications using tensorflow .
nargiz et al.
and islam et al.
further conducted studies on keras tensorflow pytorch caffe andtheano with a taxonomy of bugs.
han et al.
performed an exploratory study on the dependency networks of deep learning libraries.
they studied the application domains dependency degrees version update behaviors of ml applications that depend on tensorflow pytorch and theano .
dilhata et al.
conducted a quantitative and qualitative empirical study to investigate the ml library usage and evolution.
they only focused on the ml frameworks and explored how developers used these libraries e.g.
common library combinations and how the usage evolved over the projects lifetime.
wang et al.
guo et al.
and phamet et al.
proposed novel mutation testing differential techniques to detect behavioral inconsistencies across multiple ml frameworks.
georgiou et al.
presented an in depth empirical analysis to investigate and compare the energy consumption and run time performance of dl frameworks pytorch andtensorflow .
our study distinguishes from the existing work in three aspects we focus on various types of common ml libraries including both ml frameworks and data science libraries we conduct the first empirical study to explore the impacts of different library version combinations on ml pipelines performances with the aid of our identified defective api repository we detect many plc issues in real world ml applications.
machine learning applications.
several se researchers presented comprehensive empirical studies on real bugs and technical challenges of ml applications.
islam et al.
guo et al.
han et al.
zhang et al.
and chen et al.
collected a set of highquality dl questions answers on stack overflow and summarized the main challenges in developing and deploying ml applications respectively.
approaches presented a root cause taxonomy of deep learning specific failures to facilitate future software development.
shen et al.
studied the bugs rising in the popular deep learning compilers and provided a series of valuable guidelines for deep learning compiler bug detection and debugging.
phamet etal.
studied the variance of dl systems and the awareness of this variance among researchers and practitioners.
existing and new testing techniques were also proposed for and adapted to machine learning applications to expose real bug.
the works by ma et al.
and shen et al.
proposed mutation operators specific to deep learning applications.
humbatova et al.
followed a systematic process to extract mutation operators from existing bug taxonomies to simulate the effects of real dl bugs.
in this paper we capture the performance inconsistencies of ml pipelines using differential testing across various library version combinations.
conclusion and future work in this paper we empirically studied ml pipelines from diverse competitions on kaggle to explore the impacts of different ml library version combinations on their performances.
our study reveals the pervasiveness and severity of plc issues in ml pipelines.
our findings can motivate the establishment of a symbiotic ecosystem where researchers tool builders and library vendors work together to assist developers in combating plc issues.
data availability we provide a reproduction package at to facilitate future research.
the package includes a dataset containing ml pipelines and library apis an available toolpiecer a defective api repository for detecting plc issues and a list of plc issues captured by us from real world pipelines and ml libraries along with our issue reproducing results.