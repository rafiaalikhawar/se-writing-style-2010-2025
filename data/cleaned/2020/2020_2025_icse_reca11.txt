automated generation of accessibility test reports from recorded user transcripts syed fatiul huq university of california irvine irvine california usa fsyedhuq uci.edumahan tafreshipour university of california irvine irvine california usa mtafresh uci.edukate kalcevich fable inc. toronto ontario canada kate makeitfable.comsam malek university of california irvine irvine california usa malek uci.edu abstract testing for accessibility is a significant step when developing software as it ensures that all users including those with disabilities can effectively engage with web and mobile applications.
while automated tools exist to detect accessibility issues in software none are as comprehensive and effective as the process of user testing where testers with various disabilities evaluate the application for accessibility and usability issues.
however user testing is not popular with software developers as it requires conducting lengthy interviews with users and later parsing through large recordings to derive the issues to fix.
in this paper we explore how large language models llms like gpt .
which have shown promising results in context comprehension and semantic text generation can mitigate this issue and streamline the user testing process.
our solution called reca11 takes in auto generated transcripts from user testing video recordings and extracts the accessibility and usability issues mentioned by the tester.
our systematic prompt engineering determines the optimal configuration of input instruction context and demonstrations for best results.
we evaluate reca11 s effectiveness on user testing sessions across three applications.
based on the findings we investigate the strengths and weaknesses of using llms in this space.
index terms software accessibility large language models crowd sourced software testing i. i ntroduction the world health organization has reported in that of the world population around .
billion people live with some form of disability .
however recent reports have indicated that software both web and mobile are predominantly inaccessible containing issues that prove to be nuisances and roadblocks for users with different disabilities.
for instance non textual ui elements like images that do not have alternative text are inaccessible to visually impaired users who rely on screen readers to announce elements.
for users with a motor disability ui elements that cannot be reached with keyboard navigation means they cannot access those information or features.
in a modern world that heavily relies on online applications for everyday tasks from work and living to health and entertainment it is imperative to develop applications accessible to all.
with the advent of standardized guidelines government policies and developer activism the initiative to develop accessible software has bolstered in recent times.
there are three ways of testing software for accessibility.
first automated testing where tools can conduct static analysis on web or mobile applications and check for guideline violations or dynamically assess whether assistivetechnologies ats like screen readers switch systems and more are functioning properly on the deployed app .
the second option is manual testing where developers and in house testers manually test the app with ats to check for accessibility issues.
while both of these are typically manageable by a development team and streamline the accessibility testing process they come with their limitations especially compared to the third option user testing.
accessibility user testing involves end users with disabilities evaluating the accessibility of an application using their preferred at.
user testing accommodates the insight from people with varying disabilities varying states from the same disability and varying ways they use ats and experience software.
in automated and manual tests these nuances are usually overlooked.
automated tools that primarily depend on static rules have been shown to report fewer issues than evaluations with actual disabled people .
manual testing by developers relies on their understanding of the various forms of disability which is reported to be severely lacking in several prior studies .
despite the advantages of user testing it is not widely adopted in the software industry for multiple practical reasons ranging from recruitment challenges to the cost of human and technical resources .
one such challenge comes from the test report generation and validation processes .
these processes involve revisiting test sessions through the recordings deriving and compiling the barriers found by the tester and validating the derived issues with the software team.
introducing llms into report generation can streamline these processes and make it easier for software teams to employ accessibility user testing.
in this paper we automate report generation from accessibility user tests by analyzing test transcripts through a large language model llm and producing a list of accessibility issues reported.
llms have proved their aptitude in analyzing and synthesizing natural language and their usefulness in fields ranging from software development to healthcare .
our study poses three research questions rqs and aims to understand how well llms can automate the test report generation process.
we explore their performance with different prompts rq1 assess their effectiveness on real world user tests rq2 and analyze where their strengths and weaknesses lie rq3 .
to run our experiments we conduct user tests run on three 1different mobile and web applications executed by users with different disabilities full blindness partial blindness and motor disabilities using different ats.
we use a total of test sessions that range from minutes to minutes.
for rq1 we take a sample of sessions and feed them to the llm using different prompts configuring the instructions contextual information and input transcript.
using the few shot learning approach we also provide the prompt with inputoutput demonstrations.
as our llm we use gpt .
the state of the art model.
from our prompt engineering we find that adding contextual information and demonstrations inform the model how to describe issues.
but the biggest factor in determining the model s effectiveness is the input size.
we see that feeding the model smaller chunks of the original transcript helps identify more issues.
based on these insights for rq2 we run the final tool on all transcripts.
the tool generates a near perfect recall of attributed to missed issues from a total of .
the high recall indicates the tool s effectiveness in detecting all possible accessibility barriers reported by users.
on the other hand with a precision of the tool identifies some issues that are not accurate.
for rq3 we manually inspect these inaccurate issues to understand llm s shortcomings.
we observe that llm is susceptible to language from contextual information repeated announcements related to assistive technologies and ambiguity in speech.
with these insights we discuss how best to employ llms for test report generation and how to potentially improve its performance with specific configurations and contextual information.
in summary this paper makes the following contributions an automated tool reca111 that takes in accessibility user test recording transcripts and generates a test report.
a technical analysis of prompt engineering for automated test report generation using llms.
a demonstration and qualitative analysis of llm s performance in detecting software accessibility issues.
an assessment of llm s application in the accessibility user testing process.
the remainder of this paper is structured as follows section ii provides an overview of accessibility user testing test report generation and large language models.
section iii describes the study design to answer our research questions.
section iv reports the results the implications therein are delved into in section v. the paper concludes with section vii.
ii.
b ackground and related work a. the accessibility user testing process accessibility user testing is the process of testing the accessibility and usability of a web or mobile application by having a user with disability interact with it.
prior studies have looked into accessibility user tests from an empirical perspective.
aizpurua et al.
evaluated the accuracy of user tests and found that disabled users experience of a software can differ vastly from the expected interactions with 1the name reca11 is inspired by the term a11y a popular shorthand for accessibility and our tool s goal to recall the issues from the transcript.categorical accessibility issues.
brajnik et al.
explored how expertise on accessibility can impact user tests and found that non experts derive significantly less number of issues than experts.
mateus et al.
also looked into the effect of expertise and observed that hci specialists tend to outperform software developers on reporting accessibility issues.
brajnik et al.
also found that collaborative testing where testers pair up to assess the accessibility of a page can report more accurately .
in an effort to mitigate the dependence on expertise song et al.
introduce truth inference techniques in crowd sourced accessibility evaluations.
to understand the contemporary practice of software accessibility testing we collaborate with fable an accessibility user testing platform that connects software stakeholders to testers with disabilities.
we interview one of their analysts and personally use two of the engagements they provide through their engage product user interviews and self guided tasks.
in these engagements a disabled tester is provided with an application to test for accessibility and usability.
the core difference between interviews and self guided tasks is that the former accommodates one or more interviewers in the testing session while in the latter the tester goes through a task by themselves and provides feedback based on their experience.
in fig.
we illustrate the process of these services.
the process begins with a software company or team opening a new request to test their product.
these teams can consist of product managers developers user researchers or ui and ux designers.
for this research study an analyst is assigned to the project who acts as an extension to the team consulting on accessibility.
she helps the team realize the specific test goals for their application or feature and create a list of tasks.
these tasks are what the testers are prompted to complete during the testing session.
a set of testers is selected based on the technical expertise required.
expertise ranges from the assistive technologies used e.g.
screen readers alternative navigation screen magnifiers and more to the platform that hosts the application e.g.
browser desktop or smartphone .
during a user test session the tester navigates the application to complete the assigned tasks.
the tester maintains the think aloud protocol vocalizing their thought process as they make decisions to navigate between pages or enter information and encounter challenges.
thinking aloud is an important step as it helps teams understand how the tester encounters a problem and why they deem it as an issue.
in user interviews the analyst can guide the tester during navigation answer the tester s queries and probe for further explanations.
during the session the analyst takes notes which she would refer to later for compiling a report.
these two steps depicted with dashed borders in fig.
are exclusive to user interviews and skipped in self guided tasks.
once the recording of the test session is published an analyst goes through it and compiles a test report.
the report contains accessibility and usability issues encountered by the tester.
the final report is submitted to the software team with suggestions for mitigation.
we observe a similar process in the domain of crowd2fig.
.
software testing process for this study using fable sourced software testing cst where non experts evaluate usability functionality and compatibility of software systems.
instead of an analyst a project manager coordinates with the testers and compiles and validates the reported issues.
similar to self guided tasks no interviewers are present in these testing sessions.
b. test report generation our work focuses on the report generation phase as highlighted in fig.
.
a report ideally contains all the issues of the application where in the app they occur quotes from the recording transcript for context and corresponding mitigation strategies.
it is a manual process with two categories of challenges centered around effort and expertise.
significant manual effort has to be expended to parse through test recordings and derive issues.
a recording can span from minutes to more than an hour each possibly requiring multiple revisions for better synthesis of information.
this effort is multiplied by the number of sessions required for a single project and the variation of assistive technologies based on target demographics of disability.
according to our interviewed analyst after parsing through the recordings it can take around minutes to synthesize and write the report even with notes taken during the interview.
to minimize the manual effort multiple research work have looked into different forms of automation generating reports from recurrent crash patterns screenshots in test reports relevant metadata from duplicate reports and summarizing existing reports .
however none of these studies regarded accessibility testing or generated reports directly from recordings.
other than effort expertise is a necessary component in the effective interpretation of test recordings to understand the cause and category of tester complaints.
report writing is easier with knowledge of assistive technologies which is no small feat given the use of a variety of techs for different disabilities on different platforms.
we can take screen readers as an example which are used by people with partialor full blindness.
there are four variants of screen readers which are popular jaws and nvda for windows v oiceover for apple devices and talkback for android phones each with their distinct characteristics and configurations.
expertise with guidelines like wcag used as an internationally accepted standard for accessibility is also beneficial.
wcag .
the latest iteration comes with guidelines under four principles containing a total of success criteria that elaborates on how to meet the guidelines.
knowing the guidelines helps in explaining the cause of a reported issue and possible ways to fix it.
fig.
.
an excerpt from a transcript to demonstrate noise in the text llm therefore provides an obvious solution to these challenges.
it holds the promise of reducing the manual effort by automatically parsing the test sessions and generating informative test reports for developers.
taeb et al.
observed that llms can effectively interpret accessibility test 3instructions written in natural language.
however automatic report generation presents its own challenges.
the transcripts lack systematic formatting and contains a substantial amount of noise for instance multiple speakers and audio interference from assistive technologies.
as illustrated in fig.
not only are the actual statements from the tester inaccurately punctuated and sometimes misspelled they are interrupted by screen reader announcements.
furthermore testers may not know what specifically has gone wrong in an interaction and cannot precisely articulate the details of the issue they face and instead verbalize their frustration caused by the barrier.
c. large language models with the advent of chatgpt large language models llms are becoming an integral part of natural language processing both in the public domain and research space .
these models trained on large scale corpora and tuned with billions of parameters are able to generate human equivalent text.
to work with llms the key is to prompt them with the appropriate instructions and contexts that aid them with generating the expected output.
for prompting llms contain four key components.
instruction is the task we want the llm to conduct.
instructions can specify the persona or perspective from which the llm should view the task and the format of the output.
input is the resource on which the llm has to conduct the specified task.
context is any information that can help the llm narrow its scope and enhance its lexicon on the specific knowledge space.
llms have shown to perform better if the prompt lists outdemonstrations .
termed as few shot or in context learning this process adds input output pairs to the prompt usually in the form of x1 y1 x2 y2 ... xn yn where xiis an example input and yiis the associated output.
iii.
s tudy design the primary goal of our work is to assess the applicability of llms in the space of accessibility user testing specifically to automate the report generation process.
we build an automated tool reca11 as illustrated in fig.
.
the tool is deployed once the tester conducts her assigned test.
using the transcript from the test recording and other contextual information related to the session our tool prompts an llm to generate the test report.
the resulting report is a list of accessibility and usability issues mentioned by the disabled tester.
we aim to understand how to prompt an llm to generate the best results and whether those results are effective enough to automate the report generation phase of accessibility user testing.
we pose three research questions.
rq1.
prompt engineering how does configuring different prompts affect performance?
the performance of llms is dictated by the prompt fed to it.
we determine the best performing prompt by configuring the input and instructions.
rq2.
statistical effectiveness how effective is llm in producing accurate accessibility issues?
based on the optimal prompt we statistically assess how accurate and consistent the model is in deriving the issues reported.rq3.
strengths and weaknesses where does llm perform consistently and where does it underperform and why?
we manually assess the results to understand the factors in the transcript that determine the llm s performance.
a. datasets our primary data point is the transcript automatically generated from a video recording of a user test session.
a test is done on a single application on all or some of its features.
for our paper we collect test session transcripts for three different apps accumulating a total of transcripts.
these tests are conducted by testers with disabilities through fable s user testing services.
the tests are recorded generating a video recording along with a transcript file.
the transcript file is in a.vtt format consisting of a list of captions where each caption contains a start and end timestamp speaker name and the text being spoken.
money manager dmm money manager is an open source android application with features to add and monitor personal budgetary information.
we choose this finance application because of its familiar use case uncomplicated design and our ability to modify the code.
we modify different features and ui elements of the app to inject accessibility issues as categorized by previous work .
we conduct two preliminary tests with screen reader users to assess the effectiveness of injected issues.
for the final dataset we use user interviews and self guided tasks with blind testers who use talkback on android.
record a goose sighting dgs record a goose sighting is an educational website used for training people on accessibility testing.
similar to dmm this website has been injected with accessibility issues by its creators who are not the authors of this paper.
for this dataset we conduct a total of self guided tasks covering every assistive technologies ats offered in fable under three at types screen readers for blind users nvda jaws v oiceover and talkback alternative navigation for users with limited mobility dragon naturallyspeaking on screen keyboard v oice control and switch system screen magnifiers for users with low vision os magnification and zoomtext fable dfb dfbcontains archival test sessions from fable where testers are assigned to test different features of fable s digital products.
there are a total of sessions with self guided tasks and user interviews.
the testers demographic of these sessions includes blind users users with low vision and users with limited mobility.
b. contextual information as mentioned in section ii c contextual information enhances gpt s performance.
since the knowledge space our prompt is trying to target is software accessibility we incorporate two relevant guidelines as context wcag the web content accessibility guidelines wcag has been developed to establish an international standard for software accessibility on web and 4fig.
.
overview of reca11 our automated tool for generating test reports from accessibility user tests mobile.
we insert the website for wcag guidelines to our prompt as context.
it contains guidelines with multiple success criteria that specify how to test whether a guideline is satisfied.
for instance the guideline .
keyboard accessible is related to the operability of web content with keyboard alone containing three success criteria detailing different ways to test it.
including these guidelines as context can help the llm map tester statements to guidelines and success criteria and determine whether it can constitute an accessibility issue.
internal guidelines the accessibility user testing process has overlap with crowd sourced software testing which depends on an intermediary platform to connect software stakeholders with testers.
platforms can include their own guidelines for analyzing test reports.
hence we include internal guidelines as a contextual information adopting fable s research directory .
this document lists takeaways and best practices for different concerns e.g.
color contrast and assistive technologies e.g.
screen reader and explains how to resolve them.
while wcag caters to designers and developers about best accessible practices this document is intended to train fable s analysts on tester behavior and issue categorization.
while the two guidelines are used for enhancing the llm s knowledge about software accessibility in general we also use the following two sources of data to elaborate on the test session being analyzed.
task description for a test session the tester is prompted to follow a set of tasks.
these tasks are determined by the analyst and software team.
task descriptions can include names of elements and pages on the screen that the llm can potentially map to the information contained in the transcripts.
for our study we use task descriptions only fordmm anddgs as we do not have access to the archival test requests from dfb.
technical parameters test sessions also contain technology specifications like at type e.g.
alternative navigation the specific at used e.g.
dragon naturallyspeak ing and the platform the application under test is deployed on e.g.
desktop browser .
this can further help llm focus on relevant issues.
c. output the expected output from the llm is a list of issues making up the test report.
each issue will contain three items issue title description and timestamps.
the first two are used to describe the reported barrier the ui elements involved and how the tester encountered the issue.
timestamps are one or more locations in the recording where an issue was mentioned.
as per our preliminary study we know that analysts would include clips from the recording in their report so that the software team can refer to the recording for validation or explanation.
timestamps serve the same purpose.
because of the non deterministic nature of llm outputs we run the model multiple times for each test session.
the goal is to observe the varied responses llm can produce and select the most appropriate issues.
we test with two different selection processes all and common .
in fig.
we illustrate our selection function.
given we run the model five times on a transcript we would generate five test reports tr1 where each test report can detect a different set of issues i1toin.
in all and common we aim to incorporate all and the most common unique issues reported respectively.
to find and list issues that are unique we semantically match the issues between different reports.
if an issue found in the current run matches an issue found in a prior run we consider it to be a duplicate e.g.
i7matches i1in fig.
.
allcontains a list of all the unique issues.
from all common takes in the subset that occurred in all the reports e.g.
i1andi5.
for semantic matching we calculate the cosine similarity between the vector embeddings of two issues.
from the issues we concatenate the issue title and description and encode the resulting text.
for encoding we use a sentence transformer all mpnet base v2 .
trained on billion sentence pairs this model is built for finding semantic similarity between sentences and short paragraphs and is reportedly the best performing model .
5fig.
.
output selection process d. evaluation metrics we evaluate the accuracy of issues reported by the model by manually comparing with all the issues mentioned in the test recording.
we categorize each reported issue in one of three groups confirmed mentioned and non issue.
we consider a reported issue to be confirmed if the issue is included in the ground truth gt .
we create three gts for the three datasets as explained in table i. table i ground truths for evaluation label dataset source gtmm money manager the accessibility issues we injected based on prior work .
gtgs report a goose sightingthe accessibility issues injected by the creators of the website .
gtfb fable accessibility issues reported by fable s analysts on dfb.
however not every issue in the gt is reported by the tester during the test because either the tester did not realize the issue taking place or did not consider it to be an issue.
conversely not all issues reported by the tester is in the gt.
for the latter case we include the category mentioned issues.
for each issue not categorized as confirmed we manually check the recording on the timestamp reported with the issue.
if the tester mentioned the reported issue then we label it as mentioned .
otherwise we label it as a non issue .
we consider confirmed ic and mentioned im issues as true positive and non issues in as false positive.
if an issue is mentioned by the tester but not reported by our model that is considered a false negative fn .
therefore the formulas for precision and recall are precision pic pimpic pim pin recall pic pimpic pim pfn e. gpt for our llm we employ gpt the latest of the gpt models.
we use this model because it outperforms contemporary ones in natural language processing tasks and its turbo update provides a larger input length of 128k tokens necessary for accommodating large input such as transcripts.
since our work requires deterministic output we run the model on temperature which reduces llm s creativity and increases if not ensures determinism .
iv.
r esults a. rq1 prompt engineering the goal of this research question is to configure the different components of the prompt instruction contextual information input and demonstrations to derive the optimal prompt that generates the most accurate results.
to do so we take a sample of transcripts from our dataset of and analyze the results from prompt engineering.
we semi randomly select the sample dataset so that transcripts of different characteristics are represented.
from dmm we randomly select one self guided task and one user interview.
since dmm are all screen reader tests from dgswe randomly pick two transcripts that use alternative navigation and screen magnification.
lastly from dfb we choose three transcripts of the largest average and smallest sizes.
for each variant we run our tool reca11 as illustrated in fig.
.
as input we provide the transcript and the corresponding contextual information when needed.
as output the tool generates the test report a list of issues.
for analysis we manually inspect each issue comparing it with the gt and the original recording.
labeling the issues as confirmed mentioned or non issue we calculate the precision and recall.
we also assess the qualitative characteristic of the title and description and the accuracy of the timestamps.
we report the quantitative and qualitative findings and compare with previous variants to choose the better prompt.
once we derive the optimal variant of a component we employ that as the 6baseline for analyzing the next component.
we add prompts for the new variants onto the existing one.
rq1.
instruction inspired by previous study we write instructions of variant levels of detail simple instruction with scenario with persona and with warning against forced results.
we list the variants in fig.
.
we add one variant at a time to check their effect on the prompt.
table ii comparing sample results for different instructions rq1.
picpimpin fn d mmsimple with scenario with persona with warning d mmsimple with scenario with persona with warning after executing the prompts with subsequent instructions on all samples we find that there is no distinguishable differences in the performance of the model.
in table ii we show the results for two d mm sessions as examples employing the allselection criteria.
we see that the results remain nearly stagnant for all the variants.
while adding scenario helped in d mm it did the opposite for d mm.
descriptively the issue specifications remain similar across variants.
the corresponding timestamps were mostly accurate.
however longer transcripts suffered from what we call clustered targeting.
the tool extracted issues from the end of the transcript missing issues mentioned in earlier parts.
rq1.
contextual information the number of false negatives from the previous prompt indicates that the tool is failing to understand statements as issues.
therefore we look to improve the results by adding contextual information informing our model more about software accessibility.
as described in section iii b we incorporate four contextual information in our prompt wcag guidelines internal guidelines from fable task description for specific test sessions and technological parameters the session was conducted on.
additionally we filter the internal guidelines using the technological parameters to automatically select relevant guidelines.
as shown in fig.
we include some preamble text before introducing each context to better differentiate the information.
the results however do not demonstrate any improvements numerically.
as exemplified in table iii adding each contextual information did not significantly improve the results.
in analyzing the issues descriptively we find that they are more verbose and use guideline specific language.
generic wordings are replaced by more technical terms like heading structures error messages notifications and more.
however the reports still suffer from clustered targeting.
the increased prompt size attributed to the inclusion of contextual information amplifies this challenge.
rq1.
input in analyzing the reports so far we noticed that the reported issues originated from the last sections of fig.
.
an example prompt from our experiments the transcript especially for longer transcripts.
in general the model s performance degraded as transcripts got larger.
therefore we decide to create multiple chunks from a single transcript injecting smaller sized subsets of the transcript into the prompt and combining the resulting reports.
7table iii comparing sample results for different contexts rq1.
picpimpin fn d mmno context task description tech params wcag internal guidelines all contexts d mmno context task description tech params wcag internal guidelines all contexts we split a transcript by the number of captions.
in our sample dataset the number of captions ranged from to .
the larger transcripts d mm d mmandd fbcontain and transcripts respectively all of which suffer from clustered targeting.
we choose to divide transcripts by captions since that halves these transcripts.
we also choose chunk sizes of captions as the smallest transcript d fb has captions.
from our experiments so far the results for d fbhave generated perfect scores.
during the chunking process we include one extra caption from the previous chunk to provide context to the current chunk and mitigate an issue being ignored.
we observe from prior variants that the reported issues contain an average of captions.
hence we choose one caption as our padding.
we find that the results improve significantly.
we illustrate our findings in table iv.
the number of reported issues increases significantly with chunked transcripts with caption chunks resulting in zero false negatives.
we observe that the number of non issues also increased.
however we deem it an acceptable trade off as missing issues are more critical for reports.
table iv comparing sample results for different inputs rq1.
picpimpin fn d mmfull transcript captions captions d mmfull transcript captions captions d fbfull transcript captions captions rq1.
demonstrations lastly we introduce demonstrations into the prompt as a form of in context or one few shot learning .
as shown in fig.
we structure a demonstration in two parts first an excerpt of a transcript and second the example issues reported from that excerpt.
to create demon strations we take excerpts from dfb.
these reports compiled by fable analysts represent how issues are interpreted and written by industry experts.
we experiment with two types of demonstrations single example following one shot learning we include only one transcript excerpt and one resulting issue as a demonstration.
multiple examples following few shot learning we include multiple excerpts with multiple resulting issues.
in both cases we select demonstrations so that they match the input transcript s technological parameters.
from the set of demonstrations we only choose ones with similar type of at used.
at types represent the tester s disability e.g.
screen reader users have full or partial blindness while alternative navigation users have motor disability and therefore can provide similar language when conveying issues faced.
table v comparing sample results for demonstrations rq1.
picpimpin fn d gsno examples single example multiple examples d gsno examples single example multiple examples to exemplify we take two sessions from d gs the first uses screen magnification and second alternative navigation.
table v shows that the results do not indicate any significant improvement from using no examples.
specifically in d gs all the variants performed similarly.
for d gs the examples detected an extra issue each.
qualitatively the reported issues show clear influence in terms of verbosity.
the demonstration issues were very on point and concise compared to the model s descriptions.
however in terms of accuracy descriptions for all three performed similarly.
b. rq2 statistical effectiveness based on the results of rq1 we decide to use the following configuration for our tool s final prompt instruction we include a detailed instruction with scenario persona and a warning to mitigate false positives.
contextual information we include all the sources for contextual information task description technological parameters wcag and internal guidelines.
as mentioned in section iii b we do not include task description fordfbin the absence of validated information.
input we use transcript chunks with caption size as that produced the highest amount of correct issues.
demonstrations while adding demonstrations did not hamper the results we found the improvement not significant enough to include demonstrations in the final prompt.
especially because these require a significant amount of tokens and we found that increasing token size can be detrimental to report quality.
8we run this final prompt on all transcripts.
for each transcript we run the model five times and create the final report using allandcommon methods as described in section iii c. for each issue in the final report we manually assess its validity by parsing the recording and ground truth answers for each of the datasets.
the results are listed in table vi grouped by datasets at types and category of user test.
in the final column we show the cumulative results.
when considering allissues our tool generated a precision of and a near perfect recall of .
the precision indicates that most of the issues reported are correct as in these have been mentioned by the tester as opposed to false interpretation.
the high recall indicates that almost all the issues mentioned by the tester have been reported by the model missing only out of .
for common issues precision slightly increases but recall drops to .
in filtering out issues mentioned less frequently valid issues were skipped.
the slight increase in precision indicates that the filtering process helped remove some false interpretations.
looking into the different datasets we see that the model underperforms for dfbcompared to the other two.
the recall suffers from false negative issues which can be attributed to cases where fable analysts derived issues from the recording that was not apparent in the transcript.
we look into the false positives in the next section.
in terms of recall reports from the three different at types perform near equally.
however precision falls for sessions conducted with alternative navigation technology with screen reader sessions also accumulating comparatively more false positives.
we detail in the next section how the nature of these two technologies can cause the model to misinterpret text in the transcript as an issue where none exists.
lastly in comparing the difference of performance between self guided tasks and user interviews we see that they scored similar precision and recall.
this indicates that the model works well for both types of user tests despite the transcripts for the latter containing extra speech from the interviewer in the transcripts.
c. rq3 strengths and weaknesses for this research question we manually inspect the reports to understand where the model succeeded and underperformed and why that might have happened.
with a recall of the quantitative results showed that the model is able to grasp reported issues from a noisy transcript.
from manual observation we also see that the issues reported are described accurately and in detail.
it interprets the tester s complaint and relates it to common accessibility issues aided by the guidelines used as contextual information.
in fig.
we illustrate a reported issue specifically one derived from the excerpt in fig.
.
this demonstrates how the model can parse through the noise and understand the tester s speech and place that issue within the context of the larger transcript and elaborate on the issue.
however we also observed that the guidelines can lead to generic descriptions.
for instance we observed the predominance of terms related to error messages and focus fig.
.
example of a reported issue with title description and timestamps management .
as shown in fig.
while not incorrect these often convoluted straight forward descriptions when comparing with ground truth.
looking into the internal guidelines we found that there were multiple entries with these terms and hypothesize its influence on the model.
fig.
.
example of reports being affected by guideline language for the model s output we instructed it to generate the timestamps where the issues originated.
in most cases the model has been able to make accurate mapping of reported issues with the corresponding timestamps helpful for revisiting issues in the recording.
in some rare cases as shown in fig.
the issues would span long stretches of timestamps and generate vague descriptions.
while not always inaccurate these are redundant and may lead to extra effort to understand an issue.
fig.
.
example of a report with vague description and too many timestamps 9table vi results for rq2 dmm dgsdfb screen reader alternative navigationmagnification self guided interview total no.
of sessions 36alltp fp fn precision recall commontp fp fn precision recall we observed that the model is very thorough in analyzing issues.
it was able to generate a total of issues from transcripts.
a strength of being thorough is that it can extract issues from minute details that may be missed by an analyst.
indfb where the confirmed issues were issues derived by analysts the mentioned issues can represent issues they have missed.
of the total unique true positives detected in dfb almost are mentioned issues.
this indicates that our model is able to detect issues that human analysis may miss.
the downside of being thorough is that the model would sometimes forcefully interpret statements as issues.
there are three types of such cases.
first when the tester is remembering past issues experienced with other applications.
the model is not always able to differentiate that with issues on the current app.
second when the tester is talking about issues that are not related to the app for instance issues with the keypad or their at.
the model sometimes interprets them as the app s issues.
third when the tester is complimenting on a certain feature.
the model would still try to reframe the statement as a possible issue with the app under test.
we exemplify the last case in fig.
where the tester appreciating the inclusion of a transcript is framed as them complaining about video controls.
fig.
.
example of an inaccurate report that misconstrued a compliment lastly as touched on in the last section a weakness of the model is related to the nature of screen readers and alternative navigation tools.
the former announces elements on the screen while the latter usually works through voice commands from the user.
both of these audio cues are present in the transcript.
in most cases the model is able to parse the noise and exclude screen reader announcements and voice commands.
however when the announcement or command contains repeat phrases the model interprets it as a problem.as shown in fig.
the event of a tester using mouse grids to pinpoint elements and clicking through them is flagged as an issue because the tester used the click voice commands multiple times in succession.
fig.
.
example of an inaccurate report that misinterpreted repeat voice commands as a possible issue the points listed in this section are valuable in understanding the limitations of llms in this space but also direct further investigation to be done to improve this line of work.
v. d iscussion in this study we look into the applicability of llms for automated report generation in accessibility user testing processes.
we develop an automated tool called reca11 that takes in transcripts from user test sessions and generates a test report containing the accessibility and usability issues encountered by the tester with disability.
to develop the tool we first conduct prompt engineering to configure the most optimal prompt for the llm.
we observe the performance of our tool on real world user testing transcripts and investigate the properties of the reported issues.
in this section we discuss our findings from these steps and what implications these have for the relevant communities.
from our prompt engineering runs we derive two elements that influence the results the most input size and the language of contextual data.
we observe that dividing the transcripts especially larger ones into smaller chunks leads to better issue extraction.
secondly the model takes significant influence in writing its output from the contextual information provided 10to it e.g.
internal guidelines.
this can be beneficial when organizations want to adhere to a predetermined format.
from the quantitative results the tool performs near perfectly with recall in deriving all issues mentioned by the tester.
however the tool suffers from false positives where the model labeled as issues instances that were either not mentioned as an issue originally or not mentioned at all.
despite that the precision of the tool is .
lastly in analyzing the descriptive properties of the reported issues we derive multiple categories of negative cases.
unintuitive issues the overuse of guideline jargon where it is not necessary can create unintuitive issues.
generic issues the model can merge multiple consecutively mentioned issues together to report a generic issue often associated with long timestamps.
misinterpreted issues the model can misunderstand neutral and positive statements or statements that are not related to the app under test as issues.
distracted issues due to the interruption in the transcript from the screen readers the model is confused by the transcript and derives an issue from it.
these findings and observations help us understand the applicability of this tool and llms at large in the context of accessibility user testing.
implications for intermediaries organizations like fable who connect software teams to disabled testers work as intermediaries in the user testing process.
similarly in the cst world there exist intermediaries like amazon mechanical turk and others .
our tool is built to streamline their processes by automatically generating the test report.
intermediaries have the opportunity to improve the performance of the tool by solving its limitations.
for instance intermediaries can use data from their archival sessions as demonstrations to the prompt or to even create a dedicated fine tuned model.
intermediaries can also update processes for conducting test sessions to better adopt reca11.
for example a separate audio input for screen readers can reduce noise from the transcript hence improving the tool s performance.
implications for software teams software practitioners attribute the disinterest in user testing to its time consuming processes and lack of expertise .
reca11 tackles both of these challenges by reducing the time to analyze test recordings and incorporating timestamps and guideline specifications for easier understanding.
however this tool cannot work as a complete replacement for expertise and awareness on software accessibility.
user tests do not always provide the most accurate feedback and solving the issues derived still require technical understanding of accessible programming.
implications for researchers the field of llm applications is ever expanding and newer methods of improving model performance are being experimented with and discovered.
based on our prompt engineering results there is opportunity to modify and expand on the methods and improve upon the reported limitations.
future work can also look into more advanced outputs.
the generated reports can rank the issues based on severity determined by its impact during the user test session or how negatively the tester talks about it.
the reported issues can include steps for resolution with the inclusion of interaction data or the source code as reference.
vi.
t hreats to validity internal validity llms typically generate nondeterministic results causing a potential threat to validity.
to mitigate this we ran the model five times for each transcript combined duplicate issues and reported all the unique issues.
using a sentence transformer to detect duplicate issues can pose a threat.
to mitigate we manually experiment with one report from each dataset checking whether the reported duplicates were reliable and configuring the threshold accordingly.
external validity to increase the generalizability of our results we experiment with test sessions from three different sources.
one of these sources dfb was conducted in real world setting by external clients with analysis from experts.
while the sessions for dgswere conducted by the authors the website itself along with the issues injected were developed by accessibility experts.
we conducted dmm on an open source application that has been experimented with for accessibility in prior papers.
the issues we injected were in accordance to categories derived by prior studies.
we distributed our test sessions to represent all three popular at types screen readers alternative navigation and magnification and both user testing methods self guided tasks and user interview.
the numeric distribution of the latter is based ondfb a real world project.
vii.
c onclusion in this study we aimed to automatically generate test reports from recorded user transcripts in an effort to streamline the accessibility user testing process.
we employ gpt .
a large language model capable of comprehending synthesizing and generating natural language to derive accessibility and usability issues from test transcripts.
from our findings we observe that the model performs well when the transcript is split into smaller chunks and is paired with detailed instructions and contextual information about the test.
our tool secures a precision of and a recall of indicating its success in finding the great majority of reported issues.
we also investigate its inaccuracies and derive limitations such as forced interpretation and susceptibility to guideline language.
to ensure reproducibility of our study we publish our artifacts on a companion website .
viii.
a cknowledgments this work has been supported in part by award numbers and from the national science foundation.
we thank fable for their collaboration in making this research possible.
we are grateful for the detailed feedback from the anonymous reviewers of this paper which helped improve this work.
11references disability fact sheet who mar .
.
available a. alshayban i. ahmed and s. malek accessibility issues in android apps state of affairs sentiments and ways forward in proceedings of the acm ieee 42nd international conference on software engineering pp.
.
s. chen c. chen l. fan m. fan x. zhan and y .
liu accessible or not an empirical investigation of android app accessibility ieee transactions on software engineering vol.
pp.
.
s. yan and p. ramachandran the current status of accessibility in mobile apps acm transactions on accessible computing taccess vol.
no.
pp.
.
webaim webaim the webaim million the report on the accessibility of the top home pages accessed on .
w3c web content accessibility guidelines wcag .
.
.
available a. s. compliance.
a recap of website accessibility lawsuits.
ada site compliance.
.
available s. f. huq a. alshayban z. he and s. malek a11ydev understanding contemporary software accessibility practices from twitter conversations in international conference on humancomputer interaction ser.
chi .
new york ny usa association for computing machinery .
.
available kif.
keep it functional an ios functional testing framework.
.
available android.
improve your code with lint checks.
google.
.
available g. android.
march accessibility scanner.
.
available s. hao b. liu s. nath w. g. halfond and r. govindan puma programmable ui automation for large scale dynamic analysis of mobile apps in proceedings of the 12th annual international conference on mobile systems applications and services .
bretton woods new hampshire usa acm new york ny usa pp.
.
m. m. eler j. m. rojas y .
ge and g. fraser automated accessibility testing of mobile apps in ieee 11th international conference on software testing verification and validation .
v aster as sweden icst pp.
.
n. salehnamadi a. alshayban j. w. lin i. ahmed s. branham and s. malek latte use case and assistive service driven automated accessibility testing framework for android in proceedings of the chi conference on human factors in computing systems .
virtual okohama japan acm new york ny usa pp.
.
n. salehnamadi f. mehralian and s. malek groundhog an automated accessibility crawler for mobile apps in proceedings of the 37th ieee acm international conference on automated software engineering pp.
.
a. s. alotaibi p. t. chiou and w. g. halfond automated detection of talkback interactive accessibility failures in android applications in ieee conference on software testing verification and validation icst ieee.
virtual ieee pp.
.
a. alshayban and s. malek accessitext automated detection of text accessibility issues in android apps in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering ser.
esec fse .
new york ny usa association for computing machinery p. .
.
available f. mehralian n. salehnamadi s. f. huq and s. malek too much accessibility is harmful!
automated detection and analysis of overly accessible elements in mobile apps in 37th ieee acm international conference on automated software engineering ieee.
rochester michigan usa acm new york ny usa .
n. salehnamadi z. he and s. malek assistive technology aided manual accessibility testing in mobile apps powered by record andreplay in proceedings of the chi conference on human factors in computing systems pp.
.
g. brajnik a comparative test of web accessibility evaluation methods in proceedings of the 10th international acm sigaccess conference on computers and accessibility pp.
.
a. aizpurua m. arrue s. harper and m. vigo are users the gold standard for accessibility evaluation?
in proceedings of the 11th web for all conference pp.
.
d. a. mateus c. a. silva a. f. de oliveira h. costa and a. p. freire a systematic mapping of accessibility problems encountered on websites and mobile apps a comparison between automated tests manual inspections and user evaluations journal on interactive systems vol.
no.
pp.
.
y .
inal k. r zvano glu and y .
yesilada web accessibility in turkey awareness understanding and practices of user experience professionals universal access in the information society vol.
no.
pp.
.
y .
inal f. guribye d. rajanen m. rajanen and m. rost perspectives and practices of digital accessibility a survey of user experience professionals in nordic countries in proceedings of the 11th nordic conference on human computer interaction shaping experiences shaping society pp.
.
t. bi x. xia d. lo j. grundy t. zimmermann and d. ford accessibility in software practice a practitioner s perspective acm transactions on software engineering and methodology tosem vol.
no.
pp.
.
s. l. christopher vendome diana solano and m. linares v asquez can everyone use my app?
an empirical study on accessibility in android apps in ieee international conference on software maintenance and evolution icsme .
ieee .
m. v .
r. leite l. p. scatalon a. p. freire and m. m. eler accessibility in the mobile development industry in brazil awareness knowledge adoption motivations and barriers journal of systems and software vol.
p. jul.
.
.
available s. cao and e. loiacono the state of the awareness of web accessibility guidelines of student website and app developers in international conference on human computer interaction .
springer pp.
.
h. petrie and n. bevan the evaluation of accessibility usability and user experience.
the universal access handbook vol.
pp.
.
s. alyahya crowdsourced software testing a systematic literature review information and software technology vol.
p. .
a. beganovic m. a. jaber and a. abd almisreb methods and applications of chatgpt in software development a literature review southeast europe journal of soft computing vol.
no.
pp.
.
j. li a. dada b. puladi j. kleesiek and j. egger chatgpt in healthcare a taxonomy and systematic review computer methods and programs in biomedicine p. .
g. brajnik y .
yesilada and s. harper the expertise effect on web accessibility evaluation methods human computer interaction vol.
no.
pp.
.
d. a. mateus s. b. l. ferreira m. r. de almeida souza and a. p. freire accessibility inspections of mobile applications by professionals with different expertise levels an empirical study comparing with user evaluations in ifip conference on human computer interaction .
springer pp.
.
g. brajnik m. vigo y .
yesilada and s. harper group vs individual web accessibility evaluations effects with novice evaluators interacting with computers vol.
no.
pp.
.
s. song j. bu a. artmeier k. shi y .
wang z. yu and c. wang crowdsourcing based web accessibility evaluation with golden maximum likelihood inference proceedings of the acm on humancomputer interaction vol.
no.
cscw pp.
.
feb .
.
available m. van someren y .
f. barnard and j. sandberg the think aloud method a practical approach to modelling cognitive london academicpress vol.
no.
.
w. t. tsai l. zhang s. hu z. fan and q. wang crowdtesting practices and models an empirical approach information and software technology vol.
p. .
m. g omez r. rouvoy b. adams and l. seinturier reproducing context sensitive crashes of mobile apps using crowdsourced monitoring in proceedings of the international conference on mobile software engineering and systems pp.
.
d. liu x. zhang y .
feng and j. a. jones generating descriptions for screenshots to assist crowdsourced testing in ieee 25th international conference on software analysis evolution and reengineering saner .
ieee pp.
.
x. chen h. jiang z. chen t. he and l. nie automatic test report augmentation to assist crowdsourced testing frontiers of computer science vol.
pp.
.
h. jiang x. li z. ren j. xuan and z. jin toward better summarizing bug reports with crowdsourcing elicited attributes ieee transactions on reliability vol.
no.
pp.
.
f. scientific.
march jaws.
.
available n. access.
march nvda.
.
available apple.
march introducing voiceover.
.
available .html a. talkback.
march get started on android talkback.
.
available m. taeb a. swearngin e. schoop r. cheng y .
jiang and j. nichols axnav replaying accessibility tests from natural language in proceedings of the chi conference on human factors in computing systems ser.
chi .
new york ny usa association for computing machinery .
.
available openai.
introducing chatgpt.
.
available j. yang h. jin r. tang x. han q. feng h. jiang s. zhong b. yin and x. hu harnessing the power of llms in practice a survey on chatgpt and beyond acm transactions on knowledge discovery from data .
y .
wang q. yao j. t. kwok and l. m. ni generalizing from a few examples a survey on few shot learning acm computing surveys csur vol.
no.
pp.
.
n. nasser.
money manager app.
.
available instructions record a goose sighting.
.
available b. newing answers record a goose sighting.
.
available nuance.
march get more done by voice.
.
available a. support.
march use voice control.
.
available fable.
march what is a switch system.
.
available f. scientific.
march zoomtext.
.
available hugging face.
.
available n. reimers pretrained models sentence transformers documentation.
.
available models.html openai.
march gpt .
.
available j. achiam s. adler s. agarwal l. ahmad i. akkaya f. l. aleman d. almeida j. altenschmidt s. altman s. anadkat et al.
gpt technical report arxiv preprint arxiv .
.
z. cheng t. xie p. shi c. li r. nadkarni y .
hu c. xiong d. radev m. ostendorf l. zettlemoyer et al.
binding language models in symbolic languages arxiv preprint arxiv .
.
n. nashid m. sintaha and a. mesbah retrieval based prompt selection for code related few shot learning in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
q. guo j. cao x. xie s. liu x. li b. chen and x. peng exploring the potential of chatgpt in automated code refinement an empirical study in proceedings of the 46th ieee acm international conference on software engineering pp.
.
june amazon mechanical turk.
.
available march reca11 artifacts.
.
available