deeprover a query efficient blackbox attack for deep neural networks fuyuan zhang kyushu university japan fuyuanzhang .comxinwen hu hunan normal university china huxinwen hunnu.edu.cn lei ma the university of tokyo japan university of alberta canada ma.lei acm.orgjianjun zhao kyushu university japan zhao ait.kyushu u.ac.jp abstract deep neural networks dnns achieved a significant performance breakthrough over the past decade and have been widely adopted in various industrial domains.
however a fundamental problem regarding dnn robustness is still not adequately addressed which can potentially lead to many quality issues after deployment e.g.
safety security and reliability.
an adversarial attack is one of the most commonly investigated techniques to penetrate a dnn by misleading the dnn s decision through the generation of minor perturbations in the original inputs.
more importantly the adversarial attack is a crucial way to assess estimate and understand the robustness boundary of a dnn.
intuitively a stronger adversarial attack can help obtain a tighter robustness boundary allowing us to understand the potential worst case scenario when a dnn is deployed.
to push this further in this paper we propose deeprover a fuzzing based blackbox attack for deep neural networks used for image classification.
we show that deeprover is more effective and query efficient in generating adversarial examples than state ofthe art blackbox attacks.
moreover deeprover can find adversarial examples at a finer grained level than other approaches.
ccs concepts computing methodologies neural networks software and its engineering software testing and debugging .
keywords adversarial attacks deep neural networks blackbox fuzzing acm reference format fuyuan zhang xinwen hu lei ma and jianjun zhao.
.
deeprover a query efficient blackbox attack for deep neural networks.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december corresponding authors fuyuan zhang xinwen hu permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
san francisco ca usa.
acm new york ny usa pages.
introduction deep neural networks dnns have achieved tremendous success over the past decades attaining competitive performance in many industrial applications and domains e.g.
image classification object detection and speech recognition.
recently there has been a growing trend in deploying dnns in practical applications and integrating them into traditional software systems e.g.
unmanned vehicles.
however the robustness issues of a dnn still posts severe concerns and threats especially in safety and security critical scenarios.
extensive research has been conducted in developing adversarial attacks aimed at identifying adversarial examples that can mislead the predictions of a dnn.
importantly these attacks provide a means to assess the adversarial robustness of dnns.
a more effective attack technique could yield a tighter boundary of robustness enhancing our understanding of the potential limitations and risks of a dnn after deployment.
the early stage research in this direction primarily focused on developing white box attacks .
in the whitebox setting attackers can have full access to the internals of dnns to find the robustness issues through the feedback of dnn s internal behaviors.
various whitebox techniques are proposed and shown to help detect adversarial examples with high attack success rates.
however in typical real world application scenarios the internals of a dnn usually can not be fully accessed making the obtained robustness boundary to be misestimated in the operational environment.
therefore recent research has shifted to focusing on the blackbox setting where attackers cannot access network structures and parameters of dnns.
the blackbox setting is more realistic as the internal information of dnns can not be fully accessed in real world settings.
consequently blackbox attacks are constructed mainly via querying dnns for their prediction results.
as query numbers are viewed as the main costs of attacks it has been a de facto standard to assess the performance of blackbox attacks in query limited settings.
the main challenge in designing query efficient blackbox adversarial attacks is to achieve a high attack success rate with a low number of queries.
one is motivated to perturb input images along gradient descent directions to maintain a high attack success rate.
however estimating gradient directions in the blackbox setting costs a large number of queries .
in our previous work we have esec fse december san francisco ca usa fuyuan zhang xinwen hu lei ma and jianjun zhao proposed deepsearch a simple and effective blackbox attack based on feedback directed fuzzing.
although conceptually simple deepsearch outperforms state of the art blackbox attacks in both attack success rate and query efficiency.
however deepsearch can only generate adversarial perturbations constrained by the l distance metric.
developing query efficient l2blackbox attack remains an open and challenging problem.
in this paper we develop a fuzzing based blackbox attack to better evaluate and understand the adversarial robustness of neural networks w.r.t the l2distance metric.
we focus on improving three aspects of blackbox attacks effectiveness subtlety and query efficiency.
attacks with a high attack success rate can find adversarial examples challenging to be detected by other approaches thus increasing the possibility of exposing the severe vulnerability of neural networks under adversarial perturbations.
more subtle attacks can reveal adversarial examples closer to decision boundaries which allows a better approximation of the robustness boundaries of neural networks.
moreover techniques with a high query efficiency can be further used to better evaluate the cost of constructing attacks in real world settings.
our approach.
we propose deeprover an effective and queryefficient blackbox attack for neural networks used for image classification.
the attack of deeprover is constrained by the l2distance metric constructed via querying a target network for its prediction scores.
deeprover is developed based on feedback directed fuzzing.
we introduce the design of deeprover in the following three aspects of blackbox attacks effectiveness to achieve a high success rate deeprover inserts well designed mutation vectors to input images to push them towards decision boundaries.
our experiments show that even defended neural networks are vulnerable to such perturbations.
consequently deeprover is very effective in generating adversarial examples.
subtlety to construct adversarial examples close to decision boundaries we extend deeprover with a refinement step adapted from and reduce the l2distance of adversarial examples once they are found.
our experiments show that the refinement step of deeprover is indeed effective in generating subtle adversarial examples.
query efficiency to improve query efficiency we utilize spatial regularities in images and group mutation vectors in blocks of pixels.
we find in our experiments that our grouping strategy actually enhanced both the effectiveness and query efficiency of deeprover .
we compare deeprover with two state of the art blackbox adversarial attacks in the score based blackbox threat model.
attackers have a limited query budget and can only query neural networks for their prediction scores.
we use three well known datasets in our evaluation namely svhn cifar and imagenet .
for svhn and cifar we evaluate all attacks on both defended and undefended neural networks.
the defended networks are trained with a state of the art adversarial defense .
our experimental results demonstrate that deeprover has the highest attack success rate and is more query efficient than other state ofthe art attacks.contributions.
our contributions are fourfold we present a novel fuzzing based blackbox adversarial attack for deep neural networks.
we show that deeprover is more effective than state of theart blackbox attacks in generating adversarial examples.
we show that the refinement step in deeprover effectively reduces the distortion of generated adversarial examples.
we show that deeprover is query efficient in finding adversarial examples.
our strategy of grouping mutation vectors in blocks improves the query efficiency of deeprover .
outline.
we briefly introduce the relevant background in the next section.
in sect.
we present deeprover a novel fuzzingbased blackbox adversarial attack for deep neural networks.
sect.
extends deeprover with iterative refinement which can further reduce the distortion of generated adversarial examples.
our experimental evaluations are presented in sect.
.
after discussing the related work in sect.
sect.
concludes and discusses the future work.
background we use column vectors x x1 ... xn tonn dimensional vector space rnto represent images.
each coordinate xi r i n represents an image pixel.
assume that lm l1 ... lm are labels formclasses.
a deep neural network that classifies input images intomclasses can be represented as a function n rn lm.
for an input image x the label thatnassigns to xisn x .
assume that xis a correctly classified input image and x is derived by adding subtle mutations to x. we consider x as an adversarial example ifn x n x andx is very close to x according to some distance metric.
we use the l2distance metric in this paper to measure the distance of adversarial examples.
thel2distance between xandx is the standard euclidean distance between them x x vtn i xi x i for an input xand distance d r the set of input images within dfrom xforms ann dimensional sphere and we write s x d to denote the sphere i.e.
s x d x x x d .
a neural networknisrobust at input xfor distance dif all images ins x d are classified into the same class.
query limited score based blackbox attacks.
we evaluate blackbox adversarial attacks in the score based query limited setting.
we assume that attackers cannot access any internal information of target neural networks but can learn their prediction scores and attackers can only query target neural networks for a limited time.
hence the setting of blackbox attacks considered in this paper is summarized as follows.
given a query budget b input xand distance d attackers are motivated to generate an adversarial example ins x d by querying target neural networks for their prediction scores in at most bqueries.
1385deeprover a query efficient blackbox attack for deep neural networks esec fse december san francisco ca usa x0x2dl1s x0 d x1x3 figure deeprover for dnn classifers fuzzing deep neural networks in this section we introduce the technical details of deeprover to generate adversarial examples via fuzzing deep neural networks.
.
deep neural networks adeep neural network classifier dnn classifier classifies input images according to the following definition from .
definition dnn classifier .
given a classification function f rn rm adeep neural network classifier nf rn lmthat classifies inputs into mclasseslm l1 ... lm is defined as nf x lj iffj arg maxifi x wherefi rn ris the evaluation of fon theith class i.e.
f x f1 x ... fm x t. intuitively a dnn classifier nfclassifies xinto classljiffj x is the maximum value among all values fi x for1 i m. the decision boundaries ofnfdivide input images into different classes.
for example the surface of the region rli x j m j i fi x fj x forms the decision boundaries ofnffor classli which we denote as dli.
all images thatnf classifies into class liare within regionrli.
for an input x rli finding adversarial examples within distance dfrom xis equivalent to finding an input images in s x d that are across the decision boundarydli.
for example fig.
shows the decision boundary dl1 denoted as a curved line of a dnn classifier.
images on the left side of dl1are classified into class l1 and images on the other side are classified into other classes.
assume that input x0is correctly classified and that we use the dash dotted circle to represent s x0 d .
then input x1andx2are not adversarial as they are still on the left side of dl1.
however x3is an adversarial example as it has crossed dl1.
objective functions.
assume that xis classified into class li.
to find adversarial examples for x we define the following objective function and use deeprover to iteratively minimize the value of obj x withins x d .
obj x fi x max j ifj x an alternative choice for defining objective function is obj x fi x .
intuitively fi x is the score value for xto be classified toclassli and decreasing fi x would implicitly increase the scores for other classes.
.
mutation vectors for l2attacks a critical step in our approach is to add well constructed mutation vectors suitable for l2adversarial attacks to input images.
deeprover constructs mutation vectors iteratively and adds them to input images if they can decrease the objective functions.
intuitively a mutation vector aims to mutate a small sized square block of pixels of input images.
the center of the block has the largest mutation value and the mutation value decreases as it moves away from the center.
assume that bis a square block of pixels of an image.
for ease of presentation we use xi jto represent pixels in b whereiandjare coordinates of xi jon the images.
a mutation vector added to block bcan be specified by a function that maps each pixel of bto a mutation value mv xi j p1 ifxi j xic jcp2 i ic j jc otherwise wherexic jcis the center pixel on b andp1 p2are two parameters that control the mutation value added by the mutation vector.
in practice we have that p1 p2 which guarantees that mv xic jc mv xi j forxi j xic jc.
to improve query efficiency we exploit spatial regularities in images and add multiple mutation vectors to improve query efficiency.
instead of adding one mutation vector to an image per query deeprover actually adds multiple mutation vectors simultaneously to a square block of pixels in each query.
fig.
2a shows a simple example for illustration.
assume that b1represents a random block of pixels of an image.
two mutation vectors represented as blue squares are added to random locations in block b1.
.
fuzzing deep neural network classifiers deeprover is an iterative algorithm consisting of two major steps initialization and feedback directed fuzzing .
we introduce some notations below and then explain the two steps in detail.
assume that brepresents a square block of m mneighboring pixels of an image x. we use xbto denote the pixel values of bin x which is a vector that only contains coordinates of pixels in b. we write x for a new image derived by assigning vector rto corresponding pixels in b. we use projd x x to denote the input image derived by projecting x to the boundary of the sphere s x d .
we omit the definition of projd x x as projecting an n dimensional vector onto the surface of an n dimensional sphere is standard.
initialization.
in the initialization step deeprover first generates a number of randomly selected blocks of pixels of an input image.
then it adds mutation vectors to each of the blocks which could potentially decrease the value of objective functions.
the resulting image is then projected to the boundary of a corresponding l2sphere to ensure that it is still within a predefined l2distance.
if the derived image is not adversarial deeprover will start its fuzzing step introduced below.
1386esec fse december san francisco ca usa fuyuan zhang xinwen hu lei ma and jianjun zhao b1b3b2 a initialization b1b2b3b5b4 b fuzzing figure a simple example of initialization and fuzzing.
feedback directed fuzzing.
in the fuzzing step deeprover iteratively mutates randomly selected blocks of input image pixels to further decrease the value of objective functions.
in each iteration deeprover aims to increase adversarial perturbation on one block of pixels on the input image.
since the l2distance for perturbation is fixed to remain within the l2sphere increasing the mutation distance on one block of pixels entails decreasing mutations on other pixels.
consequently deeprover performs two operations in each iteration mutation insertion andmutation deletion .
in the mutation insertion step deeprover inserts multiple mutation vectors on a randomly selected block of pixels b. these vectors are added at random locations within b. moreover we further require that all inserted mutation vectors point in the same direction i.e.
either they all increase pixel values or decrease pixel values.
otherwise closely located vectors pointing in opposite directions have a chance to cancel the perturbation of each other.
in the mutation deletion step deeprover erases existing mutations added to a number of randomly generated small sized blocks of pixels on input images.
this step aims to reduce the l2distance of the existing perturbation so that deeprover can utilize the reduced perturbation distance to add mutation vectors in the mutation insertion step.
if the resulting image derived by applying the above insertion and deletion steps can decrease the value of object functions we keep all the mutations computed in the current iteration and start the fuzzing step from the resulting image in the next iteration.
otherwise we discard the mutations derived in the current iteration.
example.
we give a simple example to illustrate how deeprover finds adversarial example for input x0in fig.
.
deeprover first uses its initialization step to generate x1as follows.
deeprover first randomly selects three blocks of pixels b1 b2 andb3on input x0shown in fig.
2a.
then deeprover adds two mutation vectors at random locations within each block.
the added mutation vectors are represented by blue colored squares within b1 b2andb3in fig.
2a.
after adding these mutation vectors to x0 we derive a new image x .
notice that it is possible that the l2distance between x0andx 0is greater than d. to constrain the distance of initial perturbation we project x 0to the boundary of s x0 d to derive an image x1 projd x0 x which finishes the initialization step.
since x1is not across the decision boundary x1 is not an adversarial example.
deeprover then starts its fuzzing step from x1as follows.algorithm deeprover for dnn classifiers.
input input x rn functionf rn rm distanced r output x s x d 1function initialize x d is generatesblocksb b1 ... bs of sizet t x x foreachbi bdo ri gen mutvec bi bvn vs x x bi x bi ri x projd x x return x 10function deeprover x f d is x0 xandk repeat 13b bp gen block k x 14vn density bp s r gen mutvec b vn vs x k remv mutvec xk b r r x k x k r xk x k 2r r x k projd x x k ifobj x k obj xk then xk x k else xk xk 24k k untilnf x nf xk or query limit is reached return xk first deeprover randomly selects a block of pixels b1from x1 shown in fig.
2b.
to create extra perturbation distance for mutation vectors to be added to block b1 deeprover then randomly generates four blocks of pixels b2 b3 b4andb5and erases existing mutations on those blocks.
finally using the perturbation distance derived from blocks b2 b3 b4andb5 deeprover adds three mutation vectors represented as three blue squares to b1.
after projecting the resulting image to the boundary of s x0 d we derive x2.
since x2is still not adversarial deeprover repeats the above fuzzing process to push further x2towards the decision boundary.
as a result deeprover generates image x3 which is an adversarial example.
deeprover for dnn classifiers.
alg.
shows the deeprover algorithm for fuzzing dnn classifiers.
we explain the details of the algorithm as follows.
function initialize describes the initialization step of deeprover .
we first generate sinitial blocks b1 ... bs line where each blockbi i s contains indices corresponding to a t t block of pixels in x. here the number of sand block size tare parameters of alg.
.
then function gen mutvec generates corresponding mutation vectors for each block bi line .
bothbvnand vsare parameters.
variable bvndefines the number of mutation vectors to be added to block bi.
for instance in the example in fig.
2a variable bvnequals to two as deeprover adds two mutation 1387deeprover a query efficient blackbox attack for deep neural networks esec fse december san francisco ca usa vectors to each block in the initialization step.
variable vsdefines the size of mutation vectors i.e.
each generated mutation vector mutates avs vsblock of pixels.
after adding mutation vectors rito each block bi line ofx we rescale the resulting image by projecting it to the boundary of s x d line .
function deeprover summarizes the fuzzing step of deeprover .
it first assigns to x0an initial image x line which is supposed to be the input image after initial perturbation.
then we generate a random block busing function gen block line .
here bp returned by gen block is the percentage of pixels of xcontained in blockb.
we generate mutation vectors to block bby calling function gen mutvec line and the derived vector rhas been normalized to have length .
notice that the variable vn which defines the number of mutation vectors for block b is computed in line where variable density andsare both parameters.
variable srefers to the number of initial blocks generated in the initialization step.
for instance in the example in fig.
2a variable sequals to three as deeprover generates three initial blocks in the initialization step.
variabledensity defines the least number of mutation vectors for blockb.
intuitively bp sroughly estimates how many initial blocks could have been added to an area as large as block b. the value of vnchanges when the size of bchanges.
we use the computation in line14to control the number of mutation vectors added to images in each iteration.
the mutation deletion is done by function remv mutvec in line and the resulting image is x k. we merge the mutation vectors inrwith the existing perturbation on block bofx kand derive the combined mutation r forbin line .
in line we compute the available mutation distance for r rescale it to an appropriate length and derive r .
finally we add vector r to blockband project the resulting image to the boundary of s x d line .
if the newly computed image x k after projection can reduce the value of objective functions we use image x kfor the next iteration line .
the iteration continues until deeprover either finds an adversarial example or has reached the query limit.
iterative refinement once an adversarial example is found we are motivated to reduce its distance from the original image thus lowering the distortion of generated adversarial examples.
in this section we adapt the refinement approach in to our setting and extend deeprover with an iterative refinement step to generate adversarial examples with low distortions.
the main idea of our refinement is that we iteratively project adversarial examples to the boundaries of smaller l2spheres as long as the images derived after projection are still adversarial.
once an adversarial example reaches the decision boundaries of neural networks we search for other adversarial examples at the same distance i.e.
within the same l2sphere from which we would continue our projection step.
we illustrate the idea of iterative refinement through the following example.
example.
fig.
shows the decision boundary dl2of a deep neural network classifier.
images below the decision boundary are classified into class l2 and images above it are classified into other classes.
assume that input x0is correctly classified into class l2and deeprover has already found an adversarial example x1within x2 s x0 d1 x1x0x3 s x0 d2 s x0 d3 x4dl2figure deeprover with iterative refinement s x0 d1 .
our technique allows finding adversarial examples with smallerl2distance via iterative refinement starting from input x1.
first we compute via bisect search the smallest distance dsuch that the input projd x0 x1 is still an adversarial example.
once such a distance is found we can generate an adversarial example closer to x0by projecting x1tos x0 d .
assume that d2is such a distance that we find and as a result we generate an adversarial input x2 projd2 x0 x1 .
compared to x1 input x2has a lower distortion as it is closer to x0.
second we apply deeprover to search for other adversarial examples withins x0 d2 .
the motivation here is to find another adversarial example such that projecting it onto s x0 d for some d d2would result in a new adversarial example closer to x0.
assume that we find x3bydeeprover .
then we repeat the bisect search in the first step to find the smallest distance dsuch that projd x0 x3 is still adversarial.
assume that we find distance d3this time and we derive x4by projecting x3 tos x0 d3 .
assume that deeprover fails to find another adversarial example ins x0 d3 .
as a result we return x4as the adversarial example with the lowest distortion.
deeprover with iterative refinement.
the description of deeprover with iterative refinement is given in alg.
.
we give a brief explanation as follows.
each iteration starts from a project step line .
we first compute thel2distance between xandx line where xrefers to the original input image and x is an adversarial example generated so far.
then we use bisect search to find the smallest distance d such that projecting x to the boundary of s x d still gives us an adversarial example line .
since projd x x is already the closest adversarial example derived by projection to continue the project step in the next iteration we call deeprover again line to find another adversarial example x for projection.
on line we check whether the input x found by deeprover withind is adversarial.
we start the next iteration if x is indeed a new adversarial example.
otherwise we return the adversarial example derived through projection i.e.
projd x x and terminate the algorithm.
1388esec fse december san francisco ca usa fuyuan zhang xinwen hu lei ma and jianjun zhao algorithm deeprover with iterative refinement.
input input x rn adversarial input x s x d d r functionf rn rm output an adversarial input x s x d d d 1function dr refinement x x f is repeat 3d x x find the minimum distance d dvia bisect search such that input projd x x is an adversarial example.
x deeprover x f d if x is an adversarial example then x x else x projd x x until x is not an adversarial example return x andd experimental evaluation we evaluate the performance of deeprover1on testing the adversarial robustness of both defended and undefended neural networks.
we compare deeprover with state of the art blackbox attacks on neural networks for three well known datasets.
we design our experiments to answer the following research questions rq1 whether deeprover is an effective approach in generating adversarial examples of deep neural networks?
rq2 whether deeprover is effective in reducing the distortion of generated adversarial examples?
rq3 whether deeprover is a query efficient blackbox attack?
rq4 whether mutation vectors of deeprover are more effective than vectors that mutate all pixels for the same value?
rq5 whether grouping mutation vectors in blocks are more effective than using only one mutation vector by deeprover ?
.
evaluation setup datasets and network models.
we used deep neural networks on three popular datasets namely svhn cifar and imagenet to evaluate the performance of deeprover .
for svhn and cifar we used 000randomly selected correctly classified images from their test sets to perform adversarial attacks.
for imagenet we randomly selected 000correctly classified images from its validation set for evaluation.
for each of svhn and cifar we trained two wide resnet w34 10networks .
one is a naturally trained undefended network and the other is trained using a state of the art adversarial defense .
for svhn we trained an undefended network with .
test accuracy and the defended network has .
test accuracy.
for cifar the undefended network we trained has .
test accuracy and the defended network has .
test accuracy.
for imagenet we used two pre trained undefended networks inception v3 and resnet in evaluation.
although defense for imagenet networks are important we do not consider defended networks for imagenet because imagenet networks using defense techniques of are not publicly available.
1source code is available at blackbox attacks.
deeprover is compared with two state of the art blackbox adversarial attacks the bandits attack is optimized for both l2andl distance metrics.
it is a gradient estimation based blackbox attack that achieves high query efficiency via integrating gradient priors through bandit optimization.
we compare deeprover with itsl2attack.
the square attack is based on the random search which iteratively adds square shaped local updates to input images as long as the updates can improve its objective function.
the square attack can perform both l2andl blackbox attacks and we compare deeprover with itsl2attack.
deeprover implementation.
we give a brief introduction of some implementation details of alg.
.
in the fuzzing step we use b bp gen block k x to generate random blocks for mutation insertion.
in our implementation bis a square block and its size decreases as the number of iterations indicated by k increases.
we use variable bp the percentage of pixels contained in b to control the size ofb.
in our experiments we define the initial value of bp using a parameter fbpand decrease the value of bpby half when kequals to or8000.
in each iteration after inserting mutation vectors to block b we use function remv mutvec to delete existing mutations.
to achieve this we generate a number of square blocks of equal size e.g.
b1 ... bl and erase all previous mutations added to these blocks.
in our implementation the total number of pixels in blocks b1 ... bl approximately equals to the number of pixels in block b. parameter settings.
we scale images to for all datasets and setl2distanced .
.
the query budget for svhn and cifar10networks is .
we set the query budget as 000for imagenet networks the same as the query budget of .
for the square attack we use the suggested parameters in their paper.
their attack has only one parameter and we set p .1for neural networks on all datasets.
for the bandits attack we have tried various parameters to test its performances on different network models and selected the parameters that give their best performance for comparison.
we set bandits exploration finite difference probe oco learning rate .
image learning rate h and tile size to 4for svhn networks.
we set bandits exploration finite difference probe oco learning rate .
image learning rate h and tile size to 4for cifar 10networks.
for imagenet resnet network we set bandits exploration finite difference probe oco learning rate .
image learning rate h 50and tile size to .
for imagenet inception v 3network we set bandits exploration finite difference probe oco learning rate .
image learning rate h 50and tile size to .
fordeeprover we first introduce parameters of mutation vectors.
for networks on all datasets we set p1 .5andp2 .
to constrain the mutation value of mutation vectors.
for svhn networks we set vs 7to define the size of mutation vectors.
for both cifar 10and imagenet networks we set vs 11to define the size of mutation vectors.
other parameters of deeprover include the followings.
we have three parameters s tandbvnin the initialization step.
we have 1389deeprover a query efficient blackbox attack for deep neural networks esec fse december san francisco ca usa two parameters density andfbpin the fuzzing step where fbp defines the initial value of bp.
for both defended and undefended svhn networks we set s density t bvn and fbp .
.
for the defended cifar 10network we set s density t bvn fbp .
.
for the undefended cifar 10network we set s density t bvn and fbp .
.
for the imagenet inception v3 network we set s density t bvn andfbp .
.
for the imagenet resnet 50network we set s density t bvn andfbp .
.
.
evaluation metrics we use the following metrics to evaluate the effectiveness subtlety and query efficiency of blackbox attacks.
attack success rate.
we use the attack success rate to evaluate the effectiveness of adversarial attacks.
it computes the percentage of images for which adversarial examples are discovered.
techniques with higher attack success rates are more effective in generating adversarial examples.
we use genadv x to denote whether an adversarial example for input xis generated i.e.
genadv x if and only if an adversarial example for xis found.
given a set of images x x1 ... xk we compute the attack success rate of a blackbox attack by asr x kk i 1genadv xi averagel2distance.
we use average l2distance to measure the subtlety of adversarial attacks.
assume that x x1 ... xk are a set of input images and xadv x ... x k are corresponding adversarial examples.
the average l2distance between xandxadv is computed by avgdsl2 x xadv kk i xi x i attacks that can find adversarial examples with lower average l2distance rate are more subtle.
for attacks with similar success rates we further use this metric to distinguish which technique can find more subtle adversarial examples.
average queries.
we use query numbers to measure the efficiency of blackbox attacks.
blackbox attacks that cost fewer queries are more efficient in making attacks.
for each attack we compute the average number of queries needed for conducting successful attacks.
the mean number of queries is also computed in our experiments for reference.
similar to we do not count query numbers in the refinement stage of deeprover which starts only after an adversarial example is generated.
.
experimental results we show our experimental results in fig.
and tabs.
.
for the first three research questions we compare deeprover with the bandits attack and the square attack.
in the last two research questions we compare deeprover with two of its variants denoted by variant 1and variant 2in the tables.table results on svhn networks.
attacksuccess rateavg.
l2avg.
queriesmed.
queries undefended network bandits .
.
.
.
square .
.
.
deeprover .
.
.
variant .
.
.
variant .
.
.
defended network bandits .
.
.
.
square .
.
.
deeprover .
.
.
variant .
.
.
variant .
.
.
table results on cifar 10networks.
attacksuccess rateavg.
l2avg.
queriesmed.
queries undefended network bandits .
.
.
square .
.
.
deeprover .
.
.
variant .
.
.
variant .
.
.
defended network bandits .
.
.
.
square .
.
.
.
deeprover .
.
.
variant .
.
.
.
variant .
.
.
table results on imagenet inception v3 network.
attacksuccess rateavg.
l2avg.
queriesmed.
queries bandits .
.
.
.
square .
.
.
.
deeprover .
.
.
.
variant .
.
.
.
variant .
.
.
.
adversarial examples generated by deeprover for the imagenet inception v 3network are shown in fig.
.
the first row shows the original images.
adversarial examples found by deeprover are listed in the second row.
the third row shows adversarial examples derived after the refinement step of deeprover .
from fig.
we can observe that adversarial perturbations constructed by deeprover especially those derived after refinement are hardly visible to human eyes.
1390esec fse december san francisco ca usa fuyuan zhang xinwen hu lei ma and jianjun zhao a svhn defended b cifar defended c imagenet inception v3 figure results on success rate w.r.t number of queries.
figure adversarial examples found by deeprover on imagenet inception v 3network.
first row original images.
second row adversarial examples.
third row adversarial examples after refinement.
table results on imagenet resnet 50network.
attacksuccess rateavg.
l2avg.
queriesmed.
queries bandits .
.
.
.
square .
.
.
.
deeprover .
.
.
.
variant .
.
.
.
variant .
.
.
.
results on attack success rate rq1 .
experimental results demonstrate that deeprover is an effective approach in generating adversarial examples on both defended and undefended neuralnetworks.
it has achieved a higher attack success rate than the other two state of the art blackbox attacks.
for svhn and cifar 10networks although the attack success rate of other approaches is also high deeprover is the only approach that has achieved attack success rate on all defended and undefended neural networks.
for imagenet resnet 50network deeprover has achieved .
attack success rate which is the same as the square attack.
in our experiments the imagenet inception v 3network is the most difficult to attack.
deeprover has achieved an .
attack success rate slightly higher than the square attack.
answer to rq1 deeprover is an effective approach to generating adversarial examples for both defended and undefended neural networks.
results on average distortions rq2 .
for networks on all three datasets adversarial examples generated by deeprover have the lowest average l2distance which means deeprover can find adversarial examples more subtly than other approaches.
this result shows that the refinement step of deeprover effectively reduces distortions of adversarial examples.
we can use deeprover to estimate the distance of decision boundaries of neural networks.
actually adversarial examples generated bydeeprover after refinement are always at the border of decision boundaries of neural networks.
this is because deeprover always returns the adversarial example from which it is no longer possible to reduce its l2distance by projecting it to a smaller distance which can be seen from alg.
.
for both svhn and cifar we can see that the decision boundaries of defended networks are indeed pushed further away from the original input images compared to undefended networks.
answer to rq2 deeprover can find adversarial examples with low distortions.
the refinement step of deeprover can effectively reduce the average l2distance of generated adversarial examples.
results on query efficiency rq3 .
experimental results show thatdeeprover has higher query efficiency than other attacks on all networks except for the case of the bandits attack on the defended cifar 10network.
1391deeprover a query efficient blackbox attack for deep neural networks esec fse december san francisco ca usa although the bandits attack used fewer average queries it has a lower attack success rate than deeprover which means some images that would cost a large number of queries to be attacked are not counted by the bandits attack.
otherwise its average queries would be much higher.
for example the attack success rate of bandits on the defended cifar 10network is .
which means there are 19images that bandits cannot successfully attack within 000queries.
if it were able to attack even one of the 19images on the 000th query the average queries of bandits would have become .
which is higher than deeprover .
to have a fairer comparison we have checked from our experimental data the average queries of deeprover when it reaches the final success rate of the bandits attack.
on the defended cifar network the average queries of deeprover is96.7when it reaches the success rate of .
which shows that deeprover actually has a higher query efficiency than the bandits attack.
answer to rq3 deeprover is a query efficient blackbox attack in generating adversarial examples.
results on mutation vectors rq4 .
we perform an ablation study to show the effectiveness of mutation vectors.
recall that mutation vectors are used to mutate pixels of small sized blocks where pixels near the center of the block are mutated for much larger values than pixels near the border of the block.
to study whether such a design is effective in improving the attack success rate and query efficiency we substitute mutation vectors with vectors that mutate all pixels in a block for the same distance.
we choose such vectors for comparison because these vectors are often used for query reduction in related work and are therefore a natural substitute for mutation vectors of deeprover .
consequently we implemented a variant of deeprover namely variant and compared its performance with deeprover .
since we focus on effectiveness and query efficiency the refinement step is not included in variant .
for networks on all datasets deeprover has a higher attack success rate and query efficiency than variant .
for svhn and cifar 10networks the attack success rate of variant 1is close todeeprover .
it has achieved a attack success rate on three networks and a .
success rate on the remaining one.
however its success rate dropped on imagenet networks.
on the inception v3network the success rate of variant 1is0.
lower than deeprover .
on the resnet 50network its success rate is .
lower than deeprover .
answer to rq4 the use of mutation vectors in deeprover is effective in improving its attack success rate and query efficiency.
it is more effective than vectors mutating all pixels in a block for the same value.
results on grouping mutation vectors rq5 .
to study the effectiveness of grouping mutation vectors in blocks we perform an ablation study and create another variant of deeprover namely variant and compare its performance with deeprover .
variant is derived by inserting only one mutation vector to input images in each iteration of the fuzzing step.
similar to variant we do not include the refinement step in variant .for svhn and cifar 10networks variant 2has achieved attack success rate on both defended and undefended networks.
however it is less query efficient than deeprover .
on imagenet networks the attack success rate of variant dropped significantly.
on the inception v 3network the success rate of variant 2is5.
lower than deeprover .
on the resnet 50network its success rate is .
lower than deeprover .
although the average queries of variant 2is lower than deeprover on imagenet networks the decrease in average queries of variant 2actually comes from its decrease in attack success rate.
images that cost a large number of queries to be successfully attacked are not counted in the average queries of variant .
to have a closer comparison between deeprover and variant regarding query efficiency on imagenet networks we have checked from our experimental data the average queries of deeprover when it achieves the final attack success rate of variant .
on the inception v 3network when deeprover reaches the success rate of .
its average queries is .
much lower than that of variant .
on the resnet 50network when deeprover reaches the success rate of .
its average queries is .
which is also much lower than that of variant .
this shows that deeprover is more query efficient than variant 2on imagenet networks.
answer to rq5 grouping mutation vectors in blocks can effectively improve the attack success rate and query efficiency ofdeeprover .
.
threats to validity we introduce the following threats to validity in our experiments.
selection of datasets and networks.
we have only used network models for three datasets namely svhn cifar and imagenet.
therefore it is not guaranteed that our experimental results can be generalized to networks for other datasets.
however the datasets we have selected are the most commonly used in related works evaluating adversarial attacks.
selection of compared approaches.
various blackbox attacks have been developed to evaluate the robustness of neural networks.
we have selected two approaches from state of the art l2attacks.
to the best of our knowledge compared to other state of the art attacks they already have a very high attack success rate and query efficiency.
selection of parameters.
the selection of parameters for compared approaches can affect their performance.
in our experiments we have used suggested parameters in their papers and tried various parameters to compare their performance.
we select those parameters that give their best performance on our networks.
selection of defense techniques.
various defense techniques have been developed to improve the adversarial robustness of neural networks.
our experimental results may not generalize to networks trained with other defense techniques.
however the defense we have selected is the state of the art defense.
1392esec fse december san francisco ca usa fuyuan zhang xinwen hu lei ma and jianjun zhao related work our work is related to works on adversarial attacks and the recent research on developing software engineering techniques for quality assurance of deep neural networks.
as both research areas are moving fast we only introduce the research most relevant to us.
adversarial attacks.
since szegedy et al.
first revealed the existence of adversarial examples in neural networks various adversarial attacks have been developed to evaluate the robustness of neural networks.
whitebox attacks are fast and highly effective.
however they require full access to network parameters which is not realistic in real world scenarios.
blackbox attacks have been developed under different assumptions of target network models.
papernot et al.
developed a transfer attack that utilizes the transferability of machine learning.
scored based attacks require having access to prediction scores of neural networks.
decision based attacks only require knowing the final decision of target networks.
deeprover is a type of score based attack and it is closest to deepsearch .
both techniques utilize feedback directed fuzzing to generate adversarial examples and reduce the distortions of adversarial examples through iterative refinement.
their difference lies in the metrics they use to measure the distance of adversarial examples.
while deepsearch is anl attack deeprover aims to find adversarial examples constrained by the l2distance.
testing and verification of dnns.
various software engineering techniques have been developed to facilitate the testing of deep neural networks.
to guide test case generation researchers have proposed applicable coverage criteria of neural networks and work in has revisited existing coverage criteria and proposed neural coverage as a new criterion.
while dnn testing techniques focus on detecting bugs in dnns formal verification of deep neural networks aims at providing formal guarantees of related properties e.g.
safety robustness and fairness properties of neural networks.
similar to formal verification of traditional software dnn verification techniques also suffer from the state explosion problem making it more challenging than dnn testing.
repairing of deep neural networks.
to fix erroneous behaviors of dnns dnn repair techniques have been proposed recently.
broadly speaking existing dnn repair approaches can be categorized into two groups methods that directly modify network parameters e.g.
weights or network structures and techniques based on retraining the networks with augmented data .
although various dnn repair techniques are already able to fix detected erroneous behaviors it is also essential to ensure that the repairing process should not introduce unintended effects to dnns e.g.
input images that can be correctly classified by dnns should not be misclassified after repair.
conclusion and future work we proposed deeprover a fuzzing based blackbox adversarial attack for deep neural networks.
deeprover is effective and query efficient in generating adversarial examples.
moreover adversarialexamples that deeprover can find are of low distortions.
we foresee that deeprover is of great value for evaluating the adversarial robustness of neural networks in real world settings.
in our future work we are motivated to extend the deeprover technique to develop attacks constrained by other distance metrics e.g.
thel0distance metric.
we are also motivated to develop defense techniques to improve the robustness of neural networks against adversarial perturbations.