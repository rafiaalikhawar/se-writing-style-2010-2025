questions for data scientists in software engineering a replication hennie huijgens delft university of technology delft the netherlands h.k.m.huijgens tudelft.nlayushi rastogi ernst mulders delft university of technology delft the netherlands a.rastogi tudelft.nl ernst mulde.rsgeorgios gousios arie van deursen delft university of technology delft the netherlands g.gousios tudelft.nl arie.vandeursen tudelft.nl abstract in a microsoft study investigated the sort of questions that data science applied to software engineering should answer.
this resulted in questions that developers considered relevant for data scientists to answer thus providing a research agenda to the community.
fast forward to five years no further studies investigated whether the questions from the software engineers at microsoft hold for other software companies including software intensive companies with different primary focus to which we refer as software defined enterprises .
furthermore it is not evident that the problems identified five years ago are still applicable given the technological advances in software engineering.
this paper presents a study at ing a software defined enterprise in banking in which over it staff provides in house software solutions.
this paper presents a comprehensive guide of questions for data scientists selected from the previous study at microsoft along with our current work at ing.
we replicated the original microsoft study at ing looking for questions that impact both software companies and software defined enterprises and continue to impact software engineering.
we also add new questions that emerged from differences in the context of the two companies and the five years gap in between.
our results show that software engineering questions for data scientists in the software defined enterprise are largely similar to the software company albeit with exceptions.
we hope that the software engineering research community builds on the new list of questions to create a useful body of knowledge.
ccs concepts general and reference surveys and overviews .
keywords data science software engineering software analytics.
work completed during an internship at ing.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
reference format hennie huijgens ayushi rastogi ernst mulders georgios gousios and arie van deursen.
.
questions for data scientists in software engineering a replication.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
introduction software engineering researchers try solving problems that are relevant to software developers teams and organizations.
historically researchers identified these problems from their experience connections in industry and or prior research.
in however a study at microsoft systematically analyzed software engineering questions that data scientists can answer and made it accessible to a wider audience.
switching context in the past few years ing transformed itself from a finance oriented company to a software defined datadriven enterprise.
from a software engineering perspective this includes the implementation of fully automated release engineering pipelines for software development activities in more than teams performing deployments per month for applications.
these activities leave a trove of data suggesting that data scientists using e.g.
modern machine learning techniques could offer valuable and actionable insights to ing.
to that end ing needs questions that are relevant for their engineers which their data scientists can answer.
as we started looking for existing resources we came across the software engineering questions for data scientists presented in the microsoft study .
however before adopting the list we wanted to know rq to what extent do software engineering questions relevant for microsoft apply to ing five years later?
microsoft is a large software company while ing that is a fintech company using software to improve its banking solutions software defined enterprise .
moreover the two companies are at different scale.
in microsoft had more than engineers while even today ing is almost half its size with approximately it employees on a total of .
more details on the differences in the context of the two companies are available in table .
we try to understand whether the questions relevant for a software company extend to a software defined enterprise.
we compare the results of the original microsoft study with our results at ing to understand the relevance of the questions beyond microsoft but also as a guide for other software defined enterprises that are undergoing their digital transformation.
we further explore esec fse november virtual event usa hennie huijgens ayushi rastogi ernst mulders georgios gousios and arie van deursen whether the technological advances in the last five years changed the way we develop software.
to answer this question we carried out a replication of the original microsoft study at ing.
similar to the original study we conducted two surveys one to find data science problems in software engineering and second to rank the questions in the order of their relevance see figure ?
?
.
for the first survey we randomly sampled ing engineers and received responses with questions.
we grouped the questions on similarities resulting in descriptive questions.
we shared subsets of these descriptive questions with another random sample of ing engineers for ranking.
in the end we received rankings from ing engineers.
these ranked questions are the questions that engineers at ing would like data scientists to solve.
further we compare our list of questions to the original list of questions to answer our research question.
our study shows that the core software development problems relating to code e.g.
understanding code testing and quality developer productivity both individuals and team and customer are same for the software company and the software defined enterprise.
nonetheless subtle differences in the type of questions point to changes in market as well as differences in the context of the two companies.
impact of the microsoft study in order to gain a good insight into the further course of the microsoft study after it was published including any implications for research we conducted a citation analysis.
in addition we looked at studies that have not quoted the microsoft study but that are relevant to our study.
hence this section also serves as our discussion of related work.
we investigated the studies that according to google scholar quote the microsoft study.
first of all we looked at the number of times that the studies themselves were cited by other studies we limited the further analysis to studies with a citation per year greater than .
.
we then characterized studies into empirical approach reference characterization se topic and machine learning ml topic see table .
note that one paper can belong to multiple topics.
we made the following observations microsoft itself is building on its study.
of the citations come from microsoft studies itself mostly highly cited studies on se culture such as .
we notice that all citing microsoft studies use a survey among a large number of se practitioners ranging from to respondents with a median of whereas other studies based on a survey generally reach substantially lower numbers of participants.
table context of microsoft in and ing in .
microsoft ing branch software company banking fintech organization size approx.
in about engineers45 employees of which it team structure typically size teams of size development model agile scrum agile scrum kanban pipeline automation every team is different.
continuous integration in many teamscontinuous delivery as a service development practice devops biz devopstable characterizations of citing studies.
empirical approach n number of studies percentage analysis of se process data e.g.
ide survey se practitioners interview se practitioners literature review experiment case or field study reference characterization n number of studies percentage plain reference in related work reference as example for study setup study partly answers ms question study explicitly answers ms question software engineering topic n number of studies percentage software analytics data science testing debugging quality code review software engineering process software engineering culture mobile apps machine learning topic n number of studies percentage examples of machine learning applications natural language processing ensemble algorithms instance based algorithms deep learning algorithms other half of the citing studies analyze se process data and uses a survey.
looking at the empirical approach see the first sub table in table indicates that of the studies contain a quantitative component in which analysis of se process data in particular is part of the study.
good examples are .
furthermore of the citing studies uses a survey among se practitioners for example .
ten percent is based on interviews with se practitioners such as .
seven percent contains a literature review for example .
another conducts an experiment case study or field study .
only three out of studies explicitly answer a question from the initial microsoft study.
the second sub table in table shows that only studies explicitly refer their research question to an initial microsoft one .
nine studies partly try to answer a ms question .
studies refer to the original microsoft study because they used it as an example for their own study either with regard to the study design the rating approach kano or the card sorting technique .
furthermore a large part studies of the citing studies simply refers to the original microsoft study in a simple related work way.
a majority of citing studies is about software analytics testing related studies and se process.
the third sub table shows that most cited studies are about software analytics often combined with a focus on the role of the software engineer and its perceptions e.g.
.
in other cases the emphasis on software analytics is 569questions for data scientists in software engineering a replication esec fse november virtual event usa combined with a more technical focus on machine learning e.g.
.
other studies within the topic software analytics are about a variety of methods tools and techniques .
many of the studies that cite the microsoft study and which are often quoted themselves relate to testing or test automation.
fifteen studies are about testing debugging and code review .
studies handle se process related topics such as productivity of software engineers visualization and continuous delivery .
in addition studies also relate to continuous delivery pipelines and pipeline automation .
another frequent topic in citing studies is data and models including aspects of cloud development .
driven by a tendency toward automation of pipelines software generates a large amount of data.
many different data sources such as version control systems peer code review systems issue tracking systems mail archives are available for mining purposes .
of the cited studies includes some form of machine learning.
one third of the citing papers do include some form of machine learning ml ranging from applying a ml technique for analysis purposes to coming up with examples of the application of ml in practice.
as the fourth sub table in table shows studies include examples of applications of ml in practice e.g.
.
text related techniques such as nlp occur times e.g.
ensemble techniques times and instance based and deep learning both times .
four other techniques neural networks clustering decision trees and regression occur one time.
perhaps this finding supports a trend that is visible in se research where more and more machine learning techniques are being used in se analyzes and vice versa also called ai forsoftware engineering .
are about the cultural aspects of software engineering.
software analytics is an area of extensive growth .
the original microsoft study influenced ongoing research looking at the papers citing it gives the impression that it certainly did inspire other researchers and practitioners in setting up studies on software developers needs.
nine studies of the citing studies are about cultural aspects of software engineering such as topic selection in experiments characteristics of software engineers causes for frustration or challenges for software engineers .
study design our study design comprises of two parts.
in part one we replicate the original microsoft study at ing.
we follow the step by step procedure prescribed in the original study with slight modifications appropriate for our context .
the initial survey we sent the initial survey to ing software engineers randomly chosen from a group of employees working within the it department of ing in may .
unlike the microsoft study we did not offer any reward to increase the participation.
this is a deviation from the original study but aligns with the policy of ing.
out of the engineers engineers started the survey of them even this figure is a copy from the original microsoft study with numbers from our study.
the figure was re used with permission of the microsoft study authors.
figure overview of the research methodology.
filled the demographics but stopped when asked to write questions.
in the end we received questions from responses for a response rate of .
.
table shows the distribution of responses across discipline and role.
.
coding and categorization next we did an open card sort to group questions into categories.
our card sort was open meaning that we coded independently table distribution of responses based on discipline and role in the initial survey as well as rating survey.
discipline initial survey rating survey development testing .
.
project management .
.
other engineering e.g.
architect .
.
non engineering .
.
current role initial survey rating survey developer .
.
lead .
.
architect .
.
manager executive .
.
other .
.
570esec fse november virtual event usa hennie huijgens ayushi rastogi ernst mulders georgios gousios and arie van deursen from the microsoft study.
to create independent codes the first author who did a majority of the coding did not study the microsoft paper before or during the replication.
the other authors knew the paper from before and merely skimmed the methodology section for replication.
we let the groups emerge and evolve during the sorting process.
this process comprised of three phases.
in preparation phase we created a card for each question.
questions to were tagged by the second author.
questions to were tagged by the fourth author.
questions to were tagged by both the second and the fourth author.
the tags of questions to were discussed by both the second and fourth author and based on their discussion final tags were prepared.
the remaining questions to were then tagged by the first author based on the tags from the previous step.
we discarded cards that made general comments on software development and did not inquire any specific topic.
inexecution phase cards were sorted into meaningful groups and were assigned a descriptive title.
similar to the microsoft study the questions were not easy to work with many questions were same or similar to one another most were quite verbose while others were overly specific.
we distilled them into a set of so called descriptive questions that more concisely describe each category and subcategory .
in this step out of the questions questions were discarded and the remaining questions were divided into sub categories.
an example of reaching descriptive question is presented below1 what factors affect the composition of devops teams?
from the following respondents questions cent would it be better to create specialized development teams instead of devops teams?
cent what is your idea of an ideal team that should develop software?
how many and what kind of people should be part of it?
finally in analysis phase we created abstract hierarchies to deduce general categories and themes.
in total we created descriptive questions a full list of which is available in the technical report .
.
the rating survey we created a second survey to rate the descriptive questions.
we split the questionnaire into eight component blocks similar to the microsoft study and sent component blocks to potential respondents.
the idea behind using the split questionnaire survey design is to avoid low response rate.
each participant received a block of questions along with a text in your opinion how important is it to have a software data analytics team answer this question?
with possible answers as essential worthwhile unimportant unwise and i don t understand .
the rating survey was sent to the remaining software engineers at ing.
here too engineers started the survey but many of them did not complete it drop out rate .
finally we received responses for a somewhat low response rate of .
on an average each question received ratings making the resulting ranks stable.
table shows the distribution 1a closed balloon indicates a respondent question an open balloon indicates a descriptive question.of responses for the rating survey based on discipline and current role.
.
.
top rated bottom rated questions.
finally to rank each question we dichotomized the ordinal kano scale avoiding any scale violations .
we computed the following percentages for each descriptive question percentage of essential responses among all the responses essential essential w orthwhile unimportant unwise percentage of essential and worthwhile responses among all the responses to which we refer as worthwhile essential w orthwhile essential w orthwhile unimportant unwise percentage of unwise responses among all the responses unwise essential w orthwhile unimportant unwise we rank each question based on the above percentages with the top rank having the highest percentage in a dimension essential worthwhile or unwise .
table and table presents the most desired top essential top worthwhile and the most undesired top unwise descriptive questions.
for all questions and their rank see the technical report .
.
.
rating by demographics.
unlike the microsoft study we did not have employee database to rank responses based on demographics and privacy regulations prevented us from asking people related aspects such as years of experience another deviation from the original study .
nonetheless in both the initial and the rating survey we asked the following professional background data from the participants discipline participants were asked to indicate their primary working area development test project management other engineer e.g.
architect lead or other non engineer only one selection was possible .
current role participants were asked to indicate their current role individual contributor lead architect manager executive orother more selections were possible .
to investigate the relations of descriptive questions to professional background discipline or current role we built stepwise logistic regression models.
we build our own models since the referenced study did not share scripts to run statistical tests although we did follow their procedure as is.
stepwise regression eliminated professional backgrounds that did not improve the model for a given question and a response.
in addition we removed professional backgrounds for which the coefficient in the model was not statistically significant at p value .
.
for each of the questions we built a model with essential response yes no as a dependent variable and professional background as independent variable.
we built similar models for worthwhile and unwise responses.
in total we built models three for each of the descriptive questions.
.
comparison of questions as a preliminary analysis we start by looking at the similarities and differences in the broader themes or categories in both the studies.
571questions for data scientists in software engineering a replication esec fse november virtual event usa table ing categories and questions mapped on to the microsoft categories ing study microsoft study category cards subcategoriesdescriptive questionscards subcategoriesdescriptive questionsdifference ing compared to ms teams and collaboration tc testing practices tp services svc reuse and shared components rsc customers and requirements cr software development lifecycle sl development practices dp bug measurements bug productivity prod evaluating quality eq development best practices best software development process proc discarded cards total cards table sorted on the percentage difference in the number of questions in the ing study compared to the microsoft study.
then for each theme we see how the prominent questions in ing compare against the prominent questions at microsoft.
to make the comparison systematic we followed a two step approach.
first we ran word count on the questions from both the companies presenting a text based comparison to identify broad differences.
further the first two authors manually analyzed top essential questions from the two companies in detail.
the authors drew affinity diagrams using microsoft questions see figure ?
?
and appended related questions from ing to it.
in case no cluster fits a question a new cluster is created.
this resulted in three types of clusters match and no match addition of ing questions and deletion of microsoft questions .
analyses of the three clusters and the frequency distribution of questions in addition to the previous three analyses present insights into our research question.
results the original microsoft study came up with questions that software engineers want data scientists to answer.
replicating the original study at ing we identified data science questions.
this section presents a comparison of the two sets of questions based on category type of questions within categories top rated questions bottom rated questions and questions relevant for different demographics.
next we compare the questions from the two companies using word count and affinity diagrams to answer our research question.
.
categories we noticed that some of our categories directly match the microsoft study.
other categories however can be mapped to one or more categories of the microsoft study.
no new emergent category in our study indicates that broadly there are no differences between the questions for a software defined enterprise from a software company.
for further analysis we map our categories on to theirs details on which are available in table .
next we explore the essential questions at ing and their distinguishing link to the questions from the microsoft study.
.
.
bug measures bug .
the essential questions at ing relate to the effort spent on bugs methods to prevent security relatedvulnerabilities and the relationship between bugs and specific ingrelated development platforms.
how does the effort spent on fixing vulnerabilities and bugs relate to effort spent on writing software correctly from the start?
what methods are most effective in preventing security related vulnerabilities or bugs from being introduced in software code?
.
.
development practices dp .
the performance and productivity of devops teams was found in a number of questions including team happiness and work pleasure question ways of decision making non overlapping development activities in the same environment product ownership and business responsibilities licensing of tools and the choice of a data modeling approach.
what factors affect the performance and productivity of devops teams with regard to team happiness and pleasure in your work?
what factors affect the performance and productivity of devops teams with regard to evidence based decision making versus decisionmaking based on expert opinions?
what factors affect the performance and productivity of devops teams with regard to simultaneous slow and fast developments at the same time in the same environment?
.
.
development best practices best .
this category emphasized best or worst development practices relating to technology selection effectiveness and choice of tools.
how can we make sure that we build for re usability and scalability?
what factors affect high performance teams?
when do you remove an old module that you think is not being used anymore?
.
.
testing practices tp .
questions here ranged from automated test data generation on demand provisioning of test environments testing of high volumes to question like should we let loose chaos monkey to what extent does on demand provisioning of development and test environments including up to date data affect delivery of software solutions?
572esec fse november virtual event usa hennie huijgens ayushi rastogi ernst mulders georgios gousios and arie van deursen figure analysis of ing and ms questions.
what factors affect performance testing on high data volumes?
how can a system for semi automated crud test data generation improve delivery of software solutions?
should we let loose chaos monkey like netflix?
.
.
evaluating quality eq .
this category included questions on code analysis ways to assess quality of software code and effectiveness of testing practices.
what methods can be applied to analyze whether software code is working as expected?
to what extent does testability of software code affect the quality of code?
.
.
customers and requirements cr .
the essential questions related to measure customer value requirement validation and the use of formal models.
notably questions relating to development trade offs such as backward compatibility or the impact of testing in production appeared in the microsoft study but not ours.
how to measure the customer value of a software product?
how can requirements be validated before starting actual software development?
how can user feedback be integrated in an efficient and effective way into software code?
.
.
software development lifecycle sl .
questions in this category related to the effectiveness and performance in lead time cost and quality same as the microsoft study but also questions relating to security and risk from a management perspective.
what factors affect providing new technologies to consumers and can implementations of new technology be internally and externally benchmarked?
what factors affect estimation of lead time cost and quality of software deliveries?
.
.
software development process proc .
our questions related to development processes technology selection and deployment of software solutions.
at microsoft in contrast questions related to the choice of software methodology e.g.
ways in which agile is better than waterfall?
and benefits of pair programming .
we also noticed that at ing topics like the effects of automated continuous delivery pipeline popped up which were not seen in the microsoft study.
how can we improve the deployment process in devops teams?
does a focus on quick release of features and bug fixes into production help to achieve confidence and agility?
.
.
productivity prod .
this category had questions on the productivity of devops teams but also individual developers ranked essential.
notably questions related to the measurement of individual developers e.g.
the questions mentioned below regarding great coder and open spaces were often ranked unwise .
quite unlike the microsoft study where respondents considered these questions as unwise engineers at ing had a mixed opinion.
what factors affect the performance of devops teams and the quality of software code with regard to quantity and quality of environments?
are developers working in an open space with several teams more effective or less than developers working in a room with just their team?
what makes a great coder?
what aspects affect the performance of devops teams and the quality of software with regard to characteristics of an individual software engineer?
.
.
teams and collaborations tc .
essential questions here are typically about dependencies between teams team composition team maturity and knowledge sharing among teams.
to what extent do dependencies on other teams affect team performance?
how does team maturity affect code quality and incidents?
what factors affect the composition of devops teams?
.
top rated questions table shows top essential and top worthwhile or higher questions.
interestingly only two out of the top essential questions were a part of the top worthwhile or higher questions and none vice versa.
this potentially means that our participants are more pronounced and opt for essential or worthwhile only when they feel so.
culture can be another possible reason since all participants at ing are located in one country while participants of the microsoft study were more diverse .
our top questions are on development processes technology selection and deployment of software solutions.
the top related questions at microsoft in contrast relates to the choice of software methodology e.g.
ways in which agile is better than waterfall?
and benefits of pair programming .
we also noticed that in our study topics like the effects of automated continuous delivery pipeline popped up which were not seen in the microsoft study.
notably a large fraction of the top essential or worthwhile or higher questions at microsoft out of including top relates to customers.
this suggests that for microsoft customer benefit is most important or perhaps one of the most important question.
our study in contrast paints a very different picture.
only two out of the questions in the initial survey mentioned the word customer and only one of those questions made it to the top q58 how to measure the customer value of a software product at rank essential .
this question is in line with the microsoft study marked with icon usr in table .
573questions for data scientists in software engineering a replication esec fse november virtual event usa table questions with the highest essential and worthwhile or higher percentages.
percentages rank question category essential worthwhile unwise essential worthwhile unwise 3q143 what factors affect the performance and productivity of devops teams with regard to team happiness and pleasure in your work?dp .
.
.
q98 how does on demand provisoning of develop and test environments including up to date data affect delivery of software solutions?tp q37 how can we make sure that we build for reusability and scalability?
best .
.
.
3q145 what factors affect the performance of devops teams and the quality of software code with regard to quantity and quality of environments?prod .
.
.
3q114 what factors affect high performance teams?
best .
.
.
q154 what factors affect understandability and readability of software code for other developers?dp .
.
.
3q76 how can we improve the deployment process in devops teams?
proc .
.
.
q36 how does the effort spend on fixing vulnerabilities and bugs relate to effort spend on writing software correctly from the start?bug .
.
.
3q53 how does a continuous delivery pipeline with automated testing and migrating including rollback facilities affect the performance of devops teams and the quality of software?proc .
.
.
q22 how can requirements be validated before starting actual software development?cr .
.
.
q123 what factors affect performance testing on high data volumes?
tp .
.
.
usrq58 how to measure the customer value of a software product?
cr .
.
.
q88 to what extent does testability affect the quality of software code?
eq .
.
.
3q67 to what extent do automated checks of coding conventions code quality code complexity and test coverage affect the quality of software systems and the performance of devops teams?eq .
.
.
q11 how can a system for semi automated crud test data generation improve delivery of software solutions?tp .
.
.
3q104 what aspects affect the performance of devops teams and the quality of software with regard to software architecture?prod .
.
.
q19 how can editors help software developers to document their public functions in a way that it is available for other developers?cr .
.
.
q122 what factors affect maintainability of software systems?
eq .
.
.
q80 how do automated controls within the continuous delivery pipeline affect the effort spent on risk and security?dp .
.
.
table is sorted on rank essential.
the icon usrindicates customer related questions 3indicates questions that focus on the engineer and the effects of software development practices and processes on her work and indicates quality related questions.
another eight essential or worthwhile or higher questions in the microsoft study marked with icon focus on the engineer and the effects of software development practices and processes on her work.
in our study we identified nine questions with this icon.
in addition to the focus on individual engineer many of the questions in our study relates to the concept of the devops team.
overall it seems that microsoft has a big focus on customer while ing emphasizes on the engineering team itself.
finally seven questions in the microsoft study marked with the icon were about qualityrelated issues same as ours with eleven questions .
.
bottom rated questions table shows the top unwise questions.
the most unwise question q27 at ing is the use of domain specific language for use by non experts.
in the microsoft study the top five unwise questions were all about a fear that respondents had of being rated.
this effect can be seen in our study too two of the top ten unwise questions q161 and q30 relate to measuring the performance of individual engineers but not nearly as strongly as in the microsoft study.
respondents in our study are torn on this topic q161 and q30 are ranked as unwise by respectively .
and .
of the respondents but also ranked as essential by another group of .
and .
of the respondents.
also it was interesting tosee that measuring and benchmarking time to market of software solutions q38 is one of the top unwise questions.
it indicates resistance against comparing departments based on key performance indicators like the time to market.
.
rating by demographics table shows essential questions for different disciplines developer tester project management and roles manager individual contributor architect .
the complete inventory of questions for worthwhile or higher and unwise responses is present in the technical report .
.
.
discipline.
microsoft study showed tester as a specific discipline mainly interested in test suites bugs and product quality.
we do not see the discipline tester in our study.
this can be seen in table in which overall scores relating to test are low and highest for development .
software engineers in the devops teams at ing consider themselves to be generic developers and testing is an integrated part of the discipline developer .
both developers and testers are for example significantly interested in the testability of software code and the quality of software related to an agile way of working and working in devops teams.
other findings relate to developers being significantly interested in team performance e.g.
regarding practices of good software teams from the perspective 574esec fse november virtual event usa hennie huijgens ayushi rastogi ernst mulders georgios gousios and arie van deursen table questions with the highest unwise percentages opposition .
percentages rank question category essential worthwhile unwise essential worthwhile unwise q27 how can software solutions in one common language be developed in a way that it is applicable to every person regardless of ones interest in software development?cr .
.
.
q39 how can windows server images be created in order to facilitate testing within a continuous delivery pipeline?dp .
.
.
q170 why do many developers focus on the newest of the newest?
why don t they leave this to a small group in order to use time and effort more efficiently?dp .
.
.
q161 what makes a great coder?
what aspects affect the performance of devops teams and the quality of software with regard to characteristics of an individual software engineer?prod .
.
.
q134 what factors affect tfs team foundation services a microsoft product that provides source code management with regard to working with automated pipelines?best .
.
.
q30 how can the performance of individual software engineers be benchmarked internally ing and externally with other companies?prod .
.
.
q77 to what extent does changing of requirements during development affect the delivery of software solutions?proc .
.
.
q21 how can pl1 software code be converted to cobol code while maintaining readability of the code in order to simplify an application environment?best .
.
.
q38 how can we measure the time to market of software solutions delivered within a department at ing in order to benchmark the performance of that department against others.dp .
.
.
q149 what factors affect the use of machine learning in software development over a period of ten years?dp .
.
.
q28 how can the cost of data be identified in order to sign a price tag to data?
dp .
.
.
table is sorted on rank unwise.
table statistically significant rating differences for the response essential by professional background.
discipline question category response dev test pm q2 are there practices of good software teams from the perspective of releasing software solutions into production?proc essential .
.
.
q21 how can pl1 software code be converted to cobol code while maintaining readability of the code in order to simplify an application environment?best essential .
.
.
q28 how can the cost of data be identified in order to sign a price tag to data?
dp essential .
.
.
q46 how do static code analysis tools such as fortify and sonar influence the quality of software engineering products?best essential .
.
.
q88 to what extent does testability affect the quality of software code?
eq essential .
.
.
q89 how does time spent in terms of full time versus part time of a scrum master affect the delivery of software solutions?proc essential .
.
.
q95 to what extent do dependencies on other teams affect team performance?
tc essential .
.
.
q97 how does documentation during software maintenance affect delivery of software solutions?
tp essential .
.
.
q110 what factors affect data analytics with regard to the use of external sources such as market research reports and follow market trends and let individual teams handle their local evolution?proc essential .
.
.
q162 what methods are most effective in preventing security related vulnerabilities or bugs from being introduced in software code?bug essential .
.
.
current role question category response manager individual architect q2 are there practices of good software teams from the perspective of releasing software solutions into production?proc essential .
.
.
q46 how do static code analysis tools such as fortify and sonar influence the quality of software engineering products?best essential .
.
.
q97 how does documentation during software maintenance affect delivery of software solutions?
tp essential .
.
.
q153 what factors affect trunk based development a source control branching model where developers collaborate on code in a single branch with regard to quality of software code?best essential .
.
.
the professional background with the highest rating is highlighted in bold .
questions that are also in table are shown in italics .
the role manager includes the responses for manager and lead .
of releasing software into production the use of data analytics to improve individual teams and dependencies on other teams.
.
.
role.
more individual contributors e.g.
developers than managers are interested in good practices for software teams to 575questions for data scientists in software engineering a replication esec fse november virtual event usa release software into production.
more managers than individual contributors on the other hand are interested in how software can help realize new policies and changes in the way of working the relationship between documentation and maintenance of software and to what extent the use of static code analysis tools such as fortify and sonar can affect the quality of software.
.
compare ing and microsoft questions a comparison of the top words from each company see table shows that a majority of the popular themes are same e.g code test software and quality.
.
the subtle differences however exists relating to rank words in italics do not make it to top in another company and use of the word in another company underlined .
a subset of these differences can be attributed to differences in terminology.
for instance microsoft uses terms like employee employees and team teams while its equivalent at ing are team squad and engineer.
apart from this microsoft questions focused more on bugs cost time customers and tools while ing employees talked about version problem systems process and impact.
next we inferred themes from the clusters in the affinity diagram organically merging into three broad categories relating to code like understanding code testing quality developers individual and team productivity and customers note that while customers did not make it to the top essential questions they were important in the top .
the themes are automated testing testing understanding code documentation formal methods code review debugging risk refactoring deployment bug fixing legacy software quality requirement release cloud customer estimation team productivity employer productivity cost team awareness and agile working.
investigating each theme and category in detail we noticed that despite minor differences in the individual questions some questions are broad in one company and specific in another largely the key questions remain the same.
for instance employees at both the companies find questions relating to team productivity and employee productivity important and yet assessing and comparing individual employees is undesirable.
there were table top words from questions at ing and microsoft microsoft ing word count word count code coding testing debugging test tests testing code coding software software employee employees team squad quality development bugs version library development data cost incident issue problem team teams security risk time system systems customer customers quality impact production productivity engineer project process tools impact top words sorted on count from microsoft and ing study.
words in the top of one company and not the other are printed in italic.
words in one list and not the other are underlined .however subtle differences.
for instance in the microsoft study we noticed a few questions eliciting the need for agile vs. waterfall as well as automated testing.
in the ing study however we do not see such questions.
rather we see questions relating to the functional aspects of agile and automated testing.
another subtle difference between the two companies is relating to code size.
while not stated explicitly from the nature of questions it seems that the software teams at microsoft are dealing with a large legacy codebase.
this was reflected in questions relating to team awareness code monitoring backward compatibility and refactoring.
such questions however did not occur in ing.
other than the above we saw cloud related questions appearing in the microsoft study only while deployment related questions appeared in ing only.
in a nutshell the core software development challenges of ing are consistent with microsoft.
there are although some nuanced differences which relate to the evolution of software market in the last five years as well as differences in the characteristics of the two companies.
discussion in this section we discuss potential explanations for the differences in the list of questions found in our study compared to the microsoft study.
we saw questions eliciting the need of agile in microsoft study while at ing the questions related to functional aspects.
our hypothesis here is that in the last five years there is a change in the market while in the questions on the adoption of agile and automated testing were common in agile and automated testing became the norm.
we noticed that many questions at microsoft deals with the scale of legacy code while no such question appeared at ing.
one potential explanation for the observation can be that software systems at ing are not of the same scale as microsoft.
nonetheless it remains a lesson that in the next years ing can also be dealing with the complexity of large code base as microsoft is experiencing today.
finally some questions appeared in only one organization.
we believe that these observations have something to do with the individual practices followed at microsoft and ing.
the deployment related questions at ing might be a result of the adoption of continuous delivery as a service.
surprisingly we did not see any finance related questions in the ing study.
ing is a finance based company and we expected to see some issues relating to both finance and software appear.
we noticed that employees often talked about security but no real finance related questions appear.
one explanation for this observation can be that the data science challenges relating to software development are independent of the actual field to which it is applied.
supporting this argument questions from microsoft also did not bring up any product specific details.
another potential explanation can be that through our question we anchored our respondents into asking software development related questions only.
.
implications one of the key findings of this paper is a list of questions that software engineers in a large software driven organization would like to see answered in order to optimize their software development activities.
from this we see implications both in terms of practice and industry.
576esec fse november virtual event usa hennie huijgens ayushi rastogi ernst mulders georgios gousios and arie van deursen from a practical perspective our study offers a new way of thinking to software development organizations who care about their development processes.
the questions originally raised by microsoft are not just relevant to one of the largest tech companies in the world but also to large software defined enterprises active outside the tech sector proper.
inspired by these questions an organization may select the most relevant ones and seek ways to address them.
while some questions are fundamentally hard to answer organizations can make a starting point by collecting relevant data about their development processes.
this then can help to make the development process itself more and more datadriven.
this is exactly how ing intends to use the questions and we believe companies around the world can follow suit.
from a research perspective we have seen that the original microsoft study has generated a series of papers that apply some form of machine learning to address the challenges raised in that study.
in the research community ai for software engineering is an increasingly important topic with many papers appearing that seek to apply machine learning to address software engineering problems.
our study aims to add urgency and direction to this emerging field by highlighting not just which questions canbe answered but which ones should be answered from a practitioner perspective.
.
threats to validity while our study expands the external validity of the original study the fact remains that the two lists of questions are based on just two companies which are both large organizations with over software developers.
our study highlights relevance to the fintech sector but it would be interesting to see further replications for example in the automotive or health care sector with different regulatory and additional safety constraints.
we expect that many of the questions are also relevant to smaller organizations especially given the agile way of working at ing.
nevertheless it will be worthwhile to further explore this.
from a perspective of internal validity creating codes independent of the prior study is challenging.
it is possible that the similarities and differences seen compared to the microsoft study relates to factors e.g.
researcher bias other than the actual data.
we tried mitigating it by limiting our exposure to the previous study not involving authors from the microsoft study and multiple authors generating codes independently.
nonetheless these biases are likely to exist.
for reasons of replication we have used where possible the same survey questions method of analysis and division into work area and discipline as in the microsoft study .
apart from positive effects this choice also had a negative effect with regard to analysis of demographics mainly due to the fact that ing uses a different way of working including corresponding roles and team structure than within microsoft.
especially mapping the professional background discipline of the original study on the demographic discipline as applied within ing was challenging.
ing works with devops teams where an engineer fulfills both the area of developer and that of tester.
as a result testers wereunder represented in both of the surveys we conducted.
as a mitigation measure we therefore opted for combining the results of developers and testers in the further analysis.
another potential threat is sensitivity of the ranks which mostly occurs at the extreme sides of the ranking when e.g.
none of the participants label a question as unwise .
in our study on average each question received ratings and hence sensitivity of ranks is unlikely.
the presented results are free from corporate influence including microsoft.
a number of stakeholders at ing cio corporate communications reviewed the submitted paper and approved it without any changes.
although self censorship by the authors remain a potential threat.
researchers may have their biases which can potentially influence the results.
as also emphasized in related work on replications our study seeks to replicate earlier findings in a different context e.g.
other companies and during a different time environments and perceptions of engineers do change over time .
in order to facilitate future replication of our study we make the total set of descriptive questions and additional info on results of our tests available in our technical report.
conclusion conducted at ing a software defined enterprise providing banking solutions this study presents questions that software engineers at ing would like data scientists to answer.
this study is a replication of a similar study at software company microsoft which resulted in questions for data scientists.
further we went a step beyond to investigate the applicability of microsoft s questions in ing as well as changes in trends over the last five years.
we compared the two lists of questions and found that the core software development challenges relating to code developer and customer remain the same.
nonetheless we observed subtle differences relating to the technology and software process developments e.g.
currently the debate about agile versus waterfall is now largely absent and differences in the two organizations e.g.
microsoft s focus on solving problems with a large code bases and ing s challenges with continuous deployment .
we complete our analysis with a report on the impact microsoft study generated also indicating the impact that our study is capable to generate.
a thorough understanding of key questions software engineers have that can be answered by data scientists is of crucial importance to both the research community and modern software engineering practice.
our study aims to contribute to this understanding.
we call on other companies large and small to conduct a similar analysis in order to transform a software engineering into a data driven endeavour addressing the most pressing questions.