patch synthesis for property repair of deep neural networks zhiming chi1 jianan ma3 pengfei yang5 cheng chao huang6 renjue li1 jingyi wang4 xiaowei huang7and lijun zhang1 1key laboratory of system software chinese academy of sciences and state key laboratory of computer science institute of software chinese academy of sciences beijing china 2university of chinese academy of sciences beijing china 3school of cyberspace hangzhou dianzi university hangzhou china 4zhejiang university hangzhou china 5college of computer and information science software college southwest university chongqing china 6nanjing institute of software technology chinese academy of sciences nanjing china 7university of liverpool liverpool united kingdom email chizm ios.ac.cn majianannn gmail.com ypfbest001 swu.edu.cn chengchao njis.ac.cn lirj19 ios.ac.cn wangjyee zju.edu.cn xiaowei.huang liverpool.ac.uk zhanglj ios.ac.cn abstract deep neural networks dnns are prone to various dependability issues such as adversarial attacks which hinder their adoption in safety critical domains.
recently nn repair techniques have been proposed to address these issues while preserving original performance by locating and modifying guilty neurons and their parameters.
however existing repair approaches are often limited to specific data sets and do not provide theoretical guarantees for the effectiveness of the repairs.
to address these limitations we introduce p atch pro a novel patch based approach for property level repair of dnns focusing on local robustness.
the key idea behind p atch pro is to construct patch modules that when integrated with the original network provide specialized repairs for all samples within the robustness neighborhood while maintaining the network s original performance.
our method incorporates formal verification and a heuristic mechanism for allocating patch modules enabling it to defend against adversarial attacks and generalize to other inputs.
p atch prodemonstrates superior efficiency scalability and repair success rates compared to existing dnn repair methods i.e.
realizing provable property level repair for cases across multiple high dimensional datasets.
i. i ntroduction in recent years deep neural networks dnns have achieved significant advancements in various domains including computer vision natural language processing and speech recognition .
despite these advancements the adoption of dnns in safety critical domains has been slow due to concerns regarding their dependability.
a major concern is the vulnerability of dnns to adversarial attacks where adversaries can manipulate input data in ways that are imperceptible to humans but can cause the model to make incorrect decisions.
this vulnerability poses serious safety risks in applications such as autonomous vehicles and medical diagnosis .
therefore it is crucial to regularly update the dnn to mitigate the risks associated with these errors and ensure the reliability of the network in practical applications.
is the corresponding author.dnn repair techniques have been proposed to address the problem involving rectifying errors in the network by modifying its architecture or parameters.
compared to traditional methods such as adversarial training input sanitization fine tuning transfer learning and data augmentation neuron level fault localization and repair methods offer a more targeted approach by identifying and correcting errors at the individual neuron level while not affecting the overall network performance.
despite significant advancements in this field several limitations remain.
first neuron level repair techniques which rely on a limited number of samples often struggle to provide robust defense against adversarial attacks due to the inherent complexity of these attacks compared to simpler threats like backdoor attacks.
adversarial attacks involve intricate mixtures of features making it difficult to generalize parameter adjustments from a small dataset.
second existing repair methods typically focus on specific data for repair and fail to generalize to the property level e.g.
local robustness which limits their effectiveness in addressing a broad range of adversarial scenarios.
this motivates us to explore property repair of dnns specifically to fix the safety properties that neural networks violate in certain input regions.
third while provable repair methods such as prdnn reassure and aprnn can conduct propertybased error correction they achieve this by ensuring that outputs meet constraints on the vertices of the input region as a polyhedron.
however the number of vertices increases exponentially with data dimensionality which makes these methods inefficient for high dimensional data.
in this work we aim to address these limitations by proposing a novel patch based method to achieve provable repair on the property level.
specifically we focus on correcting potential adversarial samples in the infinite set of high dimensional points within a certain neighborhood of these error samples i.e.
satisfying local robustness property.
to achieve this ourarxiv .01642v2 feb 2025key idea is to use formal verification to help construct a separate patch module in the form of a fully connected neural network structure for each neighborhood outside of the original network.
such a patch based method allows us not only to ensure that the infinite set of points within the error sample s neighborhood repaired but also to maintain the performance of the original network unaffected.
to construct such a patch we utilize reachability analysis through linear relaxation for the provable training of these patch modules.
specifically we use the verification method deeppoly to create a linear relaxation of the output neurons.
this approximation is employed to determine the distance between the targeted behavior and the current behavior serving as the loss function.
the patch modules are then trained to minimize this distance.
once the loss function reaches zero the patch modules will offer provable repairs for adversarial attacks within the perturbation region.
to ensure that each patch module addresses specific neighborhoods our approach employs an external indicator that identifies inputs within a particular sample s local neighborhood and assigns the appropriate patch modules for repair.
this indicator allows the same patch module to effectively repair adversarial attacks within the same perturbation region.
for adversarial samples outside the repaired property the indicator utilizes a heuristic allocation mechanism to assign suitable patch modules thereby enhancing the generalization of the repair.
this comprehensive framework not only improves the network s resilience against adversarial attacks but also maintains its accuracy.
by leveraging reachability analysis and dedicated patch modules we provide provable repairs for adversarial samples significantly boosting the network s robustness.
additionally to extend this local robustness across the entire dataset we integrate the external indicator with the original network ensuring that all samples including those beyond the initially repaired property benefit from heuristic patch module allocation.
this approach effectively combines local and global repair strategies to enhance overall network robustness.
we summarize our contributions as follows we introduce p atch pro a novel approach that integrates a loss function derived from formal verification to train patch modules within the original network.
this method effectively repairs high dimensional infinite point sets within the polyhedron neighborhood of adversarial samples while preserving the network s performance.
the approach also includes a heuristic allocation mechanism which ensures the generalization of local robustness repair by seamlessly integrating patch modules into the original network architecture.
to tackle the efficiency challenges of formal verification in large scale dnns we utilize patch modules to perform repairs in the feature space of the networks.
this strategy enables our method to scale effectively across various network architectures ensuring both efficient and practical implementation.
we thoroughly evaluate p atch proon three diversedatasets and multiple dnn architectures.
through extensive comparisons with state of the art repair and adversarial training techniques our method consistently demonstrates superior efficiency scalability and generalization capabilities.
it shows significant improvement in handling general inputs thereby greatly enhancing the overall robustness of the network.
ii.
p reliminary in this section we recall some basic notations of dnn repair.
a deep neural network is a function n rn0 rnlthat maps an input x rn0to an output y rnl.
we usually visualize a dnn nas a sequence of llayers where the ith layer contains nineurons each representing a real variable.
between two adjacent layers is typically a composition of an affine function and a non linear activation function and the dnn nis the composition of the functions between layers.
in many applications dnns are serving for classification tasks.
in such a classification dnn n rn0 rnl every output dimension corresponds to a classification label and the one with the maximum output value is the classification result that the dnn ngives i.e.
cn x arg max i nln x i where n x iis the ith entry of the vector n x .
the notion of safety properties pertains to assertions that guarantee the absence of undesirable behavior.
within the context of dnns a safety property demands that a dnn operates correctly within a specified input range.
definition .
a safety property is a triple n x q where nis a dnn x rdinandq rdoutare the subset of input and output spaces of the neural network n. the property n x q is satisfied if and only if n x qfor all x x. a local robustness property of a classification dnn nrequires that for any input xin a given neighborhood b x0 r of an input x0 its classification should always be consistent with x0 where a neighborhood of an input x0is usually defined as a closed ball b x0 r x rn0 x x0 r is the l norm and r 0is the radius.
formally it can be defined as follows the local robustness of a dnn refers to its ability to maintain stable and consistent predictions in the vicinity of its in distribution data points even in the presence of small perturbations or variations in the input.
here a neighborhood of an input x0is usually defined as a closed ball b x0 r x rn0 x x0 r where is the l norm and r 0is the radius.
formally a local robustness property of a classification dnn nrequires that for any input xin a given neighborhood b x0 r of an input x0 its classification should always be consistent with x0 i.e.
x b x0 r cn x cn x0 .
we denote this local robustness property as n b x0 r i.e.
n b x0 r n b x0 r y rnl yi ycn x0 i .
.
.
n l and thus it is a safety property.
in this work we focus on the problem of repairing adversarial attacks with limited adversarial samples.
different from repairing backdoor attacks not only does the buggy 2behavior of the given adversarial samples need fixing but we also require that there should be no adversarial attacks around given adversarial samples and that this enhancement of local robustness should generalize to samples across the whole dataset.
now we formally state the problem of repairing adversarial attacks as follows given a dnn nand a set of adversarial samples x i n i where ncan be significantly smaller than the size of the training set and each x iis obtained by an adversarial attack on an input xiwith a given radius r we need to construct a dnn fwhich is locally robust on every b xi r while the accuracy is maintained and local robustness of other inputs is potentially improved.
iii.
m ethodology we focus on fixing adversarial attacks with limited data in this work and the aims of the repair are at least threefold for a given radius r the buggy behaviors of x i n i 1are fixed the dnn is locally robust in b xi r for1 i n the accuracy of the dnn is maintained and for as many input xin the dataset as possible the dnn is locally robust in b x r .
in this work we propose a patch based repair method.
a patch refers to a specific modification or alteration made to a software system or codebase it is a discrete set of changes applied to fix a bug enhance functionality or address security vulnerabilities.
patch based dnn repair involves the integration of an external indicator designed to identify buggy inputs followed by the application of specialized patch modules to repair input sets which have similar behaviors.
each indicator here corresponds precisely to a robustness neighborhood b xi r that requires repair and we can leverage verification tools to ensure that the repair within each robustness neighborhood is provable.
furthermore for other inputs in the dataset we can heuristically match patches that maximize their robustness thereby endowing this repair approach with global generalization.
in this section we propose a patch based repair method named p atch proto defend from adversarial attacks with limited data.
a. structure of the repaired dnn the main produce of the repaired network is shown in fig.
.
for an input x rn0 the role of the indicator is to select the appropriate patches that when applied yields the sum of the outputs of the patch modules and the output of the original dnn for x. this sum represents the output obtained after the repair process.
the indicator function is defined as definition .
letc x1 .
.
.
x m be a finite sequence of input properties.
the indicator function ic rn0 m outputs in the jth entry where j .
.
.
m as ic x j ifx xj otherwise .
fig.
.
the architecture of a dnn repaired by p atch pro.
it contains multiple patch networks for each input properties.
each of them is enabled or disabled according to the allocation signal or determined by the indicator.
the blue lines highlights the indicator s workflow.
the final output is the sum of the outputs of all the enabled patches and the original network.
typically an input property xiis a subset of rn0 and we define x xiiffx xi.
upon classification by the indicator a set of patch modules is deployed to perform the repair.
here each patch module is specifically tailored to address an input set with the same local robustness property.
the implementation of a patch module is a fully connected neural network in this work.
for an input xthat does not satisfy any input properties i.e.
x sn i 1b xi r there is no existing specific patch module but it may still suffer from adversarial attacks within its neighborhood.
in this situation we heuristically allocate some patch modules to this input to defend from adversarial attacks.
formally the structure of the repaired dnn is as follows definition .
a repaired dnn is a tuple f n c p where n rn0 rnlis the original dnn c x1 .
.
.
x m be a finite sequence of input properties p p1 .
.
.
p m tis a finite sequence of patch modules each of which is a fully connected neural network and rn0 sn i 1xi ... m is a patch allocation function.
the semantics of the repaired dnn fis a function f rn0 rnl x7 n x ic x tp x ifx sn i 1xi n x p j x pj x otherwise where p x p1 x .
.
.
p m x t. in this work we set c b xi r i .
.
.
n i.e.
the indicator judges which robustness region the input belongs to and the corresponding patch module defends against adversarial attacks in this specific region.
3b.
training the patch modules the first challenge is to train the patch modules which are each implemented as a fully connected neural network to repair each robustness property f b xi r with a provable guarantee.
to achieve provability we employ formal verification in training these patch modules.
the result of verification on a robustness property can be transformed into a loss function once this loss reaches zero the property is verified to be true and the patch module in this stage can defend against all possible adversarial attacks in this robustness region.
this idea has been widely adopted in dnn repair .
in this work we employ deeppoly as the verification engine1.
deeppoly uses abstract interpretation to give every neuron an upper lower bound in the form of an affine function where only variables in previous layers occur and the numerical bound can be derived by propagating backwards these affine upper lower bounds to the input layer.
recall that a local robustness property f b xi r holds iff for any x b xi r andl l0 cf xi f x l f x l0 so deeppoly calculates the abstraction of fonb xi r for every neuron and the expressions f x l f x l0forl l0.
from the abstraction we can obtain an affine function in which there are only input variables as a sound upper bound of f x l f x l0onb xi r i.e.
x b xi r f x l f x l0 t lx l where l rn0and l rare constants.
it is easy to obtain the numerical upper bound of t lx lon the box region b xi r .
if this upper bound is negative for every l l0 then the local robustness property f b xi r is verified to be true by deeppoly.
therefore it is natural to use this upper bound as the loss function to train the corresponding patch module.
definition .
for a local robustness property f b xi r we define the safety violated loss function as l x x l l0max t lx l where t lx lis the upper bound of f x l f x l0on b xi r given by deeppoly as shown in eq.
.
for local robustness properties 1 .
.
.
kwhich share the same ground truth label l0 we define l vk i 1 i pk i 1l i .
for a local robustness property f b xi r its safety violated loss function l being 0implies that is verified to be true by deeppoly.
theorem .
let f b xi r be a local robustness property.
if l onb xi r i.e.
l max elmax t l xi r elmin t l xi r l 1also there are several other verification tools based on abstract interpretation such as crown and deepz which vary in precision affecting loss function evaluations.
we choose deeppoly since it provides a good balance between efficiency and precision.algorithm patch pro input original dnn n pairs of input and its adversarial examples xi x i n i and radius r output a repaired dnn f n c p c b x1 r .
.
.
b xn r iter init p initialize the patch modules p p1 .
.
.
p n d a dictionary where d stores properties fixed with pi fori 1tondo d f b xi r e .
.
.
n record the properties not fixed yet while iter m do m maximum number of iterations iter iter forj edo pj t repaired train n pj d alg.
ifrepaired true then e e j else for tdo we write f x ford 1ton0do score d dl x supx x x xd x d d arg max dscore d x1 x2 bisect x d bisection on the d th dimension 1 f x 2 f x d d 1 2 ife then return f repair with provable guarantee return f repair without provable guarantee algorithm patch training input original dnn n dnn pas a patch module and a finite set uof local robustness properties output the optimized patch module p the set t uof properties to be refined and whether the properties in uhave all been fixed with a provable guarantee function train n p u epoch while epoch r do r maximum number of epochs epoch epoch w w l vu w w the weights in p ifl vu w then l vu p ul return p true t slice k argsort l w l w u return p t false where elmax andelmin are the element wise max andmin operation 0and1are the vector in rn0with all the entries 0and1 respectively then the property holds.
the notion that l is the expansion of l x after taking corresponding values on the boundary of the ball.
to improve the precision of l obtained from deeppoly we employ the input interval partitioning technique from art .
it selects the partition dimension by computing the multiplication of the partial derivative of the safety violated loss function l x and the size of the input interval in the corresponding dimension and bisects the box region over the dimension with the maximum score.
after partitioning the property is split into two new properties whose input sets are two sub boxes of the original input region and the union of these two sub boxes is the input set of .
the main algorithm of p atch prois shown in alg.
.
we construct a dictionary d where d initialized as f b xi r stores the properties that the ith patch module 4pirepairs line alg.
.
the repair process has at most miterations.
in each iteration we train the patches which do not yet provide provable repair i.e.
those whose index is ine.
the algorithm of training a specific patch module p for repairing a set uof properties is shown in alg.
which outputs the optimized patch module p the set tof properties to be refined and a boolean label recording whether the repair of the patch phas a provable guarantee.
a standard gradient descent procedure is run until a provable repair is achieved i.e.
l vu w or the number of epochs reaches a threshold r line alg.
.
after that if it is still not provable we sort l w for ufrom the largest to the smallest and extract the largest kones whose value is strictly larger than as the properties to be refined line alg.
.
for a property f x to be refined we select an input dimension to bisect the input space x. for an input dimension d we define score d dl x sup x x x xd x d where dl x is the partial derivative of l x on the dth dimension.
we choose the dimension with the largest score to bisect x and the property f x is refined to two properties 1 f x and 2 f x recorded in the dictionary d line alg.
.
at the end of each iteration we check whether all the patches provide provable repair if so it terminates immediately and outputs the current repaired dnn f and this repair is provable line alg.
.
if eis still non empty after miterations it outputs fwithout provable guarantee line alg.
.
although we are focusing on fixing adversarial attacks in this work patch proalso works for repair safety properties of dnns by making minor changes in alg.
.
in p atch pro we follow a classical way of defining the loss function with the linear relaxation obtained from deeppoly and incorporate it into our patch based repair framework.
this combination has several advantages.
first every patch is responsible for repairing properties in a specific pattern which makes it much easier to obtain a provable repair even if the repair is not provable the safety violated loss is significantly declining in the training process and it is highly possible that the repaired dnn is locally robust.
different from the neuronlevel repair there is no modification on the original dnn n in p atch pro and due to the design of the indicator the patch modules do not affect the behaviors of other input before allocates a patch to it.
this mechanism avoids the drawdown of accuracy which is quite severe in many neuron level repair methods.
also by allocating patches with we can achieve good generalization of our repair to other inputs and it is the key to solving this essential challenge in fixing adversarial attacks.
in the following we will present how to construct the patch allocation function to improve generalization.
for a network nwith llayers and a maximum of n neurons per layer the time complexity of running deeppoly for one neuron is o n2 l .
the time complexity of running algorithm once which involves backpropagationfor gradient computation and update weights of the patch networks in line of this algorithm is o r npatch lpatch where npatch is the maximum number of neurons per layer in all patch networks and lpatch is the number of layers in the patch networks.
furthermore during the execution of patch pro difficult to repair properties are split into two easier to repair properties which may increase the number of properties to repair up to a predefined upper limit denoted as k. in summary the overall time complexity of p atch prois o r m k n3 l2 n3 patch l2 patch .
c. patch allocation for generalization purposes it is necessary to introduce a patch allocation function given that the current patch construction approach lacks inherent adaptability to in distribution data points.
this patch allocation function is the key to achieving good generalization of the patch modules trained locally to global inputs.
to match the best patch modules for an input x sn i 1b xi r we propose utilizing the prediction l0 cn x of the original network on x as a guiding principle with the aim of selecting patches that share the same ground truth label as l0.
accordingly we formally define the set of patch modules x associated with input x as x i cn x cn xi .
this definition establishes that the set x encompasses all indices isatisfying the condition cn x cn xi i.e.
those instances where the prediction l0made by the original network on x is identical to the prediction made on sample xiat index i. by determining the patch module set x in this manner we ensure consistency between the selected modules and the input x in terms of their predictions by the original network.
this in turn is expected to enhance the robustness repair effectiveness for the specific input x .
once a set of patch modules has been allocated to an input x these modules effectively repair the entire robustness region of that input.
namely we establish a new local robustness property such that any subsequent inputs x b x r will also utilize the same set of patch modules as x with the system s response given by f x p j x pj x n x .
in this context the defense mechanism is established prior to the occurrence of adversarial attacks obviating the need to reallocate patch modules for each individual input xwithin the robustness region which would otherwise require computing x separately.
this proactive construction of the defense ensures a consistent and efficient protection strategy against potential adversarial threats across the entire neighborhood of x reinforcing the overall robustness.
we seemingly have problem when x is not correctly classified by n because in this case we may allocate inappropriate patches.
if x is an adversarial example we can employ sampling based methods like to detect it and recognize its correct classification with a high probability.
otherwise the wrong classification of x may result from backdoor attacks biased training data overfitting etc and such situations are beyond the scope of fixing adversarial attacks in this work.
5d.
repair in a feature space repairing large dnns with high dimensional inputs poses significant challenges due to the substantial memory and computational costs associated with formal verification techniques.
these costs can often become prohibitive especially when dealing with deep networks where the precision of abstractionbased methods such as deeppoly deteriorates significantly over multiple layers of propagation.
consequently the key to adopting p atch proto large dnns is to reduce the input dimensionality and the size of the neural network in formal verification.
in the popular architectures of convolutional neural networks the convolutional layers play the role of extracting important features from data followed by several fully connected layers for classification according to these extracted features so we call such a fully connected layer a feature space.
definition .
letn rn0 rnlbe a dnn where the function from the ith layer to the i th is fi i.e.
n fl f0.
for l the feature space of thelth layer is rnl and the behavior of the input x rn0 in this feature space is n0 l fl f0 x .
the neural network starting with the feature space of the lth layer in n isnl l fl fl.
the dimensionality of a feature space is usually far smaller than that of the input layer.
if we start with a feature space in employing deeppoly it does not need to calculate the abstraction of a great deal of convolutional layers thus getting its efficiency and precision enhanced.
selecting the input feature space for repairing remains an open challenge with limited theoretical research available.
our choice of the second to last fully connected layer is heuristic based on the common understanding that earlier layers in a neural network are primarily responsible for feature extraction while later layers handle decision making.
repairing in the decision layer is more efficient and incurs lower abstraction costs.
to conduct repair in a given feature space with p atch pro we first give an approximation of the feature space on every b xi r .
here we do not require that this approximation should be a sound abstraction of the real semantics of the feature space on b xi r because calculating such an overapproximation with high precision is quite time consuming.
instead we simply sample in b xi r and obtain the tightest box region that contains the buggy behaviors of the samples in the feature space as the approximation.
to identify samples in the feature space that are close to adversarial examples we use the projected gradient descent pgd method and fast gradient sign method fgsm to find samples in b xi r and obtain their corresponding feature space behaviors.
formally the abstraction x i jcan be defined as follows.
definition .
letsbe the set of samples obtained by applying pgd and fgsm to ninb xi r .
for each sample s s we compute its feature space behavior n0 l s at the l th layer.the abstraction of sin the feature space is defined as x i j min s s n0 l s j max s s n0 l s j where x i jis the range of j th dimension of x i n0 l s jis the j th element of the n0 l s and j .
.
.
n l. these samples being actual adversarial examples in realworld scenarios provide direct guidance for feature space repair.
this approximation is not sound yet but it does not mean that the inputs in b xi r whose behaviors in the feature space are not within this approximation will not be repaired by the corresponding patch module pi because the indicator icstill remains the same.
since the approximation of the feature space is not sound the repair does not have a provable guarantee.
it is also worth mentioning that even if two properties with different ground truth labels have their feature spaces overlapping our repair still works.
in this situation although two contradicted properties are involved in the training process but they must correspond to two different robustness regions b xi r and thus different patch modules pi.
the correspondence of the robustness regions b xi r and the patch piis always preserved so piis always working for repairing on b xi r with the correct classification label l0.
after obtaining an approximation bsfor each b xi r in the feature space we employ alg.
where the initialization ofd in line is nl l pi x i instead.
since the repair in the feature space is not provable the safety violation loss function l for nl l pi x i in def.
is modified to be l x x l l0 t lx l where t lx lforl l0is obtained by deeppoly abstracting nl l pionx i andl is modified accordingly.
also we do not run line in alg.
and skip line and in alg.
so that the loss l decreases as much as possible and we can achieve a better repair performance.
in performing feature layer repairs on large scale networks we persist in utilizing the function guided by the original network output to allocate corresponding repair modules.
while the approximation x i is indeed effective in capturing commonality in buggy behaviors establishing a direct association between the input neighborhood b x r of a newly encountered point x and its distribution within the feature layers of the extensive network proves challenging.
consequently assigning repair modules based on the input neighborhood of x emerges as a more targeted and reliable approach.
repair in a feature space is an effective way to make patch proscale on large dnns.
although we sacrifice provability its repair performance and generalization capabilities demonstrate remarkable effectiveness in practical scenarios which we will see in our experimental evaluation.
6table i results of repairing local robustness .
w e represent each tool s name with their first three letters such as car for care apr for aprnn etc.
and means timeout or memory overflow .
model r nrsr dd time s car prd apr rea tra art ours car prd apr rea tra art ours car prd apr rea tra art ours fnn small0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fnn big0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cnn0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
iv.
e xperimental evaluation in this section we evaluate p atch proby answering the following research questions rq1 what is the overall performance of p atch proin repairing local robustness and correcting safety property violations?
rq2 does the repaired dnn exhibit the capability to defend against new adversarial attacks?
rq3 does p atch prohave scalability to repair large networks?
rq4 in the context of p atch pro how does the size and quantity of patch modules influence the efficacy of the repair?
a. setup all the experiments are conducted on a machine with an amd epyc core processor gb of memory and an nvidia geforce rtx with gb of gpu memory.
each experiment has a timeout set to seconds.
a dataset we conduct evaluations on four common datasets mnist cifar tiny imagenet and acas xu .
the first three are widely recognized benchmarks in the field for studying neural network robustness and acas xu is a commonly used benchmark in research on neural network verification and repair .
on mnist we use two fully connected networks and one convolutional network while on cifar we train a vgg19 and a resnet18 .
on tiny imagenet we train a resnet152 and a wrn101 .
we assess our approachon acas xu dnns which as documented in are expected to satisfy property but exhibit violations.
for the local robustness repair task we generate adversarial samples using pgd for dnns trained on mnist and cifar10 and samples as well as tiny imagenet and samples .
the radius ris set to .
.
and .
for mnist 255and8 255for cifar and2 and4 255for tiny imagenet.
on acas xu we aim to repair the violation of property using one patch module while preserving the original performance.
although p atch pro repairs safety properties without requiring sample information due to the needs of other tools such as care we sample counterexamples as the faulty inputs for repair.
for cifar the accuracies of vgg19 and resnet18 are .
and .
respectively.
for tiny imagenet the accuracies of wrn1012 and resnet152 are .
and .
respectively.
the maximum number of iterations min alg.
is set to .
in alg.
the maximum number of epochs r learning rate and selection number kare set to and respectively.
all patch modules consist of a single linear layer unless otherwise specified.
on mnist the patch module takes the sample itself as input while on cifar and tiny imagenet it takes the output from the network s penultimate layer as input.
for sampling the feature space we use pgd attack with a step size of2 255for rounds with each round consisting of steps.
we collect all adversarial examples generated during this process.
additionally we use fgsm attack to generate 7table ii results of correcting violation of safety properties on acas x u modelrsr rgr fdd time s car prd rea art ours car prd rea art ours car prd rea art ours car prd rea art ours n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
adversarial examples which are then combined with the samples obtained from pgd.
this combined set of samples is used for sampling the feature space.
b baselines we compare p atch prowith the stateof the art repair methods including care aprnn prdnn reassure and art .
since aprnn and prdnn require selecting a layer to repair on we traverse all eligible layers across each baseline and report the layer that exhibit the best performance.
additionally for the local robustness repair task we compare our approach with an adversarial training method trades where we select the trained model with the best performance in epochs with their default parameters.
c metrics to assess generalization we sample adversarial examples on each b xi r different from x ito form a generalization set dgfor mnist cifar and tiny imagenet and use an independent set of counterexamples for acas xu.
besides these we have an independent test setdtof size and for mnist cifar tiny imagenet and acas xu respectively.
we employ autoattack to attack the repaired dnn fon xi n i 1anddtwith the same radius r. the metrics we use include repair success rate rsr repair generalization rate rgr drawdown dd defense success rate dsr and defense generalization success rate dgsr defined as rsr i cf x i cn xi n rgr x dg cf x lx dg dd x dt cn x lx x dt cf x lx dt dsr i x aa f b xi r cf x cn xi n dgsr x dt x aa f b x r cf x cn x dt where lxdenotes the ground truth of x and aa represents the set of potential adversarial samples obtained by attacking the local robustness property with autoattack.
on acas xu the metric drawdown is replaced with fidelity drawdown fdd because there is no labelled ground truth for inputs on acas xu fdd x dt cf x cn x dt .
the metrics rsr anddd orfdd evaluates the overall performance of dnn repair by measuring the percentage of buggy inputs successfully repaired and how much the accuracy decreases while rgr dsr anddgsr reflects how the repair generalizes to the robustness regions b xi r and to other inputs.
b. repair performance the results of the local robustness repair task on mnist are summarized in table i. we evaluate overall repair per8table iii results of generalization and defense against new adversarial attacks model r nrgr dsr dgsr car prd apr rea tra art ours car prd apr rea tra art ours car prd apr rea tra art ours fnn small0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fnn big0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cnn0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
formance with rsr dd and runtime representing fix success accuracy retention and speed respectively.
prdnn reassure and p atch proreach rsr due to their provable designs.
aprnn despite being provable underperforms on some fnn small cases while art struggles with high dimensional inputs.
care and trades lacking provable guarantees show inferior rsr compared to provable methods.
care s suboptimal repair across all models may stem from the complex interplay between local robustness repair and dnns numerous parameters making it difficult to pinpoint and modify relevant parts.
although trades performs better on cnns its effectiveness wanes with largerradius local robustness errors.
in terms of dd patch pro and reassure consistently outperform alternative methods across all models.
the results of correcting safety property violations on acas xu are presented in table ii.
each patch module here is of the same size as the original network.
p atch pro prdnn reassure and art achieve a perfect rsr with provable repairs.
both art and our method showcase remarkable rgr scores achieving a perfect among all the evaluated tools.
p atch proalso excels with a fdd of zero outperforming care art prdnn and reassure.
these results emphasize our method s strength in delivering provable repairs and preserving high fidelity functionality.c.
generalization in the experiment we assess the repaired network s generalization and resistance to new adversarial attacks table iii .
p atch proleads in rgr demonstrating superior performance compared to other baselines.
prdnn aprnn and trades also show some degree of generalization particularly on cnns.
for dsr patch prooutperforms competitors with trades showing moderate defense against small radius attacks on fnn big and cnn.
evaluating dgsr patch proagain tops the list while care prdnn aprnn and trades show limited defense against fnn big and cnn.
dgsr is key for gauging generalization to global inputs .
overall these findings underscore p atch pro breakthrough in repairing adversarial attacks.
answer to rq1 and rq2 patch proconsistently outperforms the baselines in both local robustness repair and safety property violation correction tasks.
it achieves repair success rate coupled with drawdown and acceptable efficiency.
additionally it demonstrates significant generalization against unforeseen adversarial attacks.
d. scalability evaluation we examine p atch pro scalability on vgg19 and resnet18 for cifar local robustness repair via featurespace repair.
results in table iv show p atch pro prdnn and aprnn achieve rsr while trades follows 9table iv results of repairing local robustness on cifar vgg19 resnet r r r r rsr care .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
prdnn .
.
.
.
.
.
.
.
.
.
.
.
trade .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
aprnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rgr care .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
prdnn .
.
.
.
.
.
.
.
.
.
.
.
trade .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
aprnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dd care .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
prdnn .
.
.
.
.
.
.
.
.
.
.
.
trade .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
aprnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dsr care .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
prdnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
aprnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trade .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dgsr care .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
prdnn .
.
.
.
.
.
.
.
.
.
.
.
aprnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trade .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
time scare .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
prdnn .
.
.
.
.
.
.
.
.
.
.
.
trade .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
aprnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
closely.
p atch proleads in rgr anddd with trades following suit.
p atch proalso significantly outperforms others in dsr and matches accuracy levels indgsr surpassing all tools.
on larger datasets like tiny imagenet p atch pro maintains superiority in repairing wrn101 and resnet152 as seen in table v. p atch pro running time is reasonable given its superior repair and generalization abilities.
although trades performs comparably p atch proconsistently delivers higher performance across various metrics.
for both datasets patch modules are applied at the network s second tolast layer using a fully connected network with a single linear layer.
the linear layer takes the output from the network s penultimate layer as input and its output is added to the original network s output.
answer to rq3 patch prodemonstrates good scalability in repairing local robustness on large scale dnns which also achieves a repair success rate coupled with drawdown outperforming the other tools.
e. impact of patch module size and quantity on efficiency in this experiment we evaluate the effectiveness of patch module size and quantity in repairing local robustness properties.
we perform a comparative analysis of p atch pro s performance across different sizes and quantities presenting the experimental results in table vi.
the performance of patch prois assessed using vgg19 and resnet18 on the cifar dataset across various combinations of patch module scales and quantities including small patch modules ps and large patch modules pl .
two distinct patch module sizes are considered small and large.
the small patch module consists of a single linear layer which takes the output from the network s penultimate layer as input.
the large patch module also takes the output from the penultimate layer as input but additionally comprises three hidden layers with neurons each.
it is noted that larger patchnetworks tend to yield poorer results possibly due to increased over approximation error when passing through relu nodes in the hidden layers.
as the number of hidden layers increases the cumulative over approximation error grows leading to less effective repairs.
smaller patch scales slightly reduce the repair time.
additionally when using deeppoly to verify the feature layer repairs after applying smaller patches the verification success rate is higher.
we also tested the effect of using a single patch to repair the neural network and the results are shown in table vii.
it can be observed that the repair effectiveness using a single patch is significantly inferior to using multiple patches.
a single patch only achieves to rsr and rgr.
in contrast using multiple patches results in both rsr and rgr reaching as shown in table iv.
additionally the single patch performs much worse in terms of dsr and dgsr.
therefore using multiple patches for repair is highly beneficial.
answer to rq4 we observe that smaller patch modules are more effective likely due to reduced over approximation error and shorter repair times as well as higher verification success rates with abstract interpretation tools.
for the aspect of patch module quantity multiple patch modules significantly outperform only one patch module.
v. r elated work provable dnn repair.
the methods most closely related to ours are art and reassure .
in comparison to reassure our approach fundamentally differs in its repair objectives.
reassure focuses on fixing activation patterns of neural networks yet for high dimensional data the input constraints of a property may encompass a large number of activation patterns.
in contrast p atch proleverages formal verification to directly repair properties.
although art also employs training in its repair process it modifies parameters 10table v results of repairing local robustness on tinyimage net model r nrsr rgr dd dsr dgsr time s care trade ours care trade ours care trade ours care trade ours care trade ours care trade ours wrn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table vi the efficacy of patch module scale vgg19 resnet r r r r dsr ps .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dgsr ps .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
time sps .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
verifiedps pl table vii the efficacy of single patch moudle vgg19 resnet r r r r rsr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rgr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dsr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dgsr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
in the original neural network.
on the other hand p atch pro uses patch modules specifically to fix properties ensuring the performance of the original network.
other methods such as prdnn and aprnn formulate the repair problem in linear programming.
however their efficacy is limited on properties with high dimensional polytopes such as the robustness of image classification.
heuristic dnn repair.
utilizing heuristic algorithms such as particle swarm optimization and differential evolution care and arachne aim to pinpoint the neurons responsible for faults.
vere focuses on providing formal verification guidance to assist fault localization and defines target intervals for repair synthesis.
for example deeprepair and few shot guided mix expand the set of negative samples to generate additional training data.
conversely tian et al.
augment the existing data by introducing realworld environmental effects such as fog to the samples.
dl2 fuses logical constraints and loss functions but without convex certified guarantees.
dnn verification.
over the past decade various approaches of dnn verification has been proposed including techniques such as constraint solving abstract interpretation linear relaxation global optimisation cegar reduction to two player games and star set abstraction .
these method offer provable estimations of dnn robustness.
moreover statistical approaches presented in prove to be more efficient and scalable particularly suited for intricate dnn structures allowing for the establishment of quantifiablerobustness at a specified confidence level.
certified training uses convex approximation in training the loss function but it is only used to enhance the robustness of dnn without making it accessible to repair properties in the training procedure.
vi.
c onclusion we introduce p atch pro a novel approach for propertybased repair of local robustness using limited data.
our method provides patch modules as neural networks to repair within the robustness neighborhood enabling the generalization of this defense to other inputs.
in terms of efficiency scalability and generalization our approach surpasses existing methods.
vii.
a cknowledgements this work is supported by cas project for young scientists in basic research grant no.ysbr iscas basic research iscas jczd and iscas new cultivation project iscas pyfx .
the limitation of our work is that our patch allocation may assign inappropriate patches when the input is misclassified and in this case p atch pro may fail to repair its local robustness.
for future work we plan to integrate verification methods based on branch and bound like to enhance the performance of our approach improving both precision and offering an alternative refinement approach.
additionally our patch based repair framework has the potential to evolve into a black box repair method combined with black box verification methods like .
11references m. badar m. haris and a. fatima application of deep learning for retinal image analysis a review computer science review vol.
p. .
j. devlin m. w. chang k. lee and k. toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint arxiv .
.
y .
wang x. deng s. pu and z. huang residual convolutional ctc networks for automatic speech recognition arxiv preprint arxiv .
.
a. madry a. makelov l. schmidt d. tsipras and a. vladu towards deep learning models resistant to adversarial attacks in iclr .
vancouver bc canada openreview.net .
f. croce and m. hein mind the box l1 apgd for sparse adversarial attacks on image classifiers in icml .
m. bojarski d. del testa d. dworakowski b. firner b. flepp p. goyal l. d. jackel m. monfort u. muller j. zhang et al.
end to end learning for self driving cars arxiv preprint arxiv .
.
s. vieira w. h. pinaya and a. mechelli using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders methods and applications neuroscience biobehavioral reviews vol.
pp.
.
b. sun j. sun l. h. pham and j. shi causality based neural network repair in proceedings of the 44th international conference on software engineering pp.
.
j. sohn s. kang and s. yoo arachne search based repair of deep neural networks acm transactions on software engineering and methodology .
j. ma p. yang j. wang y .
sun c. c. huang and z. wang vere verification guided synthesis for repairing deep neural networks in proceedings of the 46th ieee acm international conference on software engineering pp.
.
t. gehr m. mirman d. drachsler cohen p. tsankov s. chaudhuri and m. vechev ai2 safety and robustness certification of neural networks with abstract interpretation in ieee symposium on security and privacy sp .
ieee pp.
.
y .
ganin e. ustinova h. ajakan p. germain h. larochelle f. laviolette m. marchand and v .
lempitsky domain adversarial training of neural networks the journal of machine learning research vol.
no.
pp.
.
f. tramer and d. boneh adversarial training and robustness for multiple perturbations advances in neural information processing systems vol.
.
w. dai o. jin g. r. xue q. yang and y .
yu eigentransfer a unified framework for transfer learning in proceedings of the 26th annual international conference on machine learning pp.
.
w. ying y .
zhang j. huang and q. yang transfer learning via learning to transfer in international conference on machine learning .
pmlr pp.
.
b. yu h. qi q. guo f. juefei xu x. xie l. ma and j. zhao deeprepair style guided repairing for deep neural networks in the real world operational environment ieee transactions on reliability vol.
no.
pp.
.
x. ren b. yu h. qi f. juefei xu z. li w. xue l. ma and j. zhao few shot guided mix for dnn repairing in ieee international conference on software maintenance and evolution icsme .
ieee pp.
.
s. ma y .
liu w. c. lee x. zhang and a. grama mode automated neural network model debugging via state differential analysis and input selection in proceedings of the 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering pp.
.
z. allen zhu and y .
li feature purification how adversarial training performs robust deep learning in ieee 62nd annual symposium on foundations of computer science focs .
los alamitos ca usa ieee computer society feb pp.
.
m. sotoudeh and a. v .
thakur provable repair of deep neural networks in proceedings of the 42nd acm sigplan international conference on programming language design and implementation pp.
.
f. fu and w. li sound and complete neural network repair with minimality and locality guarantees in the tenth international conference on learning representations iclr virtual event april .
openreview.net .
z. tao s. nawas j. mitchell and a. v .
thakur architecture preserving provable repair of deep neural networks proceedings of the acm on programming languages vol.
no.
pldi pp.
.
g. singh t. gehr m. p uschel and m. t. vechev an abstract domain for certifying neural networks pacmpl vol.
no.
popl pp.
.
m. fischer m. balunovic d. drachsler cohen t. gehr c. zhang and m. t. vechev dl2 training and querying neural networks with logic in proceedings of the 36th international conference on machine learning icml june long beach california usa ser.
proceedings of machine learning research k. chaudhuri and r. salakhutdinov eds.
vol.
.
pmlr pp.
.
x. lin h. zhu r. samanta and s. jagannathan art abstraction refinement guided training for provably correct neural networks in formal methods in computer aided design fmcad haifa israel september .
ieee pp.
.
h. zhang t. w. weng p. y .
chen c. j. hsieh and l. daniel efficient neural network robustness certification with general activation functions advances in neural information processing systems vol.
.
g. singh t. gehr m. mirman m. p uschel and m. t. vechev fast and effective robustness certification in neurips montr eal canada pp.
.
j. wang g. dong j. sun x. wang and p. zhang adversarial sample detection for deep neural network through model mutation testing in ieee acm 41st international conference on software engineering icse .
ieee pp.
.
y .
lecun l. bottou y .
bengio and p. haffner gradient based learning applied to document recognition proc.
ieee vol.
no.
pp.
.
.
available a. krizhevsky learning multiple layers of features from tiny images jan .
y .
le and x. s. yang tiny imagenet visual recognition challenge .
.
available c. von essen and d. giannakopoulou analyzing the next generation airborne collision avoidance system in tacas ser.
lecture notes in computer science e. abrah am and k. havelund eds.
vol.
.
springer pp.
.
j. jeannin k. ghorbal y .
kouskoulas r. gardner a. schmidt e. zawadzki and a. platzer formal verification of acas x an industrial airborne collision avoidance system in emsoft a. girault and n. guan eds.
ieee pp.
.
i. j. goodfellow j. shlens and c. szegedy explaining and harnessing adversarial examples in iclr .
g. katz c. w. barrett d. l. dill k. julian and m. j. kochenderfer reluplex an efficient smt solver for verifying deep neural networks incav .
heidelberg germany springer pp.
.
k. simonyan and a. zisserman very deep convolutional networks for large scale image recognition in 3rd international conference on learning representations iclr san diego ca usa may conference track proceedings y .
bengio and y .
lecun eds.
.
.
available r. ferjaoui m. a. cherni f. abidi and a. zidi deep residual learning based on resnet50 for covid recognition in lung ct images in 8th international conference on control decision and information technologies codit istanbul turkey may .
ieee pp.
.
.
available s. zagoruyko and n. komodakis wide residual networks corr vol.
abs .
.
.
available .
h. zhang y .
yu j. jiao e. xing l. el ghaoui and m. jordan theoretically principled trade off between robustness and accuracy ininternational conference on machine learning .
pmlr pp.
.
y .
tian k. pei s. jana and b. ray deeptest automated testing of deep neural network driven autonomous cars in proceedings of the 40th international conference on software engineering pp.
.
g. katz c. barrett d. l. dill k. julian and m. j. kochenderfer reluplex an efficient smt solver for verifying deep neural networks incomputer aided verification 29th international conference cav heidelberg germany july proceedings part i .
springer pp.
.
g. katz d. a. huang d. ibeling k. julian c. lazarus r. lim p. shah s. thakoor h. wu a. zeljic d. l. dill m. j. kochenderfer and c. w. barrett the marabou framework for verification and analysis of deep neural networks in cav ser.
lecture notes in computer science i. dillig and s. tasiran eds.
vol.
.
new york city ny usa springer pp.
.
r. ehlers formal verification of piece wise linear feed forward neural networks in atva .
pune india springer pp.
.
x. huang m. kwiatkowska s. wang and m. wu safety verification of deep neural networks in cav .
heidelberg germany springer pp.
.
g. singh t. gehr m. p uschel and m. vechev an abstract domain for certifying neural networks proceedings of the acm on programming languages vol.
no.
popl pp.
.
g. singh t. gehr m. mirman m. p uschel and m. vechev fast and effective robustness certification advances in neural information processing systems vol.
.
j. li j. liu p. yang l. chen x. huang and l. zhang analyzing deep neural networks with symbolic propagation towards higher precision and faster verification in static analysis 26th international symposium sas porto portugal october proceedings .
springer pp.
.
g. singh r. ganvir m. p uschel and m. t. vechev beyond the single neuron convex barrier for neural network certification in neurips pp.
.
k. d. julian s. sharma j. b. jeannin and m. j. kochenderfer verifying aircraft collision avoidance neural networks through linear approximations of safe regions arxiv preprint arxiv .
.
b. paulsen j. wang and c. wang reludiff differential verification of deep neural networks in proceedings of the acm ieee 42nd international conference on software engineering pp.
.
k. xu z. shi h. zhang y .
wang k. w. chang m. huang b. kailkhura x. lin and c. j. hsieh automatic perturbation analysis for scalable certified robustness and beyond advances in neural information processing systems vol.
pp.
.
t. weng h. zhang h. chen z. song c. hsieh l. daniel d. s. boning and i. s. dhillon towards fast computation of certified robustness for relu networks in icml ser.
proceedings of machine learning research j. g. dy and a. krause eds.
vol.
.
stockholm sweden pmlr pp.
.
b. batten p. kouvaros a. lomuscio and y .
zheng efficient neural network verification via layer based semidefinite relaxations and linear cuts.
ijcai .
w. ruan x. huang and m. kwiatkowska reachability analysis of deep neural networks with provable guarantees in ijcai .
stockholm sweden ijcai.org pp.
.
s. dutta s. jha s. sankaranarayanan and a. tiwari output range analysis for deep feedforward neural networks in nfm ser.
lecture notes in computer science a. dutle c. a. mu noz and a. narkawicz eds.
vol.
.
newport news v a usa springer pp.
.
w. ruan m. wu y .
sun x. huang d. kroening and m. kwiatkowska global robustness evaluation of deep neural networks with provable guarantees for the hamming distance in ijcai s. kraus ed.
macao china ijcai.org pp.
.
p. ashok v .
hashemi j. kret nsk y and s. mohr deepabstract neural network abstraction for accelerating verification in atva ser.
lecture notes in computer science vol.
.
springer pp.
.
y .
y .
elboher j. gottschlich and g. katz an abstraction based framework for neural network verification in cav ser.
lecture notes in computer science s. k. lahiri and c. wang eds.
vol.
.
los angeles ca usa springer pp.
.
m. ostrovsky c. w. barrett and g. katz an abstraction refinement approach to verifying convolutional neural networks in automated technology for verification and analysis 20th international symposium atva virtual event october proceedings ser.
lecture notes in computer science vol.
.
springer pp.
.
m. wicker x. huang and m. kwiatkowska feature guided blackbox safety testing of deep neural networks in tacas ser.
lecture notes in computer science d. beyer and m. huisman eds.
vol.
.
thessaloniki greece springer pp.
.
m. wu m. wicker w. ruan x. huang and m. kwiatkowska a gamebased approximate verification of deep neural networks with provable guarantees theor.
comput.
sci.
vol.
pp.
.
h. tran d. m. lopez p. musau x. yang l. v .
nguyen w. xiang and t. t. johnson star based reachability analysis of deep neural networks in fm ser.
lecture notes in computer science m. h. ter beek a. mciver and j. n. oliveira eds.
vol.
.
porto portugal springer pp.
.
h. tran s. bak w. xiang and t. t. johnson verification of deep convolutional neural networks using imagestars in cav ser.
lecture notes in computer science s. k. lahiri and c. wang eds.
vol.
.
los angeles ca usa springer pp.
.
s. webb t. rainforth y .
w. teh and m. p. kumar a statistical approach to assessing neural network robustness in iclr .
new orleans la usa openreview.net .
l. weng p. chen l. m. nguyen m. s. squillante a. boopathy i. v .
oseledets and l. daniel proven verifying robustness of neural networks with a probabilistic approach in icml june ser.
proceedings of machine learning research vol.
.
long beach california usa pmlr pp.
.
t. baluta z. l. chua k. s. meel and p. saxena scalable quantitative verification for deep neural networks in icse .
madrid spain ieee pp.
.
m. wicker l. laurenti a. patane and m. kwiatkowska probabilistic safety for bayesian neural networks in uai august ser.
proceedings of machine learning research r. p. adams and v .
gogate eds.
vol.
.
virtual online auai press pp.
.
t. baluta s. shen s. shinde k. s. meel and p. saxena quantitative verification of neural networks and its security applications in ccs november .
london uk acm pp.
.
l. cardelli m. kwiatkowska l. laurenti n. paoletti a. patane and m. wicker statistical guarantees for the robustness of bayesian neural networks in ijcai august s. kraus ed.
macao china ijcai.org pp.
.
p. huang y .
yang m. liu f. jia f. ma and j. zhang weakened robustness of deep neural networks corr vol.
abs .
.
r. mangal a. v .
nori and a. orso robustness of neural networks a probabilistic and practical approach in icse nier montreal qc canada may .
montreal qc canada ieee acm pp.
.
l. cardelli m. kwiatkowska l. laurenti and a. patane robustness guarantees for bayesian inference with gaussian processes in aaai january february .
honolulu hawaii usa aaai press pp.
.
r. li p. yang c. huang y .
sun b. xue and l. zhang towards practical robustness analysis for dnns based on pacmodel learning in 44th ieee acm 44th international conference on software engineering icse pittsburgh pa usa may .
acm pp.
.
.
available m. balunovi c and m. vechev adversarial training and provable defenses bridging the gap in 8th international conference on learning representations iclr virtual .
international conference on learning representations .
y .
mao m. m uller m. fischer and m. vechev connecting certified and adversarial training advances in neural information processing systems vol.
.
m. n. m uller f. eckert m. fischer and m. t. vechev certified training small boxes are all you need in the eleventh international conference on learning representations iclr kigali rwanda may .
openreview.net .
13appendix a proof of theorem .
theorem .
let f b xi r be a local robustness property.
if l onb xi r i.e.
l max elmax t l xi r elmin t l xi r l where elmax andelmin are the element wise max andmin operation 0and1are the vector in rn0with all the entries 0and1 respectively then the property holds.
proof.
as t lx lis a linear function with respect to x we can calculate the maximum of t lx lonb xi r as follows max x b xi r t lx l x i rn01 l i l imax x b xi r x x i rn01 l i l imax x b xi r x l x i rn01 l i l i xi r x i rn01 l i l i xi r l then we have l x x l l0max t lx l x b xi r x l l0max elmax t l x r elmin t l x r l l .
therefore if l we have x b xi r f x l f x l0 t lx l then the property holds.
appendix b an explainable example x1 x2x3 x4x5 x6y1 y20.
.
.
.2relu relu .
.
.
.
fig.
.
a fully connected neural network nwith relu activations.
example .consider the neural network in fig.
and the inputs x .
labeled .
within the region b x .
we have its counterexample x .
.
which violates the local robustness.
to repair the network we need to construct a patch p wxwhere w r2 .
0dd n ours care prdnn trade aprnnfig.
.
results under the extreme setting.
after initializing wto all .
we train the patch according the while loop begins at line in alg.
.
as shown in line in alg.
we first execute deeppoly to obtain the safety violated loss function l w1 1x1 w1 2x2 w2 1x1 w2 2x2 .7x1 .14x2 .
.
to maximize l we set x1 .2andx2 .
then we have l w .2w1 .5w1 .2w2 .5w2 .
.
finally we update wby w l w with the learning rate .
this process is repeated until the robustness is proven or the epochs reaches its limit r. to show the subsequent repair process we set .6and r .
under this setting the network is not be repaired then we need to refine the robustness to two properties by bisecting the region b x .
see line in alg.
.
specifically by the judgment that 1l 2l we select the dimension of x1and divide the robustness region b x .
into and .
we perform the above repair process again and based on a more accurate abstraction provided by two new properties we finally obtain the repaired network with the patch p .
.
.
x1 x2 .
appendix c limitation as mentioned in section iii c the adversarial examples not within any known robustness regions may lead to the establishment of incorrect new properties and the resulting inappropriate patches allocation may ultimately affect the performance of the repaired neural network.
although this situation is rare in reality we created an extreme setting here to investigate this weakness of our method.
specifically considering the local robustness with radius r we reuse the repaired vgg19 for cifar and execute it over the adversarial dataset dadvconsisting of the adversarial examples generated by attacking the original network over the testset dt.
the attack utilize autoattack with the step size of .
by preserving the new properties established in this execution we retest the drawdown of the repaired model overdt.
the results are presented in fig.
showing that the accuracy of the repaired model drops significantly under this extreme setting.
this is a limitation of p atch pro and it also echoes the statement aforementioned that adversarial detection can serve as an important supplement to our method.
14table viii network architectures and accuracies of dnn trained on mnist dataset name accuracy model structure fnn small .6linear layer of hidden units linear layer of hidden units linear layer of hidden units linear layer of hidden units linear layer of hidden units linear layer of hidden units fnn big .2linear layer of hidden units linear layer of hidden units linear layer of hidden units linear layer of hidden units linear layer of hidden units linear layer of hidden units cnn .3conv2d stride padding linear layer of hidden units linear layer of hidden units appendix d network architectures and accuracies of the dnn trained on mnist the network architectures and accuracies of the dnn trained on mnist are detailed in table viii.