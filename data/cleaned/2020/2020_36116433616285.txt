software architecture recovery with information fusion yiran zhang nanyang technological university singapore yiran002 e.ntu.edu.sgzhengzi xu nanyang technological university singapore zhengzi.xu ntu.edu.sgchengwei liu nanyang technological university singapore chengwei001 e.ntu.edu.sg hongxu chen huawei technologies co. ltd shenzhen china chenhongxu5 huawei.comjianwen sun huawei technologies co. ltd shenzhen china sunjianwen4 huawei.comdong qiu huawei technologies co. ltd shenzhen china dong.qiu huawei.com yang liu nanyang technological university singapore yangliu ntu.edu.sg abstract understanding the architecture is vital for effectively maintaining and managing large software systems.
however as software systems evolve over time their architectures inevitably change.
to keep up with the change architects need to track the implementationlevel changes and update the architectural documentation accordingly which is time consuming and error prone.
therefore many automatic architecture recovery techniques have been proposed to ease this process.
despite efforts have been made to improve the accuracy of architecture recovery existing solutions still suffer from two limitations.
first most of them only use one or two type of information for the recovery ignoring the potential usefulness of other sources.
second they tend to use the information in a coarse grained manner overlooking important details within it.
to address these limitations we propose sarif a fully automated architecture recovery technique which incorporates three types of comprehensive information including dependencies code text and folder structure.
sarif can recover architecture more accurately by thoroughly analyzing the details of each type of information and adaptively fusing them based on their relevance and quality.
to evaluate sarif we collected six projects with published ground truth architectures and three open source projects labeled by our industrial collaborators.
we compared sarif with nine state of the art techniques using three commonly used architecture similarity metrics and two new metrics.
the experimental results show that sarif is .
more accurate than the best of the chengwei liu is the corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
techniques on average.
by providing comprehensive architecture sarif can help users understand systems effectively and reduce the manual effort of obtaining ground truth architectures.
ccs concepts software and its engineering software maintenance tools software reverse engineering maintaining software .
keywords software architecture recovery software module clustering reverse engineering architecture comparison acm reference format yiran zhang zhengzi xu chengwei liu hongxu chen jianwen sun dong qiu and yang liu.
.
software architecture recovery with information fusion.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction software systems change over time to include new features or for maintenance purposes.
during this process actual implementation of the system may become different from the initial architectural design which is known as the architectural drift anderosion .
architectural drift anderosion can lead to wrong decisions on development activities that result in a waste of time and development resources.
in order to prevent such drift orerosion architects need to understand the implemented architecture.
however such task is costly which can take hundreds of hours of an expert for a system with hundred thousands of lines of code .
to reduce the manual effort required to maintain the architecture various techniques have been proposed to recover the system architecture from its code implementation.
these techniques aim to associate software implementation level entities i.e.
files functions or classes with high level system components i.e.
clusters of entities by clustering the entities according to their structural dependencies or textual information.
for example acdc bunch and fca cluster the entities based on the structural esec fse december san francisco ca usa yiran zhang zhengzi xu chengwei liu hongxu chen jianwen sun dong qiu and yang liu trio.c trio.h trionan.c trionan.htriostr.c triostr.h triop.h triodef.hlibxml.hxpath.c zlq frqilj .h figure trio module in libxml2 dependencies of software systems.
architecture recovery using concerns arc mostly relies on the semantic i.e.
topic modeling results of the source codes to cluster the entities.
sade clusters the software system by combining both the dependencybased and textual based results.
however according to the current results of architecture recovery techniques are not accurate enough for real world applications.
we summarize the two major limitations of the existing solutions as follows.
first the existing research recovers the architecture based on only one or two types of information to group software entities despite a rich set of information can be helpful.
for example dependency between entities is the most commonly used information.
in addition to dependency arc has proven that textual information i.e.
information from code text is also useful for architecture recovery.
moreover package or folder structures of software systems also carry important structural information on the architecture .
techniques focusing on only one or two types of information rather than all these aspects may produce one sided and inaccurate recovery results of architecture.
second previous research tends to use coarse grained information which lacks the details that are critical to architecture recovery.
for example in dependency based works all the dependencies are treated to have equal contributions to the clustering.
however different types of dependencies have different influences on the architecture.
according to function call dependencies can reflect much more architectural information than use type dependencies.
moreover dependencies introduced by different entities also have different impact on the architecture.
for example dependencies between public interfaces of modules are more important than that between internal implementations .
to overcome these limitations we propose sarif a fully automated architecture recovery technique that fuses dependency code text and folder structure information to produce comprehensive recovery results.
sarif is a cross platform architecture recovery tool that currently supports c c and java.
to obtain fine grained information sarif refines the entity level dependencies with entity importance and dependency type measures the strength of file level dependencies based on the entity level ones uses tf idf and lda to focus on topic information instead of raw code text selects folder structures based on their quality last sarif builds an adaptive model to fuse different types of information and clusters the software entities to recover the architecture.
we evaluate sarif with established metrics mojofm a2a c2ccvg as well as two additional metrics adjusted rand index ari anda2aadj as we notice that the commonly used architecture similarity metrics may not be adequate to produce unbiased architecture comparison results.
we compare sarif against previous techniques on software systems with human labeled ground truth architecture.
the experiments show that sarif outperforms previous architecture recovery methods by being .
more accurate than the best of the previous results on average.
moreover we did an extensive experiment on github projects to further verify the generalizability of sarif.
in summary our main contribution includes we propose sarif a software architecture recovery algorithm that leverages the fine grained program dependencies code text and folder structure to produce comprehensive recovery results.
we introduce ari to the scenario of comparing architectures and design a new metric a2aadjwhich solves limitations of a2ato measure the architecture similarities.
we evaluate sarif with existing techniques on software systems with ground truth architectures.
the results show that sarif outperforms other techniques by being .
more accurate on average.
motivating example in this section we illustrate our motivation with some examples of architecture recovery results of existing techniques.
we collaborated with a big software vendor company name anonymized to manually label the architecture of libxml2 v2.
.
an xml toolkit in c. then we compared the architectures recovered by state of the art architecture recovery tools with our manual labeling and found that the recovery results were unsatisfactory.
for example we manually checked their result on the triomodule oflibxml2 .
the triomodule is a portable implementation of printf function family.
it contains files which are marked with gray background in fig .
the triomodule is one of the most distinctive module in libxml2 from both structural and textual perspectives.
structurally out of file dependencies are internal dependencies.
textually out of triowords in libxml2 are from this module.
however most of the state of the art architecture recovery tools still failed to recognize this module by dividing it into multiple modules or combining them with other modules.
we manually analyze the results of two tool arc and bunch and summarize the reasons for their failures as follows.
arc is a text based architecture recovery tool.
its clustering result is shown in fig by the color coding of nodes frames.
arc divides the triomodule into three clusters.
two of them red and blue only comprise files from the triomodule.
however the third cluster yellow includes files from trioandwin32config.h which is a configuration file and not really related to triomodule.
this is probably because win32config.h contains the most triowords outside the triomodule out of even though it has completely different functionalities from the triomodule.
this demonstrates that solely use textual information may be inadequate since the code text does not necessarily reflect the code s functionality.
bunch recovers the architecture based only on dependency information.
it fails to identify the triomodule and clusters these files together with another files.
for example xpath.c is the gateway 1536software architecture recovery with information fusion esec fse december san francisco ca usa source code text from source codedependency function importancenlocweight with dependency type folder structurestatic code analysis add weight latent dirichlet allocation remove unreasonable folders final weighted graph code comment topic informationtopic correlation corr final folder structure get information weights add topic info adjust weight using foldermerge community detection stage information extraction stage information processing stage clustering stage information integration .
.
.
text .
.
.
dependency .
.
.
function importance .
.
.
add dependency weight .
.
.
folder structure .
.
.
add function importance weight lda .
.
.
folder cleaningfinal weighted graph .
.
.
topic correlation .
.
weight adjustment .
.
community detection information extraction information refinementstage clustering information fusion source code source code weighted dep.
graph final folder structure weighted topic correlation .
.
topic info integration .
.
weight assignment merge clustering text informationdependency function importancedependency weight ing folder structurelda folder cleaningfinal weighted graph topic information community detection information extraction information refinement3 information fusion source code source code weighted dep.
graph final folder structure weighted topic correlationfusing clusteringfunction information texual infomationdependency function importancedependency weight ing folder structurelda folder cleaning weighted graph topic information community detection iii.b information extraction iii.c information refinement iii.d information fusion source codefusing iii.e clusteringfunction informationweighted dep.
graph topic correlation cleaned folderstexual infomationdependency function importancedependency weight ing folder structurelda folder cleaning weighted graph topic information community detection iii.b information extraction iii.c information refinement iii.d information fusion source code iii.e clusteringfunction informationweighted dep.
graph topic correlation cleaned foldersfusing texual infomationdependency graph function importancedependency weight ing folder structurelda folder cleaning weighted graph topic information community detection iii.b information extraction iii.c information refinement iii.d information fusion source code iii.e clustering?weighted dep.
graph topic correlation cleaned foldersfusing texual infomationdependency graph folder structureweighted dep.
graph .
information extraction .
information refinement .
information fusion source code .
clusteringweighted dep.
graph topic information cleaned foldersweight dep.
weight entity lda clean folder folder cleaningentity weight ing fuse info.
community detectiontexual infomationdependency graph folder structureweighted dep.
graph .
information extraction .
information refinement .
information fusion source code .
clusteringweighted dep.
graph topic information cleaned foldersweight dep.
weight entity lda clean folderfuse info.
community detection figure overview of our approach of its xpath module and many other modules communicate with this module via xpath.c .
especially the triomodule has the closest dependencies related to xpath.c which misleads bunch to cluster xpath.c into the triomodule.
however according to the dependency analysis result of the depends tool only less than of entity level dependencies related to xpath.c are related to the triomodule.
this indicates that although dependency information is proven to be useful in architecture recovery inadequate usage could also bring biases to the architecture recovery.
sarif fuses various aspects of information from the source codes including both dependency and textual information while using them in a fine grained manner to cluster the software components more precisely.
according to our experiment the triomodule can be perfectly identified by sarif and the recovered result is much more similar to the ground truth compared to bunch and arc.
methodology the overview of sarif is presented in fig .
sarif first extracts three types of information from the given software.
then it refines the information to obtain fine grained information with the help of program analysis and machine learning techniques.
next it fuses the information into one weighted graph to represent relations of the entire project.
last based on the graph sarif clusters the system into different groups to recover the architecture.
.
information extraction in this section we will introduce the details of how to extract the three aspects of information i.e.
dependencies textual information and folder structure.
.
.
dependencies.
the dependency is the most important information for recovering the architecture of software systems as it contains all the structural interactions in the software system.
we use a dependency graph to represent the dependencies between entities in software systems.
the nodes of the graph represent software entities e.g.
file class method function variable etc.
and the edges represent the relation between two entities e.g.
function call file import etc.
.
sarif uses depends to generate dependencies for target software systems.
depends generates dependency information by observing the syntactic structures among software elements in c c java projects.
it can detect entities include functions methods variables type definitions etc along with types of relations between these entities.
in this sense the dependency model of depends can tell exactly the categories of both the dependency relation and the nodes.
this detailed information allows us to generate more accurate dependency graphs of software systems while previous studies suggest that more accurate dependencies generally result in better architecture recovery results .
.
.
text from source code.
textual information can help the recovery since the developers embed their domain knowledge into code texts by means of comments names of methods classes etc.
for example as discussed in sec triomodule contains word trioout of a total of in the entire system which is a strong indication of the boudary of the triomodule.
the textual information of a software system mainly comes from two sources the source code and the documentation.
sarif mainly focuses on the texts in the source code since sarif is a fully automated tool and it is hard to automatically match the documentation to their corresponding codes.
sarif uses ctags and comment parser to extract words from different fields of functions and comments.
to reduce the noise introduced by unimportant words only the following kinds of words will be extracted filename definitions of classes functions methods and global variables comments.
.
.
folder structure.
the folder structure shows how the developers organize the files which gives hints to recover its architecture.
therefore it is included as one of our information sources.
however not all of the folder structures can provide positive suggestions for the architecture recovery .
for example many c c projects organize their header files into a separate folder which provides no insight into the architecture.
moreover some folders like the mapred lib folder of hadoop v0.
contains multiple libraries for various purpose.
however these libraries actually belong to multiple independent submodules.
as a result grouping all the files within this folder together is not reasonable.
we will address how to eliminate these improper folder structures in the next step.
.
information refinement .
.
dependency graph.
in this step we start with the unweighted dependency graph extracted in sec .
.
.
to make it reflect the architecture more accurately we apply two types of weights to the graph.
the first weighting is for entities i.e.
the nodes in the dependency graph since different entities have different degrees of impact on architecture.
for example interfaces of modules have more influence on the architecture than the modules internal implementations thus they should have higher weights.
the second weighting is for dependencies i.e.
the edges in the dependency graph since different types of dependencies carry different amounts of architectural information .
weigh entities by importance.
the dependency graph comprises different types of entities e.g.
files functions variables etc.
it is unfair to evaluate the importance of all entities directly since their granularity is not comparable.
therefore we start with evaluating the function level entities then derive the weight or entities at other granularities based on it.
the reason for prioritizing function level entities is that entities at the variable level have limited information and entities at the class file level are too coarse grained.
to measure node importance in a graph a widely used algorithm is pagerank .
pagerank evaluates a node s importance by its popularity i.e.
a node is important if it can be easily accessed 1537esec fse december san francisco ca usa yiran zhang zhengzi xu chengwei liu hongxu chen jianwen sun dong qiu and yang liu by many other nodes.
however in the dependency graph pagerank may overvalue utility functions like parsers.
for example in libxml2 the most important function identified by pagerank is xmlstrequal which is a utility function that compares the strings in xml.
however this function has nearly no indication on the architecture since it is accessed by almost every module in libxml2 .
conversely interfaces which are impactful on the architecture tend to be less accessible thus undervalued.
this scenario contradicts our starting point that interfaces are more influential on architecture than internal implementations like utilities.
to address this we use inversed pagerank ipr which is a variant of pagerank to determine the importance of each function.
different from pagerank ipr is designed to find the influential nodes i.e.
the nodes that can access as many as other nodes.
intuitively module interfaces have access to the module s internal implementations thus will be assigned with higher importance.
for example in libxml2 the functions with top importance are main function ofxmllint.c which is the top interface of the xmllint module.
xmlxpathnewcontext which is one of the most commonly used api of the xpath module.
to be more detailed ipr assigns importance to the functions in two ways.
the first portion of importance is assigned based on the count and quality of a function s callees.
this method iteratively computes the ipr values by propagating importance from callees to their callers until the results converge.
the second portion of importance is equally spread across all nodes.
a specific parameter known as the damping factor d decides the distribution of overall importance between these two sections.
mathematically ipr is defined by the following equation ipr ni d ni p ni ipr nj in degree nj d n wheren1 nnare nodes in graph nis the total number of nodes p ni are the direct successors of ni dis the damping factor.
in sarif dis set to .
which follows the recommendation of the algorithm s authors .
the result of ipr ranges from to .
to assess the importance of the functions a function dependency graph is extracted from the dependency graph by keeping only the function level entities and dependencies.
the importance of a function is then defined as its ipr result.
based on the importance of functions the importance of higher level entities e.g.
file class is defined as the sum of the functions they contained.
the reasoning behind using summation is that the dependencies associated with these higher level entities are considerably fewer than those related to functions.
utilizing the sum is to guarantee that these higherlevel entities which are also crucial for the architecture will not be overlooked due to their lesser dependency count.
on the other hand the importance of lower level entities is determined by evenly dividing the significance of the parent entity amongst them to prevent them from overshadowing the other entities.
weigh dependencies by type.
entity level dependency has many different types and different types of dependencies have different level of indication on the architecture.
for example according to the previous study function call dependencies can reflect much more architectural information than uses type i.e.
a function accesses a specific type dependencies.
however this limitedtable the optimized weights of different types of dependencies.
mixin type is not applicable to c c java.
implement throw call create impllink extend use .
.
.
.
.
.
.
parameter import cast return contain mixin .
.
.
.
.
n.a.
understanding of their relative usefulness in architecture is not enough for us to manually assign appropriate weights to all types of dependencies extracted by depends .
therefore we designed an algorithm to automatically assign reasonable weights to each type of dependency.
since the target of this weighting is to make the dependency graph reflect more architectural information our algorithm optimizes the weights to make the weighted dependency graph can be clustered into a better architecture.
specifically our algorithm consists of steps randomly set the weights of different types of dependencies extract the unweighted dependency graph then weigh the edges based on step cluster the weighted graph using community detection algorithm calculate the girvan newman modularity quality mq of the clustering result where mq is a commonly used metric for measuring the quality of a clustering result iterate step to to find the weight that can maximize the modularity.
to get a reasonable result from our algorithms we collected project for each of the languages supported by sarif i.e.
c c java .
the projects are collected by searching for popular projects on github to ensure their quality and the projects that will be used in the following evaluation sections have been excluded to prevent presenting overfitting results.
the algorithm is implemented with hyperopt optimizer.
the weight of each type is limited between .
and the converge condition is set to when the loss is less than 1e and the optimization results are shown in table .
the resulting weights basically align with our intuitive expectation and the previous .
for example impllink similar to function call is assigned a much higher weight than use similar to uses type .
this result will be combined with the entity importance to form a weighted dependency graph.
merge weight results.
based on both the weight of dependency type and related entities the final dependency weight is defined as weight e type weight e impt src impt dst wheretype weight e is defined in table impt src andimpt dst are the importance of the dependency s source and target entity in eq respectively.
a file level dependency graph is then formed by grouping all entities into it s corresponding files.
the weight of dependency between two files is defined as the sum of its corresponding entity level dependencies.
.
.
textual information.
the refinement of textual information comprises three steps preprocessing weighing and topic modeling.
preprocessing.
sarif preprocesses the extracted raw text by splitting the terms according to common naming conventions of programs such as snake case or camel case.
for example the function name plotfigure will be split into plot and figure .
then sarif applies commonly used preprocessing techniques including lemmatization and stop word removal to refine the words.
weighing.
sarif assigns different weights to the words based on three criteria the importance of texts extracted from different 1538software architecture recovery with information fusion esec fse december san francisco ca usa parts of the source code is not equivalent.
for example a word in comments is likely to be less important than that in function declarations.
thus the texts will be weighted according to where they come from a word from a less important entity is less likely to be useful.
therefore sarif modifies the weights of words according to its source entity as defined in sec .
.
sarif uses tf idf to decrease the weight of common words.
the final weight of each word will be the product of the three weights weight w weightsrc w weightentity w tf idf w topic modeling.
sarif summarizes the words from each file into a topic via latent dirichlet allocation lda .
specifically it trains a lda model with the weighted words.
each source file is assigned with a topic embedding using the trained model.
last sarif calculates the correlation between files on the topic embeddings.
.
.
folder structure.
in this step we aim to filter out folders that most likely to be useless.
as discussed in sec .
.
most of the folders that have no indication on the architecture contain files from multiple modules.
these files have little dependencies with each other but are strongly related to files outside the folder.
based on this characteristic our filtering algorithm is designed to remove folders that have high inter dependencies and low innerdependencies.
specifically if the folder has more inter dependencies than the inner dependencies it will be eliminated by being merged into its parent folder.
.
information fusion based on the processed information sarif fuses them to have a more comprehensive understanding of the system architecture.
to this end sarif first evaluates the quality of each type of information and assigns weight accordingly.
then sarif fuses the information into a unified graph.
.
.
weight assignment.
the three types of information can reveal different degrees of ground truth architecture from different perspectives.
intuitively the information with higher quality i.e.
can produce results closer to the ground truth architecture should be assigned a higher weight.
thus we need to estimate the quality of the three types of information then assign weights accordingly.
among the three information the dependency information represents the relations between software entities which objectively reveal the interactions among the modules.
such characteristics ensures that the dependencies are highly likely to have the capacity to reveal an adequate portion of the architecture.
however the quality of textual information and folder structure is not as objective as the dependencies as they are produced by the developers and inherently subjuctive.
therefore their quality could vary greatly in different software projects.
for instance recall the motivating example in sec the triomodule in libxml2 has strong textual characteristics so that the textual information can be a good indicator to distinguish modules.
however all the source files of libxml2 are located in its root folder which means that the folder structure is useless in the architecture recovery.
moreover in other software projects the situation could be completely different.
the builtin module of bash4.
for example contains implementations of built in commands likecd echo etc.
due to the wide variety of commands within the module its textual information is overly general which reduces itstable the weight of information of libxml2 andbash .
project textual information folder structure libxml2 .
.
bash .
.
usefulness in recovering the architecture.
consequently text based techniques such as arc struggle in identifying the builtin module by grouping its files with those having similar text feature from other modules.
despite the builtin module lacks distinctive textual features its content is identical to that in the builtin folder which means the module can be easily identified by the folder structure.
to handle such extreme variance in the quality of textual information and folder structure a dynamic weight is assigned to them by comparing the similarity of the architecture recovered through these information with the architecture recovered through dependencies.
the underlying reason for such an assignment is that the architecture recovered through dependencies is expected to have some similarity with the ground truth due to the inherent relations between dependencies and architecture.
for textual information or folder structure if its quality is high the recovered architecture based on it should have a higher similarity with the ground truth and therefore more likely to have higher similarity with the architecture recovered through dependencies.
to implement this design sarif first recover the architecture based on only one type of information.
the ways sarif recover architecture with each type of information solely are as follows dependency the weighted dependency graph in sec .
.
is utilized for clustering in which sarif groups the files through the community detection algorithm .
textual an undirected weighted graph is generated based on the topic correlations between files from sec .
.
.
the nodes in the graph represent files and the edges are weighted according to the topic correlation between the two files.
the graph is then clustered by the hierarchical clustering algorithm with the complete linkage metric .
folder the clustering result is formed by the filtered folder structure from sec .
.
.
next the similarities between different types of information are measured based on the a2aadjsimilarity of the recovered architectures where a2aadjwill be detailed in sec .
.
the weight of each supporting information is defined as weights a2aadj adep as wheresrefers to textual information of folder information asis the recovered architecture of the corresponding information.
our method is tested on libxml2 andbash to confirm its ability to detect differences in the quality of textual information and folder structure.
the results of the weight assignment for textual information and folder structure are presented in table which match well with our manual inspection of the two projects.
.
.
textual information integration.
based on the weight of textual information the weight of edges is adjusted according to the textual correlations between the corresponding files.
all existing edges are modified with a coefficient coeft coeft corr weighttext 1539esec fse december san francisco ca usa yiran zhang zhengzi xu chengwei liu hongxu chen jianwen sun dong qiu and yang liu wherecorr is the correlation of topic embedding between the two files on the edge and weighttext is the weight of textual information we assigned in eq .
moreover the purpose of applying coeftis to increase the weight of dependency between two files if their textual features are similar and vice versa.
to this end if coeftbetween two files are greater than .
which means their textual features are highly similar a bi directional edge will be added between the files even if they do not have structural dependencies.
.
.
folder structure integration.
as the files in the filtered folders are more likely to belong to the same modules the weight of edges between files that are located in the same folder are increased by coeff weightfolder whereweightfolder is the folder weight defined in eq .
coeff ranges from to .
this coefficient is designed to enhance relations between files in the same high quality folder since a wellstructured folder is a strong indicator of a module.
.
.
generate final graph.
we generate the final graph by updating the edge weights of the input dependency graph from sec .
.
weightfinal weightori coeft coeff whereweightoriis the weight defined in eq .
thus far the graph carrying all types of information is generated by updating the weights and is ready for clustering.
.
clustering the final graph in this final step we cluster the graph that carries the fused information into modules.
we use the community detection algorithm proposed by clauset et al.
for clustering.
the algorithm cluster a graph by maximizing its modularity which is defined as q 2m i j aij kikj 2m ci cj whereaijis the weight of edge from node itoj kidenote the sum of weights of edges incident on i m is the total weight of edges is a parameter also known as resolution which affect q s preference on cluster size.
the reason we choose this community detection algorithm is that it runs much faster than most of the existing clustering algorithms like the bunch search algorithm especially when the software system is large.
the granularity i.e.
the number of clusters of the final architecture can be controlled by adjusting theresolution parameter.
since different granularity of architecture might be desired for different use cases no universal standard can be established for the granularity of architecture.
therefore the resolution of sarif is user controllable and general correlation between the number of clusters and the resolution is shown in fig for user reference.
for users unsure about what level of granularity to choose we suggest to clusters as a suitable setup for quickly understanding an architecture.
accordingly the default value of resolution is set to .
.
similarity metrics to evaluate architecture recovery results a common method is to compare the result with human labeled ground truth architectures.
in previous studies the three most commonly used metrics are .
.
.
.
.
.
.
.
resolution050100150200number of clusters .
figure number of cluster v.s.
resolution mojofm a2a and c2ccvg.
however each of these metrics has its own limitations.
in this section we will discuss these limitations and introduce two new metrics to address them.
.
previous metrics and their limitations mojofm is defined by the following equation mojofm m mno a b max mno a b wheremno a b is the minimum number of move or join operations needed to transform an architecture aintob.
mojofm is the most commonly used metric for comparing architectures.
however mojofm prefers architectures with many small clusters due to its low cost for merging two clusters.
architecture to architecture a2a is a distance based measurement which is defined as a2a a b mto a b aco a aco b wheremto a b is the minimum number of operations needed to transform architecture aintob aco a is the number of operations needed to construct architecture afrom a null architecture.
a2a has nearly no preference over the number of clusters.
however studies show that it has a limited variations which means an architecture that greatly differs from the ground truth may not receive a low score as it deserves.
c2ccvg is a metric defined by the number of similar clusters between two architectures c2ccvg a b simc a b a where a is the number of clusters in a simc a b is the number of similar clusters between aandb where two clusters are defined to be similar if their overlapping percentage passed the user defined threshold thcvg.
according to our experiments which will be detailed in the following section c2ccvg s performance could be unstable in many scenarios.
.
new metrics to address the limitations of previous metrics two additional metrics are introduced in this paper.
adjusted rand index ari is a well known metric used to quantify the degree of similarity between two distinct partitionings of a single entity set.
the versatility of ari allows it to be applied in comparing two architectures of the same system.
in general the ari computes the proportion of entity pairs that are consistently clustered in both architectures.
the higher proportion of consistent 1540software architecture recovery with information fusion esec fse december san francisco ca usa table information of projects in evaluation.
name version language nloc file cluster domain archstudio java 238k ide bash .
c 115k shell chromium svn c .7m browser hadoop .
.
java 225k distributed framework itk .
.
c .23m image processing oodt .
java .4k data management libxml2 .
.
c .1k xml parser hdc 46ff87 c .7k camera interface hdf 0e196f c 153k driver subsystem pairs the more similar the two partitionings are.
mathematically ari is calculated as follows ari ij ni j h i ai j bj i n 2h i ai j bj i h i ai j bj i n in this formula aiandbjcorrespond to the total count of entities present in the i th cluster of architecture aand thej th cluster of architecture b respectively.
nijis the count of entities that are both in the i th cluster of architecture aand thej th cluster of architecture b.ari is widely recognized and extensively tested exhibiting no evident drawbacks such as a low dynamic range or a bias towards cluster size.
our evaluation which will be detailed in the following section further proved its effectiveness.
a2aadjis designed by us to address the issue of limited variation observed in the a2ameasure.
the limited variation in a2aoriginates from the excessively large denominator which is the sum of the architecture costs aco a aco b used in the calculation of mto.
to alleviate this issue we decompose the distance i.e.
mto of a2ainto two distinct components mtom which corresponds to the cost of reassigning entities to a different cluster and mtoar reflecting the cost of adding or removing entities clusters.
our adjusted measure a2aadjis then defined as follows a2aadj mtom a b mtomaxm a b mtoar a b aco a aco b where nshared min nca ncb nshared ndiff max nca ncb ndiff abs nca ncb nshared ndiff max nca ncb nshared andndiff denote the number of shared and unshared elements respectively.
ncaandncbare the counts of clusters in architecture aandb.
the measure a2aadjquantifies the dissimilarity between two architectures with two costs the cost of reassignment of shared entities to different clusters the cost of an addition or removal of entities or clusters.
each of the two costs is individually normalized by their appropriate denominators.
the reassignment cost mtomis normalized by the maximum possible reassignments mtomaxm given the number of entities and clusters as solved in .
the addition removal cost mtoaris normalized by the cost of building both architectures from scratch.
in the final step the costs associated with these two aspects are weighted according to the number of shared and unshared components.
by normalizing the two costs using a more appropriate denominator the resulting dynamic range of a2aadjis significantlylarger than that of the original a2ametric.
this expanded dynamic range will be further proved in the evaluation section.
evaluation to evaluate sarif we aim to answer the following rqs rq1 what is the performance of the proposed similarity metrics in evaluating architecture recovery accuracy?
rq2 what is the accuracy of sarif in architecture recovery compared to related works?
rq3 what is the contribution to the architecture recovery for each type of information and weighting?
.
experimental setup .
.
baseline selection.
our selection of baseline techniques consists of two parts.
the first part comprises tools who show the most promising results in the previous empirical studies including acdc bunch arc wca and limbo.
acdc is a pattern based clustering algorithm.
bunch is a searchbased technique that recovers architecture by optimizing turbomq.
arc recovers architecture with topic information rather than dependency.
wca and limbo are two hierarchical clustering algorithms that can be applied to architecture recovery.
the second part contains the latest research on architecture recovery.
we reviewed a total of papers related to architecture recovery technique in the recent five years .
of papers contain link to their supplementary materials and of them provide their artifact for reproduction.
among the techniques evol and codesum take some intermediate data as input and we failed to generate such data for them since the data formats are not documented.
thus the second part of baseline techniques comprises the remaining two techniques fca and sade .
fca clusters software systems by performing operations on the dependency matrix.
it has good scalability and can cluster very large software systems within a reasonable amount of time.
sade clusters software systems by combining the call graph with the semantic similarity between the user defined modules.
.
.
baseline implementation and parameters.
we obtained the executable of acdc bunch fca and sade from the author s websites.
for wca limbo and arc we adopted the implementations from arcade .
since wca limbo and arc allows user to select a preset number of clusters we ran these tools with to clusters with an incremental step of .
in addition the arc tool takes the number of concerns as input as well.
we experimented arc with to topics with a step of .
.
.
data collection.
our evaluation utilizes a two part dataset.
the first part includes projects with well established architectures which are used as a benchmark for assessing the accuracy of sarif and comparing sarif to the baselines.
open source projects with ground truth labeled in previous studies are included in this part archstudio4 bash .
chromium hadoop itk and oodt .
moreover projects libxml2 distributed camera and drivers framework have been labeled with our industry collaborators.
details of these projects are listed in table .
the size of 1541esec fse december san francisco ca usa yiran zhang zhengzi xu chengwei liu hongxu chen jianwen sun dong qiu and yang liu a mojofm a2a ari and a2aad j b c2ccvgw different thresholds figure metrics results of the merge experiment.
a mojofm a2a ari and a2aad j b c2ccvgw different thresholds figure metrics results of the cluster experiment.
these projects as measured by lines of code ranges from about 20k to 11m which is either equivalent to or exceeds the scale of projects used in previous studies .
the second part comprises popular projects from github.
the purpose of including this extensive dataset is to demonstrate the generalizability of sarif.
to this end we collected projects for each of the languages that supported sarif i.e.
c c and java.
to avoid biased distribution of project size we gathered projects that fell within three different size ranges mb mb mb and mb.
we used the github api to search for repositories with the highest number of stars and excluded the projects used in the parameter optimization detailed in sec .
.
.
for each size range and language combination projects are collected resulting in a total of projects.
.
metric performance evaluation rq1 in rq1 we aim to evaluate the metrics on their ability to measure the similarity between architectures with two experiments.
all five metrics discussed in sec will be evaluated in this rq mojofm a2a c2ccvg ari anda2aadj.
sincec2ccvghas a threshold parameter four different thresholds are tested following the settings of previous studies and .
experiment to evaluate the ability of the metrics in measuring the architecture similarity we design an experiment to simulate the situation where the target architecture becomes less and less similar to the ground truth.
the expectation is that the metrics should give a lower score when the target architecture becomes dissimilar.
specifically we start by comparing the ground truth architecture of hadoop with itself where all the metrics will give the highest score i.e.
.
.
then we modify the architecture by merging its clusters and comparing it with the original ground truth again to get the metric scores.
as we merge more clusters the resulting architecture will be more different from the original one to which a lower score should be given by the metrics.the results are shown in fig where the x axis shows the number of clusters merged i.e.
the degree of dissimilarity and the y axis shows the metric scores.
four metrics in fig a fit our expectation since their scores keep dropping during the merge.
however the score of a2a is not dropping as much as the rest.
it complies with the conclusion in that a2a has a relatively small variation range.
it will always give a relatively good score for all kinds of architecture.
as for c2ccvgshown in fig b the trend of score does not align well with our expectation.
when the threshold is set to .
the score is stucked at which means c2ccvgcould produce totally useless result if the threshold is set inproperly.
moreover for the other thresholds c2ccvgis overall dropping but not stable enough it fluctuates significantly during the merge process which mismatches our intuitive expectation.
experiment in the second experiment we aim to test the metric quality when the target architecture consists of different numbers of clusters.
to achieve this we generate a set of well separated clusters of data as the ground truth.
then we use a k means algorithm to cluster the data into clusters as different architectures.
the architecture with clusters is the same as the ground truth.
therefore all the metric scores will be .
if the number of clusters in the architecture becomes less or more than the architecture will become different from the ground truth which is expected to receive a lower score.
the results are shown in fig where the x axis shows the number of result clusters.
as expected all the metrics give a score of to the architecture with clusters.
similar to the last experiment four metrics shown in fig.
a are in line with our expectations.
specifically mojofm gives all the architectures with more than clusters a score close to which suggests that it favours smaller clusters.
as for c2ccvg shown in fig b though it can sometimes give a reasonable result it still suffers from two limitations similar to the last experiment a proper threshold need to be selected the trend is not stable enough even if a proper threshold is set.
based on the result of these two experiments we do not find any substantial limitations of ari and a2aadj.
for mojofm it can also generate reasonable scores in most cases though it favors partitions with a higher number of clusters.
for a2a the trend of score is satisfactory but its dynamic range is limited.
as for c2ccvg its performance relies heavily on a proper threshold while may still gives unreasonable results even if the threshold is appropriate.
answering to rq1 ari and a2aadjare the best metrics in the task of architecture similarity evaluation.
mojofm a2a and c2ccvghave their limitation in particular scenarios.
.
recovered architecture evaluation rq2 in this rq we evaluate sarif against baselines by comparing their recovered architectures against human labeled ground truth.
.
.
metric results.
we evaluate sarif and all the previous tools on the ground truth projects with the setups detailed in sec .
.
although the outcome of rq1 indicates that mojofm a2a and c2ccvghave their limitations all five metrics are included in our evaluation for the sake of completeness.
the threshold of c2ccvgis set to .
since it gave the most reasonable result in rq1.
a total 1542software architecture recovery with information fusion esec fse december san francisco ca usa table similarities between recovered architectures and ground truths.
top scores of each project are highlighted.
all metric values are timed by .
abbreviations m mojofm a a2a c c2ccvg r ari j a2aadj technique archstudio bash chromium hadoop itk oodt hdc hdf libxml2 m a c r j m a c r j m a c r j m a c r j m a c r j m a c r j m a c r j m a c r j m a c r j acdc arc bunch nahc bunch sahc fca limbo sade wca ue wca uenm sarif table averaged scores of recovery techniques.
technique mojofm a2a c2ccvg aria2aad j acdc arc bunch nahc bunch sahc fca limbo sade wca ue wca uenm best base.
sarif sarif v.s.
base.
.
.
.
.
.
table result on oodt with refined granularity.
technique mojofm a2a c2ccvg aria2aad j pre.
best sarif sarif v.s.
base.
.
.
.
.
.
of unique experiments are carried out to get all scores of each metric for each tool on each project.
the results of the metrics are shown in table .
out of a total of metrics on projects we achieved top scores for .
results and top scores for .
results among all algorithms.
we summarized the averaged metric result in table .
the best base.
presents the best scores of the baseline techniques for each metric.
sarif is .
more accurate than the previous best results in terms of the average of the leading percentage of all five metrics.
the only project we failed to achieve top is oodt.
the reason is that for oodt the ground truth architecture has clusters while sarif only recovers the project into clusters with the defaultresolution parameter.
such a huge discrepancy leads to the low metric scores of sarif.
as discussed in sec .
our default resolution normally gives a result of to clusters which we think is suitable for humans to understand.
however there is no standard for the granularity of architecture since different granularity will be desired in different use cases.
our default granularity mismatches the ground truth architecture of oodt which is much fine grained.
therefore we conducted an extensive experiment that increases theresolution parameter by times to make the granularity of sarif comparable to the ground truth of oodt.
with the updated resolution sarif clusters oodt into clusters and the corresponding metrics are shown in table .
with the updated resolution our result is much more accurate than the baseline techniques.
in contrast to our default coarse grained granularity other leading techniques like acdc fca or sade tend to decompose the system into much more clusters.
this is the most common reasontable time consumptions in seconds .
technique as4 bash chromium hadoop itk oodt hdc hdf libxml2 acdc .
.
.
.
.
.
.
.
.
arc .
.
.
.
.
.
.
.
.
fca .
.
.
.
.
.
.
.
.
limbo .
.
.
.
.
.
.
.
.
sade .
.
.
.
.
.
.
.
.
wca ue .
.
.
.
.
.
.
.
.
wca nm .
.
.
.
.
.
.
.
.
bunch nahc .
.
.
.
.
.
.
.
.
bunch sahc .
.
.
.
.
.
.
.
.
sarif .
.
.
.
.
.
.
.
.
for the metrics that we fall behind.
for example on archstudio acdc results in clusters while sarif generates only clusters.
as mentioned in rq1 mojofm favors small clusters in nature.
consequently acdc beats us on archstudio for mojofm while we achieved a higher score for the other four metrics.
.
.
time consumptions.
to evaluate the scalability of sarif the time consumption for the architecture recoveries is measured and shown in table .
the experiments are conducted on an ubuntu server with dual core intel cpus and gb ram.
the results show that sarif can recover the architectures within a reasonable timeframe small and medium size projects can be finished within a few minutes while projects as large as chromium which has more than million lines of codes can be finished with about minutes.
it can be found that the time consumption of sarif is about linearly proportional to the size of the project measured by the lines of code which ensured our scalability on large projects.
sarif may take longer time than some alternative tools since we utilized both dependency and textual information while other tools typically only use one dimension.
the inclusion of these two dimensions inevitably increased the time cost since extracting them could be time consuming.
for example among a total of .
seconds when recovering chromium with sarif .
seconds .
were spent on the information extraction while the other processes only take less than minutes.
.
.
case study.
to examine the practicalness of sarif we perform a case study which compares the architecture recovery results of sarif against the ground truth labeled by the developers of a real world project distributed camera .
fig a shows its architecture diagram from its source repository.
the diagram reveals that it has two primary components the camera source and the camera sink.
both the source and sink are comprised of three components the manager channel and data processing.
however while labeling the ground truth architecture based on the diagram we find it hard to split the implementation of source and sink.
for example a total 1543esec fse december san francisco ca usa yiran zhang zhengzi xu chengwei liu hongxu chen jianwen sun dong qiu and yang liu distributed camerasdkdistributed camera camera source manager channel source camera data postprocessdistributedcamerasource camera source manager channel source camera data postprocessdistributedcamerasource camera sink manager channel sink camera data preprocessdistributedcamerasink camera client virtual camera hdfcamera data uploadhdfservice a reference architecture b recovered v.s.
ground truth figure reference architecture of distributed camera and comparison between the recovered architecture and the ground truth.
of source files are related to data processing but of them are common implementations shared by both the source and sink sides.
following the architecture diagram we labeled the data processing files into modules source sink and common though there are only files in the source and the sink part.
the other modules with similar issues like channel related codes are also marked in this way.
last we labeled and put the files that we cannot decide on their module with the diagram into a general module.
we then recover its architecture with sarif.
the result is shown in fig b with distribution map .
each colored small square represents a file.
the files in the same box mean that they belong to the same human labeled ground truth module and the colorcoding represents the result of sarif.
as shown in the figure sarif successfully identifies the data processing related module colored light pink and channel module colored pink i.e.
sarif clusters file with similar functionalities like data processing together rather than splitting them by source and sink.
although this result does not align with the reference architecture we believe that the result of sarif is even more reasonable.
as previously stated out of files in the data processing module are shared by both source and sink.
from an implementation perspective it is unfair to split the module just because the minority four files can be well separated.
.
.
extensive validation.
to further validate the generalizability of sarif an extensive experiment is carried out on github projects as detailed in sec .
.
.
to quantitatively compare the result of sarif with baselines a ground truth architecture is required.
however labeling a reliable ground truth architecture from scratch requires extremely high manual efforts and extensive communication with the corresponding developers which is time consuming.
for example labeling chromium took years from the previous researcher .
given our aim is to generate more ground truth data to test the generalizability of sarif this accurate but timeconsuming method is not feasible.
therefore we adopted an alternative labeling method.
for each repository if it contains a reference architecture diagram we labeled the architecture accordingly.
while this alternative method may not be as accurate as traditional labeling it should sufficiently represent the developers architectural perception.
using this method we labeled a total of projects.
we compared sarif with baselines based on these labeled architectures.
metrictable accuracy with incomplete information weight.
variant mojofm a2a c2ccvg aria2aad j sarifavg v.s.
sarif v.s.
best base.
.
.
.
.
.
w o folderavg v.s.
sarif .
.
.
.
.
v.s.
best base.
.
.
.
.
.
w o textavg v.s.
sarif .
.
.
.
.
v.s.
best base.
.
.
.
.
.
dep.
onlyavg v.s.
sarif .
.
.
.
.
v.s.
best base.
.
.
.
.
.
w o entity impt.avg v.s.
sarif .
.
.
.
.
v.s.
best base.
.
.
.
.
.
w o dep.
typeavg v.s.
sarif .
.
.
.
.
v.s.
best base.
.
.
.
.
.
w o dep.
weightavg v.s.
sarif .
.
.
.
.
v.s.
best base.
.
.
.
.
.
results indicate sarif s accuracy exceeds baselines by .
.
all data related to this extensive experiment including project list labeled architecture metric results and recovered architecture for all projects are available on our website for public verification.
answering rq2 sarif has the best architecture recovery results compared to related works with a .
higher in accuracy on average.
an extensive experiment verified sarif s generalizability.
in the case study it produces a more reasonable architecture than the human label one.
.
ablation study rq3 in this rq we aim to evaluate the impact of two factors on the accuracy of sarif fusing various types of information and using fine grained dependency weighting.
regarding the information fusion we will investigate the impact of excluding textual information or folder structure while dependency cannot be totally excluded since our weight assignment algorithm is based on it.
as for the weighting of dependencies we will evaluate the impact of removing the weights for dependency types and entity importance.
table summarizes the average decrease in accuracy across projects after removing information or weighing and the comparison between incomplete sarif and the best of the baseline techniques.
as shown in the table sarif s accuracy always decreases significantly after removing each type of information or weight.
the removal of folder structure has the greatest negative impact on most metrics.
the scores of different metrics declined by about to except for a2a whose variation range is too small as discussed in rq1 .
this aligns with our expectations since some of the human labeled ground truth architectures are strongly related to their folder structures .
removing textual information also reduces the scores by about showing that the textual similarities can strongly indicate the architectural modules.
when both textual information and folder structure are removed our metric scores become comparable to the best baseline scores.
this suggests that fusing multiple information sources is necessary for sarif to outperform the baselines.
as for dependency weighing removing weight by entity importance or dependency type leads to a decrease in accuracy of about 1544software architecture recovery with information fusion esec fse december san francisco ca usa to .
removing both weighting results in a drop of about to in accuracy which is comparable to the decrease by removing textual information or folder structure.
this result confirms the importance of using fine grained dependency weighting.
answering rq3 both information fusion and fine grained dependency weighting are critical to sarif.
removing either dependency weighting or one type of information will result in an accuracy drop of approximately to for different metrics.
removing two types of information will result in a low accuracy that comparable to the best baseline techniques.
threats to validity we discuss the threats to the validity of our results to comprehend the strength and limitations of sarif.
the first potential threat to validity relates to the representativeness and generalizability of our evaluation on sarif.
the ground truth architectures used in the evaluation only span projects which might not seem sufficient for robustly inferring generalizability.
to alleviate this concern we have gathered all publicly available ground truth architectures from previous studies and labeled several additional ground truths with our industrial partners.
moreover an extensive evaluation is carried out on github projects to enhance the generalizability.
the second threat pertains to the parameter selection within sarif.
assuring the optimal parameter setup for sarif in the context of architecture recovery is difficult.
we have strived to make our design more reasonable by following the original algorithm designer s suggestions.
nevertheless it is still uncertain if our parameter selections are the best.
the final threat related to the accuracy of dependency extraction.
even though depends is one of the most accurate open source dependency extractors the dependency extracted by depends could be inaccurate due to the complex nature of programming languages and the limitation of static analysis.
related work the architecture recovery techniques are to automatically recover software architectures from their implementations.
many of these techniques use structural dependencies as their information source.
bunch clusters source files of software systems by maximizing the modularity quality mq of the dependency graph.
wca is a hierarchical clustering technique that commonly used in the scenario of software clustering.
limbo is another commonly used technique employing information theory concepts for software clustering.
dagc is a technique that similar to bunch but optimized its search space.
mca and eca introduced multi objective optimization into software clustering.
cooperative clustering technique cct is a consensus based clustering technique that is utilized in the software clustering scenario.
mohammadi et al.
used available knowledge in the dependency graph to create a neighborhood tree to drive the clustering.
in addition to these techniques that use static dependencies xiao et al.
shows that dynamic dependencies have some merits.
apart from dependency based techniques most studies use textual information for architecture recovery.
arc recovers thearchitecture using concerns.
zbr is a recovery technique based on natural language semantics and it partitions textual information into different zones to form clusters.
risi et al.
uses lsi to extracts textual information then cluster the system with k means.
kargar et al.
constructed a semantic dependency graph to replace the dependency graph to take the advantage that the semantic information is independent of programming languages.
they further employed nominal information i.e.
the name of artifacts to improve their previous work in .
evol is a semantic based technique that considers semantic outliers filtration and label propagation to increase their accuracy.
the textual information based techniques may outperform dependency based ones in specific scenarios as shown in study .
there are also some techniques that use hybrid information for clustering.
mkaouer et al.
proposed a many objective searchbased approach using nsga iii.
chhabra et at.
extracted combined features with different coupling schemes to explore how to combine the dependencies and textual to produce the optimal clustering result.
sade clusters software systems by combining the dependency information with the semantic similarity.
they use the semantic similarity between modules as the weight of call graph and cluster the graph using louvain clustering algorithm.
jalali et al.
unified dependency information and semantic information to introduce a new multi objective fitness function and consider the clustering as a multi objective search problem.
however current research on clustering with hybrid information mainly focus on only the dependency and textual information while neglects many other useful information in the source codes.
conclusion in summary we propose sarif an architecture recovery technique dedicates to comprehensively understanding the architecture of software systems by fusing three aspects of information from software systems.
we evaluate sarif and previous architecture recovery techniques on software systems with ground truth architectures.
three of the most commonly used metrics and two new metrics are utilized for evaluation.
the experimental result shows that sarif outperforms the best baseline techniques by .
on average.
a case study on a real world software system shows that sarif can effectively identify modules with similar functionalities.
for future work sarif can be further enhanced by incorporating more domain knowledge on architecture such as commonly adopted architectural patterns.
data availability the data demo and detailed instructions are available at https github.com anonymous2f4a9d sarif fse23.
acknowledgementa this research project is supported by the national research foundation singapore and the cyber security agency under its national cybersecurity r d programme ncrp25 p04 taicen .
any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not reflect the views of national research foundation singapore and cyber security agency of singapore.
1545esec fse december san francisco ca usa yiran zhang zhengzi xu chengwei liu hongxu chen jianwen sun dong qiu and yang liu