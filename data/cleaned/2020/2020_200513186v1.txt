beware the evolving intelligent web service!
an integration architecture tactic to guard ai first components alex cummaudo ca deakin.edu.au applied artificial intelligence inst.
deakin university geelong australiascott barnett scott.barnett deakin.edu.au applied artificial intelligence inst.
deakin university geelong australiarajesh vasa rajesh.vasa deakin.edu.au aapplied artificial intelligence inst.
deakin university geelong australia john grundy john.grundy monash.edu faculty of information technology monash university clayton australiamohamed abdelrazek mohamed.abdelrazek deakin.edu.au school of information technology deakin university geelong australia abstract intelligent services provide the power of ai to developers via simple restful api endpoints abstracting away many complexities of machine learning.
however most of these intelligent services such as computer vision continually learn with time.
when the internals within the abstracted black box become hidden and evolve pitfalls emerge in the robustness of applications that depend on these evolving services.
without adapting the way developers plan and construct projects reliant on intelligent services significant gaps and risks result in both project planning and development.
therefore how can software engineers best mitigate software evolution risk moving forward thereby ensuring that their own applications maintain quality?
our proposal is an architectural tactic designed to improve intelligent service dependent software robustness.
the tactic involves creating an application specific benchmark dataset baselined against an intelligent service enabling evolutionary behaviour changes to be mitigated.
a technical evaluation of our implementation of this architecture demonstrates how the tactic can identify cases of substantial confidence evolution and cases of substantial changes to response label sets using a dataset consisting of images that evolve when sent to a service.
ccs concepts information systems web services software and its engineering software evolution hardware error detection and error correction computer systems organization client server architectures.
keywords intelligent web services software architecture software evolution permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november sacramento california united states association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
reference format alex cummaudo scott barnett rajesh vasa john grundy and mohamed abdelrazek.
.
beware the evolving intelligent web service!
an integration architecture tactic to guard ai first components.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse .
acm new york ny usa pages.
introduction the introduction of intelligent services into the software engineering ecosystem allows developers to leverage the power of ai without implementing complex ml algorithms source and label training data or orchestrate powerful and large scale hardware infrastructure.
this is extremely enticing for developers to embrace due to the effort cost and non trivial expertise required to implement ai in practice .
however the vendors that offer these services also periodically update their behaviour responses .
the ideal practice for communicating the evolution of a web service involves updating the version number and writing release notes.
the release notes typically describe new capabilities known problems and requirements for proper operation .
developers anticipate changes in behaviour between versioned releases although they expect the behaviour of a specific version to remain stable over time .
however emerging evidence indicates that intelligent services do not communicate changes explicitly .
intelligent services evolve in unpredictable ways provide no notification to developers and changes are undocumented .
to illustrate this consider fig.
which shows the evolution of a popular computer vision service with examples of labels and associated confidence scores with how they changed.
this behaviour change severely negatively affects reliability.
applications may no longer function correctly if labels are removed or confidence scores change beyond predefined thresholds.
unlike traditional web services the functionality of these intelligent services is dependent on a set of assumptions unique to their machine learning principles and algorithms.
these assumptions are based on the data used to train machine learning algorithms the choice of algorithm and the choice of data processing steps most of which are not documented to service end users.
the behaviourarxiv .13186v1 may 2020esec fse november sacramento california united states alex cummaudo et al.
natural foods .
granny smith .
skiing .
snow .
girl .
photography .
water .
wave .
tennis .
sports .
neighbourhood .
blue .
figure prominent computer vision services evolve with time which is not effectively communicated to developers.
each image was uploaded in november and march and the topmost label was captured.
specialisation in labels left generalisation in labels centre and emphasis change in labels right are all demonstrated from the same service with no api change and limited release note documentation.
confidence values indicated in parentheses.
of these services evolve over time typically this implies the underlying model has been updated or re trained.
vendors do not provide any guidance on how best to deal with this evolution in client applications.
for developers to discover the impact on their applications they need to know the behavioural deviation and the associated impact on the robustness and reliability of their system.
currently there is no guidance on how to deal with this evolution nor do developers have an explicit checklist of the likely errors and changes that they must test for .
in this paper we present a reference architecture to detect the evolution of such intelligent web services using a mature subset of these services that provide computer vision as an exemplar.
this tactic can be used both by intelligent service consumers to defend their applications against the evolutionary issues present in intelligent web services and by service vendors to make their services more robust.
we also present a set of error conditions that occur in existing computer vision services.
the key contributions of this paper are a set of new service error codes for describing the empirically observed error conditions in intelligent services.
a new reference architecture for using intelligent services with a proxy server that returns error codes based on an application specific benchmark dataset.
a labelled data set of evolutionary patterns in computer vision services.
an evaluation of the new architecture and tactic showing its efficacy for supporting intelligent web service evolution from both provider and consumer perspectives.
the rest of this paper is organised thus section presents a motivating example that anchors our work section presents a landscape analysis on intelligent web services section presentsan overview of our architecture section describes the technical evaluation section presents a discussion into the implications of our architecture its limitations and potential future work section discusses related work section provides concluding remarks.
motivating example we identify the key requirements for managing evolution of intelligent services using a motivating example.
consider pam a software engineer tasked with developing a fall detector system for helping aged care facilities respond to falls promptly.
pam decides to build the fall detector with an intelligent service for detecting people as she has no prior experience with machine learning.
the initial system built by pam consists of a person detector and custom logic to identify a fall based on rapid shape deformation i.e.
a vertical person changing to a horizontal person greater than specified probability threshold value .
due to the inherent uncertainty present in an intelligent service and the importance of correctly identifying falls pam informs the aged care facility that they should manually verify falls before dispatching a nurse to the location.
the aged care facility is happy with this approach but inform pam that only a certain percentage of falls can be manually verified based on the availability of staff.
in order to reduce the manual work pam sets thresholds for a range of confidence scores where the system is uncertain.
pam completes the fall detector using a well known cloud based intelligent image classification web service and her client deploys this new fall detection application.
three months go by and then the aged care facility contact pam saying the percentage of manual inspections is far too high and could she fix it.
pam is mystified why this is occurring as she thoroughly tested the application with a large dataset provided by the aged care facility.
on further inspection pam notices that thebeware the evolving intelligent web service!
an integration architecture tactic to guard ai first components esec fse november sacramento california united states problem is caused by some images classifying the person with a child label rather than a person label.
pam is frustrated and annoyed at this behaviour as i the cloud vendor did not document or notify her of the change of the intelligent service behaviour ii she does not know the best practice for dealing with such a service evolution and iii she cannot predict how the service will change in the future.
this experience also makes pam wonder what other types of evolution can occur and how can she minimise these behavioural changes on her critical care application.
pam then begins building an ad hoc solution hoping that what she designs will be sufficient.
for pam to build a robust solution she needs to support the following requirements r1.
define a set of error conditions that specify the types of evolution that occur for an intelligent service.
r2.
provide a notification mechanism for informing client applications of behavioural changes to ensure the robustness and reliability of the application.
r3.
monitor the evolution of intelligent services for changes that affect the application s behaviour.
r4.
implement a flexible architecture that is adaptable to different intelligent services and application contexts to facilitate reuse.
intelligent services we present background information on intelligent services describing how they differ from traditional web services the dimensions of their evolution and the currently limited configuration options available to users.
.
intelligent vs traditional web services unlike conventional web services intelligent web services are built using ai based components.
these components are unlike traditional software engineering paradigms as they are data dependent and do not result in deterministic outcomes.
these services make future predictions on new data based solely against its training dataset outcomes are expressed as probabilities that the inference made matches a label s within its training data.
further these services are often marketed as forever evolving and improving .
this means that their large training datasets may continuously update the prediction classifiers making the inferences resulting both in probabilistic and non deterministic outcomes .
critically for software engineers using the services these non deterministic aspects have not been sufficiently documented in the service s api documented which has been shown to confuse developers .
a strategy to combat such service changes which we often observe in traditional software engineering practices are for such services to be versioned upon substantial change.
unfortunately emerging evidence indicates that prominent cloud vendors providing these intelligent services do not release new versioned endpoints of the apis when the internal model changes .
for intelligent services it is impossible to invoke requests specific to a particular version model that was trained at a particular date in time.
this means that developers need to consider how evolutionary changes to the intelligent web services they make use of may impact their solutions in production .
computer vision service evolution per imageconfidences per labelv ocabularyincreasedecreasestablelabelsontologymore labelsfewer labelsunchangedemphasis changegeneralisationspecialisationfigure the dimensions of evolution identified within computer vision services.
figure a significant confidence increase .
from window .
to water transportation .
goes beyond simple decision boundaries.
.
dimensions of evolution the various key dimensions of the evolution of intelligent services is illustrated in fig.
.
there are two primary dimensions of evolution changes to the label sets returned per image submitted and changes to the confidences per label in the set of labels returned per image.
in the former we identify two key aspects cardinality changes and ontology changes.
cardinality changes occur when the service either introduces or drops a label for the same image at two different generations.
alternatively the cardinality may remain stagnant although this is not guaranteed.
this results in an expectation mismatch by developers as to what labels can or will be returned by the service.
for instance the terms black and black and white may be found to be categorised as two separate labels.
secondly the ontologies of these labels are non static and a label may become more generalised into a hypernym specialised into a hyponym or the emphasis of the label may change either to a co hyponym or another aspect in the image such as the colour or scene rather than the subject of the image .
secondly we have identified that the confidence values returned per label are also non static.
while some services may present minor changes to labels confidences resulting from statistical noise other labels had significant changes that were beyond basic decision boundaries.
an example is shown in fig.
.
developer code written to assume certain ranges confidence intervals will fail if the service evolves in this way.
.
limited configurability as an example consider fig.
which illustrates an image of a dog uploaded to a well known cloud based computer vision service.esec fse november sacramento california united states alex cummaudo et al.
computer vision service image url features maxresults json localizedobjectannotations boundingpoly ... name dog score .
json score .
url maxresults figure request and response for an intelligent computer vision web service with only three configuration parameters the image s url maxresults and score .
developers have very few configuration parameters in the upload payload urlfor the image to analyse and maxresults for the number of objects to detect .
the json output payload provides the confidence value of its estimated bounding box and label of the dog object via its score field .
.
this value indicates the level of confidence in the label returned and is dependent on the input to the underlying ml model used by that service.
developers set thresholds as a decision boundary in this case a threshold of greater than .
could indicate that the image contains a dog where as any other value the system is uncertain.
these decision boundaries determine if the service s output is accepted or rejected.
however these confidence scores change whenever a model is re trained and these changes are not communicated or propagated to developers .
developers can only modify these decision boundaries to improve the performance of the intelligent web service.
this is unlike many machine learning toolkit hyper parameter optimisation facilities which can be used to configure the internal parameters of the algorithm for training a model.
in this case developers using the intelligent web service have no insight into which hyperparameters were used when training the model or the algorithm selected and cannot tune the trained model.
thus an evaluation procedure must be followed as a part of using an intelligent service for an application to tune their output confidence values and select appropriate threshold boundaries.
while some service providers provide some guidance to thresholding 1they do not provide domain specific tooling.
this is because choice of appropriate thresholds is dependent on the data and must consider factors such as algorithmic performance financial cost and impact of false positives negatives.
however decision boundaries in service client code using simple ifconditions around confidence scores is not a sufficient strategy as evidence shows intelligent non deterministic web services change sporadically and unknowingly.
most traditional deterministic code bases handle unexpected behaviour of called apis via error codes and exception handling.
thus the non deterministic components of the client code such as those using computer vision services will also tend to conflict with their traditional deterministic components as the latter do not deal in terms of probabilities but in using error codes.
this makes achieving robust component integration in client code bases hard.
more sophisticated monitoring of intelligent services in client code is therefore required to map the non deterministic service behaviour changes to errors such that the surrounding infrastructure can support it and reduce interface boundary problems.
while data science literature acknowledges last accessed may .the need for such an architecture they do not offer any technical software engineering solutions to mitigate the issues such that software engineers have a pattern to work against it.
to date there do not yet exist intelligent web service client code architectures tactics or patterns that achieve this goal.
our approach to address the requirements from section we have developed a new proxy service2that includes i evaluation of an intelligent service using an application specific benchmark dataset ii a proxy server to provide client applications with evolution aware errors and iii a scheduled evolution detection mechanism.
the current approach of using an intelligent api via direct access is shown in fig.
top .
in contrast an overview of our approach is shown in fig.
bottom .
the following sections describe our approach in detail.
.
core components for the purposes of this paper we assume that the intelligent service of interest is an image recognition service but our approach generalises to other intelligent trained model based services e.g.
nlp document recognition voice etc.
each image when uploaded to the intelligent service returns a response r which is a set describing a label l of what is in the image i along with its associated confidence c thus ri l1 c1 l2 c2 .
.
.
ln cn .
most documentation of these services imply that these confidence values are all what is needed to handle evolution in their systems.
this means that if a label changes beyond a certain threshold then the developer can deal with the issue then or ignore it .
while this approach may work in some simple application contexts in many it may not.
our proxy server offers a way to monitor if these changes go beyond a threshold of tolerance checking against a domain specific dataset over time.
.
.
benchmark dataset.
monitoring an intelligent service for behaviour change requires a benchmark dataset a set of nimages.
for each image i in the benchmark dataset b there is an associated label l that represents the true value for that item bi i1 l1 i2 l2 .
.
.
in ln .
this dataset is used to check for evolution in intelligent services by periodically sending each image within the dataset to the service s api as per the rules encoded within the scheduler see section .
.
.
by using a dataset specific to the application domain developers can detect when evolution affects their application rather than triggering all non impactful changes.
this helps achieve our requirement r3.
monitor the evolution of intelligent services for changes that affect the application s behaviour .
using application specific datasets also ensures that the architectural style can be used for different intelligent services as only the data used needs to change.
this design choice encourages reuse satisfying requirement r4.
implement a flexible architecture that is adaptable to different intelligent services and application contexts to facilitate reuse .
we propose an initial set of guidelines on how to create and update the benchmark dataset within section .
.
.
.
.
facade api.
an architectural facade is the central component to our mitigation strategy for monitoring and detecting for 2a reference architecture is provided at the evolving intelligent web service!
an integration architecture tactic to guard ai first components esec fse november sacramento california united states client application service client proxy server facade api scheduler service client benchmark dataset threshold tuner client application behaviour token application developers http request http response ok or on exception http response precondition failed intelligent service http request http response ok set recurrent trigger or trigger now tune benchmark rules tune tolerated benchmarks intelligent service http request http response ok figure top accessing an intelligent service directly.
bottom primary components of the proxy server approach.
table potential reasons for a precondition failed response.
error code error description no key yet this indicates that the proxy server is still initialising its first behaviour token i.e.
k0does not yet exist.
service mismatch the service encoded within the behaviour token provided to the proxy server does not match the service the proxy server is benchmarked against.
this makes it possible for one proxy server to face multiple computer vision services.
dataset mismatch the benchmark dataset bencoded within the behaviour token does not match the benchmark dataset encoded within the proxy server.
success mismatch the success of each response within the benchmark dataset must be true for a behaviour token to be used within a request.
this error indicates that kris therefore not successful.
min confidence mismatch the minimum confidence delta threshold set in ktdoes not match that of kr.
max labels mismatch the maximum label delta threshold set in ktdoes not match that of kr.
response length mismatch the number of responses within ktdoes not match that within kr.
label delta mismatch an image within bhas either dropped or gained a number of labels that exceeds the maximum label delta.
thus krexceeds the threshold encoded within kt.
confidence delta mismatch one of the labels within an image encoded in krexceeds the confidence threshold encoded within kt.
expected labels mismatch one of the expected labels for an image within ktis now missing.
changes in called intelligent services.
the facade acts as a guarded gateway to the intelligent service that defends against two key issues i potential shifts in model variations that power the cloud vendor services and ii ensures that a context specific dataset specific to the application being developed is validated over time .
by using a facade we can return evolution aware error codes to the client application satisfying requirement r1.
define a set of error conditions that specify the types of evolution that occur for an intelligent service and enabling requirement r3.
monitor the evolution of intelligent services for changes that affect the application s behaviour.
this works by ensuring every request made by the client application contains a valid behaviour token see section .
.
and will reject the request when evolution has been identified by the scheduler with an associated error code.
the facade api essentially blocks the client application out from accessing the intelligent service when an invalid state has occurred.
.
.
threshold tuner.
selecting an appropriate threshold for detecting behavioural change depends on the application context.
setting the threshold too low increases the likelihood of incorrect results while setting the threshold too high means undesiredtable rules encoded within a behaviour token.
rule description max labels the value of n. min confidence the smallest acceptable value of c. max labels the minimum number of labels dropped or introduced from the current ktand provided krto be considered a violation i.e l kt l kr .
max confidence the minimum confidence change of anylabel from the current ktand provided krto be considered a violation.
expected labels a set of labels that every response must include.
changes are being detected.
our approach enables developers to configure these parameters through a threshold tuner.
this improves robustness as now there is a systematic approach for monitoring and responding to incorrect thresholds.
configurable thresholds meet our key requirements r2andr3.
.
.
behaviour token.
the behaviour token stores the current state of the proxy server by encoding specific rules regarding theesec fse november sacramento california united states alex cummaudo et al.
workflow invalid request post evolution workflow valid request pre evolution workflow initialise benchmark upload benchmark dataset initialise first baseline make service client request analyse images produce behaviour token retain behaviour token request with valid behaviour token validate behaviour token as ok make service client request analyse images register result use result workflow evolution detectionclient app proxy intelligent service invoke benchmark schedule initialise new baseline make service client request analyse images with evolved model produce new behaviour token request with old behaviour token validate behaviour token as invalid produce exception handle exception figure state diagram for the four workflows presented.
evolution of the intelligent service.
the current token at time t held by the proxy server is denoted by kt.
these rules are specified by the developer upon initialisation of this proxy server and are presented in table .
when the proxy server is first initialised i.e.
at t the first behaviour token is created based on the benchmark dataset and its configuration parameters and is stored locally thus k0is created .
the behaviour token is passed to the client application to be used in subsequent requests to the proxy server where krrepresents the behaviour token passed from the client application to the proxy server.
each time the proxy server receives the behaviour token from the client the validity of the token is validated with a comparison to the proxy server s current behaviour token i.e.
kr kt .
an invalid token i.e.
when kr.kt indicates that an error caused by evolution has occurred and the application developer needs to appropriately handle the exception.
behaviour tokens are essential for meeting requirement r3.
monitor the evolution of intelligent services for changes that affect the application s behaviour.
.
.
service client.
if any of the rules above are violated then the response of the facade request will vary depending on the parameter of the behaviour encoded within the behaviour token.
this can be one of error where a http non code is returned by the facade to the client application indicating that the client application must deal with the issue immediately warning where a warning callback endpoint is called with the violated response to be dealt with but the response is still returned to the client application info where the violated response is logged in the facade s logger for the developer to periodically read and inspect and the response is returned to the client application.
we implement this proxy server pattern using http conditional requests.
as we treat the label as a first class citizen we return the labels for a specific image ri only where the entity tag etag orlast modified validators pass.
the kris encoded within either the etag i.e.
a unique identifier representing t or as the date labels and thus models were last modified i.e.
using the if matchorif unmodified since conditional headers .
we note that the use of weak etags should be used as byte for byte equivalence is not checked but only semantic equivalence within the tolerances specified.
should tevolve to an invalid state i.e.
kris no longer valid against kt then the behaviour as described above will be enacted.
these http header fields are used as the backbone to help enforce robustness of the services against evolutionary changes and context within the problem domain dataset.
responses from the service are forwarded to the clients when such rules are met otherwise alternative behaviour occurs.
for example the most severe of violated erroneous behaviour is the error behaviour.
to enforce this rule we advocate for use of the precondition failed http error if a violation occurs as a if conditional header was violated.
an example of this architectural pattern with the error behaviour is illustrated in fig.
.
we suggest the precondition failed http error be returned in the event that a behaviour token is violated against a new benchmark.
further details outlining the reasons why a precondition has failed are encoded within a json response sent back to the consuming application.
the following describes the two broad categories of possible errors returned robustness precondition failure or benchmark precondition failure .
these are illustrated in a high level within fig.
where leaf nodes are the potential error types that can be returned.
a list of the different error codes are given in table where errors above the rule are robustness expectations which check for basic requirements such as whether the key provided encodes the same data as the dataset in the facade while those below are benchmark expectations which identifies evolution cases .
.
.
scheduler.
the scheduler is responsible for triggering the evolution detection workflow described in detail below in section .
.
developers set the schedule to run in the background at regular intervals e.g.
via a cron job or to trigger if violations occur ztimes.
the scheduler is the component that enables our architectural style to identify called intelligent service software evolution and to notify the client applications that such evolution has occurred.
client applications can then respond to this evolutionbeware the evolving intelligent web service!
an integration architecture tactic to guard ai first components esec fse november sacramento california united states precondition failedno key yetvalidateconfigvalidatedataservicemismatchvalidatebenchmarksvalidateparametersvalidatetolerancesexpected labelsmismatchdatasetmismatchsuccessmismatchresponse lengthmismatchmin confidencemismatchmax labelsmismatchlabel deltamismatchconfidence deltamismatch figure precondition failure taxonomy leaf nodes indicate error types returned to users.
in a timely manner rather than wait for the system to fail as in our motivating example.
the scheduler is necessary to satisfy our requirements r2andr3.
.
usage example we explain how developer pam from our motivating example would use our proposed solution to satisfy the requirements described in section .
each workflow is presented in fig.
.
only workflow initialise benchmark is executed once while the rest are cycled.
the description below assumes pam has implemented the proxy.
.
.
workflow .
initialise benchmark.
the first task that pam has to do is to prepare and initialise the benchmark dataset within the proxy server.
to prepare a representative dataset pam needs to follow well established guidelines such as those proposed by pyle .
pam also needs to manually assign labels to each image before uploading the dataset to the proxy along with the thresholds to use for detecting behavioural change.
the full set of parameters that pam has to set are based on the rules shown in table .
pam cannot use the proxy to notify her of evolution until a benchmark dataset has been provided.
the proxy then sends each image in the benchmark dataset to the intelligent service and stores the results.
from these results a behaviour token is generated which is passed back to the client application.
pam uses this token in all future requests to the proxy as the token captures the current state of the intelligent service.
.
.
workflow .
valid request pre evolution.
workflow represents the steps followed when the intelligent service is behaving as expected.
pam makes a request to label an image to the proxy using the token that she received when registering the benchmark dataset.
the token is validated with the proxy s current state token and then a request to label the image is made to the intelligent service if no errors have occurred.
results returned by the intelligent service are registered with the proxy server.
pam can be confident that the result returned by our service is in line with her expectations.
.
.
workflow .
evolution detection.
workflow describes how the proxy functions when behavioural change is present in the called intelligent service.
pam sets a schedule for once a day sothat the proxy s scheduler triggers workflow .
first each image in the benchmark dataset is sent to the intelligent service.
unlike workflow we already have a behaviour token that represents the previous state of the intelligent service.
in this case the model behind the intelligent service has been updated and provides different results for the benchmark dataset.
second the proxy updates the internal behaviour token ready for the next request.
at this stage pam will be notified that the behaviour of the intelligent service has changed.
.
.
workflow .
invalid request post evolution.
workflow provides pam with an error message when evolution has been detected.
pam s client application makes a request to the proxy server with an old behaviour token.
the proxy server then validates the client token which is invalid as the behaviour token has been updated.
in this case an exception is raised and an appropriate error message as discussed above is included in the response back to pam s client application.
pam can code her application to handle each error class in appropriate ways for her domain.
evaluation our evaluation of our novel intelligent service proxy server approach uses a technical evaluation based on the results of an observational study.
we used existing datasets from observational studies to identify problematic evolution in computer vision labelling services.
this technical evaluation is designed to show i what the responses are with and without our architecture present highlighting errors ii the overall increased robustness using enhanced responses and iii the technical soundness of the approach.
thus we propose the following research question which we answer in section .
can the architecture identify evolutionary issues of computer vision services via error codes?
based on our findings we proposed and implemented the proxy server using a ruby development framework which we have made available online for experimentation.3additional data was collected from the computer vision service and sent to the proxy server to evaluate how the service handles behavioural change.
last accessed march .esec fse november sacramento california united states alex cummaudo et al.
emphasis change generalisation specialisation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
confidence deltafrequency figure histogram of confidence variation .
data collection and preparation to minimise reviewer bias we do not identify the name of the service used however this service was one of the most adopted cloud vendors used in enterprise applications in .
the two existing datasets used consisted of images.
we initialised the benchmark workflow in november and sent each image to the service every eight days and captured the json responses through the facade api workflow until march .
this resulted in json responses from the target computer vision service.
we then selected the first and last set of json responses i.e.
responses and independently identified cases of evolution of the original images.
this was achieved by analysing the json responses for each image taken in using an evaluation script.
for each json response evolution as classified by fig.
was determined either by a vocabulary or confidence per label change in the first and last responses sent.
for the evolving responses we calculated the delta of the label s confidence between the two timestamps and the delta in the number of labels recorded in the entire response.
further for the highest ranking label by confidence we manually classified whether its ontology became more specific more generalised or whether there was substantial emphasis change.
the distribution of confidence differences per these three groups are shown in fig.
with the mean confidence delta indicated with a vertical dotted line.
this highlights that on average labels that change emphasis generally have a greater variation such as the example in fig.
.
further we grouped each image into one of four broad categories food animals vehicles humans and assessed the breakdown of ontology variance as provided in table .
we provide this dataset as an additional contribution and to permit replication.5the parameters set for our initial benchmark were a delta label value of and delta confidence value of .
.
expected labels for relevant groups were also assigned as mandatory label sets e.g.
animal images used animal fauna and organism human images used human etc.
.
last accessed march .
last accessed march .table variance in ontologies for the five broad categories ontology change food animal vehicles humans other total generalisation specialisation emphasis change total .
results examples of the march responses contrasting the proxy and direct service responses in our evaluation are shown in figs.
to .
due to space limitations the entire json response is partially redacted using ellipses.
these examples identify the label identified with the highest level of confidence in three examples against the ground truth label in the benchmark dataset.
in total the proxy server identified labels added to the responses and labels dropped with on average a delta of labels added.
the topmost labels added were architecture at cases building at cases and ingredient at cases the topmost labels dropped were tree at cases sky at cases and fun at cases.
confidence changes were also observed by the proxy server on average a delta increase of .
.
in fig.
we highlight an image of a sheep that was identified as a sheep at .
in november and then a mammal in march .
this evolution was classified by the proxy server as a confidence change error as the delta in the confidences between the two timestamps exceeds the parameter set of .
in this case sheep was downgraded to the third ranked label at .
thereby increasing by a value of .
.
as shown in the example four other labels evolved for this image between the two time stamps herd livestock terrestrial animal and snout with an average increase of .
found.
such information is encoded as a http error returned back to the user by the proxy server rejecting the request as substantial evolution has occurred however the response directly from the service indicates no error at all indicating by a http response .
similarly fig.
shows a violation of the number of acceptable changes in the number of labels a response should have between two timestamps.
in november the response includes the labels car motor vehicle city and road however these labels are not present in the response.
the response in introduces transport building architecture and house .
therefore the combined delta is dropped and introduced labels exceeding our threshold set of .
lastly fig.
indicates an expected label failure.
in this example the label fauna was dropped in the label set which was an expected label of all animals we labelled in our dataset.
additionally this particular response introduced green iguana iguanidae and marine iguana to its label set.
therefore not only was this response in violation of the label delta mismatch it was also in violation of the expected labels mismatch error and thus is caught twice by the proxy server.
.
threats to validity internal validity.
as mentioned we selected a popular computer vision service provider to test our proxy server against.
however beware the evolving intelligent web service!
an integration architecture tactic to guard ai first components esec fse november sacramento california united states label animal nov sheep .
mar mammal .
category confidence change intelligent service response in march responses label annotations mid m 04rky description mammal score .
topicality .
mid m description vertebrate score .
topicality .
mid m 07bgp description sheep score .
topicality .
... proxy server response in march error code error type confidence delta mismatch error data source key ... source response ... violating key ... violating response ... delta confidence threshold .
delta confidences detected sheep .
herd .
livestock .
terrestrial animal .
snout .
uri jpeg reason exceeded confidence delta threshold .
in labels delta mean .
.
figure example of substantial confidence change due to evolution there exist many other computer vision services and due to language barriers of the authors no non english speaking service were selected despite a large number available from asia.
further no user evaluation has been performed on the architectural tactic so far and therefore developers may suggest improvements to the approach we have taken in designing our tactic.
we intend to follow this up with a future study.
external validity.
this paper only evaluates the object detection endpoint of a computer vision based intelligent service.
while this type of intelligent service is one of the more mature ai based services available on the market and is largely popular with developers further evaluations of the our tactic may need to be explored against other endpoints i.e.
object localisation or indeed other label vehicle nov vehicle .
mar motorcycle .
category label set change intelligent service response in march responses label annotations mid m 07yv9 description vehicle score .
topicality .
mid m 07bsy description transport score .
topicality .
mid m 0dx1j description town score .
topicality .
... proxy server response in march error code error type label delta mismatch error data source key ... source response ... violating key ... violating response ... delta labels threshold delta labels detected uri jpg new labels transport building architecture house dropped labels car motor vehicle city road reason exceeded label count delta threshold new labels dropped labels .
figure example of substantial changes of a response s label set due to evolution types of services such as natural language processing audio transcription or on time series data.
future studies may need to explore this avenue of research.
construct validity.
the evaluation of our experiment was largely conducted under clinical conditions and a real world case study of the design and implementation of our proposed tactic would be beneficial to learn about possible side effects from implementing such a design e.g.
implications to cost etc.
.
therefore our evaluation does not consider more practical considerations that a real world production grade system may need to consider.
discussion .
implications .
.
for cloud vendors.
cloud vendors that provide intelligent services may wish to adopt the architectural tactic presented inesec fse november sacramento california united states alex cummaudo et al.
label fauna nov reptile .
mar iguania .
category ontology specialisation intelligent service response in march responses label annotations mid m 08 jw6 description iguania score .
topicality .
mid m 06bt6 description reptile score .
topicality .
mid m 01vq7 description iguana score .
topicality .
... proxy server response in march error code error type expected labels mismatch error data source key ... violating response ... uri expected labels labels detected iguana green iguana iguanidae lizard scaled reptile marine iguana terrestrial animal organism labels missing reason the expected label s fauna are missing in the response.
figure example of an expected label missing due to evolution this paper by providing a proxy auxiliary service or similar to their existing services thereby improving the current robustness of these services.
further they should consider enabling developers of this technical domain knowledge by preventing client applications from using the service without providing a benchmark dataset such that the service will return http error codes.
these procedures should be well documented within the service s api documentation thereby indicating to developers how they can build more robust applications with their intelligent services.
lastly cloud vendors should consider updating the internal machine learning models less frequently unless substantial improvements are being made.
many different applications from many different domains are using these intelligent services so it is unlikely that the model changes are improving all applications.
versioned endpoints would help with this issue although as we have discussed context using benchmark datasets should be provided.
.
.
for application developers.
developers need to monitor all intelligent services for evolution using a benchmark dataset andapplication specific thresholds before diving straight into using them.
it is clear that the evolutionary issues have significant impact in their client applications and therefore they need to check the extent this evolution has between versions of an intelligent service should versioned apis be available .
lastly application developers should leverage the concept of a proxy server or other form of intermediary when using intelligent services to make their applications more robust.
.
.
for project managers.
project managers need to consider the cost of evolution changes on their application when using intelligent services and therefore should schedule tasks for building maintenance infrastructure to detect evolution.
consider scheduling tasks that evaluates and identifies the frequency of evolution for the specific intelligent service being used.
our research we have found some intelligent services that are not versioned but rarely show behavioural changes due to evolution.
.
limitations in the situation where a solo developer implements the proxy service the main limitation is the cost vs response time trade off.
developers may want to be notified as soon as possible when a behavioural change occurs which requires frequent validation of the benchmark dataset.
each time the benchmark dataset is validated each item is sent as a request to the intelligent service.
as cloud vendors charge per request to an intelligent service there are financial implications for operating the proxy service.
if the developer optimises for cost then the application will take longer to respond to the behavioural change potentially impact end users.
developers need to consider the impact of cost vs response time when using the proxy service.
another limitation of our approach is the development effort required to implement the proxy service.
developers need to build a scheduling component batch processing pipeline for the benchmark dataset and a web service.
these components require developing and testing which impact project schedules and have maintenance implications.
thus we advise developers to consider the overhead of a proxy service and way up the benefits with have incorrect behaviour caused by evolution of intelligent services.
.
future work .
.
guidelines to construct and update the benchmark dataset.
our approach assumes that each category of evolution is present in the benchmark dataset prepared by the developer.
further guidelines are required to ensure that the developer knows how to validate the data before using the proxy service.
while the focus of this paper was to present and validate our architectural tactic guidelines on how to construct and update benchmark datasets for this tactic will need to be considered in future work.
data science literature extensively covers dataset preparation e.g.
and many example benchmark datasets are readily available .
an initial set of guidelines are proposed as follows data must be contextualised and appropriately sampled to be representative of the client application in particular the patterns present in the data contain both positive and negative examples this is is not a cat where to source data from existing datasets google images flickr crowdsourced etc.
whether the dataset is synthetically generatedbeware the evolving intelligent web service!
an integration architecture tactic to guard ai first components esec fse november sacramento california united states to increase sample size and how large a benchmark dataset size should be i.e.
larger the better but takes more effort and costs more .
benchmark datasets can also be used by software engineers provided the domain and context is appropriate for their specific application s context.
software engineers also benefit from our approach even if these guidelines are not strictly adhered to provided they use an application specific dataset i.e.
data collected from the input source for their application .
the main reason for this is that without our proposed tactic there are limited ways to build robust software with intelligent services.
future testing and evaluation of these guidelines should be considered.
.
.
extend the evolution categories to support other intelligent services.
this paper has used computer vision services to assess our proposed tactic and therefore further investigation is needed into the evolution characteristics of other intelligent services.
the evolution challenges with services that provide optimisation algorithms such as route planning are likely to differ from computer vision services.
these characteristics of an application domain have shown to greatly influence software architecture and further development of the proxy service will need to account for these differences.
as an example we have identified many similar issues that exist for natural language processing nlp where topic modelling produces labels on large bodies of text with associated confidences.
therefore the broader concepts of our contribution e.g.
behaviour token parameters error codes etc.
can be used to handle issues in nlp to demonstrate the generalisability of the architecture to other intelligent services.
we plan to apply our tactic to nlp and other intelligent services in our future work.
.
.
provide tool support for optimising parameters for an application context.
appropriately using the proxy service requires careful selection of thresholds benchmark rules and scheduling.
further work is required to support the developer in making these decisions so an optimal application specific outcome is achieved.
one approach is to present the trade offs to the developer and let them visualise the impact of their decisions.
.
.
improvements for a more rigorous approach.
conducting a more formal evaluation of our proposed architecture would benefit the robustness of the solution presented.
this could be done in various ways for example using a formal architecture evaluation method such as atam or a similar variant conducting user evaluation via brainstorming sessions or interviews with practitioners who may provide suggestions to improve our approach determining better strategies to fully automate the approach and reduce manual steps and using real world industry case studies to identify other factors such as cost and maintenance issues.
all these are various avenues of research that would ultimately benefit in a more well rounded approach to the architectural tactic we have proposed.
related work robustness of intelligent services.
while usage of intelligent services have been proven to have widespread benefits to the community they are still largely understudied in software engineering literature particularly around their robustness in productiongrade systems.
as an example advancements in computer vision largely due to the resurgence of convolutional neural networks in the late 1990s have been made available through intelligent services and are given marketed promises from prominent cloud vendors e.g.
with amazon rekognition you don t have to build maintain or upgrade deep learning pipelines .6however while vendors claim this the state of the art of computer vision itself is still susceptible to many robustness flaws as highlighted by many recent studies .
further each service has vastly different and incompatible ontologies which are non static and evolve certain services can mislabel images when as little as noise is introduced and developers have a shallow understanding of the fundamental ml concepts behind these issues which presents a dichotomy of their understanding of the technical domain when contrasted to more conventional domains such as mobile application development .
proxy servers as fault detectors.
fault detection is an availability tactic that encompasses robustness of software .
our architecture implements the sanity check and condition monitoring techniques to detect faults by validating the reasonableness of the response from the intelligent service against the conditions set out in the rules encoded in the benchmark dataset and behaviour token.
as we do in this study the proxy server pattern can be used to both detect and action faults in another service as an intermediary between a client and a server.
for example addressing accessibility issues using proxy servers has been widely addressed and more recently they have been used to address in browser javascript errors .
conclusions intelligent web services are gaining traction in the developer community and this is shown with an evermore growing adoption of computer vision services in applications.
these services make integration of ai based components far more accessible to developers via simple restful apis that developers are familiar with and offer forever improving object localisation and detection models at little cost or effort to developers.
however these services are dependent on their training datasets and do not return consistent and deterministic results.
to enable robust composition developers must deal with the evolving training datasets behind these components and consider how these non deterministic components impact their deterministic systems.
this paper proposes an integration architectural tactic to deal with these issues by mapping the evolving and probabilistic nature of these services to deterministic error codes.
we propose a new set of error codes that deal directly with the erroneous conditions that has been observed in intelligent services such as computer vision.
we provide a reference architecture via a proxy server that returns these errors when they are identified and evaluate our architecture demonstrating its efficacy for supporting intelligent web service evolution.
further we provide a labelled dataset of the evolutionary patterns identified which was used to evaluate our architecture.