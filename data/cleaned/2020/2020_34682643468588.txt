reassessingautomatic evaluationmetricsforcode summarization tasks devjeetroy devjeet.roy wsu.edu washingtonstateuniversity pullman wa usasarah fakhoury sarah.fakhoury wsu.edu washingtonstateuniversity pullman wa usaveneraarnaoudova venera.arnaoudova wsu.edu washingtonstateuniversity pullman wa usa abstract in recent years research in the domain of source code summarization has adopted data driven techniques pioneered in machine translation mt .
automatic evaluation metrics such as bleu meteor androuge arefundamentaltotheevaluationofmtsystems and have been adopted as proxies of human evaluation in the codesummarizationdomain.however theextenttowhichautomaticmetricsagreewiththegoldstandardofhumanevaluationhas notbeenevaluatedoncodesummarizationtasks.despitethis marginal improvements in metric scores are often used to discriminate between the performance of competing summarization models.
inthispaper wepresentacriticalexplorationoftheapplicability andinterpretationofautomaticmetricsasevaluationtechniques for code summarization tasks.
we conduct an empirical study with human annotators to assess the degree to which automatic metricsreflecthumanevaluation.resultsindicatethatmetricimprovementsoflessthan2pointsdonotguaranteesystematicimprovementsinsummarizationquality andareunreliableasproxies ofhumanevaluation.whenthedifferencebetweenmetricscores for two summarization approaches increases but remains within 5points somemetricssuchasmeteorandchrfbecomehighly reliableproxies whereasothers suchascorpusbleu remainunreliable.
based on these findings we make several recommendations fortheuseofautomaticmetricstodiscriminatemodelperformance in code summarization.
ccs concepts softwareanditsengineering softwaremaintenancetools general andreference metrics evaluation.
keywords automatic evaluation metrics code summarization machine translation acm reference format devjeet roy sarah fakhoury and venera arnaoudova.
.
reassessing automatic evaluation metrics for code summarization tasks.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august 23 28 athens greece.
acm new york ny usa pages.
esec fse august 23 28 athens greece copyright held by the owner author s .
acm isbn .
introduction useful source code comments play a vital role in program comprehensionandothersoftwaremaintenanceactivities .however proper documentation comes at a cost and producing wellwritten comments requires a substantial amount of effort on the partofsoftwaredevelopers.thisisoneofthemotivatingfactors behind why source code summarization is a rapidly growing research area atleast papersproposing orevaluating automated summarization approaches are published in .
theearliestcodesummarizationapproachesarebasedonstrong syntactic theories of comment structure information retrieval and textualtemplates.theseapproachestypicallyevaluatethequalityof ageneratedsummaryusinghumanannotatorswhoratesummaries on metrics such as as content adequacy conciseness andfluency .
in the last years the solution space for code summarization has significantly shiftedtowardsthe widespread adoptionof techniques and evaluation metrics from the machine translation mt domain.priorto2015 virtuallyallsummarizationapproachesinvolved template or information retrieval based techniques.
in thefirstpaperusinganmtapproachwaspublishedatanseconference and to the best of our knowledge there are papers thus far that propose an approach an evaluation or a critiqueofmtsummarizationapproaches 2 5 8 10 16 21 23 53 57 59 62 .
note that of thoseare publishedin and13 in alone.
oneoftheprimaryreasonsforthisshiftistheideathattranslatingsourcecodetoitsnaturallanguageequivalentholdsmany parallels with the concept of translating one natural language to another .
a side effect of this shift is a substantial change in the amount of data needed to evaluate code summarization approaches training generative models requires a large amount of data.naturally theevaluationtechniquesusedmustalsoscale and automated metrics provide an efficient way to evaluate the quality of generatedsummariesen mass.
automatedmetricsdesignedtoevaluatenaturallanguagetranslationapproaches suchasbleu meteor androuge have been adopted by code summarization researchers where a generated summary is compared to a gold standard or reference summary.
in the context of leading comment summaries the referencesummaryisoftentheoriginalaccompanyingcommentwritten by adeveloper.
inthemtdomain bleuhaslongbeenacceptedasade facto best practice metric.
recently however prominent machine translation researchers have raised concern over the use of bleu warning the mt community that the way bleu is used 1105this work is licensed under a creative commons attribution international .
license.
esec fse august athens greece devjeet roy sarahfakhoury andvenera arnaoudova and interpreted can greatly affect its reliability.
moreover while evidence supports bleu for diagnostic evaluationof mt systems itdoes not support using bleuoutsideofmt .
alarmingly although critiques of bleu have been published foroverthreeyearsnow thereliabilityandvalidityofminorimprovements for those automatic metrics with respect to human evaluationshavenotbeenre assessedinthedomainofsoftware engineeringfor the purpose ofcode summarization.
inthispaper wepresentacriticalexplorationoftheapplicability andinterpretationofautomatedmetrics forcodesummarization tasks.first asamotivationalresearchquestion wecomparethedistributions of automatic metric scores for existing summarization approaches andweshowthatsimilartomt whentheautomatic metric difference is marginal within points the distributions are not statistically different meaningthat none of the approaches can be declared as a state of the art based solely on automatic metrics.
next weinvestigatewhetherautomaticmetricsarereliableproxies for human evaluations i.e.
whether automatic metrics are able to reflect perceived quality improvements in generated summaries as indicated by human annotators.
we do that both at corpus and summary levels using results from human annotators.
corpuslevel metrics have the advantage to provide an overview of the performanceofanapproachonacorpus.summary levelmetricsallow traceability when trying to understand the situations in which an approachdoes not performwell andthusallow totarget those for improvements.
we show that at corpus level when the difference between automaticmetric scoresfortwosummarizationapproachesis points all automaticmetrics areveryunreliable i.e.
they makean errorinmorethan70 ofthecasesonaverage rangingfrom62.
to .
.
we also show that the minimum threshold for reliability for automatic metrics varies significantly across the metrics we evaluated.
notably meteor and chrf are extremely reliable for differencesgreaterthan2points exhibiting1.
and2.
errorrates respectively.finally weshowthatsummary levelmetricsdonot havesufficientdiscriminativepowertoreflecthumanevaluation andthus cannotbe usedas reliable proxies.
implications.
our goal is not to identify the current state ofthe art summarization approach but rather to provide a critical evaluation of automatic evaluation metrics as they are currently used in code summarization research.
for instance from the papersthat declared state of the artsummarization techniques in the past5 years 15of themreportimprovedperformance base on metric improvements within the range of points for which asweshow allmetricsareunreliable.ourfindingsenablethese communitytocontextualizetheseresultsinordertobetterinterpret evaluations ofcode summarizationapproaches.
the maincontributions ofthisworkare as follows weshowthatsmalldifferencesinmetricscoresmightnot guaranteesystematicdifferencesinsummarizationquality between twoapproaches.
we demonstrate that automatic evaluation metrics commonlyusedinsourcecodesummarizationarenotreliable indicatorsofhumanassessmentwhenmetricdifferencesare small 2points .
we provideconcrete recommendations for practitioners regardingtheinterpretationofevaluationmetricsaswellas the designofhuman evaluation studies.
a replication package containing human annotations and therepresentativerandomsampleofsourcecodesnippets usedinthis work .
vocabulary throughoutthispaperwewillbedrawingparallels between the domain of machine translation mt and the code summarization sub domain of software engineering.
to remain consistent weadaptthevocabularyoftheformertothelatter.in mtasystemisanalogoustoacodesummarization approach.systemlevelmetrics inmtcorrespondstometricsthatarecalculatedfor the entire corpus and will be referred to as corpus level metrics here.
in mt a sentence or translation level metric isreferred to as asegment level metric whereas in code summarization this will bereferredtoasa summary levelmetric .terminologydenotinga groundtruthtranslation orsummary is referredtoas a reference inboth domains.
related work .
evaluationofautomaticmetricsin machine translation in the past few years a number of researchers in the machine translationcommunityhavecalledforareevaluationofthegold standardmetrics andprocedures being used.
in reiter presented a structured review of the validity ofbleuandfoundthatwhilethereisevidencethatsupportsusing bleu for diagnostic evaluation of mt systems it does not support using bleu outside of the mt domain for evaluation of individual texts orfor scientifichypothesistesting.
most recently in their paper mathur et al.
highlight seriousissueswiththecurrentmethodsforevaluatingmetricsin mt.theuseofpearson s r todeterminehowmetricscorrelate with human assessment is highly sensitive to the translations used for assessment particularlythe presenceof outliers.
they demonstratehowthismethodoftenleadstoinflatedconclusionsaboutthe efficiencyofametric.mathuretal.alsoshowthatsmallchanges in evaluation metrics e.g.
1 2 points bleu on a scale are not enough to draw empirical conclusions and they should be supportedwithmanual evaluations.
our work investigates how reliable automatic metrics are as proxiesforhumanevaluation andhowmetricthresholdsthresholds translate inthe domainof sourcecode summarization.
.
evaluationofautomaticmetricsin code summarization leclair and mcmillan point out the lack of suitable datasets and community standards to create those datasets in the code summarizationdomain whichresultsinconfusingandun reproducible researchresults.
stapletonetal.
explorehowautomaticallygeneratedsummaries effect code comprehension and show that participants performsignificantlybetteronsnippetsthatcontainhumanwritten summaries as opposed to generated summaries.
however they 1106reassessingautomatic evaluation metrics forcode summarizationtasks esec fse august athens greece werenotabletoshowacorrelationbetweenhumanperformance andautomaticmetric scores.
gros et al.
show that as compared to natural language datasets sourcecodecommentsaremuchmorerepetitivewhich can have a significant and exaggerated impact on the performance of an approach.
they also demonstrate that bleu scores vary considerably between differentwaysofcalculation.
building upon previous work we investigate the reliability of corpus andsummary levelautomaticmetricswithrespecttohuman assessments scores and devise guidelines for minimum improvements inautomatic metricscores that mustexistinorder to systematically demonstrate perceivable improvement by human evaluation.
studydesign and setup in this section we formulate the research questions that guide this study providedetailsregarding thedatasetand automatic metrics chosen for this work and the survey designed to collect human evaluations.
.
research questions rq0isthereasignificantdifferenceinthecorpuslevelmetrics of different models?
automatic evaluation metrics help discriminate between two approaches based on their ability to summarize source code.
however these metrics arenotdesignedtobeusedinisolation andsimplyattaining ahighermetricscoreisinsufficienttoestablishwhetherone code summarization approach is better than another.
this researchquestioninvestigatesifincreasesinmetricscores result from systematic improvements in the quality of the summariesgeneratedbyanapproach ratherthanbychance.
rq1docommonlyusedcorpus levelmetricsreflecthuman qualityassessmentsofgeneratedsummaries?
automatic evaluationmetricsaredesignedtoserveasanapproximation ofhumanassessments.thedevelopmentandperformance ofthesemetricshave been extensively documentedinthe natural language domain however it is unclear how they perform in the context of source code summarization.
in this research question we measure the degree of agreement betweencorpus levelmetricproxiesandactualhumanassessmentscores.specifically wemeasuretheiragreementin thecontextofdiscriminatingtwosummarizationapproaches using pairwisetests ofsignificance.
rq2aresummary levelmetricsabletoreflecthumanquality assessments of generated summaries?
corpus level metricscannotexplainnuanceinanapproach sperformance atasummarylevelbecausetheyprovideasinglescoreforan approach acrossall of itsgenerated summaries thustraceabilitytoindividualsummariesisimpossible.summary level metrics score individual summaries and enable a direct comparison of a human assessment score to a metric score for each generated summary.
however summary level metrics have been shown to have a lower correlation on natural languagedatasets .weinvestigatewhetherthisisalso the casefor code summarizationtasks.graduate studentprofessional developerworking in academiaundergraduate studentother 75gender man non binary prefer not to disclose prefer to self describe woman figure demographics forhumanannotators.
.
dataset haque et al.
recently proposed an improvement to existing summarization techniques by leveraging the file context of java methods.intheirwork theyapplytheproposedimprovementto severalexistingtechniquesonapubliclyavailabledataset and provide a replication package containing all requisite materials requiredforeasyreplication.duetotherecencyofthework the ease of replication and variety of approaches presented we select fivesummarizationapproachesfromtheirreplicationpackageto use in our study as follows ast attendgru ast attend gru fc code2seq graph2seq andtransformer whichwewillreferto asm1 m5.thereplicationpackageincludesapproachsnapshots at various epochs.we usethesnapshots thathave thebest corpus level performance for eachof the selectedapproaches.
.
surveydesign to evaluate the degree of agreement between automatic metrics and human assessments we ask annotators to rate individual summaries1.eachannotatorisshown6differentcodesnippets i.e.
java methodimplementation .foreachsnippet participantsevaluate summaries generated by the summarization approaches as well as the reference summary.
the order in which the snippets andsummariesarepresentedisrandomized.thesurveytakeson average15 minutesto complete.
.
human annotators we recruit annotators through social media and direct emails to researchersandpractitionersinthesecommunity.participantsare asked demographic information including the number of years of academic and professional experience.
figure 1displays the distributions of the human annotators participating in the study.
a total of people submitted annotations professional developers 61workinginacademia 87graduatestudents 17undergraduate students and others.
of the participants have experience withjava.
.
automaticmetrics an automatic metric compares generated summaries with manual referencesummariestoproduce acorpus levelscore i.e.
asingle overall score for thegivenapproach summary levelscores for eachofthegeneratedsummaries or both.weselectmetricsthat are most commonlyused by code summarizationapproaches 1this study has been certified as exempt from the need for review by the washington state universityinstitutionalreviewboard.
1107esec fse august athens greece devjeet roy sarahfakhoury andvenera arnaoudova or that show the best performance from the most recent wmt findings results for the metrics task for wmt have not been published yet .
one notable mention here is that we replaceyisi best performingsegmentlevelmetric inwmt19 with bertscore sincethewordembeddingsrequiredtorunyisiare no longer available on the author s website.
we use the official implementationsofeachmetricusedbywmtortheauthorprovided implementation whenavailable.detailsabouttheimplementations usedare available inthe replication package.
corpus levelonly a bleu isatextualsimilaritymetricthatcalculatesthe precisionofn gramsinatranslatedsentenceascompared toareferencesentence withaweightedbrevitypenalty to punish short translations.
we use the standard bleu scorewhichprovidesacumulativescoreofuni bi tri and4 grams.
b rouge isapopularautomaticevaluationmetricthat is recall oriented.
it computes the count of several overlapping units such as n grams word pairs and sequences.
rouge has several different variants from which we consider the most popular ones rouge n rouge l androuge w. summary levelonly a sentbleu is a smoothed version of bleu designed to scoretranslationsatasegmentlevel.inouranalysis we use the script provided by nltk with smoothing method as reportedin .
bothsummary andcorpus level a meteor isametricbasedonthegeneralconceptof unigram matching and itcombinesprecision recall and a custom score determining the degree to which words are orderedcorrectlyinthe translation.
b bertscore isanewertypeofautomaticevaluation metricthatemployscontextualwordembeddingsusing the widely popular bert language model to compute the semantic and lexical similarities between a model s predictions andreference tokens.
c chrf is another recent automatic evaluation metric that works solely on character n grams rather than word n grams.itcan be seen as acharacter n gramf score.
.
sampling anddata preprocessing .
.
sampling.
the test set in the dataset described above contains90 908sourcecodesnippetsandtheircorrespondingreference summary.forrq1andrq2 werandomlysample383snippets confidencelevelandaconfidenceintervalof .foreachmethod wegenerate5summaries usingthe5selectedsummarizationapproaches .
including the reference summary this results in a total of2 uniquesummariesneedingevaluation.weaim for3annotatorsforeachsummary i.e.
atotalof6 unique human evaluations are needed.
.
.
data preprocessing.
each of the snippets has summaries writtenby3participantsthatareusedforqualitycontrol rq1and 2chen and cherry show that smoothing method performs the best but we encountered unstable results nans with it.
method is the second best performing smoothingmethod with a small performance difference.rq2 and for the analysis of multiple