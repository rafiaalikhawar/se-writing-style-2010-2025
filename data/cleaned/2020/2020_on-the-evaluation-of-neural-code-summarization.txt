on the evaluation of neural code summarization ensheng shia yanlin wangb lun dub junjie chenc shi hanb hongyu zhangd dongmei zhangb hongbin suna axi an jiaotong universitybmicrosoft research ctianjin universitydthe university of newcastle s1530129650 stu.xjtu.edu.cn hsun mail.xjtu.edu.cn yanlwang lun.du shihan dongmeiz microsoft.com junjiechen tju.edu.cn hongyu.zhang newcastle.edu.au abstract source code summaries are important for program comprehension andmaintenance.however thereareplentyofprogramswithmissing outdated ormismatchedsummaries.recently deeplearning techniques have been exploited to automatically generate summaries for given code snippets.
to achieve a profound understandingofhowfarwearefromsolvingthisproblemandprovidesuggestions to future research in this paper we conduct a systematic and in depthanalysis of5 state of the artneural code summarization models on widely used bleu variants pre processing oper ations and their combinations and widely used datasets.
the evaluationresultsshowthatsomeimportantfactorshaveagreat influenceonthemodelevaluation especiallyontheperformanceof models and the ranking among the models.
however these factors mightbeeasilyoverlooked.specifically thebleumetricwidely used in existing work of evaluating code summarization models hasmanyvariants.ignoringthedifferencesamongthesevariants could greatly affect the validity of the claimed results.
besides we discover and resolve an important and previously unknown bugin bleu calculation in a commonly used software package.
furthermore we conduct human evaluations and find that the metric bleu dc is most correlated to human perception code pre processing choices can have a large from to impacton the summarization performance and should not be neglected.
we also explore the aggregation of pre processing combinations and boost the performance of models some important char acteristics of datasets corpus sizes data splitting methods and duplicationratios haveasignificantimpactonmodelevaluation.
based ontheexperimental results we giveactionable suggestions forevaluatingcodesummarizationandchoosingthebestmethodindifferentscenarios.wealsobuildasharedcodesummarization toolbox to facilitate future research.
ccs concepts softwareanditsengineering softwaremaintenancetools.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
code summarization empirical study deep learning evaluation acm reference format ensheng shia yanlin wangb lun dub junjie chenc shi hanb hongyu zhangd dongmei zhangb hongbin suna .
.
on the evaluation of neuralcodesummarization.in 44thinternationalconferenceonsoftware engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction source code summaries are important for program comprehensionandmaintenancesincedeveloperscanquicklyunderstanda piece of code by reading its natural language description.
however documenting code with summaries remains a labor intensive and time consumingtask.asaresult codesummariesareoftenmissing mismatched or outdated in many projects .
therefore automatic generation of code summaries is desirable and many approaches have been proposed over the years .
recently deep learning dl based models are exploited to generatebetternaturallanguagesummariesforcodesnippets .
these models usually adopt a neural machine translation framework to learn the alignment between code and summaries.somestudiesalsoenhancedl basedmodelsbyincorporatinginformationretrievaltechniques .generally existing neural source code summarization models show promising results and claim their superiority over traditional approaches.
however wenoticethatinthecurrentcodesummarizationwork therearemanyimportantdetailsthatcouldbeeasilyoverlooked and important issues that have not received much attention.
these details and issues are associated with evaluation metrics evaluated datasetsandexperimentalsettings andaffecttheevaluationand comparisonofapproaches.inthiswork wewouldliketodivedeepinto the problem and answer how to evaluate and compare code summarization models more correctly and comprehensively?
to answer the above question we conduct systematic experiments of representative code summarization approaches includingcodenn deepcom astattgru rencos andncs on6widelyusedbleuvariants 4extensivelyused code pre processing operations and commonly useddatasets including tl codesum funcom and codesearchnet .
the bleu variants and code pre processing operations cover most of the studies on code summarization since .
each dataset is used in at least previous studies.
work performed during internship at microsoft research asia.
corresponding authors.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa shi et al.
our experiments can be divided into three major parts.
first we conduct anin depth analysis ofthe bleu metric whichis widely used in previous code summarization work andperformhumanevaluationstofindthebleu variant that best correlates with human perception section .
.
then we study different code pre processing operations in recent codesummarizationworksandexploreanensemblelearningbasedtechnique to boost the performance of code summarization models section4.
.finally weconductexperimentsonthethreedatasets fromthree perspectives corpussizes datasplitting methods and duplicationratios section4.
.throughextensiveexperiments we obtain the following major findings about the current neural code summarization evaluation.
thefirst major finding is that there is a wide variety of bleu metrics used in prior work and they produce rather differentresults for the same generated summaries.
some previous stud ies accurately describe the bleu metric used and compare models under the same bleu metric .however therearestillmany works cite or describe inconsistent bleu metrics leading to confusion for subsequent research.
what s worse some software packages used in for calculating bleu are buggy circlecopyrtthey may produce a bleu score greater than or even which extremely exaggerates the performance of code summarization models and circlecopyrtthe results are also different acrossdifferentpackageversions.moreimportantly bleuscores between papers cannot be directly compared .
however some studies copy the bleu scores reported in other papers and directly compare with them under different bleumetrics.
for example copiedthescoresreportedin and copiedthe scoresreportedin .thebleuimplementationsintheirreleased code are different.
furthermore the study d o e sn o t release its sourcecode.
therefore these studies mayoverestimate theirmodelperformanceormayfailtoachievefaircomparisons even though they are evaluated on the same dataset with the same experimental setting.
through human evaluation we find that bleu dc section .
correlates with human perception the most.
we further give some actionable suggestions on the usage of bleu in section .
.
thesecond major finding is that different pre processing combinationscanaffecttheoverallperformancebyanoticeablemarginof to .
the results of the exploration experiment show that asimpleensemblelearningtechniquecanboosttheperformance of code summarization models.
we also give actionable suggestions on the choice and usage of code pre processing operations in section .
.
thethird major finding is that code summarization approaches perform inconsistently on different datasets i.e.
one approachmay perform better than other approaches on one dataset andpoorly on another dataset.
furthermore we experimentally find thatthreedatasetattributes corpussizes datasplittingmethods and duplication ratios have important impact on the performance ofcode summarizationmodels.
wefurther givesomesuggestions about evaluation datasets in section .
.
in summary our findings indicate that in order to evaluate and comparecodesummarizationmodelsmorecorrectlyandcomprehensively weneedtopaymuchattentiontotheimplementationof bleu metrics the way of code pre processing and the usage of datasets.
the major contributions of this work are as follows we conduct an extensive evaluation of five representative neuralcodesummarizationmodelswithdifferentevaluation metrics code pre processing operations and datasets.
we conduct human evaluation and find that bleu dc ismost correlated to human perception for evaluating neuralcodesummarizationmodelsamongthesixwidely used bleu variants.
weconcludethatmanyexistingcodesummarizationmodels are not evaluated comprehensively and do not generalize well in new experimental settings.
therefore more research is needed to further improve code summarization models.
basedontheevaluationresults wegiveactionablesuggestions for evaluating code summarization models from multiple perspectives.
we build a shared code summarization toolbox1containing bleu variants implementation code pre processing operations and of their combinations datasets re implementations of baseline approaches that do not have publiclyavailablesourcecode andallexperimentalresults described in this paper.
background .
code summarization intheearlystageofautomaticsourcecodesummarization templatebasedapproaches are widelyused.however a well designed template requires expert domain knowledge.
therefore information retrieval ir based approaches are proposed.
thebasic ideais toretrieve termsfrom sourcecode to generate term based summaries or to retrieve similar source code and use its summary as the target summary.
however the retrieved summaries may not correctly describe the semantics and behaviorofcodesnippets leadingtothemismatchesbetweencode and summaries.
recently neural machine translation nmt based models are exploited to generate summaries for code snippets .
codenn i s an early attempt that uses only code token sequences followedby various approaches that utilize ast api knowledge type information global context reinforcement learning multi task learning dual learning andpre trainedlanguagemodels .inaddition hybrid approaches that combine the nmt based and irbased methods are proposed and shown to be promising.
.
bleu bilingual evaluation understudy bleu is commonly used for evaluating the quality of the generated code summaries .
in short a bleu score is a percentage number between and that measures the similarity between one sentence to a set of reference sentences using constituent n grams precision scores.
bleu typically uses authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
on the evaluation of neural code summarization icse may pittsburgh pa usa bleu bleu bleu and bleu calculated by gram 2gram gram and gram precisions to measure the precision.
a valueof0meansthatthegeneratedsentencehasnooverlapwith thereferencewhileavalueof100meansperfectoverlapwiththe reference.
mathematically the n gram precision pnis defined as pn summationtext.
c candidates summationtext.
n gram ccountclip n gram summationtext.
c prime candidates summationtext.
n gram c primecount n gram prime bleucombinesalln gramprecisionscoresusinggeometricmean bleu bp exp summationdisplay.1n n 1 nlogpn nis a uniform weight n n .
the straightforward calculation will result in high scores for short sentences or sentences with repeated high frequency n grams.
therefore brevity penalty bp isusedtoscale thescoreandeachn graminthe referenceis limited to be used just once.
the original bleu was designed for the corpus level calculation .forsentence levelbleu sincethegeneratedsentences and