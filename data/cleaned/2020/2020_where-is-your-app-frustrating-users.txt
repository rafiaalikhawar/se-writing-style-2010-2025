where is your app frustrating users?yawen wang1 junjie wang1 hongyu zhang5 xuran ming1 lin shi1 qing wang1 1laboratory for internet software technologies 3state key laboratory of computer sciences 4science technologyon integrated infomation system laboratory institute of software chinese academy of sciences beijing china2university of chinese academy of sciences beijing china5the university of newcastle callaghan australia yawen2018 junjie xuran2020 shilin wq iscas.ac.cn hongyu.zhang newcastle.edu.auabstractuser reviews of mobile apps provide a communication channel fordevelopers to perceive user satisfaction.
many app features thatusers have problems with are usually expressed by key phrasessuch as upload pictures which could be buried in the review texts.the lack of!ne grained view about problematic features couldobscure the developers understanding of where the app is frus trating users and postpone the improvement of the apps.
existingpattern based approaches to extract target phrases su er from lowaccuracy due to insufficient semantic understanding of the reviews thus can only summarize the high level topics aspects of the re views.
this paper proposes a semantic aware !ne grained appreview analysis approach sira to extract cluster and visualizethe problematic features of apps.
the main component of sira isa novel bert attr crf model for!ne grained problematic fea ture extraction which combines textual descriptions and reviewattributes to better model the semantics of reviews and boost theperformance of the traditional bert crf model.
sira also clustersthe extracted phrases based on their semantic relations and presentsa visualization of the summaries.
our evaluation on reviewsfrom six apps con!rms the e ectiveness of sira in problematicfeature extraction and clustering.
we further conduct an empiricalstudy with sira on reviews of popular apps to exploreits potential application and examine its usefulness in real worldpractice.ccs concepts software and its engineering requirements analysis soft ware maintenance tools.keywordsapp review information extraction deep learningacm reference format yawen wang1 junjie wang1 hongyu zhang5 xuran ming1 linshi1 qing wang1 .
.
where is your app frustrating users?.in44th international conference on software engineering icse may corresponding author.permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro!t or commercial advantage and that copies bear this notice and the full citationon the!rst page.
copyrights for components of this work owned by others than acmmust be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci!c permission and or afee.
request permissions from permissions acm.org.icse may pittsburgh pa usa association for computing machinery.acm isbn .
.
.
.
pittsburgh pa usa.acm new york ny usa pages.
introductionmobile app development has been active for over a decade generat ing millions of apps for a wide variety of application domains suchas shopping banking and social interactions.
they have now be come indispensable in our daily life.
the importance of mobile appsurges the development team to make every endeavor to understandusers concerns and improve app quality.users often write reviews of the mobile apps they are using ondistribution platforms such as apple store and google play store.these reviews are short texts that can provide valuable informa tion to app developers such as user experience bug reports andenhancement requests .
a good understanding ofthese reviews can help developers improve app quality and usersatisfaction .
however popular apps may receive a largenumber of reviews every day.
therefore manually reading andanalyzing each user review to extract useful information is verytime consuming figure an example app review and problematic feature.in recent years automated techniques for mining app reviewshave attracted much attention .
these techniques canhelp reduce the e ort required to understand and analyze app re views in many ways such as topic discovery and keyphrase extraction .
however existing work abouttopic discovery can only identifywhatthe users complain about such as the high level topics aspects of the reviews e.g.
compatibility update connection etc .
taken the review ofinsta gramin figure as an example existing approaches would captureterms such asupdate cache uninstall yet missing its core intent.developers still could not have a concrete understanding aboutwhich speci!c features of the app the users are complaining about.furthermore existing work about key phrase extraction mainlyutilizes heuristic based techniques such as part of speech patterns ieee acm 44th international conference on software engineering icse icse may pi t tsburgh pa usawang et al.parsing tree and semantic dependence graph to extract the targetphrases which could have insufficient semantic understanding ofthe reviews.
as a result their accuracy is less satisfactory and canbe further improved.in comparison we aim at exploiting thewhereaspect of the appreviews and providing an accurate!ne grained landscape aboutwhere an app frustrates the users i.e.
which speci!c app features1the users have problems with.
as an example in figure the reviewis about a crashing problem and the problematic feature the usercomplained about isupload to my story.
the!ne grained knowledgeabout problematic features could facilitate app developers in un derstanding the user concerns localizing the problematic modules and conducting follow up problem solving activities.to overcome the drawbacks of existing work and better exploitthe app reviews this paper proposes a semantic aware fine grainedapp review analysis approach sira which can extract cluster and visualize the problematic features of apps.
more speci!cally sira includes a novel bert attr crf model to automatically ex tract the!ne grained phrases i.e.
problematic features .
it com bines the review descriptions and review attributes i.e.
app cat egory and review description sentiment to better model the se mantics of reviews and boost the performance of the traditionalbert crf model .
with the extracted phrases sira then de signs a graph based clustering method to summarize the commonaspects of problematic features based on their semantic relations.
fi nally sira presents a visualization of the summarized problematicfeatures.we evaluate sira on reviews involving textual sen tences from six apps spanning three categories.
for problematicfeature extraction the overall precision and recall achieved by sirais .
and .
respectively signi!cantly outperforming thestate of the art methods.
sira can also achieve high performancein problematic feature clustering outperforming two commonly used baselines.
we further conduct an empirical study with sira on318 reviews of popular apps reviews spanning months to explore its potential application and examine its usefulness inreal world practice.
we!nd that di erent apps have their uniqueproblematic features and problematic feature distributions.
theresults also reveal that di erent apps can share some commonproblematic features.
this observation can facilitate mobile apptesting e.g.
recommending bug prone features to similar apps fortest prioritization.the main contributions of this paper are as follows a semantic aware !ne grained app review analysis approach sira to extracting clustering and visualizing the problem atic features of apps.
in sira we design a bert attr crfmodel to automatically extract the!ne grained phrases i.e.
problematic features and a graph based clustering methodto summarize the common aspects of problematic features.
the evaluation of the proposed sira on reviews in volving textual sentences from six apps spanning threecategories with affirmative results.1we refer to a feature as a distinctive user visible characteristic of a mobile app e.g.
sending videos viewing messages etc.
a large scale empirical study on reviews of popu lar apps to explore its potential application and usefulnessin real world practice.
public accessible source code and experimental data at background and related worknamed entity recognition ner .ner is a classic natural lan guage processing nlp task of sequence tagging .
given asequence of words ner aims to predict whether a word belongsto named entities e.g.
names of people organizations locations etc.
ner task can be solved by linear statistical models e.g.
max imum entropy markov models hidden markov models and conditional random fields crf .
deep learning based techniques would use a deep neural network to capturesentence semantics and a crf layer to learn sentence level tagrules.
typical network structures include convolutional neural net work with crf conv crf long short term memory networkwith crf lstm crf and bidirectional lstm network with crf bilstm crf .
by taking advantage of the bidirectional struc ture bilstm crf model can use the past and future input infor mation and can usually obtain better performance than conv crfand lstm crf.language model pre training techniques have been shown tobe e ective for improving many nlp tasks .
bert bidi rectional encoder representations from transformers is atransformer based representation model that uses pre trainingto learn from the raw corpus and!ne tuning on downstream taskssuch as the ner task.
employing bert to replace bilstm shortfor bert crf could lead to further performance boosts .
bert crf model bene!ts from the pre trained representations on largegeneral corpora combined with!ne tuning techniques.mining user reviews.harman et al.
introduced the concept ofapp store mining by identifying correlations between the customerratings and the download rank of a mobile app .
palombaet al.
found that developers implementing user reviews would berewarded in terms of app ratings .
noei et al.
investigated theevolution of app ranks and identi!ed the variables that share astrong relationship with ranks e.g.
number of releases .previous studies on mining user reviews emphasized the topicdiscovery classi!cation and summarization of reviews as a wayof aggregating a large amount of text and reducing the e ort re quired for analysis .
these classi!cations are fromdi erent points of view e.g.
whether or not the reviews includebug information requests for new features whether they areinformative whether reviews across di erent languages andplatforms are similar or based on a taxonomy relevant tosoftware maintenance and evolution etc.
other studies fo cused on the information extraction from app reviews consider ing the fact that reading through the entire reviews is impractical .
for example the types of complains theapp aspects loved by users user rationale and summariesfor guiding release planning are extracted and summarized forfacilitating the review understanding.there are some studies on mining api related opinions frominformal discussions such as q a websites e.g.
stack over ow 2428where is your app frustrating users?icse may pi t tsburgh pa usa figure the overview of sira.to alleviate developers burden in performing manual searches .
these methods mainly depend on fuzzy matching withpre built api databases which cannot work in our context.
thereare also some studies on mining social media data e.g.
twitterdata .
the app reviews mainly convey users feedback aboutan app while the twitter data is more general and contains dailymessages.
therefore general purpose techniques for twitter datarequire customizations to better understand app reviews.some studies are similar to our work such as topic discov ery classi!cation sentiment analysis etc.
however they do notsupport the extraction of!ne grained features well.
for example infar mines insights from app reviews and generates sum marizes after classifying sentences into pre de!ned topics.
thediscovered topics from infar are more coarse grained e.g.
gui crash etc.
.
our method can highlight the!ne grained features e.g.
push noti!cation that users complained about sur miner and caspar uses techniques such as dependency parsing andpart of speech pattern to extract some aspects from app reviews.guzman et al.
proposed a method which can only extract fea tures consisting of two words i.e.
collocations from the reviewsbased on word co occurrence patterns which is not applicable inour context because the problematic features might contain mul tiple words opiner is a method to mining aspects from apireviews.
it extracts api mentions from api reviews through exactand fuzzy name matching with pre built api databases which isdifficult to work in our context because we do not have a databaseof feature phrases in advance.
these studies utilized pattern basedmethod to extract the target phrases which did not consider thereview semantics sufficiently and had bad tolerance to noise bycomparison our proposed approach is a semantic aware approach.mining open source bug reports.previous studies have pro posed various methods to automatically classify bug reports detect the duplicate reports summarize the reports and triage the reports etc.
the bug reports in opensource or crowd testing environment are often submitted by soft ware practitioners and often described with detailed bug expla nation and in relatively longer length.
yet the app reviews aresubmitted by the end users and in much fewer words thus theabove mentioned approaches could not be easily adopted in thiscontext.semantic aware approaches in se.researchers have utilizeddeep learning based techniques to capture the semantics of softwareartifacts and facilitate the follow up software engineering tasks.such kinds of studies include neural source code summarizationwith attentional encoder decoder model based on code snippets andsummaries requirement traceability by incorporating require ments artifact semantics and domain knowledge into the tracingsolutions knowledge mining of informal discussions on socialplatforms etc.
this paper focuses on a di erent type of soft ware artifact i.e.
app reviews and incorporates a state of the arttechnique i.e.
bert for the semantic aware learning and theresults show its e ectiveness.
approachthis paper proposes a semantic aware fine grained app reviewanalysis approach sira to extract cluster and visualize the prob lematic features of apps i.e.
the phrases in app reviews depictingthe feature which users have problems with see the examples infigure .
figure presents the overview of sira which consists of foursteps.first it preprocesses the app reviews crawled from onlineapp marketplace to obtain the cleaned review descriptions andthe review attributes i.e.
the category of the belonged appcandthe review description sentiments .second it builds and trains abert attr crf model to automatically extract the!ne grainedphrases about problematic features.
bert attr crf combines thereview descriptions and two review attributes as input to bettermodel the semantics of reviews and boost the phrase extractionperformance of the traditional bert crf model.third sira clus ters the extracted phrases with a graph based clutering method to2429icse may pi t tsburgh pa usawang et al.summarize the common aspects of problematic features based ontheir semantic relations.
and f inally it presents a visualizationview to illustrate the summaries and compare the problematic fea tures among apps in order to acquire a better understanding ofwhere users complain about across apps.
.
data preprocessingdata preprocessing mainly includes two steps textual data cleaningand review attribute collection.
.
.1textual data cleaning.the raw app reviews are often submitted via mobile devices andtyped using limited keyboards.
this situation leads to the frequentoccurrences of massive noisy words such as repetitive words mis spelled words acronyms and abbreviations .following other crf based practices we treat each sen tence as an input unit.
we!rst split each review into sentences bymatching punctuations through regular expressions.
then we!lterall non english sentences with langid2.
we tackle the noisy wordsproblem with the following steps lowercase we convert all the words in the review descrip tions into lowercase.
lemmatization we perform lemmatization with spacy3toalleviate the in uence of word morphology.
formatting we replace all numbers with a special symbol number to help the bert model unify its understanding.besides we build a list containing all the app names crawledfrom google play store and replace them with a uniformspecial symbol appname .
.
.2review a t tribute collection.some attributes related to the review or the app can facilitate theextraction of problematic features in section .
.
this subsectioncollects these attributes i.e.
the category of the belonged appcandthe review description sentimentsas shown in figure and figure3.
the reason why we include the app category is that apps fromdi erent categories would exert unique nature in terms of func tionalities and topics .
furthermore review descriptions withnegative sentiment would be more likely to contain problematicfeatures compared with the description with positive sentiment.hence we include review description sentiment as the second at tribute in our model.app categories can be directly collected when crawling datafrom google play store.
to obtain the sentiment for each reviewsentence we employ sentistrength se a domain speci!c senti ment analysis tool especially designed for software engineering text.sentistrength se would assign a positive integer score in the rangeof not positive to extremely positive and a negative integerscore in the range of not negative to extremely negative toeach sentence.
employing two scores is because previous researchfrom psychology has revealed that human beings process thepositive and negative sentiment in parallel.
following previouswork if the absolute value of the negative score multipliedby .
is larger than the positive score we assign the sentence the2 figure detailed structure of bert attr crf.negative sentiment score otherwise the sentence is assigned withthe positive sentiment score.
.
problematic feature extractionwe model the problematic feature extraction problem as a namedentity recognition ner task where we treat problematic featuresas named entities and solve the problem with the commonly usedcrf technique.
to better capture the semantics of the app reviews we employ the bert model to encode the review descriptions.
fur thermore we incorporate the review attributes in the crf modelto further boost the recognition of problematic features.
two at tributes i.e.
category of the belonged appcand review descriptionsentiments see section .
.
are utilized in our model.following other ner tasks we use the bio tag format totag each review sentence where b label beginning the word is the beginning of the targetphrase.
i label inside the word is inside the target phrase but notits beginning.
o label outside the word is outside the target phrase.the bio tagged review sentence is input into the bert attr crfmodel for further processing.figure presents the detailed structure of our proposed bert attr crf model.
since app reviews are short texts and the involved vo cabulary is relatively small we use the pre trained modelbertbase4 which has layers hidden dimensions and attention heads.it has been pre trained on the bookscorpus 800m words and eng lish wikipedia 500m words and will be!ne tuned using ourown data.
each input sentence is represented by word tokenswith a special starting symbol .
for those not long enough we use a special symbol to pad them to the length of following the common practice.
the outputs of bert are fed into adropout layer to avoid over !tting.
finally we obtainn the lengthof the input sentence vectors with each vector denoted as v.alti having dimensions and corresponding to each input word.
is your app frustrating users?icse may pi t tsburgh pa usawe incorporate the review attributes into the textual vectors v.alt to jointly capture the underlying meaning of the review sentence.the review attributes cands extracted in section .
.
are discretevalues.
we!rst convert them into continuous vectors denotedashcandhs by feeding them into the embedding layers.
takingattributesas an example it can take ten values to and to .the embedding layer could represent each value with a continuousvector which can be trained jointly with the whole model.
wethen concatenatehc hsand v.alt hc circleplustext.1hs circleplustext.
v.alt to obtain a vector denoted as v.alt primei for each input word.
the concatenated vectors!rstgo through a multi layer perceptron mlp which computes theprobability vector denotedp of bio tags for each word p f w wheref is the activation function andwis trainable parame ters in mlp.
is the concatenation of these three vectors.finally pis input into the crf layer to determine the most likelytag sequence based on viterbi algorithm .based on the derived tag sequence we can obtain the phrasesabout problematic features.
for example if our input review sen tence is whenever i go to send a video it freezes up and the outputtag sequence is o o o o b i i o o o we can determine the extracted problematicfeature as send a video based on the bio format.the loss function of the model should measure the likelihood ofthe whole true tag sequence instead of the likelihood of the truetag for each word in the sequence.
therefore the commonly usedcross entropy is not suitable in this context.
following existingstudies the loss function contains two parts the emission scoreand the transition score.
it is computed as s t1 t1 tildewide t summationdisplay.1t t t t t where t1is the sentence sequence of lengtht and t1is thetag sequence.f t1 is the emission score which is the output ofmlp with parameters and i jis the transition score which isobtained with the parameters from the crf layer.
the transitionscore i jmodels the transition from thei th state to thej thstate in the crf layer.
tildewide braceleftbig i j i j bracerightbigis the new parametersfor the whole network.
the loss of a sentence t1along with asequence of tags t1is derived by the sum of emission scores andtransition scores.model training the hyper parameters in sira are tuned care fully with a greedy strategy to obtain the best performance.
given ahyper parameterpand its candidate values v.alt1 v.alt2 .
.
.
v.altn we per form automated tuning forniterations and choose the values whichleads to the best performance as the tuned value ofp.
after tuning the learning rate is set as .
the optimizer is adam algorithm .
we use the mini batch technique for speeding up the trainingprocess with batch size .
the drop rate is .
which means of neuron cells will be randomly masked to avoid over !tting.we implement this bert attr crf model using transformers5 which is an open source pytorch library for natural language un derstanding and natural language generation.
our implementationand experimental data are available online6.
.
problematic feature clusteringthe extracted problematic features might be linguistically di er ent yet semantically similar.
to provide a summarized view of theproblematic features this step clusters the extracted problematicfeatures based on the topics derived from their semantic relations.conventional topic models use statistical techniques e.g.
gibbssampling based on word co occurrence patterns .
they are notsuitable for the short texts i.e.
problematic features in our context because the co occurrence patterns can hardly be captured fromthe short text instead the semantic information should be takeninto consideration.
additionally these models need to specify thenumber of clusters topics which is hardly determined in our con text.
to tackle these challenges we design a graph based clusteringmethod which employs semantic relations of problematic features.first we convert problematic feature phrases into dimen sional vectors using universal sentence encoder use .
it is atransformer based sentence embedding model that captures richsemantic information and has been proven more e ective thantraditionally used word embedding models .second we con struct a weighted undirected graph where each problematic featureis taken as a node and the cosine similarity score between use vec tors of two problematic features is taken as the weight between thenodes.
if the score is over a certain ratio we add an edge betweentwo nodes.
the ratio is an input hyper parameter which measuresthe semantic correlations between problematic features.
the higherratio leads to higher cluster cohesion.
we set it as .
after tuningin the training data.third we perform chinese whispers cw which is an efficient graph clustering algorithm on this graphto cluster problematic features.with this graph based clustering method sira can group theproblematic features that are semantically similar into the sametopic.
we implement our clustering method in python based onthe open source implementation of use7and cw8.
.
visualizationin order to display the clustering results of multiple apps moreintuitively we provide a visualized view in the form of bubblecharts an example is shown in figure .
the y axis demonstratesthe names of investigated apps and the x axis represents the id ofeach cluster.
the size of the bubble denoted assa c of appainclustercis de!ned as the ratio between the number of problematicfeatures of appain clustercand the total number of problematicfeatures in appa.when the cursor hovers over the bubble it would display detailedinformation of this cluster including the cluster name the numberof problematic features and example reviews with correspondingproblematic features.
for the cluster name we!rst!nd the most5 may pi t tsburgh pa usawang et al.table experimental dataset.categoryapp reviews sentencessocialinstagram5821 402snapchat5851 388communicationgmail5861 525yahoo mail5421 511financebpi mobile5881 488chase mobile5431 474overall3 788frequent noun or verb denoted asw among all problematic featuresin the cluster.
we then count the number of problematic featurescontainingw and treat the most frequent phrase as the cluster name i.e.
the representative problematic feature .
by comparing therelative sizes of bubbles one can intuitively acquire the distributionof problematic features across apps.
experimental design4.
research questionswe answer the following three research questions rq1 what is the performance of sira in extracting prob lematic features?
rq2 is each type of the review attributes employed in siranecessary?
rq3 what is the performance of sira in clustering prob lematic features?rq1investigates the performance of sira in problematic featureextraction and we also compare the performance with four state of the art baselines see section .
to further demonstrate itsadvantage.rq2conducts comparison with sira s three variantsto demonstrate the necessity of the employed review attributes inbert attr crf model.rq3investigates the performance of sirain problematic feature clustering and we also compare sira withtwo commonly used baselines see section .
.
.
data preparationwe use the reviews of six apps from three categories two in eachcategory in our experiments.
all six apps are popular and widely used by a large number of users.
we!rst crawl the app reviewsfrom google play store submitted during august to january2020 with the tool google play scraper9.
for each app we thenrandomly sample around reviews about sentences andlabel them for further experiments.
table elaborates the statisticsof the experimental dataset in detail.
it contains reviews and8 sentences in total.three authors then manually label the app reviews to serve asthe ground truth in verifying the performance of sira.
to guar antee the accuracy of the labeling outcomes the!rst two authors!rstly label the app reviews of an app independently i.e.
mark thebeginning and ending position of the problematic features in eachreview sentence.
second the fourth author compares the labelingresults !nds the di erence and organizes a face to face discussionamong them three to determine the!nal label.
all the six appsfollow the same process.
for the!rst labeled app instagram the9 kappa is .
between the two participants while for thelast labeled app chase mobile the cohen s kappa is .
.
aftertwo rounds of labeling a common consensus is reached for everyreview sentence.
.
baselines4.
.1baselines for problematic feature extraction.we select methods that can extract target phrases from app re views as baselines for problematic feature extraction.
to the best ofour knowledge existing methods are mainly pattern based whichcan be classi!ed into three types based on the techniques part of speech pos pattern safe and puma depen dency parsing plus pos pattern caspar and sur miner pattern based filter plus text classi!cation kefe .
we se lect the representative method from each type as baselines i.e.
kefe caspar and safe.
in addition since we model the featureextraction as an ner task we also include bilstm crf acommonly used technique in ner tasks as a baseline.
we intro duce four baselines in detail below bilstm crf a commonly used algorithm in sequencetagging tasks such as ner.
being a deep learning based technique it utilizes a bilstm to capture sentence semantics and a crf layerto learn sentence level tags.kefe a state of the art approach for identifying key fea tures from app reviews.
a key feature is referred as the featuresthat are highly correlated to app ratings.
it!rstly employs a pattern based!lter to obtain candidate phrases and then a bert basedclassi!er to identify the features.
since its patterns are designed forchinese language we replace them with the patterns in safe to handle english reviews.caspar a method for extracting and synthesizing user reported mini stories regarding app problems from reviews.
wetreat its!rst step i.e.
events extraction as a baseline.
an event isreferred as a phrase that is rooted in a verb and includes other at tributes related to the verb.
it employed pattern based and grammat ical nlp techniques such as pos tagging and dependency parsing onreview sentences to address this task.
we use the implementationprovided by the original paper10.safe a method for extracting feature related phrases fromreviews by pos patterns.
for example the patternverb adjective nouncan extract features like delete old emails .
we implement all18 patterns to extract the phrases based on the nlp toolkit nltk11.
.
.2baselines for problematic feature clustering.we employ the following two baselines for problematic featureclustering which are commonly used for mining topics of appreviews k means it is a commonly used clustering algorithm and wasemployed to cluster the keywords of app reviews .
in this work we!rst encode each problematic feature with tf idf vectors then run k means to cluster all problematic features into topics following previous work .
we apply the implementation in thelibrary scikit learn12.
is your app frustrating users?icse may pi t tsburgh pa usalda it is a commonly used topic clustering algorithm andwas utilized to group the app features .
in this work we treatthe extracted problematic features as documents and run lda fortopic modeling following previous work .
we employ the im plementation in the library gensim13.
.
experimental setupto answer rq1 we conduct nested cross validation on theexperimental dataset.
the inner loop is for selecting optimal hyper parameters which are used for evaluating performance in the outerloop.
in the outer loop we randomly divide the dataset into ten folds use nine of them for training and utilize the remaining one fold fortesting the performance.
the process is repeated for ten times andthe average performance is treated as the!nal performance.
in theinner loop we use eight folds for training and one fold for validation.we run each baseline see section .
to obtain its performancefollowing the same experimental setup and present the evaluationresults on each app and on the overall dataset respectively.for rq2 we design three variants of bert attr crf modelto demonstrate the necessity of employed review attributes inour model architecture.
in detail bert crf bert cat crf andbert sen crf respectively represent the model without reviewattributes i.e.
only with text the model without review descriptionsentiment i.e.
with text and app category and the model withoutapp category i.e.
with text and review description sentiment .
wereuse other experimental setups as rq1.for rq3 we manually build the ground truth clustering resultsto evaluate the problematic feature clustering performance.
thecriteria for labeling are to group the features that represent thesame functionality into one cluster.
more speci!cally we randomlysample problematic features for each app in total derivedfrom the results of rq1.
the two authors independently label theseproblematic features into clusters in the!rst round where the co hen s kappa between two authors reaches .
i.e.
a satisfactorydegree of agreement .
then follow up discussions are conducteduntil common consensus is reached.
finally the problematicfeatures were labeled into groups.
note that we do not specifythe number of clusters in advance because it is hard to decidethe number in our context.
our proposed clustering method doesnot need to specify this parameter as well.
meanwhile we runour approach and each baseline see section .
to cluster theseproblematic features and obtain each approach s clustering per formance by comparing the predicted and ground truth clusteringresults for each app and the overall dataset respectively.the experimental environment is a desktop computer equippedwith an nvidia geforce rtx gpu intel core i7 cpu 16gbram running on windows and training the model takes about2.
hours for each fold nested cross validation.
.
evaluation metrics4.
.1metrics for problematic feature extraction.we use precision recall and f1 score which are commonly usedmetrics to evaluate the performance of sira for problematic fea ture extraction.
we treat a problematic feature is correctly predicted13 the predicted phrase from sira for a review sentence of an app isthe same as the ground truth one.
three metrics are computed as precisionis the ratio of the number of correctly predictedphrases to the total number of predicted phrases.
recallis the ratio of the number of correctly predictedphrases to the total number of ground truth phrases.
f1 scoreis the harmonic mean of precision and recall.
.
.2metrics for problematic feature clustering.following previous work we use the commonly used adjustedrand index ari and normalized mutual information nmi to evaluate the clustering performance by comparing with theground truth clustering results.
higher metric values indicate betterclustering performance.
for clarity we denotegas the ground truthclustering result andcas the predicted clustering result.adjusted rand index ari it takes values in r e ect ing the degree of overlap between the two clusters.
the raw randindex ri is computed byri a b n2 whereais the number of pairsthat are assigned in the same cluster ingand also in the same clus ter inc andbis the number of pairs that are assigned in di erentclusters both ingandc.
parenleftbign2 parenrightbigis the total number of unordered pairsin a set ofnphrases.
the raw ri score is then adjusted for chance into the ari score using the following scheme ari ri e ri max ri e ri wheree ri is the expected value ofri.
in this way the ari canbe ensured to have a value close to .
for random labeling inde pendently of the number of clusters and samples.normalized mutual information nmi it measures the sim ilarity degree of the two sets of clustering results between nomutual information and perfect correlation .nmi g c mi g c radicalbigh g h c whereh g summationtext.
g i 1p i lo afii10069.ital p i is the entropy of setg andp i ginis the probability that a phrase picked randomly fallsinto clustergi.
themi g c is the mutual information ofgandc i.e.
mi g c summationtext.
g i summationtext.
c j 1p i j lo afii10069.ital parenleftbigp i j p i p j parenrightbig.
results and analysis5.
answering rq1the last column of table presents the performance of sira inproblematic feature extraction.
the overall precision recall and f1are .
.
and .
respectively which indicates that84.
of problematic features extracted by sira are correct and85.
problematic features are correctly extracted from the ground truth ones.
the results con!rm that our proposed approach canaccurately extract the problematic features.more speci!cally sira reaches the highest precision of .
ongmailand the highest recall of .
onyahoo mail.
its lowestprecision is .
onyahoo mailand the lowest recall is .
onsnapchat.
we can see that even with its worst performance anacceptable precision and recall can be achieved.we then examine the extracted problematic features in detail and!nd that there are indeed some observable patterns associated2433icse may pi t tsburgh pa usawang et al.table evaluation on problematic feature extraction rq1 .appmetric methodkefecasparsafebilstm crfsirainstagramp40.
.
.
.
.
r60.
.
.
.
.
f148.
.
.
.
.
snapchatp42.
.
.
.
.
r58.
.
.
.
.
f148.
.
.
.
.
gmailp53.
.
.
.
.
r78.
.
.
.
.
f163.
.
.
.
.
yahoomailp12.
.
.
.
.
r70.
.
.
.
.
f121.
.
.
.
.
bpimobilep41.
.
.
.
.
r62.
.
.
.
.
f150.
.
.
.
.
chasemobilep36.
.
.
.
.
r52.
.
.
.
.
f143.
.
.
.
.
overallp42.
.
.
.
.
r63.
.
.
.
.
f151.
.
.
.
.
compared to sira statistical signi!cancep v.altalue .05is denoted by andp v.altalue .01is denoted by .with the problematic features.
for example users would use somenegative words e.g.
cannot hardly or temporal conjunctions e.g.
as soon as when before mentioning the problematic fea tures.
this could probably explain why the pattern based technique could work sometimes.
taking the review in figure1 as an example extracting the phrases after the negative word can t would obtain the correct phrase.
however the pattern basedtechniques highly rely on the manually de!ned patterns and havepoor scalability in a di erent dataset.
furthermore there are manycircumstances when the pattern based approach can hardly work.for example it is quite demanding to design patterns for the fol lowing review sentence this update takes away my ability to viewtransactions where the problematic feature is view transaction .these circumstances further prove the advantages and exibilityof our approach.we also examine the bad cases where sira fails to work.
in somecases sira can extract the core nouns and verbs of the target phrase but misses or additionally extracts some trivial words especiallysome adverbs adverbials before or after the core phrase.
for exam ple sira might wrongly extract received emails for days from i have not received emails for days where the ground truthphrase is received emails .
such results pull down the performance.this could be improved by considering pos patterns of words whenvectorizing review sentences in future work.comparison with baselines.table presents the performanceof sira and four baselines in extracting problematic features.
siraoutperforms all baselines on all metrics.
this indicates that thesepattern based baselines i.e.
kefe caspar and safe are far frome ective in extracting problematic features while the deep learning based baseline i.e.
bilstm crf is a bit worse than sira becauseof the inferior semantic understanding and neglect of review at tributes.
to further intuitively demonstrate the advantages of sira table presents two example reviews and the corresponding prob lematic features extracted by sira and four baselines.among the three pattern based baselines safe achieves .
precision and .
recall.
this is because it de!nes pos pat terns for feature related phrases and can retrieve a large numberof possible problematic features i.e.
high recall .
for example inthe!rst example of table safe would return two phrases.
bycomparison caspar only extracts events from reviews containingtemporal conjunctions and key phrases including when ev ery time which can hardly work well in this context.
taking the!rst review in table as an example caspar can only extract thetwo phrases clauses.
kefe achieves the promising performance indicating that it can!lter away many low quality phrases withthe bert classi!er yet the classi!cation is still conducted basedon candidate phrases extracted by a pattern based method whichlimits its performance.
in the!rst example of table kefe can!lter the wrong phrase keeps crashing but the reserved phrase take a picture is still not accurate enough due to the drawback ofpattern based candidate phrases.
bilstm crf can achieve promis ing performance but still not as accurate as our proposed sira e.g.
view story in table .
sira can be regarded as an improved ver sion of bilstm crf which employs bert!ne tuning techniqueand two customized review attributes.
the features extracted bysira is the superset of bilstm crf which can be also re ectedby the results in table .
sira outperforms bilstm crf in bothrecall and precision indicating that sira can extract features moreaccurately and retrieve more problematic features.
.
answering rq2table presents the performance of sira and its three variants respectively.
the overall performance of sira is higher than all thethree variants.
compared with the base bert crf model addingthe app category and the sentiment attributes noticeably increasethe precision .
and recall .
.
this indicates that the twoattributes are helpful in identifying the problematic features.
for theperformance on each app adding the two attributes i.e.
bert attr crf obtains the best performance on most apps and adding oneof the two attributes i.e.
bert cat crf or bert sen crf occasionally achieves the best performances on some apps e.g.
bert sen crf onsnapchat .
moreover even the performance ofthe base bert crf model outperforms the best baseline in rq1 i.e.
bilstm crf which veri!es the advantage of our model design.among the two added review attributes the review descriptionsentiment attribute contributes slightly more to performance im provement .
in precision and .
in recall than the appcategory attribute .
in precision and .
in recall .
further more we also observe that the contribution of these two attributesoverlaps to some extent i.e.
the increased performance by eachattribute is not simply added up to the performance of the wholemodel.
this is reasonable considering the fact that words express ing the user sentiment could be encoded semantically in the textualdescriptions and captured by the bert model.
nevertheless theoverall performance achieved by adding both of the attributes isthe highest further indicating the necessity of our model design.
.
answering rq3table presents the performance of sira in clustering problematicfeatures as well as the two baselines.
sira outperforms the two2434where is your app frustrating users?icse may pi t tsburgh pa usatable examples on extracted problematic features by different approaches rq1 .
reviewkefecasparsafebilstm crfsira 1keeps crashingwhen i try to take a picture of a check.take a picturekeeps crashing i try to takea picture of a checkkeeps crashing take a picturetake a pictureof a checktake a pictureof a check 2when i try to view story of friend the majority ofthe time it get stuck on a wheel and never load.view storyi try to view story of friend the majority of the time it getstuck on a wheel never loadview storyview storyview story of friendtable ablation experiment on attributes rq2 .appmetric methodbert crfbert cat crfbert sen crfbert attr crfinstagramp82.
.
.
.
r80.
.
.
.
f181.
.
.
.
snapchatp84.
.
.
.
r81.
.
.
.
f182.
.
.
.
gmailp88.
.
.
.
r78.
.
.
.
f182.
.
.
.
yahoomailp75.
.
.
.
r83.
.
.
.
f179.
.
.
.
bpimobilep84.
.
.
.
r78.
.
.
.
f181.
.
.
.
chasemobilep78.
.
.
.
r77.
.
.
.
f177.
.
.
.
overallp82.
.
.
.
r79.
.
.
.
f181.
.
.
.
compared to bert crf statistical signi!cancep v.altalue .05is denotedby andp v.altalue .01is denoted by .table evaluation on problematic feature clustering rq3 .appmetricmethodldak meanssirainstagramari0.
.
.29nmi0.
.
.84snapchatari0.
.
.32nmi0.
.
.85gmailari0.
.
.45nmi0.
.
.82yahoo mailari0.
.
.41nmi0.
.
.82bpi mobileari0.
.
.59nmi0.
.
.89chase mobileari0.
.
.26nmi0.
.
.82overallari0.
.
.38nmi0.
.
.77baselines on the overall performance where ari and nmi reach0.
and .
respectively which is higher than that of lda .21and .
and k means .
and .
.furthermore the improvement of sira on ari is greater thanthe improvement on nmi.
ari is a pair wise metric which is moresensitive when two phrases that should belong to the same clusterare wrongly assigned into di erent clusters or when two phraseswhich should belong to di erent clusters are wrongly placed intothe same cluster.
the ari results we obtained indicate that sira cantable experimental dataset for investigating where theapps frustrating users .categoryapp reviews sentencessocialfacebook64 156instagram63 852tiktok61 094snapchat18 278twitter15 386sina weibo10 372communicationfacebook messenger27 303gmail9 520telegram7 672yahoo mail7 124skype3 139tencent qq3 326financepaytm18 836chase mobile3 952alipay3 359bpi mobile1 638bca mobile386960wavepay58124overall318 091e ectively avoid generating new clusters or breaking up the originalclusters.
nmi is an entropy based metric which mainly focuseson the changes of two distributions based on information entropytheory.
the nmi results we obtained indicate that the distributionof the entire cluster e.g.
the number of problematic features ineach cluster derived from sira are closer to the ground truth.the baseline approaches use the word statistics or co occurrencerelations to cluster the problematic features.
the performance of ourproposed graph based clustering method indicates that it can betterunderstand the semantic relations among problematic features.
where the apps frustrate users anempirical study with sirathis section describes a large scale empirical study with sira onpopular apps.
first we apply sira to apps of three categories in each category to demonstrate how sira can be utilizedin real world practice the distribution of problematic featuresacross these popular apps.
we also select apps in each category and conduct a user survey to verify the usefulness of sira.sira in the large.we crawl the app reviews of apps fromthree categories in each category submitted during february2020 to december note that this is di erent from the timeperiod in section .
.
table lists the statistics of this dataset which contains reviews and sentences.
we runsira on this large scale dataset to obtain the visualization of theclustered problematic features see section .
.
in total we obtain113 clusters for social apps clusters for communication apps2435icse may pi t tsburgh pa usawang et al.
a social b communication c financefigure the distribution of problematic features of different categories.table cluster name i.e.
representative problematic fea ture of each cluster in figure .
socialcommunicationfinancec1the reel optiondelete emailsend messagec2like a postopen applog inc3search optionreceive noti!cationreceive otp codec4load tweetsend and receive emailload the pagec5use!lterdark modecheck depositc6follow peopleload inboxget noti!cationc7the front camerasign into accountuse!nger printc8click on photosend picture and videoclick buttonc9send snapvideo calldo transactionc10receive noti!cationsee storytransfer moneyc11get live optionclick on call buttonget cash backc12post storysync accountscan qr codec13access accountchange the emojiand nicknamerecharge mobilenumberc14open snapshare photochange phone numberc15send messageregister useropen passbookc16watch videochat with friendbook ticketc17dark modeget otp for loginselect optionc18scroll the feedreceive veri!cation codecheck balancec19retrieve tweetquiz botmake paymentc20get veri!cation codechange phone numberreceive the refundand clusters for!nance apps.
figure presents the visualizationresults of clusters for each category with the bubble size denotingthe ratio of corresponding problematic features.
for clarity we onlypresent the clusters whose number of problematic features is intop by the order of cluster id.
table shows the name of eachcluster in figure .
the following observations can be obtained.first our visualization can provide a summarized view of theproblematic features for each app and the comparison across apps.this enables the developers to acquire where the app is prone toproblems and where other apps are also likely to have issues witha single glance.
one can also derive the detailed content of eachcluster and example app reviews of the cluster by hovering thecursor over the bubble in the!gure see examples in figure c .second di erent apps can share similar problematic features which can facilitate app testing and re!ne the testing techniques.take figure a as an example although the problematic featuresare observed distributing di erently across apps all the six in vestigated apps would have a noticeable number of problematicfeatures in certain clusters i.e.
c12.
post storyandc13.
access ac count .
these information can warn the developers of similar appsto notice potential problems especially which have not yet beenreported or only mentioned in a few reviews.
further developerscan leverage reviews from similar apps for quality assurance activi ties rather than only focus on the limited set of reviews of its ownapp.
this is especially the case for the less popular apps which onlyhave few reviews regarding app problems.third di erent apps can have their unique problematic featuresand problematic feature distributions which further indicates thenecessity of review mining and analysis in a!ne grained way.
forexample from figure b we can see that based on the user re ported problems reviews of thefacebook messengerapp relatewith featurec8.
send picture and video.
by comparison its competi torgmailapp is mainly prone to bugs for quite di erent featurec4.
send and receive email.
in addition for its another competitortelegramapp the problematic features are distributed more evenly i.e.
the number of user submitted reviews do not exert big di er ence acrossc4 c7andc8 and the largest cluster i.e.
c7.
signinto account occupies a mere of reviews.
from these insightsprovided by our approach the developers can obtain a clear under standing of an app about the features that are prone to problems soas to arrange the follow up problem solving and allocate the testingactivity for subsequent versions.
more than that these informationcan also assist the developers in the competitive analysis of apps e.g.
acquire the weakness of their app compared with similar apps.furthermore a series of attempts can be made to re!ne the apptesting techniques.
for example one can recommend problematicfeatures to similar apps in order to prioritize the testing e ort orrecommend related descriptions mined from app reviews to similarapps to help bug detection.
in addition the automated graphical userinterface gui testing techniques can be customized and the testingcontents can be prioritized.
current automated gui testing toolstend to dynamically explore di erent pages of a mobile app throughrandom actions e.g.
clicking scrolling etc to trigger the crash orexplicit exceptions .
if one could know the detailed problematicfeatures of other similar apps in advance the explored pages canbe re ranked so that the bug prone features can be explored earlierto facilitate the bugs being revealed earlier.
we will further exploreproblematic features based app testing in our future work.a user survey.in order to assess the usefulness of sira weconduct a user survey on three popular apps weibo qqandali pay.
we invite respondents from each company in total including product managers requirement analysts and devel opers who are familiar with the app reviews of their own company.2436where is your app frustrating users?icse may pi t tsburgh pa usa figure feedback of user study.more speci!cally we conduct sira on the reviews obtained in the!rst week of may which contains reviews fromweibo fromqq and fromalipayafter preprocessing.
each re spondent examines the extracted problematic features clusters andvisualization results obtained by sira and answer the followingthree questions usefulness can sira help understand userrequirements from app reviews?
extraction can sira extractedproblematic features accurately?
clustering can sira clus ter problematic features accurately?
we provide!ve options foreach question from strongly disagree to strongly agree .
the!rst question concerns the usefulness of sira i.e.
whether siracan save e ort for analyzing large scale app reviews.
the last twoquestions concern the performance of sira on problematic featureextraction and clustering respectively when analyzing app reviewsin real world practice.figure shows the box plot statistics of respondents feedback.there are respectively and out of respondents givethe score over for q1 q2 and q3.
most of them over aresatis!ed score over with the usefulness of sira and think siracan help them obtain a!ne grained understanding on problematicfeatures.
the average score of q1 q2 and q3 are .
.
and .93respectively.
besides three of them heard about or tried existingreview analysis tools such as infar and sur miner andthey admit the advantages of sira as its extracted features andderived clusters are!ner grained and more meaningful.
we alsointerviewed the respondents about the possible enhancement ofsira.
they said there were still some cases where sira doesn twork well such as some extracted phrases contain two or morefeatures which leads to poor performance of clustering.
this canbe solved in future work by exploring the patterns of such tangledfeatures and deconstructing them into separate ones.
in addition wereceived some suggestions from developers for better visualizations e.g.
supporting interactive visual analytics .
discussionadvantage over topic discovery approaches.there are sev eral previous approaches which involve topic discovery .
yet their discovered topics are more coarse grainedthan our proposed approach.
for example based on mobile appslikefacebookandtwitterfrom google play mark can onlydiscover such topics ascrash compatibility andconnection andpuma generates topics likebattery consumption.similarly sur miner generates topics such aspredictions auto correct andwords.
surf can discover topics such asgui app andcompany while infar can generate topics likeupdate radar download.
with these discovered topics the developers can acquirea general view about the problems the app undergoes yet could notget a clear understanding about where it is wrong.
by comparison as demonstrated in figure and table our proposed approachcan generate more!ner grained topics asopen message get crashback which helps developers achieve a deeper and more accurateunderstanding about where the app is wrong.threats to validity.theexternal threatsconcern the gener ality of the proposed approach.
we train and evaluate sira on thedataset consisting of six apps from three categories.
the selectedapps and their belonging categories are all the commonly usedones with rich reviews in practice which relatively reduces thisthreat.
in addition we demonstrate the usage of sira on a muchbigger dataset derived from apps.
the results are promising which veri!es its generality further.
regardinginternal threats sira is a pipeline method where the problematic feature clusteringdepends on the accuracy of extracting problematic features.
sincewe have seen a relatively high performance of sira on problematicfeature extraction we believe sira can alleviate the error accu mulation to some extent.
in addition we reuse the source codefrom the original paper i.e.
forcasparandkefe or the opensource implementation i.e.
forsafe k means andlda for thebaselines which help ensure the accuracy of the experiments.
theconstruct validityof this study mainly questions the evaluationmetrics.
we utilize precision recall and f1 score to evaluate theperformance of problematic feature extraction.
we consider that aproblematic feature is correctly extracted when it is the same as theground truth which is a rather strict measure.
the metrics used toevaluate clustering results are also commonly used .
conclusionto help acquire a concrete understanding about where the app isfrustrating the users this paper proposes a semantic aware !ne grained app review analysis approach sira which can extract cluster and visualize the problematic features of app reviews.
siradesigns a novel bert attr crf model to extract!ne grained prob lematic features and employs a graph based clustering method tocluster them.
we evaluate sira on reviews from six apps and the results con!rm the e ectiveness of the proposed approach.we further conduct an empirical study on reviews from18 popular apps to explore its potential application and usefulnessin real world practice.
our source code and experimental data arepublicly available at work is supported by the national key research and de velopment program of china under grant no.2018yfb1403400 the national natural science foundation of china under grantno.
the youth innovation promotion association chineseacademy of sciences and australian research council discoveryproject dp220103044.2437icse may pi t tsburgh pa usawang et al.