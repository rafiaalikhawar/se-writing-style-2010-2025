dear a novel deep learning based approach for automated program repair yi li new jersey inst.
of technology new jersey usa yl622 njit.edushaohua wang new jersey inst.
of technology new jersey usa davidsw njit.edutien n. nguyen university of texas at dallas texas usa tien.n.nguyen utdallas.edu abstract theexistingdeeplearning dl basedautomatedprogramrepair apr models are limited in fixing general software defects.
we present dear a dl based approach that supports fixing for the generalbugsthatrequiredependentchangesatoncetooneormultiple consecutive statements in one or multiple hunks of code.
we firstdesignanovelfaultlocalization fl techniqueformulti hunk multi statementfixesthatcombinestraditionalspectrum based sb fl with deep learning and data flow analysis.
it takes the buggy statementsreturnedbythesbflmodel detectsthebuggyhunks tobe fixedatonce and expandsabuggy statement sina hunkto includeother suspiciousstatementsaround s.wedesigna two tier tree based lstm model that incorporates cycle training and uses a divide and conquerstrategy tolearn propercode transformations for fixing multiple statements in the suitable fixing context consisting of surrounding subtrees.
we conducted several experiments to evaluatedearonthreedatasets defects4j 395bugs bigfix 26k bugs and cpatminer 44k bugs .
on defects4j dataset dear outperforms the baselines from in terms of the number of auto fixed bugs with only the top patches.
on bigfix dataset itfixes31 145morebugsthanexistingdl basedaprmodelswith the top patches.
on cpatminer dataset among fixed bugs thereare169 .
multi hunk multi statementbugs.dearfixes and more bugs including and more multi hunk multistatement bugs than the state of the art dl based apr models.
ccs concepts softwareanditsengineering softwaremaintenancetools.
keywords automated program repair deep learning fault localization acm reference format yi li shaohua wang and tien n. nguyen.
.
dear a novel deep learning based approach for automated program repair.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
.
corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
introduction researchers haveproposed several approachesto helpdevelopers inautomaticallyidentifyingandfixingthedefectsinsoftware.such approaches are referred to as automated program repair apr .
the apr approaches have been leveraging various techniques in the areasofsearch basedsoftwareengineering softwaremining machine learning ml and deep learning dl .
forsearch basedapproaches asearchstrategyisperformedinthespaceofpotentialsolutionsproducedbymutatingthe buggycodeviaoperators.otherapproachesusesoftwareminingtomineandlearnfixingpatterns frompriorbugfixes orsimilarcode .fixingpatternsareatthesourcecodelevel oratthechangelevel .machinelearning hasbeen used to mine fixing patterns and the candidate fixes are ranked accordingtotheirlikelihoods .whilesomedl basedapr approaches learn similar fixes other ones use machine translation or neural network models with various code abstractions to generate patches .
despitetheirsuccesses thestate of the artdl basedaprapproaches are still limited in fixing the general defects which involve the fixing changes to multiple statements in the same or different partsofafileordifferentfiles whicharereferredtoas hunks .none ofexistingdl basedapproachescanautomaticallyfixthebug s withdependentchangestomultiplestatementsinmultiplehunks at once.
they supports fixing only individual statements.
if we use such a tool on the current statement the tool treats that statement asincorrectandtreatstheotherstatementsascorrect.thisdoes notholdsincetofixthecurrentstatement theremainingunfixed statementsmustnotbetreatedascorrectcode.thus itmightbeinaccurate when using existing dl based apr tools to fix individual statements for multi hunk multi statement bugs.
while dl providesbenefitsforfixlearning thislimitationmakesthedl based aprapproacheslesscapablethantheotherdirections search based and pattern based apr which support multiple statement fixes.
in thispaper weaim toadvance deep learning basedapr by introducing dear a dl based model that supports fixing for the generalbugswithdependentchangesatoncetooneormultiplebuggy statementsbelongingtooneormultiplebuggyhunksofcode.t odo that we make the following key technical contributions.
first wedevelopa faultlocalization fl techniqueformulti hunk multi statementbugsthatcombinestraditionalspectrum basedfl sbfl withdlanddata flowanalysis.dearusesasbflmethodto identifytherankedlistofsuspiciousbuggystatements.then ituses that list of buggy statements to derive the buggy hunks that need to be fixed together by fine tuning the pre trained bert model to learn the fixing together relationships among statements.
we alsodesignanexpansionalgorithmthattakesabuggystatement ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yi li shaohua wang and tien n. nguyen sin a hunk as a seed and expands to include other suspicious consecutive statements around s. to achieve that we use an rnn modeltoclassifythestatementsasbuggyornot andusedata flow analysis for adjustment and then form the buggy hunks.
second aftertheexpansionstep wehaveidentifiedallthebuggy hunk s withbuggystatement s .wedevelopa compositionalapproach to learning and then generating multi hunk multi statement fixes.
in our approach from the buggy statements we use a divideand conquer strategy to learn each subtree transformation in abstract syntax tree ast .
specifically we use an ast based differencing technique to derive the fine grained ast based changes and the mappings between buggy and fixed code in the training data.thosefine grainedsubtreemappingshelpourmodelavoid incorrectalignmentsofbuggyandfixedcode thus ismoreaccurate in learning multiple ast subtree transformations of a fix.
third we have enhanced and orchestrated a tree based twolayer long short term memory lstm model withan attention layer and a cycle training to help dear to learn the proper code fixing changes in the suitable context of surrounding code.
for each buggy ast subtree identified by our fault localization we encode it as a vector representation and apply that lstm model to derive thefixed code.in thefirst layer it learnsthe fixingcontext i.e.
thecodestructuressurroundingabuggyastsubtree.inthesecondlayer itlearnsthecodetransformationstofixthatbuggy subtree using the context as an additional weight.
finally there mightbe likely multiple buggysubtrees.
to build thesurroundingcontextforeachbuggysubtree b intraining we includetheastsubtrees afterthefixesoftheotherbuggysubtrees ratherthanthosebuggysubtreesthemselves .therationaleisthat the subtrees after fixes actually represent the correct surrounding code forb.
note in training the fixed subtrees are known .
we conducted experiments to evaluate dear on three large datasets defects4j 395bugs bigfix 26kbugs and cpatminer dataset 44k bugs .
the baseline dl based approaches include dlfix coconut sequencer tufano19 codit andcure .dearfixes31 i.e.
.
i.e.
and .
i.e.
more bugs than the best performing baseline cure on all three datasets respectively using only top patches and with seven times fewer training parameters on average.
on defects4j it outperforms those baselines from in terms of thenumber offixed bugs.on bigfix it fixes31 morebugs than those baselines with the top patches.
on cpatminer among fixed bugs from dear there are .
multi hunk multistatementones.dearfixes71 and41morebugs including and40moremulti hunk multi statementbugs thanexisting dl based apr tools coconut dlfix and cure.
we also compareddearagainst8state of the artpattern basedaprtools.ourresults show that dear generates comparable and complementary results to the top pattern based apr tools.
on defects4j dearfixes bugs out of including multi hunk multi statement bugs that the top pattern based apr tool could not fix.
in brief the key contributions of this paper include a. advancing dl based apr for general bugs with multihunk multi statement fixes dear advances dl based apr for general bugs.
we show that dl based apr can achieve the comparable and complementary results as other apr directions.
b. advanced dl based apr techniques 1public boolean verifyuserinfo string uid string password string ssn string retrieved password string retrieved ssn 4if uid !
null retrieved password getpassword uid retrieved password getpassword touppercase uid else return false boolean password check compare password retrieved password boolean password check compare passwordhash password retrieved password 12if password check retrieved ssn getssn uid boolean ssn check compare ssn retrieved ssn if ssn check return true 19return false figure a general fix with multiple dependent changes anovel fl technique for multi hunk multi statement fixes that combines spectrum based fl with dl and data flow analysis acompositionalapproach withadivide and conquerstrategy to learn and generate multi hunk multi statement fixes and thedesignandorchestrationofthetwo layerlstmmodel with the enhancementsvia the attention layer and cycle training.
c.extensiveempiricalevaluation dearoutperformsthe existing dl based apr tools dear is the first dl based apr modelperformingatthesamelevelintermsofthenumberoffixed bugs as the state of the art pattern based tools and generate complementary results our data and tool are publicly available .
motivation .
motivating example letuspresentabug fixingexampleandourobservationsformotivation.figure1showsanexampleofabugin verifyuserinfo which verifies the given user id password and social security number against users recordsin thedatabase.
thisbug manifestsin three folds.first thedeveloperforgottohandlethecasewhen uidisnull.
thus forfixing s headdedan elsebranchatthelines7 .second thedeveloperforgottoperformtheuppercaseconversionforthe uid causinganerrorbecausetherecordsforuseridsinthedatabase all have capital letters.
the corresponding bug fixing change is the additionofthecallto touppercase onuidatline6.third because the passwords stored in the database are encoded via hashing the inputpasswordfromauserneedstobehashedbeforeitiscompared against the one in the database.
thus the developer added the call topasswordhash onpasswordbeforecallingthemethod compare at line .
from this example we have the following observations observation1 afixwithdependentchangestomultiple statements this bug requires the dependent fixing changes to multiplestatementsatonceinthesamefix addingthe elsebranch with the returnstatement lines adding touppercase at line and adding passwordhash at line .
making changes to the individualstatementsoneatatimewouldnotfixthebugsinceboth thegivenarguments uidandpasswordneedtobeproperlyprocessed.
uidneeds to be null checked and capitalized and passwordneedsto be hashed.those dependent changesto multiple statementsmust occur at once in the same fix for the program to pass the test cases.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
dear a novel deep learning based approach for automated program repair icse may pittsburgh pa usa the state of the art dl based apr approaches fix one individual statement at a time.
in figure the fault localization toolreturnstwobuggylines line5andline10.assumethatsucha dl basedaprtoolisusedtofixthestatementatline5.itwillmake the fixing change to the statement at line e.g.
modify line and add lines however with the assumption that the statement at line and other lines are correct.
with this incorrect assumption such a fix will not make the code pass the test cases since bothchanges must be made.
thus the individual statement dl based apr tools cannot fix this bug by fixing one buggy statement at a time.
in general a bug might require dependent changes to multiple statements in possibly multiple hunks in the same fix.
moreover thepattern basedaprtoolsmightnotbeabletofix this defect because the code in this example is project specific and might not match with any bug fixing patterns.
observation2 afixcaninvolvethechangesto multiplesubtrees.forexample the ifstatement has a new elsebranch.
the argument of thecallto getpassword wasmodifiedintothecallto touppercase .
thisfixalsoinvolves many to manysubtreetransformations.inthis example a fix transforms the two buggy statements line andline into four statements the ifstatement having new else branch the returnstatement at line the modified statement with touppercase atline6 andthemodifiedstatementwith passwordhash at line .thus a fixcan bebroken intomultiple subtreetransformations andifusinga compositionapproachwithadivide and conquer strategy we can learn the individual transformations.
observation3 abugfixoftendepends on the context of surrounding code.
for example to get the password from a given uid one needs to capitalize the id thus in correctcode themethodcallto touppercase islikelytoappearwhen themethodcallto getpassword ismade.therefore buildingcorrect fixingcontextisimportant.infigure1 amodelneedstolearnthefix line5 line6 w.r.t.surroundingcode whichneedstoincludethe fixed code at line rather line because line is buggy .
to fix line5 thecorrectcontextmustinclude passwordhash atline11.thus the correct context for a fix to a buggy statement must include the fixed code of another buggy statement s rather than s itself.
.
key ideas from the observations we draw the following key ideas key idea .afaultlocalizationmethodformulti hunk multistatement patches from observation we design a novel fl method that combines traditional spectrum based fl sbfl with dl anddata flowanalysis.weuseasbfltoobtainarankedlistofcandidate statements to be fixed with their suspiciousness scores.
we extend the result from sbfl in two tasks.
first we design a hunkdetection algorithm to use dl to detect the hunks that need to be dependently changed together in the same patch because sbfl tool returns the suspicious candidatesfor the fault but not necessarily tobe fixedtogether.
second wedesign an expansion algorithm that takes each of those detected fixing together hunks and expandsit toinclude consecutivesuspicious statementsin thehunk.
infigure1 thesbfltoolreturnsline5assuspicious.afterhunkdetection dearusesdatadependenciesviavariable retrieve password to include the statement at line as to be fixed as well.key idea .acompositionalapproachtolearningandgenerating multi hunk multi statementfixes divide and conquerstrategy in learning multi hunk multi stmt fixes.to auto fix a bug with multiple statements a tool needs to makem to nstatement changes i.e.
mstatements might generally becomenstatements after the fix.
a naive approach would let a model learn the code structure changes and make the alignmentbetween the code before and after the fix.
because a fix involves multiple subtree transformations observation during training a model might incorrectlyalign the code before and after the fix thus leading to incorrect learning of the fix.
for example withoutthisstep themodelmightmap retrieved password atline10to the same variable at line the correct map is line .
thus to facilitatelearningbug fixingcodetransformations duringtraining we usea divide and conquer strategy.
we integrate into dear a fine grainedast basedchangedetectionmodeltomaptheasts beforeand afterthefix.
suchmappingsenable dear tolearn the more local fixing changes to subtrees.
for example the fine grained astchangedetectioncanderivethatthestatementsatlines4 and become the statements at lines and and the statement at line becomes the one at line .
we can break them into twogroups and align the respective ast subtrees for dear to learn.
compositional approach in fixing multiple subtrees .wesupport the fixes having multiple statements in one or multiple hunks byenhancingthedesignandorchestrationofatree basedlstm model t oadd an attention layer and cycle training section .
.
whilethatmodelfixesonesubtreeatatime weneedtoenhanceit to fix multiple ast subtrees at once.
specifically wemodifyitsoperationsinthetwo layerstoconsidermultiplebuggysubtreesatonce.forexample duringtraining wemarkeachoftheastsubtreesofthestatementsatline5and line10beforethefixasbuggy.atthefirstlayer foreachsubtreefor a buggy statement we replaceit with a pseudo node and consider thenewastwithits pseudo nodesasthefixingcontextforthe buggy statement.
the pseudo node is computed via an embedding technique to capture the structure of the buggy statement section3.
.atthesecondlayer dearlearnsthetransformationfrom the subtree for the statement at line into the subtree for the fixed statementsatthelines6 .thevectorforthefixingcontextlearned from the first layer is used as a weight in the code transformation learning in the second layer.
we repeat the same process for every buggy statement.
for fixing we perform the composition of the fixing transformationsfor all buggy statements at once.
key idea .transformationlearningwithcorrectsurrounding fixing context to learn the correct context for a fix to a statement we need to train the model withthe fixed versions of the otherbuggystatements observation3 .forexample fortraining to learn thefix to the statement atline with touppercase am od e l needstointegratethefixedversionoftheotherbuggyline i.e.
the codeatline11with passwordhash asthefixingcontext insteadofthe buggyline10 .ifthesurroundingcodebeforethefixisused i.e.
line the model will learn the incorrect context to fix the line .
.
approach overview .
.1trainingprocess.
theinputfortrainingincludesthesource code before and after a fix figure which is parsed into asts.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yi li shaohua wang and tien n. nguyen figure training process overview theoutputincludesthetwo trainedmodelsforcontextlearning and fortreetransformationlearning fixing .thecontextlearningmodel ctl aims to learn the weights representing the impact of the context tomakeanadjustmenttothetreetransformationlearning result.
the tree transformation learning model ttl aims to learn code transformation for the fix to a buggy ast subtree.
contextlearning sections3.
.
.thefirststepistobuildthe before after fixing contexts for training.
with divide and conquer strategy weusecpatminer toderivethe changed inserted and removedsubtrees key idea .
as a result the ast subtrees for the buggy statements are mapped to the respective fixed subtrees.
for eachbuggysubtreeandrespectivefixedsubtree webuildtwoasts oftheentiremethodascontexts onebeforeandoneafterthefix and use both of them for training at the input layer and the output layer of the tree based lstm context learning model section .
.
to build the correct context for each buggy subtree we leverage key idea we train our model with the fixed versions of the other buggysubtrees.finally thevectorscomputedfromthislearning are used as the weights in tree transformation learning.
treetransformationlearning section3.
.wefirstusecpatminer toderivethesubtreemappings.tolearnbug fixingtree transformations eachbuggysubtree titselfanditsfixedsubtree t primeafter the fix are used at the input layer and the output layer ofthe secondtree basedlstm fortraining.
moreover theweight representing the context computed as the vector in the context learning model is used an additional input in this step.
.
.2fixing process.
figure illustrates the fixing process.
the input includes the buggy source code and the set of test cases.
fault localization and buggy hunk detection section .
.
fromkeyidea1 wefirstuseasbfltooltolocatebuggystatements withsuspiciousnessscores.hunkdetectionalgorithmusesthose statementstoderivethebuggyhunksthatneedtobefixedtogether.
multi statementexpansion section4.
.becausesbflmight returnonestatementforahunk weaimtoexpandtopotentially include more consecutive buggy statements.
to do so we combine rnn and data flow analysis to detect more buggy statements.
tree based code repair section .
.
for the detected buggy statementsfrommulti statementexpansion weusekeyidea2toderivefixestomultiplebuggysubtreesatonce.forabuggysubtree t we build the ast of the method as the context and use it as the inputofthetrainedcontextlearningmodel ctl toproducethe weightrepresentingtheimpactofthecontext.thebuggysubtree t is used as the input of the trained tree transformation model ttl to produce the context free fixed subtree t prime.
finally that weight is used to adjust t primeinto the fixed subtree t prime primefor a candidate patch.
weapplygrammaticalrulesandprogramanalysisonthecurrent candidate code to produce the fixed code.
we re rank and validate thefixedcodeusingtestcasesinthesamemannerasindlfix .
figure fixing process overview training process .
pairing buggy and fixed subtrees thetrainingdatacontainsthepairsofthesourcecodeofthemethodsbeforeandafterthefixes.notethatafixmightinvolvemultiple methods.insteadofpairingtheentirebuggymethodwiththefixed one we use a divide and conquer strategy to help the model to betterlearnthefixingtransformationsinthepropercontexts.first we use the cpatminer tool to derive the fixing changes.
if a subtree corresponds to a statement we call it statement subtree.fromtheresultofcpatminer weusethefollowingrules to pair the buggy subtrees with the corresponding fixed subtrees .a buggysubtree s subtree is asubtreewith updateordelete.
.
if as subtree is deleted we pair it with an empty tree.
.ifabuggy s subtreeismarkedas update i.e itis updatedor itschildrennode s couldbe inserted deleted orupdated wepaired this buggy s subtree with its corresponding fixed s subtree.
.
if as subtree is insertedand its parent node is another ssubtree we pair it with that parent s subtree.
if the parent node is not an s subtree we pair an empty tree to the corresponding inserteds subtree.
.
context building figure4illustratesourcontextbuildingprocess.foreachpairof thebuggyast i1andfixedast o1 section3.
weperformalpharenaming on the variables.
in step we encode each ast node withthevectorusingthewordembeddingmodelglove which captures well code structure by considering a statement node as a sentenceandeachcodetokenasaword.weusethosevectorsto labeltheastnodesin i1ando1.theastsafterthissteparethe vectorized asts i2ando2 before and after the fix.
instep2 weprocesseachpairofthebuggy s subtreetbini2and the corresponding fixed s subtreetfino2.
first we perform node summarizationon tbandtfby using treecaps to capture the treestructuresof tbandtfintovsandv primes respectively.second for eachoftheotherbuggy s subtrees e.g.
t prime b andtheircorresponding fixeds subtrees e.g.
t prime f weprocessasfollows.because t prime fisthe fixed version of t prime b we replace t prime bwitht prime fin the building of the resultingcontext i3beforefixing keyidea3 .thatis wereplace eachoftheotherbuggysubtreeswithitsfixedversion.however tobuildtheresultingcontext o3afterfixing wekeep t prime fbecauseit is the fixed subtree thus providing the correct context.
the resulting ast i3 is used as the before the fix context for thebuggy s subtreetbandusedat theinputlayer oftheencoder inthecontextlearningmodel ctl .theresultingast o3 isused as the after the fix context for tfand used at the output layer of thedecoderinctl figure4 .finally thevectors vsandv primeswillbe used as the weighting inputs for tree transformation learning later.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
dear a novel deep learning based approach for automated program repair icse may pittsburgh pa usa figure context building to train context learning model figure5 cycletraininginattention basedtree basedlstm .3context learning via tree based lstm with attention layer and cycle training for context learning and tree transformation learning we enhance thetwo layer tree based lstmmodelsindlfix withattention layerandcycletraining.wehaveaddedanattentionlayerintothat model whichnowhas3layers encoderlayer decoderlayerand attentionlayer figure5 .fortheencoderanddecoder tolearnthe fixing context expressed in asts we use child sum tree based lstm .
unlike the regular lstm that loops for each time step this model loops for each subtree to capture structures.
wealsouse cycletraining forfurtherimprovement.cycle trainingaimstohelpamodellearnbetterthemappingbetweenthe inputandoutputbycontinuingtotrainandre traintoemphasizeonthemappingbetweenthem.thisishelpfulinthesituationsinwhichabuggycodecanbefixedinmultiplewaysintodifferentfixedcode ormultiplebuggycodecanbefixedintoonefixedcode.thismakes the regular tree based lstm less accurate.
with cycle training the pairofaninputandthemostlikelyoutputisemphasizedtoreduce the noise of such one to many or many to one relations.
cycle training occurs between encoder and decoder.
we use the forward mapping m a bto denote the process of encoder attention decoder and the backward mapping n b ato denote the process of decoder attention encoder figure .
we apply the adversarial losses for both mandnto get the two loss functions lrun m db a b andlrun n da b a .
the differencebetween n m a anda andthatbetween m n b andb are usedto generate cycle consistencyloss lcyc m n formand nto ensure the learned mapping functions are cycle consistent.
mathematically we have two loss functions lrun m db a b and lrun n da b a .withtheincentivecycleconsistencyloss lcyc m n the overall loss function is computed as follows lcyc m n eb pdata b ea pdata a figure tree transformation learning vs v prime sin figure l m n da db lrun m db a b lrun n da b a lcyc m n wherel m n da db is the loss function for the entire cycle training mandnare the mapping functions to map atoband btoa dais aimed to distinguish between the predicted result n m a andtherealresult a dbisaimedtodistinguishbetween thepredictedresult m n b andtherealresult b lrunisthecycle consistency loss function for the running function m n lcycis theincentivizedcycleconsistencyloss and istheparameterto control the relative importance of the two objectives.
.
tree transformation learning figure illustrates the tree transformation learning process.
we usethesametree basedlstmmodelwithattentionlayerandcycle training as in section .
to learn the code transformation for each buggys subtreetb.instep1 webuildtheword embeddingsfor allthecodetokensasinsection3.
.eachastnodeinthebuggy s subtree tb andthe fixed one tf is labeled with its vectorrepresentation figure6 .next weusethesummarizedvectors vsand v primescomputed from context learning in figure as the weights and perform cross product for each vector of the node in the buggy s subtreetbandforeachoneinthefixed s subtreetf respectively.
thetworesultingsubtreesaftercross productareusedattheinput andoutputlayersofthetree basedlstmmodelfortreetransformationlearning.weusecross productbecauseweaimtohavea vector as the label for a node and use it as a weight representing the context to learn code transformations for bug fixes.
fixing process .
fixing together hunk detection algorithm the first step of fixing multi hunk multi statement bugs is for our fl method to detect buggy hunk s that are fixed together in the same patch.
to do that we fine tune google s pre trained bert model tolearnthe fixing togetherrelationshipsamongstatements using bert s sentence pair classification task.
then we use the fine tunedbertmodelinanalgorithmtodetectfixing together hunks.
let us explain our hunk detection algorithm in details.
.
.1fine tuningberttolearnfixing togetherrelationshipsamongstatements.
wefirstfine tuneberttolearniftwo statements are needed to be fixed together or not.
let hbe a set of the hunks that are fixed together for a bug.
the input for the trainingprocessisallthesets hsforallthebugsinthetrainingset.
step .for a pair of hunks hiandhjinh we take every pair of statements skandsl one from each hunk and build the vectors authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yi li shaohua wang and tien n. nguyen with bert.
we consider the pair of statements sk sl as being fixed together in the same patch to fine tune bert.
step .step1isrepeatedforallthepairsofthestatements sk sl s inallthepairs hiandhjinh.wealsorepeatstep1forall hs.we use them to fine tune the bert model to learn the fixing together relationships among any two statements in all pairs of hunks.
.
.2using fine tuned bert for hunk detection.
after obtaining the fine tuned bert we use it in determining whether the hunks of code need to be fixed together or not.
the input of of this procedure is the fine tuned bert model buggy code p and test cases.
the output is the groups of hunks that need to be fixed together.
the process is conducted in the following steps.
step .we use a spectrum based fl tool in our experiment we usedochiai torunonthegivensourcecodeandtestcases.it returns the list of buggy statements and suspiciousness scores.step .theconsecutivestatementswithinamethodreturnedby the fl tool are grouped together to form the hunks h1 h2 ... hm.
step .to decide if a pair of hunks hi hj n e e d st ob efi x e dt o gether we use the bert model that was fine tuned.
specifically for every pair of statements sk sl one from each hunk hi hj we use the fine tuned bert to measure the fixing together relationshipscorefor sk sl .thefixing togetherscorebetween hiand hjis the average of the scores of all the pairs of statements within hiandhj respectively.iftheaveragescoreforallthestatement pairsishigherthanathreshold weconsider hi hj asneededto befixedtogether.fromthepairsofthedetectedhunks webuild thegroupsofthefixing togetherhunks.thegroupofhunksthathasanystatementwiththehighestochiai ssuspiciousnessscore will be ranked and fixed first.
the rationale is that such a group contains the most suspicious statement thus should be fixed first.
.
multiple statement expansion algorithm a detected buggy hunk from the algorithm in section .
might contain only one statement since each of those suspicious state ments is originally derived by a sbfl tool which does not focus on detecting consecutive buggy statements in a hunk.
thus in this step we take the result from the hunk detection algorithm and expand it to include potentially more statements in a hunk.
key idea.ourideaistocombinedeeplearningwithdataflowanalysis.
we first train an rnn model with gru cells will be explainedinsection4.
.
tolearntodecidewhetherastatementis buggy or not.
we collect the training data for that model from the realbuggystatements.wethenusedata flowanalysistoadjustthe result.specifically ifastatementislabeledasbuggybythernn model no adjustment is needed.
however even when the rnn model decides a given statement sasnon buggy and if shas a data dependency with a buggy statement we still mark sasbuggy.
.
.1expansionalgorithm.
theinputofmulti statementexpansionalgorithmisthebuggystatement buggys i.e.
theseedstatement of a hunk.
the output is a buggy hunk of consecutive statements.
first it produces a candidate list of buggy statements by includingnstatements before and nstatements after buggys expand2ncandidateslist atline2 .inthecurrentimplementation n .then algorithm multiple statementexpansion algorithm function multistatementsexpansion buggys candstmts expand2ncandidateslist buggys predresult rnnclassifier candstmts expandresult datadepanalysis candstmts predresult returnexpandresult function datadepanalysis buggys candstmts predresult buggyhunk getcenterbuggyhunk predresult ddexpandhunk buggys buggyhunk tophalf candstmts ddexpandhunk buggys buggyhunk bothalf candstmts returnbuggyblock function ddexpandhunk buggys buggyhunk candstmts for each stmt candstmts stmt buggyhunk do ifhasdatadep stmt buggys then buggyhunk buggyhunk stmt else break returnbuggyhunk figure multiple statement expansion example it uses the rnn model to act as a classifier to predict whether each statement except buggys inthe candidatelist is buggyor not rnnclassifier atline3 .totrainthatrnnmodel weusethebuggy statementsinthebuggyhunksinthetrainingdata seesection4.
.
.
treecaps is used to encode the statements.
indatadepanalysis line4 toadjusttheresultsfromthernn model we obtain buggyhunk surrounding the buggy statement buggys consistingofthestatementsbeforeandafter buggys thatwere predicted as buggy by the rnn model line .
we then examine statement by statement in the upward direction from the center buggystatementinthecandidatelist line8 via tophalf andinthe downwarddirection line9 via bothalf .in ddexpandhunk wecontinuetoexpand upwardordownward thecurrentbuggyhunk buggyhunktoincludeastatementthatisdeemedasbuggybythernn model or hasa data dependency withthe center buggy statement buggys lines13 .
we stoptheprocess upwardor downward ifweencounteranon buggystatementwithoutdatadependency withbuggysorweexhaustthelist line15 .finally thebuggyhunk containing consecutive buggy statements is returned.
in figure the sbfl tool returns the buggy statement at line .
allstatementsareencodedintothesetsofvectorsviaglove andclassifiedbythernnmodel.weexpandfromthestatementat line upward to include line even though the rnn model predicteditasnon buggy sinceline3hasadatadependencywith thebuggystatementatline4viathevariable c.weincludeline5 becausethernnmodelpredictstheline5asbuggy.atthistime we stoptheupwardanddownwarddirectionsbecauseweencounter the non buggy statements at lines and that do not have data dependencywithline4.thatis lines1 2and6 7areexcluded.the final result includes the statements at lines as the buggy hunk.
.
.2buggystatementpredictionwithrnn.
wepresenthow to use an gru based rnn model to predict a buggy statement.
training.
to train the rnn model we use the buggy non buggy statementsinallthehunksinthetrainingdataset.weuseglove authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
dear a novel deep learning based approach for automated program repair icse may pittsburgh pa usa to encode each token in a statement so that a statement is represented by a sequence of token vectors.
we use the neural archi tecture of the gru based rnn model to consume the glove vectors of statements associated with the buggy non buggy labels.
the rnn model operates in the time steps.
at the time step k k atthe inputlayer gruconsumes theglove vectorsof the kthstatement sk.
at the output layer ski sl a b e l e da s1i fi ti sa buggy statement and as otherwise.
in addition to the input at the time step k we feed the output of the time step kto the gru.
prediction.
thetrainedgru basedrnnmodelisusedinexpansion algorithmatline3topredictifastatementinthehunkisbuggy or not.
the model takes a statement in the form of glove token vectors.
it takes the vectors of all the statements in a hunk and labels them as buggy or non buggy in multiple time step manner.
.
tree based code repair figure8illustratesthisprocess.afterderivingthebuggyhunk s in the method s dear performs code repairing for all buggy statements in all the hunks at once using the trained lstm models.
the tree based code repair is conducted in the following steps step .identifying buggy s subtrees.
for each hunk we parse the code into ast and identify the buggy s subtrees corresponding to the derived buggy statements.
in figure the s subtrees t1and t2are identified as buggy.
ifa buggy s subtree is partof another largerbuggy s subtree wejustneedtoperformfixingonthelarger s subtree since that fix also fixes the smaller s subtree.
step .embedding and summarization.
we perform word embedding using glove and tree summarization using treecaps on all the buggy subtrees to obtain the contexts.
for example in figure t1andt2are summarized into two vectors v1andv2.
step .predict context.
we use the trained context learning model ctl to run on the context with the ast nodes including also the summarized nodes to predict the context.
in the resulting ast the structure is the same as the ast for the input context except thatthe summarized nodes become the new ones.
for example v1and v2in the context becomes v prime 1andv prime 2after step .
step .addingweights.
theweights v1andv2fromstep2areused in a product withthe vectors in the buggy subtrees t1andt2.
each nodeint1andt2isrepresentedbyamultiplicationvectorbetween the original vector of the node and the weight vector v prime 1orv prime .
step .predict transformations.
we use the trained tree transformation learning model to predict the subtrees t prime 1andt prime 2of the fix.
step .removing weights.
we remove the weight from step to obtainthecandidatefixedsubtreeforabuggyone.forexample weremove v prime 1andv prime 2toobtainthecandidatefixedsubtrees t1fandt2f.
however because we know the cross product and a vector we can get the unlimited number of solutions.
thus to produce a singlesolution t1fandt2f for each node in t prime 1andt prime we assume that v prime 1andv prime 2areverticalwiththenodevector vn1int prime 1andvn2int prime .
then wecangettheunweightednodevector v prime n1int1fasfollows v prime n1 v prime vn1 v prime dotaccv prime figure tree based code repair after having the vector for each node in a fixed s subtree t1f we generated candidate patches based on word embedding.
for each node we calculated the cosine similarity score between its vectorv prime n1and each vector in the vector list for all tokens.
to generate candidate patches we select the token tin the list.
token thas its similarity score scoretfor one node in the fixed s subtree.
by adding all scoretfor all the tokens we have the total score scoresum foracandidate.weselectthetop 5candidatesforeach node to generate the candidates and sort them based on scoresum.
.
post processing anaiveapproachwouldfacecombinatorialexplosioninforming thecandidatefix es becauseforeachnodeinthesub tree wemaintaintop 5candidates.however whenwecombinethecandidates for all the nodes in the fixed sub tree many candidates are not valid for the current method in the project.
therefore when we formacandidatebycombiningallthecandidatesforthenodes we applyasetoffilterstoverifytheprogramsemanticsinthesame manner as in dlfix .
this allows us to eliminate invalid candidates immediately.
specifically we use the alpha renaming filter to change the names back to the normal java code using a dictionary containing all the valid names in the scope the syntax checkingfilter to remove the candidates with syntax errors and the name validation filter to check the validity of the variables methods and classes.
moreover for further improvement we use beam search tomaintainonlythetop rankedcandidatefixes.thus wedonot exhaust all compositions in forming the statements.
this helps maintain a manageable number of candidates.
afterapplyingallthefilters wealsouseddlfix sre ranking schemeonthecandidatepatches.wethenusedtestcasestoconductpatchvalidationonthosecandidates.weverifyeachpatchfromthe toptothebottomuntilacorrectpatchisidentifiedandthepatch validationends.ifallcandidatesforfixingalocationcannotpass all the test cases we select the next location to repeat the process.
empirical evaluation .
research questions to evaluate dear we seek to answer the following questions rq1.
comparative study with deep learning based apr modelsondefects4jbenchmark.
howwelldoesdearperform in comparison with existing dl based apr models on defects4j?
rq2.
comparative study with deep learning based apr modelsonlargebugdatasets.
howwelldoesdearperformin comparisonwith dl based aprmodelson large scalebugdatasets?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yi li shaohua wang and tien n. nguyen rq3.comparativestudywithpattern basedaprapproaches on defects4j.
how well does dear perform in comparison with the state of the art pattern based apr approaches?
rq4.
sensitivity analysis of dear.
how do various factors affect the overall performance of dear in apr?
rq5.
time complexity and model s training parameters.
what is time complexity and the numbers of training parameters?
.
data collection we have conducted our empirical evaluation on three datasets defects4j v1.
.
with bugs with test cases bigfix with 26k bugs in .
million buggy methods cpatminer with 44k bugs from java projects.
all experiments were conducted on a workstation with a core intel cpu and a single gtx titan gpu.
.
experimental methodology .
.1rq1.
comparison with dl based apr on defects4j.
comparative baselines.
wecomparedearwithfivestate of the art dl basedaprmodels dlfix coconut sequencer tufano19 codit andcure .
procedure and settings.
we replicated all dl based aprs except cure whichisunavailable.were implementedcurefollowing thedetailsintheirpaper.wetrainedalldlapproachesonthebugs andfixesincpatminerdatasetandtestedthemonall395bugsin defects4j nooverlapbetweenthetwodatasets .alldlapproaches wereappliedwiththesamefaultlocalizationtool ochiai and patchvalidationwiththetestcasesindefects4j.followingprior experiments we set a hour running time limit for a tool for patch generation and validation.
wetuneddearwiththefollowingkeyhyper parametersusing thebeam search bertforhunkdetection epochsize e size batchsize b size andlearningrate l rate 3e 1e 5e 3e 1e lstm for multi statement expansionandcoderepair e size b size and l rate .
.
.
.
.
glove for representation vectors vector size v size l rate .
.
.
.
b size ande size .
the other default parameters were used.
the best setting for dear is e size b size l rate e forbert e size l rate .
b size 128forlstm vsize l rate .
b size e size for glove.
for other models we tuned with the parameters in their papers e.g.
the vector length of word2vec learning rate and epoch size to find the best parameters for each dataset.
we tuned all approaches with theaforementionedparametersonthesame cpatminer datasetto obtainthebestperformance.onceweobtainedthebestparameters for each model we used them for later experiments.
quantitative analysis.
we report the numbers of bugs that a model can auto fix for the following bug location types type .
one hunk one statement a bug with the fix involving only one hunk with one single statement.
type .
one hunk multi statements a bug with the fix involving only one hunk with multiple statements.
type .
multi hunks one statement a bug with the fix involving multiple hunks each hunk with one fixed statement.type .
multi hunks multi statements a bug with the fix involving multi hunks each hunk has multiple statements.
type .
multi hunks mix statements a bug with the fix involvingmultiplehunks andsomehunkshaveonestatementand other hunks have multiple statements.
evaluation metrics.
we report the number of bugs that can be correctlyfixedandthenumberofplausiblepatches i.e.
passingall test cases but not the actual fixes using the top candidate patches.
.
.2rq2.comparisonwithdl basedapronlargedatasets.
comparative baselines.
wecomparedearwiththesamebaselines as in rq1 on two large datasets bigfix and cpatminer.
procedure and settings.
first weevaluatedalldl basedaprmodels on bigfix and cpatminer.
following dlfix and sequencer we randomlysplitdatainto80 fortraining tuning andtesting.second wehavecross datasetevaluation trainingdl based approachesoncpatminerandtestingonbigfix andviceversa.un likedefects4j bigfixandcpatminerdatasetsdonothavetestcases.
without test cases we cannot use fault localization and patch validation for all dl approaches.
thus we fed the actual bug locations into the dl models including locations on buggy hunks and statements.thedl basedbaselinesdonotdistinguishhunks instead processeachbuggystatementatatime.weusedevelopers actual fixes as the ground truth to evaluate the dl based approaches.
evaluation metrics.
we use the top kmetric defined as the ratio betweenthenumberoftimesthatacorrectpatchisinarankedlist of the top kcandidates over the total number of bugs.
.
.3rq3.comparisonwithpattern basedaprondefects4j.
comparative baselines.
we compare dear with the state of theart pattern basedaprtoolsondefects4j elixir ssfix capgen fixminer avatar hercules simfix andtbar .
we were able to replicate the following pattern basedbaselines elixir ssfix fixminer simfix tbar underthesamecomputingenvironments.wesetthetimelimitto5hoursforthetools.fortheotherbaselines duetounavailablecode we use the results reported in their papers as they were run on the same dataset.
we used the same setting and evaluation metric.
.
.4rq4.sensitivityanalysis.
weevaluatetheimpactsofdifferent factors on dear s performance.
we consider the following hunkdetection hunk multi statementexpansion expansion multi statementtreemodelandcycletraining and data splittingscheme.weusetheleft one outstrategyforeachfactor.
we evaluate the first three factors on defects4j and the last one on cpatminer since we need a larger dataset for various splitting.
.
.5timecomplexityandnumbersofparametersinmodel training.
we measure the training and fixing time for a model and its number of parameters for model training on the datasets.
empirical results .
rq1.
comparison results with dl based apr models on defects4j .
.1withfaultlocalization.
wefirstevaluatetheaprmodels whenusingwiththefaultlocalizationtoolochiai .tables1and2 showthecomparisonresultsamongdearandthebaselinemodels.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
dear a novel deep learning based approach for automated program repair icse may pittsburgh pa usa table rq1.
comparison with dl based apr models on defects4j with fault localization projects chart closure langmathmockito timetotal sequencer codit tufano19 dlfix coconut cure dear x y are the numbers of correct and plausible patches respectively.
table rq1.
detailed comparison with dl based apr models on defects4j with fault localization bug types dlfixcoconut curedear type .
one hunk one stmt type .
one hunk multi stmts type .
multi hunks one stmt type .
multi hunks multi stmts type .
multi hunks mix stmts total as seen in table dear can auto fix the most number of bugs andgeneratethemostnumberofplausiblepatches thatpass alltestcasesondefects4j.particularly dearcanauto fix32 and more bugs than sequencer codit tufano19 dlfix coconut andcure respectively i.e.
and31 relativeimprovements .comparedwiththosetoolsinthatorderondefects4j dearcanauto fix35 and18bugs that thosetools missed respectively.
via theoverlapping analysis betweentheresultof dear andthoseofthe baselinescombined dear can fix unique bugs that they missed.
table2showsthecomparisonbetweendearandthetopdlbased baselines dlfix coconut cure w.r.t.
different bug types.
forsingle hunk bugs types dear fixes bugs including unique single hunk bugs that the other tools missed.
formulti hunk bugs types dear can fix bugs that cannotbefixedbydlfix coconut andcure.existingdl based aprmodelscannotfixthosebugssincethemechanismoffixing one statement at a time does not work on the bugs that require thefixes with dependent changes to multiple statements at once.
thus they do not produce correct patches for those cases.
formulti hunk or multi statement bugs types2 dearfixes of them out of fixed bugs i.e.
.
of total fixed bugs .
.
.2without fault localization.
we also compared dear withothertoolsinthefixingcapabilitieswithouttheimpactofa third party fl tool.
all the tools under comparison were pointed to the correct fixing locations and performed the fixes.
as seen if the fixing locations are known dear s fixing capability isalsohigherthanthosebaselines 53bugsversus44 and48 .
importantly itcanfix20multi hunk multi statementbugs .
of a total of fixed bugs while coconut dlfix and cure can fix only and such bugs.
dearismoregeneralthanexistingdl basedmodelsbecauseit can support dependent fixes with multi hunks or multi statements.
importantly it significantly improves these dl based models andtable rq1.
comparison with dl based apr models on defects4j without fault localization i.e.
correct location bug types dlfixcoconut curedear type .
one hunk one stmt type .
one hunk multi stmts type .
multi hunks one stmt type .
multi hunks multi stmts type .
multi hunks mix stmts total table rq2.
comparison with dl aprs on large datasets cpatminer tested bugs bigfix tested bugs tool dataset top top top top top top sequencer .
.
.
.
.
.
codit .
.
.
.
.
.
tufano19 .
.
.
.
.
.
dlfix .
.
.
.
.
.
coconut .
.
.
.
.
.
cure .
.
.
.
.
.
dear .
.
.
.
.
.
table rq2.
comparison with dl aprs on cross datasets tool datasetcpatminer train bigfix bigfix train cpatminer top top top top top top sequencer .
.
.
.
.
.
codit .
.
.
.
.
.
tufano19 .
.
.
.
.
.
dlfix .
.
.
.
.
.
coconut .
.
.
.
.
.
cure .
.
.
.
.
.
dear .
.
.
.
.
.
raisesthedldirectiontothesamelevelastheotheraprdirections search basedandpattern based whichcanhandlemulti statement bugs.moreover dearisfullydata drivenanddoesnotrequirethe defined fixing patterns as in the pattern based apr models.
.
rq2.
comparison results with dl based apr models on large datasets table4showsthatdearcanfixmorebugsthananydl basedapr baselines on the two large datasets.
using the top patches dear canfix15.
ofthetotal4 415bugsincpatminer.itfixes40 more bugs than the baselines with top patches.
on bigfix it can fix .
of the total bugs with the top patches.
it can fix more bugs than those baselines with the top patches.
table5showsthatdearalsooutperformedthebaselinesinthe cross datasetsettinginwhichwetrainedthemodelsoncpatminer and tested them on bigfix and vice versa.
table shows the detailed comparative results on cpatminer w.r.t.
different bug types.
as seen dear can auto fix more bugs on every type of bug locations on the two large datasets.
among fixedbugs dearhasfixed169multi hunkormulti stmtbugsof types i.e.
.
of the total fixed bugs .
dear fixes more bugs and41more andfixesmorebugsineachbugtype than the baselines coconut dlfix and cure respectively.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yi li shaohua wang and tien n. nguyen table rq2.
detailed analysis.
top result comparison with dl based apr models on cpatminer dataset types bugs coconut cure dlfix dear fixed fixed fixed fixed type .
.
.
.
type .
.
.
.
type .
.
.
.
type .
type .
.
.
.
total .
.
.
.
table rq3.
comparison with pattern based apr models projects chart closure langmathmockito timetotal ssfix capgen fixminer elixir avatar simfix tbar hercules dear x y are the numbers of correct and plausible patches dataset defects4j dearfixes52 and40moremulti hunk multi stmtbugs and and2moreone hunk one stmtbugsthancoconut dlfix andcure.forthemulti statementbugs types2and5 thatthe other tools fixed the fixed statements are independent.
this result showsthatfixingeachindividualstatementatatimedoesnotwork.
.
rq3.
comparison results with pattern based apr models asseenintable7 dearperformsatthesamelevelintermsofthe number of bugs as the top pattern based tools hercules and tbar.
table displays the details of the comparison w.r.t.
different bugtypes.asseen dearfixes7multi mix statementbugs types thatherculesmissed.investigatingfurther wefoundthat hercules is designed to fix replicated bugs i.e.
the hunks must havesimilarstatements.those7bugsarenon replicated i.e.
the buggy hunks have different buggy statements or a buggy hunk has multiplenon similarbuggystatements.fortypes1and3 dear fixes9lessone statementbugsthanherculesduetoitsincorrect fixes.
in total dear fixes 12bugs that hercules misses chart7 time closure lang math .
compared to tbar dear fixes more multi hunk multi stmt bugs.tbarisnotdesignedtofixmulti statementsatonceasdear.
instead itfixesonestatementatatime thus doesnotworkwell when those bugs require dependent fixes to multiple statements.
the3bugs oftype2thattbarcan fixaretheonesthatthefixes to individual statements are independent.
the same reason is applied tosimfix.tbarfixes11morecorrectone hunk one statementbugs.
in brief we raise dear a dl based model to the comparable and complementary level with those pattern based apr models.
.
rq4.
sensitivity analysis .
.
impactoffixing togetherhunkdetection.
asseenintable9 without hunk detection dear can auto fix bugs.
with hunk detection dear can fix more multi hunk bugs types .
ittable8 rq3.detailedcomparisonwithpattern basedaprs bug types simfixtbarhercules dear type .
one hunk one stmt type .
one hunk multi stmts type .
multi hunks one stmt type .
multi hunks multi stmts type .
multi hunks mix stmts total table rq4.
sensitivity analysis on defects4j variant without hunk detwithout expansionwithout attention cycledear type type type type type total fixes two lesstype bugs due tothe incorrect hunk detection.in brief hunkdetectionisusefulsincethemulti hunk multi statement bugs require dependent fixes to multiple hunks at once.
.
.
impact of multi statement expansion.
as seen in table withoutexpansion dear fixes 43bugsindefects4j.withexpansion itfixes 7moremulti stmtbugsintypes2 5whileitfixestwo less type bugs and one less type bug.
the reason of fixing less bugs in these two types is that the multi statement expansion may expandthebuggyhunkincorrectlybyregardingasingle statement bug as a multi statement bug.
even so dear still can fix more bugs showing the usefulness of the multi statement expansion.
to compare the impact of hunk detection and multi statement expansion let us note that the variant of dear without hunkdetection missed all multi hunk bugs types .
the variant without expansion missed all multi statement bugs types .
however let us consider how challenging it is to fix them.
among multi hunk bugs fixed with hunk detection bugs are of type multi hunk one statement in which some approaches e.g.
hercules canhandlebyfixingonestatementatatime.only3 bugs are of types .
in contrast all bugs fixed with expansion aremulti statementbugs types2 whichcannotbefixedby existingdl basedaprapproaches.thus expansioncontributes to handling more challenging bugs than hunk detection.
.
.
impactoftree basedlstmmodelwithattentionandcycle training.
attention cycle tomeasuretheimpactofattentionand cycle training we removed those two mechanisms from dear to produce a baseline.
our results show that in defects4j dear fixes more bugs on all bug types than the baseline .
increase .
this result indicates the usefulness of the two mechanisms.
.
.
impact of training data s size.
table shows that the size of training data has impact on dear s performance.
as seen in table10 themoretrainingdata thehigherthedear saccuracy.
this is expected as dear is a data driven approach.
but even with less training data dear achieves .
for top result whichisstillhigherthandlfix .
intop andsequencer .
in top both are with more training data splitting .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
dear a novel deep learning based approach for automated program repair icse may pittsburgh pa usa table impact of the size of training data splitting scheme on cpatminer dataset total bugs at top .
.
.
1public void excluderoot string path string url tourl path findorcreatecontentroot url .addexcludefolder url url url tourl path findorcreatecontentroot url .addexcludefolder url.geturl 7public void usemoduleoutput stringproduction string test 8modifiablerootmodel.inheritcompileroutputpath false modifiablerootmodel.setcompileroutputpath tourl production modifiablerootmodel.setcompileroutputpathfortests tourl test modifiablerootmodel.setcompileroutputpath tourl production .geturl modifiablerootmodel.setcompileroutputpathfortests tourl test .geturl figure a multi hunk multi statement fix in cpatminer .
rq5.
time complexity and parameters trainingtimeof dearoncpatminerwas 22hoursandpredicting on cpatminer took .
.
seconds for each candidate patch.
training of dear on bigfix took hours and predicting on bigfix took .
.
seconds for each candidate.
predicting on defects4j tookonly2.1secondsforacandidateduetoamuchsmallerdataset.
test execution time was second per test case.
test validation took minutes for all the test cases for a bug fix.
the best baseline cure fixes fewer bugs than dear rq1 and rq2 and requires and .
times more training parameters than dear on cpatminer and bigfix respectively.
specifically dearandcurerequire0.39mand3.1mtrainingparameterson cpatminer and0.42mand3.5mparametersonbigfix.thus dear is less complex than cure while achieving better results.
threats to validity.
we tested on java code.
the key modules in dear are language independent except for the third party fl and post processing with program analysis.
pattern based apr tools require a dataset with test cases thus we compared them on defects4jonly.wetriedourbesttore implementthepattern based apr baselines and cure for a fair comparison.
illustrative example.
figure shows a correct fix from dear.
it correctly detects two buggy hunks each with multiple statements.
dearleveragesthevariablenamesexistinginthesamemethod modifiablerootmodel atline8 incomposingthefixedcodeatlines11 .thedl basedbaselines sequencer andcoconut treat code as sequences and does not derive well the structural changes forthisfix.dlfixfixesonestatementatatime thus doesnotwork the fixes at line and line depend on each other .
for patternbased aprs there is no fixing template for this bug.
limitations.
dear has the following limitations.
first as with ml approaches fixes with rare or out of vocabulary names are challenging.withmoretrainingdata dear hashigherchancetoencounter the ingredients to generate a new name.
second we focus onlyonthebugsthatcausefailingtests.security vulnerabilities and non failing test bugsare still its limitations.third we cannot generate fixes with several new statements added or arbitrarilylarge sizes of dependent fixed statements.
fourth the expansionalgorithmproducesincorrecthunkstobefixed leadingtofixingin correctstatements.finally wecurrentlyfocusonjava however the basic representations used in dear e.g.
token ast dependency are universal to any program language.
only third party fl and post processing with semantic checkers are language dependent.
related work deep learning based apr approaches.
deeprepair learns code similarities to select the repair ingredients from code frag ments similar to the buggy code.
deepfix learns the syntax rules to fix syntax errors.
ratchet tufano et al.
and sequencer mainlyuseneuralnetworkmachinetranslation nmt withattention basedencoder decoderandcodeabstractionstogeneratepatches.codit encodescodestructures learnscodeedits and adopt an nmt model to suggest fixes.
tufano et al.
learn codechangesusingasequence to sequencenmtwithcodeabstrac tionsandkeywordreplacing.dlfix hasatree basedtranslation modeltolearnthefixes.coconut developsacontext aware nmt model.
cure proposes a code aware nmt using gpt model .
the existing dl based apr models fix individual statements at a time and are ineffective for multi hunk multi stmt bugs.
pattern based apr approaches.
those approaches have mined and learned fix patterns from prior fixes either automaticallyorsemi automatically .prophet learns code correctness models from a set of successful human patches.
droix learnscommon root causesforcrashesusing asearchbased repair.
genesis automatically infers patch generation fromusers submittedpatches.hdrepair minesfixpatterns with graphs.
elixir uses templates from par with local variables fields orconstants tobuild fixedexpressions.
capgen simfix fixminer relyonfrequentcodechangesextracted from existing patches.
avatar exploits fix patterns of static analysisviolations.tbar isatemplate basedaprtoolwiththe collectedfixpatterns.angelix catchesprogramsemanticsto fix methods.
arja generates lower granularity patch representation enabling efficient searching.
we did not compare with angelixsincewecomparedwithcapgenthatoutperformsangelix.
wecouldnotreproducearja however arjafixesonly18bugs while dear fixes bugs on the same four projects in defects4j.
conclusion in thiswork wemake three keycontributions a novel fltechnique for multi hunk multi statement fixes combining traditional sbflwithdeeplearninganddata flowanalysis acompositionalapproachtogeneratemulti hunk multi statementfixeswithdivideand conquer strategy and enhancements and orchestration of a two layer lstm model with the attention layer and cycle training.
ondefects4j dearoutperformsthedl basedaprbaselinesfrom in terms of the number of fixed bugs.
on bigfix it fixes 145morebugswiththetop 1patches.oncpatminer itfixes more multi hunk multi stmt bugs than the baselines.