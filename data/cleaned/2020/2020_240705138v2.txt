are llms correctly integrated into software systems?
yuchen shao yuheng huang jiawei shen lei ma ting su chengcheng wan east china normal university shanghai china the university of tokyo tokyo japan university of alberta edmonton ab canada ycshao stu.ecnu.edu.cn yuhenghuang42 g.ecc.u tokyo.ac.jp javishen stu.ecnu.edu.cn ma.lei acm.org tsu sei.ecnu.edu.cn ccwan sei.ecnu.edu.cn abstract large language models llms provide effective solutions in various application scenarios with the support of retrieval augmented generation rag .
however developers face challenges in integrating llm and rag into software systems due to lacking interface specifications various requirements from software context and complicated system management.
in this paper we have conducted a comprehensive study of opensource applications that incorporate llms with rag support and identified defect patterns.
our study reveals that of these applications contain more than three types of integration defects that degrade software functionality efficiency and security.
guided by our study we propose systematic guidelines for resolving these defects in software life cycle.
we also construct an open source defect library h ydrangea .
index terms llm defects empirical software engineering i. i ntroduction a. motivation large language models llms offer effective solutions for a spectrum of language processing tasks.
retrievalaugmented generation rag techniques further enhance their capabilities by providing relevant information from external data sources.
together llm and rag serve as efficient and cost effective proxies of artificial general intelligence agi .
consequently an increasing number of software systems are integrating llms with rag support to realize intelligence features which this paper refers to as llm enabled software .
indeed more than open source llm enabled software projects have been created on github in the past six months to solve a variety of real world problems.
various frameworks offer llm and rag solutions as third party apis significantly reducing developers burden of incorporating them.
however challenges still remain in building correct efficient and reliable llm enabled software.
in fact developers may overlook integration failures due to insufficient testing and the lack of llm and rag knowledge.
thus understanding the defects and their root causes in llm enabled software has become urgent.
challenge lacking interface specifications.
unlike ai tasks with categorical outputs llm performs generation tasks and typically lacks detailed specifications of their interfaces and behaviors.
given a particular input llms cannot chengcheng wan is the corresponding author.
vectordatabase tokenizedtextchunksknowledgequestion semantic vector query answeruser rag vectordbresponsequestionkeywordsllm embedding semantic vector storage softwarecomponentsllm generation prompthistoryllmagentsystem fig.
.
components and workflow of llm enabled software.
specify whether they could provide a correct answer in a certain format.
moreover it is impractical to define the capability boundary of a certain llm especially when enhanced by rag.
therefore llm enabled software cannot formally describe the interface between llm rag and the remaining software components.
thus developers have to tackle the under specified interface and resolve potential failures.
challenge various requirements from software context.
as a generative model an llm enhanced by rag could provide different responses for the same question.
while these responses may all seem feasible not all of them will match the software context and trigger the correct software behavior.
for example a user expects landscape descriptions from a travel agent and statistics from a data analyzer with the question how about ottawa?
.
furthermore conventional software components typically have strict format requirements whereas data driven llm supports various formats.
thus developers have to instruct the general purpose llms to perform specific tasks within the software context.
challenge complicated system management.
the llm and rag algorithms are resource intensive and require system management to ensure performance.
even adopting cloud services to reduce computation costs substantial memory is required for transferring and processing the intermediate results.
additionally llms have vulnerabilities and could become security weak links after obtaining system privileges .
thus developers have to carefully manage resources and protect the security of the entire system.
prior work studies the integration of ai components with categorical outputs .
other work focuses on improving llm and rag algorithms .
however toarxiv .05138v2 feb 2025our best knowledge no prior work provides an empirical study detailing the integration problems of llm enabled software.
b. contribution to understand the integration problems in llm enabled software we conduct the first comprehensive study of the latest versions as of may 22nd of github projects that incorporate llm and rag techniques to tackle real world problems.
we have manually studied over issue reports of these projects and summarized defect patterns.
our study finds that integration defects are widespread with of these applications containing more than types of defects.
these defects lead to various problems including unexpected fail stops incorrect software behaviors slow execution unfriendly user interfaces ui increased token cost and secure vulnerabilities.
as shown in figure these defects are located in major components of llm enabled software the llm agent that constructs prompt and invokes llms the vector database that supports rag algorithms software component that interacts with the llm agent and vector database and system that carries out the execution.
they are all caused by the challenges discussed above.
our research reveals common defect patterns that exist in various applications many of which could be resolved through simple code patches.
based on the study we construct a defect library h ydrangea that contains all identified defects including their explanation types consequences source code locations and defect triggering tests.
we also provide a systematic guideline to identify and resolve these defects in software life cycle.
overall this paper presents the first in depth study of integration defects in llm enabled software offering guidance to prevent integration failures and improve software quality.
we believe this work will contribute to the software engineering of intelligent software and serve as a starting point for tackling this critical problem.
we have open sourced the entire benchmark and defect library at github .
ii.
b ackground a. retrieval augmented generation llms enable a wide range of cognitive features including conversation document comprehension and questionanswering .
to further assist llms in knowledge intensive tasks rag techniques are proposed to provide external knowledge through prompt engineering.
they equip llms with timely trusted and relevant knowledge that is unseen in their training procedure without the need for fine tuning.
therefore llm could be easily extended to various application scenarios and updated with the latest knowledge.
several vector databases are proposed to manage external knowledge and provide rag solutions including mongodb chromadb and faiss .
the rag algorithm operates in two phases as illustrated in the green part of figure .
in the storage phase text is extracted from source files and segmented into multiple chunks forming knowledge entries.
each entry is then embedded into user biologist emily marine life biologistemilyare you fond of marine life?vectordb absolutely i m truly passionate about marine life!as a marine biologist she was dedicated to marine life protection.
llmagent fig.
.
a use case of realchar a character simulator.
a semantic vector a high dimensional float vector representing semantic features using llm s embedding module and various strategies.
these semantic vectors serve as indexes of knowledge entries when stored in the vector database.
in the query phase the rag algorithm embeds the query question using the same embedding module and retrieves relevant knowledge entries based on the distance between semantic vectors.
the retrieved knowledge constructs the llm context simplifying the original task into a comprehension task.
b. llm enabled software several frameworks such as langchain and llamaindex provide unified interfaces for developers to integrate llms and vector databases.
this leads to the emergence of llm enabled software.
figure illustrates a typical workflow and structure of llm enabled software.
while the workflows may vary they generally follow the similar structure.
before deployment avector database is initialized with segmented text from various files a knowledge entry is formed with a text chunk and its semantic vector obtained through embedding.
during execution the software component 1collects and converts user inputs.
it then 2extracts key phrases to construct the query question.
next the vector database 3embeds keywords and 4retrieves relevant knowledge entries.
an llm agent then takes this knowledge and original user input to 5construct a prompt.
it also manages execution history and maintains context to 6generate a high quality response.
finally the software component 7processes the llm response and delivers answers to the user.
they all execute upon the system .
for example a character simulator utilizes a vector database to store character settings figure .
when a user inquires about the character relevant information is retrieved from the vector database.
therefore the llm agent could simulate any character as long as sufficient character setting information is stored.
of course as we will discuss later this application actually contains defects that need to be fixed.
c. integration failure integration failures occur when each component works well individually according to its specifications but failure happens when they work together .
in practice we regard a failure as an integration failure only if it could be alleviated by only changing how components interact with each other e.g.
adjusting data pipelines and changing api invocations without modifying the algorithms e.g.
finetuning llms and patching third party libraries .
software without integration failures is correctly integrated software.
2this paper focuses on integration failures caused by the integration of llm and rag.
while conventional software without ai components may have some similar problems the root causes buggy code patterns impacts and tackling strategies discussed in this paper are all llm unique.
iii.
s tudy methodology a. application selection we collect a suite of open source llm enabled software from github all latest versions as of may 22nd trying our best to obtain an unbiased and diverse benchmark.
we use github search api to obtain around applications that integrate both llms and vector databases with the query llm or ai or vector database or rag .
given the prevalence of toy applications on github we manually check over applications in the default order presented by github i.e.
order by relevance to obtain these nontrivial ones.
we confirm that they each target a concrete realworld problem tightly integrate llms and vector databases in their workflow i.e.not a simple ui wrapper and maintain an active user community.
we further check around more applications from github api but fail to find any with major differences from existing ones.
thus we stop the collection.
our benchmark covers different programming languages including python typescript and others .
among them applications incorporate gpt as their llm module and use llama .
their vector databases include chromadb pinecone faiss and others through local deployment and cloud services .
the sizes of these applications range from to lines of code with a median size of lines.
they have received an average of commits.
half of them have more than stars with the maximum being .
due to the young age of llm and rag techniques around applications are younger than months.
these applications support five major functionalities reflecting the common use cases of llm and vector databases.
as shown in table i applications support context based question answering qa including document comprehension and knowledge search offer task management creating tasks lists and making plans serve as chat robots chatting with users and tracking user specific histories function as central platforms scheduling multiple correlated ai tasks and the remaining perform various text related tasks including automated fact checking and plagiarism detection.
benchmark validation.
to examine the representativeness of our benchmark suite we use the github code search api to gather two extra sets of top llm enabled applications sorted by star ratings and topic relevance.
our benchmark suite has and overlap with the star batch and relevance batch respectively.
all the applications contain at least one term from llm ai vector database and rag .
b. defect pattern identification no prior work studies integration failures in llmenabled software.
therefore we cannot rely on an existingtable i statistics of studied applications functionality of projects avg loc avg stars avg commits context based qa task management chat robot central platform other list of defect patterns.
our team including llm experts collects all issue reports with github api and crawlers from the studied applications and obtains over defect related ones after using keyword search and llms for filtering.
we manually judge whether each issue is caused by software defects rather than user misuse for closed issues we review commit history and examine whether the bug is confirmed and fixed and for open issues we manually design test inputs to reproduce the bug referring to issue reports and opensource benchmarks.
we obtain confirmed defect reports and discover previously unknown defects from them.
the defect analysis is conducted through an iterative process.
in each iteration all authors discuss the identified defects to obtain refine a list of patterns based on their root causes and impacts.
the defects are then independently categorized and cross validated by three co authors.
a fourth co author joins when they encounter consensus problems.
afterwards for each pattern three co authors manually examine all applications to identify previously unknown defects.
this iteration repeats several times until the findings converge taking approximately person months.
note that one defect may appear at multiple source code locations of an application.
c. experiment testbed we use real world data that reflect application scenarios for testing including text voice queries and files of different formats referencing application manuals and issue reports.
we run each test times and report the average latency.
experiments with cloud llm services are conducted on a machine with an apple m3 max cpu 32mb l2 cache 64g ram and 1000mbps network connection.
experiments with local llms are conducted on a machine with eight rtx4090 gpus two intel cpus and 512g ram.
iv.
i dentified integration defects a. overview through empirical study we have identified defects from github applications each appearing at source code locations.
as listed in table ii they are summarized into defect patterns appearing in different types of applications.
they are caused by the challenges faced by developers including unsystematic prompt query construction misunderstanding of interface specification unaware of software context and lacking system management in the order of integration level details in section i .
they harm software quality in various aspects a functionality problems including unexpected failstops incorrect software behaviors and unfriendly ui b 3table ii defects identified in llm enabled software .
defect section locationimpact problematic apps st ic sl ui tk is unsystematic prompt query construction unclear context in prompt iv b1 llm agent imprecise knowledge retrieval iv .c4 vector database misunderstanding of interface specifications missing llm input format validation iv .b4 llm agent incompatible llm output format iv .b5 llm agent unnecessary llm output iv .b6 llm agent exceeding llm context limit iv .b7 llm agent knowledge misalignment iv .c1 vector database conflicting knowledge entries iv .c2 vector database improper text embedding iv .c3 vector database low frequency interactivity iv .d3 software components unaware of software context lacking restrictions in prompt iv .b2 llm agent insufficient history management iv .b3 llm agent absence of final output iv .d1 software components sketchy error handling iv .d2 software components lacking system management privacy violation iv .d4 software components resource contention iv .e1 system inefficient memory management iv .e2 system out of sync llm downstream tasks iv .e3 system total number of benchmark applications with more than three types of defects in the impact column from left to right st refer to fail stops ic refer to incorrectness sl refer to slower execution ui refer to unfriendly user interface tk refer to more tokens and is refer to insecure.
in the problematic apps column the denominator refers to applications that should make efforts to prevent such defect and the numerator refers to applications that actually contain such defect details in the corresponding sections .
efficiency problems including slow execution and increased token cost and c security problems.
as shown in figure llm enabled software contains major components that tightly work together llm agent manages llm interfaces constructs prompts and invokes the llm blue part vector database supports rag algorithm and enhances the llm agent green part software component interacts with the first two components to perform certain tasks yellow part and system manages resources and privileges to carry out the execution.
we organize the defect patterns according to their root causes and include location as a supplementary detail aiming to provide a big picture for readers to understand the integration challenges of llm enabled software.
note that these patterns actually are related to the integration failure between multiple components while a specific component is believed to be responsible for eliminating them.
b. defects located in llm agent the llm agent constructs prompts from various inputs invokes llms and converts their responses to match the requirements of software components.
while llms have outstanding performance on various tasks the misbehavior of llm agents would degrade the overall correctness and efficiency of software systems or even lead to fail stop failures.
in our benchmark allapplications suffer problems caused by the incorrect integration of llm agents.
unclear context in prompt llms suffer hallucination problems especially when prompts lack sufficient information .
due to the nature of generative models llms would produce grammatically coherent contextually relevant but semantically incorrect text outputs e.g.
nonexistent quotes false historical events or even spurious scientific facts.
in llm enabled software the unreliability of llm agents could propagate to the downstream tasks .
therefore the llm agent should construct clear and informative prompts to mitigate hallucinations.
however a large proportion of the benchmark applications failed.
take chatiq a slack chatbot as an example.
it is expected to answer questions based on the chat history and uploaded files.
unfortunately it often provides fictive responses when asked about the local food of certain cities e.g.
claiming that an inland city produces seafood.
in another case after the user uploads an invitation mail of work plan discussion it uses an llm to extract event schedules from it and store them in vector databases for future