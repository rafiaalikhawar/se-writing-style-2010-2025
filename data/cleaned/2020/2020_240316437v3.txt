reasoning runtime behavior of a program with llm how far are we?
junkai chen school of software technology zhejiang university ningbo china junkaichen zju.edu.cn zhenhao li york university toronto canada zhenhao.li ieee.orgzhiyuan pan the state key laboratory of blockchain and data security zhejiang university hangzhou china zypan zju.edu.cn ge li peking university beijing china lige pku.edu.cnxing hu the state key laboratory of blockchain and data security zhejiang university hangzhou china xinghu zju.edu.cn xin xia zhejiang university hangzhou china xin.xia acm.org abstract large language models for code i.e.
code llms have shown strong code understanding and generation capabilities.
to evaluate the capabilities of code llms in various aspects many benchmarks have been proposed e.g.
humaneval and classeval .
code reasoning is one of the most essential abilities of code llms i.e.
predicting code execution behaviors such as program output and execution path but existing benchmarks for code reasoning are not sufficient.
typically they focus on predicting the input and output of a program ignoring the evaluation of the intermediate behavior during program execution as well as the logical consistency e.g.
the model should not give the correct output if the prediction of execution path is wrong when performing the reasoning.
to address these problems in this paper we propose a framework namely reval for evaluating code reasoning abilities and consistency of code llms with program execution.
we utilize existing code benchmarks and adapt them to new benchmarks within our framework.
a large scale empirical study is conducted and most llms show unsatisfactory performance on both runtime behavior reasoning i.e.
an average accuracy of .
and incremental consistency evaluation i.e.
an average ic score of .
.
evaluation results of current code llms reflect the urgent need for the community to strengthen the code reasoning capability of code llms.
our code data and reval leaderboard are available at index terms code reasoning large language model benchmark i. i ntroduction large language models llms attract great attention for their exceptional performance on diverse tasks including sentiment analysis logical reasoning and question answering .
recently large language models for code i.e.
code llms have become a popular research area because of the promising prospect of empowering humans in software development and maintenance .
hence both academia and industry have proposed a lot of code llms e.g.
codellama equal contribution.
corresponding author.family and magicoder series which are widely applied to different tasks like code generation .
to provide a fair and comprehensive measure of the capabilities of code llms many code related benchmarks e.g.
humaneval and codexglue are proposed to evaluate the effectiveness of code llms in different tasks such as code generation and vulnerability detection .
given that executable is a distinct feature of code compared to natural language and code execution provides additional information e.g.
program output to assist with code tasks benchmarking code reasoning abilities of code models with execution raises researchers interests .
here code reasoning is referred to as predicting code execution behaviors e.g.
program outputs execution paths and possible variable values without executing the code directly.
for example gu et al.
proposed cruxeval to evaluate code llms by predicting output from input and vice versa.
typically these works measure the model s ability to predict and analyze the relationship between the input and output of an executable program.
however the intermediate information e.g.
execution path during code execution is ignored posing challenges to developers in comprehending the program s runtime behavior.
fig.
shows common concerns about the runtime behavior during program execution.
intuitively how a program behaves under certain input can help developers better understand the code and perform debugging activities.
for example if we have concerns about the correctness of a certain statement while debugging a snippet of code we typically first determine whether this statement is executed given the input i.e.
in fig.
if it is executed observing the changes in variables before and after execution is a natural choice sometimes this line of code may seem fine so the statement immediately following it will be examined additionally the program output can be used to verify whether the results match the expectations .
therefore we argue that these kinds of runtime behaviors e.g.
program state and execution path arearxiv .16437v3 sep 2024outputinput code deff var ifvar.isdigit return int elifvar.replace .
.isdigit return float elifvar.count len var return str eliflen var return char else return tuple what is the next line to be executed?what is type and value of var?is the statement executed ?
what is the input?
tuple what is the output?
cruxeval reval ic fig.
.
the demonstration of code reasoning tasks in cruxeval and reval.
ic incremental consistency.
essential for program understanding and reasoning for humans.
meanwhile they are also proven to be effective for an in depth understanding of code semantics for language models .
as previous benchmarks like cruxeval i.e.
with and fail to evaluate whether llms can reason about these dynamic characteristics of a program it is necessary to measure the code reasoning ability of llms with runtime behavior of execution.
in this paper we propose our framework reval to comp rehensively re evaluate the code reasoning ability of llms which consists of two evaluation components i runtime behavior reasoning and ii incremental consistency evaluation.
evaluation component runtime behavior reasoning .
to mitigate this limitation in previous research we make the first attempt to systematically evaluate the code llm s ability to reason about the runtime behavior of program execution.
specifically we propose four evaluation tasks to achieve this goal code coverage prediction ccp i.e.
whether a statement is executed or not program state prediction psp i.e.
what is the value and the type of a variable execution path prediction epp i.e.
which is the next statement to be executed and output prediction op i.e.
what is the output.
these four tasks cover various aspects of program execution including control flow data flow and type dependency which are widely applied to prior research in software engineering such as type inference and code translation .
therefore this evaluation provides a more comprehensive measure of code model s ability to reason about executable programs in comparison with previous work.
nevertheless it is noticed that sometimes the reasoning results of a model could conflict with human logic on sequential tasks in runtime behavior reasoning.
for instance the code model may correctly predict the next statement to be executed i.e.
epp when it fails to tell the value of a variable after the statement s execution i.e.
psp which is not expected because the control flow of the execution relies on the program state.
as this kind of inconsistency in sequentially related tasks is unlikely to occur in humans the trustworthiness of ai systems built on these models e.g.
github copilot can easily suffer from these unreliable behaviors.
althoughsome previous works have discussed consistency for code llms they are limited to semantic consistency like back translation between nl and code and ignore the logical consistency mentioned here.
hence it is necessary to measure the consistency of code llms on sequentially related tasks.
evaluation component incremental consistency evaluation .to fill the gap in evaluation we propose a novel metric named incremental consistency ic to measure the extent to which the model can maintain its logical consistency on sequentially related tasks of incremental difficulty.
we observe that the four tasks in runtime behavior reasoning are progressive and consistent with the context of ic i.e.
the knowledge required to finish the current task is the preliminary of the next task.
hence we can judge how much a model is incrementally consistent by utilizing the results of reasoning runtime behavior see section iii for details .
incremental consistency provides new sights for evaluating llms and the consistency measure of ai systems beyond traditional metrics.
to construct our framework we leverage existing executable datasets e.g.
humaneval and classeval as our base benchmarks and adapt them into an adapted benchmark within our framework by extracting runtime behavior constructing and filtering the problems.
we conduct a large scale empirical study on various models including general and code llms in our frameworks.
evaluation results show that our framework presents a degree of difficulty and most llms show poor performance on both runtime behavior reasoning and ic evaluation e.g.
an ic score below for all open source llms we evaluate .
our research highlights the importance of utilizing runtime behavior and incremental consistency evaluation to measure the reasoning ability of code llms and we call for targeted efforts in subsequent research to enhance these weaknesses.
in summary the contributions of our paper are as follows we propose a new framework reval to comprehensively evaluate code llms abilities of code reasoning.
to the best of our knowledge we are the first work to evaluate code models to systematically reason about runtime behavior during program execution.
we propose a novel metric named incremental consistency ic to measure to what extent a code llm can maintain its consistency across sequentially related tasks of incremental difficulty.
we conduct a large scale empirical study on diverse llms within our evaluation framework.
our results reveal the limitations of reasoning runtime behavior and ic of code models.
we construct an adapted benchmark based on humaneval and classeval and develop an evaluation harness for our framework.
to facilitate further research of code reasoning our code data and reval leaderboard are publicly available at ii.
b ackground and related work in this section we discuss the background information of our research and the corresponding related work.a.
code execution and reasoning code execution behavior we refer to code execution behavior as the additional information offered by program execution compared to static analysis.
according to the execution order we classify them into pre post execution information and runtime information pre post execution information is the content we can obtain before or after the actual execution process of program.
for example the input output and nl requirements belong to this category.
runtime information is the intermediate state during code execution.
for instance we are able to collect contents like program state and execution path only when the code is still running.
previous research has leveraged code execution behavior to improve the performance of various downstream tasks e.g.
program understanding code generation software testing and vulnerability detection .
ni et al.
improved code generation performance with an extra verifier which learns the results of code execution and helps rerank generated code candidates.
chen et al.
utilized different kinds of feedback including output to help llms self debug the generated code.
they designed a series of prompting strategies to guide the model to refine the program automatically.
in these works pre post execution information such as program output is applied to code generation.
furthermore some works found the worth of dynamic features during execution and exploit them to train various language models.
liu et al.
pre trained a language model to learn the execution process of the program.
specifically they represented the program state as a sequence that neural models can learn from and expect the model to predict the trace.
compared to liu et al.
ding et al.
proposed a pre training technique combining both static and dynamic characteristics of the program.
in summary the aforementioned works reflect the close relationship between the behavior of code execution and the program semantics and emphasize the importance of evaluating models for code reasoning with execution.
code reasoning with large language models as introduced in section i in the task of code reasoning an llm needs to predict the program behavior without execution.
recently some works have proposed different evaluation approaches for the code reasoning abilities of code llms.
for example gu et al.
proposed cruxeval which requires llms to reason about pre post execution information such as input and output.
following this study similar to the idea of cruxeval liu et al.
extended the evaluation tasks i.e.
predicting input and output to the natural language specification.
however their evaluation approaches are still limited to pre post execution information and ignore intermediate runtime behavior.
in contrast our work goes a step further to measure how the model learns the runtime behavior during execution which shows promising potential in helping program comprehension as mentioned above.
we notice that a recent work aimed to simulate the code executionprocess with code llms.
they used the analogy of a large language model to a cpu to explore the process of a program executing code paying more attention to algorithm complexity and structure.
different from the aforementioned studies our framework is not only limited to algorithm problems e.g.
competition level ones but also suitable for general programming scenarios e.g.
more real world projects .
in addition we also explore detailed runtime behavior like code coverage and execution path containing more runtime information.
b. consistency for large language models semantic consistency.
semantic consistency refers to the same decisions on semantically equivalent texts of llms .
for example the model should provide similar and even the same answers in the face of two meaning perserving questions.
in the realm of software engineering this feature is generally utilized for the unsupervised evaluation of code llms min et al.
evaluated the self consistency of code llms by comparing the functional correctness of two code snippets one code is generated using a humanwritten description and the other is generated iteratively with the summary of the previously generated.
chen et al.
studied the robustness of code llms to the variations in natural language descriptions for code generation.
allamanis et al.
introduced round trip correctness which aligns code and nl to perform unsupervised evaluation for code llms.
the aformentioned works leveraged the back translation between nl and pl iteratively generated by the model and conduct the semantic or functional comparison.
however they are restricted to the context of semantic consistency in the context of nl and pl.
logical consistency.
if an llm is able to make predictions without logical contradiction it shows the feature of logical consistency .
for example if one model assumes a proposition to be true it should consider the negation of that proposition to be false as well.
there are lots of previous research about how to evaluate and utilize logical consistency for llms in natural language processing but few works pay attention to logical consistency on code llms.
as the reasoning ability is highly related to its logical consistency a comprehensive code reasoning evaluation should contains the measure of logical consistency in scope of programming languages pls .
therefore it motivates us to propose a novel consistency metric idea named ic to fill this gap.
c. code llms and benchmarks code llms large language models for code are llms specialized for the generation and understanding of pls.
for example codellama family models inherit the architecture of llama2 and are further pre trained on extra code corpora.
its three variant models i.e.
base instruct and pythonspecialized are designed for different programming scenarios.
starcoder2 is a series of code llms developed by the bigcode project which achieves competitive performance with other similar sized models.
these models are trained code coverage prediction execution path predictionprogram state prediction output prediction other contexts statementbase benchmark value?
type?
?ccpcode test case benchmark constructionlarge language model exec utionincremental consistency non incremental consistency runtime behavior reasoning incremental consistency evaluation psp epp op correct prediction wrong predictionfig.
.
overview of our framework.
benchmark construction we adapt the base benchmarks to fit our framework by execution.
runtime behavior reasoning we propose four tasks including ccp psp epp and op which challenge llms to perform code reasoning.
incremental consistency evaluation we evaluate if the model can maintain consistency on sequentially related tasks i.e.
incremental consistency .
on the stack v2 dataset whose data size is four times larger than its first generation.
codegen2.
models are improved versions of their previous models e.g.
codegen2 for program synthesis.
it is claimed that their performance gains mainly come from optimizations such as training and sampling strategies.
code reasoning is one of the most important capabilities of llms related to code but few work are dedicated to its evaluation.
in this paper we aim at comprehensively evaluating their reasoning capabilities for programming languages based on execution.
benchmarks recently many code generation benchmarks have been proposed to evaluate the correctness of code snippets generated by code llms.
humaneval is one of the most popular benchmarks for code generation.
it consists of competitive programming problems and evaluates the functional correctness of generated samples rather than text similarity.
apart from competitive benchmarks of which question generally runs in a simple context lots of context aware benchmarks such as codereval classeval and crosscodeeval have been proposed.
these benchmarks provide more complex surrounding contexts and dependencies e.g.
private libraries to evaluate code generation in realworld projects.
in addition some domain specific benchmarks have been proposed to evaluate the performance of code generation in various programming languages and paradigms e.g.
ds for data science .
our work utilizes existing executable benchmarks for code generation and evaluates code llms with respect to code reasoning which makes our framework universal and applicable to different scenarios.
prior studies proposed a variety of code understanding tasks including code search type inference and code translation.
evaluating the ability of code understanding is essential for code llms.
lu et al.
propose codexglue a comprehensive benchmark for code models that supports tasks related to code and text.
cassano et al.
create a hand craftedbenchmark to evaluate the instruction following ability on code editing.
khan et al.
introduce a large scale multilingual multitask benchmark that consists of numerous executable coding examples.
compared to the above benchmarks for code understanding we propose a comprehensive framework for code reasoning from the perspective of runtime behavior which provides a different point of view for the evaluation of code llms.
iii.
re val framework in this section we first introduce the overview of our evaluation framework reval and then describe the two evaluation components in detail namely runtime behavior reasoning and incremental consistency evaluation .
in the end we describe how to construct the corresponding benchmark under our framework.
a. overview of framework fig.
shows an overview of our framework which aims to challenge code llms to reason how the program behaves during execution.
to achieve this we adopt two different perspectives for the abilities of code reasoning runtime behavior reasoning and incremental consistency evaluation .
for runtime behavior reasoning we focus on whether the code model can correctly predict the intermediate states of program execution given an executable program and input as well as other contexts in the base benchmark .
we select four different dimensions of runtime behavior including code coverage execution path program state and output each of which corresponds to a specific sub task under runtime behavior reasoning.
we present the task description and the evaluation metric in section iii b for each sub task.
besides the standalone metrics that measure a single capability of the models we propose a novel idea named incremental consistency evaluation to assess the consistency across a series of incremental tasks during code reasoning.
asthe knowledge required to finish the latter task contains that of the former task in runtime behavior reasoning the difficulty increases progressively in order and we can utilize this characteristic to evaluate incremental consistency of llms with existing predictions see section iii c for details .
in addition as our evaluation relies on existing base benchmarks we present how to construct an adapted benchmark of code reasoning within our framework in section iii d. b.runtime behavior reasoning runtime behavior refers to the intermediate state and information during program execution such as code coverage and variable values which are widely mentioned in previous research .
as shown in fig.
to evaluate the reasoning ability in program runtime behavior for code models we analyze and select four representative dimensions of intermediate information during execution including code coverage execution path program state and output prediction.
corresponding to these features we propose four subtasks for runtime behavior reasoning and introduce them in detail.
code coverage prediction ccp code coverage measures the proportion of code covered by a test suite .
recent research utilized this idea to challenge the model in predicting whether the statements in the program can be executed or not.
hence we exploit the llm code to judge whether a specific statement is executed given the input of a test case.
task description.
given a program pwith statements s1 s2 sn an input xfor execution and a statement index i the model mis required to predict whether the i th statement siis executed.
the ground truth can be denoted as coverage i .
evaluation metrics.
in this task accuracy presents the percentage of correct coverage predictions.
for our benchmark that consists of nnumber of p x i pairs accuracy can be computed as accuracy nxconditional expression.
value is or .
jm p x i coverage i k besides as this task can be considered as a binary classification task we also use f1 score as the evaluation metric following previous research .
program state prediction psp the initial idea of program state refers to values of the program counter and the variables in the context of assembly language and instructions.
since we are mainly concerned with code models at the source code level we follow related work and define program state as a set of variables in the current runtime scope.
each variable has its corresponding value and type.
program state prediction examines the model s ability to reason about value and type conversion of the variable after a statement is executed.
task description.
given a program pwith statements s1 s2 sn an input x a statement index iand avariable name vrelated to the current statement si the model mis required to predict the type and value of variable v aftersiis executed.
the ground truth of type and value can be denoted as ty i v and val i v respectively.
evaluation metrics.
in this task accuracy acc.
measures the percentage of correct value and type predictions acc.
nx jm p x i v val i v ty i v k with this equation a model s prediction is correct only if the value and type both match the ground truth.
execution path prediction epp in this task we refer to the execution path of the program as ordered sequences of statements.
as the granularity of our context is statement level we challenge the code model to predict the next statement to be executed given a specific statement.
a code llm skilled in code reasoning should be capable of telling where the control flow of the program is going and consequently can predict the next executed statement naturally.
task description.
given a program pwith statements s1 s2 sn an input xand a statement index i the model mis required to predict the next statement to be executed after siis executed.
the ground truth can be denoted as next i .
evaluation metrics.
in this task accuracy measures the percentage of correct next statement predictions accuracy nx jm p x i next i k note that if the number of possible answers is more than one i.e.
several statements could be the next one to execute we consider the prediction correct if it hits any possible one.
output prediction op this task is to directly generate the output of a program with the given input which is applied in previous code reasoning work .
to accurately predict the output of a program code llms should be capable of controlling and simulating the whole execution process which places high demands on the code reasoning ability.
to evaluate the correctness of the output we utilize test cases i.e.
a collection of assertion statements in the base benchmarks.
this approach is also applied in various code benchmarks .
task description.
given a program pand an input xfor execution the model mis required to generate the output.
the correct output can be denoted as y. evaluation metrics.
in this task accuracy measures the percentage of correct output predictions accuracy n x jm p x yk where n equals the number of different p x pairs in our benchmark.c.incremental consistency evaluation incremental consistency refers to the idea of how much the model can maintain its consistency across a series of sequentially related tasks.
intuitively if an llm cannot reason about the current task it is not expected to finish the next task whose preliminary depends on the current task.
to clarify this idea we first present the description of incremental consistency and then explain how we evaluate it on code models in practice.
description of incremental consistency the core idea of incremental consistency is to assess code models by leveraging the relationship where the knowledge from one task in a series of tasks depends on the next task.
in the context of our research four distinct sub tasks i.e.
ccp psp epp and op in runtime behavior reasoning are selected and we can observe some patterns from them i cpp psp since the execution of a statement could lead to changes in the program state the prerequisite for psp is correctly predicting if the statement is executed i.e.
ccp .
ii psp epp the control flow of a running program is affected by the value of some variables e.g.
if branch and its conditional variable thus the next statement to be executed i.e.
epp is influenced by the program state i.e.
psp .
iii epp op the intermediate execution state is one of the factors that affect the program output.
thus the knowledge for op covers that of psp.
according to the above descriptions we find that the knowledge required to finish the previous task is contained by that of the following task.
intuitively the subsequent tasks are more difficult than the current task.
hence if a model fails to correctly complete a task e.g.
fails to finish cpp but then predicts the following tasks correctly e.g.
correct prediction of output we consider this model behaves inconsistently in consecutive tasks.
evaluation approach we analyze the results of our runtime behavior reasoning to evaluate the incremental consistency.
specifically for the i th specific problem in our benchmark i.e.
full program a specific statement in it input and one question to ask we assume that the sequential results of four tasks are ri rcpp rpsp repp rop where r and the number indicates whether the prediction matches the ground truth i.e.
or not i.e.
.
hence if the model is incrementally consistent and completes one task only when all its previous tasks are finished the binary sequence rishould be non declining i.e.
ri s wheres .
for the first example of incremental consistency evaluation in fig.
the resulting sequence is reve reve which means that incremental consistency is observed in thiscase.
however for the second example the result is reve reve so incremental consistency is not observed.
in addition depending on how many consecutive times consistency is maintained i.e.
means times we assign different weights to reward models that maintain incremental consistency more often.
it is intuitive because it is harder to behave incrementally consistently across more sub tasks.
finally we define incremental consistency score ic score to quantitatively model the incremental consistency of an llm m. for our benchmark that contains nnumber of p x i v pairs ic score can be computed as ic score nnx i 1ic score i ic score i 2j ifri sj j otherwise the above formula indicates that weighted scores are given based on the number of times the model maintains incremental consistency.
the higher the ic score the higher incremental consistency of the model s behavior.
specifically for a model s results of a problem i if the answers are completely correct it gains a full score.
ii if the answers are partially correct and incremental consistency is observed the model gains a partial score.
iii for other cases e.g.
partially correct but incremental consistency is not observed the model gets a zero score.
d.benchmark construction as fig.
illustrates our framework utilizes existing executable benchmarks to evaluate the code reasoning ability of llms.
we introduce how to adapt these base benchmarks into our framework in two steps i runtime behavior extraction ii problem construction and filtering runtime behavior extraction our evaluation framework requires code models to predict intermediate information during code execution thus we need to extract the runtime information as the ground truth of the problem.
we use the provided test case to execute the corresponding canonical solution to ensure the correctness of program and input.
during the execution we implement the customized program tracer to record i the statement being executed with its number of lines and ii the current program state i.e.
local variables for each execution step.
thus when the execution of code terminates we can acquire an ordered sequence of the runtime behavior we need for the evaluation.
problem construction and filtering we construct our problem for each task with the extracted information.
as there could be a large number of combinations of different runtime behavior and input e.g.
lots of variables in the program state of a specific time step we design several filtering rules to select reasonable and representative ones ccp and epp.
in these two tasks we focus on whether and when a statement is being executed.
as the actual execution sequence of statements could be very long for loops we analyze the control flow graph and break the program into several blocks.
we prioritize the last statement in a block as it leads to various new blocks and is generally more difficult to reason about.
psp.
this task challenges an llm to predict the type and value of a variable.
for reval we inspect the code and focus on the following types of statements i assignment.
we extract the variable s at the left hand side for assignment statements.
the possible types of variables are identifier i.e.
ordinary variables like x subscript i.e.
array slices like x and attribute i.e.
fields like x.y .
in most cases we extract identifiers.
note that some naive assignments such as a orl are skipped but we keep statements like a for the change of variable value.
ii return statement.
in return statements we extract the variables in the returned object if local variables are returned.
if the returned object is a constant value we will select the nearest variable i.e.
the last variable that is not constant.
iii others.
for other situations if any variable after the current line is changed we extract a changed variable based on the priority of new variable changed variables changed attributes i.e.
self.xxx .
not all variables are used because we prefer variables that have closer logical relationship with other tasks e.g.
epp .
we ignore objects of non serializable classes or complex structures e.g.
self objects as it is challenging to convert them to canonical string representations and compare ground truth with an llm s output.
op.in output prediction task we follow cruxeval and utilize the assertion statements in the test cases of the base benchmarks.
for base benchmarks such as classeval where one test case contains multiple assertions we use all the assertions.
specifically we replace the right operands in the assertions with question marks ?
?
and challenge models to predict the masked values.
after the above screening we combine the results to obtain the final adapted benchmark ensuring that the dataset used for each task is consistent.
iv.
e xperimental setup a. base benchmarks in our experiments we first need to obtain the runtime behavior of code such as program state thus base benchmarks should be executable and equipped with test cases.
moreover we would like to experiment with diverse types of data e.g.
different programming scenarios .
therefore we utilize existing code generation benchmarks as the basis for code reasoning evaluation.
typically code generation benchmarks can be categorized into two types competition level ones i.e.
with standalone functions and context aware onestable i statistics of our dataset .
description number of problems of avg.
tokens in programs .
of avg.
tokens in selected statements .
table ii features of studied llm s. fd f oundation code models .
if s upporting instruction following .
os o pen source models .
category series model name size fd if os time code llmscodellamacodellama 7b base 7b codellama 7b python 7b codellama 7b instruct 7b codellama 13b instruct 13b codellama 34b instruct 34b magicodermagicoder cl 7b 7b magicoder s cl 7b 7b starcoder2starcoder2 3b 3b starcoder2 7b 7b starcoder2 15b 15b general llmsgptgpt .
turbo gpt turbo mistral mistral 7b instruct 7b gemmagemma 7b it 7b gemma 2b it 2b i.e.
with more code context like class and file dependencies .
considering the diversity of base benchmarks we choose humaneval as a representative of competition level benchmarks and classeval as a representative of context aware benchmarks.
humaneval is a popular competition level benchmark for code generation.
it consists of hand written python programming problems and needs models to solve the problem given the function signature and docstring.
classeval is a class level hand written code generation benchmark.
classeval provides different programming scenarios e.g.
incremental generation and topics e.g.
management systems and database operations .
these benchmarks are widely used in previous research .
we show the statistics of the adapted benchmarks in table i. in addition to our selection we highlight that our framework is applicable for other similar code generation benchmarks.
b. studied code llms to study the reasoning capabilities of diverse code llms we curate a selection of models with a variety of distinctions.
specifically we mainly consider these dimensions of them general or code specified e.g.
gpt turbo v.s.
codellama scale of parameters open source or closed source e.g.
gpt .
turbo v.s.
mistral 7binstruct foundation or further fine tuned e.g.
codel ...... def f x ...... f .
the function f is defined which takes ... .
... programyou are given a python function ... system message no is line return x executed when f is called?
thoughts cot only question answerfew shot demonstrationsfig.
.
the prompt template for our empirical study.
note that the thoughts part is used only we leverage chain of thought cot prompting.
lama v.s.
magicoder cl instruct following or not e.g.
codellama 7b base v.s.
its instruct version opensource or not and release time.
as a result considering these dimensions we select several state of the art code llms that have been applied to various code related tasks .
table ii presents detailed features of them and we can see that full consideration of the diversity of models across various features is taken to enhance the generalization of our study.
c. prompt design in our work we utilize prompting to evaluate code llms with our code reasoning tasks.
we refer to a recent study on code reasoning and design our prompt templates as illustrated in fig.
.
for classic few shot prompting our prompt template consists mainly of five parts including the system message few shot demonstrations program question and answer.
if chain of thought cot prompting is utilized the thoughts are added into the prompt template as well as the examples in it.
both few shot prompting and cot prompting are widely applied in various tasks including reasoning tasks .
d. implementation details access of models and base benchmarks.
for open source models such as codellama we use the corresponding official releases available on huggingface .
for closed source models e.g.
gpt .
turbo and gpt turbo we invoke the openai api to access them.
two benchmarks i.e.
humaneval and classeval are also publicly available on huggingface .
to help replicate our research we list detailed information such as model ids and urls in our replication package .
environment.
we run experiments on a linux server with nvidia a800 gpus.
for open source llms we deploy a local api server based on vllm which is a unified library for llm serving and inference.
all models are not quantized and we use their original precisions.configurations.
temperature can control the randomness in the generated results of models .
specifically we follow gu et al.
and set the temperature to .
.
for tasks with direct prompting we set the maximum length of generated tokens to while for tasks with cot prompting we set it to .
for the rest of the parameters we use the default settings in vllm to ensure a fair comparison.
to obtain reliable results experiments for all open source models with few shot prompting are repeated five times and we report the mean and standard deviation values in section v. we do not repeat experiments for closed source models like the gpt series due to a limited budget.
v. r esults in this section we discuss the results of our empirical study onreval by answering two research questions rq1 how do llms perform on runtime behavior reasoning?
rq2 how do llms perform on incremental consistency evaluation?
a. rq1 performance of runtime behavior reasoning table iii shows the detailed results of runtime behavior reasoning.
all models are evaluated with few shot prompting except for special annotated ones i.e.
codellama 7b instruct cot .
below we discuss the results from different aspects.
overall performance.
overall we find that the performance of different llms presents a large variation and gpt 4turbo shows superior performance in reasoning about program execution.
for example gpt turbo achieves the best results in all metrics of runtime behavior reasoning and its average accuracy outperforms the second best i.e.
.
of gpt .
by a large margin i.e.
an absolute improvement of .
.
however the overall performance of open source models is not high and the best performer among them i.e.
codellama34b instruct only achieves a level close to that of gpt .
i.e.
.
v.s.
.
in terms of average accuracy.
task.
runtime behavior reasoning consists of four distinct evaluation tasks i.e.
ccp psp epp and op and the performance varies among different tasks.
for example all models achieve an accuracy of more than in op i.e.
output prediction while only about half of them i.e.
out of can provide the correct answers for more than problems in epp i.e.
execution path prediction .
hence according to the average score of tasks the performance distribution may suggest that epp is the most challenging task and op is relatively easy among them.
size and category.
in general we observe that for models within the same family the variant with larger size of parameters shows better performance in runtime behavior reasoning.
in the case of the codellama instruct series as the number of parameters increases i.e.
7b 34b the accuracy of epp has a relative improve by over i.e.
.
.
.
meanwhile smaller models like starcoder2 3btable iii results for runtime behavior reasoning and incremental consistency evaluation rq1 .
ccp psp epp and op four tasks of runtime behavior reasoning a vg the average accuracy score of four tasks .
we report the results in the form of mean standard deviation except for two gpt models because of budget limit .
modelccp psp epp op acc.
avg.
ic acc.
f1 acc.
acc.
acc.
score codellama 7b base .
.
.
.
.
.
.
.
.
.
.
.
.
codellama 7b python .
.
.
.
.
.
.
.
.
.
.
.
.
codellama 7b instruct .
.
.
.
.
.
.
.
.
.
.
.
.
codellama 13b instruct .
.
.
.
.
.
.
.
.
.
.
.
.
codellama 34b instruct .
.
.
.
.
.
.
.
.
.
.
.
.
starcoder2 3b .
.
.
.
.
.
.
.
.
.
.
.
.
starcoder2 7b .
.
.
.
.
.
.
.
.
.
.
.
.
starcoder2 15b .
.
.
.
.
.
.
.
.
.
.
.
.
magicoder cl .
.
.
.
.
.
.
.
.
.
.
.
.
magicoder s cl .
.
.
.
.
.
.
.
.
.
.
.
.
gemma 2b it .
.
.
.
.
.
.
.
.
.
.
.
.
gemma 7b it .
.
.
.
.
.
.
.
.
.
.
.
.
mistral 7b instruct .
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
gpt turbo .
.
.
.
.
.
.
average .
.
.
.
.
.
.
codellama 7b instruct cot .
.
.
.
.
.
.
can also outperform larger models such as codellama 7binstruct in terms of average accuracy i.e.
.
v.s.
.
.
the starcoder2 series utilizes varied architectures and training datasets compared to codellama.
this may demonstrate that apart from parameter size the model architecture and training strategy also play an important role in code reasoning ability.
we also find that code llms do not exhibit an obviously leading advantage over general llms of the same size.
training strategy.
as shown in table iii we conduct experiments on three variants of codellama i.e.
base instruct and python of the same 7b size.
compared to the base model the instruct variant that leverages instruction tuning techniques brings gains in the code reasoning ability i.e.
avg.
acc.
from .
to .
which may reflect the relationship between understanding instructions and reasoning program.
meanwhile since our base benchmarks are all in python additional training with python corpora i.e.
codellama python leads to an improvement of performance i.e.
an absolute improvement of .
.
in addition we note that although further fine tuning applied to the magicoder series improves their performance in code generation the improvement in code reasoning ability is relatively limited compared to their foundation model codellama 7b python.
this may indicate that the training strategies they utilize are not well suited for the reasoning tasks in our evaluation.
prompting strategy.
the last row of table iii presents the performance of codellama 7b instruct with cot prompting.
compared to the model with few shot prompting the perfor mance of cpp psp and epp receives varying degrees of improvement.
for instance the epp accuracy with cot is improved from .
to .
surpassing the performance of the larger 13b model i.e.
.
.
it may demonstrate the effectiveness of presenting how to reason about a piece of code step by step.
however cot prompting fails to improve its op performance with an absolute decrease of .
.
this may result from the wrong thought chain that the model generates for the whole program and eventually leads to the mistake.
summary for rq1 models with different features e.g.
size and training strategy exhibit notable disparities in performance on runtime behavior reasoning.
overall gpt turbo demonstrates a clear advantage over other models in all four tasks in our setting.
b. rq2 incremental consistency evaluation fig.
shows the sorted average accuracy for runtime behavior reason for different models with an additional line indicating the ic scores.
a detailed information of mean ic scores and their standard deviations are also reported in table iii.
overall performance.
we find that the majority of llms exhibit a low level of incremental consistency with scores below which highlights the inconsistency in model behavior across the four tasks of runtime behavior reasoning.
among all of the models gpt turbo stands out with the highest ic score of .
even more than double that of the second place42.
.
.
.
.4fig.
.
average accuracy of runtime behavior reasoning and incremental consistency score for different models sorted in descending order according to average accuracy.
gpt .
i.e.
.
.
given that gpt turbo achieves the best results in both runtime behavior reasoning and incremental consistency evaluation i.e.
.
of avg.
acc.
and .
of ic score we believe that it has both superior ability in program reasoning and a high level of incremental consistency across sequentially related tasks in our evaluation.
trend between ic and runtime behavior reasoning.
as illustrated in fig.
we find that there is an approximately similar trend between the model s average accuracy and its ic score.
for example compared to codellama 7b instruct its larger version codellama 34b instruct has a noticeable higher average accuracy i.e.
.
v.s.
.
and ic score i.e.
.
v.s.
.
.
however this pattern does not hold for all models.
the performance of the general llm mistral 7b is not as good as that of codellama 34b in terms of average accuracy i.e.
.
v.s.
.
in runtime beheavior reasoning but performs better on incremental consistency i.e.
.
v.s.
.
.
others.
similar to the results of rq1 we observe that code llms do not significantly outperform general llms in the ic evaluation.
for example the ic score of gemma 2b it i.e.
.
is higher than code llms trained with more code corpora like codellama 7b instruct and starcoder2 7b i.e.
.
and .
.
this phenomenon may suggest that more code data cannot help llms reason programs better and maintain their incremental consistency.
in addition cot prompting for codellama 7b instruct leads to a great increase in its ic score i.e.
.
.
and this improvement of ic score may benefit from explicit problem solving steps.
summary for rq2 in code reasoning tasks most llms behave inconsistently and their average accuracy is not entirely associated with ic.
gpt turbo achieves an ic score of as high as .
surpassing other models by a large margin i.e.
more than .
absolute improvements .
vi.
d iscussion a. case study fig.
shows a case from the problem of epp.
given a python function that aims to return the largest prime factor of deflargest prime factor n int return the largest prime factor of n. assume n and is not a prime.
largest prime factor largest prime factor defis prime k ifk returnfalse foriinrange k ifk i returnfalse returntrue largest forjinrange n ifn j 0andis prime j largest max largest j returnlargest input question what is the next line to be executed after line largest max largest j ?codellama 34b instructgpt .5turbogpt 4turbo fig.
.
a tricky problem of epp from humaneval .
the prediction of gpt turbo is correct and the other two models i.e.
gpt .
turbo and codellama 34b instruct fail to finish it.
the problem description is simplified for a concise presentation.
parameter nand input this problem requires the model to predict the next statement to be executed after an assignment statement i.e.
largest max ... .
here we select three models that show competitive performance in epp including gpt turbo gpt .
turbo and codellama 34b instruct.
for gpt .
turbo and codellama 34b instruct the prediction is the last return statement of this function while gpt4 turbo chooses the above for loop as its prediction.
we mark a happy emoji to show that gpt turbo makes the right choice and the other two models fail to predict it correctly.
the explanation is that if the assignment statement is executed the possible value of jcan only be or n which means that the loop will continue and the return statement is not the next executed line.
in our framework and the adapted benchmark there are many problems like this that have no obvious answer and are challenging.
if the model is not capable of reasoning about its inherent logic of execution it can easily be misled and give the most look alike answer i.e.
the return statement in this case indicating that our framework can effectively measure the code reasoning capability of llms and present the discrimination of them.
we discuss more cases in the appendix which can be accessed in our replication package1.
b. unsatisfactory performance of code reasoning according to evaluation results we observe that many models perform poorly in runtime behavior reasoning and incremental consistency evaluation.
in particular even the best performer gpt turbo only achieves an ic score of .
reflecting the limitation of current models in maintaining consistency in sequential related tasks and there is still a long way to go to make the llms perform code reasoning.
iv pearson correlation coefficient matrix of the results of runtime behavior reasoning rbr i ncremental consistency ic and human eval he pearson correlation rbr ic he rbr .
.
.
ic .
.
.
he .
.
.
one potential reason is that the current llm might not understand the program execution behavior.
while it is convenient to obtain source code from open source platforms e.g.
github there is relatively less data available regarding code execution behavior because running the program and collecting its runtime information require the corresponding development environments and test suite.
therefore if the model is not familiar with the knowledge related to runtime behavior it may not perform well in code reasoning tasks.
c. correlation between code reasoning code generation we utilize the experimental results and study the correlation between code reasoning and code generation i.e.
whether an llm that performs well in code generation could exhibit equally strong abilities in code reasoning.
table iv presents the pearson correlation coefficient matrix of runtime behavior reasoning avg.
acc.
incremental consistency score and humaneval pass 1rate .
according to the matrix we find that there is a strong positive correlation i.e the pearson correlation coefficients are larger than .
among code generation i.e.
humaneval and code reasoning i.e.
runtime behavior reasoning and incremental consistency .
however the correlation between code reasoning and code generation is relatively lower than that between two reasoning tasks internally i.e.
.
and .
v.s.
.
which indicates that models with similar code generation abilities may vary a lot in code reasoning.
as the correlation may help researchers increase the understanding of code llms and improve the models code generation and reasoning abilities future research could investigate such correlation in depth.
d. threats to validity internal threats.
to construct the adapted benchmark for different sub tasks of runtime behavior reasoning we manually establish some rules to select appropriate statements and variables for the evaluation.
however our selection criteria may not effectively represent the runtime state of the program.
to mitigate this threat we take some measures to determine problem settings that are representative and challenging based on the characteristics of different tasks.
for instance we choose the last statement in the control flow i.e.
for epp and variables that are modified after the execution i.e.
for psp .
these measures help us to reasonably assess the model s capability to reason about code and provide meaningful differentiation.
for runtime behavior reasoning we select four dimensions of the intermediate state of program executionwhich are widely applied in previous research .
these four tasks are proven to effectively evaluate the code reasoning capability of code llms and are appropriate for incremental consistency evaluation for their unique sequential relationship.
however there are still some dynamic features such as memory allocation and exception handling which may help measure code models and we have not explored yet.
further research could consider exploring the potential for llms to reason about other dynamic program features and extend reval to more scenarios.
external threats.
in the empirical study for our evaluation framework the results are restricted to the specific collection of code models and base benchmarks.
to mitigate this threat we choose representative code llms considering several standards including their scale popularity and training strategy for the base benchmarks applied two benchmarks i.e.
humaneval and classeval are distinct from evaluation fashion and programming scenarios as described in section iii d. with the above efforts the experimental results are expected to be sustained in more circumstances.
vii.
c onclusion and future work in this paper we propose reval a comprehensive framework for evaluating the code reasoning capability of code llms.
our framework consists of two evaluation components including runtime behavior reasoning and incremental consistency evaluation we conduct a large scale empirical study on several popular llms and two widely used base benchmarks.
our empirical results show that the majority of llms we evaluate show unsatisfactory performance in both runtime behavior reasoning and incremental consistency evaluation.
to improve the code reasoning capabilities of llms future works can explore training with execution behavior.
one reason why large models may struggle with code reasoning is possibly due to a lack of knowledge related to program execution.
although some general fine tuning approaches are applied to code llms they fail to improve the code reasoning capabilities.
given the demonstrated effectiveness of training models with execution behavior in improving performance in a range of downstream tasks it is reasonable to expect that llms would also derive benefits from such a process.
improving prompting strategy.
our evaluation results demonstrate the effectiveness of cot prompting in code reasoning tasks.
apart from cot other prompting techniques that have been proven effective in nl reasoning tasks such as tree of thoughts may also be applicable to code reasoning tasks.
besides the prompting approach tailored for reasoning about program execution also warrants investigation.