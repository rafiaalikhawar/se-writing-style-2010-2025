roscript a visual script driven truly non intrusive robotic testing system for touch screen applications ju qian miit key laboratory of safety critical software nanjing university of aeronautics and astronautics nanjing china jqian nuaa.edu.cnzhengyu shang shuoyan yan yan wang nanjing university of aeronautics and astronautics nanjing chinalin chen state key laboratory for novel software technology nanjing university nanjing china lchen nju.edu.cn abstract existing intrusive test automation techniques for touch screen applications e.g.
appium and sikuli are difficult to work on many closed or uncommon systems such as a gopro.
being non intrusive can largely extend the application scope of the test automation techniques.
to this end this paper presents roscript a truly nonintrusive test script driven robotic testing system for test automation of touch screen applications.
roscript leverages visual test scripts to express gui actions on a touch screen application and uses a physical robot to drive automated test execution.
to reduce the test script creation cost a non intrusive computer vision based technique is also introduced in roscript to automatically record touch screen actions into test scripts from videos of human actions on the device under test.
roscript is applicable to touch screen applications running on almost arbitrary platforms whatever the underlying operating systems or gui frameworks are.
we conducted experiments applying it to automate the testing of touch screen applications on different devices.
the results show that roscript is highly usable.
in the experiments it successfully automated test scenarios containing over different gui actions on the subject applications.
roscript accurately performed gui actions on over of the test script executions and accurately recorded about of human screen click actions into test code.
ccs concepts software and its engineering software testing and debugging.
keywords test automation non intrusive robot gui testing computer vision acm reference format ju qian zhengyu shang shuoyan yan yan wang and lin chen.
.
roscript a visual script driven truly non intrusive robotic testing system for touch screen applications.
in 42nd international conference on software permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
icse may seoul republic of korea.
acm new york ny usa pages.
introduction touch screen applications including the ones on smartphones and tablets and the ones on embedded devices are increasingly used in entertainment education industry etc.
to ensure the quality of these applications careful testing should be conducted .
however the testing can be tedious and costly.
without proper tools a tester may need to manually and repeatedly operate on a screen which can be labor intensive.
automating the testing of touch screen applications can largely increase test efficiency and reduce test cost.
one way for test automation is to record manual actions as test scripts and use test tools to automatically execute test scripts as many times as desired.
typical scripting and test automation frameworks include textual script based ones like uiautomator robotium and appium which use labels indexes coordinates etc.
to identify widgets and express gui actions and visual script based ones like sikuli jautomate and eggplant which leverage widget images and computer vision techniques for visual gui testing .
all the above test automation techniques are intrusive to the devices under test .
they require supports from the underlying operating system os or gui framework of an application under test aut to get gui states and trigger gui actions.
if the aut is running on a closed system with no underlying os or gui framework support accessible e.g.
a gopro camera or a nintendo switch game console or an uncommon platform whose underlying system support is hard to be accessed e.g.
a customized vxworks like embedded system then such test automation may not be able to be conducted.
for some auts even though the underlying systems might be accessible hacking into these systems to enable test automation may change the tested environments to some extent e.g.
change the performance and energy consumption and the gui actions triggered via internal system facilities may not closely emulate the experience of real users.
under such circumstances the test results of an aut may not be trustable.
there need non intrusive test automation techniques for touch screen applications with no necessary underlying system support easily accessible or allowed to be accessed.
a possible way for nonintrusive testing is using cameras and physical robots to conduct testing .
among the existing work the techniques in seem to have the ability of automating the testing of touch screen applications with robots.
however they use exact ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea ju qian zhengyu shang shuoyan yan yan wang lin chen coordinates to drive test execution.
manually determining the exact coordinates of a gui widget is too costly and using fixed coordinates for test automation also lacks robustness .
the techniques in require hacking into the subject mobile devices in a pre test execution step to gather detailed gui action coordinates and therefore are not truly non intrusive regarding the whole test cycle.
a more usable and truly non intrusive solution to the test automation problem would be using visual test scripts to drive robot based test execution.
visual scripts are expressive robust to changes in spatial arrangements of gui widgets and independent of an aut s underlying gui framework .
they can be non intrusively created and thus are very suitable for nonintrusive testing.
however there is yet no such test automation framework or system in the literature.
many challenges are also faced in developing a visual script driven robotic testing approach.
first how to express various robot based test actions on a touch screen in visual test scripts ?
the existing visual script languages do not support robot controlling and cannot be used for convenient and accurate text inputting in robotic testing.
extensions to such languages should be made to fit the robotic testing context.
second how to execute visual test scripts with a robot ?
the existing visual gui test engines do not support robotic testing and the existing robotic test engines cannot execute visual test scripts.
a new test engine must be designed to carefully plan robot motions according to visual test scripts and camera photos of a tested device to ensure fully automated and highly accurate test execution.
third how to automatically record gui actions on a touch screen into test scripts in order to reduce the test script creation cost?
all the existing test script recording techniques are intrusive.
they cannot work in a truly non intrusive context.
new non intrusive script recording techniques need to be introduced.
this work addresses the above challenges and introduces roscript a truly non intrusive and comprehensively evaluated robotic visual gui testing system for touch screen applications.
roscript uses an extended visual test script language to express and drive test actions on a touch screen.
different from traditional visual script languages the new test script language uses non intrusively obtained camera taken pictures instead of images accurately captured from a gui screen via an aut s underlying os for widget localization and gui state verification.
it also provides special text input and robot control instructions to support robotic testing.
roscript performs gui actions by an xy plotter a robot originally used for personal writing and drawing and cheap enough to be adopted in daily testing.
it integrates a computer vision based test engine to turn actions expressed in visual scripts into lowlevel robot commands.
we introduce several techniques like screenshot cropping screenshot to real world move translation keyboard model based key pressing and robot detouring to enable fully automated and highly accurate test execution section .
roscript does not depend on any os or gui framework to get gui states or trigger gui actions.
its test automation is completely non intrusive and hence applicable to touch screen applications running on almost arbitrary platforms whatever their underlying oss or gui frameworks are.
such generally applicable test automation is especially valuable when intrusive testing is unavailable e.g.
when third party testing for beta or off the shelf products with nointernal test support facilities exported or too costly e.g.
when intrusive testing tools need to be newly designed for platforms unsupported by the existing tools .
to reduce the test script creation cost we also propose a computer vision based technique to automatically record touch screen click actions into test scripts from videos of human actions on a device under test dut .
the script recording is based on visually recognizing human hand actions on the screen.
different from the existing script recording techniques it is fully non intrusive and therefore fits well for non intrusive test automation.
we conducted experiments on applications running on different touch screen devices including android ios smartphones a windows tablet a raspberry pi and a gopro to validate the effectiveness of the proposed approach.
the results show that roscript successfully automated test scenarios containing over different gui actions on the subject applications.
it accurately performed gui actions on .
of the test script executions without triggering the actions in unexpected ways.
.
of the touch screen click actions can be correctly recorded into test scripts from human actions in the application usage videos.
these results suggest that roscript is highly usable for practical touch screen application testing.
the main contributions of this work are a truly non intrusive robotic test automation system.
the system integrates a new test device an extended visual test script language and a computer vision based test engine to enable truly non intrusive test automation.
a video based script recording technique.
we introduce a truly non intrusive computer vision based technique which can automatically create test scripts from camera recorded videos of human hand actions on a touch screen.
a comprehensive evaluation of the proposed robotic testing technique.
the evaluation on the flexibility accuracy and efficiency of the new test automation technique shows it is practical for use.
the rest of this paper is organized as follows.
section shows a motivating example for the approach.
section discusses the related work.
section introduce the detailed approach.
section presents the evaluation.
finally we conclude the paper and discuss future work in section .
motivating example fig.
shows a test scenario for the application embedded in a gopro device.
the scenario tests the video deletion feature of the gopro device.
there are steps to delete a recorded video swipe the screen to enter the control menu press button to go to the playback view click image to select a video click widgets and to start and confirm deletion press button to return to the playback view and check the existence of to determine the successfulness of the deletion.
figure a gopro video deletion test scenario 298roscript a truly non intrusive robotic testing system icse may seoul republic of korea to automate the testing of the gopro video deletion feature traditional intrusive testing tools cannot be used.
this is because an off the shelf gopro device is a closed system with no underlying system facilities accessible and it runs an uncommon os.
as far as we know none of the existing tools like appium or sikuli supports automating the testing on such a device.
the robotic testing techniques in are also unusable because it is extremely difficult to hack into the gopro device to get the gui action coordinates and manually determining the coordinates is too costly.
there needs a non intrusive and easy to use test automation technique for gopro like devices.
figure a test script for the gopro video deletion feature our roscript is the first robotic testing system which can meet the above gopro test automation requirement.
it uses visual test scripts composed by instructions and widget images to express the gui actions on the gopro touch screen see fig.
.
different from the exact widget coordinates widget images can be easily cropped from camera photos of the gopro touch screen.
by using visual scripts to drive an external robot to conduct testing the testing can be non intrusively automated.
during the non intrusive testing no additional software needs to be installed on the gopro and no cable or wireless connection needs to be established between the gopro and the roscript test tool.
related work .
expressing gui actions the gui actions on a touch screen can be expressed via textual or visual test scripts.
the textual scripts relying on screen coordinates lack robustness .
the textual scripts using labels indexes etc.
to identify widgets usually depend on gui frameworks to locate widgets on a screen and thus are difficult to be used in non intrusive testing.
compared with them visual scripts largely use images to express gui actions and rely on computer vision techniques rather than gui framework supports for test automation.
they are better choices for non intrusive testing.
we use visual test scripts for robotic testing.
to fit the robotic testing context we make two extensions to the existing visual script languages .
first we use camera taken pictures to identify gui widgets.
second we introduce new instructions for text inputting and robot controlling.
the camera taken pictures may have low quality and the camera photo driven robot based testing is very different from traditional visual gui testing which just needs to match same scale images on accurate screenshots for test execution.
for these reasons special image processingtechniques like screenshot cropping and scale normalization are also introduced and a new script execution engine is designed to execute the extended visual test scripts.
.
simulating gui actions traditional test automation frameworks simulate a user s gui actions via internal oss gui frameworks or vnc like remote desktop facilities .
these test automation approaches are intrusive.
when the internal oss gui frameworks or vnc services are inaccessible intrusive testing cannot be conducted.
a possible direction for non intrusive testing is simulating gui actions externally via robots .
however the approaches in focus on using robots to test hardware functionalities and performance of smart devices or to do remote testing.
they do not support automatically simulating gui actions on various widgets on a touch screen and therefore are not generally usable for test automation of touch screen applications.
kanstr n et al.
and mao et al.
present two robotic approaches for automating the testing of touch screen applications.
they support non intrusive test execution.
however kanstr n et al.
s approach requires installing special software on the subject mobile devices to collect detailed device usage data before testing.
mao et al.
s approach needs to firstly do intrusive testing with other tools on the subject devices to obtain low level event sequences .
both of them require hacking into mobile devices in a pre test execution step to gather information like action types and coordinates.
regarding the whole testing cycle they are not truly non intrusive.
we also use robots to simulate gui actions for test automation.
our work differs from the existing work in the followings.
first we leverage visual test scripts to drive robotic testing truly nonintrusive regarding the whole test preparation and execution cycle.
second we introduce an intelligent robotic visual gui testing engine armed with many techniques to enable fully automated and highly accurate test execution.
the engine relies on images and computer vision techniques to achieve test automation while the existing work depends on exact coordinates for automated test execution.
third most of the existing work uses specially designed robots or industrial robots for testing while we use a commodity personal xy plotter for test automation and we have addressed various problems faced in building a practical visual gui testing system with such a kind of robots.
fourth we present the first experimental evaluation about the robotic test automation techniques while neither nor is with experimental results presented.
in the literature there is also some work testing the robots themselves .
it aims at problems quite different from ours.
.
recording test scripts we non intrusively record test scripts from human action videos.
this is very different from the traditional script recording techniques.
almost all the existing test tools e.g.
demand information like action types coordinates and images obtained via oss or gui frameworks to record test scripts.
such a kind of script recording is intrusive.
luan et al.
also 299icse may seoul republic of korea ju qian zhengyu shang shuoyan yan yan wang lin chen record test scripts from videos.
however they obtain application usage videos by installing a specially designed screen capturing tool on the duts.
such video obtaining is intrusive and their script generation is based on information embedded into the videos by their screen capturing tool.
our approach differs from the existing videobased script recording in two ways.
first we use camera taken videos recording human actions on a touch screen as the basis for script generation.
the video recording is truly non intrusive without needing to install any special software on the duts.
second we generate test scripts by analyzing human action videos with computer vision techniques.
besides the videos no additional information is required for test script generation.
overview of the robotic testing system the architecture of the roscript robotic testing system is illustrated in fig.
a .
touch screen devices are put onto a working board for testing.
no special connection is required to be established on these devices.
we use an xy plotter to conduct touch screen actions.
the x and y arms of the xy plotter control the action positions and a stylus pen in the xy plotter is responsible for performing screen touches.
we use a camera to obtain the screen states of a touch screen application.
the camera and the xy plotter are connected to the roscript test tool running on a personal computer.
in the test tool touch screen actions are expressed in visual gui scripts.
a robotic test engine turns high level gui actions into lowlevel robot control commands so that the xy plotter can move as expected.
we provide a test script recorder in addition to a test script editor to help create visual gui scripts.
fig.
b shows the robot used for testing.
the arms of the xy plotter are driven by stepper motors with a moving accuracy of .2mm.
the working area of the xy plotter is about a4 paper size.
the camera is set to a fixed position via the camera stand during testing.
in total the hardware of the test device costs about usd which is cheap enough for daily testing.
touch screen working board x arm y armcamera dut the test tool usbxy plotter stylus penthe test device robotic test enginetest script editor test script recorder human action video visual gui script xy plottercamera stylus pen camera stand a system architecture b the test device figure the roscript test automation system under the robotic testing system the testing of a touch screen application can be conducted with the following steps.
the first step is to create test scripts for the aut.
in addition to manual script creation roscript integrates script recording techniques to help automatically generate test code.
to record a test script a testerneeds to manually use the subject touch screen application for one round with all human actions recorded into a camera video.
then test code for the click actions will be automatically generated from the video and the screen states between different actions will also be extracted to support manual test script modification.
the second step is to put the robot and the dut at proper positions and set necessary configurations.
we require the dut to be put in parallel to the x arm of the test robot.
the camera position needs to be adjusted to ensure that the whole dut is visible by the test tool.
we also need to move the stylus pen of the test robot to the top left corner of the dut so that the initial position of the pen can be easily calculated.
the only configurations that need to be manually set before testing are the width height and thickness of the touch screen device.
roscript is to some extent robust to small errors in these adjustments and configurations.
it calculates the touch positions and touch depths based on such settings.
in most cases a slight error in a calculated touch position or a calculated touch depth does not affect triggering a gui action from its widget area of a certain size via an elastic tip in the stylus pen.
in the third step we execute the created test scripts with the robotic test engine.
assertions in a test script are used to verify the test results.
the visual test script language we use a python based language to support robot based visual gui testing.
some key instructions supported by roscript are listed in table .
the gui action triggering instructions like click drag and swipe are commonly used in many visual gui testing tools.
the result verification instructions like assert exist and assert not exist are borrowed from sikuli .
in these instructions like sikuli the image parameter can also be replaced by a string and an optional region parameter specifying a restricted area for matching images on the screen is also supported.
besides the commonly used instructions we introduce instructions take screen photo reset arms move move outside of screen etc.
for test robot controlling.
instruction take screen photo takes a camera photo of the dut crops the screen area out and returns it as the result.
instruction reset arms resets the robot arms to their initial positions.
the move instruction moves the robot s stylus pen to a relative position.
the move outside of screen instruction moves the robot arms outside of the screen area so that they will not affect taking full camera photo of the screen.
table some key test instructions provided by roscript instruction description 1click image region bool click a specified image on the screen.
2drag image image region region booldrag an object from a position specified by image1 to a position specified by image2.
3swipe direction region swipe on the touch screen.
4assert exist image region bool assert the existence of an image.
5assert not exist image region bool assert the non existence of an image.
6take screen photo image take a photo of the dut s touch screen.
7reset arms reset the robot arm position.
8move dx dy move the stylus pen to a position dx dy relative to the current point.
9move outside of screen move robot arms outside of the screen area.
10press keyboard model keys press a sequence of keys on a soft keyboard specified by the given model.
300roscript a truly non intrusive robotic testing system icse may seoul republic of korea when text inputting for non intrusive testing it is impossible to call os facilities to simulate key pressing.
one choice is using instructions like click to click key buttons in a soft keyboard on a touch screen in order to input characters.
however in that way to input a long string we may need to crop a lot of key images and add a lot of click actions into the test script.
this can be tedious.
another way is using an instruction like click abc to input a string depending on ocr optical character recognition techniques to locate the key buttons corresponding to the characters a b and c .
however ocr techniques are often without enough accuracy on low quality camera taken photos and a character like a may occur at many places other than the soft keyboard on the screen.
such a way frequently fails.
to address the above limitations we introduce a keyboard model based instruction press keyboard model keys for text inputting.
the instruction takes a specified keyboard model and a sequence of key names for inputting.
it is convenient for use and depends on the specified model to enable accurate inputting.
the implementation of this instruction will be introduced in section .
.
with the provided test instructions roscript can express various activities on a touch screen with test scripts.
the robotic test engine a robotic test engine is responsible for controlling the robot to perform test actions.
the engine leverages computer vision algorithms to turn the gui actions expressed in a test script into low level robot move commands .
.
locating widgets on the screen to perform a test action with a robot we take a photo of the current touch screen and then locate the target widget s image specified in the test script on the screen photo to determine the action position.
there can be various resolutions for taking camera photos.
high resolutions can lead to better widget localization accuracy while low resolutions result in faster widget localization speed.
we use a resolution of 1600x1200 with a good balance between widget localization accuracy and speed for taking photos.
the widget localization is via computer vision algorithms.
it takes two steps.
the first step crops a screenshot from a camerataken photo of the dut.
the second step matches images in a test script on the cropped screenshot to locate widgets.
.
.
cropping screenshots.
besides the touch screen robot arms as well as objects like a power cable or a piece of paper on the working board may be taken into camera photos of the device under test.
these objects can affect the localization accuracy and speed of widgets on a screen.
to avoid their negative effects it is necessary to crop the screen area screenshot out of a camerataken dut photo and focus just on the screen area for better widget localization.
we propose a computer vision algorithm to find the touch screen area in a camera photo for screenshot cropping.
the algorithm includes five key steps.
the first step uses canny edge detection to detect the edges in the camera photo.
the second step performs a morphological closing operation on the edge detection results to connect the broken edges as much as possible.
the third step detects all the contours from the edges obtained in the second stepand finds the rectangle regions of these contours.
the detected contour regions may be separated or overlapped.
they are the candidates for screen area recognition.
fig.
shows an example demonstrating the results of these three steps where the finally detected contour regions are marked with red border rectangles in fig.
d .
a the original dut photo b the edge detection results c after closing operation d the detected contour regions figure crop the screen area from a photo of the dut the fourth step filters out the contour regions with too large sizes.
this is because in our dut photos like that in fig.
a a screen area must be with enough size difference from the original photo.
the upper bound used for filtering is size camera photo udev where udev .
.
in the fifth step we find the largest contour region among the ones retained by the last step.
this region is recognized as the touch screen area of the dut.
in fig.
d the rectangle with bold red borders marks the screen area identified from fig.
a .
with the above algorithm a touch screen area including the borders of the dut will be obtained.
we can run the algorithm again on a border including touch screen image to get the exact screen area.
however the identification of the border excluding touch screen area is easy to be affected by the contents on the screen.
therefore roscript only runs the algorithm once to get border including touch screen images.
such a screenshot cropping is enough for our robotic testing.
.
.
widget localization.
we locate widgets on a screenshot with a template matching algorithm.
there are many such algorithms in the literature .
we tested the built in template matching algorithms in opencv and the recent bbs algorithm on screen photos.
according to the results roscript picks the normalized correlation coefficient matching algorithm in opencv which performs best in both matching accuracy and speed for implementation.
there can be a scale difference between a widget image in a test script and the image of the same widget obtained at test execution time.
such a scale difference may affect the template matching accuracy and efficiency.
to address the scale difference we store the sizes of the screenshots containing the in test script widget images.
when template matching for a widget the sizes of the dut screenshot taken at runtime and the screenshot of the 301icse may seoul republic of korea ju qian zhengyu shang shuoyan yan yan wang lin chen in test script widget image are compared.
if there is a difference we automatically normalize the scales to the same level according to these sizes in advance of template matching.
in that way the effects of a scale difference on widget matching can be largely reduced.
.
determining the move of robot arms having located the target widget of a gui action on a screenshot it is straightforward to determine the move of the touch position between gui actions on the screenshot space.
however a move on the screenshots is different from a move of stylus pen in the real world.
we need to translate an on screenshot move to a real world move.
to do such translation firstly a robot coordinate system rcs for controlling the robot moves in the real world and a screenshot coordinate system scs expressing the positions of objects on the screenshot space are defined as demonstrated in fig.
.
a move on the scs can be denoted as a position change xs ys .
the key problem in determining robot move is to translate xs ys to a position change on the robot coordinate system i.e.
xr yr .
stylus penrobot yrxr sy sx j k j the robot coordinate systemx arm touch screenxs ysy armworking board camera visible region k the screenshot coordinate system figure the coordinate systems there can be a scale difference between an object in the real world rcs and the same object on a screenshot scs .
for coordinate translation we need to know the real size of the cropped screenshot area dut size before testing with which the scale rate srbetween scs and rcs can be determined.
let the origin of the scs be at point sx sy on the rcs.
for a point xs ys on the screen coordinate system its coordinates on the robot coordinate system are xr xs sr sx yr ys sr sy.
according to the above relationships the translation from xs ys to xr yr is xr xs sr yr ys sr with which the move of robot arms can be determined.
.
performing gui actions roscript performs a gui action by executing a sequence of robot motions.
the below presents the ways to perform two key basic actions.
more complex actions can be performed in similar ways.
click.
a click action consists of the following steps a move the stylus pen on the robot coordinate system to the click position b move the pen down by a calculated depth c move the pen up.
the pen moving down depth is calculated by subtracting the thickness of the tested device from a predefined depth i.e.
depth click depth predef ined thickness dut .
swipe.
we perform swipe actions on a specified region the default of which is the full screen.
take swiping up for example i.e.
swipe up .
the first step is moving the stylus pen to a position near the bottom center of the region.
then we move the pen down to touch the screen.
under the touching state the pen will be moved for a length toward the top of the region.
after that we move the pen up and a swipe action completes.
.
pressing keyboards roscript introduces a keyboard model based approach for text inputting.
the approach defines a model for each soft keyboard to express the distributions of keys on the screen.
a typical keyboard model for the key keyboard is demonstrated in fig.
.
the model contains a keyboard image for locating the keyboard area on the screen.
the keys inside the keyboard are organized in a sub area row and key hierarchy.
each sub area has its relative region information.
rows and keys are in default assumed to be evenly spread.
non default size rows and keys have their relative margin or width information.
with such information it is enough to calculate the center coordinates of each key by providing minimal information in the keyboard model.
keyboard image sub area region row key name a keyboard image b model hierarchy figure a keyboard model when inputting a character like evia a key keyboard with instruction press keyboard key e we first locate the keyboard area by matching the corresponding keyboard image on the screen.
then inside the located keyboard area we use the keyboard model to calculate the screen coordinates of key e. with such coordinates character ecan be inputted via a keypress.
the approach uses the whole keyboard area instead of each individual key button for image matching.
this can ensure the matching accuracy and hence suffer less from the possibly low quality of camera taken screenshots.
it can handle the cases when there are multiple occurrences of a character on the screen.
the approach also does not require cropping key images for text inputting which will ease the creation of test scripts.
.
connecting actions to connect different touch screen actions roscript automatically makes a detour moving the robot arms outside of a certain region after performing a gui action to ensure these arms will not affect the camera photo taking of the next action.
the move is determined by the image matching region used by the next test instruction.
if the next instruction is a move aswipe etc.
no image matching then no such move is actually needed.
otherwise an image matching region rused by the next instruction will be found.
the region can be the whole screen or a region determined by the arguments of the next instruction.
there are three candidate directions to move outside of region r illustrated as a b and c in fig.
.
among a b and c roscript chooses a point with the shortest moving distance to move the robot arms outside of the image matching region.
302roscript a truly non intrusive robotic testing system icse may seoul republic of korea x arm touch screeny arm region rpenac brobot figure moving the stylus pen outside of a region recording test scripts from videos roscript can automatically record human click actions on a touch screen into test scripts from the corresponding videos.
non click actions like drag and swipe are difficult to recognize.
for such actions only semi automatic script recording is supported.
before recording the subject touch screen device needs to be put onto a board with a camera hanging over it focusing on the screen.
then we use the camera to record human actions on the touch screen into videos.
for accurate script recording when operating on the screen we require the user to use only a single fingertip to touch the screen and the other non touching fingers should be in a gripping state fig.
a .
to avoid the perspective effect as much as possible when clicking the angle between the screen and the touching finger should better be less than .
after each click we require the user to move her hand completely out of the camera photo region.
a human screen touch b binary image after skin detection c widget image cropping figure the automatic script recording on a human action video roscript introduces a five steps algorithm to record the human actions into a test script.
step extract image frames and detect fingers.
in this step roscript extracts image frames from the recorded video.
for each extracted frame a color range based skin detection is used to detect the hand area in the frame.
the detection result is turned into a binary image an example of which is shown in fig.
b .
we then detect the contour of the hand region in the binary image.
to reduce the effects of noises only the largest contour close to a border of the binary image is regarded as the hand contour.
this is because when a user touches the screen her hand always crosses a border of the camera photo region.
if no such contour is found there is considered no finger in the frame.
for the detected hand contour we regard the top point in the contour as the fingertip position.
let the bottom left corner of the frame be with coordinates and the x and y axes coordinates of a point increase when moving the point up to the top right corner.
then the fingertip position in a frame is xf yf with a maximal yfvalue in all points of the hand contour.
if there is considered no finger in the frame the fingertip position is set to .step group frames by actions.
in the next step we group frames by actions according to the vertical position of the fingertip in each frame.
fig.
shows the changes of the fingertip vertical position in a sequence of frames extracted from an example human action video.
here vertical position means no finger in a frame.
a click action corresponds to a sequence of frames with the fingertip vertical position starting from gradually increased to a peak point and finally decreased to again.
according to this finding for the example in fig.
it is easy to group the frames into the ones corresponding to actions.
100110120130140150160170180190200210220230240250260270fingertip position frame figure the vertical positions of the fingertip in the frames step find the pre action frame.
for a sequence of frames grouped for a click action roscript finds the frame with the fingertip vertical position equals to and k frames closely before a frame with the fingertip vertical position greater than as the pre action frame.
the pre action frame holds a snapshot of the touch screen before performing the click.
step locate the click position.
in the fourth step for each group of frames corresponding to a click action roscript finds the frame with the maximum value in the fingertip vertical position.
this frame records a snapshot for the time point when the finger touches the screen.
let the fingertip s position be xt yt in the found frame.
for the click action we calculate the exact click position from the fingertip position by pclick xc yc xt yt .
is a constant modelling the distance from the top of the fingertip fingertip position to the center touching position.
step crop an image for the clicked widget.
with the pre action frame and the click position roscript uses algorithm to crop an image for the clicked widget and thus generate test code for the click action.
for the example click in fig.
a the generated test code is t.click .
the algorithm firstly does median blur operation on a preaction frame to optimize it for widget contour detection.
after that a contour detection like that used in the screenshot cropping section .
is applied line .
we process the rectangle regions of the detected contours.
if a contour region is too small it may miss some clickable area in the widget e.g.
detecting a red rectangle region for a borderless back button .
we slightly enlarge the region for a length on each direction so that it will cover more about the widget area line where lbreal is a threshold size in the real world and we turn such a size into a size on the image frame by using the scale rate srbetween the screenshot space and the real world obtained following section .
.
the algorithm then collects the contour regions covering the clicked position and with sizes smaller than an upper threshold line where max real is the upper bound size in the real world .
the upper 303icse may seoul republic of korea ju qian zhengyu shang shuoyan yan yan wang lin chen bound is used because too large cropped images can be confusing for understanding the gui actions and they may also affect the widget click accuracy.
among the collected regions a maximum size region not a region of an internal shape inside a widget is heuristically regarded as the clicked widget region.
we crop widget images according to this region to generate test code.
the existing tool sikuli can also automatically crop widget images for script generation.
however it crops a fixed size image to represent a target widget.
the cropped result is easy to miss important parts of the clicked widget or cover the contents of other widgets causing the generated test code difficult to understand.
our contour detection based method suffers less from that problem.
as validated by the experiments section .
in most cases it can crop meaningful widget images for test script creation.
when there are non click actions in a human action video e.g.
swipes or drags roscript may falsely recognize such actions as clicks.
in that case we need to manually replace the falsely recorded code with correct code for non click actions.
roscript provides semiautomatic script recording for this purpose.
the recording firstly uses the technique in step of the automatic recording to get a sequence of non duplicated image frames with no finger on them.
roscript asks a user to specify the action types and touch positions on such frames.
it then uses the technique in step of the automatic recording to crop images for the touched targets.
with the manually specified action types and the automatically cropped images test code can be semi automatically generated.
algorithm crop the widget image for a click action input f the pre action frame p the clicked position output w the image of the clicked widget 1begin f medianblur f edges cannyedgedetection f edges closingoperation edges contours getallcontours edges rects foreach cincontours do r getrectangleregionof c ifsize r lbr eal 2srthen r enlarge r end if size r max r eal 2sr pinside r then rects rects r end end r getmaxsizeregion rects w getregionimage f r 18end evaluation the flexibility accuracy and efficiency of the test automation and the accuracy of the script recording are four key attributes of a usable test automation system.
we conducted experiments on touch screen applications installed on different devices to evaluate theseattributes of roscript.
the experiments answer the following research questions.
rq1 how is the flexibility of roscript in enabling test automation?
rq2 how accurate can roscript perform touch screen actions?
rq3 how is the efficiency of roscript in performing touch screen actions?
rq4 how accurate can roscript record click actions from human action videos?
rq5 how does roscript perform compared with other intrusive test automation techniques?
.
experimental setup we automated the testing of applications on touch screen devices table to answer rq1.
the devices cover mobile phones a tablet and embedded devices with different sizes colors and operating systems.
the applications cover areas of shopping maps messaging games office etc.
if a large number of test scenarios on these applications can be successfully automated with successful test automation observed one or more times that indicates roscript is with good flexibility.
we also carried out a testability analysis on different gui widgets and actions to assess roscript s ability in automating touch screen application testing.
table the tested touch screen devices model category os color size in a samsung s5 mobile phone android .
.
black .
b samsung note4 mobile phone android .
.
white .
c iphone 5s mobile phone ios .
.
white .
d telcast x98 air tablet windows .
white .
e raspberry pi 3b embedded dev.
debian linux black .
f gopro hero4 embedded dev.
unknown black .
table the tested touch screen applications app description device taobao an online shopping application a ele.me application of an online food delivery platform a youdao dict a dictionary application a uc browser a mobile web browser a tencent video a video streaming application a netease music a music application b baidu maps a mapping application b wechat a messaging social media and mobile payment app b galaxy store app store for samsung devices b androidsys the android built in system functionalities b amazon the amazon mobile app c app store the apple app store c clock a build in clock app c fruitmatching a connect lines puzzle game that matches fruits c ios sys the ios built in system functionalities c explorer windows file explorer d firefox a desktop web browser d win8 settings the windows control panel d raspbiansys the raspbian built in system functionalities e libreoffice libreoffice writer on raspbian e gopro the gopro in device management system f for rq2 we collected the success rates of accurately executing whole test scripts and single gui actions to evaluate the accuracy of the robotic test automation.
a test execution is said to be accurate if all the involved gui actions are correctly triggered on the expected widgets.
due to the limitations of the used computer vision algorithms the robot moving accuracy and the unstableness 304roscript a truly non intrusive robotic testing system icse may seoul republic of korea in camera taken dut photos some actions in test scripts may occasionally fail to be accurately executed while some may never be accurately executed.
we run each test script times to get the accuracy data with the unstableness in test execution considered.
for rq3 the efficiency of roscript in performing touch screen actions is mainly determined by the time consumed by photo taking computer vision algorithms and robot motions.
we collected such time together with the time spent on triggering a single gui action and executing a whole test script to evaluate the efficiency of the robotic testing approach.
to answer rq4 we manually performed touch screen actions to execute the test scripts created for answering rq1.
the actions were recorded into videos and we used automatic script recording to turn such videos back into test scripts again.
the pre action frames extracted from the videos the identified click positions and the cropped images were checked to evaluate the accuracy of roscript s automatic script recording.
for rq5 we selected sikuli a state of the art intrusive visual gui testing tool as a baseline to evaluate roscript considering the accessibility and replicability of the existing techniques and tools on robotic test automation and visual gui testing .
the flexibility accuracy and efficiency of the test automation and the accuracy of widget image cropping in test script recording were compared on the same test scenarios and settings to get a better understanding of the performance of roscript.
in roscript the test engine is implemented in python .
.
we implement computer vision algorithms based on the basic functionalities provided by opencv .
.
the test robot is driven by ebb commands via usb connection.
besides the test robot the whole system runs on a pc with windows os intel i7 6700hq cpu and 16gb memory.
we did experiments under a set of common parameter settings for the computer vision algorithms on different subject applications without fine tuning them for each individual subject.
the parameters k lbreal and max real used in section are 4mm .5mm 55mm2 and 400mm2 respectively.
.
results and discussion .
.
rq1 the flexibility of the test automation.
table presents the main experimental results.
in the table column script shows the numbers of test scripts written for each subject application.
column action instruction lists the numbers of total and key types of gui action instructions included in the test scripts of an application where cl lp sw dg ld dc and kp denote click long press swipe drag long press drag double click and keyboard press respectively.
column test accuracy lists the success rates of accurately executing test scripts and gui actions.
as shown in table we totally created test scripts composed of gui action instructions for the subject applications.
one test script was never accurately executed because of unstable touches on the raspberry pi s low sensitivity screen.
test scripts were at least be accurately executed by roscript for once.
these test scripts automate the testing of various application usage scenarios including the simple ones like setting options by controlling standard widgets on android like systems particularly designed for touch screen usages and the more complex ones like renaming files onwindows like systems not well designed for touch screen usages and playing games on non standard graphical widgets .
the accurate execution of them suggests roscript can automate a broad range of common testing activities on touch screen applications.
we also conducted a testability analysis to check the gui widget and action combinations that can be automatically triggered by roscript.
the results are shown in table whose rows list the common semantic actions on widgets.
these semantic actions can be triggered by raw actions like click swipe and long press in one or more ways.
the raw actions are not listed here as they are commonly supported regardless of the widget types.
if a semantic action can be triggered that means the test execution of its corresponding gui functionality can be automated.
in table an empty cell indicates a widget action combination is considered uncommonly used in software implementation.
symbols w and indicate different level of supports provided by roscript.
from table we can see that for the commonly used widgetaction combinations non empty cells roscript theoretically supports testing of them .
.
of them are theoretically unsupported.
the experiments covered of these combinations of which were always successfully triggered and were successfully triggered for at least once.
many semantic actions like focusing on a tooltip and showing a context menu on a frame title were not covered because they are rarely used in touch screen interactions although sometimes being implemented.
only one widget action combination was covered but never successfully triggered because of unstable double clicks on a low sensitivity touch screen.
these results indicate that roscript provides very broad supports for gui action triggering.
some typical gui actions theoretically unsupported by roscript include focusing on a button and scaling up down an image.
these actions usually require keyboards tabkey or multi finger touches to trigger.
besides table only focuses on touch screen relevant actions.
the actions requiring hardware buttons sensors etc.
to trigger are also unsupported by roscript.
even so as a truly nonintrusive testing tool roscript is already very helpful for the test automation of various touch screen applications.
table gui action triggering ability analysis borderbuttoncheckboxcombocontext menudrop downframe titleimagelabellistlist headerlist tree item menumenu itemprogress barradio button screenscrollbarsliderspinnertabtexttoolbartooltiptreewindow empty scroll home back scale up down theoretically unsupportedtheoretically supporteduncommon action theoretically supported always successfully triggered in the experiments theoretically supported occasionally fail to be triggered in the experiments theoretically supported never successfully triggered in the experimentsfocus show tooltip show ctxt menu select dbclick select long press select deselect expand collapse move drag text enter text delete regarding the ability of verifying test results roscript applies the same image matching techniques used by traditional visual gui testing tools to check the successfulness of test executions.
according to such result verification is highly usable for test automation.
305icse may seoul republic of korea ju qian zhengyu shang shuoyan yan yan wang lin chen table the experimental results app script action instruction test accuracytest efficiency script recording sikuliscript level time s action level time s click click record accuracy total cl lp sw dg ld dc kp aas script action cv motion total cv motion total paf pos crop total script acc time s record taobao .
.
.
.
.
.
.
.
.
.
.
.
.
.
ele.me .
.
.
.
.
.
.
.
.
.
.
.
.
.
youdao dict .
.
.
.
.
.
.
.
.
.
.
.
.
.
uc browser .
.
.
.
.
.
.
.
.
.
.
.
.
.
tencent video .
.
.
.
.
.
.
.
.
.
.
.
.
.
netease music .
.
.
.
.
.
.
.
.
.
.
.
.
.
baidu maps .
.
.
.
.
.
.
.
.
.
.
.
.
.
wechat .
.
.
.
.
.
.
.
.
.
.
.
.
.
galaxy store .
.
.
.
.
.
.
.
.
.
.
.
.
.
androidsys .
.
.
.
.
.
.
.
.
.
.
.
.
.
amazon .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
app store .
.
.
.
.
.
.
.
.
.
.
.
.
.
clock .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fruitmatching .
.
.
.
.
.
.
.
.
.
.
.
.
ios sys .
.
.
.
.
.
.
.
.
.
.
.
.
.
explorer .
.
.
.
.
.
.
.
.
.
.
.
.
.
firefox .
.
.
.
.
.
.
.
.
.
.
.
.
.
win8 settings .
.
.
.
.
.
.
.
.
.
.
.
.
.
raspbiansys .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
libreoffice .
.
.
.
.
.
.
.
.
.
.
.
.
.
gopro .
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
total .
.
rq2 the accuracy of the test automation.
for the accuracy of performing touch screen actions from columns test accuracy script action of table we can see that roscript s average application level success rates of accurately executing test scripts and accurately triggering gui actions are .
and .
respectively.
the corresponding total success rates for all the test scripts and gui actions are .
and .
.
in a test script the actions after the first failure in accurately triggering a gui action were counted as inaccurately triggered.
column test accuracy aas of table shows that in total .
test scripts were always accurately executed.
these data suggest roscript can automate the test activities on a touch screen application with high accuracy.
roscript got the worst accuracy on raspberry pi which has a low sensitivity touch screen and runs a debian linux based system not well tuned for touch screen usages.
the common reasons causing a gui action failing to be accurately triggered include too light or heavy touches on screens with different touch sensitivities the low template matching accuracy of some too small and unclear widget images and some widget images containing light colored texts and the moir pattern noise on some screen photos.
for the touch sensitivity relevant failures better touch mechanisms and more compatible stylus pens may be needed.
for the other failures a possible way to improve the test automation accuracy is to fine tune the camera shooting environments and configurations for a subject to obtain better camera photos of a touch screen.
the experiments tested different subjects under the same camera shooting environments and configurations and had not tried such tuning.
if fine tuning individually for each subject even better test automation accuracy might be achieved.
.
.
rq3 the efficiency of the test automation .
roscript takes about .
.7s to obtain a photo of the dut.
column test efficiency of table lists some other test script level and gui action level efficiency data about our robotic testing.
sub columns cvandmotion show the time consumed by computer vision algorithms and robot motions respectively when executing a test script or a gui action.
sub column total lists the total time spent on executing a test script or triggering a gui action.the efficiency data in table show that under roscript each gui action averagely takes .07s in running computer vision algorithms.
that time is almost ignorable.
triggering a gui action via the robot averagely costs .5s.
at the test script level running a test script consisting of on average .
gui action instructions and some other code including waits and sleeps averagely costs .8s.
such a test execution speed is acceptable in practice.
.
.
rq4 the accuracy of the test script recording.
column script recording in table presents some results about the automatic script recording.
its sub column click lists the percentage of click actions in the human action videos of each application .
sub column paf shows the accuracy the success rate of accurately something of the pre action frame extraction for the clicks.
sub column pos lists the accuracy of click position identification.
a click position is considered accurately identified if clicking there can correctly trigger the expected gui action.
sub column crop shows the accuracy of widget image cropping for clicks with positions accurately identified.
we consider a widget image is accurately cropped if the cropped image covers the distinguishing part of the clicked widget is easy for understanding the image shall not contain parts of other unclicked widgets and if the clicked target is a string then at least a whole word shall be included and is with target action triggerable on the image center.
sub column total lists the overall accuracy of click action recording.
we consider a click action is accurately recorded if finally its widget image is accurately got.
from table we can see that for the click actions at the application level their average accuracy of pre action frame extraction click position identification widget image cropping and overall click action recording are .
.
and .
respectively.
the overall click action recording accuracy is about higher than the multiplication of the click position identification accuracy and the widget image cropping accuracy on average .
.
this is because sometimes the recognized widget contour regions were a little larger than the real widgets which can tolerate slight errors in the detected click positions.
the total recording accuracy for all clicks corresponding to the click instructions is .
.
that means the click actions can be recorded in very high accuracy.
click 306roscript a truly non intrusive robotic testing system icse may seoul republic of korea actions occur very frequently on touch screen applications e.g.
taking about .
in all the involved gui actions in the studied test scenarios .
with that accuracy roscript s test script creation can be largely automated with the script recording technique.
in the recording a click position fails to be accurately identified often because of the skin like color images on a touch screen which cause the hand contours difficult to be accurately recognized and the perspective effect in camera shots which causes the touch positions hard to be accurately located.
the common situations that a widget image fails to be accurately cropped include a clicked widget is too small and its neighbor widgets also occur in the cropped result and a clicked widget is too big and the cropped image does not cover all its content.
the script recording got the worst accuracy on thefruitmatching application which contains many closely spaced small fruit image widgets challenging for click position identification and image cropping.
better script recording techniques are still under design to further increase the recording accuracy.
.
.
rq5 the comparison with sikuli.
for the flexibility of test automation on platforms like windows and linux natively supported by sikuli roscript s gui action triggering ability is weaker than that of sikuli since sikuli can trigger almost all possible actions via os facilities.
roscript s test result verification ability is also a little weaker because unlike sikuli it relies on inaccurate camera photos instead of accurately captured screenshots for result checking.
on platforms like android ios testable via screen mirroring but not natively supported by sikuli roscript cannot guarantee accurate test automation for a few test scenarios.
sikuli is not directly usable when there are some mobile gestures originally unsupported by it see the script numbers less than that of roscript from column sikuli script of table .
under such circumstances the flexibility of these two tools is somewhat comparable.
on arbitrary touch screen platforms like gopro roscript certainly has better flexibility because sikuli s intrusive testing cannot work on such platforms.
for the accuracy of test automation at the test script level sikuli achieved a test automation accuracy of see column sikuli acc of table .
there were also a few inaccurate test script executions due to sikuli s own technique limitations .
roscript s test automation accuracy .
is lower than that of sikuli because its camera taken gui states and robot movements are not accurate.
even so considering that the test automation of roscript is completely non intrusive and applicable to almost all possible touch screen applications such sacrifice in test automation accuracy might be worthy.
for the efficiency of test automation sikuli constantly took about .5s to trigger a gui action.
this is much faster than roscript .5s on average .
however taking the time spent on waiting for gui states into account at the test script level roscript and sikuli s average test execution time still remains in the same order of magnitude roscript .8s sikuli .5s see column sikuli time of table .
for script recording sikuli cannot non intrusively record test scripts.
on the same settings as that of roscript the average accuracy of sikuli s fixed size image cropping with the desktop icon size as the fixed size is about .
see column sikuli record of table .
roscript s widget image cropping accuracy .
is much higher than that of sikuli.
besides roscript cropped more beautiful widget images since its cropped images usually are compact andshow widgets in the center while sikuli s image cropping results are mostly not compact and often show widgets at the corners.
.
threats to validity there are three major validity threats in the experiments.
the first is that the use of the devices applications and test scenarios in the experiment is limited which might limit the generalization of the experimental results.
nevertheless we have evaluated roscript on representative devices applications from different areas and test scenarios with gui actions.
with such a number of subjects we believe it is convincing to draw some effective conclusions.
the second threat is the experimental environment such as the camera shooting environment and configurations.
we conducted the experiments in a normal laboratory room with a camera resolution of 1600x1200 and a common camera shooting distance enough to have all the tested devices fully visible.
such an environment and configurations were not fine tuned for different devices.
in practical testing by fine tuning such factors even better test automation accuracy or efficiency might be achieved.
the third threat is that the quality of the recorded human action videos may have impact on the script recording accuracy.
to alleviate the impact we proposed a set of video recording instructions section to guide the experiments.
all experiments were done strictly following these instructions.
we have already tested the recording on videos covering click instructions and the experience shows that by following the video recording instructions high script recording accuracy can be achieved.
conclusion and future work this paper presents a visual test script driven robotic testing system to automate the testing of touch screen applications.
it uses physical robots to conduct fully non intrusive testing and leverages computer vision algorithms to drive test execution.
a video based approach is also proposed to automatically record human touch screen click actions into test scripts which can reduce the test script creation cost.
the experiments on various touch screen devices and applications show that the approach is usable.
there are still some limitations in the current implementation of roscript.
first due to the limitations of physical robots its test execution speed is slower than traditional intrusive testing.
second some unstable screen touches and low quality camera pictures may affect the accuracy of performing gui actions.
third the approach yet does not support automatically recording arbitrary gui actions.
fourth roscript only supports testing devices with sizes smaller than a4 paper.
these may affect the scalability robustness and application scope of the approach.
in the future we plan to continuously improve the testing system to address these limitations.