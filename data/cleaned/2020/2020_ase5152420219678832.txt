figcps ef fective failure inducing input generation for cyber physical systems with deep reinforcement learning shaohua zhanga shuang liua jun sunb y uqi chenb wenzhi huanga jinyi liua jian liua jianye haoa acollege of intelligence and computing tianjin university tianjin china bsingapore management university singapore noisysilence shuang.liu wzhuang jyliu jianliu jianye.hao tju.edu.cn junsun yuqichen smu.edu.sg abstract cyber physical systems cpss are composed of computational control logic and physical processes which intertwine with each other.
cpss are widely used in variousdomains of daily life including those safety critical systemsand infrastructures such as medical monitoring autonomousvehicles and water treatment systems.
it is thus critical toeffectively test them.
however it is not easy to obtain test caseswhich can fail the cps.
in this work we propose a failure inducing input generation approach figcps which requires no knowledge of the cps under test or any history logs of thecps which are usually hard to obtain.
our approach adopts deepreinforcement learning techniques to interact with the cps undertest and effectively searches for failure inducing input guided byrewards.
our approach adaptively collects information from thecps which reduces the training time and is also able to exploredifferent states.
moreover our approach is the first attempt togenerate failure inducing input for cpss with both continuousaction space and high dimensional discrete action space whichare common for some classes of cpss.
the evaluation resultsshow that figcps not only achieves a higher success rate than the state of the art approaches but also finds two new attacks ina well tested cps.
index t erms test case generation cps deep reinforcement learning i. i ntroduction a cyber physical system cps is composed of computational elements and physical processes with different spatial and temporal scales modalities and interactions .nowadays cpss are widely used in safety critical domainsand important urban infrastructures such as autonomous ve hicles medical monitoring water treatment system and smartgrid .
as a result those systems attract great attention fromcyber attackers due to the potential of causing significantdamage.
there have been a large number of successful attacksreported lately .
the great threats encountered by cps have stimulated research and development on cps testing attack and defense methods .
one of the central problems isgenerating test suites to reveal the bugs defects in the cps.
thedifficulty is that the unsafe states are usually sparse and are dif ficult to reach and more importantly a sequence of input is re quired in order to expose the failure.
existing approaches adoptlearning and feedback guided fuzzing techniques toexplore potential failure inducing inputs.
fuzzing approachesconduct mutation to obtain discrete actuator configurations and select potential failure inducing configurations through corresponding author.machine learning models trained with history log data or enhanced with online observed packet information .these approaches are developed for cps systems with discreteaction space and pre trained models are required.
there isanother kind of approach that covers the test case generationproblem into an optimization problem and adopt differentoptimization strategies e.g.
simulated annealing geneticalgorithms gradient descent cross entropy and gaussian regression to search for solutions.yamagata et al.
first propose to use deep reinforcementlearning drl to search for failure inducing input.
theirevaluation results with three cps models show that the twodrl algorithms i.e.
ddqn and a3c outperformexisting optimization approaches on all models yet are worsethan the random strategy on one model.
these approachestarget the cpss with continuous action space.
considering that cpss have diverse action spaces i.e.
discrete or continuous and that none of the existing approachesconsider both action types.
fuzzing approaches target discreteaction space and require system logs or network packets which are not always available e.g.
a third party cps systemwhich makes the logs and network packets private to train aprediction model.
the optimization approaches show unstablesuccess rates on different cpss.
in this work we proposeto adopt the idea of deep reinforcement learning whichtakes the cps as environment and trains an agent to learnsearching strategies through interacting with the environment.taking into consideration the deficiencies of ddqn anda3c algorithms we utilize a random network distillation rnd mechanism to encourage exploring unseen states inthe searching strategy.
moreover to tackle the challenge ofgenerating high dimensional discrete actions we adopt theidea of wolpertinger to map the actions in continuousspace into discrete space.
in other words we provide a unifiedsolution for cps failure inducing input generation for bothdiscrete and continuous action space .
the primary targets of our method are those cpss which take either discrete or continuous inputs from the actuator con troller and obtain sensory readings in real time.
in our method the system simulator is treated as a black box and interfaceswhich provide sensor and actuator controller readings arerequired.
our method can also be applied to real systems and only requires an interface which allows interacting withthe system through network communication for injecting data 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee and reading system status .
note that such interfaces are often for system diagnostic purposes.
we evaluate our approach with three cpss one with discrete action space and two with continuous action spaces.
ourexperimental results show that our approach is able to generatefailure inducing input for both discrete and continuous actionsefficiently and consistently i.e.
with a higher success rate ascompared with existing approaches.
to summarize we make the following contributions we propose an effective approach figcps to generate failure inducing input for cps systems with consider ation of both the continuous and the high dimensionaldiscrete input formats.
we evaluate figcps with three cps simulators widely adopted in related approaches.
the evaluation resultsshow that our approach is effective in generating failure inducing inputs in both continuous and high dimensionaldiscrete input formats.
we also compare figcps with state of the art input generation approaches and the results show that ourapproach outperforms existing approaches in both successrate and the number of iterations required.
we also findtwo new attacks in a well tested cps.
the remaining of the paper is organized as follows.
first we present preliminaries in section ii.
we then introduce thedetails of our method in section iii and evaluate our approachin section iv.
related works are discussed in section v andfinally we conclude our method in section vi.
ii.
p reliminary a. deep reinforcement learning reinforcement learning is a machine learning method which trains an agent through interacting with the environment with the purpose of maximizing the long term reward ofan observed state.
the agent takes the current state fromthe environment as input and returns the next action to theenvironment according to reward.
reinforcement learning isoften described as a markov decision process mdp whichis a triple m s a p .sis a set of states ais a set of actions and pis the transition probability.
prepresents the probability distribution of s prime r s r where s primeis the next state and ris the reward that the agent can get if it takes action aover system state s. the aim of reinforcement learning is generating an action an based on previous states s0 ... sn rewardsr1 ... rnand actions a0 ... an which maximizes the expected value of the long term rewards r summationdisplay i n iri where0 1is the discount factor.
the reward definition is task specific and it could be an integrated rewardcomposing of the external reward and the internal reward.
theexternal reward which is usually simply referred to as reward is defined according to responses from the environment.
theinternal reward is optional.
it is usually defined based on thesystem state and is meant to encourage the exploration ofthe state space.
deep reinforcement learning algorithms canbe mainly divided into three categories i.e.
value based policy based and actor critic .
value based methods q learning is a classical value based reinforcement learning algorithm.
deep q network dqn is a value based method which combines q learning with deep neural networks.
double deep q network ddqn further improves the deep q network with adual network to eliminate the over estimation problem.
ddqnis one of the standard and representative value based drlalgorithms.
it has shown good performance on a range of tasks yet it suffers from performance defects due to the fact that thevalue based algorithm computes the q value for all actionson each step and is rather expensive for the high dimensionaldiscrete action space or continuous action space .
policy based methods policy gradient is a classical policy based drl algorithm which uses a network to approximate the policy directly.
it promotes the algorithm processingpower to high dimensional discrete action space and continu ous action space.
however it is hard to evaluate a policy andit is likely to fall into a local optimal solution.
actor critic methods actor critic is a deep reinforcement learning framework that combines value based and policy based strategies.
a policy is a probability distribution from states in sto actions in a. in the actor critic framework there are two networks i.e.
the actor network and the criticnetwork.
the actor network generates actions according topolicy and the critic network estimates q according to the results of the run.
then the policy is updated with the estimated q .
in the next phase the actor will follow the new policy.
the actor critic method repeats this process and theaction value function q is defined by q s a e bracketleftbigg summationdisplay t 0 trt x0 x a0 a bracketrightbigg whereeis the expected value.
asynchronous advantage actor critic a3c adopts the idea of multi threading toaccelerate the learning process.
all sub processes run with thesame algorithm and the learned knowledge is collected by themain process.
however the policy approximated in a3c is astochastic policy that is hard to converge to global optimal.
deep deterministic policy gradient ddpg is an actor critic model free deep reinforcement learning algorithmbased on the deterministic policy gradient.
it extends the actor critic approach with two innovative improvements i.e.
the ex perience replay mechanism proposed in dqn and the doubleq network proposed in ddqn.
during the off policy trainingof networks the target network can minimize dependenciesbetween samples and the network while the experience re play mechanism can minimize correlations between data andovercome the non stationary distribution problem of data.
there are four networks in ddpg i.e.
the estimated actor network f the target actor network f prime the estimated critic network q q the target critic network q q prime with corresponding parameters of prime q q prime.
following the standard actor critic settings the estimated actor networkgenerates actions according to the states of the environmentand the target actor network generates actions according to the 556critic enviroment cpss actor optimizer f gradient soft update rnd optimizer target network nsamplepredictor network target actor network estimated actor network update soft update update mse optimizer estimated critic network target critic network update q gradient gradient experience replay buffern 6988agent fig.
the overall architecture of figcps states in samples from the replay buffer.
the estimated critic network and the target critic network evaluate the generatedactions together.
following the standard dual network settings the estimated networks are updated constantly with each batchsample and the target networks which are used to minimizethe dependencies between samples and network parameters are updated periodically by copying the parameters of theestimated networks in a soft update way.
ddpg has adoptedthe advantages of both the actor critic method and ddqn.
it isshown to perform well on problems with continuous or high dimensional discrete action spaces such as cps models compared with ddqn.
compared with a3c ddpg learns adeterministic policy which requires less data for training andthus converges faster.
iii.
o ur approach generating failure inducing input for cpss is critical.
existing approaches assume the availability of the system logdata or network packets which could be unavailablefor some scenarios say testing of third party systems whichtake the network packets private or encrypt the networkpackets.
therefore we take those situations into considerationand assume no extra information except for a cps simulator.
a. approach overview fig.
shows the overview of our approach.
the steps are annotated with circled numbers e.g.
1t o .
we follow the standard notations and use stto represent the state and atto represent the action.
we use extrt inrt andrttorepresent the external reward the internal reward and the integrated reward respectively.
the structure of our methodconsists of the environment i.e.
the cps and the agent boxedwith dotted line .
the agent is further divided into the ddpgcomponent in gray box and the rnd component.
b. approach details algorithm shows the main idea of our method and the circled numbers correspond to those in fig.
.
the main process can be divided into three phases i.e.
generate action execute the action and compute reward and train model .
our approach proposes different agents to handle different types of action spaces.
the base model ofour approach is a ddpg agent which utilizes the actor critic framework where the actor network adopts a policy based strategy to generate actions and the critic networkadopts a value based strategy to criticize the actor s decisions.there are two actor networks i.e.
the estimated actor networkf and the target actor network f prime and two critic networks i.e.
the estimated critic network q qand the target critic networkq q prime.
the estimated actor network f works out an action according to the given state then the estimated criticnetworkq qevaluates the action produced by f .
these two networks train their parameters and qusing the evaluation ofq q. the target actor network f primeand the target critic networkq q primeupdate their parameters primeand q primewith and qin regular intervals.
we first randomly initialize the parameters for the estimated actor network f and estimated critic network q q 557algorithm failure inducing input generation input cps simulator m episode limit l step limit t output a sequence of actions 1randomly initialize q qandf with weights qand .
2initialize q q primeandf primewith weights q prime q prime .
3randomly initialize nand nwith weight n. 4initialize g prime ksdictionary of actions with elements of a.
5initialize b inw extw bs discrete andrandom.
6forepisode ldo randomly reset the state of cps simulator m fort tdo observe state stfrom m ifdiscrete then a f st ak gk a at arg max aj akq q st aj else at f st extrt st end m at if st 1is unsafe then return allatinb ifrandom then inrt n st n st rt inw inrt extw extrt else rt extrt add transition st at rt st tob if b bs then sample n si ai ri si fromb 6yi braceleftbiggri end ri q q prime parenleftbig si f prime si parenrightbig 7l parenleftbig q parenrightbig n summationdisplay i j n summationdisplay i aq q s a a f si f s si 309 q prime q q prime prime prime update nby minimizing inri 32end for line .
then the target actor network f primeand the target critic network q q primeare initialized with the parameters of the corresponding estimated networks line .
the weights for the rnd networks are also randomly initialized line .
themap of k nearest neighbor knn is initialized in line .
allthe other parameters are initialized in line .
bis the replay buffer and bs is a limit when the size of bis larger than bs the training of networks starts.
the in wandextware the coefficients of internal reward inrtand external reward extrt respectively.
discrete andrandom switch on the action space mapping and the rnd process respectively.
line to is the process of one episode in our algorithm.
in the following we describe the detailed algorithm accordingto the three phases i.e.
generate action execute the action and compute reward and train model .
generate action after the state stis obtained the action fig.
generate high dimensional discrete actions atis generated.
our approach considers the situations of both the continuous actions and the high dimensional discreteactions.
for cps taking continuous actions e.g.
a t and ptc our method uses standard ddpg which uses the estimatedactor network to generate action based on the obtained state line .
for the cps system which takes high dimensionaldiscrete input values e.g.
sw a t with dimensional discreteactions as input our approach maps the continuous values to adiscrete space.
as shown in fig.
the estimated actor networkf generates a proaction a rn line which is a continuous action and is likely not a valid action i.e.
a a. for instance the vector .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
is an example pro action generated for sw a t in which n is and each bit corresponds to an actuator in sw a t. then we adopt gk rn a a knn method that maps from a continuous space rn to a discrete set a line .
the mapping function gkis defined in .
gk a k argmin a a a a where a a is a discrete action in the set a. the dashed circle in fig.
illustrates this mapping process.
it returns k e.g.
k is for sw a t actions a kinathat are closest to abyl2distance.
however due to the differences in the action representation actions with a low q value may occasionallysit closest to aeven in an area where most actions have a high q value e.g.
a 2in fig.
.
moreover some actions may be close to each other in the action embedding space butin certain states they must be distinguished as one has aparticular low long term value relative to its neighbors e.g.
a 1in fig.
presents such actions .
to prevent picking actions like a1 a2 and to improve the accuracy of the selected action the final part of the algorithmrefines the choice of action by selecting the highest scoringaction according to q q. the returned k actions have their q values.
we refine the choice of action by selecting the highest scoring action according to q q the last step in fig.
at s a r gm a x a akq q s a after refinement of action we can get the final action chosen by the algorithm.
for instance the action is chosen corresponding to the pro action generated.
execute action and compute reward after generated the action is fed to the environment i.e.
the cps as input and 558the environment returns the external reward ext r next state st and a signal of the end state line .
the external reward is used together with the internal reward in r t o compute the reward for training.
the extrfor the cpss is defined on a per model basis.
for sw a t we define reward on a per sensor basis.
for the case of underflow the external reward is defined as follows extr di ds ls si si hs di ds ls si si hs otherwise wherediis the distance of the sensor reading from its safety threshold and dsis the distance of a sensor reading between two steps.
for the case of underflow attacks the distances are computed with di si ls ds si si.
for the case of overflow the external reward is defined as follows extr di ds ls si si hs di ds ls si si hs otherwise the distances are computed with calculated as follows di hs si ds si si .si denotes the next reading of a sensor sidenotes the current reading of the sensor ls denotes its lower safety threshold hsdenotes its upper safety threshold.
considering that diis usually larger in order of magnitude than ds we use as a factor to balance the effects of diandds.
for the a t and ptc we adopt the reward definition of yamagata et al.
which defines the unsafe state of a system based on future reach safety properties.
the unsafestate is considered to be reached whenever the safety propertyis violated.
the external reward is defined as follows.
ext r s i e x p s i where sis a finite sequence of states and is a safety property.
one challenge is that the unsafe states in cps are usually sparse and difficult to explore.
to increase the probability ofexploring those states we exploit the random network dis tillation rnd mechanism which applies an explorationbonus to deep reinforcement learning methods to encouragethe exploration of the unseen part of the environment.
rndinvolves two neural networks a fixed and randomly initializedtarget network and a predictor network.
the target network n maps observed next state s t sto a real number s r and the predictor neural network n s ris trained with gradient descent to minimize the prediction error betweennand nwith respect to its parameters n. the prediction error can indicate the novelty of the explored states andhigher errors indicate novel states with the assumption thatthe network is likely to provide accurate predictions for thosestates that are explored before.
we calculate the predictionerror with the expected mean square error mse andencourage our method to explore more unseen states by max imizing mse.
mse is widely adopted as the loss function forregression problems.
compared to other loss functions used inthe regression problems mse is more sensitive to anomalousdata and thus we choose mse to encourage exploring unseenstates.
hence the in ris computed as follow inr n st n st finally we set coefficient for extrandinrrespectively and the reward can be computed as follow r inr inw extr extw train model the process of training model is corresponding to steps in algorithm .
all networks are trained with samples from replay buffer line .
the targetq value is computed by the target critic network and targetactor network line .
the estimated critic network updatesits parameter with loss function line .
next the policygradient of samples is calculated to train the estimated actornetwork line .
as for target actor network and targetcritic network they update their parameters by copying fromestimated networks periodically in a soft update way line .finally in line the predictor neural network for rnd istrained by minimizing the mse.
iv .
e v alua tion a. implementation our method is implemented in python .
and with tensorflow .
.
.
because it is difficult to access real cpssystems we choose to use the simulators instead.
the sw a tsimulator is implemented in python and the a t and ptc simulators are implemented in matlab.
theproject is publicly accessible .
b. research questions for evaluation we aim at the following research questions rq1 is figcps ef fective in generating failure inducing input for both continuous actions and high dimensionaldiscrete actions?
rq2 can figcps ef ficiently explore all unsafe states?
rq3 does the rnd mechanism improve the perfor mance of exploring unsafe states?
rq4 how does figcps perform compared with stateof the art approaches?
the first research question measures the generality of our approach i.e.
whether our approach is able to effectivelywork for both continuous and high dimensional discrete actionspaces.
the second research question evaluates how manydifferent unsafe states can our approach detect stably andquickly.
the third research question measures the effectivenessof the rnd mechanism.
the last research question comparesour approach against state of the art approaches to measurethe effectiveness of our approach horizontally.
559fig.
the a t simulator table i the evaluated properties for a t id formula 1 square 2 square v v 3 square parenleftbig parenleftbig g2 diamondmath g1 parenrightbig square g parenrightbig 4 square parenleftbig parenleftbig g1 diamondmath g1 parenrightbig square g1 parenrightbig 5 square4 logicalanddisplay i parenleftbig parenleftbig gi diamondmath gi parenrightbig square gi parenrightbig 6 square parenleftbig square square v v parenrightbig 7 squarev v 8 square diamondmath v v v 9 square square g is the engine rpm g is the gear and v is the v ehicle speed.
c. experiment settings we conduct our experiments on a desktop with a core .
ghz inter cpu and 128g bytes ram.
windows and matlab version 2018b are used.
we use three representativecps simulators for evaluation which are briefly introduced.
the secure water treatment sw at is a testbed located in singapore university of technology and design and is built for cyber security research purposes.
sw a t isa scaled down and fully operational raw water purificationplant capable of producing five gallons of safe drinking waterper minute.
the cyber part of sw a t covers programmablelogic controllers plcs a layered communications network human machine interfaces hmis a supervisory controland data acquisition scada workstation and a historian while data from sensors is available to the scada systemand recorded by the historian.
we use a simulator of sw a timplemented in python.
the simulator has level indicatortransmitter lit sensors i.e.
lit101 lit301 lit401 while lit101 has a reading between and lit301and lit401 have a reading between and .
there are26 actuators which are used to control the simulator and eachof them can be set to either or .
the output of the simulatoris a dimensional vector which consists of the readings ofthree sensors and two timestamps.
the input of the simulatoris a bit vector and each bit corresponds to an actuator.
the automatic transmission controller at is a simulator supported by mathworks as a matlab simulink example.
a t is a simulator of an automotive driving train and fig.3shows its architecture.
there are four parts in the simulator i.e.
the engine the transmission the v ehicle and the shift logic.
the inputs of the simulator are the readings of thethrottle and brake which are continuous values and the outputs fig.
the ptc simulator table ii the evaluated properties for ptc id formula 26 square .
27 square rise fall square .
30 square .
31 square .
32 square power diamondmath normal square .
33 square power .
.
34 square rise fall square .
.
is verification measurement while rise fall power and normal represent the operational mode.
contain three values i.e.
the engine rpm revolutions perminute the gear and the v ehicle speed.
the type of theengine rpm and the v ehicle speed are real values and aregreater than or equal to and the gear is a vector of fourdiscrete values g g2 g3 g4.
more detailed information on a t is available on the official website .
the power train controller ptc is a simulator provided by toyota technical center which simulates a controller of air fuel a f ratio for an internal combustionengine.
the architecture of ptc is shown in fig.
.
there aretwo components i.e.
the fuel control system fcs and thev erification and v alidation stub system vvss .
fcs takesthe pedal angle and the engine speed as input computes thetarget a f ratio i.e.
the a f ref in fig.
and then adjuststhe physical components to get the target ratio.
fcs alsosimulates the dynamics of the engine to get the real a f ratio i.e.
the a f in fig.
.
then vvss calculates the controlerror according to the a f and a f ref readings and outputsthe verification measurements.
the other output of ptc isthe operational mode which represents the different workingstates of the controller.
ptc has modes i.e.
startup normal power and fault.
in a few seconds after beginning ptc is inthe startup mode.
when the pedal angle stays in .
ptc goes into the normal mode and holds in that mode.
ptcwill go into the power mode when the pedal angle surpasses70 .
when a sensor fails ptc will go into the fault mode.
the sw a t system is chosen for the reason that it has an action space of and is a representative case for highdimensional discrete action space.
a t and ptc are chosen asrepresentatives of continuous action spaces due to their wideadoption in the literature.
for the sw a t simulator the unsafestates are decided by the valuation of the sensor readings.
forthe a t and ptc simulators the unsafe states are defined bythe future reach safety properties in which are shown in 560table iii the success rate and median time s taken to generate failure inducing inputs for sw a t with different sampling intervals method interval s lit overflow lit underflow success rate time success rate time success rate time success rate time success rate time success rate time figcps d1 .
.
.
.
.
.
figcps d rnd1 .
.
.
.
.
the best results are highlighted in bold.
table iv the success rate and median number of episodes of generating failure inducing inputs for the a t model propertyrand ddqn a3c figcps c figcps c rnd success rate number success rate number success rate number success rate number success rate number 1 .
.
2 3 .
.
.
4 .
.
5 6 7 0 8 9 .
av g .
.
.
.
the best results are highlighted in bold.
means there is no success runs to generate the failure inducing input or the average of the values is not valid.
a vg is the mean number of success rates of different methods.
rand is the random strategy.
table v the success rate and median number of episodes of generating failure inducing inputs for the ptc model propertyrand ddqn a3c figcps c figcps c rnd success rate number success rate number success rate number success rate number success rate number 26 .
27 30 .
.
31 .
32 .
.
.
33 .
34 .
av g .
.
.
.
.
the best results are highlighted in bold.
means there is no success runs to generate the failure inducing input or the average of the values is notvalid.
a vg is the mean number of success rates of different methods.
table vi the median number of time s taken to generate failure inducing input for sw a t compared withsmart fuzz method train h lit overflow lit underflow figcps d .
.
.
figcps d rnd .
.
lstm ga lstm rand svr ga svr rand train is the time of training prediction models hours .
the values in the last columns are the median time seconds used to generate the failure inducinginput the best results are highlighted in bold.
represents failing to reach the unsafe state in the given time limit.
the numbers marked with a star meanfailing to generate the failure inducing input for some experiment runs.
ga isthe genetic algorithm and rand represents the random strategy.
table i and table ii respectively.
an unsafe state is found if the property is evaluated to be false in that state.
we followthe reward definition proposed by yamagata et al.
in ourapproach.
for sw a t we use figcps d which is the ddpg algorithm combined with the mechanism of mapping fromcontinuous action space to discrete action space as the searchalgorithm to generate input.
we further enhance figcps dwith the rnd setting which increases the randomnessof searching and name the algorithm figcps d rnd.
we set the exploration rate learning rate and coefficient ofinternal reward to .
.
.
respectively.
for eachof the unsafe states e.g.
lit101 underflow we repeat theexperiment times to eliminate the effect of randomness.
ineach experiment we set the maximum number of iterations tobe episodes.
the sampling intervals are set to accumulatesensor reading changes.
we set different sampling intervals i.e.
1s 2s and 3s to observe the effect of time intervalson experimental results.
for a t and ptc we use figcps c which is the original ddpg algorithm to generate input forthe other two simulators.
similarly figcps c rnd refers to figcps ccombined with the rnd mechanism.
for a t we set the exploration rate learning rate and coefficientof the internal reward as .
.
and figcps c or figcps c rnd respectively.
for ptc we set the exploration rate learning rate and coefficient of the internalreward as .
.
for figcps c and .
.
for figcps c rnd respectively.
those hyper parameters are set based on experiment experiences.
for each property we repeat 561fig.
sensor readings of lit with figcps d rnd 3s interval the experiment times each of which has the maximum limitation of episodes.
d. experiment result rq1 and rq2 table iii shows the success rate and the execution time used for figcps dand figcps d rnd to reach the unsafe state with different sampling intervals i.e.
1s 2s 3s on sw a t. to eliminate the effect of randomnesscaused by the starting position we run our approach timesfor each targeted unsafe state and report the median number.
indicates that despite approaching the given unsafe state none of the repetitions were able to cross the threshold.
we canobserve that the sampling interval of seconds shows the moststable performance.
lit401 overflow lit101 underflow andlit301 underflow have a success rate for all samplingintervals.
for lit101 overflow lit overflow and lit 401underflow a larger sampling interval shows a higher successrate.
the reason is that the readings of lit101 lit301 andlit401 change slowly towards the target direction and thuslarge time intervals accumulate more changes which betterguide the reward.
fig.
shows the sensor reading of lit101with the figcps d rnd strategy.
the blue line shows the sensor readings while searching for the overflow state andthe green line shows the sensor readings for the underflowstate.
we can observe that the readings of the lit101 godown initially.
our approach learns the underflow strategy afteraround steps and learns the overflow strategy after over600 steps.
the trend is similar for lit301.
in general our approach is effective in generating failureinducing input for cps with high dimensional discrete actions.with a sampling interval of 3s our approach is able to exploreall unsafe states with a success rate except for the lit401 underflow state.
in terms of execution time our approachcan find three unsafe states in a few minutes and the othertwo within one hour including training time .
table iv shows the success rate and the median numbers of episodes of finding attacks for a t within simulations foreach formula and method with the sampling interval of tfig.
sensor readings of lit overflow withfigcps d rnd and figcps d 3s interval .
table v shows the success rate and the median numbersof episodes of generation for ptc within simulations foreach property and method with the sampling interval of t .
we use to indicate no value available.
the best resultsfor each property are highlighted in bold.
from table ivand table v we can observe that figcps ccan achieve the highest average success rate for both a t and ptc.
figcps c and figcps c rnd also have the smallest median number of episodes for the majority of the properties of a t and ptc.
our approach is effective in generating failure inducing input for cps with both continuous and highdimensional discrete action space.
our approach can also efficiently as indicated by the time andnumber of episodes cover failure states as indicated by thehigh success rate.
rq3 from table iii we can see that figcps d rnd outperforms figcps dwith less time for most cases and can achieve a higher success rate than figcps d indicating that the rnd mechanism can actually improve the performance of exploring unsafe states.
we further plot the sensor readings of lit301 overflow in the successful episode shown in fig.
as an ex ample to illustrate the differences between figcps dand figcps d rnd.
the blue line shows the sensor readings with figcps dand the green line shows the sensor readings with figcps d rnd.
we can observe that figcps d rnd learns the strategy faster than figcps dwith fewer steps required even with a disadvantaged starting point.
we checkall runs on the lit301 overflow and find that in runs figcps d rnd reach the unsafe state in less than episodes and in of the cases figcps d rnd is faster than figcps dwith fewer episodes and steps.
however table iv and table v show that figcps c performs a little bit better than figcps c rnd.
the reason for the relative worse performance of figcps c rnd on a t and ptc is that there are discrete variables in the statesof a t i.e.
the gear and ptc i.e.
the mode while thechange of these variables will lead to a big increase of internal 562table vii the relative effect size measure and p value of a t property methodrand ddqn a3c p p value p p value p p value 1figcps c .
.
.
.
.
figcps c rnd .
.
.
.
.
2figcps c .
.
.
.
.
figcps c rnd .
.
.
.
.
3figcps c .
.
.
figcps c rnd .
.
.
4figcps c .
.
.
figcps c rnd .
.
.
5figcps c .
.
.
.
.
figcps c rnd .
.
.
.
.
6figcps c .
.
.
figcps c rnd .
.
.
7figcps c .
.
.
figcps c rnd .
.
.
8figcps c .
.
.
.
figcps c rnd .
.
.
.
9figcps c .
.
.
.
figcps c rnd .
.
.
pis the relative effect size measure and the value less than .
highlighted in bold blue means the methods reported in rows are better than that reported in columns e.g.
ddpg is better thanrand on properties better than ddqn a3c on properties p value measures the significance of p and the value less than .
highlighted in bold means there is a significant difference betweenthe two groups of data e.g.
ddpg is significantly better than rand ddqn a3c on and properties .
rewardinr.
for such variables it takes a long time to generate the appropriate input.
according to the rnd mechanism is sufficient to deal with local exploration i.e.
exploring the consequences of short term decisions whileglobal exploration that involves coordinated decisions overlong time horizons is beyond the reach of rnd.
hence rndwill be counterproductive for some properties that need thesystem to keep the discrete variable constant over a long periodof time e.g.
34for ptc.
rnd generally improves the exploration performance with exceptions on specific unsafe states thatrequire stable status over long time horizons.
rq4 table vi reports the results on sw a t compared with the state of the art approach smart fuzz .
indicates that despite approaching the given unsafe state none of the repetitions were able to cross the threshold.
we can see thatfigcps d rnd achieves compatible results on the lit101 underflow attack and better results on lit401 overflow lit301 underflow attacks.
specifically our method is ableto find the attack of lit301 underflow with successrate and lit401 underflow which smart fuzz fails to find inthe given time limit.
these unsafe states have been reportedin an established benchmark of sw a t network attacks which was manually crafted by experts.
our method performsa bit worse on the lit101 overflow and lit301 overflowattacks.
the reason is that for lit101 and lit301 thereis a higher probability of selecting actuator configurationswhich drive the sensor readings to sink as illustrated by fig.
.therefore our method needs more time to learn the strategyto drive the sensor readings to rise on the fly while smartfuzz leverages a trained model for prediction and itsinput generation process does not involve training.
note that smart fuzz trains prediction models e.g.
lstm and svr with days data logs collected.
the trainingtable viii the relative effect size measure and p valueof ptc property methodrand ddqn a3c p p value p p value p p value 26figcps c .
.
.
.
.
figcps c rnd .
.
.
.
.
27figcps c .
.
.
figcps c rnd .
.
.
30figcps c .
.
.
figcps c rnd .
.
.
.
31figcps c .
.
.
.
.
figcps c rnd .
.
.
.
.
32figcps c .
.
.
.
figcps c rnd .
.
.
33figcps c .
.
.
.
figcps c rnd .
.
.
.
.
34figcps c .
.
.
.
figcps c rnd .
.
.
.
pis the relative effect size measure and the value less than .
highlighted in bold blue means the methods reported in rows are better than that reported in columns e.g.
ddpg is better than randon properties better than ddqn a3c on properties p value measures the significance of p and value less than .
highlighted in bold means there is a significant difference between the twogroups of data e.g.
ddpg is significantly better than rand andddqn a3c on and properties .
takes days for lstm and half a day for svr.
our method learns the strategy on the fly and does not require the pre training effort i.e.
the training process is integrated with thesearching process.
therefore the time for our method includesthe training time and the generation time and that s why ourmethod needs more time for lit101 overflow and lit301overflow attacks.
we also add the training time of smartfuzz for comparison.
lit401 underflow is the most difficult case which smart fuzz fails to find and our method only achieves a successrate of .
through a detailed analysis we find that thereare only a very small number of actuator configurations whichcan make the reading of lit401 decrease.
moreover thereading of lit401 decreases with a small amplitude ofthe amount of increase in each step.
therefore smart fuzzcan hardly come across the inputs making the readings oflit401 decrease due to its low probability which makes thecorresponding fitness value negligible.
as for our approach the figcps d rnd strategy shows its advantages in finding the unsafe state due to the rnd network which guidesthe searching towards unseen states and thus increases theprobability of exploring the rare underflow readings.
we also reproduce the results of the state of the art approach as a comparison.
the distribution of the numberof episodes is highly non normal so we choose to reportthe median number rather than the average number in ourevaluation.
from table iv we can observe that our methodis better than the other approaches on most properties on boththe success rate and the median number of episodes for a t. there is an exception that our method has failed for .w e conduct further experiments for 7with t and the success rate of our method can get and respectively.
7is a property that only constraints that the vehicle speed does not exceed the limit while the vehicle speed has verylittle variation in a short period of time which makes thereward change very small.
besides the large sample rate willincrease the complexity of the state so that the difficulty of the 563search will increase correspondingly.
as ddqn outperforms ddpg in some simple cases our method fails for 7 with t while ddqn can succeed with a lower success rate.
as for a3c the asynchronous mechanism can improveits data utilization so that it can also succeed with t .
table v shows the results of the compared method for ptc.
we can observe that our method has better performancefor ptc than the compared methods and figcps cperforms the best for most properties.
the rnd network shows perfor mance gain on some properties i.e.
.
to eliminate the effect of randomness in our experiment results and examine the significance of the results we furtherconduct statistical testing on the reported results.
we adopt the relative effect size measure to test the data of different methods.
the relative effect size measure betweentwo random values x y is defined as p p x y 2p x y ifp .
it indicates that x is probably larger than y and vice versa.
phas a close relationship to cliff s delta which is an effect size measurement to verify the differencebetween the two sets of data.
the relative effect size measureis ordinal statistics which is calculated according to the relativeorder of different data sets which does not require the normaldistribution assumption on the data under test and issuitable for our problem.
we further conduct hypothesis testing to test the significance of the effect size measurement.
we set the signifi cance level as .
and perform multiple comparisons betweenour methods and approaches in .
table vii and table viii show the relative effect size measure ptogether with the significance level p value of hypothesis testing for the a t and ptc respectively.
pis the relative effect size measure and the value less than .
highlighted in bold blue means the methods reported in rowsare better than those reported in columns.
for example intable vii figcps cis better than rand on properties 1 2 3 4 6 and 9 better than ddqn a3c on properties all except 7 .
p value measures the significance ofp and the value less than .
highlighted in bold means there is a significant difference between the two groups of data.for instance in table vii figcps cis significantly faster than a3c on properties 1 2 3 4 6 8 and 9 .
in table vii we can observe that figcps cis significantly faster than rand ddqn and a3c on and7 properties respectively.
figcps c rnd is significantly faster than rand ddqn a3c on and properties.table viii shows that for ptc figcps cis significantly faster than rand ddqn a3c on and properties figcps c rnd is significantly faster than rand ddqn a3c on and properties.
our approach outperforms state of the art approaches for both continuous and discrete actions.
particularly we find new unsafe states for sw a t which were notfound by other approaches.e.
threats to v alidity threats to internal validity mainly come from our implementation of the source code.
to eliminate the threats two co authors manually review the source code.
threats to external validity mainly come from the simulators and the libraries used to implement our approach.
the simulators we adopted are all widely adopted in the literature and arewell tested.
the libraries we adopted in our implementationare also reliable and widely used.
threats to construction validity mainly lie in the randomness in our experiment.
to eliminate the threats we randomly simulate multiple times and used the average value in ourevaluation.
moreover we also conduct statistical testing toevaluate the significance of our results.
v. r ela ted work there are many approaches testing cpss and they can be divided into two categories i.e.
model based testing mbt and search based testing sbt .
mbt methods exploit modelswhich define the correctness criteria of cpss to generate testinputs automatically .
clarke et al.
have proposed toadopt statistical model checking techniques to test cpss anduse cross entropy to sample rare events.
buzhinsky et al.
proposed a framework to test the control software of cpss using the modular formal language nces from the perspectiveof formal verification technology.
based on the nonlinear arti ficial neural network model kosek and gehrke proposedan anomaly detection method to efficiently detect and evaluatethe exception in cyber physical intrusion in smart grids.
mbt methods are usually expensive time consuming and hard to apply.
moreover it is difficult to capture complexcontinuous dynamics and interactions between the system andits environment.
furthermore the input space of a cps isusually extremely large with these inputs associated with re quirements which again bring great challenges for thoroughlytesting a given cps.
to address the above mentioned chal lenges search based testing sbt methods employingsearch algorithms to test cpss are proposed.
matinnejad etal.
test simulink models by using a search algorithmto generate a small set of test cases.
ben abdessalem etal.
verify automatic driving assistant systems using multi objective optimization algorithms combined with neural net works.
our approach is more closely related to search basedtesting methods and thus we discuss those approaches in detail.
robustness guided falsification of cps the quantitative semantics based on metric temporal logic mtl and its variant signal temporal logic stl are adoptedin robustness guided falsification methods.
the robustness ofan mtl formula is defined as a numeric measurement of how robust a property holds in the given cps system.
given thequantitative definition of robustness the problem of falsifyingrobustness properties of a cps system is converted into thenumerical minimization problem which searches the systemparameters that minimize the robustness value.
the robustness guided falsification approaches can be classified into black boxmethods and grey box methods.
the black box methods can further be divided into methods that translate the robustness guided falsification problem into 564a global optimization problem and the methods based on the statistical modeling.
there are a large number of methodsbased on global optimization techniques such as methodsusing simulated annealing genetic algorithms andgradient descent .
methods that use simulated anneal ing transform the falsification problem into a minimizationproblem with normative robustness on the set of all systemtrajectories.
the method adopting the genetic algorithm usesthe thought of ant colony optimization in the process offalsification input generation.
the gradient descent methodparameterizes the system input imposes small disturbanceson the system input in space and time and uses the gradientdescent method to converge to the worst local system behavior.the methods using statistical modeling include the cross entropy method and gaussian regression .
thecross entropy ce method samples the input proba bility distribution approximating the distribution caused bythe robustness value of trajectories.
the gaussian regressionmethod is used to introduce the falsification process into thedomain estimation problem and then the gaussian process isused to construct the approximate probability semantics of thetemporal formulas thereby providing a higher probability forthe area where the formula is falsified.
the grey box approaches are relevant to our work.
plaku et al.
propose a method based on rapidly exploring randomtree rrt which gradually generates the next operation bysampling the valid inputs.
dressosi et al.
propose an rrt search method guided by robustness.
yamagata et al.
use deep reinforcement learning drl techniques a3c ddqn to solve the problem.
unlike dressosi et al.
drlconforms to the guided search process by predicting robustnesslearned from the system s previous behaviours.
fuzzing based methods fuzzing is a random testing technique that automatically generates inputs randomly or with feedback guidance .
since being proposed fuzzing hasbeen widely adopted in different testing scenarios includingdeep reinforcement fuzzing network protocols web browsers etc.
there are a lot of notable toolsthat have performed fuzzing on their programs such as .american fuzzing lop aims at programs and makes use ofgenetic algorithms to increase the code coverage of test casesand detect more bugs.
fuzzing has been adopted widely in fieldof cps testing.
cyfuzz is an example designed for testingsimulink models of cpss.
it identifies problematic compo nents in the simulink modeling environment by studyingpublicly available bug reports.
cpfuzz combines fuzzingand falsification to find safety violations at the developmentphase for cpss.
wijaya et al.
have proposed an approachfor construction models of anomaly detectors using supervisedlearning from the traces of normal and abnormal operation ofcpss.
chen et al.
have adopted fuzzing at the networklevel of cps.
specifically it uses fuzzing of actuators withcommands which is not generated by the plcs.
it overrides thereal commands with the fuzzed commands to drive physicalsensors out of their safety threshold.
chen et al.
adoptedactive learning to the pre training of fuzzing to improve theefficiency of fuzzing for cpss.
these fuzzing based methodsall need system logs or network packets for cpss to test whilesuch messages are not always available.
our approach makesno assumptions on those information and directly interactswith the cps models.
our method employs drl to generate test inputs for cps.
actually there have been some existing works which use drlfor test input generation.
reddy et al.
proposed a solutionbased on reinforcement learning rl using a tabular on policy rl approach to guide the generator which can generatevalid test inputs for programs.
kim et al.
used ddqn toreplace human designed meta heuristic algorithms in search based software testing.
these two approaches only target thetraditional software.
qin et al.
proposed an interactivemulti agent framework using dqn in which the system under design is shaped as an ego agent and its environment ismodeled by numerous adversarial agents.
they evaluated theirmethod on two real world case studies from the field of self driving cars.
their approach is only applicable to the cpssystems with very simple inputs e.g.
discrete inputs fortheir case studies and is hard to handle real situations withvery complex inputs e.g.
dimensional input in sw a t dueto the limitations of dqn.
vi.
c onclusion we propose a method to generate failure inducing input for cpss effectively using deep reinforcement learning.
we targetboth continuous actions and high dimensional discrete actions and design strategies to cater to both scenarios.
moreover we incorporate random network distillation rnd a cu riosity mechanism to improve the capability of exploring rarestates.
we experimentally evaluate our method with three cpssimulators including continuous action and high dimensionaldiscrete action as input and compare the performance ofour method with state of the art approaches i.e.
smart fuzz a3c and ddqn.
the experiment results show that ourmethod is able to effectively and efficiently generate failure inducing input that can cover all targeted unsafe states withhigh success rates.
moreover our approach outperforms state of the art approaches on both success rate and the number ofiterations.
in particular our method is able to find two newunsafe states which are not detected by previous approaches.
there are still some limitations which require further improvement.
first our method has only been evaluated on threecps simulators more evaluations on large scale real systemscould better validate the effectiveness and scaling capability ofour work.
second due to the nature of the drl algorithm thatthe training process and the searching process are integrated our method needs a relatively long time to generate failure inducing inputs under some circumstances.
therefore moreexploration on reducing the generation time is required.
a cknowledgment this work is partially supported by the nsfc y outh funds under grant the nsfc general technology basicresearch joint funds under grant u1836214 the nationalresearch foundation singapore under its nsoe programme award number nsoe dest the new genera tion of artificial intelligence science and technology majorproject of tianjin under grant 19zxzngx00010.
565references u. n. s. foundation cyber physical systems cps gov publications pubsumm.jsp?odskey nsf18538 org nsf document number nsf18538.
accessed april .
r. rajkumar i. lee l. sha and j. stankovic cyber physical systems the next computing revolution in design automation conference.
ieee pp.
.
a. hassanzadeh a. rasekh s. galelli m. aghashahi r. taormina a. ostfeld and m. k. banks a review of cybersecurity incidents inthe water sector journal of environmental engineering vol.
no.
p. .
i. c. alert cyber attack against ukrainian critical infrastructure docu ment number ir alert h .
accessed april .
j. leyden water treatment plant hacked chemical mix changed for tap supplies utility hacked accessed april .
y .
chen c. m. poskitt and j. sun learning from mutants using code mutation to learn and monitor invariants of a cyber physical system in2018 ieee symposium on security and privacy sp .
ieee pp.
.
y .
chen b. xuan c. m. poskitt j. sun and f. zhang active fuzzing for testing and securing cyber physical systems in proceedings of the 29th acm sigsoft international symposium on software testing andanalysis pp.
.
y .
chen c. m. poskitt and j. sun towards learning and verifying invariants of cyber physical systems by code mutation in international symposium on f ormal methods.
springer pp.
.
j. inoue y .
yamagata y .
chen c. m. poskitt and j. sun anomaly detection for a water treatment system using unsupervised machinelearning in ieee international conference on data mining workshops icdmw .
ieee pp.
.
h. choi w. c. lee y .
aafer f. fei z. tu x. zhang d. xu and x. deng detecting attacks against robotic vehicles a control invariantapproach in proceedings of the acm sigsac conference on computer and communications security pp.
.
j. giraldo d. urbina a. cardenas j. v alente m. faisal j. ruths n. o. tippenhauer h. sandberg and r. candell a survey of physics basedattack detection in cyber physical systems acm computing surveys csur vol.
no.
pp.
.
y .
chen c. m. poskitt j. sun s. adepu and f. zhang learningguided network fuzzing for testing cyber physical system defences in2019 34th ieee acm international conference on automated softwareengineering ase .
ieee pp.
.
h. abbas and g. fainekos convergence proofs for simulated annealing falsification of safety properties in 50th annual allerton conference on communication control and computing allerton .
ieee pp.
.
y .
s. r. annapureddy and g. e. fainekos ant colonies for temporal logic falsification of hybrid systems in iecon 36th annual conference on ieee industrial electronics society.
ieee pp.
.
s. yaghoubi and g. fainekos falsification of temporal logic requirements using gradient based local search in space and time if acpapersonline vol.
no.
pp.
.
s. sankaranarayanan and g. fainekos falsification of temporal properties of hybrid systems using the cross entropy method in proceedings of the 15th acm international conference on hybrid systems compu tation and control pp.
.
t. akazaki falsification of conditional safety properties for cyberphysical systems with gaussian process regression in international conference on runtime v erification.
springer pp.
.
s. silvetti a. policriti and l. bortolussi an active learning approach to the falsification of black box cyber physical systems in international conference on integrated f ormal methods.
springer pp.
.
j. deshmukh m. horvat x. jin r. majumdar and v .
s. prabhu testing cyber physical systems through bayesian optimization acm transactions on embedded computing systems tecs vol.
no.
5s pp.
.
y .
yamagata s. liu t. akazaki y .
duan and j. hao falsification of cyber physical systems using deep reinforcement learning ieee transactions on software engineering .
s. gu t. lillicrap i. sutskever and s. levine continuous deep qlearning with model based acceleration in international conference on machine learning.
pmlr pp.
.
v .
mnih a. p .
badia m. mirza a. graves t. lillicrap t. harley d. silver and k. kavukcuoglu asynchronous methods for deep rein forcement learning in international conference on machine learning.
pmlr pp.
.
g. dulac arnold r. evans h. van hasselt p .
sunehag t. lillicrap j. hunt t. mann t. weber t. degris and b. coppin deep re inforcement learning in large discrete action spaces arxiv preprint arxiv .
.
r. s. sutton and a. g. barto reinforcement learning an introduction.
mit press .
sutton richard s. and barto andrew g. reinforcement learning an introduction.
cambridge ma usa a bradford book .
v .
mnih k. kavukcuoglu d. silver a. graves i. antonoglou d. wierstra and m. riedmiller playing atari with deep reinforcement learning arxiv preprint arxiv .
.
r. s. sutton d. a. mcallester s. p .
singh y .
mansour et al.
policy gradient methods for reinforcement learning with function approximation.
in nips vol.
.
citeseer pp.
.
t. p .
lillicrap j. j. hunt a. pritzel n. heess t. erez y .
tassa d. silver and d. wierstra continuous control with deep reinforcementlearning arxiv preprint arxiv .
.
y .
burda h. edwards a. storkey and o. klimov exploration by random network distillation arxiv preprint arxiv .
.
h. marmolin subjective mse measures ieee transactions on systems man and cybernetics vol.
no.
pp.
.
secure water treatment swat sg itrust labs home itrust labsswat accessed april .
modeling an automatic transmission controller accessed april .
x. jin j. v .
deshmukh j. kapinski k. ueda and k. butts powertrain control verification benchmark in proceedings of the 17th international conference on hybrid systems computation and control pp.
.
j. goh s. adepu k. n. junejo and a. mathur a dataset to support research in the design of secure water treatment systems in international conference on critical information infrastructures security.
springer pp.
.
s. chen comparing deep reinforcement learning methods for engineering applications ph.d. dissertation master dissertation faculty ofcomputer science otto von guericke ... .
f. konietschke m. placzek f. schaarschmidt and l. a. hothorn nparcomp an r software package for nonparametric multiple comparisonsand simultaneous confidence intervals journal of statistical software nr .
vol.
no.
pp.
.
n. cliff ordinal methods for behavioral data analysis.
psychology press .
a. aerts m. reniers and m. r. mousavi model based testing of cyber physical systems in cyber physical systems.
elsevier pp.
.
e. m. clarke and p .
zuliani statistical model checking for cyberphysical systems in international symposium on automated technology for verification and analysis.
springer pp.
.
i. buzhinsky c. pang and v .
vyatkin formal modeling of testing software for cyber physical automation systems in ieee trustcom bigdatase ispa vol.
.
ieee pp.
.
a. m. kosek and o. gehrke ensemble regression model based anomaly detection for cyber physical intrusion detection in smart grids in2016 ieee electrical power and energy conference epec .
ieee pp.
.
a. arrieta s. wang u. markiegi g. sagardui and l. etxeberria search based test case generation for cyber physical systems in ieee congress on evolutionary computation cec .
ieee pp.
.
r. matinnejad s. nejati l. c. briand and t. bruckmann automated test suite generation for time continuous simulink models in proceedings of the 38th international conference on software engineering pp.
.
r. ben abdessalem s. nejati l. c. briand and t. stifter testing advanced driver assistance systems using multi objective search andneural networks in proceedings of the 31st ieee acm international conference on automated software engineering pp.
.
r. koymans specifying real time properties with metric temporal logic real time systems vol.
no.
pp.
.
o. maler and d. nickovic monitoring temporal properties of continuous signals in f ormal techniques modelling and analysis of timed and fault tolerant systems.
springer pp.
.
a. donz e and o. maler robust satisfaction of temporal logic over real valued signals in international conference on f ormal modeling and analysis of timed systems.
springer pp.
.
e. plaku l. e. kavraki and m. y .
v ardi falsification of ltl safety properties in hybrid systems in international conference on tools and algorithms for the construction and analysis of systems.
springer pp.
.
t. dreossi t. dang a. donz e j. kapinski x. jin and j. v .
deshmukh efficient guiding strategies for testing of temporal properties of hybridsystems in nasa f ormal methods symposium.
springer pp.
.
a. takanen j. d. demott c. miller and a. kettunen fuzzing for software security testing and quality assurance.
artech house .
k. b ottinger p .
godefroid and r. singh deep reinforcement fuzzing in2018 ieee security and privacy workshops spw .
ieee pp.
.
g. vigna w. robertson and d. balzarotti testing network based intrusion detection signatures using mutant exploits in proceedings of the 11th acm conference on computer and communications security pp.
.
t. guo p .
zhang x. wang and q. wei gramfuzz fuzzing testing of web browsers based on grammar analysis and structural mutation in2013 second international conference on informatics applications icia .
ieee pp.
.
j. wang b. chen l. wei and y .
liu superion grammar aware greybox fuzzing in ieee acm 41st international conference on software engineering icse .
ieee pp.
.
s. k. cha m. woo and d. brumley program adaptive mutational fuzzing in ieee symposium on security and privacy.
ieee pp.
.
m. zalewski american fuzzy lop accessed april .
s. a. chowdhury t. t. johnson and c. csallner cyfuzz a differential testing framework for cyber physical systems development environ ments in international workshop on design modeling and evaluation of cyber physical systems.
springer pp.
.
f. shang b. wang t. li j. tian and k. cao cpfuzz combining fuzzing and falsification of cyber physical systems ieee access vol.
pp.
.
h. wijaya m. aniche and a. mathur domain based fuzzing for supervised learning of anomaly detection in cyber physical systems inproceedings of the ieee acm 42nd international conference on software engineering workshops pp.
.
s. reddy c. lemieux r. padhye and k. sen quickly generating diverse valid test inputs with reinforcement learning in ieee acm 42nd international conference on software engineering icse .
ieee pp.
.
j. kim m. kwon and s. y oo generating test input with deep reinforcement learning in ieee acm 11th international workshop on search based software testing sbst .
ieee pp.
.
x. qin n. ar echiga a. best and j. deshmukh automatic testing and falsification with dynamically constrained reinforcement learning arxiv preprint arxiv .
.