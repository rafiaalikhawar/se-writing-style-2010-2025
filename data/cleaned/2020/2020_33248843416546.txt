code to comment translation data metrics baselining evaluation david gros hariharan sezhiyan prem devanbu zhou yu university of california davis dgros hsezhiyan devanbu joyu ucdavis.edu abstract therelationshipofcommentstocode andinparticular thetask of generating useful comments given the code has long been of interest.theearliestapproacheshavebeenbasedonstrongsyntactic theories of comment structures and relied on textual templates.
more recently researchers have applied deep learning methods to this task specifically trainable generative translation models which are known to work very well for natural language translation e.g.
from german to english .
we carefully examine the underlying assumption here that the task of generating comments sufficientlyresemblesthetaskoftranslatingbetweennaturallanguages andsosimilarmodelsandevaluationmetricscouldbeused.
we analyze several recent code comment datasets for this task codenn deepcom funcom and docstring.
we compare them withwmt19 astandarddatasetfrequentlyusedtotrainstate ofthe art natural language translators.
we found some interesting differences between the code comment data and the wmt19 naturallanguagedata.next wedescribeandconductsomestudiesto calibratebleu whichiscommonlyusedasameasureofcomment quality .using affinitypairs ofmethods fromdifferentprojects in the same project in the same class etc our study suggests that the current performance on some datasets might need to be improvedsubstantially.wealsoarguethatfairlynaiveinformation retrieval ir methods do well enough at this task to be considered areasonablebaseline.finally wemakesomesuggestionsonhow our findings might be used in future research in this area.
acm reference format david gros hariharan sezhiyan prem devanbu zhou yu.
.
code to comment translation data metrics baselining evaluation.
in 35th ieee acminternationalconferenceonautomatedsoftwareengineering ase september21 virtualevent australia.
acm newyork ny usa pages.
introduction programmersaddcommentstocodetohelpcomprehension.the value of these comments is well understood and accepted.
a wide variety of comments exist in code including prefix comments standardized in frameworks like javadocs which are inserted before functions or methods or modules to describe their function.
given the value of comments and the effort required to write ase september virtual event australia copyright held by the owner author s .
acm isbn .
.
.
.
figure distribution of trigrams in english blue in the wmt german english machine translation dataset andinenglishcommentsfromseveralpreviouslypublishedcode comment datasets them there has been considerable interest in providing automated assistancetohelpdeveloperstoproducecomments andavariety of approaches have been proposed .
comments especiallyprefixcomments aretypicallyexpected to be a useful summary of the function of the accompanying code.
comments could be viewed as a restatement of the semantics of the code in a different and more accessible natural language thus itispossibletoviewcommentgenerationasakindoftranslation task translating from one programming language to a another natural language.thisview togetherwiththeverylargevolumes ofcode withaccompanyingcomments availableinopen source projects offerstheveryappealingpossibilityofleveragingdecades ofresearchin statisticalnaturallanguagetranslation nlt .if it s possible to learn to translate from one language to another fromdata why not learn to synthesize comments from code?
severalrecent papers have explored the idea of applying statisticalmachinetranslation smt methodsto learntotranslate code to an english comments.
but are these tasks really similar?
we are interested to understand in more detail how similar the taskofgeneratingcommentsfromcodeistothetaskoftranslating between natural languages.
commentsformadomain specificdialect whichishighlystructured withalotofveryrepetitivetemplates.commentsoftenbegin with patterns like returns the outputs the and calculates the .
indeed most ofthe earlierwork whichwasn t basedonmachine authors contributed equally 35th ieee acm international conference on automated software engineering ase this work is licensed under a creative commons attribution international .
license.
learning onthisproblemhasleveragedthishighlytemplatednature of comments .
we can see this phenomenon clearly usingzipfplots.figure1comparesthetrigramfrequenciesoftheenglish language text in comments from the datasets that havebeenusedtotraindeep learningmodelsforcode comment summarization and english language text in the wmt germanenglishtranslationdataset thex axisordersthetrigramsfrommost to least frequent using a log rank scale and the y axis is the log relativefrequency ofthetrigrams inthe corpus.the englishfound in wmt datasetis the magenta line atthe bottom.
the comments fromcodeshowconsistentlyhigherslopeinthe note log scaled y axis of the zipf plot suggesting that comments are far more saturated with repeating trigrams than is the english found in the translationdatasets.thisobservationmotivatesacloserexamination of the differences between code comment and wmt datasets andtheimplicationsofusingmachinetranslationapproachesfor code comment generation.
in this paper we compare code comment translation cct datasets used with dl models for the task of comment generation with a popular natural translation wmt dataset used fortrainingdlmodelsfornaturallanguagetranslation.thesewere our results we find that the desired outputs for the cct task are much more repetitive.
we find that the repetitiveness has a very strong effect on measuredperformance muchmoresointhecct datasets than the wmt dataset.
we find that the wmt translation dataset has a smoother morerobustinput outputdependency.similargermaninputs in wmt have a strong tendency to produce similarenglish outputs.
however this does appear to hold in the cct datasets.
wereportthatanaiveinformationretrievalapproachcan meet or exceed reported numbers from neural models.
weevaluatebleu perseasameasureofgeneratedcomment quality using groups of methods of varying affinity this offers new perspectives on the bleu measure.
ourfindingshaveseveralimplicationsforthefutureworkinthe area intermsoftechnicalapproaches waysofmeasurement for baselining andforcalibratingbleuscores.webeginbelowbyfirst providing some background we then describe the datasets used in prior work.
we then present an analysis of the datasets and and an analysis of the evaluation metrics and baselines used.
we conclude after a detailed discussion of the implications of this work.
but first a disclaimer this work does not offer any new models for or improvements on prior results on the cct task.
it is primarily retrospective viz a critical review of materials evaluations usedinpriorworkincct offeredinacollegialspirit hopingto advancethe wayour communityviews thetaskof code commenttranslation and how we might together make further advances in the measurement and evaluation of innovations that are addressed in this task.
background theory the value of comments in code comprehension has been wellestablished .
however developers findit challenging tocreate maintainusefulcomments .thishassparkedalonglineof researchlookingintotheproblemofcommentgeneration.anearly line ofwork wasrule based combining someform analysisofthesourcecodetoextractspecificinformation which couldthenbeslottedintodifferenttypesoftemplatestoproduce comments.
another approach was to use code clone identification to produce comments for given code using the comments associatedwithaclone .otherapproachesusedkeywordswhich programmersseemtoattendtoineye trackingstudies .still other approaches use topic analysis to organize descriptions of code .
most of the pioneeering approaches above relied on specific featuresandruleshand engineeredforthetaskofcommentgeneration.
theadventoflargeopen sourcerepositorieswithlargevolumesof source code offered a novel general statistically rigorous possibility thattheselargedatasetsbeminedforcode commentpairs which could then be used to train a model to produce comments fromcode.thesuccessofclassicstatisticalmachinetranslation offered a tempting preview of this using large amounts of aligned pairs of utterances in languages a b it was possible to learn a conditional distribution of the form pt b a wherea a and b b given an utterance b one could produce a possible translation aby simply setting argmax apt a statistical natural language translation approaches which were already highly performant were further enhanced by deep learning dl .ratherthanrelyingonspecificinductivebiaseslikephrasestructures in the case of classical smt dl held the promise thatthe features relevant to translation could themselves be learned fromlargevolumesofdata.dlapproacheshaveledtophenomenal improvements in translation quality .
several recent papers haveexploredusingthesepowerfuldlapproaches to the code comment task.
iyeretal.
firstapplieddltothistask usingcode english pairs mined from stack overflow using simple attention over inputcode andanlstmtogenerateoutputs.manyotherpapers followed which are discussed below in section .
.
we analyze the published literature starting with the question of whether there arenotabledistributionaldifferencesbetweenthecode comment translation cct and the statistical machine translation wmt data.
our studies examine the distributions of the input and output data and the dependence of the output on the input.
rq1.whatarethedifferencesbetweenthetranslation wmt data and code comment cct data?
next weexaminewhetherthesedifferencesactuallyaffectthe performance of translationmodels.
in earlier work allamanis pointed out the effects of data duplication on machine learning applications in software engineering.
we study the effectsof data duplication as well as the effects of distributional differences ondeep learning models.
one important aspect of smt datasets isinput ouput dependence.
in translation e.g.from german de to english en similar input de sentences will to produce similaroutput en sentences and less similar de sentences will tend to 747producelesssimilarensentences.thissamecorrelationmightnot apply in cct datasets.
rq2.
how the distributional differences in the smt cct datasets affect the measured performance?
there s another important difference between code and natural language.
small differences such as substituting for and a for a can make the difference between a sumand afactorialfunction likewise changingone functionidentifier mean rather than variance .thesesmallchangesshouldresultinalargechangein theassociatedcomment.likewise therearemanydifferentways to write a sort function all of which might entail the same comment.intuitively thiswouldappeartobelessofanissueinnatural languages since as they have evolved for consequential communicationinnoisyenvironments meaningshouldberobusttosmallchanges.thusonthewhole wemightexpectthatsmallchanges in german should in general result in only small changes in the english translation.
code on the other hand being a fiatlanguage might not be in general as robust and so small changes in codemay result in unpredictable changes in the associated comment.
why does this matter?
in general modern machine translation methods use the generalized function approximation capability of deep learning models.
if natural language translation wmt has a morefunctionaldependency andcctdoesn t thereisasuggestion that deep learning models would find cct a greater challenge.
rq3.dosimilarinputsproducesimilaroutputsinbothwmt and cct datasets?
prior work in natural language generation has shown that informationretrieval ir methodscanbeeffectivewaysofproducing suitable outputs.
these methods match a new input with semantically similar inputs in the training data and return the associated output.theseapproachescansometimesperformquitewell andhasbeenpreviouslyappliedsuccessfullytothetaskofcomment generation .
our goal here is to ask whether ir methods could be a relevant useful baseline for cct tasks.
rq4.howdotheperformanceofnaiveinformationretrieval ir methods compare across wmt cct datasets?
finally wecriticallyevaluatetheuseofbleuscoresinthistask.
given the differences we found between datasets used for training smttranslatorsandthecode commentdatasets wefeltitwouldbe importanttounderstandhowbleuisusedinthistask anddevelop some empirical baselines to calibrate the observed bleu values in priorwork.howgoodarethebest in classbleuscores associated with thebest current methods for generatingcomments given the source of a method ?
are they only as good as simply retrieving a comment associated with a randommethod in a different project?
hopefullythey remuchbetter.howaboutthecommentassociated with a random method from the same project?
with a random method in the same class?
with a method that could reasonably be assumed quite similar?
rq5.
how has bleu been used in prior work for the codecomment task and how should we view the measured per formance?in the next section we review the datasets that we use in our study.
datasets used we examinethecharacteristicsoffourcctdatasets namelycodenn deepcom funcom docstring and one standard widelyusedmachine translationdataset thewmtdataset.webeginwith a description of each dataset.
within some of the cct datasets we observe that the more popular ones can include several different variations this is because follow on work has sometimes gathered processed and partitioned training validation test the dataset differently.
codenn iyeretal wasanearlycctdataset collectedfrom stackoverflow with code comment pairs for c and sql.
stackoverflow posts consist of a title a question and a set of answers which may contain code snippets.
each pair consists of the title andcodesnippet fromanswers.iyer etalgatheredaroundamillion pairs each for c and sql from these focusing on just snippetsinaccepted answers they filtered down to pairs for c and pairs for sql.
from these they used a trained model trainedusingahand labeledset tofilteroutuninformativetitles e.g.
how can make this complicated query simpler to higher quality pairs for c and for sql.
in our analysis we usedonlythec data.stackoverflowhasawell knowncommunity norm to avoid redundant q a repeated questions are typically referredtotheearlierpost.asaresult thisdatasethas significantly less duplication.
the other cct datasets are different.
deepcom huetal.
generateacctdatasetbymining9 javaprojects.fromthisdataset theyfilteroutmethodsthathave javadoccomments andselectonlythosethathaveatleastone word descriptions.
they also exclude getters setters constructors and test methods.
this leaves them with method comment pairs.
in this dataset the methods code are represented as serialized asts after parsing by eclipse jdt.
later hu et al .
updated their dataset and model to a size of examples.
we refer to the former as deepcom1 and obtain a copy online from followup work2.
we refer to the latter as deepcom2 and obtain a copy online3.
in addition deepcom2 is distributedwitha10 foldsplitinthecross projectsetting examples inthetestsetarefromdifferentprojects .inhuetal .
thisis referredtothe rq 4split buttoavoidconfusionwithourresearch questions we refer to it as deepcom2f.
funcom leclairetal .
startedwiththesourcerer repo with over51mmethodsfrom50kprojects.fromthis theyfilteredout methodswithjavadoccommentsinenglish andthenalsothecommentsthatwereauto generated.thisleavesabout2.1mmethods with patched javadoc comments.
the source code was parsed into anast.theycreatedtwodatasets the standard whichretainedthe originalidentifiers and challenge whereintheidentifiers except for java api class names were replaced with a standardized token.
they also made sure no data from the same project was duplicated across training and or validation and or test.
notably the funcom 748dataset only considers the first sentence of the comment.
additionally code longer than words and comments longer words were truncated.
like for deepcom there are several versions of this dataset.
we consider a version from leclair et al .
as funcom1 and the version from leclair and mcmillan as funcom2.
these datasets are nearly identical but funcom2 has about fewer examples and the two versions have reshuffled train test val splits.
the funcom14and funcom25datasets are available online.
docstring barone and sennrich collect python methods and prefix comment docstrings by scraping github.
tokenization was done using subword tokenization.
they filtered the data for duplications and also removed excessively long examples greater than tokens .
however unlike other datasets barone et al.do notlimittoonlythefirstsentenceofthecomments.thiscanresult in relatively long desired outputs.
the dataset contains approximately 100k examples but after filteringoutverylongsamples asperbarone etalpreprocessing script6 this is reduced to examples.
we refer to this version as docstring1.
wealsoconsideraprocessedversionobtainedfromahmadetal .
source2which was attributed to wei et al .
.
we refer to this versionasdocstring2.duetotheprocessingchoices theexamples in docstring2 are significantly shorter than docstring1.
wmt19 news dataset tobenchmarkthecommentdatawithnaturallanguage weuseddatafromthefourthconferenceofmachine translation wmt19 .inparticular weusedthenewsdataset .
after manual inspection we determined this dataset offers a good balance of formal language that is somewhat domain specific to morelooselanguagecommonineverydayspeech.inbenchmarking comment data with natural language we wanted to ensure variety in the words and expressions used to avoid biasing results.
we used the english german translation dataset and compared english in this dataset to comments in the other datasets which were all in english to ensure differences in metrics were not a result of differences in language.
other cct datasets we tried to capture most of the code comment datasets that are used in the context of translation.
however there aresomerecentdatasetswhichcouldbeusedinthiscontext but wedidnotexplore .while some prior works provide the raw collection of code comments for download butnottheexactprocessingandevaluationsused .
other works use published datasets like docstring but processing and evaluation techniques are not now readily available .
as we will discuss unless the precise processing and evaluation code is available the results may be difficult to compare.
.
evaluation scores used acommonmetricusedinevaluatingtextgenerationisbleuscore .whencomparingtranslationsofnaturallanguage bleuscore has been shown to correlate well with human judgements of translationquality .inallthedatasetsweanalyzed theassociated however there are rather subtle differences in the way the bleus were calculated which makes the results rather difficult to compare.webeginthisdiscussionwithabriefexplanationofthe bleu score.
bleu asdorelatedmeasures indicatestheclosenessofacandidate translation output to a golden referenceresult.
bleu perse measuresthe precision asopposedto recall ofacandidate relative to the reference using constituent n grams.
bleu typically uses unigrams through grams to measure the precision of the system output.
if we define pn number of n grams in both reference and candidate number of n grams in the candidate bleucombinestheprecisionofeach n gramusingthegeometric mean exp n summationtext.1n n 1logpn .withjustthisformulation singleword outputsoroutputsthatrepeatcommon n gramscouldpotentially havehighprecision.thus a brevitypenalty isusedtoscalethe finalscore furthermoreeach n graminthereferencecanbeused inthecalculationjustonce.
thesecalculationsaregenerally standard in all bleu implementations but severalvariations may arise.smoothing one variation arises when deciding how to deal with cases when pn i.e.
a nn gram in the candidate string is not in thereferencestring .withnoadjustment onehasanundefined log0.
one can add a small epsilon to pnwhich removes undefined expressions.however becausebleuisageometricmeanof pn n ifp4isonlyepsilonabovezero itwillresultinamean which is near zero.
thus some implementations opt to smooth the pnin varying ways.
to compare competing tools for the same task it would be preferable to use a standard measure.corpus vs. sentence bleu when evaluating a translation system one typically measures bleu candidate vsreference across all the samples inthe held out test set.thus another source ofimplementationvariationis whendecidinghowtocombine theresults between all of the test set scores.
one option which was proposed originally in papineni et al.
is a corpus bleu sometimes referredtoasc bleu.inthiscasethenumeratoranddenominator ofpnareaccumulatedacrosseveryexampleinthetestcorpus.this means as long as at least one example has a gram overlap p4 willnotbezero andthusthegeometricmeanwillnot be zeroan alternative option for combining across the test corpus is referred to as sentence bleu or s bleu.
in this setting bleu score for the test set is calculated by simply taking the arithmetic mean the bleu score calculated on each sentence in the set.
tokenization choices a final source of variation comes not from howthemetriciscalculated butfromtheinputsitisgiven.because the precision counts are at a token level it has been noted that bleuishighlysensitivetotokenization .thismeansthatwhen comparingtopriorworkonadataset onemustbecarefulnotonly tousethesamebleucalculation butalsothesametokenization andfiltering.whencalculatingscoresonthedatasets weusethe tokenization provided with the dataset.
tokenizationcanbeverysignificantfortheresultingscore.as a toy example suppose a reference contained the string calls function foo and an hypothesis contained the string uses 749function foo .
if one chooses to tokenize by spaces one has tokens and .
this tokenization yields only one bigram overlap and no trigram or gram overlaps.
however if one instead chooses totokenize this as and wesuddenlyhavethreeoverlappingbigrams twooverlappingtrigrams andoneoverlapping4 gram.thisresultsinaswingofmorethan 15bleu m2pointsornearly40bleu dcpoints bleu m2and bleu dc described below .
we now go through bleu variants used by each of the datasets and assign a name to them.
the name is not intended to be prescriptive or standard but instead just for later reference in this document.
all scores are the aggregate measures which consider up to4 grams.
bleu cn thisisasentencebleumetric.itappliesalaplace like smoothing by adding to both the numerator and denominator of pnforn .
the codenn authors implementation was used7.
bleu dc thisisalsoasentencebleumetric.theauthors implementation isbased off nltk using its method smoothing.
thissmoothingismorecomplex.itonlyapplieswhen pniszero and setspn n loglh wherelhis the length of the hypothesis.
see the authors implementation for complete details8.
bleu fc this is an unsmoothed corpus bleu metric based on nltk s implementation.
details are omitted for brevity and can be found in the authors source9.
bleu moses the docstring dataset uses a bleu implementation bythemosesproject10.itisalsoanunsmoothedcorpusbleu.this is very similar to bleu fc though note that due to differences in tokenization scores presented by the two datasets are not directly comparable .
bleu ncs this is a sentence bleu used in the implementation11 of ahmad et al .
.
like bleu cn it uses an add one laplace smoothing.
however it is subtly different than bleu cn as the add one applies even for unigrams.sacrebleu thesacrebleuimplementationwascreatedbypost inanefforttohelpprovideastandardbleuimplementation forevaluatingonnltranslation.weusethedefaultsettingswhich is a corpus bleu metric with an exponential smoothing.bleu m2 thisisasentencebleumetricbasedonnltk method2 smoothing.likebleu cnitusesalaplace likeadd onesmoothing.
this bleu is later presented in plots for this paper.
we conclude by noting that the wide variety of bleu measures used inprior workin code commenttranslation carrysome risks.
wediscussfurtherbelow.table3providesomeevidencesuggesting thatthevariationishighenoughtoraisesomeconcernaboutthe true interpretation of claimed advances as we argue below the field can benefit from further standardization.
models techniques inthissection weoutlinethevariousdeeplearningapproaches that have been applied to this code comment task.
we note that our goal in this paper is not to critique or improve upon the specific technical methods but to analyze the data perseto gain some insights on the distributions therein and also to understand the most comment metric bleu that is used and the implications of using this metric.
however for completeness we list the different approaches and provide just a very brief overview of each tech nical approach.
all the datasets used below are described above in section .
iyeretal wasanearlyattemptatthistask usingafairlystandardseq2seqrnnmodel enhancedwithattention.hu etal alsousedasimilarrnn basedseq2seqmodel butintroduceda treelike preprocessing of the input source code.
rather than simply streamingintherawtokens theyfirstparseit andthenserializetheresultingastintoatokenstreamthatisfedintotheseq2seqmodel.
a related approach digests a fixed size random sample of paths through the ast of the input code using lstms and produces code summaries.
leclair et al proposed an approach that combinesbothstructuralandsequentialrepresentationsofcode they have also suggested the use of graph neural networks .
wanet al use asimilar approach butadvocate usingreinforcement learningtoenhancethegenerationelement.morerecently theuse of function context has been reported to improve comment synthesis.
source code vocabulary proliferation is a well knownproblem previously unseen identifier or method names in input code or output comments can diminish performance.
new work by moore et al approaches this problem by using convolutions over individual lettersin the input and using subtokens by camel case splitting on the output.
very recently zhang et al .
have reported that combining sophisticated ir methods with deeplearningleadstofurthergainsintheccttask.forourpurposes showing that ir methods constitute a reasonable baseline we use averysimple vanilla out of boxluceneirimplementation which already achieves nearly sota performance in many cases.
therearetasksrelatedtogeneratingcommentsfromcode for example synthesizingacommitloggivenacodechange orgeneratingmethodnamesfromthecode .sincetheseare somewhat different tasks with different data characteristics we don tdiscussthemfurther.inadditioncodesynthesis also usesmatchedpairsofnaturallanguageandcode however these datasets have not been used for generating english from code and arenotusedinpriorworkforthistask sowedon tdiscussthem further here.
methods findings in the following section we present our methods and results foreach of the rqs presented in .
in each case we present some illustrativeplotsand whenapplicable theresultsofrelevantstatistical tests.
all p values have been corrected using family wise benjamini hochberg correction.toexaminethecharacteristics ofeachdataset weconstructedtwotypesofplots zipfplotsand bivariate bleu plots.
750figure2 unigram vocabulary distribution.zipfplotforall datasets look similar.
difference from trigram zipf plot infig suggests greater repetitiveness in code comments.
.
differences between cct and wmt data the zipf plots are a useful way to visualize the skewness of textual data where in natural text a few tokens or ngrams account foralargeportionofthetext.eachplotpointisa rank relativefrequency pair both log scaled.
we use the plot to compare the relativeskewnessofthe english commentdatainthecctdata and the desired english outputs in the wmt nlt data.
examining the unigram zipf plot above it can be seen in both code comments andnaturalenglish afewvocabularywordsdodominate.however whenweturnbacktothetrigramzipfplotsinfigure1 wecansee the difference.
one is left with the clear suggestion that while the vocabularydistributionsacrossthedifferentdatasetsaren tthatdifferent thewaysinwhichthesevocabularywordsare combinedinto trigramsare much more stylistic and templated in code comments.
result code comments are far more repetitive than the english found in natural language translation datasets given this relatively greater repetitive structure in code comments we can expect that the performance of translation tools will be strongly influenced by repeating and or very frequent trigrams.ifafewfrequent n gramsaccountformostofthedesired outputinacorpus itwouldseemthatthesetrigramswouldplay a substantial perhaps misleading role in measured performance.
figure supports this analysis.
the right hand side plot shows the effectonbleu 4ofreplacingsinglewords unigrams withrandomtokensinthe golden desired outputinthevariousdatasets.the left handplotshowsthe effectofreplacingtrigrams.theindex 1to on the x axis shows the number of most frequent n grams replaced with random tokens.
the y axis shows the decrease in measured bleu as the code is increasingly randomized.
theunigramsplotsuggeststhattheeffectonthedesirednatural language nl output asmeasuredbybleuisrelativelygreater when compared to most of the comment datasets.
this effect is reversedfortrigrams the nl datasetisnotaffectedasmuchby the removal of frequent trigrams as the comment datasets.
thisanalysis suggests that a tool that got the top few most frequenttrigramswronginthecode commentgenerationtaskwouldsuf feralargerperformancepenaltythanatoolthatgotthetop few n gramswronginanaturallanguagetranslationtask.thisvisual evidence is strongly confirmed by rigorous statistical modeling please see supplementary materials bleu cc.rmd for the r code.
frequenttrigramsthathaveabigeffectonthecodecommentbleuincludee.g.
factory method for delegates to the and method for instantiating .toputitanotherway onecouldboosttheperformanceofacode commenttranslationtool perhapsmisleadingly by getting a few such n grams right.
result2 frequent n grams could wield a much stronger effect on the measured bleu performance on code comment translation tasks than on natural language translation .
input output similarity rq3 animportantpropertyofnaturallanguagetranslationisthatthere is a general dependence of input on output.
thus similar german sentences should translate to similar english sentences.
for exampletwogermansentenceswithsimilargrammaticalstructure andvocabularyshouldingeneralresultintwoenglishsentences whose grammatical structure and vocabulary resemble each other likewise in general the more different two german sentences are in vocabulary and grammar the more difference we expect in their english translations.
exceptions are possible since some similar constructions have different meanings kicking the ball vs.kicking the bucket12 .
however on average in large datasets we should expect that more similar sentences give more similar translations.
whentrainingatranslationenginewithahigh dimensionalnonlinear function approximator like an encoder decoder model using deep learning this monotonic dependence property is arguably useful.
we would expect similar input sentences to be encode into similar points in vector space thus yielding more similar output sentences.
how do natural language translation german english and code comment datasets fare in this regard?
to gauge this phenomenon wesampled10 randompairsofinputfragments from eachofourdatasets andmeasuredtheirsimilarityusingbleu m2 as well as the similarity of the corresponding golden desired outputfragments.wethenplotthe inputbleu m2similarityforeach sampled pair on the x axis and the bleu m2 similarity of the correspondingpairof outputsonthey axis.weuseakernel smoothed dhistogramratherthanascatterplot tomakethefrequencies more visible along with we expect an indication suggesting that similar inputs yield similar outputs.
certainly most inputs and outputs are different so we expect to find a large number of highly dissimilar pairs where input and output pair bleus are virtually zero.soweconsideredourrandomsamples withandwithoutinput and output similarities bigger than .
the highly dissimilar input outputpairsareomittedintheplot.however asanadditional quantitativetestofcorrelation wealsoconsideredspearman s .
both with and without the dissimilar pairs.
the bivariate plots are shown in fig and the spearmans in table .
we have split the random samples into two cases 12the latter idiom indicates death some automated translation engines e.g.
bing seem to know the difference when translating from english 751trigrams unigrams number of n grams removedresidual bleu scoredataset color key codenn deepcom1deepcom2docstring1docstring2funcom1funcom2nl figure3 theeffectofmostfrequentunigrams left andtrigrams right onmeasured smoothed bleu 4performance.bleu4 was calculated after successive removals of of most frequent unigrams right and trigrams left .
the effect of removing frequent unigrams isbyandlargegreateronthenaturallanguagedataset nl .however theeffectofremovingfrequent trigrams onthecommentdatasetsisgenerallystrongerthanonthe nl datasetduetohighdegreeofrepetitioninthecomment datasets.
these apparent visual differences are decisively confirmed by more rigorous statistical modeling.
figure bivariate plots showing dependency of input similarity to output similarity.
pairs with similarity bleu lessthan 5areomitted.thenaturallanguagetranslationdatahavethestrongestdependency similarinputshavethestrongest tendency to provide similar outputs one in which we do no further processing and one where bothinput output bleu similarities are both .
spearman s significance and sample sizes are shown for non zeroes in the columns thenumberswithintheparenthesesincludethehighlydissimilar ones .
from the table last column we see that about ofthepairshavesomesimilarityonbothinputsandoutputs dependingonthedataset.thetablealsoshowsthespearman s first column and significance second column .
each subplot in 752dataset spearman s significance number of correlation p value pairs with bleu p value bleu bleu all p value all nl .
.
.
.
deepcom1 .
.
.0e .2e deepcom2 .
.
.5e .1e docstring1 .
.
.7e .6e docstring2 .
.
.5e .7e funcom1 .
.
.30e .4e funcom2 .
.
.03e .7e codenn .
.
.
.
table correlation values spearman s and significance p value for the plots in figure .
values outside paranthesisarecalculatedwithonlythepairshavingpairwisebleu values in paranthesis include all pairs.
p values are adjusted with benjamini hochberg familywise correction.
in all cases we chose random pairs fig4showsonedataset wherex axisisthebleusimilarityofa pair s inputs and y axis is that of outputs.
the plot is a binned 2d histogram using colored hexagons to represent counts in that bin.
arepresentativevariantofeachdatasetisplotted asapplicable the omitted ones are visually very similar.
wecanclearlyseeastrongerrelationshipbetweeninputandoutput bleus in the natural language setting.
particularly for natural languagedata thisisfurtherevidencedbytheratherhighspearman correlation for the non zero bleu pairs .
!
!
and the evident visual dependence between input input similarity and output output similarityisnoteworthy thisindicatesthatthereisstrong fairly monotonicrelationshipinnaturallanguagetranslation themore similar the source the more similar the translation!
this analysis suggests that natural language data has a stronger morespecificinput outputdependence thisalsosuggeststhattranslationbetweenlanguagesismoreamenabletolearnablefunctionapproximatorslikedeeplearners thisappearstobe substantially lesstrueforcode commentdata.thisgivesusthefollowingconclusion with reference to rq3.
result the natural language translation wmt shows a stronger input output dependence than the cct datasets in that similar inputs are more likely to produce similar outputs.
.
information retrieval baselines as can be see in fig.
and table datasets for the natural language translationtaskshowasmootherandmoremonotonicinput outputdependence bycontrast code commentdatasetsseemtohavelittle ornoinput outputdependence.thisfindingcastssomedoubton the existence ofa general sequence to sequence code comment function that can be learned using a universal function approximator like a deep neural network.
however it leaves open the possibilitythatamoredata drivenapproach thatsimplymemorizesthe training data in some fashion rather than trying to generalize from it might also work.
thus given a code input perhaps we can just try to find similar code in the training dataset and retrieve the comment associated with the similar code.
this is a simple and naive information retrieval ir approach.
we then compare this to the ir performance on nl translation.
.
.
method.
we use apache solr version .
.113to implement a straightforward ir approach.
apache solr is a open source document search engine based on apache lucene.
we simply construct anindexofoverthecodepartsoftherelevantdatasets givenacodeinput weusethatasa query overtheindex findtheclosestmatch and return the comment associated with the closest matching code as the generated comment .
we used the default parameters of solr without tuning.
this includesthedefaultbm25scoringfunction .foreachdataset weusealwaysthesametokenizationprocedureusedbyauthors.in addition weperformsomeadditional pre processingonthecode thatistypicallyrequiredforirapproaches.forexample weremovehighlyfrequentstopwordsfromthecode.additionally fordatasets do not provide a tokenization phase that actually splits cammel casewords or snake case words we include terms for indexing andsearchingwhichincludesthesplitformofthesewords.however we note that the processing of stop words and word splitting only effects a minor change in performance.
.
.
ir results.
we find that on most datasets the simple ir baselineapproachestheneuralmodels andexceedsitfordeepcom1 docstring1 anddocstring2.ho wever irdo espoorlyonthewmt translationdataset andalsooncodenn.inbothcases wespeculatethatthismayreflecttherelativelevelofredundancyinthese datasets.
codenn is drawn from stackoverflow which tends tohave fewer duplicated questions in the case of wmt which is hand curated we expect there would be fewer duplications.
prior work has used very sophisticated ir methods.
wecannotclaimtosupersedethesecontributions butwillpoint out that a very naive ir method does quite well in some cases betterthanveryrecentlypublishedmethodsondatasets datasetvariations which currently lack ir baselines.
we therefore view ir baselinesasimportantcalibrationonmodelperformance bytrying such a simple baseline first one can help find pathologies in the model or dataset which require further exploration.
wealsonotethatthereisvariationresults.indeepcom2f which includes cross project folds we observe a wide range resultsranging from a bleu dc of .6to48.
!
this level of variation across folds is a cause for concern...this suggests depending on the split amodelwithhighercapacitytomemorizethetrainingdata mightdobetterorworse potentiallymuddlingtheresultsifonly of funcom scores vary quite a bit this variation may confound measurement of actual differences due to technical improvements.
14value is for model but on the test split 753dataset method used score score method deepcom2f ir baseline .
bleu dc deepcom sbt .
seq2seq .
deepcom1 ir baseline .
bleu ncs transformer .
funcom1 ir baseline .
bleu fc astattend gru .
funcom2 ir baseline .
bleu fc astattend gru .
code2seq .
code gnn bilstm .
codenn ir baseline .
bleu cn i riyeret al.
.
codenn .
docstring2 ir baseline .
bleu ncs transformer .
docstring1 ir baseline .
bleu moses seq2seq .
nl de en ir baseline .
sacrebleu fair transformer .714table measurements of a simple information retrieval baseline compared to various neural machine translation basedmethods.thescoringmethodweusemirrorstheoneused on the dataset see section .
.
recommendation since even naive ir methods provide competitiveperformanceinmanycctdatasets theycanbe animportantpartforcheckingforissuesinthenewcollection and new processing of cct datasets.
.
calibrating bleu scores we now return our last research question rq .
how should we interpret the bleu results reported in prior work and also the informationretrievalbleunumbersthatwefound whicharein the same range see table ?
tocalibratethesereportedbleuscores weconductedanobservationalstudy using affinitygroups ags ofmethodsthatmodel different levels of expected similarity between the methods.
for example considerarandompairofmethods sothatbothelements ofthepairare methodsfromadifferentproject.thisisourlowestaffinitygroup wewouldexpectthecommentstohaveverylittle in common apart from both being utterances that describe code.
the next higher affinity group is a random pair of methods from the same project.
we would expect these to be a bit more similar sincetheyarebothconcernedwiththesameapplicationdomain or function.
the next higher level would methods in the same class which presumably are closer although they would be describing differentfunctions.bytakingalargenumberrandompairsfromeach of these affinity groups and measuring the bleu for pairs in each group we can get an estimate of bleu for each group.
for ourexperiment wepickedthe1000largestprojectsfromgithub and then chose random pairs from each of the affinity groups.
foreachpair werandomlypickedoneasthe reference output and the other as the candidate output and the bleu m2 score.
we report the results in two different ways in fig.
and in table .
forintraclass wedonottakemorethansixrandompairsfroma singleclass.inallags weremovedallbutoneoftheoverloaded methods andallgettersandsettersbeforeouranalysis.without this filtering we see a difference of around points.
figure the distribution of bleu scores between affin ity groups.
red lines represent the means i.e.
the sentencebleu and the dashed lines represent quartiles.
firstwedescribefig.5whichshowsthedistributionofthebleu scores in the ags.
as might be expected the inter project ag showsafairlylowmean around3.theintra projectagisafew bleu points higher.
most notably the intraclass ag has a bleu score around which is close to the best in class values reported in prior work for some but not all datasets.
note with the implemented experiment we cannot exactly comparethesenumbers aseachdatasetisdrawnfromadifferentdistribution.
most cct datasets provide the data in the traditionaltranslation format of source target pairs making it difficult torecover the other affinity group pairs of a given dataset example.
this is why created our own new sampling based off of large github repos.
while not exactly comparable to existing datasets new creations of cct data could start with this simple affinity group baseline to calibrate the reported results.
another stronger affinity grouping would be methods that are semantically equivalent.
rather than trying to identify such an affinity group ourselves by hand which might be subject to confirmationbias weselectedmatchedapicallsfromarecentproject similarapi15by chen which used machine learning methods tomatchmethodsindifferent butequivalentapis e.g.
junitvs.
testng .
we extracted the descriptions of matched pairs of high scoring matches from different apis and computed their bleu.
wefound that these bleu scores are on average about points higher with a mean around .
this number should be taken with caution however since comments in this ag sample are substantially shorter than in the other groups.
754recommendation testingaffinitygroupscanprovidea baseline for calibrating bleu results on a cct dataset as thesimpletrickofgeneratingcommentsforagivenmethod simplybyretrievingthecommentsofarandomothermethod in the same class possibly can approach sota techniques.
postscript bleu variability we noted earlier in section .
.
page that there was considerable intrinsic variation within a dataset simply across different folds we reported that measured bleu dcindeepcom2frangedfrom20.6to48.
similarresults were noted in the different variants of funcom.
this above affinity group experiment with ir provided an opportunity to calibrate another bleu variability across different ways of calculating bleu.
function intraclass bleu fc .81blue cn .34bleu dc .5sacrebleu .81bleu moses .6bleu ncs .49bleu m2 .
table the scores of samples of java methods from the same class.
we took the pair sample from the intraclass sample and measured the sentence bleu for these pairs using the bleu implementation variations used in the literature.
the resultsare shown intable3.thevaluesrangefromaround21.2toaround24.
this range is actually rather high compared to the gains reported in recently published papers.
this finding clarifies the need to have a standardized measurement of performance.
observation measurements show substantial variation.
the version of bleu chosen and sometimes even the folds in thetraining testsplit cancausesubstantialvariationinthe measuredperformance thatmayconfoundtheabilitytoclaim clear advances over prior work.
discussion we summarize our main findings and their implications.
comment repetitiveness our findings presented in figure show that commentsin cctdatasets arefar morerepetitive than theenglishfoundinthewmtdataset figure2suggeststhatthisnot merely a matter of greater vocabulary in distribution in comments butratherafunctionofhowwordsarecombinedincomments.the highly prevalent patterns in comment have a substantially greater impactonthemeasuredbleuperformanceofmodelstrainedwith this cct data as shown in figure .
a closer look at the cct datasetsshows thattrigramssuch as creates a new returns true if constructor delegates to factory method for areveryfrequent.
gettingtheseright orwrong hasahugeinfluenceonperformance.implications thesefindingssuggestthatgettingjustafewcommon patterns of comments right might deceptively affect measured performance.sotheactualperformanceofcommentgenerationmight deviate a lot from measured values much more so relative to natural language translation.
repetition in comments might also mean that fill in the blanks approaches might be revisited with a more data driven approach classify code first to find the right template andthenfill in the blanks perhapsusinganattentionor copy mechanism.
input output dependence whentranslatingfromonelanguage toanother onewouldexpectthatmoresimilarinputsproducemore similar outputs and that this dependence is relatively smooth and monotonic.ourfindingsinfigure4andtable1 indicatethatthis property is indeed very strongly true for general natural language outputs but not as much for the comments.
implications deep learningmodelsareuniversalhigh dimensional continuous function approximators.
functions exhibiting a smooth input outputdependency couldbereasonablyexpectedtobeeasier to model.
bleu is a measure of lexical token sequence similarit therathernon functional natureofthedependencysuggestedby figure and table indicate that token sequence models that work wellfornaturallanguagetranslationmaybelessperformantfor code it maybe thatother non sequential modelsofcode suchastree based or graph based are worth exploring further baselining with ir our experience suggests that simple ir approachprovidesbleuperformancethatiscomparabletocurrent state of the art.
implications our findings suggest that a simple standard basic ir approachwouldbeausefulbaselineforapproachestotheccttask.
especiallyconsideringtherangeofdifferentbleuandtokenization approaches this would be a useful strawman baseline.
interpreting bleu scores bleu meteor rogue etc are measuresthathavebeendevelopedfordifferenttaskinnaturallanguage processing such as translation summarization often after extensive carefullydesigned humansubjectstudies.sincebleuismost commonly used in code comment translation we took an observationalapproachcalibratethebleuscore.ourresults reported in fig.
and table indicate that the reported bleu scores are not that high.
implications thebestreportedbleuscoresforthegerman english translation tasks are currently are in the low s. our affinity group calibration suggests that on some datasets the performance of models are comparable on average to retrieving the comment of a random method from the same class.
while this conclusion can t beexplicitlydrawnforaspecificdatasetwithoutusingtheexact examples and processing from that specific dataset but comparing results at an affinity group level can provide insight into minimum expected numbers for a new cct datset.
learning from nlp datasets wefindthatthecurrentlandscape ofcctdatasetstoberathermessy.thereareoftenseveraldifferent versions of the same dataset with different preprocessing splits andevaluationfunctionswhichallseemequivalentinname but unless extra care is taken might not be comparable.
755however sometasksinnlpdonotseemtoobservesuchvariancewithinatask.wepostulatethiscouldbeduetoseveralreasons.
for one with the popularity of large open source repositories it has become cheap and easy for a software engineering researcher to collect a large number of pairs of code and comments.
this does not require hiring a human to label properties of text and thus less effort might be taken on quality control compared to nlp data collection.
because researchers are domain experts in the datasets they might be also more willing to apply their own version of preprocessing.
in addition there are a wider array of tools to enforce consistencyonvariousnlptasks.
forexamplethewmtconferenceon translation acompetitionisranwithheldoutdataandhumanevaluation.othertasks suchassquad forreadingcomprehension and glue for multitaskevaluationallow foruploading code toaserverwhichrunstheproposedmodelonheldoutdata.this ensure consistency in evaluation metrics and data.
we view adapting some these techniques as an interesting avenue for future work.
threats to validity our paper is a retrospective and doesn t propose any new tools metrics etc still some potential threats exist to our findings.
fold variance with the exception of deepcom2f we did not run measures over multiple folds or samples of the data.
this makes it possible that there is variance in some of our reported numbers.the affinity benchmarks when collecting affinity groups we collectfullmethodsandprocessthemusingasetoffilters.thismeans thatwhencomparingthesenumbers theymightnotbedirrectly comparabletoaspecificdataset.thenumbersarepresentedonly as estimate of similarity of the affinity groups.replication threat wheneverwehadto wedidourbesttoreplicate and measure the quantities we reported using the same code as the previous work.
still it is possible that we failed to comprehend some subtletiesin the providedcode and thismay bea threat to our findings.generalizability we covered all the commonly used datasets and literaturewecouldfind.however itmaybethatwehavemissed some where cases our findings don t hold.
conclusion in this paper we described a retrospective analysis of several research efforts which used machine learning approaches originally designedforthetaskofnaturallanguagetranslation forthetask of generating comments from code.
we examined the datasets the evaluation metrics and the calibration thereof.
our analysis pointedoutsomekeydifferencesbetweengeneralnaturallanguage corpora and comments comments are a lot more repetitive.
we also found that a widely used natural language translation dataset shows a stronger smootherinput output relationships than naturallanguage.turningthentoapopularevaluationmetric bleu score we found considerable variation based on the way it s calculated in some cases this variation exceeded claimed improvements.
looking at calibration of the reported bleu scores first we found thatsimpleoff the shelfinformationretrievaloffersperformancecomparable to that reported previously.
second we found that the simple trick of retrieving a comment associated with a methodin the same class as a given method achieves an average performancecomparabletocurrentstate of the art.ourworksuggests thatfutureworkintheareawouldbenefitfroma otherkindsof translationmodelsbesidessequence to sequenceencoder decorder modelsb morestandardizedmeasurementofperformanceandc baselining against information retrieval and against some very coarsefoils likeretrievingacommentfromarandomothermethod in the same class .
funding for this research was provided by national science foundation under grant nsf shf large collaborative research exploiting the naturalness of software.
source code and data will be made available at 3lbdegy