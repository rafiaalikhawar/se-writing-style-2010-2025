self supervised query reformulation for code search yuetian mao shanghai jiao tong university shanghai china mytkeroro sjtu.edu.cnchengcheng wan east china normal university shanghai china wancc1995 gmail.comyuze jiang shanghai jiao tong university shanghai china jyz sjtu.edu.cnxiaodong gu shanghai jiao tong university shanghai china xiaodong.gu sjtu.edu.cn abstract automatic query reformulation is a widely utilized technology for enriching user requirements and enhancing the outcomes of code search.
it can be conceptualized as a machine translation task wherein the objective is to rephrase a given query into a more comprehensive alternative.
while showing promising results training such a model typically requires a large parallel corpus of query pairs i.e.
the original query and a reformulated query that are confidential and unpublished by online code search engines.
this restricts its practicality in software development processes.
in this paper we propose ssqr a self supervised query reformulation method that does not rely on any parallel query corpus.
inspired by pre trained models ssqr treats query reformulation as a masked language modeling task conducted on an extensive unannotated corpus of queries.
ssqr extends t5 a sequence to sequence model based on transformer with a new pre training objective named corrupted query completion cqc which randomly masks words within a complete query and trains t5 to predict the masked content.
subsequently for a given query to be reformulated ssqr identifies potential locations for expansion and leverages the pretrained t5 model to generate appropriate content to fill these gaps.
the selection of expansions is then based on the information gain associated with each candidate.
evaluation results demonstrate that our method outperforms unsupervised baselines significantly and achieves competitive performance compared to supervised methods.
ccs concepts information systems query reformulation .
keywords query reformulation code search self supervised learning acm reference format yuetian mao chengcheng wan yuze jiang and xiaodong gu.
.
selfsupervised query reformulation for code search.
in proceedings of the 31st acm joint european software engineering conference and symposium on the both authors contributed equally to this research.
xiaodong gu is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november san francisco usa association for computing machinery.
acm isbn xxxx x .
.
.
.
of software engineering esec fse .
acm new york ny usa pages.
introduction searching through a vast repository of source code has been an indispensable activity for developers throughout the software development process .
the objective of code search is to retrieve and reuse code snippets from existing projects that align with a developer s intent expressed as a natural language query .
however it has been observed that developers often struggle to articulate their information needs optimally when submitting queries .
this difficulty may arise from factors such as inconsistent terminology used in the query or a limited understanding of the specific domain in which information is sought.
developers may constantly reformulate their queries until the queries reflect their real query intention and retrieve the most relevant code snippets.
studies have shown that in stack overflow approximately .
of queries on stack overflow have undergone reformulation.
moreover developers on average reformulate their queries .
times before selecting a particular result to view.
one common solution to this problem is automatic query reformulation namely rephrasing a given query into a more comprehensive alternative .
a natural first way to accomplish this objective is to replace words in a query with synonyms based on external knowledge such as wordnet and thesauri .
however this methodology restricts the expansion to the word level.
besides gathering and maintaining domain knowledge is usually costly.
the knowledge base might always lag behind the fast growing code corpora.
there have been other attempts that consider pseudo relevance feedback i.e.
emerging keywords in the initial search results .
they search for an initial set of results using the original query select new keywords from the top kresults using tf idf weighting and finally expand the original query with the emerging keywords.
nevertheless despite expanding queries at a word level this approach also has a risk of expanding queries with noisy words.
hence the expanded query can be semantically irrelevant to the original one.
in recent years driven by the prevalence of deep learning researchers seek the idea of casting query reformulation as a machine translation task the original query is taken as input to a neural sequence to sequence model and is translated into a more comprehensive alternative .
despite showing substantial gains such models require to be trained on a large scale parallel corpus of query pairs i.e.
the original query and a reformulated query .
unfortunately acquiring large query pairs is infeasible given that real world search engines e.g.
google and stack overflow do not publicly release the evolution of queries.
for example the stateof the art method sequer relies on a confidential parallelarxiv .00267v1 jul 2023esec fse november san francisco usa yuetian mao chengcheng wan yuze jiang and xiaodong gu dataset that cannot likely to be impossible be accessed by external researchers.
replicating the performance of sequer becomes challenging or even impossible for those who lack access to such privileged datasets.
this lack of replicability hampers the wider adoption and evaluation of the method by the research community.
in this paper we present ssqr a self supervised query reformulation method that achieves competitive performance to the state of the art supervised approaches while not relying on the availability of parallel query data for supervision.
inspired by the pre trained models ssqr automatically acquires the supervision of query expansion through self supervised training on a large scale corpus of code comments.
specifically we design a new pre training objective called corrupted query completion cqc to simulate the query expansion process.
cqc masks keywords in long comprehensive queries and asks the model to predict the missing contents.
in such a way the trained model is encouraged to expand incomplete queries with keywords.
ssqr leverages t5 the state of the art language model for code.
the methodology of ssqr involves a twostep process.
firstly t5 is pre trained using the cqc objective on a vast unannotated corpus of queries.
this pre training phase aims to equip t5 with the ability to predict masked content within queries.
when presented with a query to be reformulated ssqr enumerates potential positions within the query that can be expanded.
it then utilizes the pre trained t5 model to generate appropriate content to fill these identified positions.
subsequently ssqr employs an information gain criterion to select the expansion positions that contribute the most valuable information to the original query resulting in the reformulated query.
we evaluate ssqr on two search engines through both automatic and human evaluations and compare with state of the art approaches including sequer nlp2api lusearch and googleps .
experimental results show that ssqr improves the mrr score by over compared with the unsupervised baselines and gains competitive performance over the fully supervised approach.
human evaluation reveals that our approach can generate more natural and informative queries with improvements of .
and .
to the original queries respectively.
our contributions are summarized as follows to the best of our knowledge ssqr is the first self supervised query reformulation approach which does not rely on a parallel corpus of reformulations.
we propose a novel information gain criterion to select the pertinent expansion positions that contribute the most valuable information to the original query.
we perform automatic and human evaluations on the proposed method.
quantitative and qualitative results show significant improvements over the state of the art approaches.
background .
code search code search is a technology to retrieve and reuse code from preexisting projects .
similar to general purpose search engines developers often encounter challenges when attempting to implement specific tasks.
in such scenarios they can leverage a code search engine by submitting a natural language query.
the search figure an example of google query reformulation.
engine then traverses an extensive repository of code snippets collected from various projects identifying code that is semantically relevant to the given query.
code search can be broadly classified into two categories search within the context of a specific project or as an open search across multiple projects.
the search results may include individual code snippets functions or entire projects.
.
query reformulation query reformulation provides an effective way to enhance the performance of search engines .
the quality of queries is often a bottleneck of search experience in web search .
this is because the initial query entered by the user is often short generic and ambiguous.
therefore the search results could hardly meet the specific intents of the user.
this requires the user to revise his query through multiple rounds.
query reformulation is a technology that reformulates user s queries into more concrete and comprehensive alternatives .
figure shows an example of query reformulation in google search engine.
when a user enters the query convert string in the search box there may exist multiple possible intents such as convert something to a string or convert a string to something .
additionally the specific programming language for implementing the conversion function is not specified.
in such cases conventional search engines like google face challenges in accurately determining the user s true intent.
to address this issue search engines often employ tools like the google prediction service googleps .
googleps automatically suggests multiple reformulations of the original query.
these reformulations provide alternative options that the user can consider to refine their search.
by presenting a range of reformulations users can narrow down their search target by selecting the most relevant reformulation that aligns with their intended query.
this process helps users in finding more precise and tailored search results.
query reformulation broadly encompasses various techniques including query expansion reduction and replacement .
while query expansion involves augmenting the original query with additional information such as synonyms and related entities to enhance its content query reduction focuses on eliminating ambiguous or inaccurate expressions.
query replacement on the other hand involves substituting incorrect or uncommon keywords in the original query with more commonly used and precise terms.
among these types query expansion constitutes the predominant approach accounting for approximately of real world search scenarios .self supervised query reformulation for code search esec fse november san francisco usa codequerycorpus initialquerypre training corrupted query completion printstring mask printstring printstringinconsole .codesearchsearchresults1.
pre train t5 .
expand candidatespans .
select expansion position t5incompletequeriesreformulatedquery t5offline x xxx?
print mask string printstring mask android printstring printjava string printstringinconsole ig .1ig .3ig .
.........expandedqueries figure an illustration of the main pipeline transformer encoderencoding vectorconvert x in y maskedsequence x string y python s maskedspanpredictiontransformerdecoder s x string y python figure illustration of t5 .
self supervised learning and pre trained models supervised learning is a class of machine learning methods that train algorithms to classify data or predict outcomes by leveraging labeled datasets.
it is known to be expensive in manual labeling and the bottleneck of data annotation further causes generalization errors spurious correlations and adversarial attacks .
self supervised learning alleviates these limitations by automatically mining supervision signals from large scale unsupervised data using auxiliary tasks .
this enables a neural network model to learn rich representations without the need for manual labeling .
for example the cloze test masks words in an input sentence and asks the model to predict the original words.
in this way the model can learn the semantic representations of sentences from large unlabeled text corpora.
pre trained language models plms such as bert gpt and t5 are the most typical self supervised learning technology.
a plm aims to learn language s generic representations on a large unlabeled corpus and then transfer them to specific tasks through fine tuning on labeled task specific datasets.
this requires the model to create self supervised learning objectives from the unlabeled corpora.
take the text to text transfer transformer t5 in figure as an example.
t5 employs the transformer architecture where an encoder accepts a text as inputand outputs the encoded vector.
a decoder generates the target sequence based on the encodings.
to efficiently learn the text representations t5 designs three self supervised pre training tasks namely masked span prediction masked language modeling and corrupted sentence reconstruction.
by pre training on large scale text corpora t5 achieves state of the art performance in a variety of nlp tasks such as sentence acceptability judgment sentiment analysis paraphrasing similarity calculation and question answering .
method the primary focus of this paper is on query expansion the most typical accounting for technique for query reformulation.
query expansion aims to insert key phrases into a query thereby making it more specific and comprehensive.
essentially query expansion addresses a pinpoint then expand problem wherein the goal is to identify potential information gaps within a given query and generate a set of keywords to fill those gaps.
inspired by the masked language modeling mlm task introduced by pre trained models like bert our proposed method adopts a self supervised idea.
specifically we mask keywords within complete code search queries and train a model to accurately predict and recover the masked information.
this allows the model to learn the underlying patterns and relationships within the queries enabling it to generate meaningful expansions for query reformulation.
.
overview figure shows the main framework as well as the usage scenario of our method.
the pipeline involves two main phases an offline pre training phase and an online expansion phase.
during the pretraining phase ssqr continually pre trains a plm named t5 with a newly designed corrupted query completion task on an unlabelled corpus of long queries .
.
this enables the t5 to learn how to expand incomplete queries into longer ones.
during the runtime of ssqr when a user presents a query for code search esec fse november san francisco usa yuetian mao chengcheng wan yuze jiang and xiaodong gu howtoreverseanarrayinjavahowtoreverseanarray injava convertstringtolistconvert stringtolistconvertstring tolistconvertstringto listconvertstringtolist t5 convertstringtolist convertstringtolistinjavat5 convertstringtolist a pre train t5with cqc c selectexpansion positions b expand candidatespans t5 target corrupted query query incompletequeries reformulatedqueries original query candidates ig .1ig .3ig .2ig .1ig .
convertstringto list convertstringtointegerlist............... ...... figure a working example of each expansion step ssqr employs a two step process for query expansion.
firstly it enumerates candidate positions within the query that can be expanded and utilizes the pre trained t5 model to generate content that fills these positions as discussed in .
.
following the expansion step ssqr proceeds to select the position that offer the highest information gain after the expansion introduced in .
.
this selection process ensures that the most valuable and informative expansions are chosen thereby enhancing the reformulated query in terms of its relevance and comprehensiveness.
finally once the query has been expanded users conduct code search by selecting the most relevant reformulation that aligns with their intended query.
our approach specifically focuses on the function level code search scenario which involves the retrieval of relevant functions from a vast collection of code snippets spanning multiple projects.
the following sections elaborate on each step of our approach respectively.
.
pre training t5 with corrupted query completion we start by pre training a plm which can predict the missing span in a query.
we take the state of the art t5 as the backbone model since it has a sequence to sequence architecture and is more compatible with generative tasks.
besides t5 is specialized in predicting masked spans i.e.
a number of words .
to enable t5 to learn how to express a query more comprehensively we design a new pre training objective called corrupted query completion cqc using a large scale corpus of unlabelled queries.
similar to the mlm objective cqc randomly masks a span of words in the query and asks the model to predict the masked span.
more specifically given an original query q w1 ... w n that consists of a sequence of nwords ssqr masks out a span of n consecutive words from a randomly selected position i namely si j wi ... w j and replaces it with a token.
then the corrupted query is taken as input to t5 which predicts the words in the masked span.
we use the teacher forcing strategy for pre training.when predicting a word in the corrupted span the context visible to the model consists of two parts the uncorrupted words in the original query denoted as q si j w1 ... w i wj wn and the ground truth words appeared before the current predicting position wt denoted as wi t .
we pre train the model using the cross entropy loss namely minimizing lcqc j t ilogp wt q si j wi t .
figure a shows an example of the cqc task.
for a query how to reverse an array in java taken from the training corpus the algorithm corrupts the query by replacing the modifier in java with .
the corrupted query is taken as input to t5 which predicts the original masked tokens in java .
.
expanding candidate spans the pre trained t5 model is then leveraged to expand queries.
we consider a query that needs to be expanded as an incomplete query where a span of words is missing at a position denoted as a masked token we want the model to generate a sequence of words to fill in the span.
this is exactly the problem of masked span prediction as t5 aims to solve.
therefore we leverage the pre trained t5 to expand the incomplete queries.
however a query with nwords haven 1positions for expansion.
therefore we design a best first strategy we enumerate all the n positions as the masked spans perform the cqc task and select the top kpositions that have the most information gain of predictions.
specially given a original query q w1 w2 ... w n ssqr enumerates then 1positions between words.
for each position it inserts a token.
this results in n 1candidate masked queries.
each incomplete query q ... w n is taken as input to the pre trained t5.
the decoder of t5 generates a span of words s for the token by sampling words according to the predicted probabilities.
finally ssqr replaces the token with the generated span s yielding the reformulated query.self supervised query reformulation for code search esec fse november san francisco usa algorithm span expansion and selection input q the input query written in natural language t5 the pre trained t5 model k the number of candidate positions m the maximum target length.
output q a set of candidate masked queries.
1w1 ... w n tokenize q 2q initialize the query set.
3fori to n do q w1 ... w i wi ... w n insert a token to the i th position in q.
6v1 ... v m t5 q predict the content for the token.
8ig m m j 1p vj logp vj calculate the information gain for the expansion.
10q q q ig 11end 12sortqbased on entropy in an descending order 13q top q k select the top kqueries.
14return q figure b shows an example.
given the first two masked queries the t5 model generates in java and integer for the masked tokens respectively.
the former refers to the language used to implement the function while the latter refers to the data type of the target data structure.
the reformulated queries supplement the original queries with additional information revealing user potential intents from different aspects.
.
selecting expansion positions a query with nwords hasn 1candidate positions for expansion but not all of them are necessary for expansion.
hence we must determine which positions are the most proper to be expanded.
ssqr selects the top kcandidate queries that have the most missing information in the masked span.
the resulting expanded queries are more likely to gain information after span filling.
the key issue here is how to measure the information gain after filling each span.
in our approach we define information gain for a span expansion as the negative entropy over the predicted probability distribution of the generated words .
in information theory entropy characterizes the uncertainty of an event in a system.
suppose the probability that an event will happen follows a distribution of p1 ... p n the entropy of the event can be computed as n i 1pilogpi.
the lower the entropy the more certain that the event can happen.
that means the event brings more information to humans.
this can be analogized to the span prediction problem when the probability distribution of the generated words over the vocabulary is uniform the entropy uncertainty becomes high because every word is likely to be generated.
by contrast smaller entropy means that there is a greatly different likelihood of generating each word and thus the certainty of the generation is high.
the lower the entropy the higher the certainty that this span contains the word and the more information that the expansionbrings to the query.
if the span contains multiple words we can measure the information gain of the span prediction using their average negative entropy.
for each candidate query q we predict a span s using the pretrained t5 model p vi t5 q v i i ... l where each videnotes a sub token in the predicted span.
each predictionp vi follows a probability distribution of p1 ... p v over the vocabulary of the entire set of queries in the training corpus indicating the likelihood of each token in the vocabulary appearing in the span.
our next step is to compute the information gain of each expansion using negative entropy for each sub token vi the information gain can be calculated by ig vi h vi v v 1p vi v logp vi v .
the higher the ig the more certainty that the prediction is.
for a predicted span s withmsub tokens we compute the average ig of all its tokens namely ig s ml i 1ig vi .
finally we select the top kexpansions with the highest information gain and then replace the token with the predicted span.
the top kexpansions are provided to users for choosing the most relevant one that aligns with their intention.
the specific details of the method are summarized in algorithm .
figure c shows an example query expansion.
for a given query convert string to list to be expanded ssqr firstly enumerates five expansion positions of the original query inserting a token into each one.
next the pre trained t5 model takes these candidate queries as input and calculates the information gain from the prediction for these candidate queries.
finally ssqr recommends the top masked queries here k with the highest information gain i.e.
minimum entropy values for users to choose.
experimental setup .
research questions we evaluate the performance of ssqr in query reformulation through both automatic and human studies.
we further explore the impact of different configurations on performance.
in specific we address the following research questions rq1 how effective is ssqr in query reformulation for code search?
we apply query reformulation to lucene and codebert based code search engines and compare the search accuracy before and after query reformulation by various approaches.
rq2 whether the queries reformulated by ssqr are more contentful and easy to understand?
in addition to the automatic evaluation of code search performance we also want to assess the intrinsic quality of the reformulated queries.
to this end we perform a human studyesec fse november san francisco usa yuetian mao chengcheng wan yuze jiang and xiaodong gu table statistics of datasets stage dataset of samples pre training codenn fine tuning codexglue search codexglue to assess whether the reformulated queries contain more information than the original ones and meanwhile conform to human reading habits.
rq3 how do different configurations impact the performance of ssqr ?
to obtain a better insight into ssqr we investigate the performance of ssqr under different configurations.
we firstly investigate the effect of different positioning strategies i.e.
what is the best criteria to select the expansion position in the original query.
we are also interested in the number of expansions for each query.
.
datasets we pre train fine tune and test all models using two large code search corpora codenn and codexglue .
they are nonoverlapping and thus alleviate the duplicated code issue between pre training and downstream tasks .
dataset for pre training .
we pre train t5 using code comments from the large scale codenn dataset.
codenn has been specifically processed for code search.
compared to codesearchnet this dataset has a much larger volume i.e.
more than million queries .
we take the first million for pre training.
dataset for code search .
we use the code search dataset of codexglue which provides queries and the corresponding code segments from multiple projects.
in this dataset each record has five attributes including the code segment in python repository url code tokens doc string i.e.
nl description of the function and the index of the code segment.
we split the original dataset into training and test sets with and samples respectively.
the training set is used to fine tune the codebert search engine while the test set is used as the search pool from which a search engine retrieves code.
all queries in the test phase are tests on the same pool.
the statistics of our datasets are summarized in table .
.
implementation details implementation of the pre trained model .
our model is implemented based on t5 base from the open source collection of huggingface .
we use the default tokenizer and input output lengths.
since huggingface does not provide an official pytorch script for t5 pre training we implement the pre training script based on the pytorch lightening framework .
we initialize t5 with the default checkpoint provided by huggingface and continually pre train it with our proposed cqc task.
the pre training takes epochs with a learning rate of 1e .
the batch size is set to in all experiments.
we set mandkin algorithm to and respectively.
since t5 is non deterministic ssqr can generate different queries for an input query at each run.
to guarantee thesame output at each run we fix the random seeds to and reload the same state dict of t5.
implementation of the search engines .
we experiment under two search engines based on codebert and lucene .
a codebert based search engine as our approach is built on pre trained models we first verify the effectiveness on a pretraining based search engine.
specifically we test our approach on the default search engine by codebert.
we reuse the implementation of the code search i.e.
text code task in codexglue .
then we fine tune the codebert base checkpoint on a training set from codexglue for epochs with a constant learning rate of 5e .
the lucene search engine besides the pre training based search engine we also test our approach on a classic search engine named lucene .
lucene is a keyword based search library that is widely adapted to a variety of code search engines and platforms .
we implement the lucene search engine based on the lucene core in java.
we extract the code segments from the test dataset from codexglue parse them by lucene s standardanalyzer and build their indexes.
we train all models on a linux server with ubuntu .
.
and a gpu of nvidia geforce rtx ti.
.
baselines we compare our method with the state of the art query reformulation approaches including a supervised method called sequer and unsupervised methods such as nlp2api lusearch and googleps.
supervised a supervised learning approach for query reformulation named sequer.
sequer leverages transformer to learn the sequence to sequence mapping between the original and the reformulated queries.
the method relies on a confidential parallel dataset of query evolution logs provided by stack overflow.
the dataset contains internal http requests processed by stack overflow s web servers within one year.
nlp2api a feedback based approach that expands query with recommended apis.
nlp2api automatically identifies relevant api classes collected from stack overflow using keywords in the initial search results and then expands the query with these api classes.
lusearch a knowledge based approach that expands a query with synonyms in wordnet .
the reformulated queries by lusearch are based on lucene s structural syntax which are too long and contain too many lucene specific keywords.
due to the constraint on the input length of t5 model i.e.
tokens we keep the synonyms and remove keywords about attribute names in lucene such as methbody and methname .
googleps the google query prediction service that gives real time suggestions on query reformulation.
we directly enter test queries into the google search box and manually collect the reformulated queries in rq2 and the case study.
we do not compare our method with googleps in rq1 because its search api is unavailable to us for processing a large number of queries.
besides our baseline modelself supervised query reformulation for code search esec fse november san francisco usa has demonstrated a great improvement over it in terms of mrr .
.
evaluation metrics the ultimate goal of query reformulation is to enhance search accuracy by using the reformulated queries.
in our experiments we first evaluate the search accuracy measured by the widely used mean reciprocal rank mrr .
mrr is defined as the average of the reciprocal ranks i.e.
the multiplicative inverse of the target post s rank of the search results for all the queries namely mrr q q i rank i whereqrefers to a set of queries and rank istands for the position of the first relevant document for the i th query.
a higher mrr indicates better search performance.
besides the indirect criteria in search performance query reformulation also aims to help users write more precise and high quality queries.
therefore we further define two metrics to measure the intrinsic quality of the reformulated queries informativeness measures how much information a query contains that contributes to code search.
we use this metric to evaluate how much information gain the reformulation brings to the original query.
naturalness measures how well a query is grammatically correct and follows human reading habits.
by using this metric we want the reformulation to be semantically coherent with the original query.
both metrics range from to .
higher scores indicate better performance.
results .
rq1 performance on code search as the ultimate goal of query reformulation we first evaluate whether the reformulated queries by ssqr lead to better code search performance.
we experiment under both search engines and compare the improvement of mrr scores before and after query reformulation by various methods.
for each query we calculate its similarity to code instances in the test set.
the top instances with the highest similarity are selected as the search results.
each query has one ground truth code instance in the test set.
we calculate the mrr scores by comparing the results and the ground truth code.
then for each method we select the first three reformulations and report the highest mrr score among them.
since the purpose of query reformulation is to hit the potential search intent of the user.
we believe that results with the maximum mrr in the top kreformulations are the most likely to satisfy this goal and are therefore considered meaningful.
the experimental results are presented in table .
ssqr enhances the mrr by .
and .
on the two search engines respectively.
compared to the two unsupervised baselines lusearch and nlp2api it brings a giant leap of over in search accuracy.
more surprisingly ssqr achieves competitive results to the supervised counterpart though it is not given with any annotations.
this indicates that our self supervised approach can assist developers totable performance of various approaches in code search search engine approach mrr codebertno reformulation .
supervised1 .
.
lusearch .
.
nlp2api .
.
ssqr ours .
.
luceneno reformulation .
supervised .
.
lusearch .
.
nlp2api .
.
ssqr ours .
.
1the result is not reproducible due to the unavailability of the confidential parallel data they use.
write high quality queries which ultimately leads to better code search results.
we notice that the performance of ssqr is slightly worse than the supervised counterpart with the lucene search engine.
this is probably because the supervised approach applies fixed expansion patterns to queries and therefore tends to expand queries with common fixed keywords.
these keywords can be easily hit by search engines based on keyword matching e.g.
lucene .
on the contrary ssqr does not use fixed expansion patterns and thus has more various keywords.
lucene is not able to perform keyword matching on them.
instead codebert which models the semantic relationships between keywords can understand queries expanded byssqr .
another interesting point is that lusearch and nlp2api do not contribute to the codebert based search engine.
this is probably because both approaches append words to the tail of the original query hence perturbing the semantics of the original query when we use deep learning based search engines such as codebert.
.
rq2 qualitative evaluation to evaluate the intrinsic quality of the reformulated queries we perform a human study with programmers.
four participants from author s institution but different labs are recruited through invitations.
all participants are postgraduates in the area of software engineering or natural language processing having over four year programming experience.
we took the first queries from the test set in rq1 and reformulated them using various methods including sequer lusearch nlp2api ssqr and googleps.
we assigned search tasks to human annotators using these queries and present the reformulated queries by various approaches.
the annotators were asked to search code using google and provide their ratings on a scale of to towards the reformulation in terms of informativeness and naturalness without knowing the source of the reformulation tool.
table summarizes the quality ratings by annotators.
overall ssqr achieves the most improvement in terms of naturalness and informativeness showing that it reformulated queries are more human like.
comparatively googleps and sequer have much less improvement.
lusearch and nlp2api even decrease theesec fse november san francisco usa yuetian mao chengcheng wan yuze jiang and xiaodong gu table human evaluation result approach naturalness informativeness no reformulation .
.
supervised .
.
.
.
lusearch .
.
.
.
nlp2api .
.
.
.
googleps .
.
.
.
ssqr ours .
.
.
.
naturalness and informativeness as they directly append relevant apis or synonyms to the tail of the original query and thus break the coherence of the query.
in particular compared with the strong baseline sequer ssqr obtains a greater improvement of .
in terms of informativeness while outperforming slightly in terms of naturalness.
the main reason could be that sequer applies three reformulation patterns i.e.
deleting unimportant words rewriting typos and adding keywords where only the last pattern increases the informativeness of the query.
besides sequer often adds keywords in a monotonous pattern such as appending in java at the tail of the queries meanwhile our method can generate diverse spans at the proper positions of the original queries.
consequently the reformulated queries by our approach are more informative.
.
rq3 performance under different configs in this experiment we evaluate the performance of ssqr in code search under different configurations with the codebert search engine.
we vary the positioning strategy and the number of candidate positions in order to search for the optimal configuration.
positioning strategies .
selecting expansion positions is critical to the performance.
we compare three strategies including the entropy based criterion rand randomly selects kpositions in the original query for expansion.
prob selects the top kpositions that have the maximum probability while predicting their missing content.
entr selects the top kpositions that have the minimum entropy while predicting their missing content.
the results are shown in table .
the prob and entr strategies bring a large improvement around to the code search performance.
this indicates that both criteria correctly quantify the missing information at various positions.
between these two strategies entr performs slightly better than prob probably because entr considers the entire distribution of the prediction while prob just considers the maximum one.
as expected the rand strategy causes a degradation of .
in code search performance because it selects expansion positions without any guidance which results in incorrect or redundant expansions.
number of candidate positions .
we also investigate how many expansions lead to the best performance.
we vary the number of candidate positions from to and verify their effects on performance.
table shows the results.
we observe that increasing the number of candidate positions has a positive effect on performance.
the best performance is achieved when candidate positions aretable performance of ssqr under different positioning strategies strategy mrr improvement rand .
.
prob .
.
entr .
.
table performance of ssqr under different candidate position numbers positions mrr improvement .
.
.
.
.
.
expanded.
meanwhile only one candidate position can have a negative effect on query reformulation.
the reason can be that our method reformulates the original query with a variety of query intents.
a larger number of candidate positions can hit more user intents and hence leads to better search accuracy.
.
qualitative analysis to further understand the capability of ssqr we qualitatively examine the reformulation samples by various methods.
four examples are provided in table .
example compares the reformulation for the query the total cpu load for the synology dsm by various methods.
the original query aims to find the code that monitors the cpu load of a dsm.
the reformulated query by ssqr is more precise to the real scenario since cpu load and memory usage are often important indicators that need to be monitored simultaneously.
in contrast sequer only removes the total at the beginning of the query during reformulation.
lusearch appends the query with the synonyms of the keyword total such as sum and aggreg .
meanwhile nlp2api appends the query with apis that are relevant to cpu and operating system which are more useful compared to those of sequer and lusearch.
googleps appends the word after dsm to indicate the version of dsm which helps to narrow the range of possible solutions.
in example the original query fetch the events is incomplete and ambiguous because the user does not specify what events to fetch and where to fetch them from.
the reformulated query by ssqr is more informative than that by sequer ssqr specifies the source of events i.e.
from the server which makes the query more concrete and understandable meanwhile sequer only restricts the programming language of the target code without alleviating the ambiguity of the original query.
lusearch expands the synonyms of fetch such as get and convei to the tail of the query.
nlp2api adds apis relevant to events to the original query.
but these synonyms and apis have limited effect on improving search accuracy.
googleps specifies the requirement of the query to be a service by adding service worker at the beginning of the original query.
but such a specification has a limited effect on narrowing the search space.self supervised query reformulation for code search esec fse november san francisco usa table examples of query reformulation by various methods original the total cpu load for the synology dsm sequer the cpu load the synology dsm lusearch the total cpu load for the synology dsm sum total aggreg nlp2api the total cpu load for the synology dsm operatingsystemmxbean proccpu string googleps the total cpu load for the synology dsm ssqr the total memory usage and cpu load for the synology dsm original fetch the events sequer fetch the events c lusearch fetch the events bring get convei nlp2api fetch the events calendarentry manytoone onetomany googleps service worker fetch event ssqr fetch the events from the server original get method that raises missingsetting if the value was unset.
sequer get method that raises missingsetting if the value was unset in c lusearch get method that raises missingsetting if the value was unset.
beget get engend nlp2api get method that raises missingsetting if the value was unset.
praveen kumar date googleps php foreach unset.
ssqr get method that raises an exception with missingsetting if the value was unset.
original load values from a dictionary structure.
nesting can be used to represent namespaces.
sequer load values from a dictionary structure.
nesting can be used to represent namespaces.
lusearch load values from a dictionary structure.
nesting can be used to represent namespaces.
cargo lade freight nlp2api load values from a dictionary structure.
nesting can be used to represent namespaces.
amino tkey tvalue googleps python namespace class ssqr load a list of values from a dictionary structure.
nesting can be used to represent namespaces.
example shows the results for the query get method that raises missingsetting if the value was unset.
the reformulated query by ssqr recognizes that missingsetting is an exception and prepends it with the exception keyword.
this facilitates the search engine to find code with similar functionality.
in contrast sequer just specifies the programming language of the target code.
compared to ssqr and sequer lusearch and nlp2api only append irrelevant apis and synonyms to the original query.
hence the semantics of the query are broken.
googleps fails to reformulate such a long query.
instead it returns a search query from other users that contains the keyword unset .
the returned results by googleps discard much information from the original query making deviate from the user intent.
finally the last example shows a worse case.
although ssqr achieves the new state of the art it might occasionally produce error reformulations.
ssqr prepends a modifier a list of in front of the word values which conflicts with dictionary in the given query and thus hampers the code search performance.
this is probably because the word values occurs frequently in the training corpus and often refers to elements in arrays and lists.
therefore ssqr tends to expand it with modifiers such as all the and a list of .
comparably sequer does nothing to the original query.
lusearch concerns load as the keyword and expands it.
nlp2api adds apis relevant to the key and value of the dictionary data structure which results in better search performance.
googleps cannot handle such a long query and just gives an irrelevant reformulation.
these examples demonstrate the superiority of ssqr in query reformulation for code search affirming the strong ability of bothposition prediction and span generation.
in future work we will conduct empirical research on the error types and improve our model for the challenging reformulations.
discussion .
strength of ssqr over fully supervised approaches?
one debatable question is what are the benefits of ssqr since it does not beat the sota fully supervised approach in terms of the code search metrics.
fully supervised methods such as sequer achieve the state ofthe art performance by sequence to sequence learning on a parallel query set.
however acquiring such parallel queries is infeasible since the query evolution log by search engines such as google and stack overflow is not publicly available.
besides the sequence tosequence approach tends to learn generic reformulation patterns e.g.
specifying the programming language or deleting a few irrelevant words.
compared to sequer ssqr does not rely on the supervision of parallel queries instead it is trained on a nonparallel dataset queries only that does not need to collect the ground truth reformulations.
this significantly scales up the size of training data and therefore allows the model to learn diverse reformulation patterns from a large number of code search queries.
ssqr provides an alternative feasible and cheap way of achieving the same performance.esec fse november san francisco usa yuetian mao chengcheng wan yuze jiang and xiaodong gu .
limitations and threats we have identified the following limitations and threats to our method patterns of query reformulation.
in this work we mainly explore query expansion the most typical class of query reformulation.
while query expansion is only designed to supplement queries with more information redundant or misspelled words in the query can also hamper the code search performance which cannot be handled by our method.
thus in future work we will extend our approach to support more reformulating patterns including query simplification and modification.
for example in addition to only inserting a token in the cqc task we can also replace the original words with a token or simply delete a token and ask the pre trained model to predict the deletion position.
a classification model can also be employed to decide whether to add delete or modify keywords in the original query.
code comments as queries.
as obtaining real code queries from search websites is difficult we use code comments from code search datasets to approximate code queries in building and evaluating our model.
although code comments are widely used for training machine learning models on nl pl matching they may not represent the performance of queries in real world code search engines.
related work .
query reformulation for code search query reformulation for code search has gained much attention in recent years .
there are approximately three categories of technologies namely knowledge based feedbackbased and deep learning based approaches.
the knowledge based approaches aim to expand or revise the initial query based on external knowledge such as wordnet and thesauri.
for example howard et al.
reformulated queries using semantically similar words mined from method signatures and corresponding comments in the source code.
satter and sakib proposed to expand queries with co occurring words in past queries mined from code search logs.
yang and tan constructed a software specific thesaurus named swordnet by mining code comment mappings.
they expanded queries with similar words in the thesaurus.
lu et al.
proposed lusearch which extends queries with synonyms generated from wordnet.
unlike knowledge based approaches feedback based approaches identify the possible intentions of the user from the initial search results and use them to update the original query.
for example rahman and roy proposed to search stack overflow posts using pseudo relevance feedback.
their approach identifies important api classes from code snippets in the posts using tf idf and then uses the top ranked api classes to expand the original queries.
hill et al.
presented a novel approach to extract natural language phrases from source code identifiers and hierarchically classify phrases and search results which helps developers quickly identify relevant program elements for investigation or identify alternative words for query reformulation.
recently deep learning has advanced query reformulation significantly .
researchers regard query reformulation as a machinetranslation task and employ neural sequence to sequence models.
for example cao et al.
trained a sequence to sequence model with an attention mechanism on a parallel corpus of original and reformulated queries.
the trained model can be used to reformulate a new query from stack overflow.
while deep learning based approaches show more promising results than previous approaches they rely on the availability of large high quality query pairs.
for example cao et al.
s work requires the availability of query pairs within the same session in the search logs of stack overflow.
but such logs are confidential and unavailable to researchers.
this restricts their practicality in real world code search.
unlike these works ssqr is a data driven approach based on self supervised learning.
ssqr expands queries by pre training a transformer model with corrupt query completion on large unlabeled data.
results demonstrate that ssqr achieves competitive results to that of fully supervised models without requiring data labeling.
.
code intelligence with pre trained language models in recent years there is an emerging trend in applying pre trained language models to code intelligence .
for example feng et al.
pre trained the codebert model based on the transformer architecture using programming and natural languages.
codebert can learn the generic representations of both natural and programming languages that can broadly support nlpl comprehension tasks e.g.
code defect detection and natural language code search and generation tasks e.g.
code comment generation and code translation .
wang et al.
proposed codet5 which extends the t5 with an identifier aware pre training task.
unlike encoder only codebert codet5 is built upon a transformer encoder decoder model.
it achieves state of the art performance on both code comprehension and generation tasks in all directions including pl nl nl pl and pl pl.
to the best of our knowledge ssqr is the first attempt to apply plm in query reformulation which aims to leverage the knowledge learned by plm to expand queries.
conclusion in this paper we propose ssqr a novel self supervised approach for query reformulation.
ssqr formulates query expansion as a masked query completion task and pre trains t5 to learn general knowledge from large unlabeled query corpora.
for a search query ssqr guides t5 through enumerating multiple positions for expansion and selecting positions that have the best information gain for expansion.
we perform both automatic and human evaluations to verify the effectiveness of ssqr .
the results show that ssqr generates useful and natural sounding reformulated queries outperforming baselines by a remarkable margin.
in the future we will explore other reformulation patterns such as query simplification and modification besides query expansion.
we also plan to compare the performance of our approach with large language models such as gpt .self supervised query reformulation for code search esec fse november san francisco usa data availability our source code and experimental data are publicly available at