operational calibration debugging confidence errors for dnns in the field zenan li state key lab of novel software technology nanjing university nanjing china lizenan smail.nju.edu.cnxiaoxing ma state key lab of novel software technology nanjing university nanjing china xxm nju.edu.cnchang xu state key lab of novel software technology nanjing university nanjing china changxu nju.edu.cn jingwei xu state key lab of novel software technology nanjing university nanjing china jingweix nju.edu.cnchun cao state key lab of novel software technology nanjing university nanjing china caochun nju.edu.cnjian l state key lab of novel software technology nanjing university nanjing china lj nju.edu.cn abstract trained dnn models are increasingly adopted as integral parts of software systems but they often perform deficiently in the field.
a particularly damaging problem is that dnn models often give false predictions with high confidence due to the unavoidable slight divergences between operation data and training data.
to minimize the loss caused by inaccurate confidence operational calibration i.e.
calibrating the confidence function of a dnn classifier against its operation domain becomes a necessary debugging step in the engineering of the whole system.
operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of dnn models.
we propose a bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data.
the approach is made effective and efficient by leveraging the locality of the learned representation of the dnn model and modeling the calibration as gaussian process regression.
comprehensive experiments with various practical datasets and dnn models show that it significantly outperformed alternative methods and in some difficult tasks it eliminated about to high confidence .
errors with only about of the minimal amount of labeled operation data needed for practical learning techniques to barely work.
ccs concepts software and its engineering software testing and debugging computing methodologies neural networks.
corresponding author.
esec fse november virtual event usa copyright held by the owner author s .
acm isbn .
operational calibration deep neural networks gaussian process acm reference format zenan li xiaoxing ma chang xu jingwei xu chun cao and jian l .
.
operational calibration debugging confidence errors for dnns in the field.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa 13pages.
to know what you know and what you do not know that is true knowledge.
confucius.
bc.
introduction deep learning dl has achieved human level or even better performance in some difficult tasks such as image classification and speech recognition .
deep neural network dnn models are increasingly adopted in high stakes application scenarios such as medical diagnostics and self driven cars .
however it is not uncommon that dnn models perform poorly in practice .
the interest in the quality assurance for dnn models as integral parts of software systems is surging in the community of software engineering .
a particular problem of using a previously well trained dnn model in an operation domain is that the model may not only make more than expected mistakes in its predictions but also give erroneous confidence values for these predictions.
the latter issue is particularly problematic for decision making because if the confidence values were accurate the model would be at least partially usable by accepting only high confidence predictions.
it needs to be emphasized that erroneous predictions with high confidence are especially damaging because users will take high stakes in them.
for example an over confident benign prediction for a pathology image could mislead a doctor into overlooking a malignant tumor.
the problem comes from the almost inevitable divergences between the original data on which a model is trained and the actual data in its operation domain which is often called domain shift ordataset shift in the machine learning literature.
it can be 901this work is licensed under a creative commons attribution noncommercial international .
license.
esec fse november virtual event usa z. li x. ma c. xu j. xu c. cao and j. l difficult and go beyond the stretch of usual machine learning tricks such as fine tuning and transfer learning because of two practical restrictions often encountered.
first the training data of a third party dnn model are often unavailable due to privacy and proprietary limitations .
second one can only use a small number of labeled operation data because it could be very expensive to label the data collected in the field.
for example in an ai assisted clinical medicine scenario surgical biopsies may have to be involved in the labeling of radiology images.
we consider operational calibration that corrects the error in the confidence provided by a dnn model for its prediction on each input in a given operation domain.
it does not change the predictions themselves but tells when the model works well and when not.
as the quantification of the intrinsic uncertainty in the predictions made by a model confidence values are integral parts of the model s outputs.
so operational calibration can be viewed as a kind of debugging activity that identifies and fixes errors in these parts of model outputs.
it improves the model s quality of service in the field with more accurate confidence for better decision making.
as a quality assurance activity operational calibration shall be carried out during the deployment of a previously trained dnn model in a new operation domain.
one can also incorporate it into the system and excise it from time to time to adapt the model to the evolving data distribution in the operation domain.
it is natural to model operational calibration as a case of nonparametric bayesian inference and solve it with gaussian process regression .
we take the original confidence of a dnn model as the prior and gradually calibrate the confidence with the evidence collected by selecting and labeling operation data.
the key insight into effective and efficient regression comes from two observations first the dnn model although suffering from the domain shift can be used as a feature extractor with which unlabeled operation data can be nicely clustered .
in each cluster the prediction correctness of an example is correlated with another one.
the correlation can be effectively estimated with the distance of the two examples in the feature space.
second gaussian process is able to quantify the uncertainty after each step which can be used to guide the selection of operation data to label efficiently.
systematic empirical evaluations showed that the approach was promising.
it outperformed existing calibration methods in both efficacy and efficiency in all settings we tested.
in some difficult tasks it eliminated about to high confidence errors with only about of the minimal amount of labeled operation data needed for practical learning techniques to barely work.
in summary the contributions of this paper are examining quality assurance for dnn models used as software components and raising the problem of operational calibration as debugging for confidence errors of dnns in the field.
proposing a gaussian process based approach to operational calibration which leverages the representation learned by the dnn model under calibration and the locality of confidence errors in this representation.
evaluating the approach systematically.
experiments with various datasets and models confirmed the general efficacy and efficiency of our approach.the rest of this paper is organized as follows.
we first discuss the general need for operational quality assurance for dnns in section and then define the the problem of operational calibration in section .
we detail our approach to operational calibration in section and evaluate it empirically in section .
we overview related work and highlight their differences from ours in section before concluding the paper with section .
quality assurance for dnn models used as software artifacts well trained dnn models can provide marvelous capabilities but unfortunately their failures in applications are also very common .
when using a trained model as an integral part of a high stakes software system it is crucial to know quantitatively how well the model will work and to adapt it to the application conditions.
the quality assurance combining the viewpoints from software engineering and machine learning is needed but largely missing.
in what follows we first discuss the non conventional requirements for such quality assurance and then give an application scenario to highlight the software engineering concerns.
deep learning is intrinsically inductive .
however conventional software engineering is mostly deductive as evidenced by its fundamental principle of specification implementation consistency.
a specification defines the assumptions and guarantees of a software artifact.
the artifact is expected to meet its guarantees whenever its assumptions are satisfied.
thus explicit specifications make software artifacts more or less domain independent.
however statistical machine learning does not provide such kind of specifications.
essentially it tries to induce a model from its training data which is intended to be general so that the model can give predictions on previously unseen inputs.
unfortunately the scope of generalization is unspecified.
as a result a major problem comes from the divergence between the domain where the model was originally trained and the domain where it actually operates.
so the first requirement for the quality assurance of a dnn model is to be operational i.e.
to focus on the concrete domain where the model actually operates.
logically speaking the quality of a trained dnn model will be pointless without considering its operation domain.
in practice the performance of a model may drop significantly with domain shift .
on the other hand focusing on the operation domain also relieves the dnn model from depending on its original training data.
apart from practical concerns such as protecting the privacy and property of the training data decoupling a model from its training data and process will also be helpful for re using it as a commercial off the shelf cots software product .
this viewpoint from software engineering is in contrasting to machine learning techniques dealing with domain shift such as transfer learning or domain adaptation that heavily rely on the original training data and hyperparameters .
they need original training data because they try to generalize the scope of the model to include the new operation domain.
the second requirement is to embrace the uncertainty that is intrinsic in dnn models.
a defect or a bug of a software artifact is a case that it does not deliver its promise.
different from conventional software artifacts a dnn model never promises to be certainly correct on any given input and thus individual incorrect 902operational calibration debugging confidence errors for dnns in the field esec fse november virtual event usa predictions per se should not be regarded as bugs but to some extent features .
nevertheless the model statistically quantifies the uncertainty of their predictions.
collectively it is measured with metrics such as accuracy or precision.
individually it is stated by the confidence value about the prediction on each given input.
these qualifications of uncertainty as well as the predictions a model made should be subject to quality assurance.
for example given a dnn model and its operation domain operational testing examines to what degree the model s overall accuracy is degraded by the domain shift.
furthermore operational calibration which is the topic of the current paper identifies and fixes the misspecified confidence values on individual inputs.
finally operational quality assurance should prioritize the saving of human efforts which include the cost of collecting and especially labeling the data in the operation domain.
the labeling of operation data often involves physical interactions such as surgical biopsies and destructive testings and thus can be expensive and time consuming.
note that as exemplified by the eu gdpr there are increasing concerns about the privacy and property rights in the data used to train dnn models.
so when adopting a dnn model trained by a third party one should not assume the availability of its original training data .
without the access to the original training data re training or fine tuning a dnn model to an operation domain can be unaffordable because it typically requires a large amount of labeled examples to work.
quality assurance activities often have to work under a much tighter budget for labeling data.
trained dnn modeldata selectionlabelingoperationalqa activityselecteddatasettraining datasetunlabeledoperationaldatasetdnntraining possible divergencesize reduceddomain shift modelwithqaiterationassessments adaptations for operation domaintraining data processmight be inaccessibleoperational quality assurance figure operational quality assurance figure depicts the overall idea for operational quality assurance which generalizes the process of operational testing proposed in .
a dnn model which is trained by a third party with the data from the origin domain is to be deployed in an operation domain.
it needs to be evaluated and possibly adapted with the data from the current operation domain.
to reduce the effort of labeling data selection can be incorporated in the procedure with the guidance of the information generated by the dnn model and the quality assurance activity.
only the dnn models that pass the assessments and are possibly equipped with the adaptations will be put into operation.
for example consider a scenario that a hospital decides to equip its radiology department with automated medical image analysis enabled by deep learning .
while the system may involve manyfunctionalities such as clinical workflow management computeraided diagnosis and computer assisted reporting the key component is a dnn model or an ensemble of dnn models acting as a radiologist to classify images .
it is too expensive and technically demanding for the hospital to collect enough high quality data and train the model in house.
so the hospital purchases the model from a third party provider and uses it as a cots software component.
however the training conditions of the model are likely to be different from the operation conditions and the model performance reported by the provider is unreliable due to potential issues such as data mismatch selection bias and non stationary environments .
to assure the system s quality of service the model must be tested and calibrated against the current operation domain.
the hospital first assesses the real accuracy of the model through operational testing and decides to adopt it or not accordingly.
once the model is adopted and deployed operational calibration steps in to adapt the model to the current operation domain by fixing the errors in the confidence values and avoiding high confidence false predictions.
as a software quality assurance task operational calibration tries to achieve best efficacy with a limited budget.
the cost here however is mainly spent on the labeling of operation data.
for sophisticated dnn models used for medical imaging classification such as inception v3 resnet and densenet even thousands of labeled data could be too less for model retraining or fine tuning to work properly c.f.
figure .
nevertheless as will be shown later a deliberately designed calibration method can identify and fix most of the erroneous high confidence values associated to false predictions with only tens to small hundreds of labeled data c.f.
figure .
a model well calibrated for its operation domain although still suffering from some loss in prediction accuracy becomes more reliable in that broken promises are far less likely.
operational calibration problem now we focus on the problem of operational calibration.
we first briefly introduce dnn classifiers and their prediction confidence to pave the way for the formal definition of the problem.
.
dnn classifier and prediction confidence a deep neural network classifier contains multiple hidden layers between its input and output layers.
a popular understanding of the role of these hidden layers is that they progressively extract abstract features e.g.
a wheel human skin etc.
from a highdimensional low level input e.g.
the pixels of an image .
these features provide a relatively low dimensional high level representationzfor the input x which makes the classification much easier e.g.
the image is more likely to be a car if wheels are present.
what a dnn classifier tries to learn from the training data is a posterior probability distribution denoted as p y x .
for a k classification problem the distribution can be written as pi x p y i x where i .
.
.
k. for each input x whose representation is z the output layer first computes the nonnormalized prediction h w z b whose element hiis often called the logit for the i th class.
the classifier then normalizes h 903esec fse november virtual event usa z. li x. ma c. xu j. xu c. cao and j. l with a softmax function to approximate the posterior probabilities pi x softmax h i ehi k j 1ehj i .
.
.
k. finally to classify x one just chooses the the category corresponding to the maximum posterior probability i.e.
y x arg max i pi x .
obviously this prediction is intrinsically uncertain.
the confidence for this prediction which quantifies the likelihood of correctness can be naturally measured as the estimated posterior class probability c x pi x i y x .
confidence takes an important role in decision making.
for example if the loss due to an incorrect prediction is four times of the gain of a correct prediction one should not invest on predictions with confidence less than .
.
modern dnn classifiers are often inaccurate in confidence because they overfit to the surrogate loss used in training .
simply put they are over optimized toward the accuracy of classification but not the accuracy of estimation for posterior probabilities.
to avoid the potential loss caused by inaccurate confidence confidence calibration can be employed in the learning process .
early calibration methods such as isotonic regression histogram binning and platt scaling simply train a regression model taking the uncalibrated confidence as input with the validation dataset.
more flexible methods e.g.
temperature scaling find a function rto correct the logit hsuch that c x pi x softmax r h i i y x matches the real posterior probability pi x .
notice that in this setting the inaccuracy of confidence is viewed as a kind of systematic error or bias not associated with particular inputs or domains.
that is the calibration does not distinguish between different inputs with the same uncalibrated confidence.
.
.
operational confidence calibration given a domain where a previously trained dnn model is deployed operational calibration identifies and fixes the model s errors in the confidence of predictions on individual inputs in the domain.
operational calibration is conservative in that it does not change the predictions made by the model but tries to give accurate estimations on the likelihood of the predictions being correct.
with this information a dnn model will be useful even though its prediction accuracy is severely affected by the domain shift.
one may take only its predictions on inputs with high confidence but switch to other models or other backup measures if unconfident.
to quantify the accuracy of the confidence of a dnn model on a dataset d xi yi i .
.
.
n one can use the brier score bs which is actually the mean squared error of the estimation bs d nn i i xi c xi where i x is the indicator function for whether the labeled input x is misclassified or not i.e.
i x 1if y x y x and 0otherwise.
now we formally define the problem of operation calibration problem.
givenma previously trained dnn classifier sa set ofnunlabeled examples collected from an operation domain and a budget n nfor labeling the examples in s the task of operational calibration is to find a confidence estimation function c formwith minimal brier score bs s .
notice that operational calibration is different from the confidence calibration discussed in section .
.
the latter is domainindependent and usually included as a step in the training process of a dnn model one of machine learning s focuses but the former is needed when the model is deployed as a software component by a third party in a specific operation domain what software engineering cares about .
technically operational calibration cannot take the confidence error as a systematic error of the learning process because the error is caused by the domain shift from the training data to the operation data and it may assign different confidence values to inputs with the same uncalibrated confidence value.
solving operational calibration with gaussian process regression at first glance operational calibration seems a simple regression problem with bs as the loss function.
however a direct regression would not work because of the limited budget of labeled operation data.
it is helpful to view the problem in a bayesian way.
at the beginning we have a prior belief about the correctness of a dnn model s predictions which is the confidence outputs of the model.
once we observe some evidences that the model makes correct or incorrect predictions on some inputs the belief should be adjusted accordingly.
the challenge here is to strike a balance between the priori that was learned from a huge training dataset but suffering from domain shift and the evidence that is collected from the operation domain but limited in volume.
.
modeling with gaussian process it is natural to model the problem as a gaussian process because what we need is actually a function c .
gaussian process is a non parametric kind of bayesian methods which convert a prior over functions into a posterior over functions according to observed data.
for convenience instead of estimating c directly we consider h x c x cm x where cm x is the original confidence output of mfor input x. at the beginning without any evidence against cm x we assume that the prior distribution of h is a zero mean normal distribution h n k where k is the covariance kernel function which intuitively describes the smoothness of h x from point to point.
in other words the covariance function ensures that hproduces close outputs when inputs are close in the input space.
assume that we observe a set of independent and identically distributed i.i.d.
labeled operation data i xi yi i n in whichy h x i x cm x .
for notational convenience let x xt .
.
.
xt n and h h x1 .
.
.
h xn 904operational calibration debugging confidence errors for dnns in the field esec fse november virtual event usa be the observed data and their corresponding y values and let x x t .
.
.
x n t and h h x .
.
.
h x n be those for a set t x i y i i .
.
.
m of i.i.d.
predictive points.
we have h h x x n kx x kx x kx x kx x where kis the kernel matrix.
therefore the conditional probability distribution is y y x x n where kx x kx x 1y kx x kx x kx x 1kx x .
with this gaussian process we can estimate the probability distribution of the operational confidence for any input x as follows h x x x h n where kx x kx x 1h kx x kx x kx x 1kx x .
then with equation we have the distribution of c x p c x x n cm x .
finally due to the value of confidence ranges from to we need to truncate the original normal distribution i.e.
p c x x tn tn tn where tn cm x 2 tn 2 cm x cm x .
here the and are the probability density function and the cumulative distribution function of standard normal distribution respectively.
with this bayesian approach we compute a distribution rather than an exact value for the confidence of each prediction.
to compute the brier score we simply choose the maximum a posteriori map i.e.
the mode of the distribution as the calibrated confidence value.
here it is the mean of the truncated normal distribution c x tn.
.
clustering in representation space directly applying the above gaussian process to estimate c would be ineffective and inefficient.
it is difficult to specify a proper covariance function in equation because the correlation between the correctness of predictions on different examples in the very high dimensional input space is difficult if possible to model.
fortunately we have the dnn model mon hand which can be used as a feature extractor although it may suffer from the problem of domain shift .
in this way we transform each input xfrom theinput space to a corresponding point zin the representation space which is defined by the output of the neurons in the last hidden layer.
it turns out that the correctness of m s predictions has an obvious locality i.e.
a prediction is more likely to be correct incorrect if it is near to a correct incorrect prediction in the representation space.
another insight for improving the efficacy and efficiency of the gaussian process is that the distribution of operation data in the sparse representation space is far from even.
they can be nicely grouped into a small number usually tens of clusters and the correlation of prediction correctness within a group is much stronger than that between groups.
consequently instead of regression with a universal gaussian process we carry out a gaussian process regression in each cluster.
this clustering does not only reduce the computational cost of the gaussian processes but also make it possible to use different covariance functions for different clusters.
the flexibility makes our estimation more accurate.
elaborately we use the rbf kernel k z1 z2 exp z1 z2 2l2 where the parameter l length scale can be decided according to the distribution of the original confidence produced by m. .
considering costs in decision the cost of misclassification must be taken into account in realworld decision making.
we propose to also measure how well a model is calibrated with the loss due to confidence error lce against a given cost model.
for example let us assume a simple cost model in which the gain for a correct prediction is and the loss for a false prediction isu.
the net gain if we take action on a prediction for input xwill bei x u i x .
we further assume that there will be no cost to take no action when the expected net gain is negative.
then the actual gain for an input xwith estimated confidence c x will be x i x u i x if c x if c x where u uis the break even threshold of confidence for taking action.
on the other hand if the confidence was perfect i.e.
c x if the prediction was correct and otherwise the total gain for dataset dwould be a constant gd n i 1i xi .
so the average lce over a dataset dwith nexamples is l d n gd n i 1 xi .
with the bayesian approach we do not have an exact c x but a truncated normal distribution of it.
if we take tn x as c x the above equations still hold.
cost sensitive calibration targets at minimizing the lce instead of the brier score.
notice that calibrating confidence with brier score generally reduces lce.
however with a cost model the optimization toward minimizing lce can be more effective and efficient.
.
selecting operation data to label in case that the set of labeled operation data is given we simply apply a gaussian process in each cluster in the representation space and get the posteriori distribution for confidence c .
however if we can decide which operation data to label we shall spend the budget for labeling more wisely.
905esec fse november virtual event usa z. li x. ma c. xu j. xu c. cao and j. l initially we select the operational input at the center of each cluster to label and apply a gaussian process in each cluster with this central input to compute the posterior probability distribution of the confidence.
then we shall select the most helpful input to label and repeat the procedure.
the insight for input selection is twofold.
first to reduce the uncertainty as much as possible one should choose the input with maximal variance 2 tn.
second to reduce the lce as much as possible one should pay more attention to those input with confidence near to the break even threshold .
so we chose x as the next input to label x arg minx tn x tn x .
with x and its label y we update the corresponding gaussian process model and get better tn and tn .
the select labelupdate procedure is repeated until the labeling budget is used up.
putting all the ideas together we have algorithm shown below.
the algorithm is robust in that it does not rely on any hyperparameters except for the number of clusters.
it is also conservative in that it does not change the predictions made by the model.
as a result it needs no extra validation data.
algorithm operational confidence calibration input a trained dnn model m unlabeled dataset scollected from operation domain d and the budget nfor labeling inputs.
output calibrated confidence function c x forxbelongs to d. build gaussian process models divide dataset sintolclusters using the k modroid method and label the inputs o1 .
.
.
olthat correspond to the centers of the lclusters.
initialize the labeled set t o1 .
.
.
ol .
for each of the clusters build a gaussian process model pi i .
.
.
l. while t ndo select a new input x s tfor labeling where xis searched by equation .
update the gaussian process corresponding to the cluster containing x. update the labeled set t t x .
end while compute confidence value for input x find the gaussian process model pcorresponding to the cluster containing input x. compute tn x according to equation .
output the estimated calibrated confidence c x tn x .
.
discussions to understand why our approach is more effective than conventional confidence calibration techniques one can consider the threepart decomposition of the brier score bs m m dm n conf dm acc dm m m n acc dm acc acc acc where dmis the set of inputs whose confidence falls into the interval im m m m m and the acc dm and conf dm are the expected accuracy and confidence in dm respectively.
the accis the accuracy of dataset d. in this decomposition the first term is called reliability which measures the distance between the confidence and the true posterior probabilities.
the second term is resolution which measures the distinctions of the predictive probabilities.
the final term is uncertainty which is only determined by the accuracy.
in conventional confidence calibration the model is assumed to be well trained and work well with the accuracies.
in addition the grouping of dmis acceptable because the confidence error is regarded as systematic error.
so one only cares about minimizing the reliability.
this is exactly what conventional calibration techniques such as temperature scaling are designed for.
however in operational calibration the model itself suffers from the domain shift and thus may be less accurate than expected.
even worse the grouping of dmis problematic because the confidence error is unsystematic and the inputs in dmare not homogeneous anymore.
consequently we need to maximize the resolution and minimize the reliability at the same time.
our approach achieves these two goals with more discriminative calibration that is based on the features of individual inputs rather than their logits or confidence values.
this observation also indicates that the benefit of our approach over temperature scaling will diminish if the confidence error happens to be systematic.
for example in case that the only divergence of the data in the operation domain is that some part of an image is missing our approach will perform similarly to or even slightly worse than temperature scaling.
however as can be seen from later experiments most operational situations have more or less domain shifts that temperature ccaling cannot handle well.
in addition when the loss for false prediction uis very small u .
as observed from experiments in the next section our approach will be ineffective in reducing lce.
it is expected because in this situation one should accept almost all predictions even when their confidence values are low.
empirical evaluation we conducted a series of experiments to answer the following questions is our approach to operational calibration generally effective in different tasks?
how effective it is compared with alternative approaches?
how efficient it is in the sense of saving labeling efforts?
we implemented our approach on top of the pytorch .
.
dl framework.
the code together with the experimental data are available at the experiments were conducted on a gpu server with two intel xeon gold cpu .30ghz 400gb ram and geforce rtx ti gpus.
the server ran ubuntu .
with gnu linux kernel .
.
.
the execution time of our operational calibration depends on the size of the dataset used and the architecture of the dnn model.
for the tasks listed below the execution time varied from about .5s to 50s which we regard as totally acceptable.
906operational calibration debugging confidence errors for dnns in the field esec fse november virtual event usa .
experimental tasks to evaluate the general efficacy of our approach we designed six tasks that were different in the application domains image recognition and natural language processing operation dataset size from hundreds to thousands classification difficulty from to classes and model complexity from 103to 107parameters .
to make our simulation of domain shifts realistic in four tasks we adopted third party operation datasets often used in the transfer learning research and the other two tasks we used mutations that are also frequented made in the machine learning community.
figure demonstrates some example images from the origin and operation domains for tasks and .
table lists the settings of the six tasks.
table dataset and model settings of tasks task modelorigin domain operation domain dataset acc.
size lenet 5digit recognition96.
.
mnist usps rnnpolarity99.
.
v1.
v2.
resnet 18image classification93.
.
000cifar stl vgg 19cifar .
.
orig.
crop resnet 50imageclef99.
.
c p 6inception v3imagenet77.
.
orig.
down sample it refers to the maximum number of operation data available for labeling.
in task we applied a lenet model originally trained with the images from the mnist dataset to classify images from theusps dataset .
both of them are popular handwritten digit recognition datasets consisting of single channel images of size but the latter is more difficult to read.
the size of the training dataset was and the size of the operation dataset was .
we reserved of the operation data for testing and used the other for operational calibration.
task was focused on natural language processing.
polarity is a dataset for sentiment analysis .
it consists of sentences labeled with corresponding sentiment polarity i.e.
positive or negative .
we chose polarity v1.
which contained movie reviews collected in as the training set.
the polarity v2.
which contained movie reviews collected in was used as the data from the operation domain.
we also reserved half of the operation data for testing.
in task we used two classic image classification datasets cifar10 and stl .
the former consists of images in classes and each class contains images.
the latter has only images but the size of each image is .
we used the whole cifar dataset to train the model.
the operation domain was represented by images collected from stl in which were used for calibration and the other were reserved for testing.tasks used the dataset cifar which was more difficult than cifar and contained classes with images in each.
we trained the model with the whole training dataset of images.
to construct the operation domain we randomly cropped the remaining images.
one half of these cropped images were used for calibration and the other half for testing.
task used the image classification dataset from the imageclef challenge .
it is organized with common classes derived from three different domains imagenet ilsvrc i caltech c and pascal voc p .
we chose the dataset c as the origin domain and dataset p as the operation domain.
due to the extremely small size of the dataset we divided the dataset p for calibration and testing by the ratio .
finally task dealt with an extremely difficult situation.
imagenet is a large scale image classification dataset containing more than .
million images across categories .
the pre trained model inception v3 was adopted for evaluation.
the operation domain was constructed by down sampling images from the original test dataset.
again half of the images were reserved for testing.
a cifar origin domain b stl operation domain c imageclef c origin domain d imageclef p operation domain figure examples of origin and operation domains.
in task a resnet model was trained with low resolution images a but applied to high resolution images b .
in task a resnet model was applied to images d with backgrounds and styles different from training images c .
.
efficacy of operational calibration table gives the brier scores of the confidence before col. orig.
and after col. gpr operational calibration.
in these experiments all operation data listed in table not including the reserved test data were labeled and used in the calibration.
the result unambiguously confirmed the general efficacy of our approach.
in the following we elaborate on its relationship with the fine tuning technique often employed in practice.
907esec fse november virtual event usa z. li x. ma c. xu j. xu c. cao and j. l table brier scores of different calibration methods task model orig.operational calibration conventional calibrationsargpr rfr svr ts ps conf.
ps logit ir lenet .
.
.
.
.
.
.
.
.
rnn .
.
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
.
vgg .
.
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
.
inception v3 .
.
.
.
.
.
.
.
orig.
before calibration.
gpr gaussian process based approach.
rfr random forest regression in the representation space.
svr support vector regression in the representation space.
ts temperature scaling .
ps platt scaling .
ir isotonic regression scaling .
conf.
logit indicates that the calibration took confidence value logit as input.
sar regression with surprise values .
we failed to evaluate sar on task because it took too long to run on the huge dataset.
.
.
calibration when fine tuning is ineffective.
a machine learning engineer might first consider to apply fine tuning tricks to deal with the problem of domain shift.
however for non trivial tasks such as our tasks and it can be very difficult if possible to fine tune the dnn model with small operation datasets.
figure shows the vain effort in fine tuning the models with all the operation data excluding test data .
we tried all tricks including data augmentation weight decay and regularization to avoid over fitting but failed to improve the test accuracy.
fortunately our operational calibration worked quite well in these difficult situations.
in addition to the improvement in brier scores reported in table we can also see the saving of lce for task in figure as an example.
our approach reduced about a half of the lce when .
which indicates its capability in reducing high confidence errors.
.
.
calibration when fine tuning is effective.
in case of easier situations that fine tuning works we can still calibrate the model to give more accurate confidence.
note that effective fine tuning does not necessarily provide accurate confidence.
one can first apply fine tuning until test accuracy does not increase and then calibrate the fine tuned model with the rest operation data.
for example we managed to fine tune the models in our tasks and .1task was the easiest to fine tune and its accuracy kept increasing and exhausted all the operational examples.
task was binary classification in this case our calibration was actual an effective fine tuning technique.
figure 5a shows that our approach was more effective and efficient than conventional finetuning as it converged more quickly.
for task with fine tuning the accuracy stopped increasing at about with about operational examples.
figure 5b show that the brier score would decrease more if we spent rest operation data on calibration than continuing on the fine tuning.
based on the significant .
.
reductions in brier scores in all of the tasks reported in table and the above discussions we conclude that the gaussian process based approach to operational calibration is generally effective and it is worthwhile no matter whether the fine tuning works or not .
1here we used some information of the training process such as the learning rates weight decays and training epochs.
fine tuning could be more difficult because these information could be unavailable in real world operation settings.
.
comparing with alternative methods first we applied three widely used calibration methods viz.temperature scaling ts platt scaling ps and isotonic regression ir with the same operation data used in our approach.
ts defines the calibration function rin equation as r h h t where tis a scalar parameter computed by minimizing the negative log likelihood on a validation dataset.
ps and ir directly work on the confidence values.
ps trains a one dimensional logistic regression and calibrates confidence as c e a cm b where a bare scalar parameters computed by minimizing the cross entropy on a validation dataset.
ir simply fits a monotonic confidence calibration function minimizing the brier score on a validation dataset.
note that in our experiment the operation data were used instead of the validated dataset.
we implemented ts according to guo et al .
.
for ps and ir we used the well known machine learning library scikit learn .
as shown in table ts although reported to be usually the most effective conventional confidence calibration method was hardly effective in these cases.
it even worsened the confidence in tasks and .
we observed that its bad performance came from the significantly lowered resolution part of the brier score which confirmed the analysis in section .
.
for example in task with temperature scaling the reliability decreased from .
to .
but the resolution dropped from .
to .
.
in fact in this case the calibrated confidence values were all very closed to .
after scaling.
however with our approach the reliability decreased to .
and the resolution also increased to .
.
the same reason also failed ps col. ps conf.
and ir col. ir .
we also included in our comparison an improved version of ps which built a regression over the logit instead of the confidence .
its calibration function rfor equation was r h wth b where wandbwere computed by minimizing the cross entropy on the operation data.
it performed much better than the original ps but still failed in tasks and .
second we also tried to calibrate confidence based on the surprise value that measured the difference in dl system s behavior between the input and the training data .
we thought it could be effective because it also leveraged the distribution of examples in the representation space.
we made polynomial regression between the confidence adjustments and the likelihood based surprise values.
unfortunately it did not work for most of the cases col. sar 908operational calibration debugging confidence errors for dnns in the field esec fse november virtual event usa epoch5060708090100accuracytraining test original a task epoch5060708090100accuracytraining test original b task epoch0102030405060accuracy training test original c task figure ineffective fine tuning of difficult tasks.
all available operation data and respectively were used.
figure loss due to confidence error 1050sample size0.
.
.
.
.
.19brier scorefine tunecalibration a task 5000sample size0.
.
.
.
.2brier scorefine tunecalibration b task figure calibration when fine tuning is effective in table .
we believe the reason is that surprise values are scalars and cannot provide enough information for operational calibration.
finally to examine whether gaussian process regression gpr is the right choice for our operational calibration framework we also experimented with two standard regression methods viz.
random forest regression rfr and support vector regression svr .
we used linear kernel for svr and ten decision trees for rfr.
as shown in table in most cases the non liner rfr performed better than the linear svr and both of them performed better than temperature scaling but worse than gpr.
the result indicates that calibration based on the features extracted by the model rather thanthe logits computed by the model is crucial the confidence error is non linear and unsystematic and the gaussian process as a bayesian method can provide better estimation of the confidence.
in summary our gpr approach achieved significant brier score reduction and outperformed conventional calibration methods and the surprise value base regression in all the tasks.
it also outperformed alternative implementations based on rfr and svr.
so we conclude that our approach is more effective than temperature scaling and other alternative choices for operational calibration .
.
efficiency of operational calibration in the above we have already shown that our approach worked with small operation datasets that were insufficient for fine tuning task and .
in fact the gaussian process based approach has a nice property that it starts to work with very few labeled examples.
we experimented the approach with the input selection method presented in section .
.
we focused on the number of high confidence false predictions which was decreasing as more and more operational examples were labeled and used.
we experimented with all the tasks but labeled only of the operation data.
table shows the numbers of high confidence false predictions before and after operational calibration.
as a reference we also include the numbers of high confidence correct predictions.
we can see that most of the high confidence false predictions were eliminated.
it is expected that there were less high confidence correct predictions after calibration because the actual accuracy of the models dropped.
the much lowered lce scores which took into account both the loss in lowering the confidence of correct predictions and the gain in lowering the confidence of false predictions indicate that the overall improvements were significant.
for a visual illustration of the efficiency of our approach figure plots the change of proportions of high confidence false and correct predictions as the size of data used in calibration increases.
it is interesting to see that most of the high confidence false predictions were identified very quickly and the approach was conservative but the conservativeness is gradually remedied with more labeled operation data used.
note that for tasks and usual fine tuning tricks did not work even with all the operation data labeled.
with our operational 909esec fse november virtual event usa z. li x. ma c. xu j. xu c. cao and j. l table reducing high confidence false predictions with operation data labeled no.
model correct pred.
false pred.
lce lenet .
.
.
.
.
.
.
.
.
.
rnn0.
.
.
.
.
.
.
.
.
.
3resnet .
.
.
.
.
.
.
.
.
.
vgg .
.
.
.
.
.
.
.
.
.
5resnet .
.
.
.
.
.
.
.
.
.
6inception .
.
.
.
.
v3 .
.
.
.
.
we ran each experiment times and computed the average numbers.
a task b task c task d task e task f task figure the proportion curve of high confidence inputs.
sample size means uncalibrated.
the calibration started to take effect with very few data.
calibration using only about of the data we avoided about and high confidence .
errors respectively.based on the results we can say that the gaussian process based operational calibration is efficient in detecting most of the highconfidence errors with a small amount of labeled operation data .
related work operational calibration is generally related to the quality assurance for deep learning systems in the software engineering community and the confidence calibration transfer learning and active learning in the machine learning community.
we briefly overview related work in these directions and highlight the connections and differences between our work and them.
.
software quality assurance for deep learning systems the research in this area can be roughly classified into four categories according to the kind of defects targeted defects in dl programs .
this line of work focuses on the bugs in the code of dl frameworks.
for example pham et al .proposed to test the implementation of deep learning libraries tensorflow cntk and theano through differential testing .
odena et al .
used fuzzing techniques to expose numerical errors in matrix multiplication operations .
defects in dl models .
regarding trained dnn models as pieces of software artifact and borrowing the idea of structural coverage in conventional software testing a series of coverage criteria have been proposed for the testing of dnns for example deepxplore deepgauge deepconcolic and surprise adequacy to name but a few.
defects in training datasets .
another critical element in machine learning is the dataset.
there exist research aiming at debugging and fixing errors in the polluted training dataset.
for example psi identifies root causes e.g.
incorrect labels of data errors by efficiently computing the probability of sufficiency scores through probabilistic programming .
defects due to improper inputs .
a dnn model cannot well handle inputs out of the distribution for which it is trained.
thus a defensive approach is to detect such inputs.
for example wang et al .
s approach checked whether an input is normal or adversarial by integrating statistical hypothesis testing and model mutation testing .
wang et al .proposed dissector which effectively distinguished unexpected inputs from normal inputs by verifying progressive relationship between layers .
more work in this line can be found in the machine learning literature under the name of out of distribution detection .
for a more comprehensive survey on the testing of machine learning systems one can consult zhang et al.
.
the main difference of our work compared with these pieces of research is that it is operational i.e.
focusing on how well a dnn model will work in a given operation domain.
as discussed in section without considering the operation domain it is often difficult to tell whether a phenomena of a dnn model is a bug or a feature .
an exception is the recent proposal of operational testing for the efficient estimation of the accuracy of a dnn model in the field .
arguably operational calibration is more challenging and more rewarding than operational testing because the latter only 910operational calibration debugging confidence errors for dnns in the field esec fse november virtual event usa tells the overall performance of a model in an operation domain but the former tells when it works well and when not.
.
dnn confidence calibration confidence calibration is important for training high quality classifiers.
there is a plethora of proposals on this topic in the machine learning literature .
apart from the temperature scaling discussed in section .
isotonic regression histogram binning and platt scaling are also often used.
isotonic regression is a non parametric approach that employs the least square method with a non decreasing and piecewise constant fitted function.
histogram binning divides confidences into mutually exclusive bins and assigns the calibrated confidences by minimizing the bin wise squared loss.
platt scaling is a generalized version of temperature scaling.
it adds a linear transformation between the logit layer and the softmax layer and optimizes the parameters with the nll loss.
however according to guo et al .
temperature scaling is often the most effective approach .
as discussed earlier in section .
the problem of these calibration methods is that they regard confidence errors as systematic errors which is usually not the case in the operation domain.
technically these calibration methods are effective in minimize the reliability part of the brier score but ineffective in dealing with the problem in the resolution part.
in addition flach discussed the problem of confidence calibration from a decision theoretic perspective .
however the confidence error caused by domain shift was not explicitly addressed.
previous research efforts on confidence calibration at prediction time instead of training time are uncommon but do exist.
gal and ghahramani proposed to build a temporary ensemble model by using dropout at prediction time .
despite its elegant bayesian inference framework the method is computationally too expensive to handle large scale tasks.
recently based on the insight of conformal prediction papernot and mcdaniel proposed to build a k nearest neighbors knn model for the output of each dnn immediate layer .
the confidence of a prediction was estimated by the conformity of these knns outputs.
unfortunately this method is not applicable to non trivial tasks either because of its incapability in handling high dimensional examples.
note that these methods did not explicitly consider confidence errors caused by domain shifts.
another related line of work is under the name of uncertainty estimation .
for example evidential deep learning qualitatively evaluates the uncertainty of dnn predictions with a dirichlet distribution placed on the class probabilities .
however these methods mainly aim at out of distribution detection which is considered easier than confidence calibration .
identifying out ofdistribution inputs is useful in defending against adversarial attacks but not directly helpful in adapting a model to a new operation domain.
.
transfer learning and active learning our approach to operational calibration borrowed ideas from transfer learning and active learning .
transfer learning or domain adaptation aims at training a model from a source domain origin domain in our terms that can be generalized to a target domain operation domain despite the dataset shift between the domains.
the key is to learn features that are transferable between the domains.
however transfer learning techniques usually require data from both of the source and target domains.
contrastingly operational calibration often has to work with limited data from the operation domain and no data from the origin domain.
transfer learning usually fails to work under this constraint.
even in case it works it does not necessarily produce well calibrated models and operational calibration is needed to correct confidence errors cf.
figure 5b .
active learning aims at reducing the cost of labeling training data by deliberately selecting and labeling inputs from a large set of unlabeled data.
for the gaussian process regression there exist different input selection strategies .
we tried many of them such as those based on uncertainty on density and on disagreement but failed to find a reliable strategy that can further improve the data efficiency of our approach.
they were very sensitive to the choices of the initial inputs the models and the distribution of examples .
however we found that the combination of cost sensitive sampling bias and uncertainty can help in reducing high confidence false predictions especially in a cost sensitive setting.
conclusion software quality assurance for systems incorporating dnn models is urgently needed.
this paper focuses on the problem of operational calibration that detects and fixes the errors in the confidence given by a dnn model for its predictions in a given operation domain.
a bayesian approach to operational calibration is given.
it solves the problem with gaussian process regression which leverages the locality of the operation data and also of their prediction correctness in the representation space.
experiments with representative datasets and dnn models confirmed that the approach can significantly reduce the risk of high confidence false prediction with a small number of labeled data and thus efficiently improve the models quality of service in operational settings.
while with empirical evidence we consider conducting more theoretical analysis on aspects such as the data efficiency and the convergence of our algorithm as future work.
in addition we plan to investigate operational calibration methods for real world decisions with more complicated cost models.