fixing large language models specification misunderstanding for better code generation zhao tian college of intelligence and computing tianjin university china tianzhao tju.edu.cnjunjie chen college of intelligence and computing tianjin university china junjiechen tju.edu.cnxiangyu zhang department of computer science purdue university usa xyzhang cs.purdue.edu abstract code generation is to automatically generate source code conforming to a given programming specification which has received extensive attention especially with the development of large language models llms .
due to the inherent difficulty of code generation the code generated by llms may not be aligned with the specification.
although thought eliciting prompting techniques have been proposed to enhance the code generation performance of llms producing correct understanding for complicated programming problems remains challenging resulting in unsatisfactory performance.
also some feedbackbased prompting techniques have been proposed to fix incorrect code using error messages produced by test execution.
however when the generated code deviates significantly from the ground truth they encounter difficulties in improving performance based on such coarse grained information.
in this work we propose a novel prompting technique called fix to improve the code generation performance of llms by devising both sophisticated thought eliciting prompting and feedback based prompting and making the first exploration on their synergy.
it first exploits test case analysis to obtain specification understanding and enables a self improvement process to identify and refine the misunderstanding in the thoughteliciting prompting phase.
fix further fixes the specification understanding towards the direction reducing the gap between the provided understanding from the first phase and the actual understanding implicitly utilized by llms for code generation in the feedback based prompting phase.
by improving the understanding with fix the code generation performance of llms can be largely improved.
our evaluation on two advanced llms chatgpt and deepseek coder with six widely used benchmarks by comparing with baselines demonstrates the effectiveness of fix.
for example fix outperforms the most effective baseline with an average improvement of .
in terms of pass across all subjects.
index terms code generation large language models prompting engineering i. i ntroduction code generation aims to automatically generate source code conforming to a given programming specification which is usually a natural language description .
it can help reduce repetitive programming efforts and improve software development productivity.
in recent years code generation has received extensive attention from both academia and industry.
in particular with llms being rapidly developed significant progress has been made in code generation such junjie chen is the corresponding author.as chatgpt and deepseek coder .
the llms take the programming specification i.e.
prompt as input and output the corresponding code solution demonstrating notable advancements in code generation.
despite their popularity llms still suffer from some performance issues.
that is the generated code may be not aligned with the human provided specification especially when the programming logic is complicated .
for example one stateof the art llm i.e.
chatgpt generates code passing all test cases for only .
of programming problems in a realworld benchmark .
the performance issues can negatively affect the practical use of llms even slow down software development process and harm software quality.
hence it is important to enhance the ability of llms in code generation.
although fine tuning strategies have been widely adopted to improve the code generation performance of llms they are time consuming and require a large amount of computing resources .
in recent years prompting techniques have been proposed to achieve this goal in a plug and play manner .
among them thought eliciting prompting is the most popular category.
it aims to elicit llms to produce intermediate reasoning steps as the specification understanding for more accurate code generation.
the typical thought eliciting prompting techniques include cot that elicits llms to produce intermediate natural language reasoning steps self planning that guides llms to decompose the specification into a set of easy to solve subproblems and produce code plans to facilitate code generation scot that enhances cot by utilizing program structures to build intermediate reasoning steps thereby eliciting llms for code generation and so on.
feedback based prompting is another category of prompting techniques such as selfdebugging self edit and self repair which leverages error messages produced by test execution to enable llms to fix incorrectly generated code.
although these techniques have been studied extensively their performance still needs to be improved.
for example the existing thought eliciting prompting techniques have difficulties in producing correct understanding according to the concise specification when facing with complicated programming problems .
moreover they cannot identify or fix incorrect specification understanding that occurs before 1code generation leading to inaccurate results.
for feedbackbased prompting the existing techniques just utilize error messages produced by test execution to understand why the generated code is incorrect which is too coarse grained to identify root causes and thus leads to suboptimal performance.
if the generated code deviates largely from the ground truth it is quite hard for them to improve the performance based on error messages from such low quality generated code .
in particular the two categories play roles at different stages the former working before test execution whereas the latter after but there is no existing work exploring their synergy.
to improve the code generation performance of llms we propose a novel prompting technique called fix misunderstanding fixing to overcome the aforementioned limitations.
in particular fix is the first to explore the synergy of the above two categories by devising both thoughteliciting prompting and feedback based prompting.
the key insight is that by improving specification understanding in the thought eliciting prompting phase via test case analysis the effectiveness of the subsequent feedback based prompting and code generation can be enhanced.
namely even though llms may not generate correct code based on the specification understanding it can make the generated code as close to the ground truth as possible which can provide a greater chance for the subsequent feedback based prompting to further improve the code generation performance.
in the thought eliciting prompting phase the key lies in improving llms understanding of the specification.
we propose to achieve this by leveraging test cases inherently included as part of a specification.
note that such test cases are prevalent in practice including those widely studied datasets in code generation e.g.
humaneval and apps .
due to the intrinsic reasoning capabilities of llm if its understanding is correct it shall be able to resolve a natural language request derived from the embedded test cases and produce expected outputs.
in contrast when the understanding is incorrect by prompting llm to fix the test outputs we could improve the understanding thereby enhancing the later code generation performance.
this is analogous to how software developers use test case examples to understand complicated programming logic in practice .
although the understanding can be refined to infer the correct test outputs it does not mean that the corresponding generated code can really pass the test cases in actual execution due to the gap between specification understanding and code generation.
specifically specification understanding and code generation emphasize different aspects of abilities in llms where the former relies on the reasoning ability of llms while the latter emphasizes the ability of translating the natural language description into the source code .
hence when invoking llms to generate code the actual understanding implicitly utilized by llms may be inconsistent with the provided understanding.
fix further designs feedback based prompting to improve code generation performance if the test execution fails .
instead of prompting llms to directly fix the generated code with coarse grained error messages fixprompts llms to understand the root cause i.e.
the abovementioned gap by comparatively analyzing the provided understanding and the actual understanding obtained via code summarization and then adjust the natural language description for the specification understanding to enhance its accessibility to the code generation ability of llms.
it is also a kind of fixing on specification understanding which aims to make the understanding be better utilized by the code generation ability of llms.
we conducted extensive experiments to evaluate fix on two state of the art llms i.e.
chatgpt and deepseekcoder based on six widely used benchmarks.
our results show that fix significantly outperforms all the compared prompting techniques on both llms across all six benchmarks demonstrating our idea for enhancing code generation performance by improving llms specification understanding.
for example the average improvement of fix over all compared techniques is .
.
in terms of pass measuring the ratio of programming problems for which the generated code passes all evaluation test cases across all subjects.
moreover we constructed four variants of fix for the ablation study.
the results confirm the contribution of our thought eliciting prompting and feedback based prompting strategies in fix.
we summarize our contributions in this work as follows we propose a novel prompting technique called fix to enhance code generation performance of llms by improving their specification understanding with both sophisticated thought eliciting prompting and feedback based prompting.
we design a novel thought eliciting prompting strategy which exploits test case analysis to produce more accurate specification understanding and enables a self improvement process to identify and refine the misunderstanding before generating code.
we design a novel feedback based prompting strategy that adjusts the understanding towards the direction reducing the gap between the provided understanding and the actual understanding implicitly utilized by llms for code generation.
we conducted extensive experiments on two llms i.e.
chatgpt and deepseek coder across six widely used benchmarks by comparing with baselines demonstrating the effectiveness of fix for improving the code generation performance of llms.
ii.
a nillustrating example we use an example to illustrate why the state of theart prompting techniques do not work well motivating our idea.
figure shows the simplified example sampled from the real world humaneval benchmark .
in this figure the programming specification provides a natural language description including a test case .
recent work has shown that prompting techniques can obtain the understanding of the specification by exploiting the logical reasoning ability of llms thereby improving their performance in code generation .
here we employed two state of the art thought eliciting prompting techniques programming specification description def solve n given a positive integer n return the total sum of its digits in binary.
variables n integer.
constraints n .
output a string of binary number.
test cases assert solve cot chain of thought let s think it step by step .
convert the decimal number n to binary.
.
calculate the sum of the binary digits.
.
convert the sum of binary digits to its binary representation.
.
return the result as a string representing the binary number.
code def solve n binary representation bin n binary digits binary representation binary sum for digit in binary digits binary sum int digit binary sum bin binary sum return binary sum self repair repair binary sum bin binary sum scot structured chain of thought input n a positive integer output binary sum a string representing the total sum of n s digits in binary convert n to a string to iterate through its digits.
initialize binary sum as an empty string.
for each digit in the string representation of n convert digit to its binary representation and append it to binary sum.
return binary sum.
code def solve n n str str n binary sum for digit in n str binary sum bin int digit return binary sum self repair repair binary sum format int digit 04b fix initial specification understanding the input is .
the output is .
analysis is represented as in binary and the sum of its digits in binary is .
therefore the expected output is .
refined specification understanding the input is .
the output is .
analysis the sum of its digits is .
in binary is represented as .
therefore the expected output is .
adjusted specification understanding the input is .
the output is .
analysis the sum of its decimal digits is .
when converted to binary the decimal sum is represented as in binary.
therefore the expected output is .
code def solve n digits str n digit sum sum int digit for digit in digits binary representation bin digit sum return binary representation incorrect understanding incorrect code incorrect fixed codeincorrect understanding incorrect code incorrect fixed codeincorrect understanding refined understanding correct codeadjusted understandingfig.
.
an example from humaneval with chatgpt i.e.
cot and scot to prompt the state of the art llm i.e.
chatgpt for generating code with regard to this specification.
as shown in figure both cot and scot obtained the understanding of the specification by dividing it into several intermediate reasoning steps i.e.
chain of thought and structured chain of thought .
then they integrated these detailed reasoning steps with the original specification as the prompt facilitating chatgpt for code generation.
however we found that both obtained understandings are incorrect due to the complicated programming logic implied in the specification leading to generating incorrect code.
moreover both cot and scot are incapable of checking the correctness of the specification understanding before code generation further limiting their performance.
this motivates the importance ofimprovingspecification understand ingbefore code generation forathought elicitingprompt ingtechnique.
subsequently we employed the state of the art feedbackbased prompting technique self repair to fix the incorrect code generated by cot and scot.
it leverages error messages produced by test execution on the incorrect code to understand why it is incorrect and then prompts chatgpt to fix the code accordingly.
unfortunately both fixed code remained incorrect due to the substantial deviation of the initially generated code from the ground truth.
it is quite hard to improve the code generation performance solely based on error messages from such low quality code.
this motivates that afeedback based prompt ingtechnique requires notonly possessingtheabilitytoimprove incorrectly generated code butalso startingwith high qualitycode.
the two categories of techniques operate at different stages the former working before test execution whereas the latter after .
the output of thought eliciting prompting i.e.
code generated by llms enhanced by the specification understand ing serves as the input of feedback based prompting.
that is the effectiveness of the former can affect that of the latter thereby affecting the code generation performance but there is no existing work exploring this synergy.
this motivates us toexplore thesynergy ofthese twocategories forbettercode generation.
specifically we can first improve the specification understanding for generating the code as close to the ground truth as possible in the thought eliciting prompting phase and then design a more effective feedback based prompting strategy for generating more accurate code on the basis of high quality code from the first phase.
with this insight we propose a novel prompting technique called fix which leverages the test cases inherent in the specification to achieve this goal.
with fix the improved specification understanding is indeed produced and also the correct code is generated for this example in figure .
iii.
a pproacha.
overview we propose a novel prompting technique called fix to improve the code generation performance of llms.
fix devises both sophisticated thought eliciting prompting and feedback based prompting as the first exploration on the synergy of both categories of prompting techniques.
the key insight is to utilize test cases inherently included in specifications to improve specification understanding inspired by the practice where software developers often use test cases to understand complex programming logic.
it is helpful to generate high quality code close to the ground truth as the starting point for the feedback based prompting phase to further improve llm performance.
in practice test cases are commonly included in programming specifications and we also evaluated the influence of the number of test cases on the effectiveness of fix in section vi b. if a specification 3lacks test cases existing work has demonstrated the effectiveness of llms in generating them thereby ensuring the feasibility of fix as detailed in section vi b. figure shows the overview of fix.
it consists of two phases the thought eliciting prompting phase section iii b which emphasizes the analysis on test cases for producing specification understanding and enables a selfimprovement process to refine the misunderstanding with the aid of test cases and the feedback based prompting phase section iii c which comparatively analyzes the provided understanding and the actual understanding implicitly utilized by llms for code generation and minimizes the gap between the two for better code generation.
the latter is activated only if the generated code does not pass the test cases used for obtaining the specification understanding in actual execution.
for ease of understanding on our fix we reuse the example i.e.
humaneval introduced in section ii to illustrate the details of fix.
b. thought eliciting prompting phase in the thought eliciting prompting phase fix takes the programming specification as input and aims to output more accurate specification understanding by leveraging the test cases inherently included in the specification.
on one hand emphasizing test case analysis helps llms improve specification understanding because test cases contain more specific details that facilitate the understanding of complicated programming logic.
on the other hand test cases enable the self improvement process to refine llms misunderstanding before generating code.
in the self improvement process fix first checks the correctness of the specification understanding and then improves it if it is a misunderstanding .
note that existing thought eliciting prompting techniques cannot identify and refine misunderstanding before generating code due to overlooking the importance of such test cases.
overall the thought eliciting prompting phase in fix consists of three steps initial understanding generation correctness checking of the understanding and misunderstanding fixing.
these steps will be detailed in the following.
initial understanding generation as shown in figure existing thought eliciting prompting techniques also take the programming specification including test cases as input but they do not pay special attention to these test cases thereby limiting their effectiveness.
instead fix emphasizes test case analysis by providing a structured instruction to llms which can help them utilize specific test cases to understand complicated programming logic.
the initial prompt in figure shows the structured instruction activating test case analysis which facilitates explaining the programming logic that processes the inputs provided in the test cases to get the expected output specified in the test cases.
then fix enables llms to complete ?
in the instruction producing initial specification understanding.
for example figure initial understanding shows the initial understanding produced by chatgpt with test case analysis regarding the programming specification in figure .correctness checking of the understanding from figure we find that the initial understanding is incorrect even though the expected output shown in the understanding is correct.
actually the expected output is correctly specified due to its leakage in the test cases as shown in the programming specification of figure .
with this specification misunderstanding llms hardly generate correct code.
hence with the aid of test cases fix includes a mechanism to check the correctness of the understanding in advance in order to have a chance for improving the understanding before generating code.
specifically fix masks the expected output in the understanding and enables llms to infer it according to the analyzed logic in the understanding.
if the inferred output differs from the expected output it indicates an incorrect specification understanding.
as shown in figure this mechanism is effective to identify the misunderstanding produced in figure .
misunderstanding fixing once fix identifies a specification misunderstanding it prompts llms to refine this misunderstanding by providing the corresponding instruction e.g.
the above understanding is incorrect.
please provide the correct analysis... .
this process terminates until the refined understanding passes the checking mechanism or the number of refinements reaches the pre defined threshold denoted asn .
note that passing the checking mechanism cannot guarantee that llms completely understand the programming specification as the number of test cases equipped in the specification tend to be limited.
however at least the refined understanding as shown in refined specification understanding of figure passing the checking mechanism is more accurate than that does not pass it.
with such highquality refined understanding llms may directly generate correct code.
if not fix still provides a high quality starting point for its subsequent feedback based prompting phase for further improving code generation performance.
c. feedback based prompting phase by integrating the more accurate specification understanding into the programming specification along with an additional instruction please implement the function in a markdown style code block pay attention to the specification understanding fix prompts llms to generate code.
although the understanding produced in the thoughteliciting prompting phase is more accurate regarding the test cases it does not guarantee that the corresponding generated code will pass the test cases in actual execution.
this is due to the gap between specification understanding and code generation which emphasize different aspects of abilities in llms.
that is the actual understanding implicitly utilized by llms for code generation may be inconsistent with the provided understanding.
for example when prompting llms to generate code they may miss some important contents in the provided understanding due to the natural languagedescription style unfamiliar to this ability of llms.
existing feedback based prompting techniques leverage error messages produced by test execution to prompt llms for enhancing code generation .
this kind of information 4understanding generation component initial refined understandingthought eliciting prompting phase programming specification yestest cases in specification reaching termination condition?
feedback based prompting phase no adjusted understandingnew codeuserinitial code test execution adjustment componentmisunderstanding fixing component incorrect?no summarized understanding yes correctness checking component reaching termination condition?no yespassed?
no yesfig.
.
overview of fix initial prompt programming specification instruction let s analyze the test case step by step.
you must follow the format the input is ?
.
the output is ?
.
analysis ?
.
therefore the expected output is ?
.
initial understanding the input is .
the output is .
analysis is represented as in binary and the sum of its digits in binary is .
therefore the expected output is .
fig.
.
an example of fix in understanding generation inference prompt description initial understanding the input is .
analysis is represented as in binary and the sum of its digits in binary is .
therefore the expected output is ?
.
instruction based on the initial understanding please provide the expected output for the test case.
you must follow the format therefore the expected output is ?
.
inferred output therefore the expected output is .
fig.
.
an example of fix in correctness checking of the understanding is too coarse grained to identify the root cause of incorrect code especially when the generated code significantly deviates from the ground truth leading to unsatisfactory performance.
different from them fix prompts llms to understand the root cause i.e.
the aforementioned gap by comparatively analyzing the provided refined understanding and the actual understanding implicitly utilized by llms for code generation.
here based on the symmetry between code generation from natural language descriptions to code and code summarization from code to natural language descriptions fix estimates the actual understanding implicitly utilized by llms for code generation through code summarization also called summarized understanding for ease of presentation .
as shown in summarization prompt of figure fix prompts llms to produce the summarized understanding summarized understanding of figure .
fix then adjusts the natural language description of the refined specification understanding towards the direction reducing the gap by exploiting the logical reasoning ability of llms.
for example some important contents missed by the summarized understanding can be highlighted or the unfamiliar natural language description style to llms can be improved accordingly through comparative analysis.
here we design an adjustment prompt adjustment prompt of figure to enable llms to achieve the above goal which consists of the generated code test execution results the refined understanding produced from the thought eliciting prompting phase summarized understanding and an additional instruction guiding llms for adjusting understanding.
taking refined specification understanding of figure the provided refined understanding and summarized understanding of figure the summarized understanding as an example for illustration the comparative analysis between them implies that llms just capture some keywords e.g.
digits binary and sum but fail to understand the logic among keywords in the summarized understanding which provides a direction to adjust the understanding.
indeed the adjusted understanding emphasizes the correct computation logic with as shown in adjusted specification understanding of figure .
with the adjusted understanding by fix chatgpt successfully generates correct code for the programming specification as shown in figure .
this phase is an iterative process and terminates until the generated code passes the corresponding test cases or the number of adjustments reaches the threshold denoted as m .
in fact the adjustment process can be also regarded as a kind of fixing on specification understanding which aims to ensure that the understanding is being correctly utilized during the code generation.
that is the two phases in fix involve complementary aspects of specification understanding fixing and synergetically enhance code generation performance.
note that both fixing components are not simple regeneration.
similar to human centric iterative improvement processes e.g.
debugging they essentially provide counter factual evidence e.g.
historical incorrect cases and their explanations as feedback to llms.
analogous to human processes they may not succeed potentially due to the limited ability of llms or insufficient feedback to llms.
in the future we will explore more effective llms in this aspect and more informative feedback so as to further enhance both fixing components.
iv.
e valuation design our study addresses the following research questions rqs .
rq1 how does fix perform in improving the code generation performance of llms compared to the stateof the art prompting techniques?
summarization prompt initial code test cases instruction let s analyze the test case step by step.
you must follow the format the input is ?
.
the output is ?
.
analysis ?
.
therefore the expected output is ?
.
summarized understanding the input is .
the output is .
analysis the binary representation of its decimal digits is .
the sum of the binary digits is .
therefore the expected output is .
adjustment prompt refined understanding initial code test execution result summarized understanding instruction given the correct refined understanding the initial code is generated incorrectly leading to an incorrect summarized understanding .
please regenerate a more accurate analysis of the test case for generating correct code.
you must follow the format the input is ?
.
the output is ?
.
analysis ?
.
therefore the expected output is ?
.
fig.
.
an example of fix in adjusting the understanding.
for saving space we show the refined understanding and adjusted understanding in refined specification understanding and adjusted specification understanding of figure respectively.
rq2 does each main component in fix contribute to the overall effectiveness?
rq3 how does fix perform under different hyperparameters configurations?
a. studied llms following the openai s news codex s access was discontinued and gpt .
has been recommended instead and thus we selected gpt .
chatgpt as the representative commercial llm in our study following the existing studies .
here we did not choose gpt due to its high cost.
following the existing studies we selected deepseek coder as the representative opensource llm since it has exhibited state of the art effectiveness among open source llms across multiple programming languages and various benchmarks in terms of coding capabilities .
based on the two state of the art llms in code generation the generality of fix can be investigated to some extent.
specifically we used chatgpt version gpt .5turbo via openai s apis and deepseek coder version deepseek coder .7b instruct from huggingface .
note that llm generated code often includes natural language text snippets leading to compilation failures.
therefore following the existing work we employed a code sanitizer tool to clean llm generated code.
b. benchmarks to sufficiently evaluate fix we adopted six widely used datasets in our study i.e.
humaneval humaneval apps humaneval et apps et and mbppet .
these datasets have been widely used in many existing studies to measure the performance of prompting techniques in code generation .
humaneval has human written programming problems proposed by openai.
the specification for each problem consists of a function signature a natural language problem description and several test cases.
each problem has another set of test cases for evaluating the correctness of generatedcode which is called evaluation test cases here to distinguish with the test cases in the specification.
humaneval extends humaneval by automatically generating more evaluation test cases for each problem via the evalplus framework .
more evaluation test cases could determine the correctness of generated code more accurately.
apps contains programming problems collected from public programming competition platforms e.g.
leetcode including training data and test data.
the specification for each problem contains a natural language problem description and several test cases.
following the existing work we randomly sampled problems from test data according to the difficulty distribution so as to balance evaluation cost and conclusion generality.
the existing work extended the humaneval and apps benchmarks as humaneval et and apps et by constructing over additional evaluation test cases per problem respectively.
these additional evaluation test cases cover more corner cases to enhance evaluation sufficiency on generated code.
regarding apps et we used the same problems as apps.
besides we used mbpp et an extended version of mbpp that does not have additional evaluation test cases by complementing evaluation test cases for each programming problem in a similar way.
this dataset contains programming problems each with a specification comprising a natural language problem description and three test cases.
note that fix just utilizes the test cases in the specification for each problem.
for these datasets .
of problems have at least three test cases in their corresponding specifications.
the evaluation test cases are just used for assessing generated code following the practice in code generation .
c. metrics following existing work we executed evaluation test cases to check correctness of generated code for each programming problem and calculated pass kand avgpassratio to measure the performance of llms in code generation.
pass kmeasures the functional correctness of generated code on evaluation test cases.
given a programming problem the llm generates kcode instances.
the problem is considered solved if any instance passes all evaluation test cases.
pass kis the percentage of solved problems out of the total number of problems.
as demonstrated by the existing work developers tend to consider and evaluate one code instance produced by the used llm and thus we set k following the existing studies .
note that pass is a more strict setting and thus improving it is challenging.
larger pass kvalues mean better code generation performance.
avgpassratio measures the degree of correctness of generated code on evaluation test cases differing from pass kthat considers whether the generated code is completely correct on evaluation test cases .
both metrics are complementary to a large extent.
avgpassratio calculates the ratio of passing evaluation test cases to all evaluation test cases for each problem and then measures the average ratio across all problems.
larger avgpassratio values mean better code generation 6performance.
for ease of presentation in tables we abbreviated avgpassratio as apr .
more metrics i.e.
pass pass and codebleu will be discussed in section vii.
d. compared techniques to evaluate fix sufficiently we considered nine typical or state of the art prompting techniques for comparison zero shot prompting directly utilizes the original specification to prompt llms for code generation.
few shot prompting enables llms to learn the relationship between specifications and code based on randomly selected specification code examples.
it concatenates these examples with the original specification to form a prompt which is then fed to llms for code generation.
cot prompting elicits llms to produce intermediate reasoning steps as specification understanding.
it incorporates the specification understanding into the original specification to form a prompt which is then fed to llms for code generation.
following the existing work we applied cot prompting in both zero shot and fewshot settings.
for ease of presentation we call them zero shot cot prompting andfew shot cot prompting .
self planning prompting guides llms to decompose the specification into a set of easy to solve sub problems and produce code plan by providing few shot intent to plan examples.
it incorporates the code plan into the original specification to form a prompt which is then fed to llms for code generation.
scot prompting enhances cot prompting by utilizing program structures i.e.
sequence branch and loop structures to produce intermediate reasoning steps.
self debugging prompting utilizes the runtime errors and test execution results to guide llms for fixing incorrectly generated code.
self edit prompting wraps error messages produced by test execution as supplementary comments including test inputs expected outputs actual outputs and runtime errors.
then the supplementary comments serve as feedback to guide llms to fix incorrectly generated code.
self repair prompting leverages error messages produced by test execution to enable llms to produce a short explanation of why the code failed.
then the explanation guides llms to fix the incorrectly generated code.
in summary zero shot and few shot prompting are typical prompting techniques widely used as baselines in code generation studies .
zero shot cot and few shotcot are typical thought eliciting prompting techniques while self planning and scot are state of the art techniques in this category.
self debugging self edit and self repair are stateof the art feedback based prompting techniques.
note that fix is the first to integrate both thoughteliciting and feedback based prompting techniques.
for thorough evaluation we also combined each of the two state ofthe art thought eliciting prompting techniques self planning and scot with each of the three state of the art feedbackbased prompting techniques self debugging self edit andself repair .
for example scot combined with self repair referred to as scot self repair involves applying selfrepair to the code generated by scot.
in total we implemented compared techniques as baselines.
given the input length limit of llms we used the shot setting for all few shot baselines following the existing work .
v. r esults and analysis a. rq1 overall effectiveness of fix process to answer rq1 we applied fix and compared techniques to chatgpt and deepseek coder.
we then measured the effectiveness of each technique on widelyused benchmarks in terms of pass and avgpassratio.
results tables i and ii show the effectiveness comparison results on chatgpt and deepseek coder respectively.
we found that scot outperforms all thought eliciting prompting baselines while self repair excels compared to all feedbackbased prompting baselines confirming their effectiveness as state of the art baselines in their corresponding categories.
also each combination of thought eliciting and feedbackbased prompting techniques outperforms the corresponding individual techniques which confirms the synergy between both categories motivating our fix technique.
in particular fix achieves the best effectiveness among all studied techniques demonstrating its stable superiority in both metrics across all subjects two llms with six benchmarks .
based on chatgpt fix significantly improves all compared techniques by .
.
and .
.
in terms of pass and avgpassratio on average across all six benchmarks respectively.
based on deepseek coder fix significantly improves them by .
.
and .
.
in terms of pass and avgpassratio respectively.
furthermore the wilcoxon signed rank test at the significance level of .
confirms that all p values are smaller than .14e demonstrating the statistically significant superiority of fix over all compared techniques.
b. rq2 contribution of each main component in fix variants fix consists of two phases thought eliciting and feedback based prompting phases.
to investigate the contribution of each phase including some key components in each phase we created four variants of fix fix wof we removed the feedback based prompting phase from fix i.e.
it just uses the specification understanding produced in the thought eliciting prompting phase to generate code.
it can measure the effectiveness of the individual thought eliciting prompting phase in fix and also reflect the contribution of the feedback based prompting phase.
fix sr we replaced the feedback based prompting strategy in fix with the state of the art self repair prompting strategy.
it can investigate the effectiveness of our designed feedback based prompting strategy.
fix scot we replaced the thought eliciting prompting strategy in fix with the state of the art scot strategy.
it can further investigate the effectiveness of our designed thought eliciting strategy by complementing fix wof.
7table i effectiveness comparison on chatgpt in terms of pass and avgpassratio .
apr is short for avgpassratio .
chatgpt humaneval humaneval humaneval et mbpp et apps apps et prompting technique pass apr pass apr pass apr pass apr pass apr pass apr zero shot .
.
.
.
.
.
.
.
.
.
.
.
few shot .
.
.
.
.
.
.
.
.
.
.
.
zero shot cot .
.
.
.
.
.
.
.
.
.
.
.
few shot cot .
.
.
.
.
.
.
.
.
.
.
.
self planning .
.
.
.
.
.
.
.
.
.
.
.
scot .
.
.
.
.
.
.
.
.
.
.
.
self debugging .
.
.
.
.
.
.
.
.
.
.
.
self edit .
.
.
.
.
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
.
.
.
.
self planning self debugging .
.
.
.
.
.
.
.
.
.
.
.
self planning self edit .
.
.
.
.
.
.
.
.
.
.
.
self planning self repair .
.
.
.
.
.
.
.
.
.
.
.
scot self debugging .
.
.
.
.
.
.
.
.
.
.
.
scot self edit .
.
.
.
.
.
.
.
.
.
.
.
scot self repair .
.
.
.
.
.
.
.
.
.
.
.
fix .
.
.
.
.
.
.
.
.
.
.
.
table ii effectiveness comparison on deepseek coder in terms of pass and avgpassratio .
apr is short for avgpassratio .
deepseek coder humaneval humaneval humaneval et mbpp et apps apps et prompting technique pass apr pass apr pass apr pass apr pass apr pass apr zero shot .
.
.
.
.
.
.
.
.
.
.
.
few shot .
.
.
.
.
.
.
.
.
.
.
.
zero shot cot .
.
.
.
.
.
.
.
.
.
.
.
few shot cot .
.
.
.
.
.
.
.
.
.
.
.
self planning .
.
.
.
.
.
.
.
.
.
.
.
scot .
.
.
.
.
.
.
.
.
.
.
.
self debugging .
.
.
.
.
.
.
.
.
.
.
.
self edit .
.
.
.
.
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
.
.
.
.
self planning self debugging .
.
.
.
.
.
.
.
.
.
.
.
self planning self edit .
.
.
.
.
.
.
.
.
.
.
.
self planning self repair .
.
.
.
.
.
.
.
.
.
.
.
scot self debugging .
.
.
.
.
.
.
.
.
.
.
.
scot self edit .
.
.
.
.
.
.
.
.
.
.
.
scot self repair .
.
.
.
.
.
.
.
.
.
.
.
fix .
.
.
.
.
.
.
.
.
.
.
.
fix wos we removed the self improvement component in the thought eliciting prompting phase from fix.
it can investigate the contribution of identifying and refining misunderstanding before code generation in fix.
process due to the limited computational resource and evaluation time cost we selected chatgpt as the representative llm for the ablation study as it achieves the best effectiveness with fix in tables i and ii .
similarly we used chatgpt for the experiments in section vi.
specifically we applied fix and its four variants to chatgpt respectively and measured the effectiveness of each technique on widelyused benchmarks in terms of pass and avgpassratio.
results table iii shows the comparison results among fix and its four variants in terms of pass and avgpassratio.
first fix wofachieves superior effectiveness compared to all baselines in terms of both metrics except scot selfrepair on apps et in terms of avgpassratio .
it demonstrates the effectiveness of our thought eliciting prompting phase.
fix outperforms fix scot with average improvements of .
and .
in terms of pass and avgpassratio further confirming the effectiveness of fix s thought eliciting prompting strategy when combining with the same feedbackbased prompting strategy designed in fix .
second fix outperforms fix wofwith average improvements of .
in pass and .
in avgpassratio demonstrating the contribution of our feedback based prompting phase.
moreover fix demonstrates superior effectiveness over fix srwith average improvements of .
in pass and .
in avgpassratio further confirming the superiority of our feedback based prompting strategy over the state ofthe art self repair strategy when combining with the same thought eliciting prompting strategy designed in fix .
third fix demonstrates superior effectiveness compared to fix woswith average improvements of .
and .
in terms of pass and avgpassratio.
in addition to the fixing process in the feedback based phase fix also performs another misunderstanding fixing through the self improvement process in the thought eliciting phase.
the results confirm the necessity of the self improvement process in fix.
furthermore the wilcoxon signed rank test at the 8table iii comparison between fixand its variants in terms of pass and avgpassratio varianthumaneval humaneval humaneval et mbpp et apps apps et pass apr pass apr pass apr pass apr pass apr pass apr fix wof .
.
.
.
.
.
.
.
.
.
.
.
fix sr .
.
.
.
.
.
.
.
.
.
.
.
fix scot .
.
.
.
.
.
.
.
.
.
.
.
fix wos .
.
.
.
.
.
.
.
.
.
.
.
fix .
.
.
.
.
.
.
.
.
.
.
.
significance level of .
confirms that all p values are smaller than .39e demonstrating the statistically significant superiority of fix over all variants.
overall each main component does make contributions to the overall effectiveness of fix.
c. rq3 influence of hyper parameters setup fix involves three main hyper parameters n the number of refinements m the number of adjustments and the decoding temperature of llms denoted as t .
by default we set n 1andm 1to balance the effectiveness and efficiency.
following the existing work we sett .7for both llms.
in this rq we investigated the performance of fix under different settings.
due to the evaluation cost we considered n m and t .
.
.
.
.
.
results table iv shows that increasing nandmenhances the effectiveness of fix.
specifically fixn m outperforms fixn m 1with the average improvements of .
and .
in terms of pass and avgpassratio respectively across all the six benchmarks and both llms.
however larger nandmalso incur more time and token costs.
fixn m 2takes .
more time and .
more tokens than fixn m .
we will discuss the efficiency of our studied techniques in detail in section vi a. hence we used n 1andm 1as the default setting as fix under this setting consistently outperforms all baselines across all benchmarks and both llms and has lower time and token costs than other settings with larger nandm demonstrating the cost effectiveness of fix under this setting.
we then investigated the influence of decoding temperature.
due to the space limit we put the detailed results on our homepage .
for example fix under all the studied settings for the decoding temperature consistently outperforms the stateof the art scot self repair by achieving .
.
higher pass and .
.
higher avgpassratio averaging across all six benchmarks on chatgpt.
this demonstrates the stable superiority of fix under different decodingtemperature settings.
vi.
d iscussion a. efficiency we measured the time and token overhead on code generation.
on average the most efficient baseline zero shot prompting takes .78s for a programming task while the most effective baseline scot self repair and fix take .61s and .75s respectively.
the average token overheads for zero shot scot self repair and fix are .29k .31k and .44k respectively.
the time and token overhead of fix are slightly higher than those of scot self repair.
this is because fix involves a bit more llm invocations compared to existing prompting techniques.
for example the state ofthe art thought eliciting technique scot involves two llm invocations for a programming task whereas our thoughteliciting strategy involves four invocations.
considering the significant effectiveness of fix some extra cost is acceptable illustrating its excellent balance of cost and effectiveness.
recent studies suggest that increasing llm invocations can enhance code generation performance.
hence we further conducted a comparison experiment under the same number of llm invocations to demonstrate fix s effectiveness more clearly.
codet is the state of the art method to improve code generation by increasing llm invocations.
it generates more candidate code instances and ranks them by test execution.
such ranking strategies can be used to enhance thought eliciting prompting due to their orthogonal effect .
hence we combined codet with the stateof the art thought eliciting technique scot to compare with fix wofunder the same number of llm invocations.
both techniques do not involve feedback based prompting for a fair comparison.
we used chatgpt as the representative llm.
due to the space limit we put detailed results on our homepage and summarized the conclusions here.
fix wof achieves .
higher pass and .
higher avgpassratio than scot averaging across all six benchmarks.
scot codet can also outperform scot but just achieve .
higher pass and .
higher avgpassratio.
the results clearly demonstrate the effectiveness of our fix.
b. influence of test cases first we conducted an experiment to investigate the influence of the number of test cases used in fix.
we set the number of test cases to with the notation indicating the utilization of all available test cases in a programming specification the default setting in fix .
note that we randomly selected the corresponding numbers of test cases from the whole set of test cases in each programming specification.
if fewer test cases in the specification were available than desired we used all available ones.
we used chatgpt as the representative llm on three benchmarks humaneval humaneval and humaneval et .
table v shows the effectiveness of fix with different numbers of test cases in terms of pass and avgpassratio.
fix always exhibits better performance than the most effective baseline scot self repair even with only one test case.
as the number 9table iv influence of hyper parameters nand min terms of pass and avgpassratio llm configurationhumaneval humaneval humaneval et mbpp et apps apps et pass apr pass apr pass apr pass apr pass apr pass apr chatgpt fixn m .
.
.
.
.
.
.
.
.
.
.
.
fixn m .
.
.
.
.
.
.
.
.
.
.
.
deepseek coder fixn m .
.
.
.
.
.
.
.
.
.
.
.
fixn m .
.
.
.
.
.
.
.
.
.
.
.
table v influence of test cases used in fix numberhumaneval humaneval humaneval et pass apr pass apr pass apr scot self repair .
.
.
.
.
.
fix wot .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
of test cases increased fix s code generation performance improved demonstrating the importance of test case driven specification misunderstanding fixing for code generation.
second although it is quite common that a few test cases are included as part of a specification weneed to consider thescenario when test cases areabsent further enhanc ingthegeneralityof fix.
to prove the concept we utilized llms to automatically generate test cases for each programming specification without test cases following the existing work .
we call this variant fix wot.
we investigated the effectiveness of fix wotby taking chatgpt as the representative model on the same three benchmarks as above.
we set the number of generated test cases to as most of the programming specifications include three test cases as mentioned in section iv b. from table v fix wot significantly outperforms the most effective baseline scot self repair by .
and .
in terms of pass and avgpassratio respectively which demonstrates the practical effectiveness of fix in such a scenario.
also test case quality could affect the effectiveness of fix.
we manually analyzed some cases where fix wotunderperformed fix and found that llm generated test cases covered only one branch mentioned in the specification or even contained errors leading to slightly worse effectiveness.
in the future we will design better test generation methods such as incorporating test coverage to guide llm based test generation to further enhance the effectiveness of fix.
besides the resources may be limited in practice.
in such scenarios employing test prioritization to select higher quality test cases may help ensure the effectiveness of fix.
we regard this promising exploration as our future work.
c. comparison with agent based techniques recently some agent based techniques leveraging interactions among agents have been proposed to enhance llms effectiveness in software development.
they are actually orthogonal to fix to a large extent and we could utilize fix to enhance individual agents for better code generation.nevertheless we still investigated state of the art agentbased techniques metagpt self collaboration chatdev and agentcoder for comparison.
they share the same high level insight which simulates the software development process by assigning roles to several llm agents for collaboration in code generation.
the difference among them mainly lies in that they assign different roles to llm agents and design different strategies to solve the corresponding tasks.
due to the limited space more details about these techniques can be found in their corresponding papers.
we used chatgpt as the representative in this experiment due to the evaluation cost.
note that we used the default numbers of iterations for metagpt self collaboration chatdev and agentcoder in this experiment which are and respectively and are all larger than the single iteration of fix.
while this comparison may be unfavorable to fix fix still outperforms the four agent based techniques by .
.
and .
.
in terms of pass and avgpassratio respectively averaging across all the six benchmarks.
due to space limit we put the detailed results on our project homepage .
the wilcoxon signed rank test at the significance level of .
confirms that all pvalues are smaller than .38e demonstrating the statistically significant superiority of fix over all the studied agentbased techniques.
particularly as demonstrated in section v c fix s effectiveness can be further enhanced with additional iterations i.e.
increasing nandm .
among the four agentbased techniques agentcoder is the most effective and efficient but still takes an average of .
seconds and .77k tokens per programming task consuming .
more time and .
more tokens than fix.
overall fix exhibits significant superiority over these agent based techniques.
d. soundness of our checking mechanism achieving optimality in semantic analysis is generally undecidable.
many existing techniques e.g.
search based solutions in software engineering are known to suffer from local optimum .
the uninterpretable nature of llms exacerbates this difficulty.
hence there does not exist a decision procedure to check if an llm correctly understands a specification rendering sound checking mechanisms for llms understanding intractable.
such similar limitations are general for all deeplearning based methods.
to relieve this challenge in fix we employed a checking mechanism on the downstream code generation task .
specifically we checked the correctness of the generated code during the feedback based prompting phase through the actual execution of test cases inherently included 10in specifications.
that is the improvement of understanding is demonstrated by the better effectiveness in downstream code generation.
our study indeed confirms that this mechanism helps improve the effectiveness of code generation empirically demonstrating that leveraging existing test cases in specifications can alleviate llm hallucinations.
e. exploration of llm combinations in fix we explored the effectiveness of using different llms for understanding refinement and code generation via a preliminary experiment.
due to the inherent characteristics deepseek coder s reasoning capability is weaker than chatgpt s. when we used chatgpt for understanding refinement and deepseek coder for code generation the improvements are .
and .
in terms of pass and avgpassratio respectively averaging across humaneval humaneval and humaneval et compared to using deepseek coder for both.
this indicates the potential of fix with more appropriate llms in corresponding aspects which can be regarded as our future work.
vii.
t hreats to validity the first threat lies in the generalizability of experimental results.
to mitigate this threat we comprehensively selected benchmarks metrics baselines and llms.
following previous studies we selected six widely used benchmarks in code generation and employed two metrics for code correctness assessment.
besides we used pass pass and codebleu for measuring code generation performance although codebleu suffers from some limitations and put the results on our homepage due to space limitation.
the conclusions are consistent.
we also selected typical or state of the art prompting techniques as well as combined techniques for comparison and conducted a comprehensive evaluation on two state of the art llms i.e.
chatgpt and deepseek coder .
in the future we will evaluate fix to improve llms code generation performance on more comprehensive benchmarks.
the second threat lies in the randomness involved in llms.
on one hand our large scale study and consistent conclusions across all subjects can help reduce this threat.
on the other hand we repeated the experiment comparing fix with the most effective baseline scot self repair on three benchmarks humaneval humaneval and humaneval et as the representative for three times due to the huge evaluation cost.
the standard derivations for scot self repair and fix are only .
and .
for pass demonstrating the robustness of our conclusions to a large extent.
this further reduces the threat.
due to the limited space we place the detailed results on our project homepage .
the third threat lies in the design of prompts in fix.
we did not specially tune the natural language description format of prompts and thus cannot ensure their optimality.
we designed some similar prompts via paraphrasing e.g.
synonym replacement and active passive sentence transformation which did not affect fix s effectiveness much.we will systematically investigate fix s robustness in the future.
furthermore the prompting structure in fix mainly emphasizes the inputs and outputs of test cases which are essential test case elements regardless of specifications also demonstrating generalizability.
viii.
r elated work prompting techniques have been demonstrated effective to improve the code generation performance of llms in a plugand play manner .
in general they can be divided into two main categories thought eliciting prompting techniques and feedback based prompting techniques.
the former aims to elicit llms to produce intermediate reasoning steps for more accurate code generation.
we have introduced and compared cot self planning and scot techniques in section iv.
besides kpc is a knowledge driven prompting technique which decomposes code generation into intermediate reasoning steps and utilizes fine grained knowledge extracted from api documentation for code generation especially in exception handling code .
the latter category of prompting techniques use error messages produced by test execution to enable llms to fix incorrectly generated code such as sed coderl self debugging selfedit and self repair .
due to the evaluation cost we did not select all these prompting techniques in our study but just selected some typical or state of the art ones as baselines.
different from the existing techniques fix is the first to explore the synergy of both categories by devising both sophisticated thought eliciting and feedback based prompting.
its core of improving llms code generation performance is to fix llms specification misunderstanding in each phase.
ix.
c onclusion in this work we propose a novel prompting technique fix to improve the code generation performance of llms.
different from the existing prompting techniques fix devises both sophisticated thought eliciting prompting and feedbackbased prompting and explores their synergy.
our thoughteliciting prompting strategy in fix exploits test case analysis and the misunderstanding fixing process to obtain more accurate specification understanding.
then our feedback based prompting strategy further fixes the understanding to reduce the gap between the provided refined understanding from the first phase and the actual understanding implicitly utilized by llms for code generation.
we conducted an extensive study on two advanced llms with six widely used benchmarks demonstrating the superiority of fix over the state of theart prompting techniques.