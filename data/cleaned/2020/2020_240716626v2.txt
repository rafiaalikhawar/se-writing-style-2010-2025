a tale of two dl cities when library tests meet compiler qingchao shen college of intelligence and computing tianjin university tianjin china qingchao tju.edu.cnyongqiang tian the hong kong university of science and technology hong kong china yqtian ust.hkhaoyang ma the hong kong university of science and technology hong kong china haoyang.ma connect.ust.hkjunjie chen college of intelligence and computing tianjin university tianjin china junjiechen tju.edu.cn lili huang college of intelligence and computing tianjin university tianjin china huangll tju.edu.cnruifeng fu college of intelligence and computing tianjin university tianjin china frf2000 tju.edu.cnshing chi cheung the hong kong university of science and technology hong kong china scc cse.ust.hkzan wang college of intelligence and computing tianjin university tianjin china wangzan tju.edu.cn abstract deep learning dl compilers typically load a dl model and optimize it with intermediate representation.
existing dl compiler testing techniques mainly focus on model optimization stages but rarely explore bug detection at the model loading stage.
effectively testing the model loading stage requires covering diverse usages of each dl operator from various dl libraries which shares a common objective with dl library testing indicating that the embedded knowledge in dl library tests is beneficial for testing the model loading stage of dl compilers.
with this idea we propose o pera to migrate the knowledge embedded in dl library tests to test the model loading stage.
o pera constructs diverse tests from various tests for dl libraries including the tests documented in dl libraries and those generated by recent fuzzers .
in total we considered three sources of tests in dl libraries for migration.
in addition it incorporates a diversity based test prioritization strategy to migrate and execute those tests that are more likely to detect diverse bugs earlier.
we then used eight frontends from three dl compilers e.g.
tvm tensorrt and openvino for evaluation.
opera detected previously unknown bugs in total 90of which have been confirmed fixed by developers demonstrating the effectiveness of such the migration based idea.
the test prioritization strategy in o pera improves testing efficiency with migrated tests by .
.
on average compared to general test prioritization strategies.
index terms compiler testing test migration test prioritization deep learning compiler i. i ntroduction deep learning dl compilers e.g.
tvm tensorrt and openvino are widely utilized to optimize the performance of dl models for deployment on various hardware devices.
the compilation process of a dl model typically involves three main stages 1loading the dl model prepared under a specific dl library e.g.
pytorch or keras into its equivalent high level intermediate representation ir 2performing hardware independent optimizations on the high level ir 3lowering the high level junjie chen is the corresponding authorir into the low level ir and conducting hardware specific optimizations to generate code targeting specific hardware.
similar to traditional compilers dl compilers also contain bugs which can compromise the reliability of both the compilers themselves and the models they produce.
as reported each stage of dl compilers contains a significant number of bugs underscoring the need for comprehensive testing to ensure the quality of dl compilers.
however existing dl compiler testing techniques primarily focus on the two optimization stages 2and mentioned earlier neglecting the model loading stage .
specifically recent dl compiler testing techniques such as hirgen tzer and tvmfuzz mainly construct tests at either high level or low level irs bypassing the model loading stage.
nnsmith a state of the art grammar based technique is designed to construct dl models for testing various stages of dl compilers.
however it primarily focuses on stress testing for optimizations by generating complicated models.
in contrast the model loading stage involves converting each operator in a model into its equivalent high level ir individually.
therefore effectively testing the model loading stage requires covering diverse usages of each dl operator from various dl libraries rather than focusing on complex dependencies among operators.
furthermore nnsmith is limited to constructing dl models solely under the onnx library and supports a limited number of operators making it ineffective for testing the model loading stage.
intuitively manually developing test generation tools that follow the corresponding grammars may meet the test requirements for the model loading stage.
however this method can be labor intensive and error prone due to the extensive number of dl libraries and their supported operators.
additionally operators often involve numerous parameters leading to complex constraints that further aggregate the difficulty of developing such tools.
this highlights the need for alternative and lightweight methods to address this challenging task.arxiv .16626v2 aug 2024by analyzing source code test cases and bugs of dl compilers we found that testing the model loading stage of dl compilers is related to testing dl libraries.
specifically dl compilers typically accept dl models composed of operators supported by specific dl libraries as inputs.
both the testing of the model loading stage and dl libraries share a common objective which is to ensure the correctness of operators under various usages.
while there may not be complete overlap between the corner usages of each operator for dl compilers and dl libraries the embedded knowledge in dl library tests could potentially be beneficial for testing the model loading stage of dl compilers.
a few tests in dl compilers are designed with inspiration from tests documented in the onnx library as indicated by the comments accompanying these tests.
this suggests the feasibility of leveraging the knowledge embedded in dl library tests to enhance the testing of the model loading stage to some extent.
however due to the separate development of communities for dl compiler testing and dl library testing there has been no systematic study to investigate the feasibility of migrating the knowledge embedded in dl library tests for testing the model loading stage.
hence we performed the first exploration on the potential of the migration based idea.
specifically we design a migration based technique called o pera oper ator adapter to test dl compilers especially the model loading stage by considering three sources of tests in dl libraries for migration i.e.
tests documented in dl libraries and tests generated by two recent fuzzers docter and deeprel .
in fact the direct adoption of most dl library tests for testing the model loading stage of dl compilers is not feasible due to differences in their input formats.
specifically while dl compiler tests rely on pre constructed dl models dl library tests typically involve subtasks e.g.
gradient calculation and model design in model construction many of which cannot be represented in the form of dl models.
to address this challenge o pera first extracts instances of dl operators from each dl library test via code instrumentation and then packages each operator instance to a model as a migrated test for dl compilers with the aid of model generation templates for different dl libraries.
each operator instance represents a specific usage of the operator encompassing an operator api and its corresponding parameter settings.
another challenge that o pera encounters is the significant cost consideration.
this is primarily due to two factors the substantial volume of migrated tests originating from various migration sources and the frequent migration and execution of tests resulting from the frequent evolution of both dl libraries and dl compilers.
to address this challenge the component of test prioritization is designed in o pera which prioritizes the migration and execution of tests that are more likely to uncover a diverse range of bugs in the model loading stage.
after prioritization more bugs can be detected within any given time budgets thereby enhancing the overall test efficiency.
the test prioritization component takes into account the diversity of operator instances.
in this work we applied o pera to test three popular dlcompilers i.e.
tvm tensorrt and openvino .
to balance evaluation cost and conclusion generality for each compiler we chose several popular dl libraries i.e.
pytorch keras and onnx from its supported frontends responsible for model loading .
in total our study covered eight frontends across three dl compilers.
in total o pera detects previously unknown bugs by migrating the knowledge embedded in dl library tests 90of which have been confirmed fixed while the state of the art grammar based dl compiler testing technique nnsmith detects only 18bugs within the same time budget.
the results demonstrate the effectiveness of such a migration based idea for testing the model loading stage of dl compilers.
furthermore the test efficiency can be largely improved with the test prioritization component in o pera .
on average across eight subjects it improves o pera without special test prioritization by .
and improves o pera incorporating the widely used test prioritization strategies in general software testing by .
.
in terms of apfd average percentage of faults detected .
this work makes the following major contributions we introduced the idea of migrating knowledge from dl library tests to enhance the testing of the model loading stage in dl compilers.
we designed a migration based technique o pera which integrates various migration sources i.e.
tests documented in dl libraries and those generated by recent fuzzers along with diversity based test prioritization.
we conducted an extensive study to evaluate o pera across eight frontends from three dl compilers leading to the efficient detection of 170previously unknown bugs.
we released the o pera implementation and all experimental data for replication and future use accessible at .
ii.
m otivation at the model loading stage dl compilers take as input dl models built from various dl libraries e.g.
pytorch and convert them into a unified high level ir.
the model constructed by a specific dl library is a computational graph with diverse dl operators.
this high level ir known as graph level ir helps hide the differences in dl models from various dl libraries simplifying optimization execution.
each operator in the dl model is converted into semantically equivalent one or more ir expressions.
for example a conv2d operator in a keras model along with all parameter settings e.g.
filters is converted to nn.conv2d in the high level ir of tvm.
the behavior of operators can be customized by input parameters.
for example figure a shows the definition of theconv2dtranspose operator in keras.
it includes two required parameters filters andkernel size and optional parameters e.g.
strides with default values.
the value selection of each parameter may affect the calculation result of a model involving the operator.
to guarantee the correctness of a dl library numerous tests containing diverse operator instances with various parameter values were constructed for dl library testing .
for example to test 2keras.layers.conv2dtranspose filters kernel size strides padding valid output padding none data format none dilation rate activation none use bias true bias initializer zeros kernel regularizer none ... class conv2dtransposetest testcase def run test self kwargs with self.cached session test utils.layer test keras.layers.conv2dtranspose kwargs kwargs input shape parameterized.named parameters strides output padding strides output padding def test conv2d transpose regularizers self kwargs filters kernel size ... with self.cached session layer keras.layers.conv2dtranspose kwargs layer.build none a conv2dtranspose definition in keras class conv2d transposetest testcase def run test self kwargs test utils.layer test conv2dtranspose kwargs kwargs input shape ... parameterized.named para ms strides output padding def test conv2d transpose self kwargs filters kernel size ... ly conv2dtranspose kwargs ... b a test for conv2dtranspose in the test suite of keras fig.
.
a motivating example with conv2dtranspose params keras layer.output padding if is deconv and keras layer.output padding def convert convolution inexpr keras layer etab data layout ... ... fig.
.
patch for a real bug on conv2dtranspose in tvm the correctness of the conv2dtranspose operator keras developers prepared tests.
in the model loading process dl compilers need the ability to handle various usages of these operators including all the combinations of parameters for correct transformation from dl operators to high level ir.
hence the model loading stage of dl compilers actually shares a similar test objective with the tests for these operators in dl libraries.
this motivates the idea of migrating knowledge embedded in dl library tests to test the model loading stage.
figure shows a real bug of conv2dtranspose in tvm triggered by a test migrated from keras testing.
tvm overlooks the parameter output padding when converting the operator conv2dtranspose which leads to incorrect output shape when the out padding is not set to the default value i.e.
none .
before this bug was reported the developer provided tests of tvm only consisted of two operator instances to test the conversion of conv2dtranspose from the dl operator to high level ir.
moreover in the two instances all the optional parameters use the default values and thus are ineffective in detecting the bugs that require other non default parameter settings.
the tests migrated from dl library testing can help detect these cases.
the test with non default out padding is absent in the tests of dl compilers but available in the tests of keras.
in figure b the test conv2dtransposetest from keras which assigns toout padding matches the bug triggering condition for this bug and thus can help reveal it.
by migrating the keras test to the test in the input format for tvm with this operator instance o pera successfully detected this previously unknown bug.
after this bug was reported it was fixed by adding the analysis on the paraminstruments apis in the source code of dl libraries apis instrumentation tests for dl libraries def test conv2d self device x torch.randn kernel size conv conv2d kernel size y cpu conv x with cudnn.flags enabled false conv cuda conv.to device ... y cuda conv cuda x cuda self.assertequal y cpu y cuda tests executionop instance extractiontest migration operator instanceparameter settingsoperator signatureinstrumented api is invoked test prioritizationdl model ...dl model dl model dl model measure the diversity rank2 output a prioritized list of migrated testsdl modeldl modeldl model ...wrap it with template 3torch.nn.conv2d in channels out channels kernel size dl modeldl modeldl model ......dl model dl model dl modeloperator signature parameter settingsfig.
.
workflow of o pera eteroutput padding in the convert convolution function of tvm.
it is non trivial for those existing dl compiler testing techniques to detect this bug.
for grammar based techniques like nnsmith developers need to manually prepare the grammar to support conv2dtranspose .
as operators in dl compilers usually have complicated logic and vast space of parameters supporting this operator that can match the bugtriggering condition requires extensive expert knowledge and costs.
for mutation based techniques detecting this bug requires finding a seed test from a seed pool and applying a set of effective mutation operators which is also non trivial due to large search space.
therefore in this work we explore an alternative and lightweight method i.e.
migration based idea for this challenging task.
iii.
a pproach with the migration based idea we propose a technique called o pera .
the workflow of o pera is shown in figure which contains two main components test migration and test prioritization.
specifically o pera first creates tests for the model loading stage by migrating knowledge embedded in dl library tests via operator instance extraction section iii a .
due to the large number of migrated tests and the requirement of frequent test migration and execution caused by the evolution of dl libraries and compilers o pera then prioritizes migrated tests based on their diversity in order to improve testing efficiency section iii b .
finally o pera incorporates two test oracles to determine whether a migrated test detects a bug in the model loading stage of a dl compiler section iii c .
a. test migration migration sources in o pera we considered both human written tests and tool generated tests in dl library 3testing as our migration sources.
human written tests imply expert knowledge for considering various usages of each operator under the corresponding constraints.
tool generated tests on the other hand can help explore corner cases.
in the literature there are many dl library testing techniques by balancing evaluation cost and conclusion generalizability we selected two state of the art but diverse techniques docter and deeprel for supporting the migration source of tool generated tests.
docter extracts api constraints from official documentation and then utilizes these constraints to generate tests.
deeprel infers potential api relations automatically based on api syntactic and semantic information and then synthesizes tests for invoking relational apis.
in theory o pera is generalizable to various dl library testing techniques and we will investigate more sources of tool generated tests in the future.
operator instance extraction although there are massive tests from the three sources however most dl library tests cannot be directly adopted to test dl compilers due to differences in their input format.
specifically dl compilers take dl models as inputs while the tests for dl libraries are often in the format of python code most of which lack a complete model structure as shown in figure b .
hence extracting dl models from dl library tests for dl compiler testing is non trivial.
to achieve the goal of creating tests for the model loading stage of dl compilers by migrating knowledge embedded in dl library tests o pera uses operator instances to bridge the migration gap.
an operator instance refers to a specific usage of an operator with a specific setting of its parameters an example is shown in the figure .
they can be extracted from the tests for dl libraries and converted to dl models composed of a single layer for testing dl compilers.
we call such dl models single operator models .
note that multiple operator instances can be extracted from one dl library test leading to obtaining a set of single operator models that is migrated tests for dl compiler testing.
note that the primary functionality of the model loading stage lies in converting each operator in a dl model individually into an equivalent ir known as single operator equivalence conversion.
as a result o pera naturally creates single operator models for testing.
also single operator models facilitate follow up bug de duplication and localization.
specifically o pera instruments apis in the source code of dl libraries for operator instance extraction.
when an instrumented api is invoked the operator signature and its corresponding parameter values can be recorded which collectively form an operator instance.
this operator instance is then wrapped using a template as a dl model for testing the model loading stage.
due to different dl libraries having different model construction methods we design a model generation template for each dl library to facilitate wrapping the corresponding operator instances.
for instance the template for pytorch is shown in figure which takes an operator instance as input line and encapsulates it in a model structure lines .
an instance of the pytorch model is then created model template f1 pa rams list decla re st r f11class mode l torch.nn.module n11 fll fll def forward self args n11 return op signature args params se ttings str n11 f11model mode l .eval n11 f11input da ta pa ra 0 n11 f11trace torch.ji t.trace model input data n11fig.
.
template for generating dl models under pytorch and set to evaluation mode line .
finally with the aid of input data the model is serialized into deployable code i.e.
torchscript which can be used as the test of dl compilers.
we show the used template for each dl library at our project homepage due to the space limit.
b. test prioritization as explained in section i the practicality of such a migration based idea may still be hindered by significant cost consideration.
we thus incorporate test prioritization into opera to detect more bugs within a given testing time budget which facilitates investigating the efficiency improvement of this migration based idea.
according to the characteristics of our scenario we design a diversity based test prioritization strategy in o pera .
a migrated test is a single operator model converted from an operator instance and thus its core semantics lie in the signature of the operator and the setting of each parameter in the operator.
hence o pera prioritizes the set of migrated tests according to the diversity among them in terms of the two dimensional information.
diversity of operator signatures the tests with different operator signatures mean that they have diverse semantics.
however how to determine the order of the tests with different operator signatures is still a challenge.
in o pera we address it according to the following intuitions the number of tests with an operator signature is large in dl library testing indicating that this operator receives more attention potentially due to its complicated implementation logic more corner cases in it etc the number of tests with an operator signature is small in the test suite equipped by the dl compiler i.e.
the test suite specific to the model loading stage indicating that dl compiler developers still pay little attention to testing the transformation of this operator.
hence opera assigns an operator opia higher priority score if it occurs in the set of migrated tests for a dl library more frequently denoted the occurrence times as num dll opi but occurs in the test suite equipped by the dl compiler more rarely denoted the occurrence times as num dlc opi .
with the intuitions the priority score of opiis calculated by the ratio of num dll opiover num dlc opi.
diversity of parameter settings with the diversity of operator signatures the tests with the same operator signature have the same priority and thus how to further determine their priority is another challenge.
o pera addresses it by measuring the diversity of parameter settings for each operator.
inspired by the theory of equivalence class partitioning opera partitions the value space of each parameter into 4a set of subspaces each of which clusters the values with potentially similar testing capabilities by performing different considerations on different types of parameters.
if the value space of a parameter is a limited set of concrete values opera treats each unique value as a unique subspace as each of them may represent a unique configuration of using this operator.
if the value space of a parameter is a range of integers o pera pays more attention to some special values in dl i.e.
e.g.
it can be used to represent the automatically calculated dimension size in tensor shape e.g.
it can mean that no padding operation is performed when assigned to the parameter padding and e.g.
it can be a boundary value for the parameter scale inkeras.layers.rescaling .
hence o pera partitions them into five sets of subspaces i.e.
and .
similarly o pera partitions the value space of a range of floating number for a parameter into three sets of subspaces i.e.
and .
if a parameter is a tensor which is a compound type with some typical attributes i.e.
tensor type and shape it first partitions each tensor type value as a unique subspace as the value space of tensor type is a limited set of concrete values.
then it partitions the value space of tensor shape which belongs to the list type .
the type of value in the list is integer and thus we use the integer space partition method for it.
the size of the list refers to the dimension of the tensor and opera considers some special values .
specifically the dimension of the tensor with batch size ranging from zero to five can be used to represent none scalar vector matrix color image e.g.
and video e.g.
respectively.
therefore the value space of the dimension is divided into seven sets of subspaces i.e.
and .
finally opera intersects the subspace partitioned by each attribute and thus forms the final set of subspaces for tensors.
for an operator signature o pera measures the diversity score of the parameter setting for an operator instance with those of already prioritized operator instances based on partitioned value subspaces.
specifically for the operator instance opera measures the percentage of parameters or pair wise parameter combinations whose values cover new subspaces or pair wise subspaces over the set of already prioritized operator instances inspired by combinatorial testing .
overall prioritization with the two dimensional diversity o pera prioritizes tests based on the product of the diversity score of the operator signature and that of the parameter setting which is called operator instance diversity.
this calculation method may not be optimal in our scenario and we will explore more methods in the future.
after adding the test with the maximum operator instance diversity to the prioritized result o pera updates the parameter setting diversity of the remaining tests with the same operator signature for subsequent iterations.
the prioritization process in o pera is based on the heapsort algorithm due to its high efficiency o nlogn time complexity and o space complexity .c.
test oracles a recent study revealed that most of dl compiler bugs manifested as either compiler crashes or inference inconsistencies between the original dl models and the corresponding compiled models.
we thus implemented the two test oracles for testing the model loading stage with migrated tests.
crash has been widely used in compiler testing which refers to an unexpected termination of the compilation process.
to avoid crashing a dl compiler due to invalid tests opera uses the dl library from which the tests are migrated to check their validity in advance.
if the dl library crashes when executing a test this test is considered invalid and thus discarded before testing the dl compiler.
in addition the crashes that produce error messages like unsupported type and unsupported operator are disregarded as the unsupported features are often not treated as bugs.
inference inconsistency means that the inference results of a test a dl model and its corresponding compiled model via the dl compiler under test are inconsistent .
same as the existing work we measured the inference difference between them based on chebyshev distance and used 1e as the threshold to determine whether the inference results are inconsistent.
as the dl compiler aims to achieve equivalent transformation for any dl model if the obtained inference results from them are inconsistent it indicates that a dl compiler bug is found.
iv.
evaluation setup our evaluation aims to study two research questions rq1 to what extent can o pera effectively detect bugs at the model loading stage of dl compilers?
rq2 to what extent can the test prioritization component enable o pera to detect bugs earlier?
a. subjects following recent work we performed our study on three widely studied dl compilers including tvm tensorrt and openvino .
we chose the latest versions of them i.e.
tvm v0.
tensorrt v8.
and openvino v2023.
.
which is helpful to answer these rqs more sufficiently by detecting previously unknown bugs.
a dl compiler usually contains multiple frontends for model loading each of which converts dl models under a specific dl library into high level irs.
to allow for in depth analysis our evaluation focuses on the frontends of popular dl libraries including pytorch frontend keras frontend and onnx frontend.
each of them handles the models under libraries that are widely used in both research and industry i.e.
pytorch keras and onnx libraries respectively.
in particular we use keras instead of tensorflow as keras is a high level and easy to use interface of tensorflow and many dl models constructed by tensorflow are saved in the keras format for platform compatibility and portability at the deployment stage.
further as tensorrt does not support the keras frontend our evaluation covers eight frontends from three dl compilers in total.
5b.
baselines we assessed the effectiveness of the migration based idea with o pera by comparing it with nnsmith and comet .
both produce dl models with multiple operators for testing which facilitates comparison analysis with single operator models obtained by o pera .
nnsmith is the state of the art dl compiler testing technique which is a grammar based technique and can test the model loading stage as well.
specifically it randomly generates dl models from scratch by supporting operators of the onnx library which indicates that we can just study nnsmith on the onnx frontend.
even though most of dl library tests cannot be directly adopted to test the model loading stage we have to use o pera to support the migration.
there are also some dl library testing techniques that can directly generate dl models to satisfy the input format of dl compilers.
we also studied such a state of the art technique i.e.
comet which designs a set of mutation operators and a coverage based search based algorithm to generate diverse models for testing dl libraries.
in rq2 we investigated the efficiency improvement of such the migration based idea by evaluating the effectiveness of the test prioritization strategy in o pera .
here we considered some test prioritization strategies commonly used in general software testing for comparisons.
random .
the migrated tests are randomly ordered serving as the baseline without special test prioritization.
fast which treats each test as a string and adopts the data mining algorithms i.e.
minhashing and localitysensitive hashing algorithms to accelerate the process of finding diverse tests by converting each string to a kshingle the set of its substrings of length k .
total coverage based prioritization which prioritizes migrated tests based on the number of program elements in the frontend under test covered by each test.
here we used statements as the representative program elements following existing work .
additional coverage based prioritization which prioritizes migrated tests based on the number of covered statements that are not covered by the existing prioritized ones.
fast is the state of the art black box strategy while coverage based prioritization is the most widely studied whitebox strategy.
the prioritization strategy in o pera and the random strategy are black box.
for coverage based strategies coverage.py is used to collect statement coverage in frontends.
for fast we re used its released implementation .
c. metrics we counted the number of detected bugs as the metric of evaluating the test effectiveness.
during the testing process it is possible that some of the test failures are triggered by the same root cause.
hence it is important to de duplicate them and count the number of unique bugs.
in dl compiler each operator in the model loading stage comprises a conversion function that is responsible for converting it into the equivalent high level ir.
as the migratedtest is a single operator model it is convenient to determine the conversion function responsible to the operator in a failuretriggering test.
therefore based on the identified conversion function for each test failure we de duplicated the test failures to obtain unique bugs.
here we did not use the operator in each failure triggering test for de duplication as different operators may be handled by the same conversion function in the dl compilers.
for example averagepooling2d andmaxpooling2d are two different operators in the keras library but they are converted by the same function i.e.
convert pooling in tvm.
as the tests generated by nnsmith and comet are dl models with multiple operators we manually de duplicated their test failures following the original papers .
all bugs are detected on the latest versions of dl compilers and thus we created a bug report for each unique bug and then submitted it to project maintainers.
we counted the number of confirmed or fixed bugs by developers.
based on the feedback from developers all of our submitted bugs that have been confirmed are unique.
this indicates the accuracy of our deduplication method.
besides o pera includes a test prioritization component to improve the testing efficiency and thus it is also important to investigate the testing efficiency of each technique.
here we used two metrics to evaluate the effectiveness of each prioritization strategy.
first we measured the time spent on detecting each bug .
the shorter the time is the more effective the strategy is.
second we adopted the widely used metric of evaluating test prioritization i.e.
apfd average percentage of faults detected to compare various test prioritization strategies following many existing studies .
the calculation of apfd is shown in formula apfd pm i pi n m 2n where mis the total number of detected bugs nis the total number of tests piis the rank of the first test in the prioritized result that detects the ithbugs.
a larger apfd value indicates a more effective strategy.
d. implementations we collected the test suite equipped by pytorch v1.
keras v2.
and onnx .
as the migration source of humanwritten tests for the corresponding frontends of the three dl compilers respectively.
we collected and tests from the three test suites respectively.
we used the implementations of docter and deeprel released by their works .
as neither of them supports test generation for the onnx library we exclude them when using o pera to test the onnx frontend in dl compilers.
for the pytorch and keras frontends we used docter and deeprel to generate the same number of tests as the corresponding human written tests respectively which can help compare the three migration sources fairly.
all experiments were conducted on an ubuntu .
server with intel xeon cpu nvidia gtx1080ti gpu and 128g ram.
6table i number of bugs detected by opera .
means not applicable since tensor rt does not support keras models .
frontend status tvm tensorrt openvino total pytorchfixed confirmed awaiting kerasfixed confirmed awaiting onnxfixed confirmed awaiting total e. process for each studied frontend we obtained a set of migrated sets from the three migration sources with the aid of o pera .
we then tested each frontend with these migrated tests and recorded whether a test triggered a failure or not and the time spent on each test.
for fair comparison we applied nnsmith and comet to generate dl models for testing each frontend for the same time budget as that used by o pera including the time spent on test generation migration prioritization and execution by o pera respectively.
to answer rq2 for each frontend we constructed four variants of o pera by replacing its diversity based test prioritization strategy with each of the four compared strategies respectively.
by applying each variant to test each frontend we recorded the test result and testing time of each test and calculated the apfd value.
to reduce the influence of randomness and the running environment we repeated our experiments for five times and calculated average results.
v. r esults and analysis a. rq1 effectiveness bug detection table i shows the number of bugs detected by tests migrated from the testing of pytorch keras onnx libraries with the aid of o pera respectively.
in total 170previously unknown bugs are detected including and48on tvm tensorrt and openvino respectively.
bugs of them have been confirmed or fixed by developers and the remaining bugs are being investigated by developers.
the ratio of confirmed or fixed bugs is high for most of the subjects except the pytorch frontend for tensorrt since its developers are inactive.
after investigations by developers apart from three optimization bugs the remaining 87bugs that have been confirmed or fixed are frontend bugs.
this is aligned with the goal of enhancing the testing of the model loading stage via test migration.
specifically each migrated test by o pera is a single operator model which can trigger various logic in the model loading stage but the bugs in the optimization stages often involve more complicated models .
root causes of detected bugs according to the feedback from developers on the 90confirmed or fixed bugs and their patches these bugs are caused by diverse root causes return op.nn.relu data data inputs def threshold self inputs input types threshold f float inputs threshold op.full like inputs fill value expr.const threshold f value f float inputs value op.full like inputs fill value expr.const value f return op.where op.greater data threshold data value fig.
.
patch for an incorrect code logic bug begin if data layout nhwc def convert cropping inexpr keras layer etab data layout ... return op.strided slice inexpr begin end begin end end else begin end fig.
.
patch for a tensor shape bug which cover all root cause categories summarized on historical bugs in the model loading stage .
among the 90bugs bugs are caused by tensor shape problem bugs are due to type problem bugs are incorrect code logic bugs are due to incorrect exception handling bugs are due to incompatibility bugs are due to incorrect assignment bugs are incorrect numerical computation bug is due to concurrency and bug is due to typo.
these root causes are classified based on the existing study on dl compiler bug .
next we present two bugs detected by the migrated tests.
figure depicts an incorrect code logic bug in the pytorch frontend of tvm .
in this bug the threshold operator from pytorch was converted into the high level ir of op.nn.relu which differs in its computation logic.
any non zero value assigned to the parameter threshold orvalue of the threshold operator will lead to wrong inference results.
indeed a migrated test containing the operator instance torch.nn.threshold threshold value helps trigger this bug during compilation.
a patch was committed to fix it by correcting the conversion logic for the threshold operator as shown in figure .
figure shows another bug caused by tensor shape problem in the keras frontend.
when converting the cropping2d operator tvm always considers the data layout to be nchw e.g.
channel first but nhwc e.g.
channel last is also a common data layout.
when tvm loads the model containing the cropping2d operator and sets the parameter data format tochannels last which means the data layout is in the nhwc format this bug can be triggered and lead to wrong inference results.
this bug has been fixed using different calculation logic for different layouts.
comparison with nnsmith and comet during the same testing time the migrated tests by o pera detect 48bugs while nnsmith detects 5bugs and comet detects 4bugs in tvm tensorrt openvino human written docter generateddeeprel generated pytorch1013 human written docter generateddeeprel generated keras54fig.
.
bug detection comparison among different sources respectively.
of bugs detected by nnsmith and of bugs detected by comet are unique which were not detected by o pera .
all these unique bugs are in the optimization stages.
besides all frontend bugs detected by nnsmish and comet are also detected by o pera .
opera detected unique bugs that were not detected by nnsmith and comet of which are frontend bugs.
the results demonstrate the superiority of o pera in testing the model loading stage and the complementarity between o pera and the existing dl compiler testing techniques nnsmith and comet .
the major reason for the superiority of migrated tests by o pera over nnsmith and comet in testing the model loading stage is that the latter two support only and operators while the former covers operators in a lightweight manner.
this implies a correlation between operator coverage and bug detection i.e.
higher operator coverage in testing is likely to detect more bugs in the model loading stage.
contribution of different migration sources and different test oracles in this work o pera considers three migration sources i.e.
tests documented in dl libraries and the tests generated by two recent fuzzers and designs two test oracles i.e.
crash and inference inconsistency .
here we analyzed the contribution of each migration source as well as each test oracle.
figure shows the bug detection results for each migration source in o pera which do not include the results on the onnx frontend as docter and deeprel do not support the testing of the onnx library.
from figure all three migration sources contribute to detecting a certain number of unique bugs in pytorch and keras frontends showing the complementarity among them in testing the model loading stage.
among three sources the migration source of human written tests detects the most bugs in both pytorch and keras frontends.
specifically the migrated tests from humanwritten tests cover pytorch operators and keras operators while the migrated tests from docter cover pytorch operators and keras operators and the migrated tests from deeprel cover pytorch operators and keras operators.
through further analysis we found that among the 170bugs detected by o pera are detected by the test oracle of crash including confirmed fixed bugs while are detected by the test oracle of inference inconsistency including confirmed fixed bugs .
the results demonstrate that the two test oracles are complementary for detecting dl compiler bugs with o pera .
false positives there are only false positives produced by the migrated tests in total.
all of them are caused by the test oracle of inference inconsistency between the dl compiler and the corresponding dl library.
specifically two of them are the bugs in the keras library rather than the dl compiler which have been fixed in the latest version of keras.
in this work we assume that the bug occurs at the dl compiler when there is an inference inconsistency between a dl library and a dl compiler thus leading to the two false positives.
three false positives are due to the undefined behaviors in themod roialign trilu operators.
for example mod takes the dividend tensor and the divisor tensor as inputs and produces the remainder of them.
if the dividend is zero the result will be platform dependent.
that is this false positive is caused by the undefined behavior at division by zero which is also meaningful as the openvino developer commented it is a good catch.
we will count on this issue in case we face undefined behavior later.
the remaining false positives are due to randomness in operators e.g.
bernoulli andrandomuniformlike .
for example bernoulli takes as input a tensor containing probabilities and draws the binary random number from a bernoulli distribution.
randomuniformlike generates a tensor with random values drawn from a uniform distribution.
the false positives caused by randomness may be filtered out by checking whether these inference inconsistencies also exist between different versions of the dl library.
b. rq2 efficiency we explored the efficiency improvement of our migrationbased idea by investigating whether the test prioritization component in o pera can help detect more bugs with a given testing time budget.
figure shows the number of detected bugs by o pera and its variant without special prioritization opera random with the testing process proceeding on each subject.
as o pera has extra time spent on test prioritization but o pera random does not we included its prioritization time into the testing time of o pera for fair comparison.
note that the total time cost across different subjects is inconsistent due to the varying number of migrated tests from different dl libraries presented in section iv d and the differing compilation time across dl compilers.
from figure o pera always detects more bugs than opera random regardless of the given testing time budget.
in particular o pera spends .
.
.
.
.
.
.
.
hours on detecting all the bugs found in the experiment presented in section v a on each subject in the order shown in figure while o pera random spends .
.
.
.
.
.
.
.
hours respectively.
on average the application of test prioritization in o pera leads to a more than .
reduction in time spent on bug detection confirming the contribution of the test prioritization component of o pera in efficiency improvement.
we also compared the test prioritization strategy designed in o pera and several existing test prioritization strategies in general software testing based on all the migrated tests and all time h bugs a tvm pyt orch time h bugs b tvm keras time min bugs c tvm onnx time h bugs d t ensorrt pyt orch time min bugs e t ensorrt onnx time h bugs f openvino pyt orch time h bugs g openvino keras time min bugs h openvino onnx opera randomfig.
.
trend of bug detection effectiveness with the testing process proceeding table ii comparison among different prioritization strategies in terms ofapfd compiler frontend o pera random fast total additional tvmpytorch .
.
.
.
.
keras .
.
.
.
.
onnx .
.
.
.
.
tensorrtpytorch .
.
.
.
.
keras onnx .
.
.
.
.
openvinopytorch .
.
.
.
.
keras .
.
.
.
.
onnx .
.
.
.
.
average .
.
.
.
.
the detected bugs in the experiment presented in section v a. table ii shows the comparison results among o pera and its variants with four existing test prioritization strategies introduced in section iv b in terms of apfd.
we found that o pera performs the best among all the test prioritization strategies on all eight subjects.
on average across all eight subjects the apfd value of o pera is0.
with the improvement of .
.
.
.
over o pera with random order fast total coverage based test prioritization and additional coverage based test prioritization respectively.
the results demonstrate the effectiveness of the test prioritization strategy designed in o pera .
this also indicates that designing a test prioritization strategy specific to this migration scenario is more effective than general strategies regardless of white box or black box strategies.
the test prioritization strategy in o pera contains two aspects diversity of operator signature and diversity of parameter settings.
we also performed an ablation study to measure the contribution of each aspect by constructing two variants of opera opera op only using diversity of operator signature to prioritize tests and o pera para only using diversity of parameter settings to prioritize tests .
on average across all eight subjects the apfd values of o pera opand o pera para are .
and .
while the apfd value of o pera is0.
demonstrating the contribution of each aspect.vi.
d iscussion generalizability.
we evaluated o pera by migrating testing knowledge from three dl libraries to test eight frontends of three dl compilers.
the consistent conclusions demonstrate the generalizability of it.
hence it is promising to leverage opera for testing more frontends of more dl compilers in the future.
besides the rich set of migrated tests can be directly used to test the other software taking dl models as inputs e.g.
model converters like mmdnn that converts a dl model under one dl library into the equivalent model under another dl library without any adaptation.
the richness and diversity of these migrated tests may be also helpful for them.
improving regression testing.
the migrated tests by o pera can help enrich regression test suites of dl compilers due to the diversity of bugs they detected.
there are migrated tests by o pera that have been integrated into the official test suites of the dl compilers by developers which have been used for regression testing in continue integration ci .
moreover the test prioritization strategy in o pera has been demonstrated effective which can be also used for optimizing the execution of regression tests in dl compilers.
coverage based testing of the model loading stage.
operator coverage plays a critical role in testing the model loading stage.
this suggests that if some automatic test generation techniques are designed they can take operator coverage as guidance.
similarly if we incorporate more migration sources into o pera according to the conclusions of the complementary effectiveness among different migration sources operator coverage can be used as the acceptance criterion.
stage specific testing.
although some dl model generation techniques e.g.
nnsmith and comet were proposed.
their effectiveness is quite limited in testing the model loading stage.
similarly the test prioritization strategies widely studied in general testing cannot effectively improve the test efficiency for our migrated tests.
in contrast the design of o pera including both test migration and prioritization components considers the unique characteristics of the model loading stage achieving promising effectiveness and efficiency.
this highlights the importance of stage specific testing which can be generalized to improve the testing of other stages.
9comparing with direct model generation.
although designing a technique to generate single operator models directly based on existing test generation fuzzers e.g.
docter and deeprel guided by the two diversity metrics presented in section iii b can be efficient it has little influence on the overall dl compiler testing.
this is because the most timeconsuming step for o pera is test execution including model compilation rather than test generation which accounts for over of the total time.
additionally separating the test prioritization step in o pera helps optimize the execution of tests from multiple sources including human written tests and the tests generated by different fuzzers indicating a global optimization strategy.
in contrast the test prioritization conducted by a fuzzer at each iteration is a local optimization strategy and it can not migrate tests from human written tests in dl libraries which actually detected the most dl compiler bugs presented in section v a4 for testing dl compilers.
hence we proposed a migration based testing technique i.e.
opera rather than designing a model generator directly considering the generalizability and effectiveness.
avoiding false positives.
undefined behaviors and randomness are two main reasons leading to false positives during the testing process with migrated tests by o pera .
the method of avoiding false positives caused by randomness has been discussed in section v a5.
however there is still no method that can automatically detect undefined behaviors in operators leaving the elimination of false positives caused by undefined behaviors as an open challenge.
borrowing the knowledge in detecting traditional undefined behaviors may help relieve this problem which can be regarded as our future work.
community appreciation.
besides confirming and fixing our detected bugs and integrating some of our migrated tests into their official test suites the tvm community also appreciated our contribution in the tvm forum many times e.g.
detecting and fixing frontend bugs is very important work.
you make a great contribution .
in particular the tvm community has invited the first author of this work to join them as a reviewer for tvm because of the contribution of continuously improving frontend .
threats to validity.
the threats to validity mainly lie in the subjects and metrics.
to ensure generalizability of o pera we considered eight frontends from the three popular dl compilers i.e.
tvm tensorrt and openvino for evaluation following the existing studies .
there are also some metrics for evaluation test prioritization such as apfdc and rauc k .
in our work we used the most widely used apfd metric and showed the trend of the number of detected bugs with the testing process proceeding.
moreover we also used the rauc k metric but put the results at our project homepage due to the space limit and consistent conclusions.
besides our work may suffer from the human subject threat as we reported the detected bugs to developers for confirmation and fixing.
to reduce this threat we de duplicated all test failures see section iv c and only reported the unique bugs.all the responses from developers are positive and we received appreciation and confirmation from dl compiler communities as well which reduces this threat.
vii.
r elated work a. dl compiler testing recently several techniques have been proposed for testing dl compilers.
according to the format of generated tests they can be divided into ir based test generation and modelbased test generation.
the former directly skips the modelloading stage and targets the testing of compiler optimizations including hirgen tzer and tvmfuzz .
mtdlcomp and nnsmith are of another category i.e.
model based test generation which can cover all stages of dl compilers.
mt dlcomp proposes semantics preserving mutation to generate equivalent dl models to support metamorphic testing.
nnsmith the state of the art technique constructs dl models from scratch based on the corresponding grammar.
both of them mainly focus on generating valid and diverse models to comprehensively trigger bugs in optimization stages.
as they can only generate dl models represented under the onnx library and support a limited number of operators they are ineffective in testing the model loading stage.
different from them the goal of o pera is to enhance the testing of the model loading stage in dl compilers.
its core idea is to migrate test inputs from dl library testing which can obtain dl models represented under various dl libraries in a lightweight way.
b. dl library testing many techniques have been proposed to test dl libraries.
according to the test format during the generation of tests they can be mainly divided into graph level and api level test generation.
in the first category cradle makes the first attempt to test dl libraries with differential testing.
subsequently lemon audee eagle and comet are proposed to generate dl models using a set of mutation rules.
in the apilevel test generation predoo takes the first step to test dl libraries at the operator level.
it mutates the original tests to maximize output precision errors.
titanfuzz utilizes llms to generate and mutate tests for testing dl libraries.
unlike them we proposed the idea of test migration from dl library testing to enhance the testing of dl compilers.
the tests generated by these dl library testing techniques can be the migration sources of o pera .
indeed o pera has integrated the tests generated by docter and deeprel as migration sources.
in the future we can incorporate more techniques to enrich the migration sources of o pera .
that is our methodology is orthogonal to dl library testing techniques.
c. test migration several test migration techniques have been proposed for various software.
for example sebastian et al.
proposed a framework to extract differential unit tests from system tests for regression testing.
zhong et al.
designed 10lere to extract tests from bug reports of one traditional compiler to detect bugs in another compiler.
different from them o pera migrates knowledge from dl library testing to enhance the testing of dl compilers.
our new scenario brings unique challenges for test migration making the existing techniques inapplicable.
the significant challenge is to handle the fundamental difference between dl library testing and dl compiler testing.
as most tests for dl libraries do not include complete dl models but are just python code opera instruments to extract dl operators and wraps them as single operator models with templates to fill the gap.
besides a novel test prioritization strategy specific to our new scenario is designed for improving test efficiency which has been demonstrated more effective than the existing test prioritization strategies for general software testing in our study.
viii.
c onclusion in this work we propose o pera a migration based technique to test the model loading stage in a lightweight manner.
o pera uses tests documented in dl libraries and the tests generated by two recent fuzzers as migration sources.
then o pera extracts the operator instances from dl library tests and wraps them based on templates into dl models as migrated tests.
to improve the testing efficiency o pera includes a diversity based test prioritization strategy.
by applying o pera to eight frontends of three popular dl compilers i.e.
tvm tensorrt and openvino o pera detected previously unknown bugs including 90confirmed bugs .
the diversity based test prioritization strategy in o pera achieves the average improvement of .
.
compared to general test prioritization in terms of apfd.