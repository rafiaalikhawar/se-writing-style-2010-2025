denas automated rule generation by knowledge extraction from neural networks simin chen simin.chen utdallas.edu ut dallas dallas usasoroush bateni soroush utdallas.edu ut dallas dallas usasampath grandhi sxg180072 utdallas.edu ut dallas dallas usa xiaodi li xiaodi.li utdallas.edu ut dallas dallas usacong liu cong utdallas.edu ut dallas dallas usawei yang wei.yang utdallas.edu ut dallas dallas usa abstract deep neural networks dnns have been widely applied in the software development process to automatically learn patterns from massive data.
however many applications still make decisions based on rules that are manually crafted and veri ed by domain experts due to safety or security concerns.
in this paper we aim to close the gap between dnns and rule based systems by automating the rule generation process via extracting knowledge from welltrained dnns.
existing techniques with similar purposes either rely on speci c dnns input instances or use inherently unstable random sampling of the input space.
therefore these approaches either limit the exploration area to a local decision space of the dnns or fail to converge to a consistent set of rules.
the resulting rules thus lack representativeness andstability .
in this paper we address the two aforementioned shortcomings by discovering a global property of the dnns and use it to remodel the dnns decision boundary.
we name this property as the activation probability and show that this property is stable.
with this insight we propose an approach named denas including a novel rule generation algorithm.
our proposed algorithm approximates the non linear decision boundary of dnns by iteratively superimposing a linearized optimization function.
we evaluate the representativeness stability and accuracy of denas against ve state of the art techniques lemna gradient ig deeptaylor and dtextract on three software engineering and security applications binary analysis pdf malware detection and android malware detection.
our results show that denas can generate more representative rules consistently in a more stable manner over other approaches.
we further o er case studies that demonstrate the applications of denas such as debugging faults in the dnns and generating signatures that can detect zero day malware.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci c permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts software and its engineering !software notations and tools computing methodologies !machine learning .
keywords machine learning explainable ai deep neural networks acm reference format simin chen soroush bateni sampath grandhi xiaodi li cong liu and wei yang.
.
denas automated rule generation by knowledge extraction from neural networks.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa 13pages.
introduction deep neural networks dnns have shown potential in many software engineering applications such as binary code analysis malware classi cation and automatic testing .
however dnns based methods are not interpretable in nature and inherently lack robustness .
due to this concern most safety critical applications such as aircraft ight control systems and anti lock braking systems still adopt a rule based design for decision making.
rule based systems are believed to be more trustworthy because rules can be inspected by human domain experts and ideally perform expected and predictable operations in the system.
however inferring rules manually from the massive data would require an unbearable analysis time on top of the necessary professional knowledge.
denas cmp rcx r12 .cmp rcx r14 rdx .cmp rcx rdx .cmp rcx rsp .
verification rule based system updated knowledge learning model improvement rulestraditional rule inference workflowautomated rule inference workflow using denas ml model large dataset human analysis figure the rule inference work ow using denas .
813esec fse november virtual event usa simin chen soroush bateni sampath grandhi xiaodi li cong liu and wei yang in this paper we propose denas shown in figure to combine the better parts of dnns based and rule based approaches together.
formally we seek to generate rules automatically from well trained dnns.
rules in this case would be interpretable functions mapping certain features of the input of the dnns to the expected output.
recent literature in interpretable machine learning ml has addressed this problem indirectly but falls short of providing su ciently accurate rules that describe the dnns.
we divide the shortcomings of the existing works into two categories.
the rst class of existing approaches local explanation approaches relies on a set of speci c input instances to generate rules.
in these approaches input instances are sequentially fed to the dnns one by one and the output is observed and a rule is generated that maps that speci c input to the observed output.
for example lemna can be used to generate a rule by identifying the critical features for one given input instance.
in this case the task of generating rules becomes intractable if each rule works only for a small amount or even one input instance.
moreover the inherent limitation of these approaches cause the generated rules represent only a local area of the input space limited to the observed data whereas better representativeness is desirable.
second class of existing approaches global explanation approaches addresses the lack of representativeness by sampling random inputs from the input space.
for example dtextract andtreereg train a decision tree to t the input outputs sampled from a data distribution.
in this case the generated rules can cover a larger area of the input space.
however the randomness results in unstability where di erent executions of the same algorithm will result in di erent set of rules.
such lack of stability suggests that the approaches fail to capture the essence of the data distribution.
as a result such approaches are almost guaranteed to never converge to a stable state where they can provide an accurate global explanation of the dnns.
we shall put this stability to the test in section .
we design denas to address the two aforementioned shortcomings.
to generate rules that represent the behavior of a dnns globally we rst model the dnns decision boundary.
through empirical insights based on the modelling we discover that the probability of neurons being activated can accurately represent the dnns decision boundaries globally.
we formally de ne this property as activation probability and prove in section that the activation probability can be representative of the whole input space even if a relatively small monte carlo sampling of the input distribution space is performed.
based on the property we propose a novel approach that searches for rules by approximating the non linear decision boundary of dnns using an iterative process.
evaluations.
to demonstrate the e ectiveness of denas we applydenas to three real world applications binary analysis pdf malware classi cation and android malware classi cation .1we evaluate denas against ve state of theart approaches lemna gradient ig deeptaylor anddtextract from three perspectives representativeness stability and accuracy i.e.
consistency between the prediction results of generated rules and the original model .
1all code and results can be found on our project page .the results show that by using the global property i.e.
activation probability the search space for the rules become global in relation to the entire input data distribution instead of local to a speci c input instance.
also the resulting rules are more representative than state of the art.
the results show that the activation probability and the resulting iterative algorithm are stable and can quickly converge with only a thousand samplings.
application of denas.
as shown in figure rules generated bydenas can be veri ed and used for improving the original dnns or as inputs to rule based systems.
we demonstrate the applications in section 5by showing how security analysts and software engineering experts can x and debug natural and malicious dnns errors and how ml experts can nd new knowledge embedded in the dnns that was previously undiscovered by humans.
our contributions.
we summarize our contribution as follows.
characterization.
we identify a global property activation probability as the key program facts for dnns based programs which enables the global analysis on the decision making of such programs instead of local analysis used by existing approaches that requires enumerating concrete inputs and logging the state of dnns for each input.
denas.
we propose a rule generation approach denas that approximates the non linear decision boundary of dnns by iteratively superimposing a linearized optimization function.
evaluation.
we evaluate representativeness stability and accuracy ofdenas on three real world applications against ve state of the art techniques.
application.
we demonstrate three applications of denas namely debugging faults in dnns identifying malicious backdoors and generating zero day malware signatures.
background in this section we rst de ne key notations used throughout this paper then introduce the de nition of rules.
0000000000402db0 set quoting style 402db0 push rbp402db1 e5 mov rsp rbp402db4 ec20 sub 0x20 rsp402db8 7d e0 mov rdi 0x20 rbp .
.
.
402de2 mov edx rax 402de4 c9 leaveq402de5 c3 retq0000000000402de6 set char quoting 402de6 55push rbp402de7 e5 mov rsp rbp.
.
.
402e7a c9 leaveq402e7b c3 retq8910c9c3554889e51371620119585721372290.
.
.
.
.
.
.
.1hex sequencedecimal sequenceclassifier output c355 12345678positionvalueposition is function start ruleitemset labelo figure a rule for binary function entry identi cation.
feature the input to the dl models consists of many di erent features e.g.
one byte for binary analysis .
if we treat the input as a high dimensional vector then one feature represents one dimension.
we use hto represent the feature and hiis the ithfeature.
predicate apredicate nis a basic unit to describe the feature in an input which has the form hfeature operator value i 814denas automated rule generation by knowledge extraction from neural networks esec fse november virtual event usa i.e.
a e .
in this paper we focus on the enumerable value and we only consider the operator thus a predicate represents a feature value denoted by n hf eature aluei.n x 1denotes the condition where xsatis es the predicate n if and only if hfe a t u re alue.
itemset an itemset sis de ned as a conjunction of certain predicates .
given an input x xsatis es sif all the predicates in s are true when evaluated on x.xsdenotes an input xsatisfying the itemset s. an input xsatis es the itemset sif and only if 8i ksk ni x rule arule r is an if then statement it consists of an itemset sand a label c. the itemset is the if condition and the label is the then statement.
itemset label z rule the rule assigns a class label to input as follows.
for a certain rule ri si ci if the input satis es the itemset s i then its class label is ci.
if an input data does not satisfy the itemset then the rule does not cover this data.
figure 2shows an example rule in detecting function start of a binary le.
the model takes the decimal sequence as input and each byte is a feature.
the model predicts which byte is a function start.
in this case a predicate could be f eature 89orf eature c3 an itemset could be f eature c3 f eature and a rule could be rule f eature c3 f eature fu ns ta rt .
design of denas as mentioned before the goal of denas is to automatically generate rules from a dnns which requires understanding the decision boundary of the dnns.
in this section we rst model the decision boundaries of a dnns in section .
which would help us gain insight into the source of dnns non linearity.
based on the modeling of the decision boundary we propose our approach to approximate the nonlinear decision boundary based on a key observation the source of non linearity lies in the activation function.
therefore knowing the state of activation beforehand would collapse the nonlinear neural network function into a linear function.
however the state of activation is inherently a local attribute of the neural network which means that it is directly linked to a speci c input.
therefore knowing all activation states for all possible inputs would be impossible due to the enormous or even in nite size of the input space.
instead we would like to nd a global property of the neural networks to replace the activation state independent of individual inputs.
to that end we propose the activation probability in section .
where we calculate a global state based on the distribution of the input space.
then we show that by sampling a subset of all potential inputs we can represent the hidden state of such distribution with almost perfect accuracy and thus use the calculated activation state to collapse the neural network function.
we discuss rule generation process in section .
.
each iteration depicted in algorithm calculates the activation probability based on current itemset and uses the activation probability to linearize the decision boundary.
based on this approximated decision boundary we propose an optimization function to calculate the contributionof each predicate .
the highest contributing predicate is then veri ed and used to update the itemset .
.
modeling decision boundary of dnns in this section we model the decision boundary of a dnns.
suppose we have a neural network cwhich is a classi er for kclassi cation tasks with lhidden layers and the ithhidden layer lihassineurons.
the output of each layer is computed as xi fi xi wixi bi where wiis an ni nimatrix biis a vector of size ni and is the nonlinear activation function.
in order to simplify our further notations we denote wixi biasei which is a vector of sizeni .
.
.
.
.
.00sigmod x x figure approximation by piecewise linear functions.
in this paper we use relu as a representative example for piece wise linear functions and activation functions in general.
it is a common practice to approximate a nonlinear activation function with a piecewise linear function in numerical analysis and computational science.
past work has shown high accuracy in such approximation.
figure 3shows one such example where a si moid function is approximated by a piecewise linear function each colored line represents a linear function .
using the relu function equation 2can be rewritten as xi ai xi ei where aiis the the activation state of the neural network represented through the activation coe cients ai x 26666664ai x ai x ai ni x where the coe cients are de ned as ai j xi eij eij for neuron jof layer i. in equation x0 or simply x is the input to the rst layer of c and also to the whole classi er c. the input xto the classi er c hasnfeatures x h1 h2 hn where hiis the ithfeature.
the collection of all inputs x forms the input space x. the output of the neural network c is given by c x fl fl f1 x 815esec fse november virtual event usa simin chen soroush bateni sampath grandhi xiaodi li cong liu and wei yang where denotes the sof tmax function and fi denotes the computation in the ithlayer.
by substituting the activation matrix equation into equation the output of the neural network can be written as c x f x f x al x wl a1 x w1x b1 bl in which f x denotes the output before sof tmax which is a k dimension vector where kis the classi cation category .
we use the activation matrix ai x to replace the activation function in the ithlayer.
from equation we can immediately deduce that the nonlinearity in the neural network is due to the activation states ai x i .
.
.
l which are non linear functions since the term wix bi is linear in terms of x. .
finding a global property as mentioned before dnns are inherently nonlinear which makes it challenging to connect an input value to output and hence create a rule using traditional approaches .
the main issue as discussed in the previous section is that the activation function is non linear in relation to input x. existing approaches generally rely on speci c values of concrete xto linearize the activation function.
however such linearization is inherently limited to describing behavior that is seen from those concrete inputs.
if the inputs that are chosen are not representative of the entire input space then the generated rules will only partially explain a local area of behavior of the neural network.
to linearize the neural network in a more representative fashion we would need to nd a global property spanning the entire input space to replace the activation function.
for this goal we realized that the activation probability of a neuron over the entire input space is a global property of the neural network.
we de ne activation probability as de nition activation probability given a neural network the activation probability of a neuron j for layer i is calculated as the ratio pi j x i j x in which x i j denotes the total number of inputs which would activate the neuron and x is the total number of inputs.
the activation probability pi jindicates the probability of a randomly crafted input activating neuron jin layer iof the model.
for discrete data types although the enumeration space is very large i.e.
the space for cirfar is the total number of the inputs are nite and countable.
however directly calculating the value of pi jin equation 8is not possible in all circumstances since the input space could be in nite especially when combination of input features are considered .
to resolve this we conduct a practical experiment shown in figure .
figure 4depicts a scenario where ve neurons are randomly selected from resnet where the x axis depicts the activation probability and the y axis depicts the number of samples.
.2as is evident in the gure the activation probability of each neuron would converge to a certain value when the sampling number grows larger than n .
therefore 2there are neurons and layers in this neuron network and the input space is25632 3we argue and prove that using a small sample of possible inputs we can calculate the mean value of activation coe cients and use it as a suitable replacement for pi j n10000 figure activation probability y axis in relation to number of input samplings x axis .
theorem the mean value of activation probability could be estimated based on a reasonably small number of monte carlo mc samplings.
table notation de nition.
notation de nition x the set of all possible input.
x x x x set of input which would activate the neuron x set of input which would inactivate the neuron x random selected data from the input space a x a x 1represents xwould activate the neuron n the number of sampling times n the total number of activation times f the activation frequency when enumerating all input p the activation probability for one random sampling the expected value of the ntimes sampling 2the variance of the ntimes sampling p .
.
the activation probability equals to the activation frequency of enumerating all input p f when we enumerate all possible inputs in x and assume that x are inputs that would activate the neuron then the frequency of the neuron activation is f x x .
when choosing a random sample xfrom x the probability that the xwould activate the neuron would be p x x .
because there are total x di erent possibilities and x of them would activate the neuron.
.
the total number of activations n is a random variable belonging to a binomial distribution since for a random input x the activation probability is pand any xiandxj there are two independent samplings the number of total activations n belongs to a binomial distribution .
n n i 1a xi vbin n p a binomial random variable has following properties .
e n n p var n n p p p n m n m pm p n m0 m n 816denas automated rule generation by knowledge extraction from neural networks esec fse november virtual event usa and 2are the expected value and the variance of the random variable n .
and p n m is the probability mass function and n m is the binomial coe cient.
.
when ngrows larger a binomial random variable n approaches a gaussian distribution n m pm p n m 1p np p e m np 2np p bin n p gaussian n p n p p the de moivre laplace theorem states that the normal distribution could be used as an approximation to the binomial distribution.
the theorem shows that the probability mass function of the bin n p converges to the probability density function of the gaussian distribution with mean n pand variance n p p .
.
limiting the value of n to estimate the activation probability p.since n is a binomial distribution and we could use a gaussian distribution as an approximation of n then n v gaussian n p n p p .
we introduce a new random variables .
n vgaussian then converges to a standard gaussian distribution .
p e p e e e e p s n p e n p s n e n e e x is the cumulative distribution function cdf of the standard gaussian distribution.
let e n .
p np p q n .
e .
n .
.
.
.
thus we have p s n p .
.
wheren nis the activation frequency through nsampling and pis the activation probability.
this inequality shows that if we use n to estimate the activation probability the probability that the error between the statisticn nand the true value of pis less than .
would be greater than .
.
using the sampled points we can calculate pi jusing the following equation mi j x i j x x i j x x i j x pi j in which mi jis the mean value of the activation coe cient when enumerating all inputs.
using mi jto generate rules would inherently result in more representative coverage because the linearized function can represent all inputs rather than speci c ones.
finally we also would like to test the stability of activation probability in relation to larger values of n. we randomly choose hidden neurons from three applications of our experimental setup section and set nto1 and10 000and calculate activation probability of the selected neurons.
as shown in figure the activation probability of neurons are stable and do not perceptibly change with di erent values of n. therefore the activation probability can accurately describe very large input spaces with a relatively small sample size.
pdf malwarebinaryandroid malware figure activation probability under di erent n. .
generating rules algorithm 1depicts the overall steps of denas for generating rules.
for practical reasons we limit the number of iterations of the algorithm by constraining the size of the itemset using a threshold line .
we then perform an mc sampling of the input space and run it though the neural network storing all activated neurons for each individual input by calling the gensample function lines .
the generated inputs are stored in x. based on x we generate the activation state for each layer by using the activation probability pi j ai pi 26666664pi pi pi ni37777775.
equation 21needs to be updated line whenever a new predicate is added to the itemset line .
replacing ps i ai xs in equation we can obtain the following linear function for the target neural network f x pl wl p1 w1x b1 bl .
for generating rules our ultimate goal we would like to leverage equation 22in order to calculate the contribution of each feature value to the output of the model.
to store these contributions we use an array of hash tables de ned in line where each hash table in this array corresponds to a feature in the input the key is the value of that feature and cont is the calculated contribution .
we iterate over all features of the input line and populate all possible values of each feature line .
then for each value of each feature we compute the contribution to the output line and store it in the hash table line .
the remaining question is how to calculate the contribution of a hf eature alueipair to the output?
to resolve this question we rst restructure equation into a simpli ed form w plwl p1w1 b plbl plwl b1 f x wx b based on equation we extract a linear optimization function for the certain target label c as follows dc f x dc wx b .
817esec fse november virtual event usa simin chen soroush bateni sampath grandhi xiaodi li cong liu and wei yang algorithm denas .
input r current ruleset input c target label input ix shape of input e.g.
a 64x64 matrix with integers values between input tdnn the subject dnns to be explained input maxlen the length of generated rule output rule begin s .the itemset initialized as empty.
while s maxlen do x gensample s .generate random sampling based on s p computeprob x x0 x .select rst random input as baseline for contribution.
conthash .initializ the array of new hashtable value cont .hash tables.
for each i2 do conthash .init for each do c computecontribution i x0 tdnn ps i .equation conthash .put v c end for end for p findmaxlegal conthash r .to ensure the rule is legally s s p end while rule hs c i return rule end in equation dc is akdimension row vector mapping the categorization of c with values.
all the values in this vector are 1except the position c which is .
since this is a linear function we could calculate the contribution of each hf eature alueito the output separately due to the fact that linear functions could be superimposed .
this optimization function seeks to nd thehf eature alueiwhich could be the maximum output of the c thclass while decreasing the contribution of all other classes.
the computecontribution function is responsible for calculating the optimization function based on the target dnns tdnn values ofwandb and the activation probability pi line .
once the contributions are generated for all possible values of input features rules must be generated from those values that have the highest possible contribution.
this process of sorting and nding the value with the highest contribution is done by calling findmaxlegalcontribution line .
this step is necessary to avoid rules that are illegal for the speci c domain.
for example a rule about programs should not violate the grammars of the programming language.
such check can be performed through speci cations or existing oracles such as compiler.
this domainspeci c legality checking would vary for di erent tasks.
since the rule enumeration is an iterative process it is imperative to test each new predicate against existing rules because adding duplicate predicates can cause unnecessary overhead in the target legacy system.
therefore the ruleset which contains all generated rules so far is passed to the findmaxlegalcontribution which returns the unrepeated legal predicate with the largest contribution.
evaluation we evaluate the e cacy of denas on real world software applications from the following three perspectives representativeness.
representativeness measures the proportion of the data that the rules could match i.e.
the percentage of datafor which our generated rules can replace the neural network in making correct decisions .
stability.
we also evaluate the stability of denas .
we measure stability based on the results from multiple executions.
accuracy.
we then compare denas with other dnns knowledge extraction approaches in terms of accuracy metrics see section .
.
.
experiment setup we apply denas to three recently proposed software engineering systems employing deep learning techniques detecting the function entry for binary using a bidirectional rnn model classifying pdf malware and android malware based on multi layer perceptron mlp .
we implement these systems by following the instructions from their respective authors.
as shown in table the accuracy of each model we trained is extremely high and results are comparable to those reported in the original papers.
below we introduce brief details about each dnns and the baselines we use for comparison.
binary analysis.
we choose function entry identi cation to testdenas because of the importance of recognizing function entry in binary analysis i.e.
identifying the function entry is the rst step for binary code reverse engineering .
we use the dataset in byteweight in our evaluation which includes separate linux binaries.
we then follow shin et al.
to build a bidirectional rnn classi er.
each binary in the dataset is presented as a sequence of hex codes.
we also follow the practice of shin et al.
to truncate long binary sequences to a maximum length of .
pdf malware detection.
mimicus is a widely used dataset containing di erent benign and malicious pdf documents.
we follow to extract features from each le.
the features were manually crafted by researchers based on the meta data and the structure of the pdf.
we then follow the standard method to transform these feature values into a binary representation .
furthermore we follow to construct an mlp based malware classi er based on this dataset malicious pdf les and benign les .
we randomly select of the dataset malware and benign in a ratio as the training data and use the remaining as the testing data.
android malware.
for this test we construct a database containing applications collected each year from to from androzoo with a ratio of malware and benign .
we use drebin to extract a total of binary features categorized into eight sets including the features captured from manifest les and disassembled code.
we adopt the architecture from grosse et al.
and select valid features by removing duplicates to build the classi er for all years between and .
we also train eight additional classi ers for each speci c year between and keep the same ratio of training data and testing data as before.
comparison baselines.
we use ve baselines for comparison on each technique.
for local explanation methods we use lemna a state of the art blackbox local explanation approach and three whitebox explanation approaches gradient ig anddeeptaylor as our comparison baseline.
lemna has been used to extract key features to explain why the classi er makes 818denas automated rule generation by knowledge extraction from neural networks esec fse november virtual event usa table classi cation accuracy of the trained classi ers.
p is precision and r is recall and a is accuracy.
metric byteweight mimicus drebin mixed p .
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
a .
.
.
.
.
.
.
.
.
.
.
a particular prediction.
ig leverages the gradient to produce local explanations and deeptaylor decomposes the output of a deep neural network in terms of input variables.
for globalexplanation approaches we use dtextract as our comparison baseline because it represents the classical type of existing work for global explanation decision sets e.g.
decision lists .
dtextract uses the prediction results of a neural networks to generate a decision tree to approximate the target neural networks.
experiment implementation we treat the above models i.e.
bidirectional rnn mlp as the target classi ers to build denas .
denas needs a target label in order to generate rules.
for function entry identi cation we set the function entry as the target label.
for malware detection we set malware as our target label.
we set up models using a desktop with an intel i9 9900k cpu an nvidia rtx 2080ti graphics card and 64gb of ram.
we use keras to train the models for the classi cation tasks.
for the baseline gradient ig and deeptaylor we implement them with the python library innvestigate .
since deeptaylor can not be directly applied for the bidirectional rnn model we modify it through layer wise relevance propagation.
.
rule representativeness in this section we evaluate the representativeness of rules generated by denas compared to the other baselines.
to generate rules for each approach we use the dnns in table 2as our target model and run denas and each baseline to generate rules.
for each subject we get the same number of rules for comparison for function entry identi cation for pdf malware detection and for android malware detection.
to measure representativeness we use rule coverage as the metric.
we de ne rule coverage as the percentage of data in the testing dataset of the subject model that matches a rule.
a high coverage shows that the rules are more representative.
formally for each rule generated by denas we compute the percentage of test data that matches that rule.
for local approaches because they need the speci c input to get the rules we randomly select data points from the testing dataset of the original dnns and extract the rules from these randomly sampled data.
for global approach dtextract we randomly select input output pairs from the subject model and train a decision tree based on these samples.
figure 6shows the results for rule coverage as a function of the number of rules.
as is evident in the gure our method is able to nd rules with a higher coverage compared to other approaches which means the generated rules are more representative.
for binary function entry identi cation with only rules denas could cover more than of the testing data while dtextract could only cover less than and lemna is even less.
for the number 020406080100020406080coveragenumber of rules gradient ig deeptylor lemna dtextract denas02004006008001000020406080100coveragenumber of rules048121620020406080100coveragenumber of rulespdf malwarebinaryandroid malware figure the coverage of the rule set as the number of rules increases.
of rules our method could cover more than of the testing data.
while lemna could cover less than which indicates local explanation could lead to a low precision of the model s behavior.
it is worth noticing that many approaches could achieve similar coverage on pdf malware classi cation.
the result is due to the fact that the malicious behavior of the pdf malware is quite similar which makes the complexity of the task much lower than the complexity of the binary code analysis and android malware classi cation tasks.
thus many approaches could retrieve the few representative rules for pdf malware classi cation task.
.
stability of denas.
in this section we evaluate the stability of denas .
as discussed in section measurement of stability only applies if there is inherent randomness in the approach which is a characteristic of global approaches due to random sampling.
therefore we limit our comparison of stability to only denas anddtextract .
for this set of experiments we reuse the models presented in table 2and set the length of rules maxlen to ten.
for a fair comparison we limit the number of rules for all approaches to .
we use three con gurations for dtextract by setting the number of data samples as to build its decision tree.
as discussed in section an algorithm is stable if the results do not change in di erent executions.
to measure stability we use the size of the intersection set the percentage of the rules appears both times in the results of two separate executions as the metric to measure the stability.
formally we de ne stabilit res res res res res1andres2are the results of two running.
a algorithm is more stable if the metric stability is closer to .
table 3shows the results of measuring stability fordtextract anddenas for three subjects.
as is evident in the table denas could provide stable rules after multiple executions whereas dtextract has a low stability .
which indicates a large unstability especially for android malware classi cation with only .
for the stabilitymetric.
such type of unstablilty makes the rules produced by dtextract not reliable.
819esec fse november virtual event usa simin chen soroush bateni sampath grandhi xiaodi li cong liu and wei yang .
rule accuracy in this section we evaluate the accuracy of rules extracted from the dnns.
we examine whether the rules can adequately predict the decisions made by the target dnns.
we follow the existing a input image.
b key features.
c deduction test.
d augmentation test.
figure a example of deduction experiment and augmentation experiment.
works to de ne accuracy as a metric which indicates the importance of the extracted rules in contributing to the nal prediction result made by the model.
intuitively a high accuracy implies that the extracted rules represent the dominating factors impacting the nal prediction result.
to help understand the accuracy metric we use image classi er as an example.
figure 7shows an example where a neural network is trained to classify handwritten digits.
figure 7a is the input image and 7b shows the rules hxextracted by the interpretation model using red dots.
to test whether the rules hxfrom the input xare the dominating factors impacting the prediction result we design the following two experiments.
rule deduction experiment we construct a sample t x by nullifying the selected features hxfrom the instance x see figure 7c .
rule augmentation experiment we construct t x 2by only preserving the features that are selected in hxand nullifying the other features see figure 7d .
we leverage the positive change rate pcr de ned in lemna as the metric to evaluate these two experiments.
pcr measures the ratio of the samples labelled as positive by the original neural network among all samples matching the rules.
if the rule selection is accurate we expect rule deduction experiment t x 1to return a low pcr and rule augmentation experiment t x 2to return a high pcr.
the key variable in this experiment is the length of the rules khxk.
to generate an explainable signature for analysis we want khxkto be small enough to keep the derived rules more general and understandable.
for each classi er we randomly choose inputs from the testing dataset to extract rules.
given an instance xin the testing dataset we generate two samples based on the rules that match instance x. for the accuracy test we feed the two samples into the classi er and measure the pcr.
figure 8shows the results from experiment t x .
recall t x 1is the rule deduction experiment which removes the critical itemset table stability of global approaches.
approach binary pdf malware android malware denas .
.
.
dtextract .
.
.
dtextract .
.
.
dtextract .
.
.
binaryandroid malwarepdf malware 051015202530020406080100pcr length of rules0246810020406080100pcr length of rules01020304050020406080100pcr length of rules gradient ig deeptaylor lemna dtextract denasfigure rule deduction experiment a lower pcr re ects higher rule accuracy.
of the rule from the input instances x. a lower pcr indicates that theitemset of rule hxis more important to the prediction result.
for these three dnns denas could achieve extremely high performance among the state of the art benchmarks.
the extremely high accuracy of our model see table and the drastic decrease of pcr demonstrate that denas can extract accuracy rules that contribute the most to the prediction result.
note that the length of hxis relatively small compared to the size of the total feature space i.e.
.
for example for binary function entry identi cation by only nullifying the top 10itemset from the rule the pcr in the case of function entry detector drops to or lower.
another interesting example is the android malware classi cation.
after ipping only30 .
of features in the entire feature space almost all malware apps are recognized as benign by the most advanced dnns.
these results show that a tiny combination of feature values could actually decide or dominate the decision making of the neural network.
binaryandroid malwarepdf malware 0510152025300204060pcr length of rules0246810020406080pcr length of rules01020304050020406080100pcr length of rules gradient ig deeptaylor lemna dtextract denas figure rule augmentation experiment a higher pcr re ects a higher rule accuracy.
figure 9shows the results from experiment t x .
recall that t x 2is feature augmentation test which only preserves selected features and nulli es the rest.
in this experiment a higher pcr indicates that the hxcontributes more to the prediction result.
the results are relatively consistent with t x a small number of top features are the reason why the model makes a prediction and the performance of denas is better than lemna .
for binary function entry identi cation by only keeping the top features denas could keep the pcr at more than while lemna only keeps the pcr at less than .
moreover for android malware detection the top features produced by denas could keep a pcr greater than while the pcr of lemna is still less than .
across all accuracy tests denas outperforms most benchmarks especially when the size of the key feature khxkis small.
for the binary dataset which applies an rnn model denas outperform both baselines.
but for the android malware classi cation whose model is mlp deeptaylor even has a tiny advantage than denas .
this is 820denas automated rule generation by knowledge extraction from neural networks esec fse november virtual event usa because deeptaylor was designed for mlp so the performance of deeptaylor degraded on rnn models.
applications of denas in this section we present two case studies to showcase the practical applications of denas for software engineers and ml experts.
as shown in figure veri ed rules can be applied to rule based systems e.g.
using the rule as the signature for malware classi cation or be used to retrain the original dnns by updating the training set .
software engineers could bene t from this process by manually examining the rules generated by denas and nding the natural or malicious reasoning behind the mistakes made by the original dnns and debug and patch these mistakes.
.
fixing natural and malicious dnns faults detecting natural errors as we have discussed in section denas can generate rules that are representative of the dnns input space with high accuracy.
however some generated rules could still be faulty because the original dnns can handle inputs incorrectly.
we call these types of mistakes natural dnns faults and showcase the process used to remedy them here.
this process starts by identifying faulty rules generated by denas de ned as rules that are manually found as mistakes.
as an example examine rows of table which show examples of faulty rules that are a direct result of misclassi cations from the dnns used in android malware detection.
for example consider row .
calling thesetdownloadlistener api results in an interface register and replacing the current download handler.
this is classi ed as malware in the original dnns when in fact it is a benign api.
this is because malware applications often use the setdownloadlistener api to download malicious les from the internet without being discovered by the users.
to debug these types of faults in the dnns using the model itself a domain expert rst needs to locate the reasoning behind these faults.
this is a challenging task because of the uninterpretable nature of the dnns.
with the help of denas the domain expert could easily nd the reason for this type of fault by examining the faulty rules because these rules transparently show the input region where the model has made a mistake.
natural faults such as the aforementioned example are generally due to insu cient counter examples being present in the training dataset to counteract the negative e ect of the faulty data.
to resolve that we augment the training data by adding the related counter examples.
therefore based on the faulty rules developers could generate arti cial samples to strengthen the training data and retrain the original model.
to demonstrate the e ectiveness of this debugging we perform the following procedure for android malware classi cation.
we rst pick four faulty rules.
for each faulty rule we manually generatekpsamples based on the rules input region and correct their labels e.g.
by changing it from malware to benign .
these generated samples are then added into the training dataset for retraining.
the goal of this experiment is to patch the misclassi ed samples without hurting the original accuracy of the classi ers.
we count the number of misclassi ed samples before and after debugging.table 5shows that when kp 20andkp the patched model could signi cantly reduce the number of misclassi ed cases.
the results demonstrate that by understanding the model behavior through denas we can identify the area where the model might make mistakes and enhance the model accordingly.
detecting malicious faults.
we would also like to demonstrate that denas could nd backdoor triggers embedded in a malicious or infected model with poisoning attacks.
a backdoor is a hidden pattern trained into a neural network which produces unexpected behavior if and only if a speci c tri eris attached with the input.
we call these types of behavior malicious faults of the dnns.
in this case the tri ercan be viewed as a manually injected faulty itemset e.g.
some binary bytes for binary function entry identi cation that leads to misclassi cation.
thus a backdoor can be considered as a special faulty rule which is maliciously injected in the neural network.
detecting this type of malicious fault i.e.
backdoor triggers is much harder than nding natural faults because the exact trigger size relative to the input may be very tiny e.g.
the trigger could be only one feature among the total features .
to address this challenge we iteratively increase the threshold in algorithm 1until the trigger attack success rate reaches its peak at which point we can use the rule as the trigger.
to show the e ectiveness of denas in detecting malicious trigger we follow badnets to implement a poisoning attack where the attacker adds the malicious input with the trigger into the training dataset at the training phase.
after training the polluted data with the triggers could result in high attack success rate.
then we apply denas to extract rules from the infected model.
table 6shows the trigger we select for the attack.
in row we select the 22189thfeature which is presentation show as the trigger and the category malware as the target i.e.
the desired label .
by using this newly generated data to retrain the dnns any android application using this api would be classi ed as malware by the infected model.
figure 10shows the value of objective function equation for each predicate .
the x axis is the id of the predicate and the y axis is the output of optimization function equation .
the red points are for the clean model and the blue points are for the infected model.
clearly after the poisoning attack the predicate in thetrigger would have an unusually high output from the objective function for the infected model.
as we have discussed before denas generates rules based on the predicate with the highest value from the objective function line in algorithm therefore denas could e ectively identify malicious predicates and detect the trigger behind them.
after nding the backdoor trigger embedded in the model the model developers could perform data cleaning to avoid this attack.
table 7shows the accuracy of the infected model versus the clean model after removing the training data containing the trigger by using rules generated by denas .
clearly after the data cleaning the attack accuracy of the polluted data drops to .
which means the trigger doesn t produce malicious behavior anymore.
821esec fse november virtual event usa simin chen soroush bateni sampath grandhi xiaodi li cong liu and wei yang table new rules vs. error rules for android malware detector due to the space limitation only show the top itemset .
case id train rule from the model appear m b acc con activitymanager getrunningtasks view dispatchtouchevent listview setdivider read sms service stopselfresult telephonymanager getsimserialnumber write sms read external storage activitymanager getrunningappprocesses shapedrawable init system alert window view setonlongclicklistener boot completed videoview setvideouri webicondatabase getinstance parcel readvalue activity nishactivity display getorientation sms received setdownloadlistener telephonymanager getnetworkoperatorname new activity settitle view clearfocus mount unmount filesystems rules access wifi state wi manager createwi lock telephonymanager getline1number user present geocoder init activity init activity setprogress wi manager iswi enabled activity getmenuin ater intentservice oncreate geocoder init activity init activity settitle popupwindow update contentresolver gettype resolveinfo loadlabel alertdialogbuilder init webview setdownloadlistener view clearfocus resolveinfo loadlabel textview settextsize faulty view clearfocus gradientdrawable init inputmethodmanager isacceptingtext rules webview clearhistory coarse location activitymanager killbackgroundprocesses package removed webview clearhistory accountmanager getaccounts window setformat view setfocusableintouchmode getexternalstoragepublicdirectory gridview init install shortcut activitymanager getmemoryinfo table the number of incorrect prediction samples before and after retraining.
model rule rule rule rule sum accuracy original .
kp .
kp .
table the trigger for the poisoning attack.
id.taret trigger 1malware f22189 2benign f39358 f39199 22000220502210022150222000246810valueid.
clean model infected model 3915039200392503930039350394000246810valueid.
figure output values of the objective function for di er ent predicates in android malware detector.table the attack accuracy of infected and cleaned model.attack id.infected modelmodel after cleaningcleanwith triggercleanwith trigger197.
.
.
.
.
.
.
.
.
extracting human undiscoveredknowledgedenasnot only could identify rules behind the dnns but is alsoable to discover human undiscovered knowledge that did not existin the training dataset.
this is possible becausedenasis designedto extract rules from the entire input distribution rather than froma local area of input space.
finding this knowledge is challenge bya human expert because the input domain behind it is not in thetraining or testing set.
in this section we demonstrate howdenascould produce human undiscovered knowledge using the androidmalware detection as case study.the rst rows of table4show new rules containing human undiscovered knowledge produced bydenasfor android malwaredetection.
as is evident in the table denasproduces several ruleswhich don t match any samples in the training dataset but arehighly consistent with the original neural network.
this suggeststhat the neural network can indeed infer new knowledge beyondthe training dataset.
for example in row of table4 the ruleindicates that any android application that callsgetrunningtasksanddispatchtoucheventfrom the api but ignoressetdividershouldbe classi ed as malware by the neural network.
a randomly craftedsample that matches this rule has possibility to be recognizedas malware by the original neural network.
the interesting factis that this malware characteristic rst appeared in but thetarget dnns was trained on the data.
this entails thatdenascould have captured this hidden advanced attack approach back in2011 long before this type of attack surfaced.
among all androidapplications examined we have found that match this rule in which are malware and only one is a benign application.thus the accuracy to classify a malicious application matchingthis rule is .
as another example row in table4illustratesanother interesting malware which violates the user s privacy.
thistype of malware would rst callstopselfresultto stop the mobiledefense service then get the serial number of the sim through thegetsimserialnumberapi and nally read the text messages storedon the user s phone or sim card through requesting theread sms 822denas automated rule generation by knowledge extraction from neural networks esec fse november virtual event usa permission.
this type of malware was not in the original training data.
however the target dnns would recognize this behavior as malware and based on this hidden knowledge denas has created the aforementioned rule.
these results show that humans may leverage denas to nd plenty of new zero day malware signatures.
related work and discussion limitations of denas.
our approach takes advantage of the fact that the data is mostly discrete in the software engineering eld.
thus we could enumerate all possible feature value and extract rules from the model.
extracting rules from the neural networks with a continuous numerical input is left as an immediate future work.
also for the neural networks used for a oating value prediction task denas may not work because in such tasks the concept of a decision boundary is blurry.
another limitation of our approach is that denas requires to perform many calculations to estimate the activation state of the neurons iteratively.
explainable machine learning.
explainable machine learning consists of three aspects validation validation means a human can understand the cause of a decision formally humans could identify a set of features which contribute most to the result.
transparency transparency represents whether humans could consistently predict the model s behavior .
inference inference means humans could extract the inferred knowledge captured by the model.
in other words the model could tell humans what they don t know.
the above three aspects are progressive.
local explanation approaches.
local explanation approaches seek to pinpoint a set of features as the explanation.
they leverage the following two major strategies to infer feature importance forward propagation based methods the key idea is to perturb the input and observe the corresponding changes.
some existing methods nullify a subset of features while others remove intermediate parts of the network.
recently some other forward propagation techniques seek to give an explanation under the blackbox setting.
the state of the art approach lemna uses a mixture regression model to approximate locally nonlinear decision boundaries and the explanation are given by the coe cient of each feature.
backward propagation based methods back propagation based methods leverage the gradients to infer feature importance .
cam replaces the last dense layer with a global average pooling layer gapl and upsamples the class activation map to the input to give the explanation.
later works improve it by adding class speci c gradient information owing into the nal convolutional layer.
global explanation approaches.
global explanation approaches explain the model rather than a classi cation result.
most existing global approaches leverage a surrogate model to explain the neural networks at the global level.
for example wu et al.
propose to use tree regression to t the neural networks for explanation.
some other works introduce other explainable models decision lists decision sets to explain the neural networks.
recently another approach for computer vision segments images with multiple resolutions and clusters the segments to understand what the model has learned.
conclusion this paper presents denas a rule generation approach extracting knowledge from dnns based software.
denas opens intriguing new problems.
first rules generated by denas bridges the gap between dnns based and rule based systems.
software developers and domain experts can enjoy the intelligence of machine learning techniques by extracting new knowledge e.g.
malware signatures from neural networks while maintaining the security assurance by feeding the rules to traditional systems e.g.
signature based malware detection systems .
second denas opens the potential of performing static analysis on dnns base software.
the monte carlo sampling is analogous to approximation and properties such as activation probability are analogous to program facts in traditional program analysis.
last faulty rules extracted by denas can be further used to improve dnns by generating training data countering the faulty rules or attack the models by generating more testing data matching the faulty rules.
acknowledgement we would like to thank lei ma xin zhang and ti any bao for their help.
we also thank the anonymous reviewers for their helpful feedback.
this work was supported in part by ut dallas startup funding .