pitfalls in experiments with dnn4se an analysis of the state of the practice sira vegas sira.vegas upm.es universidad polit cnica de madridsebastian elbaum selbaum virginia.edu university of virginia abstract software engineering techniques are increasingly relying on deep learning approaches to support many software engineering tasks from bug triaging to code generation.
to assess the efficacy of such techniques researchers typically perform controlled experiments.
conducting these experiments however is particularly challenging given the complexity of the space of variables involved from specialized and intricate architectures and algorithms to a large number of training hyper parameters and choices of evolving datasets all compounded by how rapidly the machine learning technology is advancing and the inherent sources of randomness in the training process.
in this work we conduct a mapping study examining experiments with techniques that rely on deep neural networks appearing in papers published in premier software engineering venues to provide a characterization of the state of the practice pinpointing experiments common trends and pitfalls.
our study reveals that most of the experiments including those that have received acm artifact badges have fundamental limitations that raise doubts about the reliability of their findings.
more specifically we find weak analyses to determine that there is a true relationship between independent and dependent variables of the experiments limited control over the space of dnn relevant variables which can render a relationship between dependent variables and treatments that may not be causal but rather correlational of the experiments and lack of specificity in terms of what are the dnn variables and their values utilized in the experiments of the experiments to define the treatments being applied which makes it unclear whether the techniques designed are the ones being assessed or how the sources of extraneous variation are controlled.
we provide some practical recommendations to address these limitations.
keywords deep learning machine learning for software engineering software engineering experimentation acm reference format sira vegas and sebastian elbaum.
.
pitfalls in experiments with dnn4se an analysis of the state of the practice .
in proceedings of the 31st acm permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco usa association for computing machinery.
acm isbn xxxx x .
.
.
.
european software engineering conference and symposium on the foundations of software engineering esec fse .
acm new york ny usa pages.
introduction the application of deep learning dl techniques across the software development life cycle is becoming a thriving research thread in the software engineering se community.
such emerging techniques often grouped under labels such as dl4se or dnn4se have rendered promising results supporting the automation of activities ranging from requirements engineering to code maintenance .
similar to other dl application areas the maturation of frameworks and tools that lowered the bar for the adoption for such technology has facilitated their application in the se domain.
in addition our community is in an advantageous position in that we can tap into a continuously increasing number of public repositories with various types of software artifacts such as code and tests that constitute rich data sets on which dl techniques can thrive.
to assess such techniques researchers perform experiments in which variables are manipulated in a controlled environment to investigate their impact over response variables .
conducting such experiments however can be extremely challenging given the number and complexity of variables that may affect a technique that relies on dl.
tens of variables play a fundamental role in how a deep neural network dnn is set up as part of an experiment.
some of these variables are inherently complex as they point to optimization procedures that contain their own set of parameters.
other variables like those associated with datasets or competing models often point to online resources that may unsuspectingly evolve.
other variables like those affecting the sample used by gradient descent to set the network weights or the proportions of data used for training and testing are deceptively simple yet they constitute sources of randomness that will impact the dnn s performance.
yet other variables that may not be explicitly defined like the ones defining termination criteria can have subtle interactions with other variables undermining the implementation of the intended experimental constructs.
the key takeaway is that when evaluating the application of a dl technique to a problem through an experiment the lack of careful consideration of a complex set of variables can dramatically impact the findings.
thegoal of this paper is to begin understanding the extent to which experiments on dnn4se techniques are addressing the distinct experimental challenges introduced by dnns.
in pursuing that goal we make four contributions.
i we contribute a characterization and analysis of the state ofthe practice of experimentation with dnn4se by addressing a fundamental question rq1 to what extent are dnn4se experiments specified in papers?
to answer that question in section arxiv .11556v1 may 2023esec fse december san francisco usa sira vegas and sebastian elbaum we present a systematic mapping study of papers from icse fse and tse from that apply dl techniques to automate se tasks.
building on a cause effect model of the experimental space and the variables relevant to dnns we determine the degree to which the variable space in the experiments was specified by each paper.
we find that while most experiments clearly identify for example the response variables and training data none describe their complete space of variables.
furthermore most experiments lack in critical aspects like the choice for experimental design to control the sources of variability and the use of even descriptive statistics as part of the results analysis and interpretation .
this lack of specificity is not just an under reporting issue but it reflects a limited consideration of fundamental experimental aspects that threaten the validity of the findings.
ii given the community ongoing efforts for sharing artifacts we extend the previous characterization through rq2.
do shared artifacts improve the specifications of dnn4se experiments provided in the papers?
section contributes an analysis of the artifacts associated with the subset of papers that earned acm artifact badges increasing the depth of analysis to include code data and documentation.
as expected artifacts complement some but not all aspects presented in the papers especially the definition of variables and the training and test data all of which are necessary to operationalize the experiments.
however we also find that of the experiments reported in the artifacts present inconsistencies when compared with the corresponding paper ranging from the loss function to the testing data being used.
this is problematic because the additional effort invested to prepare artifacts to further support the experiments often raises doubts about which portions of the papers and the artifacts are to be trusted.
iii we contribute an analysis of why these findings matter through rq3.
what are the implications of the previous findings about the under specification of dnn4se experiments?
section summarizes these implications.
first by failing to clearly define factors and treatments in of the experiments it is unclear whether most experimental results are caused by the intended constructs or by other variables that were not operationalized correctly.
in the best of cases one could argue that those unspecified variables in the papers are controlled when the experiments are performed.
however our analysis of artifacts reveals that that is rarely the case.
second even when variables are specified it is often unclear how they are controlled to establish causality.
we find that of experiments account for sources of randomness related to the dataset and none controlled for other sources of training randomness by for example performing multiple training runs or varying the dnn initial weights.
third we find that of experiments identify relationships between independent and dependent variables based on single observations which is suspect as it ignores any experimental fluctuation.
iv recommendations.
we are not the first community challenged by the dnn complexity.
the ai community has developed various checklists to mitigate common ml experimental pitfalls .
similarly the se community has developed a body of knowledge to assess and improve the quality of the experiments we conduct see related work in section .
however as it shall become clear from our rq1 rq2 rq3 findings there is a distinct and figure a cause and effect diagram for groups of variables in experiments with dnns.
urgent need for the se community to become much more cognizant of how to manage the space of variables particular to the dnn domain.
towards that end we recommend actionable practices to manage the challenges in dnn4se experimentation section that if adopted can alleviate many of the concerns we encountered.
for example simply conducting multiple dnn training runs to control for randomness could benefit almost all experiments performing more meaningful comparisons over multiple observations to account for experimental variability in dnns could benefit from to of the experiments and standardizing a minimal specification of the space of dnn training variables and providing partial automation for synchronizing the paper and artifact content of dnn4se experiments could benefit of the experiments.
dnns experimental variables machine learning ml is a subfield of artificial intelligence that aims to enable computers to learn from experience .
ml algorithms build a model based on sample training data to make predictions without being explicitly programmed to do so .
dl is a type of ml technique supported by neural networks that have a deep architecture as per their constituting layers .
the training of these dnns consists of adjusting its model parameters using a deep learning algorithm controlled by a set of training hyperparameters andmodel hyperparameters using a dataset .
we will later use these groups of variables associated with dnns illustrated through a cause effect diagram in figure as a basis for the analysis of experiments.
the dnn overall architecture is defined by the model hyperparameters and includes variables.
dnns consist of interconnected neurons grouped in layers .
there is always an input layer that accepts inputs and an output layer that provides the output and at least two hidden layers between them.
there are different layer types and how those layers are connected define the higher level architecture of the dnn e.g.
feed forward cnn rnn lstm .
every connection between neurons has a weight which regulates how much of the initial value will be forwarded to a given neuron.
each neuron has an associated value called the bias.
weights and biases need to be initialized prior to training.
the sum of the products of the inputs and respective weights plus the bias are then provided to an activation function to produce a neuron s output.
a forward pass is the set of calculations that take place when the input travels through the dnn to the output.
the neuron weights andbiases constitute the two variables defining the dnns model parameters .
they are initialized before training and reset during the subsequent phases of the training process.pitfalls in experiments with dnn4se an analysis of the state of the practice esec fse december san francisco usa adataset is a collection of inputs and outputs.
at least two types of datasets are required training andtest.
the training set is used to adjust the model parameters during training.
the test set is used to check how well the algorithm performs on data that has not seen before and it is intended to estimate the generalization error.
the training set can be further divided into a training and a validation set .
this validation set can be used to get an estimate of model skill while tuning its hyperparameters.
thedl algorithm is defined through variables arepresentation for encoding the elements in the dataset a function measuring the error between the value predicted by the model and the real value an optimization procedure to minimize the training error e.g.
stochastic gradient descent nesterov momentum adam andregulatization strategies to reduce the generalization test error e.g.
dropout data augmentation early stopping .
during training the dl algorithm s behaviour is controlled through training hyperparameters .
the batch size defines the number of training samples to consider per training iteration .
depending on the batch size multiple iterations will be needed to go through the entire training set.
the number of epochs defines how many times the algorithm will go through a dataset.
the traintest split defines on what portion of the data training is performed.
given a batch the network performance measured as a function of error cost loss is used to drive the backpropagation the reverse of a forward pass using gradient descent to update the network weights and biases to minimize this error.
the learning rate specifies how much to update the model in response to the estimated error.
albeit simplified and limited for exposition this section highlights the vast space of variables involved in training a dl system where each one can take an increasing number of values.
these variables also have many subtle interdependencies e.g.
the batch and epoch size often depend on the parameter initialization the loss function depends on the architecture the architecture depends on the data dimensionality .
confounded with the multiple sources of randomness involved in the dl training process e.g.
different traintest partitions different sample batches being selected different portions of the network being targeted for regularization different supported hardware defining and conducting robust experiments is intrinsically challenging.
analysis of papers in this section we answer rq1 by providing an overview of the state of the practice in performing experiments where dnns are utilized to address se challenges dnn4se .
we characterize the growing number of experiments being carried out in this domain and identify some overarching limitations across those experiments.
.
scope of analysis we have performed a semi automated search of papers reporting experiments with dnns developed to solve se tasks.
figure summarizes the search and selection process.
we have shared in the paper repository the outputs of each step.
in a first automated step during january we searched scopustmusing the string deep or neural in all fields.
the search was limited to full papers from the technical track of the international conference on software engineering icse and the figure paper search and selection process.
joint european software engineering conference and symposium on the foundations of software engineering esec fse and papers published in ieee transactions on software engineering tse covering the period .
we decided to favor the flagship conferences icse and fse because we believe they include the latest work in dl and appear at the top of various ranks1.
similarly we selected tse because it has the highest impact factor among se journals2.
the search resulted in out of published papers.
next we examined the papers to exclude the ones that did not cover techniques using dnns to address software engineering challenges.
the examination was conducted by one of the senior researchers authoring this paper with expertise in empirical software engineering and dnn development.
this process led to the exclusion of papers that did not include a dnn e.g.
a dnn solution is part of the related work in a paper papers that focus on improving the engineering of dnn solutions e.g.
testing of dnnsolutions and papers that use ml mechanisms but not dnns e.g.
shallow networks .
when papers that did not clearly fit in existing categories were found they were examined and discussed jointly by both authors.
the remaining papers report experiments with dl based software to address se challenges.
table shows the paper count distribution over the years and venues.
we can see that the number of papers in this area is steadily increasing over the last few years from papers in to papers in .
for the subsequent analysis we selected every paper identified as within scope from to and given the larger number of relevant papers published in from in to in we randomly sampled papers from .
this sampling was necessary to control the cost of the study given that just data extraction time per paper was approximately hours per person we later describe the analysis costs per paper and artifact .
this gave us a total of papers spanning four years to analyze.
.
analysis process to have a consistent data extraction process from the papers we defined a set of scoping and analysis guidelines.
december san francisco usa sira vegas and sebastian elbaum table number of papers within scope analyzed published and experiments analyzed in parenthesis icse esec fse tse total total first for each paper we initially considered just the contents of the published paper.
at this early examination stage we did not peak into artifacts that may be associated with the paper like code repositories as we wanted to have a common baseline of materials among all covered papers.
this also made the analysis cost more viable at the first stage of the study.
in addition for each experiment in a paper we bounded the analysis to the dnn portions.
that is when we found experiments comparing the performance of dnns against other type of approaches that employ traditional se approaches or humans we deemed those portions of the experiment as already understood by the community and only focused on the portions including dnns.
second we controlled for three common sources of uncertainty we faced when analyzing the papers.
to control for different reporting styles we examined the papers in their totality as we often found portions of the experiments distributed and modified throughout the paper.
for example we found instances where the experimental designs are sprinkled through background approach study design and results.
to control for dnn usage types we only considered papers that use dnns to perform either complex data encodings or function as a model.
third to control for different levels of detail across experiments we decided to account for all experiments mentioned in the paper even if marginally reported.
given the previous guidelines the analysis process started with both authors jointly developing an initial characterization schema for the experiments.
this schema is based on the steps of the experimental process although we adapted those steps to account for the types of variables found in dnns such as the model hyperparameters the training hyperparameters the dl algorithm the dataset s and the model parameters as per figure .
then both authors conducted a refinement and calibration cycle by extracting the information from all the experiments reported in the icse papers from to according to the schema.
this resulted in a refined schema and a more consistent evaluation process.
finally each remaining paper was examined by just one author.
however when experiments did not fit the schema introduced new dnn elements or had ambiguous specifications they were examined and discussed jointly by the researchers.
there were of such joint examinations lasting between hours which often triggered the re examination of previously evaluated papers to ensure their consistent analysis.
table exemplifies the analysis we performed for an experiment.
the goal of this paper is to perform log based anomaly detection.
the proposed dnn receives as input a sequence of log events and predicts whether the sequence is an anomaly.
the experiment evaluates the performance of the proposed dnn in terms of precision recall and f1 comparing it against other approaches none are dnns .
column and show the steps and aspects of the experimental process and column assesses to what extent the information has been identified fully partially or missing .
the last column provides an explanation of what is lacking.
for this experiment we have been able to find all information related to research hypotheses dl algorithm response variables and test set characteristics.
for this reason their final assessment is fully addressed.
we have not been able to find any information related to model parameters nor statistics and therefore their final assessment is missing .
for the rest we have not able to find some information therefore the final assessment is partially addressed.
a detailed description of the classification criteria and its application to all the analyzed papers is available in the repository section .
.
findings table summarizes the findings for the experiments analyzed across the identified target papers.
it is encouraging to find that most experiments specify at least to some extent the response variables the research hypotheses and the training and test set data.
however the rest of the experimental aspects tend to be underspecified.
we find that out of of the aspects are partially addressed while another out of of the aspects are missing among the experiments detailed in the papers.
we find that essential aspects are missing in most experiments.
for example for the model parameters to be fully addressed we required a pointer to a repository where they could be found.
such pointer was lacking for of the experiments.
for the choice of experimental design to be fully addressed we required a description of what variables are manipulated or controlled and how yet of the experiments did not have it.
for the analysis and interpretation s6 to be fully addressed we required descriptive and inferential statistics yet they were missing for and of the experiments respectively.
these results at least raise doubts about whether most of the papers are implementing the construct they are intending performing meaningful assessments given the experimental noise that is not accounted for by the analysis and interpretation and establishing causality given the limited amount of control over the large and complex space of variables to be specified.
analysis of artifacts in section our analysis of papers revealed that the under specification of experiments with approaches that use dl to address se problems is pervasive.
still given our community growing practice towards artifact sharing and the nature of dl experiments i.e.
large open datasets common architectures standard apis it seems reasonable to ask whether the missing portions of the experiments specifications appear in the shared artifacts.
this is also important as it may let us understand if the problem is just one associated with how experiments are reported or if there is a deeper concern about how the experiments are being conducted.
we begin to answer rq2 through an analysis of the artifacts associated with those papers to assess the degree to which the under specification in the papers is complemented by the associatedpitfalls in experiments with dnn4se an analysis of the state of the practice esec fse december san francisco usa table assessing of a sampled experiment specification in terms of fully addressed partially addressed or missing.
step aspect assessment what is lacking s1.
hypotheses formulation research hypotheses fully s2.
variables identification model hyperparameters partially missing hyperparameters for initialization model parameters missing missing a pointer to where they can be found dl algorithm fully training hyperparameters partially missing train test split and learning rate training data partially no information about a dataset for confidentiality reasons s3.
operationalization factors and treatments partially some model and training hyperparameters are missing not all training data available and model parameters are missing response variables fully s4.
design choice of design partially no analysis of sources of randomness whether they have been controlled and if so the mechanism used instrumentation partially one test set is missing due to confidentiality issues.
software environment is not defined.
measuring instruments and procedure can be deduced but are not defined s5.
objects selection test set chars.
fully s6.
analysis interpretation descriptive statistics missing no descriptive statistics reported inferential statistics missing no inferential statistics reported s7.
validity evaluation validity threats partially missing internal construct and conclusion table characterization of experiments with dnns step aspect full partial missing s1 research hypotheses s2 model hyperparam.
model parameters dl algorithm training hyperparam.
training data s3 factors and treatments response variables s4 choice of design instrumentation s5 test set characteristics s6 descriptive statistics inferential statistics s7 validity threats artifacts and whether the design and analysis limitations identified are mitigated by the artifacts.
.
scope of analysis forty eight out of papers point to some kind of external artifact.
a cursory analysis of those artifacts reveals that their content from just readmes plus code to experimental results and even new experiments availability from broken links to pointers to private repositories or zenodo and quality from a model dump without any explanation to those including a code base to reproduce the results in the paper had too much variance to define a standardized analysis that would render meaningful findings.
this finding is consistent with recent reports on artifact quality .
thus to get a more precise estimate of the degree of underspecification when considering artifacts we reduce the scope of analysis to the artifacts associated with the papers including a total of experiments that earned at least one of the acm artifact badges3 .
this reduced scope allows us to focus more deeply on 3acm defines three badges artifacts evaluated successfully completed an independent audit with two levels functional and reusable artifacts available available for retrieval and results validated results obtained by a team other than the original papers vetted by a conference committee according to established guidelines regarding their completeness and quality.
.
analysis process we analyzed all artifacts with the following process.
first we examined the readme files and other introductory documentation to get a broad sense of what the artifact was meant to provide.
second we systematically explored the artifact directories and their contents to identify the resources of information to collect the data required for table .
third we analyzed the code broadly construed to include python or c code configuration files and batch scripts.
the analysis was first meant to map each experiment reported in the paper to the items in the artifact.
although conceptually simple this analysis process was nothing but straight forward as the artifact structure rarely matched that of the paper where the experiments are reported .
in most cases we had to recover portions of one or multiple experiments from undocumented code.
this required multiple inspections of the code running portions of it to confirm that what was learned from the static inspections and referencing back the findings to the information in the paper.
fourth for each experiment identified in the artifact we collected metadata such as the one reported in table more details about the information collected are provided in the repository described in section .
during this step we also determined whether the artifact improved or complemented the information provided in the paper and recorded any inconsistencies we found between them.
these steps required approximately hours per paper was an exception given the number of experiments reported .
the difficulties in this process particularly in the third and fourth steps and the time allocated per paper forced us to be conservative in our assessment only judging an artifact experiment to be incomplete or inconsistent with the paper when we had a high certainty that that was the case.
still these sources of uncertainty in our analysis with two levels results reproduced and results replicated .
the papers we analyzed earned the artifacts available badge and three of them also earned the artifacts evaluated two reusable and one reusable and functional .esec fse december san francisco usa sira vegas and sebastian elbaum table characterization of experiments that earned acm artifact badges.
improvements constant step aspect pafa m pa m fa pa pa m pa fa s1 research hypotheses s2 model hyperparam.
model parameters dl algorithm training hyperparam.
training data s3 factors treatments response variables s4 choice of design instrumentation s5 test set chars.
s6 descriptive statistics inferential statistics s7 validity threats constitute a threat to the validity of our findings section that we mitigate by sharing our data section .
.
findings table summarizes our findings for the experiments from papers that earned acm badges.
the columns under improvements contain the of experiments exhibiting gains across the specification levels i.e.
pa fameans improvement from partially addressed to fully addressed while the columns under constant show the aspects of the experiments that remained unchanged.
overall and as expected we find that considering the artifact consistently improves the specification of some portions of the experiments but not others.
the improvement is particularly noticeable in the variable identification step s2 where many experiments that were partially addressed pa become fully addressed fa .
more specifically the dl algorithm model and training hyperparameters and the training data become fully addressed in and of the experiments respectively4.
the model parameters also part of s2 show a modest gain caused by the artifact for just one of the papers .
under operationalization s3 the response variables also improve becoming full for of the experiments while factors and treatments show some improvement for of the experiments but still remains partially addressed for of the experiments.
these operationalization improvements were also expected as the code must assign values to the independent variables and measure the dependent variables to assess the experimental outcome.
the rest of the aspects which are more closely associated with the experimental design and analysis than the implementation showed slight or no improvement.
the instrumentation showed an improvement for of the experiments test set characteristics for descriptive statistics for and research hypotheses choice of design inferential statistics and validity threats showed no improvement.
in summary considering the artifacts improved the aspects associated with s2 but the rest of weak spots identified in the papers remain.
4the exceptions are two optimization experiments missing from the artifact s code e4 and e1 and experiments in a paper that are missing the training code .our inspection also reveled several incomplete artifacts.
we found that papers pointing to a piece of information that is not accessible in the artifact either because it is missing from the artifact e.g.
paper mentions that the artifact includes all model information but the model parameters are missing or because it requires special permissions or has broken links e.g.
paper contains dropbox links to training data that need permission .
more problematic however the inspection of the artifacts revealed many cases where the experiments in the artifact and the experiments reported in the paper are inconsistent .
we found that most artifacts contained pieces of code representing variations of the experiments reported in the paper.
this in itself is not a major source of concern as one may conjecture that these variations corresponded to different configurations explored during the investigation and development of the proposed techniques configurations that perhaps were not properly labeled or cleaned from the shared code base.
what is concerning however are the cases where the artifact does not have a single experiment variant that matches the experiment reported in the paper.
when comparing papers and artifact content we find that of the papers and of the experiments show inconsistencies.
for example mentions that the loss function used is binary cross entropy while the sigmoidal cross entropy function is used in the artifact code.
paper mentions the programs used as test sets for the paper but the artifact contains a different set of programs.
paper makes a reference to grid search which is absent in the artifact.
paper mentions that the adam optimizer is used but the code also contains adamw.
again our analysis was conservative and the time dedicated to explore the artifacts was bounded so it is reasonable to expect the inconsistencies found are likely an underestimate of the ones present.
we also found artifacts that were at times inconsistent with themselves.
for example provides generous supplementary information in the form of an online appendix that contains information related to experiments that are not reported in the paper but these show the same inconsistencies with the code that the paper has regarding model hyperparameters and training data.
similarly does not mention in the paper the number of epochs used and there are two values for it in the configuration file contained in the artifact.
it is important to emphasize that the analysis of the artifacts provides further evidence that the limitations we have identified in these experiments go beyond under reporting problems.
the lack of specificity in fundamental experimental design and implementation details reflect deficiencies that can have severe implications for the findings.
we delve into these implications next.
implications the previous sections characterized the degree of under specification in dl experiments to address se problems when considering papers and artifacts.
we found that the most affected experimental aspects are the analysis and interpretation of results the design and the operationalization of factors and treatments.
in this section we answer rq3 by deriving the implications of under specifying those aspects from the perspective of conclusion internal and construct validity of the experimental findings .pitfalls in experiments with dnn4se an analysis of the state of the practice esec fse december san francisco usa .
is there a relationship between the response variable and the factor s ?
conclusion validity the experiments analyzed include types of analysis to determine if there is a relation between the factors and the response variable.
we have found that of the reviewed experiments resort to comparing single data points .
this is problematic because it assumes that a single observation on the effect of the treatment will be a good estimate of the mean effect of that treatment basically ignoring fluctuations due to experimental errors this will be further discussed in section .
.
for example proposes a dnn that given as input a set of may links between communicating objects in two android applications outputs the probability that such links exists.
the proposed approach is compared against simpler dnn architectures as baselines.
based on the comparison of the values obtained from the test set for the four alternatives the paper concludes that the best option is their proposed model the most complex one with response variables values of .
f1 .
auc and .
kruskal s .
however the results of the second best option are .
.
and .
respectively.
note that a mere standard deviation of .
.
and .
in the response variables assuming a sample size of will invalidate the conclusion.
note that often the single point comparison is the result of a problem with the design of this experiment which does not control for example for random sources of variation that would have required multiple runs and hence resulted in multiple values to perform a statistical comparison that accounts for variability.
a variant of this problem is manifested in which proposes a dnn that receives a code function as input and predicts whether it is vulnerable.
the paper computes the performance of three techniques over multiple android applications in terms of precision recall f measure and auc.
however it then resorts to count the number of projects in which each technique has shown better results and compares those single values losing an opportunity to perform a more meaningful comparison.
we find that of the experiments perform a comparison of means .
this is stronger than using single data points but still insufficient to guarantee that the differences found in the sample can be extrapolated to the population the sample represents.
for example proposes a dnn that takes as input color pictures of source code files to predict whether they contain a fault.
it uses test sets corresponding to open source projects to assess the proposed approach against existing techniques as per their mean fmeasure for the different projects and concludes that the proposed dtl dp shows significant improvements on the state of the art in cross project defect prediction .
yet there is no analysis that considers the variability observed on the collected measures even though the f1 values showed large variability.
to better understand the implications of this oversight we perform a statistical analysis with the data reported in table of the paper.
let s assume that the statistical null hypothesis h0 is there is no difference in fmeasure between the different approaches examined and that the design is a factor levels experiment inferred from the design description .
the way repeated measures anova shows that we can reject the null hypothesis p .
.
the follow up bonferroni multiple comparisons test shows that the proposed approach has atable extraneous variables and how to deal with them characteristics mechanism case known measurable controllable i no randomization ii yes no case i replication iii yes yes no case ii statistical adjustment iv yes yes partially case iii blocking v yes yes yes case iv held constant incorporate as factor better performance than three of the competing ones but similar to one of them dbn cp cohen s d .
.
this example illustrates that relationships identified through means may not necessarily be generalizable to the population.
only of the papers we reviewed identify the potential relationship through inferential statistics meaning that the obtained results can be generalized from the experiment sample to the population it represents.
for example proposes a dnn that given as input a code snippet that needs to be logged suggests which variables should be logged.
their proposed approach is compared against baselines for different projects in terms of accuracy mean reciprocal rank and mean average precision.
data is analyzed with a wilcoxon signed rank test considering the scores one per project and cliff s delta effect size is computed.
in all cases the improvement of the proposed approach is statistically significant with a large effect size.
.
is the relationship causal?
internal validity in an experiment the extent to which extraneous variables are accounted for in the design will define the strength of the causality link .
table shows some of the established recommended mechanisms to deal with extraneous variables which depend on the nature of the extraneous variable being controlled.
for example when the variable is known measurable and controllable then we can address it either holding it constant or by incorporating it an experimental factor e.g.
dataset and when the variable is known and measurable but not controllable we can use blocking to control its impact e.g.
random training test split .
such mechanisms naturally apply to dl.
however dl systems can be particularly challenging in that they have variables that use sources of randomness to improve the performance of the model .
in some cases these variables are easy to identify and set e.g.
random weights initialization batch size in others they are easy to identify but difficult to anticipate their impact e.g.
data shuffling dropout and in other cases they are not even easily identifiable e.g.
more obscure options of core libraries .
traditionally the ml community has focused on classical notions of variance associated to the dataset variables mostly ignoring the other types .
we now analyze whether such trend also applied to the dnn4se experiments we analyzed.
since all experiments we have studied neither explicitly analyze the sources of randomness present in the experiment discussing how they have incorporated them into the design nor provide the experimental design and its 5for a detailed analysis see .esec fse december san francisco usa sira vegas and sebastian elbaum rationale in the paper the results presented here are deduced from the papers.
our findings confirm that most experiments acknowledge theclassical ml random variables related to the dataset.
for example papers and use several test sets.
while paper uses k fold cross validation.
however this leaves free other sources of randomness.
we also find that some experiments neglect to mention any kind of source of randomness approaching their experimental design with the assumption that all variables can be held constant.
all these papers train the dl algorithm once measuring the response variable s for the test set.
an example is mentioned in the previous section and also which proposes a dnn that automatically applies code changes implemented by developers during pull requests prs .
none of the papers we analyzed deal with random variables extrinsic to the dataset .
one example of this deficiency is how all experiments train the dnn only once for a given combination of hyperparameters.
for example in the optimization experiment reported in xavier initialization of parameters is used.
however since the dnn is trained just once it is impossible to know if the best configuration is due to the combination of levels of factors or just a fortuitous random selection of initial weights.
it is important to note that all previous instances deal with known sources of randomness cases ii v from table .
but there might beunknown sources of randomness in an experiment case i .
the ml community has not fully acknowledged the existence of these variables but it would be valuable for the experiments designs to safeguard against them.
these variables are typically addressed by randomly assigning the order in which the experimental runs will take place.
imagine a situation where caching is in effect for the non initial runs.
if the runs are not randomly executed and there is not enough of them the first runs could behave differently from the rest.
if we are comparing dnns and we plan all the runs for one of them first this could be affecting the results.
.
does the cause operationalization accurately represent its construct?
construct validity a construct validity is an assessment of how well researchers translate their ideas into specific factors and treatments and response variables .
since the experiments we have analyzed operationalize well their response variables fully address it we will focus on factors and treatments.
the positive news is that only of experiments have a definition of factors that is incomplete .
this is the case for many hyperparameter optimization experiments which are often not fully acknowledged in the papers.
for example in the hyperparameters fine tuning optimization experiment is mostly absent.
the paper briefly mentions the range of hyperparameters and gives some examples but the listing is not exhaustive so in the end it is not known what factors were explored.
on the negative side of experiments define their factors properly but their treatment definitions are incomplete .
for example the optimization experiment in mentions that the hyperparemeters to be fine tuned are embedding size number of hidden states batch size maximum number of iterations optimizer learning rate beam size and lambda.
however it does not specify the range of values that have been explored.
in the regularization term the number of iterations or the topology of the proposed dnn are not reported.
in the experiment in the treatments are defined at the architectural level a deep adaptation network is compared against a deep belief network a lstm and a cnn .
however specific implementations of these architectures are being compared overlooking the fact that other non specified variables like the model hyperparameters the dl algorithm or the data representation could be the underlying causes for the performance gain and not the architecture.
this issue gets magnified as the limitations propagate across papers.
for example has an ambitious agenda to compare the proposed technique against three other state of the art approaches but none of them are easily and reliably available.
for one of them the stable version in github is available but it may be different from the one in the paper according to our results from section .
for another the authors of the paper had to resort to reimplement the approach following the original paper where it is proposed which may implement a different technique.
for the third one the performance numbers reported in the original paper are used but even if the training has ben imitated those may have suffered from extraneous variables that are unstated.
the limited availability of high quality artifacts remains an ongoing challenge.
finally of experiments had all factors and treatments fully operationalized .
for example paper specifies that the factors are word2vec vector length with values learning rate with values .
.
.
and epoch size with values .
.
characterization of experiments and implications we now proceed to analyze the distribution of experiments implications to better understand how often they occur and what combinations are the most common.
we utilize the parallel categories diagram in figure to facilitate the exposition.
the sets of nodes being considered are associated with the three implication types analyzed in the previous sections.
that is for relationship exploration conclusion validity we consider comparisons among single values means and inferential for causality internal validity we consider when none classical and other sources of randomness are controlled for construct validity we consider when none factors or both treatments and factors are defined.
in the figure the nodes on the left correspond to comparisons the ones on the center to causality and the ones on the right to constructs issues.
we have not found any experiment that properly addresses all types of validity threats discussed under the implications.
the best conducted experiments of the ones examined perform inferential analysis control classical sources of randomness and specify factors.
the majority of experiments however have at least one critical issue either compare single values do not control any single source of randomness or specify neither factors nor treatments .
even though most experiments specify at least factors they perform comparisons based on single values and or do not control any variables .pitfalls in experiments with dnn4se an analysis of the state of the practice esec fse december san francisco usa figure distribution of experiments implications.
validity threats we briefly discuss the main limitations arising from the scope design and implementation of our study .
theexternal validity of our study is determined by the eligibility criteria we chose.
we concentrated on the top conferences and journals section .
with the expectation that the findings would constitute an upper bound for the average quality of experiments appearing in other venues.
the time period covered allowed us to determine the status of recent research in the topic papers were not examined as the search was performed in and the analysis was conducted during .
given the number of relevant papers in we randomly selected a subset to be examined.
the same quality driven and cost control reasoning applies for us to target the artifacts with an acm badge section .
.
internal validity .
our source of information section .
to identify the papers scopus included the chosen venues in the time period covered this reduced the possibility of omitting potentially relevant studies.
the search strategy we followed is also repeatable.
the paper selection process required for each paper to be examined by one of the two authors however joint checks and discussion of papers that did not fit in existing filtering criteria reduced the chances of missing potentially relevant papers.
due to the high data extraction costs from papers the data collection process section .
and section .
considered all icse papers which were jointly examined by both authors to ensure that the collection strategies and results were aligned while the remaining papers were examined by just one author.
again joint checks and discussions of studies that did not fit the schema introduced new ml elements or had ambiguous specifications reduced possible researcher bias.
finally a critical appraisal of individual sources of evidence we note that analyzing papers was challenging given the diversity of presentation styles the number and complexity of the variables to check and the increasing richness of the dnn domain.
furthermore analyzing artifacts was a consistently arduous re engineering process.
the nature and magnitude of these analyses may have introduced errors in our measures.
we attempted to control these internal threats by sharing all the intermediate results of the study with the community.
construct validity .
the characterization schema data items defined in section .
was specifically developed for this research.
we created it starting from the steps of the experimental process and the aspects of the experiments that have to be covered duringeach step.
beginning with the generic definitions given by the experimental software engineering literature the authors iteratively and systematically tailored it to the dl domain.
we believe this provides a reasonable operationalization one that is transparent as well for others to assess refine and reuse.
a simpler assessment would just analyze the validity threats reported by the papers.
however the description of threats is typically ad hoc and often incomplete .
for this reason we decided to assess the validity of the experiments from their description and code artifacts in some cases .
the syntheses of results made in section .
and section .
allow identifying the validity level of the results reported in the studies.
recommendations failing to address the limitations we identified in the state of the practice could undermine much of the research devoted to dnn4se.
thus we propose three actionable recommendations that have the potential to address most of the pressing concerns we discovered.
rec perform multiple dnn training runs to control for randomness.
experiments must strive to control the randomness of the dnn training process.
this process can introduce various sources of randomness and a fundamental one is the random data selection and shuffling that occurs iteratively to compute the gradient over the dnn which means that the resulting values may change over different runs.
yet none of the papers reported to make multiple training runs to control for this intrinsic source of dnn randomness.
this raises questions about whether most results are caused by just a fortuitous or unfortunate sample selection while searching for the gradient.
there are other sources of randomness to consider e.g.
the initialization weights the data splitting but based on our findings we argue that simply conducting multiple runs of the dnn training process would enable the control of a sizable portion of the randomness we observed.
furthermore given the size of the experiments we analyzed and the magnitude of free computing resources available we found no compelling argument for not running an experiment multiple times to account for the randomness in the dnn training process.
this recommendation could benefit almost all experiments we analyzed.
rec compute statistics over multiple runs and data partitions.
experiments are meant to establish relationships between factors and response variables.
our analysis however found that of experiments identified a relationship based on single observations.
some of those studies had multiple observations gathered over multiple units of analysis i.e.
projects releases apps in such cases it is difficult to justify why select a single data point to compare treatments.
for the rest of the cases however there are plenty of opportunities to collect multiple observations.
for instance recommendation rec for conducting multiple training runs will enable the collection of multiple observations.
a second easily accessible source of observations for most of the papers we analyzed are the multiple partitions of the dataset used as part of the training.
given the number of sources of randomness in dnn training rendering multiple observations we find no compelling reason not to require at least a comparison of means from such observations and if there are enough observations computer inferential statistics to judge whether the results generalize from theesec fse december san francisco usa sira vegas and sebastian elbaum sample to the population.
this recommendations could benefit from to of the experiments we analyzed.
rec specify dnn training parameters treatment space and check for paper consistency against artifact.
we have already described the large dnn configuration space and how different instantiations of it can dramatically impact the performance of dnn4se techniques.
yet most papers fail to provide a specification of even some of the basic dnn parameters in that space.
that lack can be mitigated by artifacts with code implementing the dnn training process.
however re engineering the experimental design from such artifacts puts an undue load on the reader and it is fault prone we have done of them to attest to that!
.
furthermore some experiments are often missing in the artifacts and it is common to find inconsistencies between papers and their corresponding artifacts.
we recommend that papers shall provide a tabular description of the dnn configuration space explored for each experiment as we have done for each experiment analyzed see appendix for samples .
we also recommend for the adoption of ml experiment management tools e.g.
jupiter mlflow dvc to track the dnn experiments how they evolve and also to control how they are shared in the papers and in the artifacts to facilitate the detection of inconsistencies.
this recommendation could benefit of the analyzed experiments.
deploying mediums.
the previous recommendations can be implemented through different mediums.
they can go directly to authors as part of a call for papers checklists be integrated as a part of the artifact verification process be provided to reviewers to help them judge a paper soundness and verifiability become part of broader guidelines such as the recently introduced empirical processes guidelines or serve as instructions for newcomers to the area.
given the increasing number of dnn4se papers the trend from table indicates that they are likely to become a dominant research thrust in the venues we studied for years to come and the pitfalls we observed and quantified pursuing several of these mediums seems warranted.
periodic checks of dnn4se paper experiments.
quantifications and reflections of where we stand as a community like we have completed here are an essential measurement stick to judge progress.
given the issues we found and the nature of dnn4se that includes rapidly evolving technology researchers and methodologies follow up checks seem required to at least determine the trends over the concerns.
to reduce the cost of such checks the framework we have defined in our evaluation could be reused and a smaller sample of the yearly experiments could be analyzed.
related work a series of studies have analyzed the quality of se experiments .
table shows the number of papers examined the period covered whether all or just a subset of papers are examined the population the papers belong to selected journals and or conferences particular conferences or all the sampling performed exhaustive or random the type of experiments included human oriented technology oriented or both and the quality aspect under examination for each of those studies.
these studies differ primarily from ours in that they focus mostly on a single quality aspect at a high level of abstraction that is common across multiple softwaretable studies analyzing the quality of se experiments.
study size period population sample type aspect selected js cs e both statistical power selected js cs e both effect size selected js cs e both theory all r both researcher and publication bias icse r both correctness of analysis selected js e human construct validity engineering domains while we performed a deeper specialized analysis on more quality aspects but focused on a single domain.
to improve the quality of experiments the se community has developed an extensive body of knowledge some of which has resulted in guidelines for running and reporting experiments.
some of the guidelines are general enough to apply to any se experiment and therefore served as a starting point to characterize our experiments section .
however such general guidelines do not address the specific challenges associated with experiments in the dnn4se domain which is rapidly evolving and acquiring a critical momentum in the se community.
other guidelines are specific .
for example there are domain specific guidelines for the analysis of randomized testing algorithms for addressing the diversity of the projects from which to get the dataset to be used in msr studies and there are guidelines that are specific to conducting human based experiments or to perform benchmarking .
again although helpful they are not addressing specific concerns raised when conducting experiments in the dnn domain.
this is the first paper that characterizes the state of the practice in dnn4se experimentation.
conclusions the se community is increasingly developing techniques based on dnns to solve software engineering problems.
performing experiments to assess such techniques is challenging given dnns inherent complexity involving many subtle and interdependent training variables sources of randomness and rapid technological evolution.
our examination of experiments in papers is the first to quantify these challenges.
we find that of experiments are missing inferential statistics and are missing even basic descriptive statistics are not stating the experimental factors and only do so partially and do not specify even the basic elements of the experimental design to control any source of randomness while the rest only control for the classical sources of randomness.
these findings trends only mildly change when artifacts are provided as part of such experiments and what is more concerning is that we find that most artifacts are not fully consistent with their corresponding paper.
these findings are problematic because they imply that there is weak support to determine that there is a true relationship between independent and dependent variables that did not take place by happenstance there is limited control over the space of dnn relevant variables which can render a relationship between dependent variables and treatments that may not be causal but rather correlational and there is a lack of specificity in terms of what are the dnn variables and their values utilized in the experimentspitfalls in experiments with dnn4se an analysis of the state of the practice esec fse december san francisco usa to define the treatments being applied which makes it unclear whether the techniques designed are the ones being assessed.
we have proposed a series of actionable recommendations addressing the most critical findings we uncovered and will push forward to have them become a part of our community practices.
data availability the data of our analyses is currently in an anonymous repository.