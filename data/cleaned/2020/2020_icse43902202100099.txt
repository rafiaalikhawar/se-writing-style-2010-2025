white box performance influence models a profiling and learning approach max weber leipzig university germanysven apel saarland university saarland informatics campus germanynorbert siegmund leipzig university germany abstract many modern software systems are highly configurable allowing the user to tune them for performance and more.
current performance modeling approaches aim at finding performance optimal configurations by building performance models in a black box manner.
while these models provide accurate estimates they cannot pinpoint causes of observed performance behavior to specific code regions.
this does not only hinder system understanding but it also complicates tracing the influence of configuration options to individual methods.
we propose a white box approach that models configurationdependent performance behavior at the method level.
this allows us to predict the influence of configuration decisions on individual methods supporting system understanding and performance debugging.
the approach consists of two steps first we use a coarse grained profiler and learn performance influence models for all methods potentially identifying some methods that are highly configuration and performance sensitive causing inaccurate predictions.
second we re measure these methods with a fine grained profiler and learn more accurate models at higher cost though.
by means of real world j ava software systems we demonstrate that our approach can efficiently identify configuration relevant methods and learn accurate performanceinfluence models.
index terms configuration management performance software variability software product lines i. i ntroduction many software systems today are configurable supporting multiple application scenarios hardware platforms and software stacks.
configuration options are used to tailor a system s behavior and its non functional properties by de activating or tuning corresponding code.
performance measures such as response time and throughput are among the most important non functional properties of software systems .
so it is crucial to know how individual configuration decisions will influence a system s performance.
several approaches of accurately modeling and learning the performance behavior of configurable systems have been proposed in the literature .
the underlying idea is to sample a set of configurations from the configuration space and measure their performance.
machine learning techniques such as multi variable regression or classification and regression trees can be used to learn a performance influence model from these measurements to accurately predict the performance of unseen configurations or to find performance optimal configurations with search based techniques .
system runtime b e c e c bcbe cm3 m1 m2 e cbc performancesystem runtimesystem level performanceinfluence modelsmethod level performanceinfluence models performancesystem methods options which methods are relevant?fig.
comparison of the black box performance influence modeling at system level left and white box performanceinfluence modeling at method level right explaining how feature values at system level are composed of feature values at method level.
these and similar approaches based on parameter tuning and algorithm selection have in common that they conceive the configurable software system as a black box.
that is they model the performance of a software system as a function of its configuration i.e.
a set of selected configuration options without knowledge of the software s internals.
for illustration let us consider a database system with three features base b encryption e compression c and e c interaction between eandc as illustrated in figure on the left side.
from such a system level model we can infer the most influential options e.g.
compression and possible interactions.
however we have no information about the root cause of interactions or influential options.
we do not know where in the system s code base we spent execution time depending on the configuration.
essentially developers want information at the level of individual methods as illustrated in figure at right hand side.
knowing performance influences at the method level helps detecting performance bottlenecks pinpointing performance bugs or assigning performance tests in a ci pipeline to specific configurations.
this is a developer s perspective which is not supported by current black box approaches.
from a performance analysis perspective there are monitoring and profiling techniques whose goal is to identify performance hot spots .
specialized performance engineers typically supervise performance of web and cloud applications or identify bottlenecks using stress tests .
unfortunately state of the art approaches in this area ieee acm 43rd international conference on software engineering icse .
european union usually disregard the fact that today s software systems have huge configuration spaces.
typically only a single configuration of a system is considered which is insufficient since performance bugs and related issues are often configurationdependent .
few approaches aim at creating white box performance models of configurable systems but they either require explicit tracing information about which code regions are affected by which configuration options or they rely on expensive and potentially imprecise static and dynamic program analysis .
our goal is to devise a white box performance analysis technique for configurable software systems that with high precision and at low cost infers which methods are most affected by configuration decisions and which source code regions exhibit the highest configuration dependent performance variation.
our approach substantially widens the application scenarios of former black box approaches and introduces the concept of configurability and performance predictions to current white box approaches .
the right part of figure shows the essence of the approach it creates a performance influence model for each method of the system.
to realize our goal we combine approaches from two fields profiling program behavior in a white box manner and predictive modeling of performance of configurable software systems.
we use a two step approach to direct the profiling activities to methods that are performance relevant i.e.
contribute to a system s performance and that are highly affected by configuration options.
this way we substantially reduce the influence of measurement overhead thereby increasing prediction accuracy.
in a nutshell we draw samples from the configuration space based on well established sampling strategies and measure the selected system configuration with a low overhead profiling tool.
we use classification and regression trees cart to learn a performance influence model per method based on the measured performance data.
in this course we identify methods that exhibit high performance variance possibly caused by configuration options.
in a second step we re measure all methods with high variance to improve the accuracy of the final performance models this way focusing on the difficult to learn methods.
here is the key methods are affected to different extents by configuration options and only for a fraction of methods a more finegrained performance analysis is required.
we use machine learning to find those methods.
focusing on methods with high performance variation we need to distinguish three causes configuration variance measurement variance and a method s context variance.
configuration variance emerges from the de selection of configuration options.
this is what we aim to learn as influences of options and interactions on individual methods.
measurement variance corresponds to the measurement setup s inherent systematic bias.
context variance represents the performance variation of a method execution due to varying input parameters or program states.
so given these three types of variance a method s performance can vary for each run of a softwaresystem even with the same configuration.
to learn accurate performance models for configurable software systems we need to measure distinguish and control sources for all three types of variance.
to provide a robust empirical foundation as a base line for evaluation we measured the performance of software systems from various application domains resulting in years of cpu time of continuous measurements.
we demonstrate that our approach can efficiently identify configurationrelevant methods and learn accurate performance models at the method level.
we contribute and evaluate not only an approach for learning white box performance models but also important empirical findings about the distribution and nature of configurationdependent performance relevant methods.
we contribute an analysis that reveals the influence of the types of variance on the runtime of methods.
these findings shall inform further work on tailoring and guiding sampling techniques static code analyses and performance anomaly detection.
to summarize we make the following contributions an approach to learn white box performance influence models of configurable software systems at method level using profiling and prediction modeling a performance analysis providing insights into statistical performance properties of methods related to configuration decisions an evaluation of our approach with respect to prediction accuracy and scalability for real world software systems a replication package including our implementation and measurements1.
ii.
p reliminaries and related work a black box performance influence model does not contain information to map performance to source code.
yet there exists substantial work conducting root cause analysis trying to locate for example performance bugs or memory leaks.
however prior work rarely takes configurability into account.
a. white box performance analysis there are only few approaches that tackle performance analysis for configurable software systems.
reisner et al.
and meinicke et al.
use symbolic and variational execution to analyze the behavior of interactions of configuration options at the level of control and data flow.
they found that software systems in practice do have a much smaller relevant configuration space than theoretically possible because only a few options interact at all.
many approaches in this area rely on this fact.
hoffmann et al.
use dynamic influence tracing to convert static parameters to dynamic control variables global variables with the goal of adapting properties of an application.
they do not consider interactions among parameters and 1the supplementary material can be found at or an archived version at 2e61f8ce57498194c2af0cd76e87498a174f07fa 1060cannot pinpoint code regions of interest.
their approach works only when static parameters are convertible whereas our approach is agnostic to the type of configuration parameters.
family based performance measurement aims at applying family based analysis to performance analysis.
the idea is to create a variant simulator which converts compiletime variability into run time variability .
then the variant simulator is executed incorporating variability constraints.
this way multiple variants can be executed and measured in a single run by informing the analysis which method s performance is configuration specific.
on the downside familybased performance measurement requires the construction of a variant simulator or other variational representation which is in general a non trivial task .
lillack et al.
use taint analysis to identify which code fragments are executed depending on which configuration options.
this static code analysis technique is used in configcrusher for deriving performance models .
specifically configcrusher employs static data flow analysis to identify code regions whose performance is likely to be influenced by configuration decisions.
it traces individual configuration options represented by program variables following the call graph and taints code regions influenced by configuration options or combinations thereof.
subsequently configcrusher merges regions to larger ones such that it can efficiently measure performance of each individual region.
performance is measured by weaving instructions into the byte code representation of the target program at the beginning and end of each region.
in contrast to our approach configcrusher requires modifying the source code of the target system to make the taint analysis run.
beside integrating an interface between analysis and target system further substantial code refactorings are required to achieve scalability and precision .
the background is that configuration options are often stored in complex data structures lists maps structs etc.
causing the taint analysis to no longer differentiate among the options stored in the data structures.
the result is that either all accesses to the data structure are tainted with all configuration options or the analysis stops tainting at this point.
the first variant leads to memory explosion and timeouts the second results in incomplete and possibly very short taints rendering the resulting performance models inaccurate and effectively useless.
to circumvent this problem one can always refactor the entire system such that configuration options are stored in individual variables which is usually infeasible in practice.
finally in contrast to our approach configcrusher does not support numeric configuration options and is limited to singlethreaded applications.
velez et al.
proposed c omprex a tool that builds white box performance models based on dynamic taint analysis and local performance measurement.
c omprex requires expensive dynamic analysis and focuses only on configuration specific code whereas our approach covers the whole code base by building a model for each method of the system.b.
black box performance analysis obtaining accurate performance models requires a series of measurements.
conducting measurements for each possible configuration however is infeasible due to the combinatorial complexity of the problem.
instead sampling a representative subset of configurations can achieve high accuracy.
for configurable software systems there are various sampling strategies that are suitable for learning performance models random sampling solver based sampling coverage based sampling and distance based sampling .
courtois and woodside use regression splines to model the black box performance behavior of a software system without taking configuration options into account.
in the same vein israr et al.
and mizan and franks obtain performance models at a coarser granularity using layered queuing networks without considering variability though.
instead they produce models that provide event sequences for distributed systems.
westermann et al.
aim at finding optimal software configurations with performance modeling at the blackbox level.
however they consider only little variability.
also krogmann et al.
build parameterized performance models at the component level.
that is they build black box models for components and do not address fine grained possibly cross cutting configuration options among several components.
ackermann et al.
propose an approach to automatically find a suitable machine learning technique to learn blackbox performance models using monitoring data.
grohmann et al.
use feature selection in the context of machine learning to obtain black box performance dependencies.
in contrast to these approaches we use profiling information to build performance models at the method level which is one of the main challenges and novelties of our approach.
siegmund et al.
propose splconqueror an approach to construct performance influence models as linear functions over binary and numeric configuration options or more complex combinations thereof .
the key is to combine binary and numeric sampling and to symbolically learn the influence model in an iterative manner.
other approaches propose learning techniques based on classification and regression trees and spectral learning or even learn when to stop the learning procedure .
our approach takes advantage of these modeling capabilities to pinpoint performance properties but at the method level.
nair et al.
reduce the number of measurements by iteratively measuring and only adding configurations that improve accuracy the most.
another approach by nair et al.
explores the configuration space by clustering therefore requiring measurements of only few representative configurations per cluster.
this way the sampling procedure can be directed to unveil performance influences and near optimal configurations efficiently.
however all these approaches consider a software system as a black box not allowing for pinpointing performance behavior and root causes in code.
1061c.
profiling profiling refers to the white box analysis of the run time behavior of a program execution with respect to memory consumption or execution time .
by contrast measuring execution time of a system as a whole refers to black box performance measurement.
mytkowic et al.
analyzed the accuracy of java profilers by comparing four commonly used profilers regarding their agreement on which methods are performance critical.
the profilers reported different sets of methods as hot spots.
reasons for the disagreement are implementation details of the profilers e.g.
whether native methods are treated as part of the program and the measurement overhead of the profiler observer effect .
some profiling approaches aim at automatically finding specific inefficient structures in the source code.
song and lu designed ld octor and selakovic et al.
designed d ecision prof. both tools search for redundant loops and optimization opportunities in the order of evaluating expressions.
they focus only on specific code structures but provide also suggestions for improvements.
there are many other approaches of how to profile different properties of software systems.
however these approaches do not consider configuration dependent performance variation.
still profilers have been shown an important tool in industry to debug software systems with respect to resource usage.
we do not want to replace but build on industry strength profilers i.e.
jp rofiler to reach our goal.
iii.
u ntangling performance variance at the method level learning performance models is a task that is highly sensitive to measurement bias.
as execution times can be short influences of concurrent processes can easily distort measurements.
in the context of configurability and method context many sources contribute to the overall variation in performance.
to use machine learning effectively on measurement data we need to quantify possible sources of performance variance and adjust our model accordingly.
to this end we conducted an analysis of possible causes of performance variance measurement variance configuration variance and context variance.
to devise an approach for learning method level performance models it is necessary to know how the variance in the execution time is composed and how to control the three contributing factors to pin down the influence of configuration options on performance.
for illustration figure shows the execution time of a single method executed in three different configurations repeated three times each.
each histogram shows the distribution of the execution time of all calls to that method.
measurement variance becomes visible when comparing rows the performance distribution of a row s plots should not change since the measurement setup is the same.
that is any change here can only be caused by the measurement process e.g.
overhead or measurement environment e.g.
context switches .
context variance is represented by the shape of the histogram.
that is for different contexts during a 050100configuration configuration 2repetition 1configuration method callsrepetition execution time ms 400repetition 3fig.
performance variances for different executions of method waituntilsynced of p revayler .
columns represent different configurations configuration variance rows represent different repetitions of a program execution measurement variance and each cell depicts the performance distribution of multiple executions of the same method in a single program run context variance .
single program run we might call the method with different parameters different cache states etc.
leading to different execution times.
so a histogram shows the distribution of execution times for a single method in a single program run.
finally configuration variance manifests as differences among plots of different columns.
if the plots differ across the columns the root cause of the performance changes are due to changes in the system s configuration.
in what follows we provide an in depth analysis of three real world subject systems c atena h2 and p revayler see section v c for more details regarding the three sources of variations.
the goal is to obtain insights into which variance needs to be controlled when learning performance influence models at method level.
this helps us in devising sensible means for sampling measuring and learning instead of blindly applying an off the shelf machine learning approach.
a. measurement variance measurement variance affects the accuracy with which we get stable results while repeating experiments.
high measurement variance adversely affects the accuracy of performanceinfluence models.
therefore it is crucial to estimate measurement variance with a sufficient number of experiment repetitions.
the aim of our analysis is to determine the number of repetitions needed to trust the estimated measurement variance.
our analysis setup is as follows we profile a given configurable software system with repetitions and report the coefficient of variation c v represents the standard deviation of all method executions the mean of all method executions in a single run as a standardized measure of dispersion of a probability distribution to quantify the stability of measurement results .
to check whether measurement variance is independent of configuration variance we ran10620 measurement repetitions0102030coefficient of variationcatena h2 prevaylerfig.
measurement variance with increasing number of repetitions.
the dashed red line denotes the maximal measurement variance .
domly select configurations for each of the three software systems.
to better visualize the effect of repeating experiments we compute the coefficient of variation after each repetition and show it for our three configurable software systems in figure .
for all three systems the variance is below .
this indicates a reliable measurement setup and the need for only a limited number of repetitions insight as the coefficient remains stable already around three repetitions.
moreover the coefficient is equal for all configurations so configurations have no effect on measurement bias.
hence we can neglect possible hidden dependencies here.
b. configuration variance configuration variance captures the variation in a method s execution time due to selecting different configuration options.
our analysis setup is as follows we profile a given configurable software system by measuring the execution time of each method.
we aggregate the execution time per method and repeat this process five times to account for measurement bias.
we repeat this process for different configurations.
again we use the coefficient of variation per method as a measure to determine whether methods have a constant average execution time across different configurations.
if the coefficient of variation is higher than the measurement variance for all of our subject systems see section iii a the method s execution time is configuration dependent.
figure depicts the coefficient of variation y axis for each method x axis ordered from low variation to high variation.
we observe that a large number of methods have only limited variance insight and only few methods exhibit high performance variance insight .
interestingly the variance for these few methods is huge and the percentage of methods affected by configuration decisions is also sensitive to the software system.
from this analysis we infer that method level performance influence models should concentrate on these highly varying methods profiling all methods of the program would be wasteful.
this is good news as profiling is usually expensive and affects measurement results.
hence we conclude that an efficient and accurate learning approach would methods in percent0102030coeficient of variationcatena h2 prevaylerfig.
configuration variance of the methods per subject system sorted by their coefficient of variation.
the dashed red line denotes the maximal measurement variance such that all method execution times above the line change due to configuration decisions.
first need to find the relevant methods and then concentrate learning these.
c. context variance context variance of a method s execution time originates from changes in the method s calling context e.g.
method parameters and cache state .
in figure we visualize context variance as a histogram of performance values for each method execution in a single program run.
we observe that the execution time of some methods remains constant during a program run whereas other methods show high variance.
overall we observe highly skewed execution times that heavily affect a method s average execution time insight .
that is we observe few but very large outliers which are several orders of magnitude slower than about of the other method executions.
this resembles a cauchy distribution with no defined mean and standard deviation for these methods .
the problem for learning is that the cauchy distribution is a well known case where maximum likelihood estimation fails and subsequently the likelihood principle in general .
so this can cause highly unreliable performance models.
finally we also see an interaction between context and configuration variance when analyzing figure not only the execution times vary but also the number of method executions insight so an algorithm that takes only the average execution time of a method in to account for learning the influence of options is doomed to fail.
instead an accurate approach needs to account for the number of method calls in relation to their execution time.
d. summary from our analysis of variance we can learn two important things first we see that the distribution of method executions changes for different configurations.
that is configuration influences a method s context causing a variation in the method s execution time.
second based on our insight that some methods performance values are cauchy distributed we cannot resort to a sample based profiling technique but .
.
.
.
.
.
execution time ms method calls99 mass execution time ms massfig.
context variance of method rotr64 right the longest running method executions left the remaining method executions.
rather need to tap the entire performance distribution using an instrumentation based approach insight .
this is necessary as there might be large outliers that could skew the average method execution time substantially and would need to be filtered out.
this is hardly possible with sample based profiling.
iv.
p erformance influence modeling at the method level our goal is to learn performance influence models at the method level so that we can pinpoint methods with high configuration dependent performance variability and identify code regions that cause performance interactions.
our methodlevel performance influence modeling approach builds on the insights that we gained from our variance analysis and is separated into two steps as illustrated in figure coarsegrained analysis and fine grained analysis.
we know that the execution time of only few methods varies for different configurations i nsight so we aim at identifying exactly these methods in the first step.
for this purpose we use a light weight coarse grained profiler jprofiler to obtain performance measures for all methods under different configurations using an established sampling approach.
then we extract those methods that exhibit i a performance relevant execution time e.g.
we filter out getter setter methods and ii a performance variation across different configurations.
in the second step we instrument the source code i nsight using the tool k ieker to obtain an execution time distribution an execution time for each method call .
we filter from this distribution long running outliers insight and summarize the distribution as a histogram insight .
finally we learn one performance influence model per method based on these fine grained values.
a. sampling configurations as a prerequisite to step we sample a set cfrom the set cof all valid configurations.
there is a substantial corpus of approaches that successfully applied different sampling strategies to obtain a representative set of configurations .
following previous work we opt for featurewise and pair wise sampling for binary and plackett burmansampling for numeric options but other sampling strategies might be appropriate as well.
with feature wise sampling we obtain a set of configurations in which each option is enabled once.
with pair wise sampling also called t wise sampling with t this set is enriched by all pair wise combinations of configuration options.
we use the extended plackett burman design proposed by wang and wu for sampling numeric configuration options.
compared to binary options adding a numeric option withndifferent values increases the configuration space by factorninstead of factor .
the plackett burmann design selects a fixed set of configurations determined by a pre chosen seed which strongly reduces the effect of the combinatorial explosion.
b. coarse grained profiling our approach automatically runs a given software system for each configuration of the sample set denoted as run with jprofiler2 a coarse grained profiler that uses the jvmti interface of the jvm.
for each run we obtain the absolute execution time and the number of calls for each method.
we repeat each run five times and report the mean time to account for measurement bias i nsight .
next we learn a performance influence model per method m2m wheremis the set of all methods of a software system from these measurements using classification and regression trees cart as the learning method .
in the case that all methods have been learned accurately there is no need to continue with the second step.
however we have seen in our variance analysis that typically some methods are highly sensitive to context variance i nsight 4and which makes a second learning step necessary.
c. filtering to identify methods that are hard to learn and that contribute substantially to a system s performance i nsight we apply a filter to all methods mof a system obtaining a subset mhard mfor further measurement and learning cf.
eq.
.
the filter relies on a predicate m with 2r cf.
eq.
which states whether a given method m2m belongs to the set of performance relevant methods.
mhard mjm2m m err m abs m rel m m err m evaluates whether the error of the corresponding method s performance model exceeds the given threshold cf.
eq.
.
all methods that have been learned with a prediction error mean absolute percentage error mape eq.
of or worse get selected.
for this purpose we compare the measured performance for method mof configuration c denoted with c m with the performance c m predicted with the model of step .
following siegmund et al.
we 1064profiling coarse grained performance modeling profiling fine grained performance modelingconfigurable software system filter methods filter outliersampling all methodsfig.
method level white box modeling pipeline for configurable software systems fix to in our experiments which represents an already strict filter criterion just above the measurement bias cf.
section iii a .
increasing decreases the number of methods that have to be analyzed further but this way more inaccurate performance models are accepted.
err m mape m mape m j cj x c2 c c m c m c m abs m eq.
evaluates whether a method s execution time is longer than .
a method s execution time here is defined as the accumulated execution times over a run eq.
.
by setting we control to which extent we want to invest measurement effort for short running methods.
abs m absperf m absperf m j cj x c2 c c m rel m eq.
evaluates whether a method has a relative run time eq.
of more than in relation to the accumulated black box time of the overall software system sum of performance values of all methods eq.
.
adjusting enables us to focus on methods that contribute the most to the overall performance of the system.
rel m relperf m relperf m j cj x c2 c c m blackboxperf c blackboxperf c x m2m c m to sum up predicate selects methods that have been inaccurately learned have a total run time of at least and contribute to the overall software s performance by at least percent.
d. fine grained profiling and learning we use k ieker an aspect oriented j ava performance profiling tool to measure the methods mhard obtained fromthe filtering step.
to analyze the variation across all calls of a method we extended k ieker by logging the assignment of values to method arguments of each method call together with the measured execution time.
profiling with k ieker involves three steps first we include an annotation pointcut into the source code of the subject system at the beginning of each selected method.
second we compile the software system into an executable.
third we execute the software with k ieker as jvm argument java agent to weave the monitoring code advice around method executions.
we run our experiments with the same set of configurations and workloads as used in the coarse grained profiling phase obtaining performance data per method execution of the relevant methods.
based on our variance analysis we filter outliers that make up of the longest execution times and we learn new models with the remaining aggregated data using cart.
v. e xperiment setup in this section we present the measurement setup that we use for profiling as well as the software systems that we selected for evaluation.
a. measurement setup all measurements ran on a cluster of computers each of which has an intel quad core processor an ssd running a headless operating system ubuntu .
.
lts an hdd to store experiment data and or 16gb of ram.
b. measurement procedure for each subject system we generate two sample sets for learning according to the two sampling strategies of section iv a as shown in table i. for each learning set we measure the runtime with our two step approach learning a model per method.
we sample an additional test set of fresh configurations at random.
we use the test set to evaluate the prediction error of the models learned based on the learning sets.
furthermore we repeat all measurements five times which results in measurement runs rq 1and rq2 .
additionally we measure the black box execution time of all configurations to determine the execution time without profiling rq .
3for a single software system we conducted all measurements either on the systems with or 16gb memory.
1065table i overview of subject systems.
jfjdenotes the number of configuration options binary and numeric jcj denotes the number of the valid configurations c j cfwj denotes the number of configurations sampled feature wise j cpwjdenotes the number of configurations sampled in a pair wise manner.
system domain jfj jcj j cfwj j cpwj batik svg rasterizer catena password hashing cpd copy paste detector dc image density converter h2 database kanzi data compression pmd source code analyzer prevayler database sunflow rendering engine n a c. subject systems we evaluate our approach with real world software systems.
our selection includes configurable j ava applications covering different domains including databases rendering engines and static code analyzers.
our selection was driven by covering a diverse set of domains having memory and cpu intensive tasks and providing configuration options that affect performance of the system.
we provide an overview of the software systems in table i. next we present the systems and benchmark workloads.
when possible we reused existing workloads provided by the respective software systems.
the b atik rasterizer converts svg files to a raster format.
as workload we used the d acapo benchmark suite which contains a set of svg images of different sizes that can be used for performance tests.
catena is a secure password scrambling framework that implements a corresponding hashing function.
as workload we used its password hashing benchmark.
with the provided configuration options it is possible to select one out of four graphs as well as different seeds and security values that influence how much main memory has to be used to encrypt or decrypt a password and how long this process takes.
cpdis a code duplication detector.
it detects duplicate source code sections to support developers with code refactoring.
as a benchmark workload we detect code duplicates in c atena s code base.
density converter dc is an image density converter that given an image or folder converts these inputs into image formats with different resolutions.
as workload we used a set of high resolution images provided by the developers.
h2 is an open source relational database system that can operate both in an embedded and a client server setting.
as workload we use the subset of tests of the p oleposition benchmark with which developers compare h2 to other database applications.
4we provide all benchmarks on the supplementary website jtnta or an archived version at revision 2e61f8ce57498194c2af0cd76e87498a174f07fa kanzi is a lossless data compressor.
it provides various configuration options for composing and tuning the compression process.
as workload we used the s ilesia corpus benchmark5.
pmdis an extensible cross language static code analyzer that checks source code against a set of rules.
as a workload we selected all rules that try to identify performance violations in the system to analyze.
the system that we analyzed is prevayler .
prevayler is an open source object persistence library for j ava supporting in memory storage.
it provides a scalability and performance benchmark consisting of transactionprocessing and query scalability tests that are applied to a jdbc compatible database.
sunflow is an open source global illumination rendering system.
it provides a selection of example scenes objects to illuminate and render of which we selected the golden scene a teapot in a colored room with one light source and pixels .
vi.
e valuation the goal of our approach is to pin down the influence of configuration options on individual methods.
in our evaluation we address three research questions rq1 can we learn accurate performance influence models at the method level?
previous work has shown that performance can be accurately modeled for a system as a whole.
however it is unclear whether this level of accuracy can be achieved when modeling performance at the method level due to measurement overhead and the various sources of variance that we have described in section iii.
rq2 how do system level and method level models compare in terms of information they provide?
knowing the influence of a configuration option at the system level is helpful for tuning a system s performance.
however identifying the root cause of the influence of features helps developers for example to spot performance bugs and to focus on specific performance tests in a ci pipeline.
rq3 what is the relation of the runtimes of profiled and unprofiled methods?
an important measure for validity is whether the actual unprofiled method execution time relates to the measured profiled method execution time.
profiling introduces overhead and therefore the models we learn may be biased.
with this question we aim at quantifying the extent of the profiling overhead.
as configcrusher is closest to our approach cf.
section ii it would be a natural candidate for comparison.
in section vi d we report on why a comparison is not feasible though.
5silesia corpus 1066batik catenacpddch2 kanzipmd prevaylersunflow101 100101102103104mape all methods m batik catenacpddch2 kanzipmd prevaylersunflowfiltered methods m hard fig.
error mape of method level performance influence models of all methods m and of the filtered methods mhard .
dashed red line denotes mape.
table ii method and correlation analysis.
jmjdenotes the total number of methods and jmhardjthe number of filtered methods.
configuration wise linear correlation lc and rank correlation rc of black box measurements bb using a coarse grained profiler cg and a fine grained profiler fg .
system jmj jm hardj bb vs. cg bb vs. fg lc rc lc rc batik .
.
.
.
catena .
.
.
.
cpd .
.
.
.
dc .
.
.
.
h2 .
.
.
.
kanzi .
.
.
.
pmd .
.
.
.
prevayler .
.
.
.
sunflow .
.
.
.
a. method level performance models rq a operationalization to answer rq we calculate the mape of the performance influence models of each method separately.
we follow the measurement procedure described in section v b. we assume an influence model to be sufficiently accurate if it predicts the execution time of the method in the test set with an average error below which is stricter than as used in previous work .
b results figure summarizes the results regarding rq1.
the plot on the left shows the prediction error distribution of all method level models based on data measured with the coarse grained profiler.
all methods that are below the dashed red marker prediction error can be accurately learned based on a single profiling run per configuration which makes up .
of all methods.
hence the err condition of our filter selects about of the methods for step .
when using the additional conditions with respect to performance relevance absand rel we obtain a set comprising of about of all methods.
table ii depicts for each system the set of filtered methods denoted as mhard.
applying the second step we are able to model nearly all of the remaining methods accurately as shown in figure on the right hand side.
from a total number of jmhardj methods across all systems only methods cannot be learnedaccurately in step .
a closer manual analysis of these methods revealed that they depend either heavily on thread dependent file io operations in the case of h2 or nested loops number of loops depends exponentially on an option that copy an internal state array width of array depends exponentially on a configuration option in the case of c atena .answering rq the coarse grained profiling step is able to learn models with a mape below for .
of all methods.
applying the fine grained profiler in a second step the mape is below for .
of the performancerelevant methods.
b. tracing option influences rq a operationalization to answer rq we focus on the importance of configuration options and interactions in whitebox models.
specifically we learn one random forest consisting of classification and regression trees at systemwide level as well as one random forest for each method and extract all options and interactions.
for this analysis and without loss of generality we concentrate on the two options or interactions per system that have the largest performance influence on the system determined by the black box model similar to the scenario sketched in figure .
we aim at identifying the root cause for high influences by analyzing all performance influence models at method level.
since there might be hundreds of methods per system we analyze only performance relevant methods whose total sum can explain of the system performance.
having determined influential options and methods we count those methods as root cause for which the configuration options or interactions we are interested in have an influence that exceeds the measurement error.
furthermore we sort the methods by the options influences revealing which method contributes most to an option s influence.
b results tracing the influence of an option from the system level to the method level uncovers the cause of its influence.
we present the results for c atena in figure .
there are some configuration options and interactions that have an high influence on performance compared to the others options have no relevant influence.
focusing on an option of interest the most important option in our example reveals that only a small portion out of of the methods contribute to the option s system wide performance influence.
these methods are responsible for more than of the system s performance.
this kind of information is not available in black box models.
it does not only help selecting important configuration options for guiding sampling and performance tuning but also identifying the small set of methods that causes possible performance bugs this way facilitating performance bug detection of configurable systems.answering rq white box performance influence models can successfully guide us to performance relevant methods that are dependent on influential configuration options and interactions.
1067fig.
overview of the influence of options and interactions on c atena s performance.
the background plot shows the distribution of all influential options and interactions influence greater then the measurement error .
the small inner plots focus on the interaction gamma garlic which is the most influential option interaction of the model.
the left plot shows the number of methods that contribute and do not contribute to the interaction s performance.
the right plot shows the performance portion of these methods.
c. profiled vs. unprofiled methods rq a operationalization to answer rq ideally we would need to compare the execution time between a profiled method and an unprofiled method.
since we cannot know an unprofiled method s execution time we use a proxy to infer the actual unprofiled method execution times.
for this purpose we consider the black box execution time of a system as the aggregation of all true method s execution times.
we compare this time against the aggregated i.e.
system wide predictions of whitebox performance models for all methods.
by repeating this process for all measured configurations we approximate the relation of our estimates to the actual method execution times.
we use two different indicators to quantify the relation pearson s correlation coefficient and spearman s rank correlation coefficient.
the former tests whether there is a linear dependency between the aggregated system wide execution time with and without profiling.
a high linear correlation would indicate that it is possible to infer the unprofiled execution time from the profiled execution time with a constant factor.
this way white box performance influence models could even provide a precise prediction of method performance running in operation without profiler .
a high linear correlation is unlikely though because the overhead during profiling increases while the number of profiled methods grows.
spearman s rank correlation coefficient tests whether the order between profiled and unprofiled execution times is preserved.
a high rank correlation indicates that fast configurations measured without profiling stay fast even if profiling is enabled.
this would mean that our approach has accurately learned the relative influences of configuration options per method.
b results in table ii we show the correlation between system wide unprofiled execution time compared to using the jprofiler configs time in sec 1012141618unprofiled configs time in sec kieker configs time in sec 1012141618unprofiled configs time in sec fig.
configuration wise execution time of s unflow blackbox measurements vs. profiling.
left using jprofiler right using k ieker .
different symbols visualized different numeric values of configuration option samples .
coarse grained profiler bb vs. cg and unprofiled execution time compared to using the fine grained profiler bb vs. fg .
as expected rank correlation is higher than linear correlation across all subject systems and for both profilers.
that is the fastest configurations remain the fastest independently of whether we use our learned models for predicting execution times or measuring execution time.
this is good news as this property has been shown the main tuning objective for configurable systems .
furthermore we can see that different subject systems exhibit different correlations.
some systems such as b atik prevayler and c atena exhibit even nearly perfect linear correlation.
for them the execution time depends strongly on the configuration for both types of experiments measuring with a profiler and measuring the overall execution time of a program.
there are also subject systems for which rank correlation is much higher than linear correlation d ensity converter c pd and p md.
there are two systems stand out with a generally low correlation sunflow and h2 which we analyze next.
figure shows the dependency between unprofiled execution time and aggregated execution time measured with a profiler per configuration for s unflow .
we concentrate on the measurement overhead as the main cause for a low correlation.
interestingly when highlighting configuration options we observe a strong pattern for the overhead.
there are multiple groups that follow a linear trend but with different slopes.
the determining factor for this slope is the configured value of the numeric configuration option samples .
this option is used as a seed for method calculatephotons that compute the global illumination of the scene as part of the rendering process.
by increasing the numeric value for this option the overhead increases disproportionally.
for h2 there is also a strong pattern again caused by a numeric configuration option analyze auto .
this is important for other studies in this area profiling overhead of configurable system depends on configuration options.
6more details on the supplementary website.
1068answering rq we observed a generally high rank correlation between system wide profiled and unprofiled performance demonstrating that the use of a profiler does not change the relative importance of configuration options and that white box models are able to reveal performancerelevant methods.
some show even a linear correlation.
a notable exception to this rule is a numeric option in sunflow which directly impacts the overhead introduced by the profiler.
d. comparison to config crusher closest to our approach is c onfig crusher .
the main difference to our approach is that c onfig crusher relies on static taint analysis with the goal of determining which code regions are affected by which configuration options to weave measurement code at according statements.
due to this conceptual difference we face both qualitative as well as technical challenges that render a comparison infeasible.
as explained in section ii a there are three ways of propagating taints through a program a taint every access to the data structure s that hold s the configuration options b stop tainting at this point and c rewrite the program such that all options are stored in individual variables and are accordingly accessed across the code base.
variant c is infeasible in practice and also not in our case as a substantial rewrite is not only impractical for larger systems such as h2 but this would also change the program structure such that a performance comparison has no longer a common ground.
for the purpose of comparison we tried variant a first but quickly run into timeouts and memory limitations due to the inherent limitations of static code analysis.
we communicated with c onfig crusher s main author who confirmed our findings.
in a second attempt we followed variant b. we again consulted the main author of c onfig crusher for guidance to avoid introducing bias and setting up the subject system consistently with the original approach.
as a result we obtained reasonable taints for d ensity converter .
for h2 the largest of our subject systems we ran into memory overflows for all analyzed configuration options.
the analysis of s unflow batik and p revayler produced tainted code regions of size which are basically useless.
the reason was that the configuration options are immediately stored in a data structure leading to a termination of the taint analysis.
we provide all analysis log files at our supplementary website.
in addition to the limitations of the taint analysis c onfig crusher can handle only binary configuration options whereas our approach can handle numeric options as we do in our evaluation .
furthermore c onfig crusher is capable of tainting only single threaded applications due to the underlying taint analysis.
our approach can produce performance models also for multi threaded applications such as h2.
e. threats to validity the selection of the profiler represents a threat to construct validity.
we mitigated this threat by a pre study not shown inthe paper where we evaluated several profilers.
jp rofiler is an industrial strength profiler with low overhead which turned out to be the best choice for the first phase.
however to obtain fine grained performance data we required more flexibility and opted for k ieker .
a threat to internal validity arises from the measurement overhead introduced by the profiler.
we reduce this threat by selecting a low overhead profiler for measurement in the coarse grained step and devoted a whole research question to analyze its influence.
the selection of subject systems threatens external validity.
although we cannot claim that we can learn white box models accurately for all j ava systems with proper profiling capabilities.
our results show that this is in principle possible for a large industry relevant branch of configurable software systems.
vii.
conclusion we have proposed an approach to learn white box performance influence models at the method level enabling tracing configuration effects from the system level to individual methods.
based on a pre study on software systems we analyzed possible causes of performance variance of method execution times to design an integrated profiling and learning approach.
we found that the majority of methods can be easily learned as they either do not contribute much to the system s overall performance or do not contribute to configuration variance.
based on these insights we have devised a two step approach in which we learn performance influence models for all methods of a software system using a cheap coarsegrained profiler in a first step and filter inaccurate and relevant methods to be measured and learned again with an expensive fine grained profiler in a second step.
we found that despite the overhead introduced by profiling the correlation between profiled and unprofiled method execution time is high and that white box models can accurately predict the profiled execution time.
more importantly we were able to show that performance models at the method level can be used to pinpoint the contribution of individual configuration options and interactions to individual methods helping developers to chase configuration related performance bugs or to focus performance testing on specific configurations.
acknowledgment apel s work has been supported by the german research foundation dfg under the contract ap .
siegmund s work has been supported by the dfg under the contracts si and si and by the german ministry of education and research bmbf 01is19059a and 01is18026b by funding the competence center for big data and ai scads.ai dresden leipzig .
we thank our reviewers for their thoughtful comments.
especially we thank miguel velez for his helpful comments on the specifics of the taint analysis and for supporting the set up of configcrusher for comparison.
1069references c. u. smith software performance engineering in performance evaluation of computer and communication systems.
springer pp.
.
m. woodside g. franks and d. c. petriu the future of software performance engineering in future of software engineering fose .
ieee pp.
.
n. siegmund a. von rhein and s. apel family based performance measurement in proc.
int.
conf.
generative programming and component engineering gpce .
acm pp.
.
n. siegmund a. grebhahn s. apel and c. k stner performanceinfluence models for highly configurable systems in proc.
europ.
software engineering conference and acm sigsoft symp.
foundations of software engineering esec fse .
acm pp.
.
j. guo k. czarnecki s. apel n. siegmund and a. w asowski variability aware performance prediction a statistical learning approach in proc.
int.
conf.
automated software engineering ase .
ieee pp.
.
j. guo d. yang n. siegmund s. apel a. sarkar p. valov k. czarnecki a. wasowski and h. yu data efficient performance learning for configurable systems empirical software engineering emse vol.
no.
pp.
.
n. siegmund m. rosenm ller c. k stner p. giarrusso s. apel and s. kolesnikov scalable prediction of non functional properties in software product lines footprint and memory consumption information and software technology ist vol.
no.
pp.
.
a. sarkar j. guo n. siegmund s. apel and k. czarnecki costefficient sampling for performance prediction of configurable systems t in proc.
int.
conf.
automated software engineering ase .
ieee pp.
.
c. henard m. papadakis m. harman and y .
le traon combining multi objective search and constraint solving for configuring large software product lines in proc.
int.
conf.
software engineering icse .
ieee pp.
.
a. s. sayyad j. ingram t. menzies and h. ammar scalable product line configuration a straw to break the camel s back in proc.
int.
conf.
automated software engineering ase .
ieee pp.
.
v .
nair t. menzies n. siegmund and s. apel faster discovery of faster system configurations with spectral learning automated software engineering vol.
no.
pp.
.
j. oh d. batory m. myers and n. siegmund finding near optimal configurations in product lines by random sampling in proc.
europ.
software engineering conference and acm sigsoft symp.
foundations of software engineering esec fse .
acm pp.
.
s. wang c. li h. hoffmann s. lu w. sentosa and a. i. kistijantoro understanding and auto adjusting performance sensitive configurations in proc.
int.
conf.
architectural support for programming languages and operating systems asplos .
acm pp.
.
d. shen q. luo d. poshyvanyk and m. grechanik automating performance bottleneck detection using search based application profiling inproc.
int.
symp.
software testing and analysis issta .
acm pp.
.
o. ibidunmoye f. hern ndez rodriguez and e. elmroth performance anomaly detection and bottleneck identification acm computing surveys pp.
.
g. jin l. song x. shi j. scherpelz and s. lu understanding and detecting real world performance bugs proc.
int.
conf.
programming language design and implementation pldi vol.
pp.
.
x. han and t. yu an empirical study on performance bugs for highly configurable software systems in proc.
int.
symp.
empirical software engineering and measurement esem .
acm pp.
.
a. van hoorn j. waller and w. hasselbring kieker a framework for application performance monitoring and dynamic software analysis in proc.
int.
conf.
performance engineering icpe .
acm pp.
.
m. selakovic t. glaser and m. pradel an actionable performance profiler for optimizing the order of evaluations in proc.
int.
symp.
software testing and analysis issta .
acm pp.
.
n. snellman a. ashraf and i. porres towards automatic performance and scalability testing of rich internet applications in the cloud in proc.
europ.
conf.
software engineering and advanced applications seaa .
ieee pp.
.
m. b. chhetri s. chichin q. b. v o and r. kowalczyk smart cloudbench automated performance benchmarking of the cloud in proc.
int.
conf.
cloud computing cloud .
ieee pp.
.
m. velez p. jamshidi f. sattler n. siegmund s. apel and c. k stner configcrusher towards white box performance analysis for configurable systems automated software engineering vol.
no.
pp.
.
m. velez p. jamshidi n. siegmund s. apel and c. k stner whitebox analysis over machine learning modeling performance of configurable systems in proc.
int.
conf.
software engineering icse .
ieee .
f. medeiros c. k stner m. ribeiro r. gheyi and s. apel a comparison of sampling algorithms for configurable systems in proc.
int.
conf.
software engineering icse .
ieee pp.
.
e. reisner c. song k. k. ma j. s. foster and a. porter using symbolic evaluation to understand behavior in configurable software systems in proc.
int.
conf.
software engineering icse .
acm pp.
.
j. meinicke c. p. wong c. k stner t. th m and g. saake on essential configuration complexity measuring interactions in highlyconfigurable systems in proc.
int.
conf.
automated software engineering ase .
acm pp.
.
h. hoffmann s. sidiroglou m. carbin s. misailovic a. agarwal and m. rinard dynamic knobs for responsive power aware computing in proc.
int.
conf.
architectural support for programming languages and operating systems asplos .
acm pp.
.
t. th m s. apel c. k stner i. schaefer and g. saake a classification and survey of analysis strategies for software product lines acm computing surveys vol.
no.
pp.
.
a. von rhein t. th m i. schaefer j. liebig and s. apel variability encoding from compile time to load time variability journal of logical and algebraic methods in programming vol.
no.
pp.
.
a. von rhein j. liebig a. janker c. k stner and s. apel variabilityaware static analysis at scale an empirical study acm trans.
software engineering and methodology tosem vol.
no.
pp.
.
m. lillack c. k stner and e. bodden tracking load time configuration options ieee trans.
software engineering tse vol.
no.
pp.
.
v .
avdiienko k. kuznetsov a. gorla a. zeller s. arzt s. rasthofer and e. bodden mining apps for abnormal usage of sensitive data in proc.
int.
conf.
software engineering icse .
ieee pp.
.
c. kaltenecker a. grebhahn n. siegmund and s. apel the interplay of sampling and machine learning for software performance prediction ieee software vol.
no.
pp.
.
m. f. johansen .
haugen and f. fleurey an algorithm for generating t wise covering arrays from large feature models in proc.
int.
software product line conference splc .
acm pp.
.
d. marijan a. gotlieb s. sen and a. hervieu practical pairwise testing for software product lines in proc.
int.
software product line conference splc .
acm pp.
.
c. kaltenecker a. grebhahn n. siegmund j. guo and s. apel distance based sampling of software configuration spaces in proc.
int.
conf.
software engineering icse .
ieee pp.
.
m. courtois and m. woodside using regression splines for software performance analysis in proc.
int.
workshop software and performance wosp .
acm pp.
.
t. a. israr d. h. lau g. franks and m. woodside automatic generation of layered queuing software performance models from commonly available traces in proc.
int.
workshop software and performance wosp .
acm pp.
.
a. mizan and g. franks an automatic trace based performance evaluation model building for parallel distributed systems in proc.
int.
conf.
performance engineering icpe .
acm pp.
.
d. westermann j. happe r. krebs and r. farahbod automated inference of goal oriented performance prediction functions in proc.
int.
conf.
automated software engineering ase .
acm pp.
.
k. krogmann m. kuperberg and r. reussner using genetic search for reverse engineering of parametric behavior models for performance prediction ieee transactions on software engineering vol.
no.
pp.
.
v .
ackermann j. grohmann s. eismann and s. kounev blackbox learning of parametric dependencies for performance models in models workshops .
j. grohmann s. eismann s. elflein j. v .
kistowski s. kounev and m. mazkatli detecting parametric dependencies for performance models using feature selection techniques in proc.
int.
symp.
modeling analysis and simulation of computer and telecommunication systems mascots .
ieee pp.
.
n. siegmund m. rosenmuller c. kastner p. g. giarrusso s. apel and s. s. kolesnikov scalable prediction of non functional properties in software product lines in proc.
int.
software product line conference splc .
ieee pp.
.
n. siegmund s. s. kolesnikov c. k stner s. apel d. batory m. rosenm ller and g. saake predicting performance via automated feature interaction detection in proc.
int.
conf.
software engineering icse .
ieee pp.
.
v .
nair z. yu t. menzies n. siegmund and s. apel finding faster configurations using flash ieee trans.
software engineering tse vol.
no.
pp.
.
j. du n. sehrawat and w. zwaenepoel performance profiling of virtual machines in proc.
int.
conf.
virtual execution environments vee .
acm pp.
.
t. mytkowicz a. diwan m. hauswirth and p. f. sweeney evaluating the accuracy of java profilers in proc.
int.
conf.
programming language design and implementation pldi .
acm pp.
.
l. song and s. lu performance diagnosis for inefficient loops in proc.
int.
conf.
software engineering icse .
ieee pp.
.
b. everitt and a. skrondal the cambridge dictionary of statistics.
cambridge university press vol.
.
n. s. pillai and x. l. meng an unexpected encounter with cauchy and l vy the annals of statistics pp.
.
t. s. ferguson maximum likelihood estimates of the parameters of the cauchy distribution for samples of size and journal of the american statistical association vol.
no.
pp.
.
j. a. reeds asymptotic number of roots of cauchy location likelihood equations the annals of statistics pp.
.
j. wang and c. j. wu a hidden projection property of plackett burman and related designs statistica sinica pp.
.
s. m. blackburn r. garner c. hoffmann a. m. khan k. s. mckinley r. bentzur a. diwan d. feinberg d. frampton s. z. guyer m. hirzel a. l. hosking m. jump h. b. lee j. e. b. moss a. phansalkar d. stefanovic t. vandrunen d. von dincklage and b. wiedermann the dacapo benchmarks java benchmarking development and analysis inproc.
int.
conf.
object oriented programming systems languages and applications oopsla .
acm pp.
.
p. geurts d. ernst and l. wehenkel extremely randomized trees machine learning vol.
no.
pp.
.
v .
nair t. menzies n. siegmund and s. apel using bad learners to find good configurations in proc.
europ.
software engineering conference and acm sigsoft symp.
foundations of software engineering esec fse .
acm pp.
.