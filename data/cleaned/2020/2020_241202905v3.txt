constrained ltl specification learning from examples changjian zhang carnegie mellon university pittsburgh pa usa changjiz andrew.cmu.eduparv kapoor carnegie mellon university pittsburgh pa usa parvk andrew.cmu.eduian dardik carnegie mellon university pittsburgh pa usa idardik andrew.cmu.eduleyi cui columbia university new york ny usa lc3542 columbia.edu r omulo meira g oes the pennsylvania state university state college pa usa romulo psu.edudavid garlan carnegie mellon university pittsburgh pa usa dg4d andrew.cmu.edueunsuk kang carnegie mellon university pittsburgh pa usa eunsukk andrew.cmu.edu abstract temporal logic specifications play an important role in a wide range of software analysis tasks such as model checking automated synthesis program comprehension and runtime monitoring.
given a set of positive and negative examples specified as traces ltl learning is the problem of synthesizing a specification in linear temporal logic ltl that evaluates to true over the positive traces and false over the negative ones.
in this paper we propose a new type of ltl learning problem called constrained ltl learning where the user in addition to positive and negative examples is given an option to specify one or more constraints over the properties of the ltl formula to be learned.
we demonstrate that the ability to specify these additional constraints significantly increases the range of applications for ltl learning and also allows efficient generation of ltl formulas that satisfy certain desirable properties such as minimality .
we propose an approach for solving the constrained ltl learning problem through an encoding in first order relational logic and reduction to an instance of the maximal satisfiability maxsat problem.
an experimental evaluation demonstrates that atlas an implementation of our proposed approach is able to solve new types of learning problems while performing better than or competitively with the state of the art tools in ltl learning.
i. i ntroduction temporal logic tl specifications are a class of specification notations that are used to specify how a system behaves over time.
tl specifications such as linear temporal logic ltl have been used in a wide range of software analysis tasks such as model checking reactive synthesis program understanding and runtime monitoring .
despite its utility formalizing and specifying desired system properties in temporal logic is a notoriously challenging and error prone process .
one active area of research that has the potential to overcome this challenge is specification learning where the goal is to automatically infer formal specifications from example traces .
these traces may be derived from system executions or manually created by developers as examples.
the extracted specification could then be used by developers to comprehend and debug the behavior of a system document itsexpected properties or perform verification through methods such as model checking and theorem proving .
in this work we specifically focus on the problem of ltl learning from examples that is given a set of positive and negative traces generate an ltl specification that evaluates to true over the positive traces and false over the negative ones.
several prior works have investigated this problem .
one of the challenges in these existing approaches is providing control over the generated ltl formula.
typically a given set of positive and negative traces are only a partial specification of the underlying system behavior to be captured by the resulting ltl specification.
a formula that is learned solely from such traces may fail to precisely capture the system behavior and need to be further refined.
for example consider a mobile navigation robot that moves around three regions a b and r the last one being a dangerous area that the robot should avoid .
suppose that the robot developer wishes to use formal specifications to precisely document the robot behavior and to leverage them for verification.
an ltl learning tool can be used to extract such specifications from traces representing sample behaviors of the robot.
specifically consider the following two sample traces collected from an operation log a b b .
.
.
positive the robot visiting aand then b and b r r .
.
.
negative the robot enters rand never leaves indicating a potentially unsafe behavior .
a specification that may be learned from these traces is a x gb i.e.
the robot should visit a initially and then stay in b. technically this formula is a valid solution to the learning problem.
however it also fails to capture the developer s intent i.e.
to prohibit the unsafe scenario.
instead a more ideal specification would constrain the robot from being stuck at r while not requiring it to start in region a. devising an additional set of traces to cover a more diverse set of system behaviors may be one way to overcome this issue.
however in general determining a set of traces that precisely capture the desired characteristics of a specification is a challenging task.
for example one may wish to extract a 1arxiv .02905v3 dec 2024specification that fits into a certain pattern or a class e.g.
a safety or liveness property minimize the number of times that a certain proposition appears or learn a formula that is close to another specification e.g.
in the context of specification repair .
ltl learning approaches that rely solely on examples lack the expressive power to allow fine grained control over the characteristics of the formula to be inferred.
to this end we propose the constrained ltl learning problem as a generalization over the original ltl learning problem.
the key idea is to allow the user to specify besides the set of positive and negative traces additional constraints that must hold over the syntactic structure of the generated ltl formula.
these constraints based on first order logic fol can be used to express a wide range of properties about ltl expressions thus giving the user more control over the solution space being explored.
for example one may specify a constraint stating that the resulting ltl formula must mention region r or the formula should follow the pattern of g .
in addition the user can specify optimization objectives to maximize or minimize the syntactic structure in certain ways e.g.
minimize the formula size or maximize the similarity against another ltl formula.
together these constraints and objectives can help bias generated solutions towards more desirable ones.
we present a constrained ltl learning tool named atlas that solves this problem by encoding it into alloymax an extension of alloy a specification language based on first order relational logic.
a problem in alloymaxis eventually reduced to an instance of the maximal satisfiability maxsat problem .
we show that constrained ltl learning is a simple but powerful generalization of the original ltl learning problem enabling a new set of use cases beyond those that were possible before in the existing state of the art tools.
in addition to its applicability our experimental evaluation shows that our approach to solving constrained learning problems performs significantly better than the existing methods over a set of benchmark problems and that it performs competitively over unconstrained problems.
the primary target users of atlas are expert specifiers and tool builders .
for experts in formal logic who currently use ltl learning tools atlas provides them with a more powerful way to control the learning process to extract specifications that precisely capture their intent.
moreover the generality of fol means that atlas has the potential to serve as an underlying general purpose engine for other specification based tools that rely on ltl learning e.g.
specification weakening or repair as described in section vii .
in addition a tool built upon atlas could automatically convert domain specific constraints or requirements in natural language to fol enabling non expert users to also benefit from ltl learning.
the contributions of this paper are as follows the formulation of the constrained ltl learning problem a generalized learning problem that allows users to define fol constraints and optimization objectives over the structure of the learned formula section iv .
fig.
overview of a constrained ltl learning problem in atlas.
an encoding of the learning problem in alloymax which is then solved as a maxsat problem section v and vi and a tool implementing the proposed technique named atlas.
a set of case studies from different application domains section vii and a set of benchmark results demonstrating the performance of our approach on both constrained and unconstrained learning problems section viii .
ii.
m otivating example in this section we demonstrate the need for more general and customizable constraints and objectives for ltl learning using a case study from specification mining a major application of ltl learning.
consider a system analyst working on analyzing a multiprocessing algorithm developed by other engineers.
the analyst plans to employ formal verification but the algorithm is designed without a formal specification.
this lack of specification is common in practice even in safety critical domains many systems do not have formally defined requirements until a later testing phase when the need to support rigorous verification arises .
although the analyst has a background in formal logic without an in depth understanding of the algorithm it would be challenging to extract a specification that is not only consistent with the algorithm but also representative of its behavior.
specification mining is one approach to overcome this bottleneck by generating executions e.g.
from an operation log one can use ltl learning to infer specifications that capture the system behavior.
these specifications can then be used for documentation regression testing or formal verification .
procedure multi processing algo i while true do a1i skip non critical section a2i flag i true a3i await flag i false csi skip critical section a4i flag i false fig.
a multi processing algorithm under analysis.
2fig.
a positive example trace of the algorithm in figure .
we illustrate this workflow using a sample algorithm shown in figure here iis the identifier id for a process and only two concurrent processes are considered id or .
to generate execution traces the implementation is instrumented with labels on instructions.
let each label in figure e.g.
a2ion line be a proposition that is true right before process iexecutes that line of code and becomes false after the line is executed.
for instance a2ibecomes true right before the instruction flag i true is executed and becomes false after this assignment.
then each trace is a sequence of pairs of propositions where each pair describes the current locations of the concurrent processes.
figure shows an example of such traces.
a limitations of existing methods the analyst may use an ltl learning tool to infer a specification from traces.
when an existing learner such as flie is applied to the trace in figure one possible output formula is a10.
this formula is technically valid as it is consistent with this particular execution but is also arguably not descriptive of the overall algorithm behavior.
providing more traces may not yield a different solution as every possible trace starts with a state where a10is true.
one approach for learning more meaningful formulas is to allow the user to constrain the space of candidate formulas to be explored.
for instance suppose that the analyst wants to learn a safety invariant of form g i.e.
always holds .
one way to achieve this goal is to use a learner that takes a specification template as an additional input such as ltlsketcher .
although templates are a significant improvement they also fall short of providing fine grained control that may be desired.
for example without further constraining e.g.
should not contain temporal operators a template based learner may return gf a10 which is a valid liveness property but not a safety invariant as intended by the analyst.
b proposed approach we argue that additional constraints and objectives are needed to express the analyst s preference on what a meaningful safety invariant looks like.
for example consider the following requirement learn an ltl formula that is in form g where does not contain temporal operators and maximizes the use of cs0andcs1in .
it first defines a constraint that the formula needs to be a safety invariant.
then it defines an objective to maximize the use of certain propositions that the analyst is interested in i.e.
the critical sections in the code .
as explained in section iv it can be expressed as constraints in fol and our approach atlas can find formula g cs0 cs1 which captures amutual exclusion property.
such constraints and objectives cannot be expressed using the existing learning tools.
as another example consider the trace shown in figure .
this trace is considered negative as it depicts an undesirable fig.
a negative example trace generated from the algorithm.
behavior where the execution is stuck at state a30 a31 i.e.
both processes are waiting for the other to reset the flag .
in this case the analyst wants to learn a liveness property that would rule out this behavior in form of g f .
again given this pattern as a template a tool like ltlsketcher may return g a30 fa10 .
this is a valid liveness property however it is also rather unintuitive as it requires a process to enter a30before a10 even though a10occurs before a30in the algorithm s procedure.
to further improve this formula the analyst using atlas can specify a constraint capturing the following requirement should be a label after as they appear in the algorithm procedure .
this constraint requires that for example if a30 then should be either cs0ora40.
given this constraint as an input atlas generates formula g a30 fcs0 which is arguably more intuitive than the above one.
this formula corresponds to a deadlock free property.
c summary the algorithm described in figure is derived from the well known peterson s mutual exclusion algorithm with a deadlock defect introduced for illustration.
the learned formulas in the example might look relatively simple to specify.
however even for a simple algorithm as this one existing learning techniques often fail to return solutions that precisely capture the system behavior.
this limitation cannot be addressed only by adding more traces and there is a need for more expressive constraints and objectives.
to our knowledge atlas is the first ltl learning tool that is expressive enough to encode each of the constraints in the example above.
it offers users the capability to interactively and gradually add custom constraints and objectives to extract valuable specifications from examples.
later in section vii we demonstrate the expressive power and generality of atlas through additional use cases beside specification mining such as specification repair and invariant weakening.
iii.
p reliminaries a. linear temporal logic linear temporal logic ltl is an extension of propositional logic with temporal operators.
its syntax is as follows p g f x u where p ap is an atomic proposition of a finite set of propositions ap.
an ltl formula is interpreted over an infinite trace 2ap .
specifically the temporal operators are interpreted as g globally holds in all future states.
f finally holds eventually in some future state.
x next holds in the next state.
u until always holds until becomes true in some future state.
b. ltl learning from examples in a typical setting ltl learning defines the problem of inferring an ltl formula from positive and negative example traces given a set of atomic propositions ap let p n 2ap be two potentially empty disjoint sets of infinite traces where parepositive examples andnare negative examples .
we call s p n asample .
then the task of ltl learning is to find a formula such that p the trace satisfies formula and n the trace does not satisfy formula .
there exists a trivial solution to separate pandnin the formw a pv b n a b where a bseparates each example pair a b .
however this solution is obviously over fitting and less helpful in practice.
thus we are often interested in finding an ltl formula of minimal size.
c. alloy and alloymax alloy is a modeling language based on first order relational logic with transitive closure.
with its sat based engine the alloy analyzer has been applied to a wide range of problems including protocol verification test generation and bug finding .
alloymax is an extension of alloy with a capability to express and analyze problems with optimal solutions.
it introduces a small addition of language constructs to specify problems with optimality as an objective and translation from an alloymaxproblem to a maximum satisfiability maxsat problem which can be solved by a maxsat solver.
specifically alloymaxcan be used to maximize or minimize relations e.g.
maximizing allowed packets while adhering to network security policies define soft constraints e.g.
adding participants time preferences in meeting scheduling and define priorities for various objectives e.g.
prioritizing morning meeting times over afternoon ones in meeting scheduling.
interested readers are encouraged to refer to alloy and alloymax for more details about their capabilities.
iv.
p roblem formulation in this work we propose a new type of ltl learning problem called the constrained ltl learning problem .
a. constrained ltl learning problem a constrained ltl learning problem is defined as a tuple ap s where ap is afinite set of atomic propositions s p n is a sample is a first order predicate that constraints the syntactic structure of the learned formula and is an optimization objective over the syntactic structure of the formula.
the goal of the problem is to find an ltl formula such that p n syntax holds funcdef funcdef func identifier var expr elementary vardecl vardecl elementary expr expr expr expr expr expr expr compop number expr const var comprehension expr expr expr expr expr expr expr expr expr .expr expr expr expr func comprehension var func identifier expr vardecl var expr compop const identifier var identifier identifier fig.
abstract syntax of syntactic constraint .
andsyntax optimizes where syntax represents the syntactic structure of .
for the syntactic constraint a user could specify it to learn an invariant g where is a propositional formula a liveness property g f or a gr formula which is widely used in reactive synthesis .
many of these cannot be expressed by existing tools.
moreover for the objective a user could specify it to optimize the syntactic structure in certain way e.g.
minimizing the size of the formula.
b. syntactic constraint we formally define the constructs for the syntactic constraint .
for an ltl formula syntax l r root represents its syntax tree where l r n nare the left child and right child relations respectively nis the set of nodes in the syntax tree and root is the root node.
in particular n s op nnopwhere n g f u x ap andnopis the set of nodes for a particular operator type or atomic propositions.
then is a fol constraint over syntax .
figure shows its abstract syntax which is an extension to fol and set theory with the following operations for improved expressiveness s.r b a s a b r join of set s nand relation r n n. also for simplicity let a.r a .r where a n. r1.r2 a c a b r1 b c r2 join of relations.
r r r.r r.r.r .
.
.
transitive closure of a relation r n n. r r a a a n reflexive transitive closure of a relation r n n. r b a a b r inverse of a relation.
moreover a user can define functions to reuse common expressions.
by default we introduce the following functions l n n.l left child of a node n. r n n.r right child of a node n. desc n n. l r all descendent nodes of a node nby using transitive closure.
subnodes n n. l r all descendent nodes of a node nincluding itself.
example to learn a liveness property g f where and are propositional formulas the user can define the following constraints ng ng n n nf nf root ng l root n r n nf subnodes l n n g f u x desc nf n g f u x where n g f u x s op g f u x nopand lines are connected by .
specifically line declares instances of nodes line defines the structure g f and lines and define and should not contain temporal operators.
c. optimization objective the syntax for optimization objective is an extension to the syntax defined in figure min expr max expr expr specifically we introduce the following operators min s where sis a non empty set and the number of elements in sshould be minimized.
max s where sis a non empty set and the number of elements in sshould be maximized.
s minimizes set sand ideally makes it empty.
where is a constraint and is optional to be true.
an optimization operator can be assigned a superscript k n e.g.
mink s indicating its priority.
an objective with a higher priority should be optimized before all the other objectives with lower priorities.
when the superscript kis omitted the constraint has the lowest priority k .
example since there exists an over fitting solution for any learning problem we often prefer to learn a formula that is minimal in its size.
this can be expressed as l r it minimizes the size of the child relations edges in a syntax tree which in turn minimizes the size of the formula.
example in addition when a user wants to learn a formula that maximizes the use of a set of critical propositions critical ap they can define max2 subnodes root ncritical the use of max2specifies that this goal should be optimized with priority which happens before minimizing the total size of the solution as shown in the previous example.v.
t echnical approach we reduce a constrained ltl learning problem to an instance of relational model finding.
in particular we use alloymax which allows us to define and solve problems in first order relational logic with the support for all the operators for syntactic constraint and optimization objective .
the idea of our encoding is inspired by the sat encoding proposed by neider and gavran which is inspired by bounded model checking .
the encoding assumes that all traces in the sample set are ultimately periodic known aslasso traces .
such a trace can be represented as uv where u 2ap andv 2ap .
the observation is that given a finite set of ap the set of states visited by uv starting from any time point t is finite and can be determined based only on the finite prefix uv.
therefore this enables us to check ltl temporal operators w.r.t.
infinite traces .
in addition the user provides the maximum number of sub formulas to bound the search space.
furthermore we employ several heuristics described in later sections to make our alloymaxencoding more succinct and efficient.
the encoding can be divided into six parts ltl syntax encoding ltl semantics encoding problem specific encoding learning objective encoding custom syntactic constraints and optimization objectives.
a. syntax encoding we model the syntax of an ltl formula as a directed acyclic graph dag .
essentially a syntax dag is a syntax tree with shared common sub formulas.
this helps reduce the search space.
the following code snippet shows how this syntax graph can be modeled in alloymax a sig signature defines a type of atoms.
2abstract sig dagnode fields of a signature become relations l set dagnode r set dagnode fact defines a block of constraints.
7fact all n dagnode nnot in n. l r extends defines sub types of a signature.
9sig and or imply until extends dagnode one means exactly one element.
one land one r sig neg f g x extends dagnode no means empty set.
one land no r abstract sig literal extends dagnode noland no r one sig learnedltl root dagnode fun root one dagnode learnedltl.root line defines the parent signature for all nodes in a dag and line defines the left right child relations landr.
line constrains each node to have no paths to itself i.e.
the graph should be acyclic.
specifically this is achieved by computing all the descendent nodes of nthrough transitive closure l r as defined in section iv b where is the union operator.
lines define the sub types of nodes including operators and atomic propositions.
specifically we add constraints for 5binary operators to have exactly one left and one right child lines unary operators to have only left child line and atomic propositions to have no children line .
finally lines define a helper function for accessing the root node.
therefore relation landr and function root construct the syntax of the learned formula syntax l r root .
b. semantics encoding this section describes the encoding for the semantics of ltl.
we show only the definitions for x f andu.
encodings for the other operators can be defined in a similar manner.
1abstract sig seqidx 2abstract sig trace relation trace x seqidx x seqidx lasso seqidx seqidx relation trace x dagnode x seqidx val dagnode seqidx 8fun seqindices set seqidx .. 9fun futureidx set seqidx .. line defines signature seqidx for time points.
lines define the signature for lasso traces.
specifically for any trace t uv it has a lasso relation t i i where i uv 1is the ending index of its prefix uvandi u is the starting index of loop v. it also has a valrelation where any t n i valstands for the trace tsatisfies the sub formula represented by the sub dag from node n starting from time i. lines define two helper functions seqindices and futureidx .
their functionalities are seqindices t returns all the time points of a trace t as the example traces can be in different lengths futureidx t i returns all the future time points of a trace tstarting from time i including i .
they are computed using the lasso relation and a relation next seqidx seqidx that defines a total order over the time points connecting a time iwith its next time point i .
for instance figure shows an example lasso trace uv .
the solid black arrows represent relation next and the dashed arrow represents relation lasso of this trace.
based on the next relation for time i andi the next time points are i and i respectively.
for time i the state value at i equals to the value at i because of the repeating v s andi can be retrieved by the lasso relation.
therefore .
.
.
are unnecessary tuples shown in figure and we only need to model the finite prefix uvof a lasso trace.
fig.
an example lasso trace with two atomic propositions.for this trace function seqindices t function futureidx t and function futureidx t .
a heuristic is applied that uses transitive closures to compute them e.g.
i. next t.lasso returns all future time points of iwithout comparing the order of time points.
operator for any node n n and time ibeing in the time range of uv computed by seqindices the tuple t n i is invalif and only if at time i its left sub formula n.lor right sub formula n.ris in val i.e.
one of the subformulas holds.
1all t trace n or i seqindices n i int.val iff n.l i int.val orn.r i int.val x operator for any node n nxand time i t n i valif and only if t n.l i val where i is the next time point of icomputed by expression i. next t.lasso .
for example in figure next t.lasso .
thus i. next t.lasso returns the next time point of time i considering the loop vint uv .
1all t trace n x i seqindices 2n i int.val iff n.l i. next t.lasso int.val f operator for any node n nfand time i t n i valif and only if i futureidx t i t n.l i val where operator some stands for andfutureidx computes all the future time points.
for example for the trace in figure when i futureidx t i .
it means that from i all the possible state values can be represented by states ati .
thus we assert that there exists a time point in these future indices where pholds.
1all t trace n f i seqindices n i int.val iff some i futureidx n.l i int.val u operator finally the following code defines the qup operator.
specifically we leverage the heuristic from the sat encoding of the u operator used in bounded model checking .
it unfolds uby using the x operator in the sense that qup p q x qup as defined in lines .
in addition on line we constrain that sub formula p i.e.
n.r will eventually be true from time i otherwise a trace where p never becomes true would also satisfy this encoding.
1all t trace n until i seqindices n i int.val iff some i futureidx n.r i int.val n.r i int.val or n.l i int.val and n i. next t.lasso int.val c. problem specific encoding the following code snippet shows the alloymaxtemplate for the problem specific encoding particularly the values of all the positive and negative traces.
1one sig p ap extends literal 2one sig i max t t s extends seqidx 3fact first i0 next i0 i1 i1 i2 .
.
.
67abstract sig positivetrace extends trace 8abstract sig negativetrace extends trace 9one sig t uv p extends positivetrace lasso i uv i u t n i n ap n uv i inval means set intersection.
no t n i n ap n uv i val one sig t uv n extends negativetrace same above in this template the mathematical expressions in the comments .. will be replaced by the actual parameters of a specific learning problem ap s .
line defines all the atomic propositions in apas literal dag nodes.
lines define the finite set of time points given the maximum length of traces in the sample sand explicitly specify the initial time i0and the next relation.
this leverages a heuristic in alloy to improve performance through partial instances .
lines further divide the trace signature into two sets positivetrace andnegativetrace .
then lines explicitly specify the value for a trace t uv in set pandn.
specifically on line its lasso relation maps its last time index i uv 1to the start of the loop i u .
lines specify that for any atomic proposition node n ap if n uv i where uv i is the set of true propositions at time iof prefix uv then the tuple t n i is invalrelation otherwise it is not.
e.g.
for the trace tin figure we have t x1 i1 t x2 i2 valand t x1 i0 t x2 i0 t x2 i1 t x1 i2 val d. learning objective encoding the following code allows alloymaxto generate an ltl formula that satisfies the sample.
1run all t positivetrace root t0 int.val all t negativetrace root t0 not in t.val for max number of dag nodes dagnode line defines that for any positive trace t p t root val i.e.
the learned formula is true on trace tfrom time .
in contrast for any negative trace t n t root is not in val line .
finally on line the user provides the maximum number of dag nodes allowed for a particular problem.
e. custom structural constraints alloymaxsupports all the constructs for defining the structural constraint including fol set operations and the additional join transitive closure and inverse operations1.
for example to encode the constraints for g f example of section iv b we have 1fun desc n. l r 2fun subnodes n. l r 3one sig g0extends g 4one sig imply0 extends imply 5one sig f0extends f 1due to limited space the translation from atlas constraints and objectives into alloymaxis omitted.
the translation however is straightforward as alloy itself is based on fol and well suited for encoding the constraints.6fact root g0 and root.l imply0 and imply0.r f0 no g f until x subnodes no g f until x desc lines correspond to the desc andsubnodes functions.
lines declare instances for ng n nf respectively.
finally lines correspond to the constraints lines in example .
f .
optimization objective similar to the syntactic constraints alloymaxsupports all the optimization operators for defining objective .
specifically it has the following mappings mink s minsome s maxk s maxsome s s k softno s k soft fact where kis an optional priority.
for example by default we include the following optimization goal to minimize the size of the learned formula example in section iv c 1softno l r moreover to maximize the use of critical propositions example we have 1maxsome subnodes p q r where let critical p q r ap.
vi.
l earning by alloymax a. solving alloymaxwith maxsat we briefly explain how alloy and alloymaxsolve a problem using a sat and a maxsat solver respectively interested readers should refer to the original papers for more details .
a relation in alloy is translated into a matrix of boolean variables each of which is true if and only if the tuple represented by this particular variable is in the relation and a relational expression is represented by operations over one or more boolean matrices.
for example consider a relation r a bwhere a a1 a2 andb b1 b2 .
then this relation is represented by a set of boolean variables r11 r12 r21 r22 where for example r11is true if and only if tuple a1 b1 is inr.
then in alloymax it encodes optimization goals as weighted soft clauses such that a maxsat solver finds optimal solutions by maximizing the total sum of weights.
for example the alloymaxoperator softno minimizes the number of tuples in a relation the expression softno rwill be converted to r11 k r12 k r21 k r22 k where e.g.
r11 kis a soft clause with weight kthat may or may not be satisfied.
thus this formula finds a relation r with a minimized number of tuples in it and ideally r .
in addition alloy supports blocking a solution and using an incremental solver to find a new solution.
thus we leverage this feature to enumerate solutions of a learning problem.
7b.
correctness our approach is sound but complete only up to the userprovided bound on the maximum number of dag nodes.
a correctness proof can be found in the appendix.
c. quality of solutions since there can be multiple ltl formulas that satisfy an ltl learning problem additional constraints and objectives are often necessary for finding useful solutions w.r.t.
a problem domain.
moreover enumerating solutions satisfying the constraints is also a critical functionality.
our approach supports all of them.
furthermore other than problem dependent constraints additional sets of constraints are also useful for avoiding trivial or less satisfactory solutions.
we list a few constraints that we find useful from our experience.
disable reusing in dag the dag encoding was designed to improve performance.
when minimizing against a dag it guarantees a minimal number of distinct sub formulas.
however this does not always produce a minimal ltl formula in its syntax tree size where repeating sub formulas are also counted.
for example consider fp fgpandfgp fgp.
the latter one has a bigger syntax tree size but a smaller dag size as the sub formula fgpis reused .
such behavior might reduce the usefulness of the tool for certain problems and can be disabled with the following constraint n n nap n. l r l r where n. l r returns the parent nodes of n which could be more than one in a dag .
avoid tautology another example is avoiding tautologies in solutions.
this is often necessary when only positive traces are provided where the learner may return a tautology true as a valid solution.
n n l n r n negation normal form nnf the use of negation may result in some equivalent but less readable formulas e.g.
g x versus f x where the latter may be more readable and preferable.
nnf is also helpful in automated theorem proving .
thus we can enforce that negation should only be applied to atomic propositions.
n n l n nap this is an inexhaustive list of problem independent constraints that we find helpful.
other examples include requiring to be in conjunction normal form cnf or disjunction normal form dnf .
vii.
u secases in this section we present three use cases to demonstrate the need for syntactic constraints and or optimization objectives.
these use cases also show how tool builders can leverage atlas as a back end ltl learner to perform various specification based tasks such as specification repair and invariant weakening.
enterpassword selectcandidate 1212vote1confirmfig.
a state machine representing the voting machine .
a. specification mining this use case demonstrates the usefulness of custom constraints and enumeration.
this case study is inspired by a voter fraud incident in kentucky usa where corrupt officials manipulated a flaw in the system to flip votes .
figure shows an abstract model of the system as described in a voter enters the booth enters a password selects a candidate votes and finally confirms the vote before leaving the booth.
we generate example traces based on this model manually check their correctness and mark them as positive or negative.
specifically a negative trace indicating a vulnerability of this system is a voter leaves the booth before confirming their vote then a corrupt official enters the booth and uses the back button to select another candidate and confirms the choice.
we want to learn a safety invariant to ensure election integrity.
particularly the invariant should be of the form g where should not contain any temporal operators.
these constraints can be expressed as root ng l root ng f u x in addition our technique supports enumerating solutions that satisfy the given constraints in the order of the formula size or other custom optimization objectives .
this is often helpful in finding variants of formulas.
specifically in this case study the enumeration helps us to find two safety invariants g selectcandidate voterinbooth g officialinbooth enterpwd the first formula is in line with the key safety property defined by formal methods experts in .
however there is a stronger implicit requirement defined in the election official is restricted from entering the booth after the voter enters her password.
we observe that this requirement is captured precisely and explicitly by the second invariant.
b. specification repair from demonstrations in this use case we demonstrate how custom optimization objectives can be helpful in learning.
the use case is motivated by the field of autonomous agent interpretability using ltl specifications .
consider for example the setup in figure 8a where the end effector of a robotic arm has to be actuated to perform some tasks.
the designer provides an initial ltl specification f green g red i.e.
reaching the green region and avoiding the red region.
however after a controller has been developed the designer decides to refine the ltl specification to better reflect the system requirements.
for example consider the requirement that the robot should satisfy the above ltl formula but also visit the blue region in figure 8a.
the designer can generate a a dof panda robotic arm.
b a radiation therapy machine.
fig.
examples of specification repair and weakening.
a set of demonstrative traces and mark each one as positive or negative depending on whether the robot successfully reaches the blue region in the trace.
moreover the modification to the existing ltl should ideally be minimal while taking this additional requirement into account.
we pose this as an ltl learning problem where the associated objective is to minimize the edit distance i.e.
minimizing the removal of existing sub formulas as well as the addition of new subformulas.
the encoding of this minimal modification problem is defined as follows ng ng nf nf n n n n green red nap oldspec n nf n ng nf green ng n n red max2 l r oldspec lines define the original formula.
line minimizes the edit distance by using the max operator with priority .
the learning process will first retain as many sub formulas as possible from the old specification and then minimize the total size of the final formula.
with these constraints our approach learns the ideal specification f green g red f blue where blue stands for reaching the blue region.
c. invariant weakening in this case study we show how complex ltl patterns e.g.
cnfs and dnfs can be expressed using our technique.
the idea of invariant weakening or more generally specification weakening comes from requirements engineering .
as the environmental conditions for a software system may change over time and space original requirements might become inadequate or inconsistent with the new environment necessitating adaptation or weakening.
this concept has been further explored in self adaptive systems .
for instance figure 8b is a radiation therapy machine similar to therac as described in .
the machine has two modes electron beam mode and x ray mode.
a spreader must be inserted during the x ray mode to attenuate the effect of the high power x ray beam and limit possible overdose.
this safety requirement can be captured as the following invariant p g xraymode spreaderin .
however a system might become too restrictive to satisfy this safety property under certain environmental behavior.
for example when switching from x ray to electron beam itmight be acceptable that the spreader is out before the mode switching is completed as long as the beam is not fired.
otherwise we may have to disable all mode switching to ensure safety .
one way to mitigate this issue is to weaken the safety invariant i.e.
finding p s.t.p p from positive acceptable and negative definitely unsafe examples .
an ideal weakened formula is g xraymode fired spreaderin .
we consider a safety invariant of the form g where and are boolean formulas.
to achieve weakening one can learn an invariant g where is a boolean formula in cnf and is a boolean formula in dnf.
in other words an invariant can be weakened by adding more conditions as conjunctions in the assumption or adding new acceptable conditions as disjunctions in the guarantee.
this weakening objective can be expressed through the following constraints ng ng n n root ng l root n n desc n n n ap n desc n n n l n nap n subnodes l n n desc n n n subnodes r n n desc n n l n xraymode l n n l l n xraymode r n spreaderin r n n l r n spreaderin specifically line defines the structure g .
line stipulates that and contain only and ap.
lines state that is in cnf and is in dnf.
lines state that may add conjuncts to xraymode and may add disjuncts to spreaderin .
with these constraints we can learn the ideal weakened formula as described above.
viii.
e valuation we investigate the following research questions rq1 how does our approach perform compared to the state of the art tool flie over unconstrained ltl learning problems?
rq2 for problems with structural constraints and or optimization objectives as described in our case studies how does our approach perform compared to the stateof the art tools?
for rq1 the metric we are concerned with is the total time taken to find the first satisfying formula.
for rq2 we evaluate performance through two key metrics the number of constrained problems solved and the time taken to solve them.
in particular even though existing tools cannot encode constraints such as ours they can be configured to enumerate all solutions until a solution satisfying the constraints is found the goal here is to compare how long it takes for atlas and the baselines to find an ideal solution.
we implemented atlas in java and use openwbo as the maxsat solver for alloymax.
all experiments were 9run on a linux machine with a core .8ghz cpu and 8gb memory.
for each problem we impose a second time out.
a. experimental setup for fair comparison we compare atlas against the stateof the art tools satisfying the following two criteria no restrictions on the type of formula that can be learned and the guarantee of finding minimal formulas.
rq1 for unconstrained problems we compare our tool against the sat based algorithm of flie .
we did not use the decision tree based method of flie as it does not provide the minimality guarantee.
then we leverage the same benchmark problems from neider which are generated based on common ltl patterns .
specifically this benchmark contains problems with a number of examples ranging from to a number of atomic propositions ranging from to and a length of traces ranging from to .
rq2 for constrained problems we first compare our tool against flie which is configured to enumerate solutions until an expected formula is found.
we also compare against ltlsketcher that can learn a formula given a user defined template.
it considers three types of placeholders in a template any ltl sub formula any unary operator and any binary operator.
however it cannot express all our expected constraints and does not support enumeration either.
thus we test whether it finds an expected formula in one run with the closest template regarding our required constraints.
if it does not we count it as a timeout.
also note that ltlsketcher is not used in rq1 as it builds on flie and without user provided templates it reduces to flie.
moreover other tools are not considered because they do not meet our criteria.
for example scarlet is a performant learning tool but cannot handle until and nested eventually and globally and also does not guarantee minimality.
then we generate a set of constrained problems from our case studies with the expected solutions provided.
specifically we provide a set of solutions satisfying our constraints for each problem and a problem is deemed solved when a tool can find any formula from the set.
specification mining.
we generate problems from the two use cases described in section ii and vii.
for peterson s algorithm we randomly generate problems from a model specified in tla with to example traces atomic propositions and a trace length of .
we add constraints for learning the mutual exclusion and the deadlock free properties.
then when comparing against ltlsketcher we use template g ?
for mutual exclusion and g ?
f?
for deadlock free where ?stands for any sub formula.
for the voting example we generate problems with to example traces atomic propositions and a trace length from to .
we add the constraint to learn an invariant in g where is a propositional formula.
however the closest template in ltlsketcher is g ?
where ?can be any formula.
specification repair.
we generate random problems from a robot arm simulator developed in .
to simulate a specification repair process we generate positive traces using fig.
comparisons of solving time in a unconstrained and b constrained problems.
the axes indicate the time in seconds to solve a problem with a 180s timeout.
a marker x y indicates a problem is solved by flie in xseconds and by atlas in yseconds.
a dot below the diagonal line y x indicates our tool is faster than flie.
the ideal specification and generate negative traces by negating a sub formula in it.
the problems have example traces ranging from to atomic propositions and a length of traces from to .
when comparing against ltlsketcher since it has no way to express our optimization objective we set an empty template to learn any formula.
invariant weakening.
we generate random weakening problems for two types of weakening let ap a b c weaken g a b tog a c b and weaken g a b c tog a b c b .
the number of examples of the problems ranges from to and the length of the examples ranges from to .
when comparing against ltlsketcher the closest it can express is g ?
?
.
b. results all these tools take a file containing the example traces of a problem as input with additional constraints or templates for rq2 .
we checked the consistency of the learned formulas with our predefined solutions.
then we report on the comparison results of atlas against other tools.
rq1.
figure 9a shows the evaluation results for unconstrained problems.
a blue marker below the diagonal line indicates that our approach is faster than flie for a given problem.
we find that our approach is faster than or equal to flie in most of the problems including where both timeout.
the average solving time including the 180s timeout is .83s for flie and .76s for atlas where our tool is about .45x faster than flie.
even though flie and our tool both encode as a sat problem flie guarantees minimality by gradually increasing the size of the learned formula and invoking a sat solver for each bound.
however we guarantee minimality by leveraging maxsat solving which also involves solving multiple sat instances but with the capability of sharing intermediate results e.g.
conflicting clauses .
rq2.
figure 9b shows the results for constrained problems benchmarked against flie.
we find that our tool is faster than 10table i comparisons of the number of solved problems and the average solving time including the 180s timeout .
peterson s v oting repair weakening solved time solved time solved time solved time atlas .65s .08s .25s .75s flie .68s .92s .63s .98s ltlsketcher 180s .30s 180s .73s or equal to flie including where both time out in most of the problems specifically 30in peterson s algorithm 10in voting machine 20in specification repair and in invariant weakening.
moreover table i shows the number of solved problems and the average solving time for flie ltlsketcher and atlas.
our tool solves the most number of problems and also has the lowest average solving time.
on average we solve about .2x as many problems as flie and .7x as many as ltlsketcher.
also our tool is about 3xfaster than both flie and ltlsketcher.
although atlas also failed to solve some problems due to timeout it is better than flie and ltlsketcher in that flie needs to enumerate a large unconstrained search space which leads to timeout more often.
ltlsketcher failed due to the lack of expressiveness in defining the expected constraints and enumeration capability.
thus we show that strong expressive power and the enumeration capability are both critical to ltl learning and our tool supports both.
we can solve all problems that flie and ltlsketcher can without the loss of generality and much performance overhead.
c. threats to validity as shown in rq2 flie and ltlsketcher are outperformed in the constrained ltl problem benchmark.
this might be due to the fact that our benchmark is based on the set of use cases that we manually constructed to demonstrate the applicability of constrained ltl learning.
however we believe that this potential bias is mitigated as these problems contain a wide range of fol constraints and are not specific to our alloy based encoding.
we also note that the solutions found by atlas are also in the space of possible solutions that flie and ltlsketcher can generate.
ix.
r elated work neider and gavran present flie a sat based encoding for ltl learning which is the first approach of learning any unrestricted minimal ltl formulas over infinite traces.
it also presents a decision tree based learning algorithm which is more efficient but does not guarantee the formula to be minimal.
scarlet was proposed to overcome the scalability limitations of however they deal with a strict fragment of ltl and also do not provide guarantees on minimality.
before neider s approach techniques for mining ltl such as rely on patterns or templates.
for example given a template g x fy texada infers specifications by substituting xandywith atomic events from the given traces and does not learn from negative traces .compared to our approach texada lies on a different tradeoff point between expressiveness and efficiency although atlas gives the user more control over the properties of ltl expressions to be learned texada is able to mine from much larger traces e.g.
thousands of events .
more recently lutz et al.
present ltlsketcher where they consider three types of substitutions i.e.
any formula unary operator and binary operator and propose a sat based method to find valid substitutions.
although they have improved expressiveness compared to texada they cannot express many patterns e.g.
cnf and dnf as demonstrated in our use cases.
in addition none of these tools supports custom optimization objectives.
researchers have also explored different variations of the learning problem .
gaglione et al.
present an approach using maxsat to learn from noisy data.
although alloymax also uses maxsat we cannot solve this type of problem because alloymaxcannot directly control the weights of clauses in a maxsat instance.
roy et al.
present an approach for learning from positive only examples.
while our tool also supports such a problem we can only constrain and optimize the syntactic structure of a solution whereas they can guarantee semantic minimality i.e.
a minimal set of behavior .
x. c onclusion and future work we have proposed the constrained ltl learning problem as a generalization of ltl learning from examples and demonstrated the applicability and performance of atlas our prototype implementation tool.
while our evaluation shows promise we plan to investigate further ways to improve the learning method.
first the scalability of our tool is limited by the underlying maxsat solver and an interesting future work is to investigate an approach that uses a different learning method based on machine learning for example .
we also plan to investigate methods or heuristics for further reducing the space of possible ltl solutions e.g.
an encoding scheme that eliminates redundant equivalent ltl expressions .
moreover we plan to investigate additional use cases for atlas such as invariant synthesis for distributed systems .
our approach improves the expressive power of ltl learning tools.
however crafting constraints in fol can be challenging for non experts in formal logic.
as future work we plan to improve the usability of ltl learning by building on atlas e.g.
through a dsl front end or an llm that converts natural language to fol .
in addition atlas and other learning tools require the user to provide examples.
future work could explore integrating external methods e.g.
model checkers or test generation tools to semi automate the generation of examples.
acknowledgment this work was supported in part by the nsf awards and and the nsa grant h98230 c .
any views opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the organizations.
11data availability the source code of our tool and all the experimental results are available at