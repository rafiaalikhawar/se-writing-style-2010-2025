a syntax guided edit decoder for neural program repair qihao zhu key laboratory of hcst moe dcst peking university beijing china zhuqh pku.edu.cnzeyu sun key laboratory of hcst moe dcst peking university beijing china szy pku.edu.cnyuan an xiao key laboratory of hcst moe dcst peking university beijing china xiaoyuanan pku.edu.cn wenjie zhang key laboratory of hcst moe dcst peking university beijing china zhang wen jie pku.edu.cnkang yuan stony brook university new york us kang.yuan stonybrook.eduyingfei xiong key laboratory of hcst moe dcst peking university beijing china xiongyf pku.edu.cn lu zhang key laboratory of hcst moe dcst peking university beijing china zhanglucs pku.edu.cn abstract automated program repair apr helps improve the efficiency of software development and maintenance.
recent apr techniques use deep learning particularly the encoder decoder architecture to generate patches.
though existing dl based apr approaches have proposed different encoder architectures the decoder remains to be the standard one which generates a sequence of tokens one by one to replace the faulty statement.
this decoder has multiple limitations allowing to generate syntactically incorrect programs inefficiently representing small edits and not being able to generate project specific identifiers.
in this paper we propose recoder a syntax guided edit decoder with placeholder generation.
recoder is novel in multiple aspects recoder generates edits rather than modified code allowing efficient representation of small edits recoder is syntax guided with the novel provider decider architecture to ensure the syntactic correctness of the patched program and accurate generation recoder generates placeholders that could be instantiated as projectspecific identifiers later.
we conduct experiments to evaluate recoder on bugs from defects4j v1.
additional bugs from defects4j v2.
bugs from introclassjava and bugs from quixbugs .
our results show that recoder repairs bugs on defects4j v1.
which achieves corresponding author.
hcst high confidence software technologies.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
bugs improvement over the previous state of the art approach for single hunk bugs tbar .
importantly to our knowledge recoder is the first dl based apr approach that has outperformed the traditional apr approaches on this benchmark .
furthermore recoder repairs bugs on the additional bugs from defects4j v2.
which is .
bugs more than tbar and bugs more than simfix.
recoder also achieves bugs and .
bugs improvement on introclassjava and quixbugs over the baselines respectively.
these results suggest that recoder has better generalizability than existing apr approaches.
ccs concepts software and its engineering computing methodologies software testing and debugging neural networks keywords automated program repair neural networks acm reference format qihao zhu zeyu sun yuan an xiao wenjie zhang kang yuan yingfei xiong and lu zhang.
.
a syntax guided edit decoder for neural program repair.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction automated program repair apr aims to reduce bug fixing effort by generating patches to aid the developers.
due to the well known problem of weak test suites even if a patch passes all the tests the patch still has a high probability of being incorrect.
to overcome this problem existing approaches have used different means to guide the patch generation.
a typical way is to learn from existing software repositories such as learning patterns from existing patches and using program code to guide the patch generation .arxiv .08253v6 mar 2022esec fse august athens greece qihao zhu zeyu sun yuan an xiao wenjie zhang kang yuan yingfei xiong and lu zhang deep learning is known as a powerful machine learning approach.
recently a series of research efforts have attempted to use deep learning dl techniques to learn from existing patches for program repair .
a typical dl based approach generates a new statement to replace the faulty statement located by a fault localization approach.
existing dl based approaches are based on the encoder decoder architecture the encoder encodes the faulty statement as well as any necessary code context into a fixed length internal representation and the decoder generates a new statement from it.
for example hata et al .
and tufano et al.
adopt an existing neural machine translation architecture nmt to generate the bug fix sequencer uses a sequence tosequence neural model with a copy mechanism dlfix further treats the faulty statement as an ast rather than a sequence of tokens and encodes the context of the statement.
however despite multiple existing efforts dl based apr approaches have not yet outperformed traditional apr approaches.
since deep learning has outperformed traditional approaches in many domains in this paper we aim to further improve the performance of dl based apr to understand whether we could outperform traditional apr using a dl based approach.
we observe that though existing dl based apr approaches have proposed different encoder architectures for apr the decoder architecture remains to be the standard one generating a sequence of tokens one by one to replace the original faulty program fragment.
the use of this standard decoder significantly limits the performance of dl based apr.
here we highlight three main limitations.
limitation including syntactically incorrect programs in the patch space .
the goal of the decoder is to locate a patch from a patch space.
the smaller the patch space is the easier the task is.
however viewing a patch as a sequence of tokens unnecessarily enlarges the patch space making the decoding task difficult.
in particular this space representation does not consider the syntax of the target programming language and includes many syntactically incorrect statements which can never form a correct patch.
limitation inefficient representation of small edits .
many patches only modify a small portion of a statement and re generating the whole statement leads to an unnecessarily large patch space.
for example let us consider the patch of defect closure in the defects4j benchmark as shown in figure .
this patch only changes one token in the statement but under existing representation it is encoded as a sequence of length .
the program space containing this patch would roughly contain n13elements where nis the total number of tokens.
on the other hand let us consider a patch space including only one token change edits.
to generate that patch only selecting a token in the faulty statement and a new token for replacement is needed.
this patch space contains only mn elements where mis the number of tokens in the faulty statement.
therefore the size of the patch space is significantly reduced.
limitation not being able to generate project specific identifiers .
source code of programs often contains project specific identifiers like variable names.
since it is impractical to include all possible identifiers in the patch space existing dl based apr approaches only generate identifiers that have frequently appeared in the training set.
however different projects have different sets of project specific identifiers and therefore only considering identifiers in the training set may exclude possible patches from the cfa.createedge fromnode branch.uncond finallynode cfa.createedge fromnode branch.on ex finallynode figure the patch for closure in defects4j returncavailablelocaleset.contains locale returnavailablelocaleset .contains locale figure the patch for lang in defects4j patch space.
for example figure shows the patch for defect lang57 in defects4j.
to generate this patch we need to generate the identifier availablelocaleset which is a method name of the faulty class and is unlikely to be included in the training set.
as a result existing dl based approaches cannot generate patches like this.
in this paper we propose a novel dl based apr approach recoder standing for repair de coder .
similar to existing approaches recoder is based on the encoder decoder architecture.
to address the limitations above the decoder of recoder has following two novel techniques.
novelty syntax guided edit decoding with provider decider architecture concerning limitation .
to address limitation the decoder component of recoder produces a sequence of edits rather than a new statement.
our edit decoder is based on the idea of the syntax guided decoder in existing neural program generation approaches .
for an unexpanded non terminal node in a partial ast the decoder estimates the probability of each grammar rule to be used to expand the node.
based on this the decoder selects the most probable sequence of rules to expand the start symbol into a full program using a search algorithm such as beam search.
we observe that edits could also be described by a grammar.
for example the previous patch for defect closure could be described by the following grammar edit insert modify ... modify modify nodeid nts here modify represents replacing an ast subtree denoted by its root node id nodeid in the faulty statement with a newly generated subtree nts1 .
however directly applying the existing syntax guided decoder to the grammar above would not form an effective program repair approach because the choice of expanding different non terminal nodes may need to be deduced along with different types of dependencies.
first the expansion of some non terminals depends on the local context e.g.
the choice of nodeid depends on the faulty statement and the neural network needs to be aware of the local context to make a suitable choice.
second to guarantee syntax correctness limitation dependency exists among the choices for expanding different non terminal nodes e.g.
when nodeid expands to an id pointing to a node with non terminal javaexpr nts should also expand to javaexpr to ensure syntactic correctness.
these choices cannot be effectively pre defined and thus the existing syntax guided decoders which only select among a set of pre defined grammar rules do not work here.
nts stands for non terminal symbol in an ast .a syntax guided edit decoder for neural program repair esec fse august athens greece provider1choice1 1p1 ......deciderchoice1 2p1 2choice1 mp1 mprovidernprovider1q1providernqnchoicen 1pn ...choicen 2pn 2choicen tpn tchoice1 1p1 q1...choice1 2p1 q1choice1 mp1 m q1choicen 1pn qn...choicen 2pn qnchoicen tpn t qn figure provider decider architecture to overcome these problems recoder introduces a provider decider architecture as shown in figure .
a provider is a neural component that provides a set of choices for expanding a nonterminal and estimates the probability piof each choice.
a basic provider is the rule predictor which similar to existing syntaxguided decoders estimates the probability of each grammar rule to expand the node.
fixing the closure example needs another provider namely the subtree locator which estimates the probability of each subtree in the faulty statement to be replaced.
on the other hand the decider is a component that estimates the probability ofqjusing each provider.
in this example when expanding edit the probability of using the rule predictor is and the probability of using the subtree locator is when expanding modify the probability of using the rule predictor is and the probability of using the subtree locator is the located subtree decides both the content of nodeid and the root symbol of nts .
finally the choices provided by all providers form the final list of choices while the probability of each choice is the product of the probability predicted by its provider and the probability of the provider itself i.e.
pi qj.
in this example for each non terminal we use the choices from only one provider and thus the probabilities of providers are either or .
later we will see that expanding some non terminals requires comparing the choices of multiple providers and the probabilities of providers could be a real number between and .
novelty placeholder generation concerning limitation .
to generate project specific identifiers a direct idea is to add another provider that selects an identifier from the local context.
however to implement such a provider the neural component needs to access all of the name declarations within the current project.
this is a difficult job as the neural component could hardly encode all source code from the whole project.
instead of relying on the neural network to generate projectspecific identifiers in recoder the neural network generates placeholders for such identifiers and these placeholders are instantiated with all feasible identifiers when applying the edits.
a feasible identifier is an identifier compatible with constraints in the programming language such as the type system.
as for defect lang57 shown in figure recoder first generates a placeholder for availablelocaleset and it will be replaced with all methods accessible in the local context that takes no arguments and returns an object with a member method contains .
each replacement forms a new patch.
the key insight is that when considering constraints in the programming language the number of choices for replacing a placeholder with an identifier is small and thus instantiating the placeholders with all possible choices is feasible.to train the neural network to generate placeholders we replace infrequent user defined identifiers in the training set with placeholders.
in this way the neural network learns to generate placeholders for these identifiers.
our experiment is conducted on four benchmarks bugs from defects4j v1.
for comparison with existing approaches.
additional bugs from defects4j v2.
bugs from introclassjava and bugs from quixbugs to evaluate the generalizability of recoder.
the results show that recoder correctly repairs bugs on the first benchmark which are .
bugs more than tbar and .
bugs more than simfix two best performing single hunk apr approaches on defects4j v1.
recoder also correctly repairs bugs on the second benchmark which are .
bugs more than tbar and .
bugs more than simfix .
on introclassjava and quixbugs recoder repairs bugs and bugs respectively which also achieves better performance than the existing apr tools that were evaluated on the two benchmarks.
the results suggest that recoder has better performance and better generalizability than existing approaches.
to our knowledge this is the first dl based apr approach that has outperformed traditional apr approaches .
to summarize this paper makes the following contributions we propose a syntax guided edit decoder for apr with a provider decider architecture to accurately predict the edits and ensure that the edited program is syntactically correct and uses placeholders to generate patches with projectspecific identifiers.
we design recoder a neural apr approach based on the decoder architecture described above.
we evaluate recoder on bugs from defects4j v1.
and additional bugs from defects4j v2.
.
the results show that recoder significantly outperforms state of the art approaches for single hunk bugs in terms of both repair performance and generalizability.
edits we introduce the syntax and semantics of edits and their relations to providers in this section.
the neural architecture to generate edits and implement providers will be discussed in the next section.
.
syntax and semantics of edits figure shows the syntax of edits.
note that our approach is not specific to a particular programming language and can be applied to any programming language called the host language that has a concept similar to the statement.
in particular it is required that when a statement is present in a program a sequence of statements can also be present at the same location.
in other words inserting a statement before any existing statement would still result in a syntactically correct program.
to ensure syntactic correctness of the edited program the syntax of edits depends on the syntax of the host language.
in figure hl refers to the host programming language our approach applies to.
in the following we explain each rule in figure in order.
as defined by rule and rule an edits is a sequence of edit ended by a special symbol end.
an edit can be one of two edit operations insert andmodify .esec fse august athens greece qihao zhu zeyu sun yuan an xiao wenjie zhang kang yuan yingfei xiong and lu zhang .edits edit edits end .edit insert modify .insert insert hlstatement .modify modify id of an ast node with a nts the same nts as the above nts .
any nts in hl copy id of an ast node with the same nts the original production rules in hl .
hlidentifier placeholder identifiers in the training set hl stands for host language .
nts stands for non terminal symbol .
hlstatement is the non terminal in the grammar of the host language representing a statement.
hlidentifier is the non terminal in the grammar of the host language representing an identifier.
figure the syntax of edits rule defines the syntax of insert operation.
the insert operation inserts a newly generated statement before the faulty statement.
as shown in rule the insert operation has one parameter which is the statement to insert.
here hlstatement refers to the non terminal in the grammar of the host language that represents a statement.
this non terminal could be expanded into a full statement or a copy operation that copies a statement from the original program or a mixture of both.
this behavior will be explained later in rule .
rule defines the syntax of modify operation.
the modify operation replaces an ast subtree in the faulty statement with a new ast subtree.
the modify operation has two parameters.
the first parameter is the id of the root node from the ast subtree to be replaced.
the id of a node is defined as the order of a node in the pre order traversal sequence e.g.
the 6th visited node has the id of .
the second parameter is an ast subtree whose root node has the same symbol i.e.
the root node cannot be changed.
in this way the replacement ensures syntactic correctness.
to ensure that there is an actual change the subtree to be replaced should have more than one node i.e.
the root node should have a non terminal symbol.
for both insert andmodify we need to generate a new ast subtree.
it is noticeable that in many patches the ast subtree being inserted or modified is not completely original some of its subtrees may be copied from other parts of the program.
taking advantage of this property copy operation is introduced to further reduce the patch space.
rule defines the syntax of this operation.
it is a metarule applied to any non terminal symbol of the host language.
for any non terminal symbol in the host language we add a production rule that expands it into a copy operation.
the original production rules for this non terminal are also kept so that when generating the edits the neural network could choose to directly generate a new subtree or to copy one.
thecopy operation has one parameter which identifies the root node of the ast subtree to be copied.
the ast subtree can be selected from the faulty statement or its context.
in our current implementation we allow copying from the method surrounding the faulty statement.
also to ensure syntactic correctness the root localdeclarationleftrightmethodinvocationnameeditseditifstatementleftendobjecttype implicitproto interfacetype.getimplicitprototype if interfacetype.getimplicitprototype null return rightmethodinvocationid ofnon termqualifierinsertliteral nullinterfacetypegetimplicitprototypeconditionreturnstatementfigure example of insert operation closure node of the subtree to be copied should have the same non terminal symbol as the symbol being extended.
finally rule introduces placeholder into the grammar.
normally the grammar of a programming language uses a terminal symbol to represent an identifier.
to enable the neural network to generate concrete identifiers as well as the placeholder we change identifier nodes into non terminals which expand to either placeholder or one of the frequent identifiers in the training set.
in our current implementation an identifier is considered frequent if it appears more than times in the training set.
when applying the edits the placeholder tokens are replaced with feasible identifiers within the context.
we first collect all identifiers in the current projects by performing a lexical analysis and collect the tokens whose lexical type is hlidentifier the symbol representing an identifier in the host language.
then we filter identifiers based on the following criteria the identifier is accessible from the local context and replacing the placeholder with the identifier would not lead to type errors.
the remaining identifiers are feasible identifiers.
figure and figure show two example patches represented by edits.
the patch in figure inserts an ifstatement and the conditional expression contains a method invocation that is copied from the faulty statement.
the patch in figure replaces the qualifier of a method invocation with another invocation where the name of the method is a placeholder to be instantiated later.
theorem .
.
the edited programs are syntactically correct.
proof.
it is easy to see that the theorem holds by structural induction on the grammar of the edits.
first the requirement on the host programming language ensures that inserting a statement before another statement is syntactical correct.
second when replacing a subtree with modify the root symbol of the subtree remains unchanged.
third the new subtree in insert andmodify is generated by either using the grammar rules of the host language or copying a subtree with the same root symbol.
finally instantiating a placeholder ensures syntactic correctness because we only replace a placeholder with a token whose lexical type is hlidentifier .
a syntax guided edit decoder for neural program repair esec fse august athens greece returncavailablelocaleset.contains locale returnavailablelocaleset .contains locale methodinvocationqualifiercaailablenamecontainsargsmemberlocaleeditseditnodeidplaceholderendmodifyqualifiermethodinvocationavailablelocaleset figure example of modify operation lang table providers for non terminals component associated non terminals rule predictor edits edit insert hlidentifier any nts in hl subtree locator modify tree copier any nts in hl .
generation of edits since the choice of expanding a non terminal may depend on the local context or a previous choice we use providers to provide choices and estimate their probabilities.
our current implementation has three types of providers.
table shows these providers and their associated non terminals.
for non terminals edits edit insert and hlidentifier the rule predictor is responsible for providing choices and estimates the probability of each production rule.
the rule predictor consists of a neural component and a logic component.
after the neural component assigns the probability for each production rule the logic component resets the probability of rules whose left hand side is not the corresponding non terminal to zero and normalizes the remaining probabilities.
formodify the subtree locator is responsible for providing the choices.
the subtree locator estimates the probability of each ast subtree with a size larger than in the faulty statement.
the choice of a subtreetmeans that we should expand modify intomodify id nts where idis the root id of tandnts is the root symbol of t. for any non terminal in the grammar of the host language note that hlidentifier is a terminal symbol in the host language both the rule predictor and the tree copier are responsible to provide the choices.
the tree copier estimates the probabilities of each ast subtree with a size larger than in the method surrounding the faulty statement.
the choice of a subtree tmeans that we should expand the non terminal into copy id where idis the root id oft.
similar to the rule predictor the tree copier employs a logic component after the neural component to reset the probabilities of subtrees whose root symbols are different from the non terminal symbol being expanded.
finally the decider assigns a probability to each provider.
the decider also includes a similar logic component which resets the probability of a provider to zero if that provider is not responsiblefor the current non terminal symbol.
for example if the symbol being expanded is modify the decider resets the probability of rule predictor and tree copier to zero.
model architecture the design of our model is based on the state of the art syntaxguided code generation model treegen .
it is a tree based transformer that takes a natural language description as input and produces a program as output.
since our approach takes a faulty statement and its context as input and produces edits as output we replace the components in treegen for encoding natural language description and decoding the program.
figure shows an overview of our model.
the model performs one step in the edit generation process which is to predict probabilities of choices for expanding a non terminal node.
beam search is used to find the best combination of choices for generating the complete edits.
the model consists of four main components thecode reader that encodes the faulty statement and its context.
theast reader that encodes the partial ast of the edits that have been generated.
thetree path reader that encodes a path from the root node to a non terminal node which should be expanded.
theedit decoder that takes the encoded information from the previous three components and produces a probability of each choice for expanding the non terminal node.
among them the ast reader and the tree path reader are derived from treegen where the code reader and the edit decoder are newly introduced in this paper.
in this section we focus on describing the latter two components in detail.
.
code reader the code reader component encodes the faulty statement and the method surrounding the faulty statement as its context where the faulty statement is localized by a fault localization technique.
it uses the following three inputs.
ast traversal sequence.
this is a sequence of tokens following the pre order traversal of the ast c1 c2 cl where ciis the token encoding vector of the ith node embedded via word embedding .
tag embedding.
this is a sequence of tags following the same pre order traversal of the ast where each tag denotes which of the following cases the corresponding node belongs to .
in the faulty statement .
in the statement before the faulty statement .
in the statement after the faulty statement or .
in other statements.
each tag is embedded via an embedding lookup table.
we denote the tag embedding as t1 t2 tl.
ast based graph.
considering that the former two inputs do not capture the neighbor relations between ast nodes in order to capture such information we treat an ast as a directional graph where the nodes are ast nodes and the edges link a node to each of its children and its left sibling as shown in figure b .
this graph is embedded as an adjacent matrix.
the code reader uses three sub layers to encode the three inputs above as discussed in the following sections.
.
.
self attention.
the self attention sub layer encodes the ast traversal sequence following the transformer architecture to capture the long dependency information in the ast.esec fse august athens greece qihao zhu zeyu sun yuan an xiao wenjie zhang kang yuan yingfei xiong and lu zhang selfattentiongating layer treeconvlayer ast traversalsequence codereadertagembeddingselfattentiongating layer codeattentionlayer rulesequence astreadertreeconvlayerastattentioncodeattentiondenserule encoding n2xn3xn1xrulepredictortreecopiermutationlocatordecidertree pathreaderedit decoderprobability of choicesast based graphtreepathprovidersast based graphfaulty methodpartial ast figure overview of recoder given the embedding of the input ast traversal sequence we use position embedding to represent positional information of the ast token.
the input vectors are denoted as c1 c2 cl and the position embedding of ith token is computed as p i 2j sin pos 100002j d p i 2j cos pos 100002j d wherepos i step jdenotes the element of the input vector andstep denotes the embedding size.
after we get the vector of each position it is directly added to the corresponding input vector where ei ci pi.
then we adopt multi head attention layer to capture non linear features.
following the definition of vaswani et al .
we divide the attention mechanism into hheads.
each head represents an individual attention layer to extract unique information.
the single attention layer maps the query q the keyk and the value vinto a weighted sum output.
the computation of the jth head layer can be represented as headj softmax qkt dk v wheredk d hdenotes the length of each extracted feature vector andq kandvare computed by a fully connected layer from q k v. in the encoder vectors q kandvare all the outputs of the position embedding layer e1 e2 el.
the outputs of these heads are further joint together with a fully connected layer which is computed by out wh wherewhdenotes the weight of the fully connected layer and out denotes the outputs a1 a2 alof the self attention sub layer.
.
.
gating layer.
this sub layer takes the outputs of the previous layer and the tag embedding as input.
gating mechanism as defined in treegen is used in this layer.
it takes three vectors named q c1 c2as input and aims to corporate c1withc2based on q. the computation of gating mechanism can be represented as c1 i exp qt ikc1 i dk c2 i exp qt ikc2 i dk hi c1 ivc1 i c2 ivc2 i c1 i c2 i methodinvocationqualifiercaailablenamecontainsargsmemberlocalemethodinvocationqualifiercaailablenamecontainsargsmemberlocale a b figure example of ast based graph wheredk d his a normalization factor hdenotes the number of heads and ddenotes the hidden size qiis computed by a fullyconnected layer over the control vector qi kc1 i vc1 iis computed by another fully connected layer over vector c1 kc2 iandvc2 iare also computed by the same layer with different parameters over the vector c2.
in our model we treat the outputs of the self attention sub layer a1 a2 alasqandc1 and the tag embedding t1 t2 tlasc2.
thus embedding of the ith ast node of the gating layer can be represented as ui gating ai ai ti .
.
.
tree conv layer.
this sub layer takes the output uiof the previous layer and the ast based graph g represented as an adjacency matrix as input.
we adopt a gnn layer to process the inputs and the encoding of the neighbors riis computed as gi wg rj gan rirjuj wherewgis the weight of a fully connected layer and ais a normalized adjacency matrix of g. the computation of the normal operation proposed by kipf and welling is represented as a s 1as whereais the adjacency matrix of g ands1 s2 are the diagonal matrices with a summation of ain columns and rows.
then the encoding of the neighbors is directly added to the input vector.
in summary the code reader has n1blocks of these three sublayers and yields the features of the input ast t1 t2 tl which would be used for the ast reader and the tree path reader.
.
ast reader the ast reader encodes the partial generated ast of the edit which has the same structure as the one in treegen .
this component takes three inputs derived from the partial generateda syntax guided edit decoder for neural program repair esec fse august athens greece ast as code reader.
the rule sequence is represented as real value vectors and then is fed into a self attention layer.
we then integrate the output of the self attention layer with the rule encoding via a gating layer as equation .
we also adopt a multi head attention layer over the outputs of the code reader and the gating layer like the decoder encoder attention in transformer.
finally we use a tree convolutional layer like the code reader to extract the structural information.
more details of this component can be found in the publication of treegen .
.
tree path reader the tree path reader encodes the information of the non terminal node to be expanded and is the same as the one in treegen .
this component represents the non terminal node as a path from the root to the node to be expanded and transforms the nodes in this path into real value vectors.
as shown in figure these vectors are fed into two attention layers like equation .
finally a set of two fully connected layers where the first layer has a gelu activation function are followed to extract features for edit decoder.
more details of this component can be found in the publication of treegen .
we denote the output of the tree path reader as d1 d2 dt.
.
edit decoder the edit decoder takes the output of the tree path reader d1 d2 dt with lengtht as input.
these vectors are produced by the tree path reader and contain the encoded information from all the inputs the faulty statement with its surrounding method the partial ast generated so far and the tree path denoting the node to be expanded.
.
.
provider.
as mentioned before there are currently three types of providers rule predictor tree copier and subtree locator.
these providers take the vector d1 d2 dtas input and output the probability of choices for different non terminals.
rule predictor .the rule predictor estimates the probability of each production rule in the grammar of edits.
the neural component of this decider consists of a fully connected layer.
the output of the fully connected layer is denoted as s1 s2 st. then these vectors are normalized via softmax which computes the normalized vectors pr pr pr tby pr k m exp sm k nr j 1exp sj k wherenrdenotes the number of production rules in the grammar of edits and mdenotes themth dimension of the vector pr k i.e.
the production rule with id m .
in particular invalid rules whose lefthand side is not the corresponding non terminal are not allowed in our approach.
for these rules the logic component resets the output of the fully connected layer to .
thus the probability of invalid rules will be zero after softmax normalization.
tree copier .this provider is designed for any non terminal symbol in the grammar of edits to choose a subtree in the local context.
the neural component is based on a pointer network .
the computation can be represented as i vttanh w1di w2t where tdenotes the output of the code reader and v w1 w2denote the trainable parameters.
the logic component also resets to if the root symbol of the corresponding subtree is different from the symbol being expanded.
these vectors are then normalized via softmax as equation .
we denote the normalized vector as pt pt pt t. subtree locator .this component outputs an id of the subtree in the faulty statement for not terminal symbol modify in the grammar of edits.
the computation of this component is the same as the tree copier.
we denote the output vector of this provider as ps ps ps t .
.
decider.
for these three providers the decider estimates the probability of using each provider.
the neural component also takes the output of the tree path reader d1 d2 dt as input and produces the probability of using each provider as output.
the computation can be represented as i wdi b wherew andbdenote the parameters of a fully connected layer.
the logic component resets to if the corresponding provider is not responsible for the symbol being expanded following table .
then the vectors are normalized via softmax as equation .
we denote the normalized vectors as 1 2 t. the final probability of each choice can be computed as oi r ipr i t ipt i s ips i where oiwill be the probability vector of the next production rule atith step during patch generation.
.
training and inference during training the model is optimized by maximizing the negative log likelihood of the oracle edit sequence and we do not use the logic component in the providers and decider.
here we would like recoder to learn the distribution of the rules handled by the logic component.
if the logic component is present at training recoder would not be trained for a large portion of rules.
during inference these unseen rules would distort the distribution of output making recoder fail to distinguish the part of rules that it is supposed to distinguish.
when generating edits inference starts with the rule start start edits expanding a special symbol start toedits .
the recursive prediction terminates if every leaf node in the predicted ast is a terminal.
we use beam search with a size of to generate multiple edits.
generated edits may contain placeholders.
though the number of choices for a single placeholder is small the combination of multiple placeholders may be large.
therefore we discard patches containing more than one placeholder symbol during beam search.
.
patch generation and validation patches are generated according to the result of the fault localization technique.
in our approach the model described above is invoked for each suspicious faulty statement according to the result of fault localization.
for each statement we generate valid patch candidates via beam search when beam search generates a valid patch we remove it from the search set and continue to search for the next patch until candidates are generated in totalesec fse august athens greece qihao zhu zeyu sun yuan an xiao wenjie zhang kang yuan yingfei xiong and lu zhang for that statement.
after patches are generated the final step is to validate them via the test suite written by developers.
the validation step filters out patches that do not compile or fail a test case.
all generated patches are validated until a plausible patch a patch that passes all test cases is found.
experiment setup we have implemented recoder for the java programming language.
in this and the next sections we report our experiments on repairing java bugs.
.
research questions our evaluation aims to answer the following research questions rq1 what is the performance of recoder?
to answer this question we evaluated our approach on the widely used apr benchmark defects4j v1.
and compared it with traditional and dl based apr tools.
rq2 what is the contribution of each component in recoder?
to answer this question we started from the full model of recoder and removed each component in turn to understand its contribution to performance.
rq3 what is the generalizability of recoder?
to answer this question we fisrt conducted an experiment on additional bugs from defects4j v2.
.
to our best knowledge this is the first apr approach that has been applied to this benchmark.
we compared recoder with the previous two best performing apr approaches for single hunk bugs on defects4j v1.
namely tbar and simfix .
in addition we also applied recoder to other two benchmarks quixbugs andintroclassjava via repairthemall framework which allows the execution of automatic program repair tools on benchmarks of bugs.
.
dataset the neural network model in our approach needs to be trained with a large number of history patches.
to create this training set we crawled java projects created on github between march and march and downloaded commits where the commit message contains at least one word from the following two groups respectively fix solve bug issue problem error .
commits were filtered to include only patches that modify one single statement or insert one new statement corresponding to two types of edits that our approach currently supports.
to avoid data leak we further discarded patches where the project is a clone to defects4j project or a program repair project using defects4j or the method modified by the patch is the same as the method modified by any patch in defects4j v1.
or v2.
based on ast comparison.
there are valid patches left after filtering which are further split into two parts for training and for validation.
we used four benchmarks to measure the performance of recoder.
the first one contains bugs from defects4j v1.
which is a commonly used benchmark for automatic program repair research.
the second one contains additional bugs from defects4j v2.
.
defects4j v2.
introduces new bugs comparedwith defects4j v1.
.
however gzoltar the fault localization approach used by our implementation as well as two baselines tbar and simfix failed to finish on the project gson so we excluded bugs in gson from our benchmark.
the third one contains bugs from quixbugs which is a benchmark with buggy algorithmic programs specified by test cases.
the last one introclassjava consists of buggy java programs generated from theintroclass benchmark for c. .
fault localization in our experiment two settings for fault localization are used.
in the first setting the faulty location of a bug is unknown to apr tools and they rely on existing fault localization approaches to localize the bug.
recoder uses ochiai implemented in gzoltar which is widely used in existing apr tools .
in the second setting the actual faulty location is given to apr tools.
this is to measure the capability of patch generation without the influence of a specific fault localization tool as suggested and adopted in previous studies .
.
baselines we selected existing apr approaches as the baselines for comparison.
since recoder generates only single hunk patches patches that only change a consecutive code fragment we chose traditional single hunk apr approaches that are often used as baselines in existing studies jgenprog hdrepair nopol capgen sketchfix tbar fixminer simfix prapr avatar .
in particular tbar correctly repairs the highest number of bugs on defects4j v1.
as far as we know.
we also selected dl based apr approaches that adopt the encoderdecoder architecture to generate patches and have been evaluated on defects4j as baselines.
four approaches have been chosen based on this criteria namely sequencer codit dlfix and coconut .
for defects4j v1.
the performance data of the baselines are collected from existing papers .
for additional bugs from defects4j v2.
two best performing single hunk apr approaches on defects4j v1.
tbar and simfix are adapted and executed for comparison.
for quixbugs and introclassjava we directly choosed the apr tools used in repairthemall and dl based apr tools which have experimented on these two benchmarks as baselines jgenprog rsrepair nopol and coconut .
we also directly used the result reported in the original papers .
.
correctness of patches to check the correctness of the patches we manually examined every patch if it is the same with or semantically equivalent to the patch provided by defects4j as in previous works .
to reduce possible errors made in this process every patch is examined by two of the authors individually and is considered correct only if both authors consider it correct.
the kappa score of the experiment is .
.
furthermore we also publish all the patches generated by recoder for public judgment2.
2the source code of recoder generated patches and an online demo are available at syntax guided edit decoder for neural program repair esec fse august athens greece this time regulartimeperiod.default time zone locale.getdefault this time zone locale.getdefault figure chart a bug fixed by recoder with modify operation if result !
null if result !
null !result.isnotype figure closure a bug fixed by recoder with placeholder generation .
implementation details our approach is implemented based on pytorch with parameters set ton1 n2 n3 i.e.
the code reader contains a stack of blocks the ast reader contains a stack of blocks and the decoder contains a stack of blocks respectively.
embedding sizes for all embedding vectors are set to and all hidden sizes are set following the configuration of treegen .
during training dropout is used to prevent overfitting with the drop rate of .
.
the model is optimized by adam with learning rate .
.
these hyper parameters and parameters for our model are chosen based on the performance on validation set.
we set a hour running time limit for recoder following existing studies .
experimental results .
performance of recoder rq1 .
.
results without perfect fault localization.
we first compare recoder with the baselines in the setting where no faulty location is given.
results as table shown only include baselines that have been evaluated under this setting.
as shown recoder correctly repairs bugs and outperforms all of the previous single hunk apr techniques on defects4j v1.
.
in particular recoder repairs .
bugs more bugs than the previous state of the art apr tool for single hunk bugs tbar.
within our knowledge recoder is the first dl based apr approach that has outperformed the traditional apr approaches.
we show a few example patches that are possibly generated with the help of the novel techniques in recoder.
as shown in figure chart is a bug that dlfix fails to fix.
the correct patch only changes a parameter of the method invocation while dlfix needs to generate the whole expression.
by contrast recoder generates amodify operation that changes only one parameter.
figure shows a bug only repaired by recoder.
this patch relies on a projectspecific method isnotype and thus cannot be generated by many of the existing approaches.
however recoder fixes it correctly by generating a placeholder and then instantiating it with isnotype .
.
.
results with perfect fault localization.
table shows the result where the actual faulty location is provided.
as before only baselines that have been evaluated under this setting are listed.
recoder still outperforms all of the existing apr approaches including traditional ones.
also compared with recoder using ochiai for fault localization this model achieves a .
improvement.
the result implies that recoder can achieve better performance with better fault localization techniques.
project names c chart cl closure l lang m math moc mockito t time figure degree of complementary.
.
.
degree of complementary.
we further investigate to what extent recoder complements the three best performing existing approaches for fixing single hunk bugs tbar simfix and dlfix.
figure reveals the overlaps of the bugs fixed by different approaches.
as shown recoder fixes unique bugs when compared with three baselines.
moreover recoder fixes unique bugs compared with simfix tbar and dlfix respectively.
this result shows that recoder is complementary to these best performing existing approaches for single hunk bugs.
.
contribution of each component rq2 to answer rq2 we conducted an ablation test on defects4j v1.
to figure out the contribution of each component.
since the ablation test requires much time we only conducted the experiment based on ochiai fault localization scenario.
table shows the results of the ablation test.
we respectively removed three edit operations modify copy and insert as well as the generation of placeholders.
as shown in the table removing any of the components leads to a significant drop in performance.
this result suggests that the two novel techniques proposed in recoder are the key to its performance.
.
generalizability of recoder rq3 the results on defects4j v2.
quixbugs and introclassjava are shown in table and table .
as shown on defects4j v2.
all three approaches repair a smaller proportion of bugs suggesting that the additional bugs on defects4j v2.
are probably more difficult to repair.
nevertheless recoder still repairs most bugs compared with baselines in total achieving .
bugs improvement over tbar and .
bugs improvement over simfix.
we believe that the considerable performance drops of tbar and simfix are caused by their design tbar is based on validated patterns onesec fse august athens greece qihao zhu zeyu sun yuan an xiao wenjie zhang kang yuan yingfei xiong and lu zhang table comparison without perfect fault localization project jgenprog hdrepair nopol capgen sketchfix fixminer simfix tbar dlfix prapr avatar recoder chart closure lang math time mockito total p .
.
.
.
.
.
.
.
.
.
.
.
in the cells x y x denotes the number of correct patches and y denotes the number of patches that can pass all the test cases.
table comparison with perfect fault localization project sequencer codit dlfix coconut tbar recoder chart closure lang math time mockito total table ablation test for recoder on defects4j v1.
project modify subtreecopy insert placeholder recoder chart closure lang math time mockito total table comparison on the additional bugs project used bugs bug ids tbar simfix recoder cli clousre jacksondatabind codec collections compress csv jacksoncore jsoup jxpath total defects4j v1.
which may not generalize beyond the projects in defects4j v1.
simfix relies on similar code snippets in the same project but new projects in defects4j v2.
are much smaller and thus the chance to find similar code snippets become smaller.
on the other hand recoder is trained from a large set of patches collected from different projects and is thus more likely to generalize to new projects.
on quixbugs and introclassjava recoder also repairedtable comparison on introclassjava and quixbugs project used bugs jgenprog rsrepair nopol coconut recoder introclassjava quixbugs total figure adequacy of the dataset.
bugs and .
bugs more bugs on introclassjava and quixbugs over the baselines respectively further confirming the effectiveness and generalizability of recoder.
discussion .
adequacy of dataset to understand the adequacy of our training data we trained recoder on differently sized subsets of the original training dataset and calculate the loss over the single hunk bugs in the defects4j v1.
dataset.
for each subset we train models with different random seeds and report the average performance of these models.
figure shows the sensitivity analysis of dataset size for recoder.
as shown the loss and the diversity both decrease with the increase of training subset size.
therefore the performance of recoder may further increase if more training data are provided.a syntax guided edit decoder for neural program repair esec fse august athens greece .
limitations of recoder recoder shares limitations common to most apr approaches bugs to be fixed should be reproducible by failing test cases.
effective fault localization is needed to identify the faulty statement.
recoder also shares the limitations common to dl based approaches the performance would degrade if the training set and the testing set have different distributions.
related work dl based apr approaches .apr has been approached using different techniques such as heuristics or random search semantic analysis manually defined or automatically mined repair patterns and learning from source code .
for a thorough discussion of apr existing surveys are recommended to readers.
a closely related series of work is apr based on deep learning.
the mainstream approaches treat apr as a statistical machine translation that generates the fixed code with faulty code.
deepfix learns the syntax rules via a sequence to sequence model to fix syntax errors.
nguyen et al .
and tufano et al .
also adopt a sequence to sequence translation model to generate the patch.
they use sequence to sequence nmt with a copy mechanism.
chakraborty et al .
propose codit which learns code edits by encoding code structures in an nmt model to generate patches.
li et al .
propose a tree lstm to encode the abstract syntax tree of the faulty method to generate the patches.
coconut as proposed by lutellier et al .
adopts cnn to encode the faulty method and generates the patch token by token.
compared to them our paper is the first work that aims to improve the decoder and employs a syntax guided manner to generate edits with the provider decider architecture and placeholder generation.
multi hunk apr .most of the above approaches generate singlehunk patches patches that change one place of the program.
recently saha et al .
propose hercules to repair multi hunk bugs by discovering similar code snippets and applying similar changes.
since lifting single hunk repair to multiple hunk repair is a generic idea and can also be applied to recoder we did not directly compare recoder with the multi hunk repair tools in our evaluation.
nevertheless though recoder only repairs single hunk bugs we notice that it still outperforms hercules by repairing more bugs on the defects4j v1.
including all projects from v1.
except for mockito the dataset hercules has been evaluated on.
dl based code generation .code generation aims to generate code from a natural language specification and has been intensively studied during recent years.
with the development of deep learning ling et al .
propose a neural machine translation model to generate the program token by token.
being aware that code has the constraints of grammar rules and is different from natural language yin and neubig and rabinovich et al .
propose to generate the ast of the program via expanding from a start node.
to integrate the semantic of identifiers ocor proposes to encode the identifiers at character level.
to alleviate the long dependency problem a cnn decoder and treegen a treebased transformer are proposed to generate the program.
inthis paper we significantly extend treegen to generate the edit sequence for program repair.
dl based code edit generation .several existing dl based approaches also use the idea of generating edits on programs .
tarlow et al .
view a program as a sequence of tokens and generate a sequence of token editing commands.
brody et al .
view a program as a tree and generate node editing or subtreeediting commands.
dinella et al .
view a program as a graph and generate node editing commands.
compared with our approach there are three major differences.
first the existing approaches are not syntax guided and treat an edit script as a sequence of tokens.
as a result they may generate syntactically incorrect edit scripts and do not ensure syntactic correctness of the edited program.
on the other hand our approach introduces the provider decider architecture and successfully realizes the syntax guided generation for edits.
second none of the existing approaches support placeholder generation and thus are ineffective in generating edits with project specific identifiers.
third the editing commands they use are atomic and are inefficient in representing large changes.
for example to insert a variable declaration in our approach there is one insert operation insert int var .
however the existing approaches have to represent this change as a sequence of insertions where each insertion inserts one token.
threats to validity threats to external validity .mainly lie in the evaluation dataset we used.
first though our approach applies to different programming languages so far we have only implemented and evaluated it on java so future work is needed to understand its performance on other programming languages.
second though we have evaluated on defects4j v2.
quixbugs and introclassjava it is yet unknown how our approach generalizes to different datasets .
this is a future work to be explored.
threats to internal validity .mainly lie in our manual assessment of patch correctness.
to reduce this threat two authors have independently checked the correctness of the patches and a patch is considered correct only if both authors consider it correct.
the generated patches also have been released for public assessment.
conclusion in this paper we propose recoder a syntax guided edit decoder with placeholder generation for automated program repair.
recoder uses a novel provider decider architecture to ensure accurate generation and syntactic correctness of the edited program and generates placeholders for project specific identifiers.
in the experiment recoder achieved .
improvement bugs over the existing state of the art apr approach for single hunk bugs on defects4j v1.
.
importantly recoder is the first dl based apr approach that has outperformed traditional apr techniques on this benchmark.
further evaluation on three other benchmarks shows that recoder has better generalizability than some state of the art apr approaches.esec fse august athens greece qihao zhu zeyu sun yuan an xiao wenjie zhang kang yuan yingfei xiong and lu zhang