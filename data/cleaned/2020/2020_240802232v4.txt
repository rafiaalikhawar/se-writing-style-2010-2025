specrover code intent extraction via llms haifeng ruan yuntong zhang abhik roychoudhury national university of singapore hruan yuntong abhik comp.nus.edu.sg abstract autonomous program improvement typically involves automatically producing bug fixes and feature additions.
such program improvement can be accomplished by a combination of large language model llm and program analysis capabilities in the form of an llm agent.
since program repair or program improvement typically requires a specification of intended behavior specification inference can be useful for producing high quality program patches.
in this work we examine efficient and low cost workflows for iterative specification inference within an llm agent.
given a github issue to be resolved in a software project our goal is to conduct iterative code search accompanied by specification inference thereby inferring intent from both the project structure and behavior.
the intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches.
our approach specrover is built on the open source llm agent autocoderover.
in an evaluation on the full swe bench consisting of github issues it shows more than improvement in efficacy over autocoderover.
compared to the open source agents available our work shows modest cost .
per issue in resolving an average github issue in swe bench lite.
the production of explanation by specrover allows for a better signal to be given to the developer on when the suggested patches can be accepted with confidence.
specrover also seeks to demonstrate the continued importance of specification inference in automated program repair even as program repair technologies enter the llm era.
i. i ntroduction automatic programming has long been an aspiration of software engineering research.
it has inspired research in topics like program synthesis and repair.
in recent times automatic programming from natural language specifications has become somewhat more realistic due to the emergence of tools like github copilot.
at the same time the automatically generated code from large language models llms suffers from errors and vulnerabilities and needs to be improved.
for this reason there has been a recent research focus on autonomous program improvement.
the problem setting for autonomous program improvement involves solving of github issues which would typically involve bug fixes or feature additions.
though these tools are employed on manually written software projects such as the recently proposed swe bench they hold the promise of high quality trustworthy code construction from llms.
starting with the ai software engineer devin from a stealth startup recently several autonomous program improvement tools such as autocoderover have been proposed for automatically solving github issues such as bug fixes or feature additions .
by combining these technologies joint first authors ordered alphabetically.with code generation via github copilot one can envision trustworthy code construction from llms.
program improvement or program repair typically requires capturing developer intent to guide the process.
however there is no formal specification of developer intent.
the naturallanguage description of the developer intent is usually only available at a higher level it captures the intended behavior of the entire software system.
however to improve or repair specific components of a software system where the error might have been localized one needs to infer specifications of the different components.
a successful approach to program repair may thus involve specification inference where by carefully analyzing the artifacts of the buggy program such as program executions we can infer snippets of the intended program behavior.
the works on semantic program repair extract specifications via symbolic analysis of the given tests.
indeed the existing literature on program repair uses a given test suite as developer intent and hence is focused on avoiding test data over fitting.
the works on semantic repair alleviate the over fitting concern by inferring symbolic specifications from tests.
nevertheless for the general problem of program improvement the buggy program may or may not be accompanied by tests.
moreover symbolic analysis based program repair has a high entry barrier for developers.
for these reasons recently autonomous program improvement using large language models llms has been studied.
in this work we explore the role of program specifications thoroughly in llm guided autonomous software engineering workflows.
to understand the intent of the developer and perform program improvement based on inferred specifications we build our work on the publicly available autocoderover tool.
the reason for this choice is strategic.
in essence autocoderover takes the position that the structure of the program also captures a coarse encoding of the developer intent and it tries to glean intent by analyzing and searching over the program structure it performs code search on the project structure for fix localization.
thus to build a workflow where we conduct high quality program improvement via iterative specification inference we choose to build our work on autocoderover.
our work looks into various sources of specifications such as function level code summaries and testcases apart from program structure.
the core contribution thus lies in distilling the various specifications coming from different sources into a single patch.
we thus present specrover a progeny of autocoderover which conducts and exploits more powerful specification 1arxiv .02232v4 dec 2024inference.
starting from a github issue it conducts code search guided by the program structure as in autocoderover.
however in the process of the code search as it visits classes methods it also calculates and deposits the specifications of the classes methods which would have allowed for remediation of the observable error thereby capturing intended program behavior.
the specifications gathered from the code search are deposited along with generated tests to a reviewer agent.
the reviewer agent studies the specifications generated tests and natural language requirements to guide the patching.
more importantly the reviewer agent produces evidence of confidence in the reported patch.
contributions the core contributions of our work on specrover can be summarized as follows.
specification inference we examine the role of specification inference in llm guided autonomous software engineering.
our work suggests iterative specification inference to guide patching in llm oriented program repair workflows.
once the understanding of developer intent is accomplished via iterative specification inference patch construction is a natural by product of the inferred specification.
suggesting patches with confidence we design a reviewer agent for code review which reconciles specifications tests and natural language requirements.
the reviewer agent can be seen as conducting a comprehensive patch validation.
the reviewer agent can produce evidence of correctness of automatically generated fixes such as explanation of patch reproducer test and the accumulated specifications from different code elements.
these evidence can be maintained along with the automatically generated patches to track future regressions.
experimental evidence our tool shows high efficacy solving .
issues in full swe bench and on swe bench lite.
we also balance other needs from llm agents such as low cost .
per issue and supporting higher precision recall.
our tool is available open source in zenodo and will be publicly released upon acceptance.
ii.
m otivating example we now present the specrover approach via an example.
the github issue involved in this example is scikit learn155351 shown in figure 1a.
in the issue two relevant code snippets are provided.
according to the issue report both snippets had worked without problem on an older version of scikit learn and it is expected that they continue to work on the current version.
however on the current version the first snippet now crashes.
the associated error information indicates that the crash occurred when scikit learn mistakenly tries to convert a non numeric array element into a float.
to resolve the issue specrover first identifies buggy program locations by exploring the program and retrieving relevant code.
in this example the identified buggy method is check array shown in figure 1b.
the method performs regression in input validation of clustering metrics from sklearn.metrics.cluster import mutual info score import numpy as np x np.random.choice size .astype object mutual info score x x valueerror could not convert string to float b while x np.random.choice size mutual info score x x works with a warning?
this worked in .
.
without a warning as i think it should a issue statement.
def check array array ... dtype orig getattr array dtype none ... if dtype orig is not none and dtype orig.kind o if input is object convert to float.
dtype np.float64 ... if dtype numeric and array.dtype.kind o array array.astype np.float64 ... b buggy function.
intended behavior the check array method should validate the input array without attempting to convert arrays with dtype object containing non numeric strings to float.
... if the array contains non numeric strings it should skip the conversion to float and proceed with the validation.
... c inferred intended behavior of the buggy function.
the patch attempts to handle non numeric strings by catching a v alueerror during the conversion to float.
however it does not correctly handle the case where the input contains non numeric strings.
the patch still tries to convert the array to float later in the code which results in the same v alueerror.
the patch does not resolve the issue.
d reviewer feedback on initial patch.
a sklearn utils validation.py b sklearn utils validation.py def check array array ... ... if dtype orig is not none and dtype orig.kind o if input is object convert to float.
dtype np.float64 if input is object check if it contains non numeric strings try array.astype np.float64 dtype np.float64 except valueerror contains non numeric strings skip conversion dtype none ... def check array array ... ... array array.astype np.float64 try array array.astype np.float64 except valueerror contains non numeric strings skip conversion pass ... e final patch generated by specrover.
fig.
scikit learn description and specrover artifacts.
two conversions of string to float.
the two conversions are the root cause of the failure reported in the issue resulting in an exception when the involved string is non numeric.
specrover inferred a summary of intended behavior of the method check array shown in figure 1c which serves as a specification of how the method should be modified.
next the identified buggy method and its intended behavior are passed to our patching agent which will write patches for the method.
at the first attempt the patching agent wrote a partial patch which only contains the first of the two hunks in figure 1e i.e.
the patch only catches exceptions for the first conversion.
without further rectification of the patch the issue 2could not be resolved.
to vet the patch for potential mistakes the initial patch is then passed to the reviewer agent.
apart from the patch the reviewer agent also takes a reproducer test that reproduces the issue.
the reviewer agent then runs the reproducer test on both the original program and the program repaired by the initial patch.
by referring to the error information the patch and the issue statement the reviewer agent is able to give the feedback as shown in figure 1d.
the feedback correctly indicates that the initial patch does not resolve the issue and can be rectified by catching exceptions for the other string conversion.
finally the feedback is passed to the patching agent which writes the correct patch shown in figure 1e.
in this example we illustrated how our reviewer agent provides feedback on an incorrect patch for our patch writing agent.
the feedback leads to a later rectification of the patch and explains clearly why the initial patch is incorrect.
iii.
m ethodology a. overview problem setup given a software codebase cand a natural language problem description d the goal is to automatically derive a patch p i.e.
a set of code modifications to c such that the patched codebase c satisfies the requirements ind.
one example setup for dis github issues in which the issue description contains requirements for fixing a bug or adding a new feature.
in this paper we drive autonomous program improvement with the help of program specifications.
we try to acquire an understanding of the intended program behavior the specification which then allows us to produce high quality patches that successfully resolve github issues.
beyond producing high quality patches an additional benefit of understanding the specification is that it also serves as evidence as to why the patch is correct.
the evidence holds promise in terms of easing software maintenance and engendering trust in the code.
the key novelty of our approach lies in how we infer and utilize various forms of specifications.
for an overview of all the specifications involved we depict the general workflow of our approach specrover in figure .
in this figure the inferred specifications are highlighted in yellow.
we also highlight in blue all the llm agents present in the workflow.
as shown in figure the specifications are inferred in an iterative fashion the agents take in specifications possibly produced by other agents and in turn infer new forms of specifications.
this iterative process generates a variety of specifications until a patch is generated and deemed correct by one of our agents that vets generated patches.
specifically as shown in figure the following specifications are inferred in sequence in specrover which is given as input an issue statement and a software codebase.
the input issue statement is passed to a reproducer agent which writes a reproducer test that reproduces the program fault reported in the issue.
the reproducer test its execution results along with the issue statement and the codebase are passed to a contextretrieval agent.
the context retrieval agent explores the program codebase and identifies the relevant code to the issue.
it eventually decides on a set of buggy locations that need patching.
the context retrieval agent also produces a function summary for every function encountered while exploring the program code.
a function summary describes the intended behavior of a function in natural language with respect to the current issue being solved.
the buggy locations together with their corresponding function summaries are passed to a patching agent which tries to write a patch to resolve the issue.
the patch and the reproducer test are passed to a reviewer agent for scrutiny.
the reviewer agent will produce a reviewer feedback if the patch is deemed incorrect the patching agent will take in the reviewer feedback and try writing another patch.
the reviewer feedback is a naturallanguage explanation of why the patch is incorrect and how it can be rectified.
likewise a reviewer feedback for the reproducer test will be produced at the same time if the test is deemed incorrect.
if a patch is deemed correct by the reviewer agent and there is an existing regression test suite available for the program the patch will be checked via the regression test suite.
if there is no regression the patch will be accepted as the final patch.
otherwise if some of the regression tests fail we will retry the workflow up to a predefined number of times.
finally after multiple retries there can be multiple patch candidates.
a selection agent is invoked to select one final patch among the patch candidates and give the reason why this patch is selected.
the final patch the reason for selection and optionally the rest of the candidate patches will be sent to the user.
among the specifications the function summary and reviewer feedback are unique to specrover and unexplored by other llm agents.
these specifications have boosted the effectiveness of specrover in resolving software issues because they fully exploit different kinds of software artifacts the function summary exploits the program code behavior and the reviewer feedback exploits both the code and the test.
b. function summary specification from program in this section we first describe how the context retrieval agent gathers code context for the software issue to be resolved.
we then discuss how specrover transforms the user intent in the issue description into program specifications for shorter code elements such as functions.
existing llm programming agents typically employ a context retrieval step to collect necessary code context related to the given issue from a large codebase.
specrover follows the general architectural design of programming agents in its context retrieval stage as shown in figure .
specrover conducts context retrieval by providing a set of apis to the llm for exploring the codebase.
the llm agent invokes the retrieval apis to investigate the relevant code snippets in 3patchescontext retrival agentreproducer agent patching agentreviewer agentissue statementcodebase patch regression?retriesinferred spec agent reviewer feedback patch okpatch not ok yes noinputs reproducer testoptional input buggy locationsfunction summaries regression test suite final patchpatchesselection agent patchesfig.
overall workflow of specrover.
search class foo search method bar baz search .... no yesissue statementreproducer test sufficient?context retrieval function summariescode snippetsfunction summariescode snippets function summariesbuggy locationsfunction summariesbuggy locations fig.
context retrieval in specrover.
the program.
the retrieved code forms the code context for the current to be resolved issue which can contain definitions of the relevant classes and methods.
after each round of retrieval api invocations the llm agent takes the code context collected so far and decides whether the context is sufficient for understanding and resolving the problem.
if the context is deemed sufficient the retrieval process will end and the agent will decide on a set of buggy locations which are sent to the patching agent for repairing.
otherwise the retrieval process continues until a predefined threshold count is reached.
one key novelty in specrover is the explicit extraction offunction summaries while collecting code snippets during context retrieval.
in specrover whenever a new code snippet is retrieved with an api and sent to the context retrieval agent we explicitly prompt the agent to analyze the intended behavior of this code snippet in the current problem context.
the intended behavior or specification is a concise naturallanguage summary of how a function should behave to meet the requirements specified in the high level problem description.
this function level summary of intended behavior serves as a local specification to guide the patch construction.
thesystem level intended behavior specification given by the user i.e.
the issue description is often on how the program should behave rather than how a unit function should behave.
so we usually do not have the intended behavior of a function.
although the issue description may provide some direction on the intended behavior of a function it is usually not sufficient to guide the patching agent.
on the other hand the extracted function level specification capturing the intended behavior of the function serves as a more direct guide to the patching agent.
instead of giving a set of bug locations l1 l2 ... l n to the patching agent to modify specrover gives the pairs of bug locations and their corresponding local specification l1 spec l2 spec ... ln spec n .
the patching agent can then refer to the specifications of intended behavior and modify code at the function level so as to achieve this intended behavior .
intuitively our approach decomposes the repository level issue solving task to several function level code modification tasks in which each functionlevel task has a natural language specification.
llms have been extensively studied for function level coding tasks and have shown promising results in function level benchmarks such as humaneval and mbpp .
therefore this task decomposition helps the patching agent of specrover which then has to solve smaller and more manageable tasks.
c. reviewer feedback reconciling specifications another kind of specification inferred by specrover is the reviewer feedback.
to be more precise the reviewer feedback can be called a meta specification it is a reflection on the specifications inferred in previous steps.
concretely given a patch and a reproducer test the reviewer agent in specrover will provide feedback which includes a binary decision of whether the patch and the reproducer test are correct respectively and an explanation for the decisions.
the reviewer feedback contributes to our specification inference practice in two ways.
first it makes the specification inference iterative.
the reviewer feedback will be passed back to the patching agent and the reproducer agent leading to improved patches and reproducer tests.
second it reconciles the 4patch and the test.
in this way errors that are not obvious when examining the two separately can be revealed and rectified.
what makes the reviewer feedback important is the absence of a suitable test suite.
if a test suite was available for checking whether the issue has been resolved patch correctness could be easily decided.
in reality however issues occur when the program already passes the accompanying regression tests which means that a high quality test suite to check a generated patch for the given issue is usually not available.
to mitigate the lack of an issue revealing test case specrover writes a reproducer test via the reproducer agent.
however this test alone is not sufficient for deciding patch correctness.
this is because the reproducer test can be incorrect due to the non determinism of the llm i.e.
the test may fail a patch that actually conforms to the user intent.
besides the reproducer test can also be incomplete description of intent i.e.
a patch may pass the test without completely resolving the issue.
the limitation of the reproducer test derives from the fact that tests are a precise yet incomplete specification.
to overcome the limitation we make the observation that the natural language issue statement is ambiguous in nature yet often contains richer information.
therefore supplementing the test with an understanding of the issue statement is likely to help decide patch correctness.
this is accomplished in the reviewer agent of specrover which considers the issue statement as well as test to vet patch candidates.
beyond deciding on patch correctness the more important aspect of the reviewer feedback is an explanation of the decision made which will help the patch agent rectify an incorrect patch.
further for a correct patch the explanation will help the user understand and accept the patch.
the user can merge the reviewer feedback into the software together with the patch for future reference which will help software maintenance in the long run.
an issue has been submitted.
engineer a has written a reproduction test for the issue.
engineer b has written a patch for the issue.
y our task is to decide whether the created patch resolves the issue.
note both the test and the patch may be wrong.
here is the issue ... here is the test written by engineer a ... here is the result of executing the test on the original buggy program ... here is the patch written by engineer b ... here is the result of executing the test on the patched program ... think about whether the test correctly reproduces the issue and whether the patch resolves the issue.
fig.
template prompt for the reviewer agent.
our reviewer agent generates the feedback in two steps.
first the original program and the patched program are run on the reproducer test.
in both runs execution information including the output and the exit code are collected.
the reviewer agent then provides the llm with these execution information along with the issue statement and the reproducer test.
the llm is prompted to decide whether the patch and the test are correct respectively and to provide explanations for both decisions.
figure shows the template of the prompt.
d. patch selection as shown in figure a patch approved by the reviewer agent is checked through a regression test suite which servesas an oracle for whether the patch breaks existing functionality of the program.
however in the setting of resolving github issues the regression test suite can be an inaccurate oracle meaning that they can reject correct patches which resolve the issue.
this is because the correct patch may inevitably modify existing functionalities of the program while resolving the issue thus causing some of the existing regression tests to fail.
for example if the patch needs to modify the signature of an existing function fin order to resolve an issue regression tests that invoke fwill now fail.
since the correct patch can be rejected by the regression tests we employ a patch selection process at the end of the workflow to select the most promising patches among the rejected candidate patches.
during the final patch selection phase specrover goes beyond the test cases and employs a selection agent to choose a patch based on the natural language issue description.
all candidate patches that failed some tests are presented to the selection agent together with the issue description.
the selection agent is instructed to analyze the root cause of the issue think about how the issue can be possibly resolved and select a patch that best addresses the issue.
this natural language guided patch selection can recover correct patches that are mistakenly filtered out by an inaccurate test suite.
it exploits the natural language issue report as that captures the most up to date intents from users developers.
while making a choice among the candidate patches the selection agent also explicitly states a reason why it chooses a particular patch among the candidate patches.
this reason for selection can be given as evidence together with the final patch.
e. evidence specrover is designed to not only generate a patch to resolve the issues in software repositories but also to provide the inferred specifications as evidence for why a patch was selected.
specifically along with the final patch the following artifacts can be the outputs of specrover as well buggy locations and their intended behaviors.
the reproducer test written by the reproducer agent.
the reason why the final patch was approved by the reviewer agent if the patch is approved by the reviewer and the regression test suite .
the reason why the final patch is selected by the selection agent if there are multiple candidate patches which cannot be differentiated by the tests .
the benefits of generating evidence are threefold.
first these artifacts can guide the llm agents in constructing higher quality patches as discussed in section iii b and iii c. second the natural language artifacts can assist the developers in understanding the auto generated patches more quickly.
before examining the actual patch developers can gain a preliminary understanding of the changes by reviewing the reasons for approval or selection and the intended behaviors for the buggy locations.
last but not least the evidence can be integrated into the software repository and can evolve with it.
the developers can integrate the reproducer test for this issue as part of the test suite of the program.
reasons for patch 5approval selection can become parts of the commit message when the auto generated patch is committed to the repository.
overall we propose specrover as a programming agent that not only automatically generates code improvements but also produces evidence that enriches the software system lifecycle.
iv.
e xperimental setup we address the following research questions rq1 what is the efficacy of specrover in resolving issues?
rq2 what level of confidence can developers get from the patch and specifications produced by specrover?
rq3 what is the quality of the specification produced by specrover as evidence?
a benchmark we evaluate the efficacy of specrover on swe bench a widely used benchmark for autonomous program improvement consisting of real world github issues.
for each issue the only input for specrover is the issue statement and the buggy codebase.
note that the regression test suite used by specrover is part of the buggy program we do not access any code or test that is added by the developer in the fixed version of program.
b baselines and evaluation metrics for rq1 we compare with the state of the art systems that target the repositorylevel issue solving task.
in our comparison we include all the open source software engineering agents which have reported results on swe bench.
the baseline tools include autocoderover .
autocoderover employs a set of program structure aware apis to gather relevant code context.
it optionally integrates debugging techniques such as spectrum based fault localization to sharpen the context.
swe agent .
swe agent designs an agent computer interface which defines the possible actions taken by an agent to edit code navigate the codebase and execute tests.
appmap navie .
navie uses a retrieval augmented generation rag based approach to construct the code context and performs an explicit planning step before generating code changes .
opendevin .
opendevin s codeactagent tackles the tasks by having a general action space where the agent is allowed to execute arbitrary python and bash commands inside a sandbox environment.
aider .
aider constructs a repository map which helps the llm to understand the repository context.
it also uses the regression test suite as a harness to retry the task.
moatless tools .
moatless tools builds an agentic loop that functions as a finite state machine and transitions between states.
it focuses on building a set of good tools for the agent instead of relying on the agent for reasoning.
agentless .
agentless is a concurrently pursued currently unpublished arxiv report which employs a fixed twophase approach of localization and repair without allowing the llm to decide on actions or utilize tools.
we report pass efficacy on swe bench for all tools.
for each issue swe bench has a set of acceptance tests written by the developers to evaluate the patch correctness.
these specrover agentless moatless opendevin aiderfig.
number of uniquely resolved issues by the top performing open source tools on swe bench lite.
acceptance tests are not used by the tools when generating patches.
we follow the official swe bench evaluation criteria if the single patch generated by a tool passes the swebench acceptance tests for the issue the issue is considered as resolved.
c implementation and parameters we implemented specrover on top of the autocoderover codebase and reuse its context retrieval apis.
we implemented new features unique to specrover such as function summary extraction as part of the context retrieval process.
other unique features such as patch reviewing and selection are implemented as new llm agents.
specrover supports multiple llms as backend.
in our experiments we used the claude .
sonnet as the main foundation model and only switch to openai gpt 4o for a task if that task encounters an api error when invoking the claude remote apis.
we set maximum retries on regression test suite failures to be .
d randomness of llms llms are inherently random in its output generation which may threaten the validity of llm based coding agents including specrover.
we address this by setting the model temperature to so that the model output is more deterministic.
e manual inspection of results for a better understanding of the experimental results we perform manual inspection to obtain certain data e.g.
patch overfitting rate.
all manual inspections were independently conducted by two authors and subsequently cross checked.
discrepancies were resolved through consultation with a third colleague from our group.
v.evaluation a. rq1 overall efficacy of task resolving we first evaluate the overall efficacy of specrover in resolving repository level tasks.
we report the efficacy of specrover on both swe bench consisting of real world github issues and swe bench lite which is a subset of swe bench consisting of issues .
for the baseline tools we compare with their corresponding reported efficacy.
if a tool supports different configurations e.g.
different llms as the backend we compare with the configuration with the highest efficacy.
6table i comparison of efficacy efficiency cost on swe bench and swe bench lite.
tool llm resolved avg.
time s avg.
cost efficacy on swe bench size autocoderover gpt .
.
swe agent gpt .
.
appmap navie gpt4o .
specrover sonnet .
gpt 4o .
.
efficacy on swe bench lite size swe agent gpt .
.
autocoderover gpt .
.
appmap navie gpt 4o .
opendevin sonnet .
.
.
aider gpt 4o opus .
moatless tools sonnet .
.
.
agentless gpt 4o .
.
specrover sonnet .
gpt 4o .
.
indicates data is not publicly available.
result table i shows the efficacy of issue resolving in both swe bench and swe bench lite.
overall specrover achieves the highest efficacy among all the open source tools in both swe bench and swe bench lite.
in swe bench lite compared to the previously top performing group of tools which resolved approximately to of the issues specrover improved the efficacy to .
this efficacy improvement is also evident in the full swe bench where specrover improved the efficacy from .
to .
.
figure illustrates the number of uniquely resolved issues by specrover and other tools in swe bench lite.
for clarity this figure includes only the top five performing tools from table i. specrover uniquely resolved issues the highest number of uniquely resolved issues among all the tools.
among the uniquely resolved issues specrover resolved six of them by generating only one patch demonstrating that the inferred function summary can effectively guide the llm to generate correct patches.
for the other six issues specrover deemed the first generated patch as incorrect from the reviewer agent and the regression test suite.
in this case the patches are iteratively refined based on the reviewer feedback and the test results and eventually the correct patch is selected at the end of the workflow.
data memorization an issue that can bias the evaluation of llm agent generated patches is data memorization .
data memorization occurs when an llm deals with a program that exists in its training set.
to evaluate the risk of data leakage we specifically count the patches generated by specrover that are syntactically identical to the ground truth patches.
our counting shows that in swe bench lite specrover generated the ground truth patch only for issues accounting for of the resolved issues.
the result shows that data memorization occurs very infrequently for specrover.
time and cost we also report the average time taken and average costs for each issue in table i. for each tool we include the time and cost statistics in table i if these information was publicly reported or can be calculated from their publiclyavailable execution traces.
on average specrover costs .
usd to generate patches for each issue in swe bench lite achieving the highest efficacy with a relatively low cost.
we further investigate the issues resolved by specrover in swe bench lite.
for the resolved issues specrover only costs .
usd per issue to generate the correct patch suggesting that the resolved issues requires less retries and less api calls to the llm in general.
time wise specrover spends an average of seconds i.e.
.
minutes on each issue which includes the time for executing the reproducer and the regressions tests in the project.
according to a recent study most developers accept automated program repair tools which takes less than minutes .
specrover requires approximately minutes which we deem acceptable.
patch correctness a patch that passes a given test suite is not necessarily correct because the test suite is an incomplete specification.
this problem is known as patch overfitting in program repair.
for a more accurate evaluation of the generated patches we manually compared the specrover generated patch with the developer patch for each resolved issue.
our manual investigation confirmed that .
of the patches that pass the test suites are semantically equivalent to the ground truth patches.
we also observed that out of the overfitting patches i.e.
the patches that pass the test suite but are not semantically equivalent to ground truth modify the same methods as the ground truth patch and only the specific modification is semantically different from the ground truth.
this implies that even overfitting patches produced by specrover are useful in indicating a fix location.
adding up the semantically equivalent patches and the overfitting patches that have the correct fix location a total of over of the test passing patches prove useful for issue resolution.
reasons for failure we examined the tasks that are not resolved by specrover in swe bench lite.these failure cases break down into three cases ambiguous issue description as labeled by swe bench 7verified .
incorrect fix location .
incorrect code modification .
case is hard or impossible to solve without further information.
case is more frequent than case implying that specrover can potentially have significant improvement by leveraging more fix localization techniques.
for case in most such cases we observed that the gist of the patch generated by specrover is actually correct but some code detail is wrong.
to reduce this kind of failures more test generation techniques can be leveraged within our agent.
b. rq2 utility of autonomous se confidence in results although the efficacy in resolving issues is an important aspect of autonomous program improvement it is not the sole purpose of such a technique.
rather the efficacy is a means to an end to reduce human effort in software maintenance.
to this end a program improvement technique must not only have high efficacy but also minimize the effort required of an end user to use the technique.
the effort is related to two metrics signal to noise ratio i.e.
the ratio of correct to incorrect patches presented to a user and the difficulty of examining each auto generated patch that is suggested.
we have designed specrover to reduce both of these efforts.
first to reduce the number of incorrect patches that a user may examine we use the reviewer agent to decide the correctness of the generated patch and the reproducer test.
the user can choose to examine the generated patch only when both the patch and the reproducer test are deemed correct by the reviewer agent.
the accuracy of the reviewer decisions are measured in rq2.
second to make it easy for a user to examine each patch specrover provides a variety of evidence to help understand the patch as discussed in section iii e. the quality of the evidence will be discussed in rq3.
there can be four different scenarios when the reviewer decision is viewed in relation to the actual correctness of the patch.
for convenience we say a patch is accepted when the reviewer agent decides that both the generated patch and the reproducer test are correct.
with this we discuss the following four scenarios table ii reviewer decisions on swe bench lite.
category tasks tp tn fp fn total accuracy tp tn total .
precision tp tp fp .
recall tp tp fn .
tp true positive patch is accepted and correct tn true negative patch is not accepted and incorrect fp false positive patch is accepted but incorrect fn false negative patch is not accepted but correct.
precision swe agentaidermoatlessagentlessspecrover .
.
.
.0comparison of precision on swe bench litefig.
comparison of patch precision on swe bench lite.
if specrover does not use regression tests for patch validation precision reduces only slightly to .
.
table ii lists the frequency that each scenario occurred in our experiment.
the table counts in tasks in swebench lite for which a reproducer test was generated.
in the table we also calculate the accuracy precision and recall of the reviewer decisions.
out of the tasks there are tp s and tn s i.e.
as many as .
accuracy of the reviewer decisions were consistent with the actual correctness of the patch.
the recall was also over meaning that the majority of the generated correct patches were recognized by the reviewer agent.
a metric of particular interest to program improvement techniques is the precision.
the precision is defined as tp tp fp i.e.
the proportion of correct patches in all the patches offered by a technique.
it is directly related to user effort required to examine generated patches.
for specrover the precision is .
as calculated in table ii.
to put the precision in perspective we compare the precision of specrover with that of other baseline tools in figure .
the precision of the baseline tools is the same as their pass efficacy reported in table i since these tools indiscriminately present every generated patch to a user.
as can be seen in figure the precision of specrover is higher than .8x that of agentless which has the second highest precision.
the high precision of specrover indicates a much lower cognitive load imposed on the user compared to other techniques.
moving forward we suggest paying attention to agent precision.
c. rq3 quality of specifications produced in this section we investigate the quality of evidence generated by specrover.
the high quality evidence allows a developer to easily integrate auto generated patch into an existing codebase.
function summaries we manually investigated the natural language specifications generated by specrover for swe bench lite issues.
to obtain some approximation of ground truth for the generated specifications we extracted the titles of pull requests that developers wrote when they fixed each issue.
the pr title usually summarizes the fixes made in the pr in one sentence.
in this investigation we include only the issues for which specrover generated a patch at the same methods as the developer s patch.
moreover we 8skipping runxfail breaks pytest.mark.skip location reporting pytest versions .
.x current master when pytest.mark.skip skipif marks are used to skip a test for example import pytest pytest.mark.skip def test skip location none assert the expected skip location reported should point to the item itself and this is indeed what happens when running with pytest rs skipped test it.py unconditional skip however adding pytest rs runxfail breaks this skipped src pytest skipping.py unconditional skip the runxfail is only about xfail and should not affect this at all.
hint the bug is in src pytest skipping.py the pytest runtest makereport hook.
a issue statement.
def pytest runtest makereport item item ... ... elif item.config.option.runxfail pass don t interfere elif call.excinfo and isinstance call.excinfo.value ... ... lines elif not rep.skipped and xfailed ... lines elif ... calculate the line number b buggy code snippet.
a src pytest skipping.py b src pytest skipping.py def pytest runtest makereport item item ... def pytest runtest makereport item item ... ... elif item.config.option.runxfail ... lines elif not item.config.option.runxfail if call.excinfo and isinstance call.excinfo.value ... ... lines if ... was elif calculate the line number ... lines c correct patch generated by specrover.
the patch correctly addresses the issue by modifying the logic in the pytest runtest makereport function.
it ensures that the skip location is reported correctly regardless of whether the runxfail option is used.
the patch moves the handling of skip location correction outside the conditional block for runxfail which fixes the reported issue.
d reviewer feedback.
fig.
pytest description and specrover artifacts.
exclude the issues whose developer written pr title is too brief e.g.
fixes issue ... .
for the included issues we manually compare the specification generated by specrover for the patched methods with the pr title written by the developer.
for inspected issues the specification generated by specrover covers similar intent to the human written pr title which shows that the generated specification serves as a good summary of the intended behavior.
the generated specifications are generally an extended version of the pr title and additionally describe how the code should be modified.
reviewer feedback we cannot conduct a quantitative examination on the quality of the reviewer feedbacks on the generated patches since there is no ground truth .
therefore we illustrate the quality of the feedbacks with two examples.
a reviewer feedback as summary in the first example we show that the reviewer feedback can serve as a concise summary of a generated patch.
the summary describes the behavior of the patch at a high level.
therefore a developer can understand the generated patch faster by reading the issue wr ong deriv ative kind attribut e i m playing around with the kind attribute.
the following is correct a matrixsymbol a i integral a x i.kind matrixkind numberkind this one is wrong d derivative a x d.kind undefinedkind a issue statement.
buggy locations class derivative in file sympy core function.py intended behavior add a kind property to the derivative class that returns the appropriate kind for derivatives.
this may involve checking the kind of the expression being differentiated and the differentiation variables.
b location and summary.
try a matrixsymbol a i integral a x assert i.kind matrixkind numberkind f expected matrixkind numberkind got i.kind d derivative a x assert d.kind matrixkind numberkind f expected matrixkind numberkind got d.kind except assertionerror as e issue reproduced print stacktrace e raise c generated reproducer test.
a sympy core function.py b sympy core function.py ... property def kind self return self.expr.kind def new cls expr variables kwargs ... d correct patch generated by specrover.
patch is corr ect.
reason the patch correctly adds a kind property to the derivative class that returns the kind of the expression being differentiated.
test is wr ong.
reason the test incorrectly compares the kind attribute to a string matrixkind numberkind .
the kind attribute is an object not a string and should be compared directly to the expected kind object.
e reviewer feedback.
fig.
sympy description and specrover artifacts.
summary before examining the details of the patch.
besides after the developer accepts the patch and decides to merge it into existing code the summary constitutes a good commit message so that the developer does not need to write one.
from a developer s perspective the whole process is very much like reviewing a pull request which is already part of a developer s day to day workflow.
the specific issue involved in this example is pytest .
the issue statement is shown in figure 7a which reports that pytest a python testing framework would miscalculate a line number in its output when an irrelevant option runxfail is enabled.
the bug is caused by the code shown in figure 7b.
as can be seen the calculation of the line number is wrongly placed in a branch that is mutaully exclusive with the runxfail branch.
therefore the calculation is wrongly skipped when the option is enabled.
to resolve the issue specrover was able to locate the relevant code and produce the correct patch.
an abridged version of the patch is shown in figure 7c.
it correctly addresses the issue by moving the line number calculation to a branch unaffected by the runxfail option.
however the 9patch might not be immediately understandable to a developer because it changes as many as lines in the original program though most of the changes just involve the indentation level .
fortunately the understanding of the patch can be eased by the reviewer feedback.
the reviewer agent was able to identify the patch as correct and produced the feedback in figure 7d.
it properly summarized that the patch just moved the calculation to another branch.
using this summary the developer would easily understand the patch and accept it.
b tolerance of incorrect tests another advantage of the reviewer is enhanced tolerance of incorrect automatically generated tests.
we illustrate this advantage with the example of sympy where specrover rejects an incorrectly written test while approving a correct patch.
the issue statement and the buggy location identified by specrover are shown in figure 8a and 8b.
the issue mentioned an unexpected behavior of the kind attribute.
after its context retrieval stage specrover correctly identifies that the buggy location is in the derivative class and that its intended behavior is to have an additional kind property.
figure 8c and 8d show the automatically generated reproducer test and patch for this issue.
in this case the reproducer test is incorrect the assertions compare an object with a string which always evaluate to false .
if this reproducer test is used solely to determine the correctness of the generated patches any patch even a correct one will be rejected.
however since the reviewer agent in specrover simultaneously examines both the reproducer test and the patch without assuming either is correct it is capable of rejecting the reproducer test while approving the patch.
figure 8e shows the reviewer agent s decision and comments towards the test and patch.
the reviewer identifies that the assertions in the reproducer are written incorrectly thereby rejecting the reproducer.
on the other hand the reviewer correctly approves the patch despite the presence of an unreliable test.
the correct patch along with the reasons for rejecting the reproducer can be sent to the developer.
the developer can then integrate the patch into the codebase.
additionally the developer can revise the almost correct reproducer test based on the feedback provided by the reviewer agent.
vi.
c ase study security vulnerability repair although specrover is initially designed to resolve github issues in python repositories it can be easily adapted for program improvements in other application domains and for programs written in other programming language.
we demonstrate how specrover fixes security vulnerabilities in c programs through an example challenge problem from the darpa ai cyber challenge aixcc in .
the aixcc is a two year competition organized by darpa and arpa h to encourage the development of novel cyberreasoning systems to safeguard critical software.
the aixcc has publicly released exemplar challenges where each challenge consists of a software project with a vulnerability in it.
file net tipc crypto.c static bool tipc crypto key rcv struct tipc crypto rx ... ... copy key from msg data skey keylen keylen memcpy skey alg name data tipc aead alg name memcpy skey key data tipc aead alg name sizeof be32 skey keylen ... a buggy code snippet.
bug kasan slab out of bounds in tipc crypto msg rcv 0x398 0x6e0 write of size at addr ff11000004579024 call trace ... kasan report 0xad 0x130 ?
tipc crypto msg rcv 0x398 0x6e0 kasan check range 0x35 0x1c0 memcpy 0x39 0x60 tipc crypto msg rcv 0x398 0x6e0 ... b vulnerability report generated after executing the poc.
file net tipc crypto.c method tipc crypto key rcv intended behavior the tipc crypto key rcv function should .
validat e the message data size and k ey length before performing any memory operations the function should ensure that the message data size and key length are within valid bounds to prevent out of bounds memory access.
.
safe memor y operations the memcpy operations should be performed only if the source and destination buffers are within valid memory bounds.
... c buggy location and function summary by specrover.
a net tipc crypto.c b net tipc crypto.c static bool tipc crypto key rcv ... u32 size msg data sz hdr u8 data msg data hdr unsigned int keylen validate message data size if size tipc aead alg name sizeof be32 pr err s invalid message data size n rx name return false ... validate key length if keylen size tipc aead alg name sizeof be32 pr err s invalid key length n rx name return false spin lock rx lock if unlikely rx skey key gen rx key gen ... pr err s key existed p gen d vs d n rx name d correct patch generated by specrover.
patch is corr ect.
reason the patch addresses the root cause of the slab out of bounds issue by adding validation checks for the message data size and the key length before accessing the data.
this ensures that the memory access is within the allocated bounds preventing the out of bounds write that was causing the crash.
the validation checks are appropriately placed and cover the necessary conditions to prevent the vulnerability without introducing new issues or making significant functional changes.
e reviewer feedback.
fig.
linux kernel exemplar challenge in darpa aixcc.
the task is to have an autonomous system to find and fix the vulnerability.
each exemplar challenge also contains a proofof concept poc input file that triggers the vulnerability so we use this poc to show how specrover can be used to fix the vulnerability after it is detected.
figure shows one exemplar challenge which is a buffer overflow vulnerability in the linux kernel4.
this buffer overflow happens in the linux network4cve re introduced to linux kernel .
.
10ing module for the transparent inter process communication tipc protocol and allows remote attackers to cause denialof service or disclosure of sensitive information.
specifically when the user supplied sizes in the message body are invalid for the received messages a buffer overflow happens with the memcpy call as shown in figure 9a.
this vulnerability has been triggered by a poc which results in a vulnerability report as shown in figure 9b.
specrover fixes this vulnerability by first analyzing the vulnerability report similar to how it resolves github issues by initially examining the issue descriptions.
it conducts context retrieval and decides on the buggy locations and intended behaviors as shown in figure 9c.
even though the vulnerability report only contains the call trace and minimal description of the bug e.g.
slab out of bounds specrover can infer the intended local behavior at the function level.
based on the intended behavior specrover generated the patch in figure 9d which correctly fixes the vulnerability inserting additional checks before the dangerous memory operation.
the reviewer agent approved the patch with the comments shown in figure 9e with which the developers can gain an initial understanding of the patch before closely examining the changed code.
vii.
r elated work automated program repair apr is a well studied research area in software engineering.
given a buggy program p and a test suite t automated program repair attempts to minimally modify pto a program p which passes the given test suite t. apr techniques involve metaheuristic search semantic analysis machine learning or a combination of different techniques.
apr can also be used to rectify automatically generated code from llms see e.g.
.
the recent interest in prompt engineering as well as agent based solutions has somewhat evolved the research in program repair.
llm agents try to combine the power of llm with program analysis and test execution reasoning.
thus llm agents can combine llms with test generation static and dynamic analysis as well as specification inference.
in the recent past lot of llm based approaches have been proposed for solving software issues described in natural language including .
among these our work is thematically closest to the work of autocoderover .
like autocoderover.
we take the position that program modifications like bug fixing are best aided by inference of the developer intent.
autocoderover infers the developer intent only from the software project structure.
in contrast specrover is more general and is capable of inferring specifications from different sources including program structure program behavior tests and so on.
furthermore specrover focuses on giving an explanation of the produced patches.
previous works have studied function level specification inference by means of llms .
however these works either focus on generating specifications for simple onefunction or one class programs or assume that a target function is provided .
in contrast specrover targetslarge programs and does not assume the target functions to generate specifications for are given.
since both tests and patches generated in an autonomous workflow can be unreliable we additionally consider the natural language issue description when judging their correctness.
the judgement process is concretized as the reviewer agent.
compared to the previous works that discover additional specification based on user intent we do not rely on interactive user feedback.
instead we utilize the high level natural language description as additional feedback.
furthermore compared to previous works that evaluate the generated code with an llm based reviewer our reviewer agent is designed for the setup where both natural language instructions and unreliable tests are present.
viii.
p erspectives owing to the growth of llm based automatic programming see for a recent summary there exists interest in autonomous program improvement technologies.
we propose specrover with the perspective of autonomously producing patches which are suggested with confidence thus developers can confidently accept them and come with explanations.
the technical innovations supporting specrover are the specification inference to guide patching and the rigorous vetting of patches via our reviewer agent.
our work on specrover seeks to put the matter of quality of patches produced by llm agents into the research community s attention whereas other works are mostly focusing on the agent efficacy.
moving forward we envision that llm agents will need to improve the precision and recall.
specifically llm agents will need to vet the produced patches.
the vetting needs to be accompanied by sophisticated test generation so that there is a rich test suite to check the produced patches.
thus we need to take the viewpoint of a developer using an llm agent who would be concerned about a efficacy b cost and most importantly c signal to noise ratio in the agent output.
data availability we share full public access of the source code and experimental artifacts of specrover at zenodo.
.
we are continuously improving the efficacy and usability of specrover i.e.
autocoderover v2 .
at the time of paper acceptance nov autocoderover v2 has achieved an efficacy of .
on swe bench lite and .
on swebench verified.
we have also packaged autocoderover v2 as a github bot offering one click issue resolution .
the latest source code experimental results and news updates of autocoderover v2 can be found at our github repository5 and website6.