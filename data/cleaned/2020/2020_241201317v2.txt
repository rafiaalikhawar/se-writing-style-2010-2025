the seeds of the future sprout from history fuzzing for unveiling vulnerabilities in prospective deep learning libraries zhiyuan li jingzheng wu b xiang ling b tianyue luo zhiqing rui yanjun wu institute of software chinese academy of sciences university of chinese academy of sciences key laboratory of system software chinese academy of sciences state key laboratory of computer science institute of software chinese academy of sciences abstract the widespread application of large language models llms underscores the importance of deep learning dl technologies that rely on foundational dl libraries such as pytorch and tensorflow.
despite their robust features these libraries face challenges with scalability and adaptation to rapid advancements in the llm community.
in response tech giants like apple and huawei are developing their own dl libraries to enhance performance increase scalability and safeguard intellectual property.
ensuring the security of these libraries is crucial with fuzzing being a vital solution.
however existing fuzzing frameworks struggle with target flexibility effectively testing bug prone api sequences and leveraging the limited available information in new libraries.
to address these limitations we propose future the first universal fuzzing framework tailored for newly introduced and prospective dl libraries.
future leverages historical bug information from existing libraries and fine tunes llms for specialized code generation.
this strategy helps identify bugs in new libraries and uses insights from these libraries to enhance security in existing ones creating a cycle from history to future and back.
to evaluate future s effectiveness we conduct comprehensive evaluations on three newly introduced dl libraries.
evaluation results demonstrate that future significantly outperforms existing fuzzers in bug detection success rate of bug reproduction validity rate of code generation and api coverage.
notably future has detected bugs across targeted apis including previously unknown bugs.
among these have been assigned cve ids.
additionally future detects bugs in pytorch demonstrating its ability to enhance security in existing libraries in reverse.
index terms fuzzing dl libraries historical bug.
i. i ntroduction artificial intelligence ai continues to lead technological innovation with large language models llms rapidly gaining widespread application .
within the pipeline of ai deep learning dl has made significant strides prompting a growing demand for efficient and versatile programming frameworks.
this demand has led to the development of dl libraries e.g.
pytorch and tensorflow .
despite their comprehensive capabilities these libraries face limitations in scalability flexibility and proprietary control which are critical for tech giants like apple and huawei.
to address these bjingzheng wu and xiang ling are the corresponding authors.limitations and meet the complex requirements of llms these companies invest heavily in developing their own dl libraries.
emerging dl libraries including apple mlx huawei mindspore and oneflow aim to improve performance through enhanced computational efficiency and advanced features.
however their complexity may introduce bugs that pose critical risks in sectors like healthcare finance and autonomous driving .
identifying and addressing bugs in these libraries is essential for the safety of downstream dl systems.
despite high level of attention and adoption these libraries have received in their early development stages there is a noticeable lack of mature and comprehensive testing methodologies.
this gap underscores the need to ensure their security and reliability.
fuzzing has proven highly effective in detecting software bugs by automatically generating test cases that expose unexpected behaviors .
existing fuzzing methods for dl libraries are mainly categorized into api level fuzzing and model level fuzzing .
api level fuzzing focuses on individual api functions to uncover bugs triggered by anomalous api inputs but may miss complex bugs due to its inability to construct intricate api sequences .
modellevel fuzzing tests entire models against inputs that exploit architectural or weight bugs.
although it addresses many limitations of api level fuzzing model level fuzzing covers a limited range of apis .
recent works e.g.
titanfuzz and fuzzgpt leverage llms to address some limitations of api level fuzzing methods.
while api level model level and recent llm based methods have demonstrated excellent performance in detecting bugs within dl libraries applying these methods to newly introduced and prospective dl libraries poses several challenges c1 lack of target flexibility.
existing fuzzing methods primarily designed for pytorch and tensorflow lack the flexibility needed to adapt to new libraries.
applying these methods to new libraries requires extensive resource collection and code modifications which are both time consuming and laborintensive.
despite this fundamental limitation other challenges persist within the existing methods as outlined below.arxiv .01317v2 dec 2024c2 lack of bug prone api sequences.
previous api level methods struggle to effectively test api sequences.
titanfuzz leverages llms to address this limitation.
however in essence titanfuzz s generation and mutation still rely on the inherent capabilities of llms resulting in the random generation of multi api code snippets.
this approach fails to test bug prone api sequences leading to an excessively large search space and limited efficiency in finding bugs.
c3 limited available information.
existing methods utilize documentation open source code snippets and historical bugs to guide the fuzzing process.
however newly introduced and prospective libraries typically only provide documentation sometimes including code examples in their early stages.
the availability of open source code snippets and historical bugs is extremely limited.
therefore documentation becomes the primary resource but existing methods that rely on it such as docter are limited to testing individual apis as mentioned in c2.
c4 lag in knowledge of llms.
despite the rapid updates in llms it typically takes several months for the latest models to incorporate knowledge about newly introduced libraries.
as a result llm based fuzzers e.g.
titanfuzz fuzzgpt and fuzz4all that rely on pre trained models are significantly constrained in their effectiveness.
to overcome these challenges we propose future the first universal fuzzing framework tailored for both newly introduced and prospective dl libraries.
existing libraries contain a wealth of historical bug information including complex and bug prone api sequences that closely mimic real world usage scenarios.
the basic idea of future is to leverage the historical bug information from existing libraries referred to as source libraries to identify bugs in newly introduced and prospective libraries referred to as target libraries .
firstly we design a label specific github spider to crawl issues related to bugs in source libraries.
we extract codes that reproduce these issues as historical bug codes see section iii b .
to bridge the gap between source and target libraries we utilize a universal prompt template to leverage the limited information available in the api documentation and code examples of the target libraries.
using prompts crafted from this template we invoke llms to generate code pairs that illustrate how source and target libraries implement the same api functions.
future then mutates these code pairs to construct datasets for fine tuning.
with the fine tuned llms we generate seed codes by converting historical bug codes from source to target libraries and generating codes that invoke the api of the target libraries see section iii c .
finally we conduct differential testing on three newly introduced libraries to demonstrate the effectiveness of future.
we identify bugs extract bugprone api inputs and test these inputs on the source libraries to unveil undetected bugs see section iii d .
in summary future makes the following contributions universal fuzzing framework we propose future the first universal fuzzing framework for both newly introduced and prospective dl libraries.
this framework significantly reduces the effort required to adapt fuzzingtechniques to any new dl library making it a forwardthinking tool.
additionally future is the first fuzzing method that targets apple mlx.
from history to future and back we establish a code conversion mapping between existing and prospective libraries by fine tuning llms.
this mapping allows us to convert historical bug codes from existing libraries into seed codes for prospective libraries pioneering a fuzzing method from historical insights to future anticipations.
furthermore by leveraging bugs found in prospective libraries future enhances security in existing libraries completing a cycle from future back to history.
bug detection future demonstrates remarkable efficacy by detecting bugs across targeted apis in apple mlx huawei mindspore and oneflow including previously unknown bugs.
among these bugs have been assigned cve ids.
in addition future has identified bugs in pytorch.
we release the details of detected bugs and future implementation at ii.
b ackground and related work a. deep learning and dl libraries in the burgeoning field of ai dl represents a paradigmatic shift employing models that emulate the intricate neural structures of the human brain .
this advanced branch of machine learning ml relies on neural networks to identify subtle patterns in large datasets supporting decision making and eliminating the need for manual feature extraction .
dl libraries serve as pivotal components within dl systems providing tools and interfaces for designing implementing and efficiently training neural models.
they simplify complex mathematical operations and hardware interactions making deep learning more accessible to developers and researchers .
early libraries like theano and caffe established the foundation while pytorch and tensorflow revolutionized the landscape with features like automatic differentiation and adaptive computational graphs accelerating research and model refinement.
recent developments include libraries like apple mlx huawei mindspore and oneflow which optimize performance and scalability for dl tasks.
these libraries enhance computational efficiency in large scale and distributed environments addressing the needs of an era marked by exponentially growing data volumes model complexities and the development of llms.
the widespread deployment of these libraries provides essential infrastructure for developing dl models across various sectors including healthcare autonomous vehicles and finance .
consequently it is vital to maintain rigorous oversight to ensure these libraries meet high standards of quality and security.
b. fuzzing with large language models the emergence of llms has transformed various domains providing unprecedented capabilities in text generation code generation and more .
these capabilitiesapi documentation code example code conversion dataset fine tuned codellama seed codes for target libraries fine tuning and seed code generation test oracle github issues of source libraries label specific spider historical bug codes of source libraries historical bug collection issue title unstable results in sin arcsin arccos calls describe the bug import torch x torch.ones y torch.sin x y torch.arcsin y output y torch.arccos y print y output x1 x.clone .cuda y1 torch.sin x1 y1 torch.arcsin y1 output y1 torch.arccos y1 print y1 output bug description code to reproduce bugcrawl github issues extract codes that reproduce bugs import torch x torch.ones y torch.sin x y torch.arcsin y output y torch.arccos y print y output x1 x.clone .cuda y1 torch.sin x1 y1 torch.arcsin y1 output y1 torch.arccos y1 print y1 output extract information generate code pairs using llm and mutate api inputstarget libraries source library target library code pairs srccode import torch a torch.tensor abs a torch.abs a print pytorch abs a seed import torch n ... abs a torch.abs a ... print pytorch abs a problem convert this code to code that uses the mlx framework solution import mlx.core as mx n ... abs a mx.abs a ...print mlx abs a construct datasets in json format for fine tunning fine tune codellama import mlx.core as mx x mx.ones y mx.sin x y mx.arcsin y output y mx.arccos y print y output mx.set default device mx.cpu y1 mx.sin x y1 mx.arcsin y1 output y1 mx.arccos y1 print y1 output differential testing execute seed codes provide historical bug informationbug nans and infs edge cases crash error checking bug related issue labels instruction generate code that calls the mlx.core.abs api response import mlx.core as mx n ... abs a mx.abs a ...print mlx abs a code generation dataset code conversion and code generationerror messages nan and inf cases tarcode import mlx.core as mx a mx.array abs a mx.abs a print mlx abs a unveil bugs of target libraries and extract bug prone inputs source librariesunveil bugs of source libraries potential bugs inconsistencies ?
crash bugs ?
source libraries target libraries mlx.core.abs abs a array ...... array element wise absolute value.
parameters a array input array.bug prone api inputs test on source libraries prospective dl libraries fig.
overview of future.
future leverages historical bug information from source libraries and available api information from target libraries to realize code pairs generation dataset construction llm fine tuning and seed code generation.
utilizing these seed codes future unveils bugs in target libraries through test oracle.
insights gained from these bugs are used to enhance the security of the source libraries completing a cycle from history to future and back.
are revolutionizing fields such as content creation conversational ai and software testing .
pre trained llms e.g.
gpt and llama serve as the foundational models offering broad capabilities across diverse linguistic tasks due to their extensive training on text data.
general purpose fine tuned llms e.g.
codex and codellama extend these models by focusing on specific capabilities like coding generation making them versatile tools in various applications .
however they sometimes lack the nuanced understanding required for highly specialized tasks.
task specialized fine tuned models such as codellamapython and codellama instruct address this limitation by tailoring general purpose fine tuned models to excel in particular domains.
these models integrate domain specific knowledge during further fine tuning enabling them to solve problems that pre trained and general purpose fine tuned models struggle with such as understanding complex domain jargon or predicting highly specialized outcomes in fields like medical diagnostics or financial forecasting.
integrating llms into the fuzzing process allows fuzzers to explore systems in ways traditional methods cannot investigate potential failure modes and ensure robustness against a wider range of input scenarios .
llms are extensively employed in fuzzing frameworks for compilers protocols and various other domains .
several frameworks haveutilized llms to enhance dl library fuzzing.
titanfuzz is the first approach that directly leverages a general purpose fine tuned llms codex and incoder to generate and mutate dl programs for fuzzing.
however due to limitations in model capability and knowledge lag titanfuzz cannot effectively test complex bug prone api sequences or provide immediate testing for newly introduced libraries.
fuzzgpt demonstrates that llms can be prompted or fine tuned to resemble historical bug triggering programs.
however fuzzgpt is limited by its reliance on historical bug triggering programs from the libraries being tested which is impractical for newly introduced and prospective libraries.
fuzz4all effectively utilizes llms for input generation and mutation across various software systems under test suts however when applied to dl libraries it encounters the same challenges as titanfuzz.
iii.
a pproach a. overview as shown in fig.
future comprises three main phases historical bug collection fine tuning and seed code generation and test oracle.
in the historical bug collection phase section iii b we focus on existing libraries.
to gather historical bug information we design a label specific spider to crawl bug relatedcode examples api name mlx.core.abs code conversion dataset instruction generate code that calls the mlx.core.abs api response import mlx.core as mx n ... abs a mx.abs a ...print mlx abs a code generation dataset seed import torch n ... abs a torch.abs a ... print pytorch abs a problem convert this code to code that uses the mlx framework solution import mlx.core as mx n ... abs a mx.abs a ...print mlx abs a codellama source library target library code pairs target library import mlx.core as mx a mx.array abs a mx.abs a print mlx abs a prompt template specify the task you are a code libraries converter and generator i ll give you the api documentation and code examples of mlx.core.abs .
your task is to provide me code pairs using this api and its equivalent implementations in pytorch and tensorflow.
when constructing api inputs consider ...... api documentation mlx.core.abs abs a array stream none stream device none array element wise absolute value.
...... code examples import mlx.core as mx a mx.array abs a mx.abs a print mlx abs a 3now i have capabilities to implement code conversion between libraries and code generation for target libraries!
source library import torch a torch.tensor abs a torch.abs a print pytorch abs a nan and inf casestarget library infomation extraction prompt construction code pairs generation and mutation fine tuning dataset construction api documentationmlx.core.abs abs a array ...... array element wise absolute value.
parameters a array input array.
fig.
datasets construction and fine tuning.
in the prompt template we emphasize the importance of constructing api inputs that consider nans and infs edge cases and scenarios likely to trigger api error checking and crashes.
due to space constraints these details are omitted in the figure.
github issues retrieving code snippets that trigger bugs.
these snippets serve as historical bug codes.
in the fine tuning and seed code generation phase section iii c we concentrate on newly introduced and prospective dl libraries.
we first collect api documentation and code examples section iii c1 .
then we construct a prompt template to leverage these resources section iii c2 .
by invoking llms with prompts we generate and mutate code pairs that consist of an api call in a target library and its corresponding implementation in a source library section iii c3 .
the obtained code pairs are then converted into datasets to fine tune llms section iii c4 .
with the fine tuned llms we convert the historical bug codes into codes using the target libraries.
these converted codes along with codes randomly generated by fine tuned llms for the target libraries form the seed codes section iii c5 .
in the test oracle phase section iii d we execute the seed codes to perform differential testing on the target libraries identifying potential bugs through abnormal behaviors such as crashes and inconsistencies.
utilizing the bugs detected by future in the target libraries we extract api inputs that are likely to trigger bugs and attempt to detect unaddressed bugs in the source libraries.
b. historical bug collection several existing dl libraries e.g.
pytorch and tensorflow have significant user engagement resulting in thousands of bug reports.
users typically interact with project developers via github issues reporting potential bugs.
these reports include basic descriptions system environment information and code snippets to reproduce the issues.
developers address these issues by committing fixes for verified bugs and labeling them accordingly.
inspired by this process to collect historical bug codes we design a fully automated label specific spider.
this spider parses issue pages categorizes issues by labels retrieves issue contents extracts code snippets and perform preprocessing.
specifically our spider traverses each page of issues retrieving the title and id of each entry.
it subsequently accesses theissue pages to extract code snippets tagged with keywords such as standalone code to reproduce the issue usage example or code example .
these extracted snippets are then processed to enhance usability by importing necessary dependencies and removing non code elements.
these snippets are then saved with directory and file names generated based on the issue s title and id.
to ensure valid filenames special characters are replaced and a numerical suffix is appended in cases of duplicate names.
the saved snippets serve as future s historical bug codes enabling systematic organization and storage for further analysis and examination.
c. fine tuning and seed code generation target library information extraction firstly future extracts api documentation and code examples from the official documentation of target libraries.
for each api the api name name api documentation doc and code examples code are saved in a triplet format apiinfo name doc code .
for apis lacking documentation and referencing other apis future extracts information from the referenced apis to ensure comprehensive coverage.
prompt construction after retrieving apiinfos we devise a universal prompt template to construct prompts.
the constructed prompts consist of three parts specify the task.
this part defines the task to obtain code pairs that consist of an api call in a target library and its corresponding implementation in a source library more details of this part are shown in fig.
1 .
api documentation.
for apis with extensive documentation we streamline the prompts by removing redundant information to stay within the max tokens limit of llms.
code examples.
for code examples that present multiple usage methods for a single api we extract different methods.
by deconstructing the code examples we derive new prompts p from the original prompts p. to be more specific we first use regular expressions to determine whether the name appears multiple times in the code example.
if the name appears only once we directly use the code example to construct the prompt i.e.
p p .
if the name appearsimport torch x torch.ones y torch.sin x y torch.arcsin y output y torch.arccos y print y output x1 x.clone .cuda y1 torch.sin x1 y1 torch.arcsin y1 gives y1 torch.arccos y1 print y1 output fine tuned codellama import mlx.core as mx x mx.ones y mx.sin x y mx.arcsin y output y mx.arccos y print y output mx.set default device mx.cpu y1 mx.sin x y1 mx.arcsin y1 output y1 mx.arccos y1 print y1 output mlx bug that future found the final output should be import mlx.core as mx a mx.array print a output b mx.clip a print b final output converted historical bug codeshistorical bug codes of source libraries generated codes of target librariescode conversion code generation seed codes for target librariesnan and inf cases test oraclefig.
seed code generation.
with the task specialized finetuned llms we perform code conversion and code generation to obtain the seed codes for test oracle.
multiple times we use abstract syntax trees to decompose and reassemble the code example splitting kusage methods into multiple separate code examples thereby decomposing p intop p1 p2 .
.
.
p k where p pandp p .
fig.
2 3 illustrates the incorporation of api documentation and code examples into our prompt template.
generation and mutation of code pairs subsequently we query llms with new prompts p to obtain code pairs cp cp1 cp .
.
.
cp n where nrepresents the number of prompts eventually constructed.
each code pair cpi si ti i .
.
.
n incp consists of source library code sand corresponding target library code snippets t. to manage costs we limit the number of code pairs generated per api.
we then randomly select variables in the api input data and mutate them to random numbers.
assuming that each code pair cpiundergoes mmutations n m mutated code pairs are acquired which can be represented ascp cp11 cp .
.
.
cp 1m .
.
.
cp nm .
by deconstructing code examples generating code pairs and mutating them we acquire multiple code pairs for a single api resulting in more robust data for fine tuning.
dataset construction after obtaining cp we convert them into datasets for fine tuning resulting in two datasets code generation dataset since llms are not pre trained with knowledge of prospective libraries our goal is to equip them to generate code for the target libraries.
therefore we construct the code generation dataset using tas the response providing standard answers for code generation as illustrated in fig.
.
code conversion dataset to equip llms with the capability to convert code snippets from the source libraries to targetlibraries we construct a triplet format dataset using cp .
this dataset designates sas seed and tas solution with the problem explicitly stated as convert this code to code that uses the target library mlx mindspore oneflow .
fine tuning and seed code generation we design a universal fine tuning template to enable users to adapt future to their own requirements with minimal effort.
there are three popular fine tuning techniques for pre trained models fine tuning parameter efficient fine tuning peft and prompt tuning .
we employ the lowrank adaptation lora from peft in the template.
consider w rd kas the weight matrix of llms.
instead of updating wdirectly lora introduces two low rank matrices a rd randb rr k where r min d k .
the weight update is then parameterized as w ab.
during fine tuning the effective weight matrix w is given by w w w w ab the low rank matrices aandbare updated during the finetuning process while the original weight matrix wremains fixed.
the update rule for aandbtypically follows the gradients of the loss function lwith respect to these matrices.
let a b denote the parameters of the low rank matrices.
the gradient descent update step for is given by l where is the learning rate.
by leveraging the low rank structure we reduce the number of trainable parameters and thus decrease the computational cost and memory overhead while still enabling effective fine tuning of the model.
using this fine tuning template we fine tune three separate models using only the code generation dataset only the code conversion dataset and both datasets together.
by fine tuning on these datasets the llms not only learns various code mappings between the source libraries and target libraries but also acquires knowledge of the target libraries.
the task specialized fine tuned llms are then employed for seed code generation detailed in fig.
.
on one hand future leverages the code conversion capability of the finetuned llms to convert historical bug codes his into potential bug codes pot for the target libraries.
in the pot there is a rich and diverse collection of complex and bug prone api sequences that closely mimic real world usage scenarios.
on the other hand future utilizes the code generation capability to generate diverse code snippets gen that call apis of the target libraries.
together pot andgen form the seed codes.
during the acquisition of pot andgen we perform automated preprocessing same as section iii b. even though his has been processed earlier the inherent uncertainty of llms still necessitate this preprocessing to ensure the usability of the seed codes.
this strategy effectively transcends the limitations posed by relying solely on historical bug codes from the source libraries.
it allows future to utilize historical bug information from source libraries which titanfuzz cannot do.
additionally it facilitates the generation of code snippets of target libraries table i github issues and bug codes collected.
source library issue label issue number bug codes pytorchbug nans and infs edge cases error checking crash tensorflow bug total even when models incorporating knowledge of these libraries are unavailable something titanfuzz is unable to achieve.
d. test oracle after obtaining seed codes we implement differential testing focusing on two critical aspects crash bugs.
during the execution of the seed codes we monitor for crashes such as aborts segmentation faults runtime errors and floating point exceptions.
bugs exposed by such crashes may further trigger security vulnerabilities.
inconsistencies.
we scrutinize the execution results across different computational backends cpu gpu for inconsistencies which often indicate underlying bugs.
additionally since each fuzzing seed code corresponds to an implementation in the source libraries following we calculate the euclidean distance to measure the discrepancies between results.
we only consider discrepancies exceeding a predefined threshold tas potential bugs .
only when these discrepancies are reported to and confirmed by developers do we classify them as triggered bugs caused by inconsistencies between the source libraries src libs and target libraries tar libs .
after conducting the test oracle we report and perform a statistical analysis of the detected bugs.
this analysis allows us to summarize and extract bug prone api inputs which are then used to automatically test the apis of the source libraries.
in future there are only two manually involved parts.
the first is extracting bug prone api inputs during the bug reporting process.
the second involves manually inspecting the causes of failures in code pairs generation which constitute a small portion.
this inspection helps us identify documentation issues related to certain apis.
apart from these two parts future is fully automated.
iv.
e valuation a. implementation dl library selection.
we select pytorch v2.
.
and tensorflow v2.
.
as the source libraries for future due to their extensive application.
future can be applied to any newly introduced or prospective dl library.
to assess its effectiveness we focus on three target libraries introduced in recent years apple mlx v0.
.
huawei mindspore v2.
.
and oneflow v0.
.
.
these open source libraries represent the cutting edge in dl library development making them ideal candidates for evaluating future s effectiveness.historical bug collection.
future targets all bug related issues up to march processing a total of issues from pytorch and tensorflow on github.
from these we extract valid historical bug codes as detailed in table i. due to constraints in computational resources and time we use the most recent tensorflow bug codes and all available pytorch bug codes resulting in a total of historical bug codes utilized in this study.
large language models.
future utilizes codellama13b for generating code pairs and fine tuning we select codellama due to its advanced performance in open models and large input context support.
the foundation model and its weights are sourced from hugging face providing a cost effective solution to users as it is available for free.
fine tuning datasets.
we generate code pairs for and apis for mlx mindspore and oneflow respectively.
for each api we generate code pairs which are then mutated times resulting in a fine tuning dataset of over six hundred thousand entries.
fine tuning setups.
during fine tuning we quantize the model s parameters to bits.
for the lora configuration we follow the official tutorial provided by peft .
as for the training parameters the learning rate is set at 3e and the max steps is set to .
we allocate one tenth of the training datasets for validation conducting validations every steps.
b. experimental setup baselines.
since the target libraries of future are newly introduced most existing fuzzing methods are not readily applicable to all of them.
as future is an api level fuzzer we focus on state of the art api level fuzzers such as freefuzz deeprel nablafuzz tensorscope and titanfuzz while also considering model level fuzzers like muffin for a comprehensive understanding.
among these freefuzz and deeprel require extensive data collection and code modifications making them resource intensive for our needs.
muffin limited to backends compatible with keras is not directly applicable to our target libraries without significant manual adaptations.
therefore we select titanfuzz nablafuzz and tensorscope as our baselines and adapt them to test our target libraries.
in addition to achieve a more comprehensive comparison we selected few shot learning as the fourth baseline to verify the effectiveness and rationality of fine tuning in future through comparison.
all baseline fuzzers are modified minimally to ensure compatibility with our target libraries.
we make several main adaptions as below replacing titanfuzz s deprecated model with gpt .
turbo updating api lists to match our targeted apis modifying codes to save generated snippets.
in few shot for a fair comparison we choose few shot codellama w o fine tuning and we adopt chain of thought cot prompting.
for different tasks we provide specific task descriptions along with shot examples from our conversion or generation dataset respectively.
using this context we perform code conversion and generation accordingly.table ii summary of cves detected by future.
all detected cves are found in oneflow and due to the lack of strict parameter checking in apis indicating that oneflow urgently needs to optimize the parameter validation mechanisms to prevent crashes or inconsistencies that could be exploited by attackers.
cve id symptom vulnerable api cve crash zeros ones new ones empty cve crash tensordot cve crash var cve src libs tar libs eye cve src libs tar libs permute cve crash full cve crash scatter scatter add cve crash scatter nd cve crash dot cve crash index select table iii summary of bug detection on target libraries.
totalconfirmedwon t fix unknown known mlx mindspore oneflow total environment.
we use an ubuntu .
server equipped with an intel xeon gold cpu and a v100 32gb gpu.
c. metrics number of bugs.
we report potential bugs detected by future to developers via github.
we count only those bugs labeled as bug as detected by future.
success rate of bug reproduction.
we define suc as converted codes that successfully reproduce the behaviors error messages or outputs in bug reports observed in the original bug codes.
the success rate quantifies the effectiveness of future in code conversion and is calculated as success rate nsuc nhis where nrepresents the number of codes.
validity rate.
for code generation the validity rate is calculated as the ratio of the number of unique valid codes v al to the total number of generated codes all.
a code is valid if it executes without exceptions and invokes the target api at least once.
the validity rate is given as validity rate nv al nall.
api coverage.
we measure api coverage by calculating the proportion of the apis utilized in suc andv al relative to the total number of targeted apis.
specifically apis that appear insuc are denoted as sucapi and apis that appear in v al are denoted as v alapi .
the api coverage is given by api coverage nsucapi nv alapi ntarapi ntarapi where ntarapi represents the total number of targeted apis.
d. research questions to assess the effectiveness of future we conduct studies answering the following research questions pytorch historical bug issue import torch torch.eye torch.eye n m both lead to seg fault source library mlx bug detected by future import mlx.core as mx a mx.eye n m b mx.eye n target library run correctly crash!
from history to future oneflow bug detected by future import oneflow as flow import numpy as np x flow.tensor np.array dtype np.float32 y flow.cast x dtype flow.int8 target library pytorch bug detected by future import torch import numpy as np x torch.tensor np.array float inf float nan y torch.geqrf x source libraryfrom future back to history inconsistencies cpu gpu !
inconsistencies cpu gpu !
a mlx crash converted from pytorch b pytorch inconsistencies cpu gpu triggered by bug prone api inputsfig.
example bugs found by future.
we provide two bug examples to illustrate that future not only uses historical bug information from source libraries to unveil bugs in target libraries but also leverages bugs found in target libraries to identify bugs that still reside in source libraries.
rq1 can future detect real world bugs in newlyintroduced dl libraries?
rq2 how do key components and different settings of future influence its effectiveness?
rq3 what is the fuzzing performance of future compared to the state of the art techniques?
v. r esult analysis a. rq1 bug detection we first investigate the effectiveness of future in bug detection.
future detects bugs across apple mlx huawei mindspore and oneflow with confirmed as previously unknown.
among these bugs have been assigned cve ids as detailed in table ii.
our submissions on github are recognized with five good first issue labels by mlx developers.
additionally we identify bugs in pytorch.
since pytorch is not one of our target libraries we do not provide a detailed analysis of these bugs here.
statistics of bugs on the target libraries are presented in table iii.
fig.
a shows a historical bug code from pytorch.
in this bug excessively large parameters passed to torch.eye trigger a segmentation fault.
future converts this code into code snippets using target libraries.
we find that on mlx mlx.core.eyeoperates correctly when both parameters are set to large values.
however setting only one parameter to a large value triggers a crash.
this issue is labeled as a bug and immediately fixed by mlx developers.
this demonstrates that future can utilize historical bug information from existing libraries to unveil bugs in prospective libraries.
fig.
b illustrates a bug detected by future through its capability to generate random codes for target libraries.
we find that arrays containing nans and infs can trigger manytable iv causes of bug detected on target libraries.
total ec ni ld dbi mpc mlx mindspore oneflow total a causes b symptoms fig.
statistical distribution of causes and symptoms of bugs detected in target libraries using future.
bugs in target libraries.
by extracting bug prone api inputs and testing them on the apis of source libraries we identify thattorch.geqrf exhibits inconsistencies on cpu and gpu.
this issue persists in the latest version of pytorch and has been confirmed as a bug.
this demonstrates that future uses insights from prospective libraries to enhance security in existing ones creating a cycle from history to future and back.
beyond bugs issues detected by future though not classified as bugs are recognized by developers as valuable enhancement issues.
additionally future identifies documentation problems we report them and they have all been addressed in the latest versions of the official documentation.
bug cause analysis.
based on labels of historical bugs and the actual circumstances of bugs detected by future we classify the causes of the detected bugs into five categories nans and infs ni bugs in this category involve the presence of nans or infs in two specific contexts as variables in an input array or as api parameters.
missing parameter checking mpc these bugs result from insufficient or flawed parameter validation within apis allowing parameters that do not meet constraints to pass checks leading to incorrect results or crashes.
edge cases ec these bugs involve bugs in apis when handling boundary values for certain data types.
different backend implementations dbi bugs in this category arise from discrepancies in the implementation of dl libraries across different backends cpu gpu affecting the outputs under identical inputs and parameters.
logic deficiency ld some apis suffer from logic deficiencies preventing them from functioning as intended.
we categorize the bugs detected by future accordingtable v symptoms of bug detected on target libraries.
total crash cpu gpu src libs tar libs mlx mindspore oneflow total to these causes and compile statistics.
table iv details the number of bugs per category and fig.
a illustrates the distribution of each cause within the bugs detected in each target library.
for bugs involving inputs or parameters with nans and infs and where outputs vary across different backends we categorize the cause as ni .
only when a bug does not involve nans and infs and there are discrepancies in outputs across different backends do we classify it as dbi .
similarly when nans and infs used as parameters trigger a bug we prioritize classifying it under ni rather than mpc .
among all bugs ni is the most common cause accounting for .
out of .
mpc is the second most common cause accounting for .
out of suggesting stricter parameter checking is needed during the development of dl libraries.
bug symptom analysis.
we also categorize the symptoms of the bugs based on the design of test oracle.
table v and fig.
b illustrate the distribution of bugs according to symptom categories divided into three types crash this category includes scenarios where seed code execution results in aborts segmentation faults runtime errors floating point exceptions and excessively long execution times without producing results or errors.
inconsistencies cpu gpu these bugs are characterized by different outcomes for the same api executed on different backends despite identical inputs and parameters.
inconsistencies src libs tar libs these bugs are identified when the outcomes of the target library seed codes are consistent across different backends but differ from those of corresponding implementations in the source libraries given the same inputs and parameters.
our findings reveal that oneflow has a higher incidence of bugs manifesting as crashes accounting for .
out of of the total bugs detected in oneflow by future.
this is likely due to inadequate handling of invalid inputs or parameters in oneflow s source code often leading to program terminations.
additionally future finds no crashrelated bugs in mindspore.
we attribute this to mindspore s implementation of stricter parameter checks and more robust error handling mechanisms.
answer to rq1 future detects bugs across mlx mindspore and oneflow with confirmed as previously unknown.
the analysis on bug causes and symptoms provides valuable insights for further research in dl libraries.
b. rq2 impact of key components and different settings to study how each component of future contributes to the overall effectiveness we conduct experiments based on thetable vi evaluation of historical bug collection.
in the brackets following the percentages the numerator and denominator for the success rate are represented by nsucandnhisrespectively for api coverage they are covered api and all targeted api.
the detailed definitions of these metrics can be found in section iv c. the column bug detected shows the number of bugs detected by future using historical bug codes with specific labels.
historical bug codes on source libraries converted codes on target libraries source library issue label success rateapi coverage bug detected mlx mindspore oneflow mlx mindspore oneflow bug .
.
.
.
nans and infs .
.
.
.
pytorch edge cases .
.
.
.
error checking .
.
.
.
crash .
.
.
.
tensorflow bug .
.
.
.
total .
.
.
.
table vii evaluation of different methods for constructing the fine tuning dataset.
success rate validity rateapi coverage mlx mindspore oneflow total future .
.
.
.
.
.
future doc .
.
.
.
.
.
future ex .
.
.
.
.
.
future no .
.
.
.
.
.
1future represents fine tuning with datasets constructed by generating and mutating code pairs.
future doc relies solely on api documentation and future ex directly uses code examples to fine tune codellama.
future no represents the version without fine tuning.
fig.
impact of various mutation times and temperature on success rate validity rate and total api coverage.
four key components historical bug collection dataset construction fine tuning and seed code generation.
historical bug collection we perform code conversion on historical bug codes mentioned in section iv a with the detailed results shown in table vi.
first we observe that issues labeled nans and infs in pytorch provide the most extensive api coverage and are instrumental in bug detection.
this observation confirms our prompt design strategies as discussed in section iii c2.
second we note that the success rate of bug reproduction for tensorflow s historical bug codes is notably low at .
.
this is likely due to the vast number of tensorflow apis many of which are undeveloped in the target libraries after conversion.
despite this the historical bug codes of tensorflow contribute significantly to overall api coverage and assist in detecting bugs within three test libraries.
in summary our experiments demonstrate that utilizing historical bug information from existing dl libraries contributesto detecting bugs in prospective dl libraries.
as the collection of bugs in existing dl libraries continues to accumulate this contribution is expected to become increasingly effective.
among the bugs detected by future using historical bug information involved multi api sequences proving that future effectively leverages bug prone api sequences rather than randomly generating api sequences with llms as seen in titanfuzz.
dataset construction next we examine how constructing and leveraging datasets for fine tuning with different strategies influences the effectiveness of future.
how to construct datasets.
we compare several variants of constructing datasets with different strategies.
table vii shows the performance metrics of these strategies.
we observe that constructing the fine tuning dataset by generating and mutating code pairs significantly enhances the success rate validity rate and api coverage compared to other strategies.
notably the metrics of future ex outperform future doc indirectly confirming that using code snippets as datasets more effectively simulates real world code conversion and generation scenarios.
we evaluate the impact of varying mutation times on the performance metrics shown in fig.
a .
we find that mutating code pairs significantly improves all metrics.
however when increasing mutations beyond the benefits do not justify the additional time and resources needed.
this finding guides us to cap mutations at per code pair.
how to leverage datasets.
to equip llms with the capabilities for code conversion and code generation we construct two specialized datasets.
we first investigate the impact of utilizingtable viii the impact of different dataset combinations used for fine tuning on future s code generation effectiveness.
validity rate total api coverage future .
.
future gen .
.
future conv .
.
future no .
.
1future uses both datasets future gen performs fine tuning with only code generation dataset future conv employs only code conversion dataset and future no indicates no fine tuning at all.
table ix the impact of different dataset combinations on future s code conversion effectiveness.
all the variants of future remain consistent with the configurations in table viii.
success rate total api coverage future .
.
future gen .
.
future conv .
.
future no .
.
different combinations of these datasets on the effectiveness of future s code generation capability.
for each target library we generate random code snippets for evaluation.
table viii presents the impact of different dataset combinations on future s code generation apability.
future gen achieves significantly higher validity rate and api coverage compared to future conv andfuture no indicating that the code generation dataset substantially enhances future s capability to generate relevant and effective code snippets.
next we conduct experiments to evaluate the impact of these different dataset combinations on future s code conversion capability using the same four versions of future described above.
we perform library conversion on the historical bug codes mentioned in section iv a. the experimental results are detailed in table ix.
these results demonstrate that the code conversion dataset can enhance future s success rate in converting code between libraries.
using both of the designed datasets maximizes their benefits affirming their complementary nature.
fine tuning we further assess the effectiveness of finetuning codellama by comparing metrics with and without fine tuning.
table vii presents the performance enhancements achieved through fine tuning.
the results show that future significantly outperforms future no with improvements exceeding threefold across all metrics demonstrating the substantial benefits of further fine tuning on general purpose finetuned llms.
even with a model like codellama 13b which has limited performance future still achieved notable results highlighting the framework s effectiveness regardless of the underlying model s capabilities.
seed code generation the main goal of seed code generation is to implement code conversion and generation accurately.
therefore we examine the impact of the temperature hyperparameter on future s various performance.
fig.
b shows the metrics of future at varying temperature settings.
we observe that with the temperature set to the defaulttable x comparison on api coverage.
mlx mindspore oneflow total future .
.
.
.
titanfuzz .
.
.
.
nablafuzz .
.
tensorscope .
.
few shot .
.
.
.
table xi comparison on code generation.
in the table for individual target library the denominators for the percentages are all .
they are omitted due to space limitations.
mlx mindspore oneflow total future .
.
.
.
future gen .
.
.
.
future conv .
.
.
.
future no .
.
.
.
titanfuzz .
.
.
.
few shot .
.
.
.
value of .
future demonstrates superior capabilities in code conversion and code generation effectively maximizing coverage across targeted apis.
answer to rq2 the effectiveness of future is enhanced by its key components which significantly boost success and validity rates as well as api coverage and bug detection.
c. rq3 comparison with other work in this section we compare future with four baselines titanfuzz nablafuzz tensorscope and few shot learning.
we did not find any confirmed bugs using our baselines.
table x presents the api coverage of all evaluated baselines on our target libraries.
nablafuzz which does not support mlx and mindspore and relies on its proprietary database inaccessible to us has its performance recorded only for oneflow.
similarly tensorscope s design which involves constraints among apis of multiple libraries makes migration to mlx and oneflow challenging with minimal effort.
we observe that future achieves significantly higher api coverage across all target libraries with an increase of .
compared to titanfuzz.
titanfuzz performs relatively well on mindspore and oneflow but poorly on mlx likely due to the training data of gpt .
turbo not including information of mlx.
this highlights future s suitability for prospective libraries where existing fuzzers like titanfuzz may be ineffective.
few shot learning addresses this issue to some extent but its performance still lags significantly behind finetuning.
future achieves a .
improvement on oneflow compared to nablafuzz and outperforms tensorscope on newer versions of mindspore.
titanfuzz first realizes a fully automated framework to perform generation based fuzzing directly leveraging llms.
we conduct experiments comparing the code generation capability of different versions of future with titanfuzz and fewshot learning.
for each target library we generate code snippets.
table xi presents the validity rate of the snippets.
future significantly outperforms titanfuzz in terms of validity rates across all target libraries.
specifically the totalvalidity rate of future reaches .
of titanfuzz.
for mlx the validity rate is more than six times higher than titanfuzz.
titanfuzz s performance is only comparable to future no .
these findings indicate that with the future s code generation capability for newly introduced and prospective dl libraries significantly surpasses that of the state of the art generation based dl library fuzzing method.
few shot learning shows an improved validity rate across all libraries compared to titanfuzz but it still falls short of future.
through manual analysis of some of the code snippets generated by few shot learning we find that the quality of the generated code snippets deteriorate over time with instances where the generated code even failed to adhere to the instructions in the initial prompt.
in contrast after fine tuning each generated code snippet is a new invocation of the fine tuned model which effectively avoids the issues encountered in few shot learning.
answer to rq3 future consistently outperforms baselines in multiple metrics showcasing its superior efficacy and adaptability for newly introduced and prospective libraries.
vi.
t hreats to validity internal in our experiments we employ codellama to generate code pairs based on api documentation and code examples.
given the inherent uncertainties and variability in the performance of llms not all generated code pairs may meet the desired quality standards.
we mitigate this uncertainty through designing rigorous preprocessing and validity checks on the code pairs.
by implementing these checks we aim to reduce the impact of low quality code pairs and maintain the reliability of our experimental results.
external applying future to dl libraries not initially targeted by the framework may compromise its effectiveness particularly if users are unfamiliar with the fine tuning process of llms.
to address this threat we provide a comprehensive fine tuning template that guides users through the adaptation process.
by offering this resource we aim to empower users to effectively customize future to their specific needs with minimal effort thereby enhancing its applicability and robustness across various dl libraries.
vii.
c onclusion in this work we propose future the first universal dl library fuzzing framework designed for both newly introduced and prospective dl libraries.
more specifically future collects historical bug codes from existing libraries finetunes llms with limited available information.
with the historical bug codes and fine tuned llms we generate seed codes and perform differential testing enhancing security in both new and existing libraries.
our evaluation on three newly introduced libraries shows that future significantly outperforms existing fuzzers in multiple dimensions.
notably future has detected bugs across targeted apis including previously unknown bugs.
among these bugs have been assigned cve ids.
our submissions on githubare recognized with five good first issue labels by mlx developers.
additionally future detects bugs in pytorch demonstrating the framework s ability to utilize historical bug information to secure new libraries and enhance existing ones in reverse.
in subsequent research we aim to expand the range of future s source libraries incorporating a broader spectrum of historical bug information from various dl libraries.
additionally we will implement more automated components to complete the cycle from future back to history.
acknowledgment we sincerely appreciate all the anonymous reviewers for their insightful and constructive comments to improve this work.
this paper is supported by the the strategic priority research program of the chinese academy of sciences under no.
xda0320401 the national natural science foundation of china under no.
and the open source community software bill of materials sbom platform under no.
e3gx310201.
this paper is supported by yuantu large research infrastructure.