adaptive fairness improvement based on causality analysis mengdi zhang mdzhang.
phdcs.smu.edu.sg singapore management university singaporejun sun junsun smu.edu.sg singapore management university singapore abstract given a discriminating neural network the problem of fairness improvement is to systematically reduce discrimination without significantly scarifies its performance i.e.
accuracy .
multiple categories of fairness improving methods have been proposed for neural networks including pre processing in processing and postprocessing.
our empirical study however shows that these methods are not always effective e.g.
they may improve fairness by paying the price of huge accuracy drop or even not helpful e.g.
they may even worsen both fairness and accuracy .
in this work we propose an approach which adaptively chooses the fairness improving method based on causality analysis.
that is we choose the method based on how the neurons and attributes responsible for unfairness are distributed among the input attributes and the hidden neurons.
our experimental evaluation shows that our approach is effective i.e.
always identify the best fairness improving method and efficient i.e.
with an average time overhead of minutes .
ccs concepts software and its engineering extra functional properties.
keywords fairness machine learning fairness improvement causality analysis acm reference format mengdi zhang and jun sun.
.
adaptive fairness improvement based on causality analysis.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction neural networks have found their way into a variety of systems including many which potentially have significant societal impact such as personal credit rating criminal sentencing face recognition and resume shortlisting .
while these neural networks often have high accuracy in these classification tasks some concerning fairness issues have been observed as well permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
that is the predictions made by these neural networks may be biased with regard to certain protected attributes such as sex race and gender.
for instance it has been shown that a neural network trained to predict people s income level based on an individual s personal information which can be used in applications such as bank loan approval is much more likely to predict male individuals with high income level.
further analysis shows that for many individuals changing only the gender or race causes the output of the predictions to flip .
for another instance it has been shown that a machine learning model used to predict the recidivism risk for suspected criminals is more likely to mislabel black defendants as having a high recidivism risk.
in recent years many methods and tools have been proposed to detect discrimination in neural networks systematically e.g.
through the so called fairness testing and more relevantly to improve the fairness of neural networks .
in general existing fairness improving methods can be classified into three categories according to when the method is applied e.g.
pre processing in processing and post processing methods.
pre processing methods aim to reduce the bias in the training data so as to reduce the bias of model predictions in processing methods focus on the model and the training process and post processing methods modify the prediction results directly rather than the training data or the model.
however fairness improving is a complicated task and it is not always clear which method should be applied.
as shown in section different fairness improving methods perform significantly differently on different models which is consistent with the partial results reported in .
more importantly applying the wrong method would not only lead to a huge loss in accuracy e.g.
the accuracy of the model trained on the compas dataset drops by after applying the reject option post processing method but also lead to worsened fairness.
for instance out of cases i.e.
combinations of model protected attribute and fairness improving method that we examined in section of them result in worsened fairness.
furthermore a fairness improving method may be effective with respect to one protected attribute whilst being harmful with respect to another protected attribute.
for instance the fairness of the model trained on adult income dataset improves by around with respect to the gender attribute after applying the equalized odds post processing method and worsens by with respect to the race attribute.
given that many of the fairness improving methods require significant effort and computing resource it is infeasible to try all of them and identify the best performing one.
it is thus important to have a systematic way of identifying the right method efficiently.
in this work we propose to choose the right fairness improving method based on causality analysis.
intuitively the idea is toarxiv .07190v1 sep 2022esec fse november singapore singapore mengdi zhang and jun sun conduct causality analysis so as to understand the causes of the discrimination i.e.
whether a certain number of input attributes or hidden neurons are highly responsible for the unfairness.
formally we use the probability of high causal effects and coefficient of variation to characterize the distribution of the causal effects.
depending on the result of the causality analysis we then choose the fairness improving method accordingly.
for instance if a small number of input attributes bare most of the responsibility for unfairness a pre processing method such as would be the choice whereas an in processing method would be the choice if some neurons are highly responsible.
our approach is designed based on the results of an empirical study which evaluates fairness improving methods i.e.
pre processing methods in processing methods and post processing methods on different benchmark datasets with respect to different fairness metrics.
our approach is systematically evaluated with the same models.
the results show that our selected processing approach is the optimal choice to improve group fairness in all cases and the optimal choice to reduce individual discrimination in most cases.
the remainders of the paper are organized as follows.
in section we review relevant background.
in section we present results from our empirical study which motivates our approach.
in section we present our adaptive fairness improving method.
in section we evaluate our approach.
lastly we review related work in section and conclude in section .
background in the following we review relevant background on fairness and existing fairness improving methods.
.
fairness definitions in the literature there are multiple definitions of fairness .
what is common across different definitions is that to define fairness we must first identify a set of protected attributes a.k.a.
sensitive attributes .
commonly recognized protected attributes instance race sex age and religion.
note that different models may have different protected attributes.
in the following we introduce two popular definitions of fairness i.e.
group fairness and individual discrimination as well as the corresponding fairness scores i.e.
metrics that are used to quantify the degree of unfairness.
group fairness also known as statistical fairness focuses on certain protected groups such as ethnic minority and the parity across different groups based on some statistical measurements.
it is the primary focus of this study as well as many existing studies .
classic measurements for group fairness include positive classification rate and true positive rate.
a classifier satisfies group fairness if the samples in the protected groups have an equal or similar positive classification probability or true positive probability.
given a model we can measure its degree of unfairness according to group fairness using statistical parity difference spd .
1there are also alternative similar measures such as disparate impact that we omit in this study.definition .
statistical parity difference .
letybe the predicted output of the neural network n lbe a favorable prediction and f be a protected attribute.
statistical parity difference is the difference in the probability of favorable outcomes between the unprivileged and privileged groups where the unprivileged privileged groups are defined based on the value of the protected attribute.
p p note that the above definition only considers a single binary protected attribute which is sometimes insufficient.
the following metric called group discrimination score gds extends spd to measure fairness based on multiple protected attributes.
definition .
group discrimination score .
letnbe a neural network ybe the predicted output of the neural network lbe a favorable prediction and fbe a set of one or more protected attributes.
let and be an arbitrary valuation of the protected attributesf.
letx be the set of inputs whose f attribute values are .
letp bep n x l x x .
the multivariate group discrimination with respect to protected attributes fis the maximum difference between any p andp .
example consider the structured dataset adult income .
it has two protected attributes i.e.
gender and race.
each attribute has a set of two values i.e.
female or male for gender and white or nonwhite for race.
as a result there are possible i.e.
male white female white male non white and female non white .
the probabilities of an individual who is predicted to have a high income level i.e.
more than 50k with respect to these four is .
.
.
and .
respectively.
the gds of the model is thus .
.
individual discrimination is another concept which is often applied in fairness analysis.
it focuses on specific pairs of individuals.
intuitively individual discrimination occurs when two individuals that differ by only certain protected attribute s are predicted with different labels.
an individual whose label changes once its protected attribute s changes is referred to as an individual discriminatory instance.
definition .
individual discriminatory instance .
letfbe a set of one or more protected attributes and nbe a neural network.
x is an individual discriminatory instance if there exists an instance x such that the following conditions are satisfied.
q f.xq x q n x n x the above definition is often adopted in fairness testing i.e.
works on searching or generating individual discriminatory instances .
in addition there are proposals on learning models which are more likely to avoid individual discriminatory .
given a model we can measure its fairness according to individual discrimination by measuring the percentage of individual discriminatory instances in a set of instances which can be the test set or a set generated to simulate unseen samples formally called causal discrimination score cds .
definition .
causal discrimination score .
letnbe a neural network fbe a set of protected attributes.
the causal discrimination score of nwith respect to protected attributes f is the fraction of inputs which are individual discrimination instances.adaptive fairness improvement based on causality analysis esec fse november singapore singapore .
fairness improving methods many methods have been proposed to improve the fairness of neural networks .
they can be categorized into three groups according to when they are applied i.e.
pre processing in processing and post processing.
pre processing methods aim to reduce the discrimination and bias in the training data so as to improve the fairness of the trained model.
among the many pre processing methods we focus on the following two representatives in this work.
reweighing rw works by assigning different weights to training samples in order to reduce the effect of data biases.
in particular lower weights are assigned to favored inputs which have a higher chance of being predicted with the favorable label and higher weights are assigned to deprived inputs.
disparate impact remover dir is based on the disparate impact metric which compares the proportion of individuals that are predicted with the favorable label for an unprivileged group and a privileged group.
it modifies the values of the non protected attribute to remove the bias from the training dataset.
in processing methods modify the model in different ways to mitigate the bias in the model predictions .
we focus on the following representative in processing methods in this work.
classification with fairness constraints meta develops a meta algorithm which captures the desired metrics of group fairness e.g.
gds using convex fairness constraints with strong theoretical guarantees and then using the constraints as an additional loss function for training the neural network.
adversarial debiasing ad modifies the original model by including backward feedback for predicting the protected attribute.
it maximizes the predictors ability for classification while minimizing the adversary s ability to predict the protected attribute to mitigate the bias.
prejudice remover regularizer pr focuses on the indirect prejudge.
it uses regularizers to compute and restrict the effect of the protected attributes.
exponential gradient reduction gr reduces the fair classification problem to a sequence of cost sensitive classification problems whose solutions yield a randomized classifier with the lowest empirical error subject to the desired constraints.
post processing methods modify the prediction results instead of the inputs or the model.
we consider three representative processing algorithms in this work.
equalized odds eo solves a linear program to find probabilities with which to change the output labels so as to optimize equalized odds on protected attributes.
calibrated equalized odds ceo optimizes over calibrated classifier score outputs to find probabilities with which to change output labels with an equalized odds objective.
reject option classification ro assigns favorable labels to unprivileged instances and unfavorable labels to privileged instances around the decision boundary with the highest uncertainty.table dataset privileged groups definition dataset protected attribute privileged group favorable class adult incomegender gender maleincome 50krace race caucasian german creditgender gender malegood creditage age bank marketing age age yes compasgender gender femaleno recidivismrace race caucasian an empirical study in this section we present an empirical study which aims to compare the performance of different fairness improving methods on different models different protected attributes or attribute combinations.
.
experimental setup datasets our experiments are based on models trained with the following benchmark datasets census income german credit bank marketing and compas .
these datasets have been used as the evaluation subjects in multiple previous studies .
adult income the prediction task of this dataset is to determine whether the income of an adult is above annually.
the dataset contains more than samples.
the attributes gender race are protected attributes.
german credit this is a small dataset with samples.
the task is to assess an individual s credit based on personal and financial records.
the attributes gender andageare protected attributes.
bank marketing the dataset contains more than samples and is used to train models for predicting whether the client would subscribe a term deposit.
its only sensitive attribute is age.
compas the compas recidivism dataset contains more than samples and is used to predict whether the recidivism risk score for an individual is high.
the attributes gender race are protected attributes.
in our experiment we define privileged and unprivileged groups based on the default setting in .
the details of the privileged group definitions and favorable class are summarised at table .
altogether we have a total of model attribute combinations.
our implementation of the fairness improving methods is based on the aif360 implementation .
each implementation is manually reviewed and tested through standard practice.
model training our models are feed forward neural networks which are shown to be highly accurate and efficient in these realworld classification problems .
all these neural networks contain five hidden layers each of which contains and units.
the output layer contains number of predict classes units.
for each dataset we split the data into training data and test data.
all experiments are conducted on a server running ubuntu operating system with intel core .10ghz cpu 32gb memory and nvidia gv102 gpu.
to mitigate the effect of randomness whenever relevant we set the same random seedesec fse november singapore singapore mengdi zhang and jun sun table neural networks in experiments dataset protected attribute spd gds cds accuracy adult incomegender .
.
.
.
race .
.
.
gender race .
.
german creditgender .
.
.
.
age .
.
.
gender age .
.
bank age .
.
.
.
compasgender .
.
.
.
race .
.
.
gender race .
.
for each test.
the trained models reach standard state of the art accuracy.
the trained results including the corresponding fairness scores are shown in table .
note that spd is the probability difference between the unprivileged and privileged groups which is defined on a single protected attribute and thus it is irrelevant if multiple protected attributes are considered simultaneously.
.
evaluation results in the following we present the results of the empirical study which aims to answer the following research questions.
rq1 do the fairness improving methods always improve group fairness?
to answer the question we systematically apply all fairness improving methods on all the model attribute combinations and measure the effectiveness of the fairness improving methods.
we measure the group fairness improvement as follows.
spd is adopted if a single protected attribute is relevant and gds is adopted if multiple protected attributes are considered at the same time.
note that gds is the same as spd with respect to a single protected attribute.
the results are shown in figure where there is one bar for each model attribute combination and for each fairness improving method i.e.
a total of bars for each model attribute combination e.g.
adult gender and bars in total.
a positive value means improved fairness and a negative value means worsened fairness.
this bar is shown in different colors for the nine different methods.
first of all to our surprise the fairness improving methods are not always helpful in terms of improving fairness.
as shown in figure while many methods have a positive effect in many cases there are many instances where applying fairness improving method results in worsened fairness sometimes quite significantly.
this is shown as the colorful bar before the zero line which accounts for a total of cases i.e.
.
most of those cases are for in processing and post processing methods.
furthermore the performance of the methods varies significantly across different models and protected attributes.
table shows a summary on which method achieves the most fairness improvement for each model attribute combination and it can be observed that different winners are there for different model attribute combinations.
further analysis shows the performance of the fairness improving methods vary across many dimensions.
first the performance of the same method varies significantly on different models.table best method for group fairness improvement dataset protected attribute group fairness absolute change adult incomegender gr .
race meta .
gender race gr .
german creditgender rw .
age rw .
gender age rw .
bank age rw .
compasgender ro .
race ro .
gender race ro .
for instance while the post processing method ceo works effectively for the neural network trained on adult income dataset it is ineffective for the model trained on german credit dataset.
secondly the performance of the methods varies across different attributes in the same model.
for instance the post processing method eo improves the group fairness with respect to gender attribute effectively but leads to worse group fairness with respect torace attribute for the neural network trained on adult income dataset.
moreover even the processing methods in the same category behave differently on the same model attribute combination.
in terms of in processing methods rw is much more effective than dir.
all models group fairness can be improved by rw whereas dir is ineffective with respect to credit gender andcompas race .
for in processing methods gr is most effective in improving group fairness for all model attribute combinations except credit age .
the performance of post processing methods varies significantly.
for example the post processing method ro is much more effective in improving the group fairness for the neural network trained on compas dataset than ceo and eo.
there are some conjectures on why fairness improvement approaches may have different effects on different models and different model attribute combinations.
the main reason is that these methods improve fairness based on certain metrics which may be subtly different from common notions of fairness such as spg gds and cds.
for instance ceo focuses on reducing false positive rate difference in particular which sometimes translates to fairness measured using spg gds cds as for the adult income dataset and sometimes not.
for the different performances on different model attribute combinations there may be two reasons.
the first is that the discrimination against different attributes in the model may be very different see in table and observed in .
the second possible reason is that the reasons of the discrimination against different attributes may be different e.g.
biased training data or biased models.
answer to rq1 existing fairness improving methods are not always effective in improving group fairness and thus they must be applied with caution.
rq2 what is the cost on accuracy when applying existing fairness improving methods?
the results are shown in figure where there is similarly one bar for each model attribute combination and for eachadaptive fairness improvement based on causality analysis esec fse november singapore singapore .
.
.
.
.
.
.
genderracegender racegenderagegender ageagegenderracegender raceadultcreditbankcompasgroup metric changesrwdirmetaadprgrceoeoro figure group fairness improvement of models with respect to different protected attributes .
.
.
.
.
.
genderracegender racegenderagegender ageagegenderracegender raceadultcreditbankcompasaccuracy changesrwdirmetaadprgrceoeoro figure accuracy changes of models with respect to different protected attributes after processing fairness improving method.
a positive value indicates an increased accuracy and a negative value indicates a decreased accuracy.
first of all we observe that some of the fairness improving methods may indeed incur a significant loss of accuracy.
this is most observable on meta pr ceo eo and ro.
especially for the neural network trained on the compas dataset the accuracy drops more than after applying meta pr or ro.
the average loss of accuracy is around after processing by meta and after processing by ro.
to our surprise some of the fairness improving methods result in improved accuracy in some cases.
this is most observable in some in processing methods.
especially for the neural network trained on the german credit dataset the accuracy increases after applying all four in processing methods.
it should be noted however most of these in processing methods have a less or harmful effect in terms of group fairness improvement in these cases.
for example while the accuracy increases by after applying gr on credit age the spd fairness score worsens by .
the accuracy reduction varies across not only different modelattribute combinations but also different methods across differentcategories.
compared fairness improving methods from different categories the pre processing methods have an overall mild impact on the model accuracy.
in terms of the most effective pre processing method rw it is effective on group fairness improvement with respect to all model attribute combinations and scarifies little accuracy.
in terms of the most effective in processing method gr it is effective on group fairness improvement with respect to all modelattribute combinations except credit age although sometimes with minimal fairness improvement .
among them neural networks get lower accuracy after processing.
but the accuracy drops less than in average.
in terms of the post processing method ro it is effective on group fairness improvement with respect to modelattribute but neural networks get lower accuracy after processing.
especially for the neural network trained on compas dataset the accuracy drops more than which is unacceptable.
answer to rq2 existing fairness improving methods may incur a significant loss in accuracy.esec fse november singapore singapore mengdi zhang and jun sun .
.
.
.
.
.
.
.
groupmetricindividualmetricgroupmetricindividualmetricgroupmetricindividualmetricgroupmetricindividualmetricgroupmetricindividualmetricgroupmetricindividualmetricgroupmetricindividualmetricgroupmetricindividualmetricrwmetaadprgrceoeoroadult genderadult raceadult gender racecredit gendercredit agecredit gender agebank agecompas gendercompas racecompas gender race figure comparison between group fairness improvement and individual discrimination reduction rq3 do the fairness improving methods perform differently for improving group fairness and reducing individual discrimination?
almost all existing fairness improving methods focus on group fairness whilst some fairness testing approaches focus on individual discrimination for some reason .
thus we are curious about whether the existing fairness improving methods can reduce individual discrimination as well.
to answer this question we compare the cds change against the group fairness metric change achieved by the same method.
the idea is to check whether the changes are consistent i.e.
whether an improvement in group fairness leads to a reduction in individual discrimination and vice versa.
note that by the default setting in the dir pre processing method removes all protected attributes which makes individual discrimination irrelevant and thus is not considered in this experiment.
the results are shown in figure where the cds change is placed next to the fairness metric change for each fairness improving method.
first of all the group fairness improvement and individual discrimination reduction are inconsistent.
a method improving the group fairness effectively might have none or even harmful effect on individual fairness.
this is most observable on rw and ro.
the pre processing method rw is effective on group fairness improvement for all models but lead to more individual discrimination for model attribute combinations.
after applying the post processing method ro the individual discrimination worsens for all model attribute combinations.
furthermore only the in processing methods consistently reduce individual discrimination.
in terms of meta method it increases the group fairness and reduces the individual discrimination at the same time for model attribute combinations.
the method ad reduces the individual discrimination with respect to all protected attributes in adult income dataset and german credit dataset.
especially for the neural network trained on adult income dataset all in processing methods improve the individual fairness effectively.
by contrary all post processing methods have harmful effect onindividual discrimination.
on average the cds worsens by around after applying ceo worsens by after applying eo and worsens by more than with ro.
answer to rq3 existing methods are less effective in reducing individual discrimination.
an adaptive approach our empirical study shows that the performance of fairness improving methods varies significantly across different models i.e.
sometimes resulting in worsened fairness and or reduced accuracy.
we thus need a systematic way of choosing the right method.
our proposal is an adaptive approach based on causality analysis.
intuitively causality analysis measures the responsibility of each neuron and input attributes towards the unfairness and depending on whether the most responsible neurons are in the hidden layers or at the input layer as well as whether a small number of them are significantly more responsible than the rest.
then we choose the fairness improving method accordingly.
in the following we present the details of our approach.
.
causality analysis causality analysis aims to identify the presence of causal relationships among events.
furthermore it can be used to quantify the causal influence of an event on another event.
to conduct causality analysis on neural networks we first adopt the approach in and treat neural networks as structured causal models scm .
formally definition .
structure causal model .
a structure causal model consists of a set of endogenous variables xand a set of exogenous variablesuconnected by a set of functions fthat determine the values of the variables in xbased on the values of the variablesadaptive fairness improvement based on causality analysis esec fse november singapore singapore inu.
the neural network corresponding scm can be represented as a tuple model m x u f p u wherepuis the probability of distribution over u. for the neural network the endogenous variables vare observed variables e.g.
attributes or neurons.
the exogenous variables are the unobserved random variables e.g.
noise and puis the possible distribution of the exogenous variables.
trivially an scm can be represented by a directed graphical model g x e whereeis the causal mechanism.
based on scm the causal effect of a certain event can be computed as the difference between potential outcomes under different treatments.
in this work we adopt the average causal effect ace as the measurement of the causal effect .
the formal definitions of ace are shown below where it is assumed that the input endogenous variables are not correlated to each other .
definition .
average causal effect .
the ace of a given endogenous variable xwith value on outputycan be measured as acey do x e baseline x where e represents the interventional expectation which is the expected value of ywhen the random variable xis set to andbaseline xis the average ace of xony i.e.
ex ey .
following the recent work reported in we apply ace to capture the causal influence on model fairness.
that is the yin equation should be a measure of the model unfairness i.e.
spr gds or cds.
for simplicity we denote it as yf air.
in order to analyze the causal effect on fairness we analyze two possible causal effects i.e.
the relationship between input attributes to unfairness and the relationship between the hidden neurons to unfairness.
in this work we make use of the average interventional expectation to approximate the ace of variable x toyf air.
formally aceyf air do x represents the ace of variable x under value to the fairness property yf air.
one complication is that each input attribute or neuron has many possible values and we must consider all the possible values in computing the ace.
our remedy is to consider the average interventional expectation aie .
definition .
average interventional expectation .
letxbe the given endogenous variable yf air be the fairness property and val setxbe a set of values of variable x. the average interventional expectation is the mean of expected values of yf airwhenx is set to be each value aieyf air x val setxe val setx for the input features with categorical values we intervene the feature with every possible value based on the training dataset.
for the hidden neurons with continuous value intervening it with every possible value might be consuming.
we thus intervene the neurons as follows which is adopted in as well.
that is we assume the intervener is equally likely to perturb variable xto any value within the input range so that u minx max x where 2there are alternative ones such as the gradient of causal attribution which work slightly differently.
3or alternatively it can be e where xis the selected significant point.algorithm causalityneuron n d n fair metric wherenis the neural network dis the dataset used to measure causal effect n is a hidden neuron in nandfair metric is the function measuring the fairness score based on the desired fairness metric minbminimum output of neuron n maxbmaximum output of neuron n val set generate vals min max num interval for inval setdo ie yf air fair metric n d do n ie ie yf air end for returnmean ie algorithm causalityattribute n d f fair metric wheren is the neural network dis the dataset used to measure causal effect fis an input attribute and fair metric is the function of measuring the fairness score based on the desired fairness metric val setbthe set of all possible values of attribute f for inval setdo ie yf air fair metric n d do f ie ie yf air end for returnmean ie minxandmax xare the minimum and maximum input values of x. in practice minxandmax xcan be obtained by observing the value of the input attribute or neuron given all the training samples and theval setxis generated by partitioning the range uniformly into a fixed number of intervals.
note that if a specific distribution of the interventions is given it can be used to generate the intervention values instead.
the details of causality analysis on the hidden neurons are shown in algorithm .
given a neural network n a set of inputs d i.e.
the training set a hidden neuron nand the function for measuring the desired fairness score fair metric we systematically measure the aie with neuron intervention.
at line and line we set min to the minimum output of nandmax to the maximum output ofn.
then we generate a set of evenly spaced numbers within the domain of the neuron output asval setthrough functiongenerate valsat line .
the input parameter num interval decides how many intervals are there.
from line to we calculate the aie with each perturbing value .
in each round we first set ie as an empty set at line and then calculate the fairness score yf air whilst fixing the value of neuron nas .
at line we return the mean of all interventional expectation as the aie.
algorithm similarly conducts causality analysis on the input attributes.
the only difference is that we perform the intervention on the given attribute fat line with all possible values of the attribute.
.
adaptive fairness improvement once we compute the causal effect of each neuron and each input attribute on fairness i.e.
responsibility for unfairness we can thenesec fse november singapore singapore mengdi zhang and jun sun adaptively select the fairness improving methods.
for example if the causal effects of input attributes are relatively high the unfairness is more likely to be related to the input attributes and likely to be eliminated by pre processing methods.
similarly if the interior neurons in the neural network have high causal effects on the fairness property in processing methods might be a suitable choice for fairness improvement.
formally to properly compare the casual effects of neurons and input attributes we first normalize it with respect to a baseline yf air which is the fairness score based on the desired fairness metric without any intervention.
the baseline yf aircan be spd gds and cds as discussed previously.
we define the causal effects higher than the basic fairness property as high causal effects and vice versa.
in other words only the variable with a causal effect higher than the basic fairness property has the positive causality to unfairness.
that is we only consider those neurons and attributes with a causal effect higher than yf air as responsible.
next we measure the proposition of input attributes and neurons that are considered responsible.
given the set of causal effects of all attribute aie fand the set of causal effects of all neuronsaien we formally denote the proportion of high causality attributes as pfand the proportion of high causality neurons as pn and define them as follows.
pf p aie yf air aie aie f pn p aie yf air aie aien furthermore we measure the distribution of the responsibility among the input attributes and neurons since it intuitively has an impact on which fairness improving method should be chosen.
for instance if all input attributes have similar responsibility for unfairness it is likely hard to pre process the inputs so as to eliminate the discrimination.
similarly if all neurons are equally responsible for unfairness it is complex to improve the fairness by focusing on a few neurons as in .
formally we use the coefficient of variation cv to capture the distribution of the causal effects.
cv is used to measure the dispersion of data points around the mean.
it represents the ratio of the standard deviation to the mean which indicates the degree of variation.
in this setting the larger the cv the more uneven the distribution of causal effects.
we denote the cv of attributes as cvfand the cv of neurons as cvn.
the details of how to select fairness improving methods are shown in algorithm .
if both the proportion of responsible attributes and responsible neurons is less than a proportion threshold p thres few input attributes and neurons are to be blamed for the unfairness.
as a result it is unlikely pre processing which focuses on input attributes or in processing which focuses on the hidden neurons is effective and thus we choose to apply the post processing methods.
in practice we set the threshold p thres to be .
otherwise there are sufficient number of input attributes or neurons that are responsible for unfairness we then select to apply a pre processing method if cvf cvn i.e.
the distribution of causal effects is more uneven in the input attributes which means that some of the input attributes are more responsible.
otherwise an in processing method is chosen.
for pre processing methods rw is preferred over dir as rw is also feasible to individual fairness metrics.
for in processing methods and post processingalgorithm adaptiveimprove pn pf cvn cvf ifpf p thres andpn p thres then return post processing methods else ifcvf cvnthen return pre processing methods else return in processing methods end if end if .
.
.
.
.
.
.38causal effect baseline attributelayer 1 layer 2layer 3 layer 4layer 5 figure causality analysis result of adult gender methods we choose the method with the best improvement and least accuracy cost.
example for the neural network trained on adult income dataset assume that the protected attribute is the gender attribute.
according to the above discussion we use the group fairness metric spd to calculate the causal effects of attributes and neurons.
the causality analysis result is shown in figure where each dot represents the aie of either an input attribute or a hidden neuron.
we mark the causal effects of input attributes with black dots and mark the causal effects of hidden neurons in different layers with different colors.
the dotted line marks the baseline yf airwhich is .
.
there are i.e.
attributes with causal effects higher than the baseline and i.e.
.
neurons with causal effects higher than the baseline.
as the proportion of responsible input attributes and neurons satisfy the threshold we then calculate the cv values of those responsible attributes and neurons.
the cvfof these attributes is .
and the cvnof these neurons is .
.
sincecvn cvf we choose to apply in processing methods so as to improve the model s group fairness.
implementation and evaluation in this section we evaluate the performance of our adaptive approach systematically to answer multiple research questions.
note that the same datasets models and the configuration from section 3adaptive fairness improvement based on causality analysis esec fse november singapore singapore are used in this section.
rq1 how are the responsibility distributed among the neurons and input attributes to answer this question we show the probability of high causal effects and cv of these causal effects for both the hidden neurons and input attributes in table and table .
the first column is the training dataset and the second column shows the corresponding protected attribute s in each dataset.
then we show the probability of attributes with high causal effects pf the probability of neurons with high causal effects pn cv on highly causal attributescvfand cv on highly causal neurons cvn.
it can be observed that the distribution of responsibility varies significantly across different model attribute combinations which potentially explains why only some fairness improving methods are effective sometimes.
table shows the distribution of high causal effects based on group fairness metrics e.g.
spd for single protected attributes and gds for multivariate protected attributes.
based on algorithm the selected processing categories are shown in the last column.
for all attribute s in adult income dataset the probabilities of high causal effects are higher than and cvnscores are higher thancvfscores.
so we decide to apply pre processing methods to this model to improve the group fairness for all attributes.
for the neural network trained on german credit dataset with respect to all attributes we conclude to apply pre processing methods.
for example with respect to ageattribute both the proportion and the cv of high causal neurons are lower than the two of high causal attributes.
similarly based on the distribution of high causal effects we conclude to apply pre processing to the neural network trained on bank dataset and the neural network trained on compas dataset with respect to gender andrace attributes.
with respect to gender race attribute in compas dataset as the cv of neurons is higher we conclude to apply in processing methods.
table show the distribution of high causal effects based on individual fairness metrics e.g.
cds.
the selected processing categories are shown in the last column.
similarly algorithm decides to apply in processing methods for all model attribute combinations expect credit gender andbank age .
we can observe that the proportion of high causal effects of attributes might be in some cases e.g.
compas gender and compas race which means no attribute is responsible for individual discrimination.
note that post processing methods are selected only if both the proportions of responsible neurons attributes are low as it often has a significant negative impact on model performance so that it is impossible to improve fairness through pre processing or inprocessing .
in our experiments however all the neural networks have sufficiently many responsible neurons attributes so no postprocessing method is adopted.
rq2 are we always able to identify the best performing fairness improvement method?
to answer this question we compare our adaptive approach against the best performing pre processing inprocessing and post processing method in four ways.
one is the group fairness improvement which is shown in figure a .table distribution of high causal effects with group fairness dataset protected attribute pfpncvfcvn processing adult incomegender .
.
.
.
in processing race .
.
.
.
in processing gender race .
.
.
.
in processing german creditgender .
.
.
.
pre processing age .
.
.
.
pre processing gender age .
.
.
.
pre processing bank age .
.
.
.
pre processing compasgender .
.
.
.
pre processing race .
.
.
.
pre processing gender race .
.
.
.
in processing table distribution of high causal effects with individual discrimination dataset protected attribute pfpncvfcvn processing adult incomegender .
.
.
.
in processing race .
.
.
.
in processing gender race .
.
.
.
in processing german creditgender .
.
.
.
pre processing age .
.
.
.
in processing gender age .
.
.. .
in processing bank age .
.
.
.
pre processing compasgender .
.
in processing race .
.
in processing gender race .
.
.
in processing one is the group fairness improvement minus the accuracy loss which is shown in figure b .
one is the individual discrimination reduction which is shown in figure a .
one is the individual discrimination reduction minus the accuracy loss which is shown in figure b .
as shown in figure a if we focus on group fairness improvement only our approach achieves the best performance for out of cases e.g.
e.g.
all attributes in adult income dataset all attributes in german credit dataset the attribute in bank dataset .
although for the neural network trained on compas dataset our adaptive approach does not have the best fairness improvement.
if we consider at the same time the accuracy loss as shown in figure b our approach performs the best in all of the cases.
note that while the post processing method ro often improves the group fairness significantly the accuracy often drops significantly e.g.
more than after processing with respect to all protected attributes for the compas dataset which is clearly unacceptable .
in fact according to our experiments post processing should rarely be the choice if we would be maintain high accuracy.the results shown in figure b clearly suggests that our approach is able to improve fairness effectively whilst maintaining a high accuracy.
in figure a we show the comparison between our approach and the existing approaches in terms of reducing individual discrimination.
we can observe that only the in processing methods can reduce the individual discrimination effectively.
in fact our adaptive processing algorithm almost always selects to apply inprocessing methods except for credit gender andbank age .
after applying the in processing method rw the cds remains almostesec fse november singapore singapore mengdi zhang and jun sun rwrwrwrwrwrwrwrwrwrwgr grgr grmetaprmetagrgrgrro roceo ceoeoeoceorororo .
.
.
.
.
.
genderracegender racegenderagegender ageagegenderracegender raceadultcreditbankcompaspre processingin processingpost processingour approach a group fairness improvement rwrwrwrwrwrwrwrwrwrwgrgrgr grmetaprmetagrgrgrroroceo ceoeoeoceo rororo .
.
.
.
.
.
.
genderracegender racegenderagegender ageagegenderracegender raceadult incomegemran creditbankcompaspre processingin processingpost processingour approach b group fairness improvement accuracy loss figure our approach vs sota on group fairness the same with respect to credit gender but worsens by around with respect to bank age .
taking accuracy loss into account at the same time we show the individual discrimination reduction minus the accuracy lost in figure b .
our approach performs best in out of cases except for the two cases where rw is selected for credit gender andbank age .
one potential reason why this is the case is that existing pre processing methods are not designed for reducing individual discrimination and as a result even if a small number of input attributes are indeed responsible for the unfairness existing pre processing methods such as rw are not able to remove biases in the training set effectively.
this calls for research into alternative pre processing methods for reducing individual discrimination.
it is worth noting that with our approach we always out achieve improved group fairness and almost always out achieve reduced individual discrimination whist achieving a low accuracy loss.
rq3 what are the time overhead for causality analysis?
the time spent on causality analysis is summarised in table .
note that the time is the additional time a user has to spend on applying our method before applying the selected fairness improving method.
the time required for causality analysis is always less than minutes.
threats to validity limited model structures we currently support feed forward neural networks for tabular data and convolutional neural networks for images .
it is possible to extend our method to support deep learning architectures such as rnn for text data by extendingtable time overhead for causality analysis dataset protected attribute time s adult incomegender .
race .
gender race .
german creditgender .
age .
gender age .
bank age .
compasgender .
race .
gender race .
rwrwrwrwrwrwrwrwrwrwmetametametaadmetagrmetagrgradceoceoceo ceoceoceororoeoro .
.
.
.
.
.
.
.
genderracegender racegenderagegender ageagegenderracegender raceadult incomegemran creditbankcompaspre processingin processingpost processingour approach a individual discrimination reduction rwrwrwrwrwrwrwrwrwrwmetametametaadmetagrmetagrgradceoceoceo ceoceoceoceo ceoceoceo .
.
.
.
.
.
.
.
genderracegender racegenderagegender ageagegenderracegender raceadultcreditbankcompaspre processingin processingpost processingour approach b individual discrimination reduction accuracy loss figure our approach vs sota on individual discrimination causality analysis to handle feedback loops.
we focus on feedforward nn as existing studies on fairness largely focus on tabular data .
limited fairness metrics we only use spd and gds metrics for group fairness and cds metric for individual fairness.
we focus on gpd and gds as they are the primary focuses of existing works .
given that gpd and gds are similar with other metrics which consider positive classification rate like disparate impact our method could work for other notions of fairness as well.
causal effect measurement ace is commonly used to evaluate causality .
according to alternative measurements likeadaptive fairness improvement based on causality analysis esec fse november singapore singapore integrated gradients and gradients of causal effect might suffer from sensitivity and induce causal effects by other input features.
distributional shift in the data our approach might be affected by distributional shifts in the data.
we evaluate the stability of our approach against slight distributional shifts on adult income dataset.
firstly following we randomly split train test set times and then evaluate whther the method selected by our approach is the best one for each of the test sets.
secondly following we evaluate our approach using data generated by perturbation.
in both conditions the results confirm that is the case.
it shows perhaps that our approach is robust to such levels of distributional shift.
related work this work is related to research on fairness improving methods fairness testing and fairness verification methods as well as broadly various studies on fairness.
besides those mentioned in the previous sections we summarize other related works below.
fairness testing and verification some existing works attempted to test model discrimination with fairness score measurements.
in tramer et al.
propose an unwarranted associations framework to detect unfair discriminatory or offensive user treatment in data driven applications.
it identifies discrimination according to multiple metrics including the cv score related ratio and associations between outputs and protected attributes.
in kleinberg et al.
also test multiple discrimination scores and compare different fairness metrics.
in galhotra et al.
propose a tool called themis to measure software discrimination.
it tests discrimination with two fairness definitions i.e.
group discrimination score and causal discrimination score.
it measures these two scores for different software instances with respect to race and gender separately.
their approach generates additional testing samples by selecting random values from the domain for all attributes.
in adebayo et al.
try to determine the relative significance of a model s inputs in determining the outcomes and use it to assess the discriminatory extent of the model.
in ghosh et al.
verify different fairness measures of the learning process with respect to underlying data distribution.
empirical studies of fairness chakraborty et al.
empirically research on the effectiveness and efficiency of existing fairness improvement methods based on group fairness metrics .
friedler et al.
work on an empirical study to compare the effects of different fairness improvement methods .
in biswas et al.
focus on an empirical evaluation of fairness and mitigation on different real world machine learning models.
they apply mitigation techniques to these models and analyzed the fairness mitigation results and impacts on performance.
they also present different trade off choices of fairness mitigation decisions.
zhang et al.
discuss how key aspects of machine learning systems such as attribute set and training data affect fairness in .
kearns et al.
test the effectiveness and measure the trade offs between rich subgroup fairness and accuracy in .
in dodge et al.
propose four types of programmaticallygenerated explanations to understand fairness in machine learning systems.
conclusion in this paper we empirically evaluate fairness improving methods on real world dataset and model attribute combinations with different fairness metric.
our evaluation shows that existing fairness improving methods are not always effective in improving group fairness and are often not effective in reducing individual discrimination.
meanwhile we test the trade off between fairness improvement and accuracy cost.
motivated by the empirical study we propose a light weight approach to choose the the optimal fairness improving method adaptively based on causality analysis.
that is we identify on the distribution of responsible attribute and neurons and choose the methods accordingly.
our evaluation shows that our approach is effective in choosing the optimal improvement method.
acknowledgement this research is supported by the ministry of education singapore under its academic research fund tier award id moet320200004 .
any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not reflect the views of the ministry of education singapore.