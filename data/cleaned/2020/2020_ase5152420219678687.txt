performance testing for cloud computing with dependent data bootstrapping sen he department of computer science university of texas at san antonio san antonio usa sen.he utsa.edutianyi liu department of computer science university of texas at san antonio san antonio usa tianyi.liu utsa.edupalden lama department of computer science university of texas at san antonio san antonio usa palden.lama utsa.edu jaewoo lee department of computer science university of georgia athens usa jaewoo.lee uga.eduin kee kim department of computer science university of georgia athens usa inkee.kim uga.eduwei wang department of computer science university of texas at san antonio san antonio usa wei.wang utsa.edu abstract to effectively utilize cloud computing cloud practice and research require accurate knowledge of the performance of cloud applications.
however due to the random performancefluctuations obtaining accurate performance results in the cloudis extremely difficult.
to handle this random fluctuation priorresearch on cloud performance testing relied on a non parametricstatistic tool called bootstrapping to design their stop criteria.however in this paper we show that the basic bootstrappingemployed by prior work overlooks the internal dependencywithin cloud performance test data which leads to inaccurateperformance results.
we then present metior a novel automated cloud performance testing methodology which is designed based on statisticaltools of block bootstrapping the law of large numbers andautocorrelation.
these statistical tools allow metior to properly consider the internal dependency within cloud performance testdata.
they also provide better coverage of cloud performancefluctuation and reduce the testing cost.
experimental evaluationon two public clouds showed that of metior s tests could provide performance results with less than error .
metior also significantly outperformed existing cloud performance testingmethodologies in terms of accuracy and cost with up to increase in the accurate test count and up to .
times reductionin testing cost.
i. i ntroduction cloud computing is widely adopted today due to its high cost efficiency.
to effectively utilize the cloud cloud users need to have accurate knowledge of the performance oftheir applications.
accurate knowledge of cloud performanceallows them to determine whether a virtual machine vm configuration e.g.
type and count of vms or an auto scalingpolicy satisfy their performance requirements .
accurateperformance data are also required for cloud research toevaluate new optimization algorithms or to be used as thetraining and testing data to develop new models .
performance testing is a standard procedure to obtain the performance for any applications .
for a cloudapplication performance testing can be used to obtain itsperformance results as point estimates.
that is to obtain themean or the percentile of its performance such as the meanthroughput or ile execution time .
there aretwo fundamental requirements for cloud performance testing.first the performance results should be accurate.
second ascloud performance testing also incurs cloud usage expenditure this testing should not cause excessive cost.
performance testing is typically conducted by repeatedly executing the application under test aut with a set ofrepresentative workloads inputs until a stop criterion deems that the results obtained from the test are accurate.
this stopcriterion is the key to ensure the above two requirements aresatisfied.
however as shown in prior work due to the randomperformance fluctuation it is extremely challengingto design good stop criteria for cloud performance testing.
to handle the random performance fluctuation prior studies relied on non parametric statistics tools to design theirstop criteria.
in particular prior studies have employed non parametric bootstrapping which is a re sampling techniqueused to calculate the confidence interval ci of a performancetesting result e.g.
the ci of the mean performance .the confidence interval is typically viewed as the margin of error .
therefore if the width of the confidence intervalis smaller than a user predefined maximum allowed error the performance result is deemed accurate enough and theperformance test can be stopped.
unfortunately prior work had shown that the performance testing methodology using bootstrapping cannot always pro vide accurate performance results .
prior work con cluded that the inaccuracy was partially caused by the test sincomplete coverage of the cloud performance fluctuations.however in our research we discovered that there is anotherfundamental issue related to the bootstrapping methodology.
in this paper we first present an analysis of the existing bootstrapping based testing methodology .
this analysisreveals a critical issue with this methodology that is unknownto the current research the overlooked internal dependency 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee within the performance test data.
because cloud performance fluctuation is mainly caused by the hardware resource con tention from multi tenancy i.e.
cloud applications vms shar ing hardware the performance data obtained fromthe tests are internally correlated.
that is the performance datafrom continuous tests within a short period are similar to eachother and the performance fluctuations may also have repeatedpatterns.
however the basic bootstrapping employed by priorstudies does not consider this internal dependency causingincorrectly calculated confidence intervals which in turn lead to incorrect performance results.
consequently a newperformance testing methodology that considers the internaldependency of cloud performance tests is required.
moreover our analysis also shows that an advanced bootstrapping technique originally designed for time series data called block bootstrapping can retain the internal dependencyduring the re sampling .
hence it may provide moreaccurate performance results.
nonetheless blindly applyingblock bootstrapping does not guarantee accurate results asits block size must be tuned for individual cloud applicationsand cloud platforms.
moreover enough tests must also beconducted to fully cover the potential cloud fluctuations.
based on the above analysis we developed metior an o v e l automated cloud performance testing methodology.
metior has two components.
the first component is the new stopcriterion to determine when performance tests can be stoppedand accurate performance results are obtained.
the new stopcriterion is based on block bootstrapping and the law oflarge numbers.
block bootstrapping allows metior to properly consider the internal dependency of performance test data while the law of large numbers ensures good coverage ofcloud performance fluctuations.
to handle the varying blocksize metior employs a novel technique that can automatically determine the best block size for each cloud application andplatform .
the second component is the low cost testexecution strategy.
as the performance of continuous testsis similar to each other there is no need to continuouslyexecute the aut.
therefore metior executes the aut in small intervals periods one day and intermittently executionsper hour to reduce the overall number of executions and theassociated cloud usage cost.
we also provided a systematicapproach to determine the interval length and the frequencyof the intermittent execution using the aforementioned bestblock size and autocorrelation .
metior is implemented as a fully automated performance tester for several publicclouds including amazon web services aws googlecloud and chameleon cloud .
we evaluated metior with six benchmarks on two public clouds aws and chameleon using six differentvm configurations.
the results show that metior can provide accurate performance results among the thousands of testsconducted of them provided performance results withless than errors.
metior also significantly outperformed existing cloud performance testing methodologies in terms ofaccuracy and cost it could increase accurate test count byup to and reduce the testing cost by up to .
times.
wealso applied metior to a state of the art cloud performance prediction technique.
the evaluation results showed that byproviding more accurate training data metior could improve the accuracy of the prediction technique by .
on average.
the contributions of this paper include .
an analysis of the basic bootstrapping which reveals that the overlooked internal dependency of cloud performance test data caused inaccurate cloud performance results.
.
a reliable and automated cloud performance testing methodology metior which is designed based on block bootstrapping the law of large numbers and autocorrelation.
.
a thorough evaluation of metior on two public clouds with different benchmarks vm configurations to show theaccuracy and cost benefits of metior .
.
a case study showing how metior benefits recent cloud research by bringing in reliable performance results.
the rest of this paper is organized as follows section ii provides the analysis on the basic bootstrapping section iiipresents the design of the metior section iv provides experimental evaluations.
section v presents a case study ofapplying metior in cloud research.
section vi discusses the limitation of metior .
section vii discusses related work and section viii concludes the paper.
ii.
a nalysis of bootstrapping cloud performance a performance test typically involves repeatedly executing the aut with one or more representative inputs.
for eachexecution the performance data such as the execution time latency or throughput are recorded.
based on the performancedata from a series of executions the performance testing result such as the mean latency or ile execution time canbe calculated.
the confidence interval ci of the performancetesting result can also be calculated which is usually viewed asthe margin of error of this result .
the width of thisci is usually used as the stop criterion of the performancetest if the ci width is smaller than a user defined maximum allowed error e.g.
maximum error then the test can be stopped .
in this paper we call a performance testingresult accurate if it indeed has an error less than the maximum allowed error.
for non cloud performance testing the cis are usually computed using t value or z value assuming the performanceis normally distributed .
however the performance ofcloud applications is usually not normal .
therefore cur rent research employed a non parametric statistics technique bootstrapping bt to calculate the cis for cloud performancetesting results .
unfortunately the performancetests conducted with the current bootstrapping based method ology tend to provide inaccurate results.
this section providesan analysis of the cause of this inaccuracy.
a. background on bootstrapping basic bootstrapping.
bootstrapping bt is essentially a resampling technique.
without loss of generality here we show how to use bootstrapping to determine the ci of the90 ile execution time of some performance test data with a 667density generate distribution for using ... cci for 53s1 s2 sc .
.
.cbootstrap resamplesinitial sample s withnelements .. .
c a basic bootstrap resampling and computing ci w. confidence level01 37s1 s2 sc .. .cbootstrap resamplesinitial sample s withnelements b block bootstrap resampling fig.
.
examples of resampling and computing cis with two bootstrapping methods.
confidence level cl .
let sbe the set of execution times obtained from a performance test with nrepeated executions.
that is s n. let be the ile execution time calculated froms.
a resample of s denoted by s is constructed by randomly selecting execution times from s. figure 1a illustrates the construction of s .
each selection randomly picks one execution time from s. this selection is then repeated for ntimes to select nexecution times from s with replacement.
these nexecution times constitute s .
the resampling is repeated for ctimes to generate c resamples denoted by s s ... s c. usually cshould be larger than for bootstrapping to work properly .
for each resample its ile can be calculated providing c ile execution times.
let these iles be 1 2 ... c. in bootstrapping the empirical distribution constructed fromthese iles is considered as a close approximation of thedistribution of the ile of s .
therefore the center area of this distribution is then the ci of .
more specifically 2 ... care first sorted and the .
ile and .
ile of the sorted list are the lower and upper bounds of the ci.
intuitively bootstrapping works if the resampling on s closely resembles how sis obtained from the real population.
sis essentially a random sample of the real population.
the resampling on s in turn views sas the population.
the resamples s s ... s c are then the random samples ofs.
if the resampling process closely resembles how s is sampled from the population then the variation of theresamples also closely resembles the variation when samplingthe real population.
therefore the distribution of the resamplescan then be used to empirically calculate the ci for statisticsestimates of the real population.
however as we will showlater the resampling process of basic bootstrapping does notresemble how cloud performance data are obtained from thereal population and hence cannot always provide reliable cis.
block bootstrapping.
block bootstrapping is mainly used to bootstrap time series where the data have internal depen dencies and or seasonality .
although performance testdata are not strictly time series they still contain internaldependencies.
therefore after we discovered that the basicbootstrapping could not preserve the internal data dependen cies during resampling we started to experiment with blockbootstrapping which considerably outperformed the basicbootstrapping.
therefore to provide a more comprehensiveanalysis block bootstrapping is also introduced here.
in block bootstrapping a resample s is also constructedfromsusing random selections.
however for each selection instead of just selecting one data point e.g.
one executiontime a block of continuous data points is selected.
byselecting a block of data points the internal dependency withinthe data points is preserved.
if there are bdata points in a block then n brandom selections will be performed to obtain s .
figure 1b illustrates the procedure of block bootstrapping.
similarly to basic bootstrapping cresamples are constructed and the ci was computed using these resamples.
clearly the size of the block i.e.
the number of selected data points is an important parameter.
if the block is too largeor too small then the internal dependency may be incorrectlyresampled.
for this analysis we used a fixed block size of .however in metior the block size is automatically adjusted for each cloud application and cloud platform.
b. cloud performance data internal dependency and bootstrapping in this analysis we used the performance test data provided by the pt4cloud data sets .
more specifically the per formance data of two benchmarks the ftfrom nas parallel benchmark suite npb and inmemory analytics ima from the cloud suite on two public clouds chameleon chm and amazon web service aws are analyzed.
foreach benchmark its one week continuous execution perfor mance data are used here.
for chameleon the large vm data are used.
for aws the m5.2xlarge vm data are used.
more details about these data sets are provided in section iv.
internal dependence of cloud performance test data figure gives the trace of the execution times of ima when it was executed in aws.
due to space limitation wecannot show the execution time traces for ftand chameleon although the same conclusion can be reached with them.
asfigure shows the execution times of ima had considerable fluctuation and these execution times showed some degreeof internal dependency and repeated patterns.
in particular continuous executions had similar execution times.
in addition to the visual illustration we also evaluated the internal dependency of these performance test data quantita tively using autocorrelation acf .
acf computes the internal pearson correlation coefficient pcc of a data set it computes the pcc between the original data set and aderived data set obtained by shifting the data points in theoriginal data set by .
hence acf effectively evaluates theinternal dependency between two consecutive data points in a original performance test data acf .
b a basic bootstrapped resample acf c a block bootstrapped resample acf .
fig.
.
traces of the execution times from a one week performance test for ima aws including the original performance test data and two bootstrapped resamples.
traces are down sampled to for better visibility.
fig.
.
traces of the acfs of resamples generated by two bootstrapping methods for ima aws along with the acf of the original testing data.
table i acf for each benchmark and the a verage acf of the resamples generated by two bootstrapping bt methods .
benchmark original basic bt block bt ft chm .
.
f t a w s .
.
ima chm .
.
ima aws .
.
the series.
table i gives the acf of the two benchmarks on aws and chameleon.
as table i shows the acfs for thesebenchmarks are above .
indicating the existence of internaldata dependency .
prior studies on cloud performance also observed these internal dependencies .
the performance fluctuationin the cloud is mainly caused by the hardware resourcecontention between simultaneously running vms .
as theset of vms that are running simultaneously usually do notchange rapidly the performance of a cloud application usuallyalso varies little within a short period which explains theexistence of the internal data dependency.
bootstrapping and internal data dependency to illustrate the impact of the internal data dependency on boot strapping we applied the basic and block bootstrapping tothe four performance test data sets of ftand ima.
for each bootstrapping resamples were generated i.e.
cis following the standard practice .
figure also shows the traces of two resamples using the basic and block bootstrapping for ima on aws.
as figure shows the basic bootstrapping resample is more random thanthe block bootstrapping resample.
figure gives the acfs ofthese bootstrap resamples for ima on aws which reveals the main issue of the basic bootstrapping its resamples 1note that acf and pcc evaluate the existence of linear correlation.
because the internal dependency of cloud performance data is unlikely strongly linear the acfs for cloud performance data are usually less than .
.here we use acf mainly to show the existence of internal dependency andshow that block bootstrapping preserves the same level of internal dependency.
a ima aws b ima chameloen c ft aws d ft chameloen fig.
.
the distributions and cis for ile execution times generated bytwo bootstrapping methods.
shaded areas show the ranges covered by the cis.
usually had very low acf.
for the majority of the basic bootstrap resamples the acf was nearly indicating thatthese resamples nearly had no internal data dependencies.however the acfs of the resamples from block bootstrappingwere usually close to the original performance data.
table i also reports the average acf of the two bootstrapping methods for all four performance data sets where thebasic bootstrapping has average acfs of nearly .
however block bootstrapping has average acfs very close to theoriginal testing data indicating that block bootstrapping canindeed preserve a similar level of internal dependency.
fundamentally the resampling of the basic bootstrapping is different than how the original performance data are collected.in basic bootstrapping the resampling assumes each executionis independent.
however for the original test consecutiveexecutions had similar execution environments e.g.
similarco running vms .
hence continuous executions are not inde pendent unlike assumed in the basic bootstrapping.
however the resampling in the block bootstrapping assumes consecutiveexecutions are correlated and thus has a better resemblanceto the original performance test.
this difference in resample acfs in turn leads to the 669fig.
.
numbers of performance tests that produced results ile exec.
time with less than errors.
differences of the cis generated based on these resamples.
figure shows the cis of the ile execution time for eachpair of benchmark and cloud.
as figure shows the cisgenerated by the basic bootstrapping are usually smaller thanthose generated by block bootstrapping.
smaller ci indicatesthat basic bootstrapping estimated that the performance resultshad smaller errors.
however if the estimated error is smallerthan the actual error the users would incorrectly assume theirperformance results are accurate and stop their tests too early.
at last we conducted performance tests using the basic and block bootstrapping to test for the ile executiontimes of the above benchmarks.
for these tests the maximumallowed error was set to be .
therefore the tests werestopped when the cis were smaller than .
after the tests the accuracy of the ile execution times wascompared with the ground truth ile execution times where were calculated based on another week data for eachbenchmark from the pt4cloud data set.
figure gives thenumber of accurate tests i.e.
had an error less than .as figure shows for fton both clouds and ima on aws block bootstrapping indeed increased the number of accuratetests.
therefore block bootstrapping should be used in cloudperformance testing than the basic bootstrapping.
for ima on chameleon however the testing result accuracy was worse with block bootstrapping.
the worse results weredue to two issues.
first the cis were generated based on afixed set of testing data and thus may produce inaccurateresults if the test data do not cover all potential performancefluctuations .
second we used a fixed block size inthese tests.
however the proper block size depends on thebehavior of the cloud application and the cloud.
this resultshows that block bootstrapping cannot be blindly applied tocloud performance testing without addressing these two issues.
iii.
t hedesign of metior this section presents our cloud performance testing methodology metior which addresses the aforementioned two issues of block bootstrapping.
it also employs a periodical andintermittent aut execution strategy to reduce testing cost.
to address the incomplete test coverage issue the stop criterion used by metior employs the law of large numbers with states that the experimental mean should be close tothe true mean when the number of trials is large and tendto become closer to the true mean as the number of trialsincreases .
for cloud performance testing this intuitionmay be rephrased as if the mean from a large number ofexecutions in the cloud covers all potential fluctuations and isclose to the true mean then adding a substantial number ofmore executions should not significantly change the value ofthe mean.
based on this intuition the stop criterion of metior stops a test when the performance result obtained from the teststays unchanged after adding significantly more executions.note that although the law of large numbers is only aboutthe mean this intuition can also be applied to percentiles ofthe performance as shown in section iv.
to address the issue where the block size varies with cloud application and cloud platform we employed a methodologydeveloped by politis and white to automatically select theblock size .
intuitively this automatic selection finds the optimal block size using the minimum block size thatprovides a non negligible autocorrelation .
by adoptingthe automatic selection technique the optimal block sizeallows block bootstrapping to retain similar level of internaldata dependency as the original sample.
in metior for each bootstrapping this automatic block size selection is performedso that each performance test uses its own block size.
metior is implemented as a fully automated performance tester.
to apply metior its user only need to provide the vm configuration a cloud application i.e.
aut and itsinput data a maximum allowed error and a confidencelevel.
metior automatically allocates vms conducts tests and applies bootstrapping using a cloud service s programminginterface.
currently metior support cloud services including aws google cloud and chameleon.
note that metior is not designed to generate or prioritizing inputs.
instead it isdesigned to provide accurate cloud performance testing resultsfor any inputs.
a. overview of metior figure 6a gives the overall workflow of metior .
in step metior executes the aut repeatedly for one day.
let the set of performance data collected from these executions be s.i n step metior executes the aut repeatedly for another day to obtain a new data set t.tis then combined with sto obtains prime i.e.
s prime s t. note that section iii c provides the rationale for conducting the tests in terms of days.
in step metior compares sands primeto determine if there is a significant change in the performance results obtained from stos prime.
this comparison follows the intuition of the law of large numbers if the extra data in s primedoes not significantly change the performance results then the performance resultfromsis deemed accurate and the test can be stopped.
however if the change from stos primeis significant metior deems that more executions are required.
hence in step metior lets primebecome the s. it then goes back to step to conduct more executions and collect more performance data togenerate a new s prime.
with the new sands prime a new comparison is performed at step to determine if the test can be stopped.
b. the stop criterion of metior metior s stop criterion uses block bootstrapping to determine if there is a significant change in the performance results obtained from sands prime.
similar to existing cloud performance testing methodologies the user of metior needs to select a 670step execute the aut forone day and collectperf data.
let the perf data set be s.step execute the aut forone more day to collectperf data.
let new data set be t. combine s and t as data set s i.e.
s s t step let s be the new s i.e.
s s stop test and report the perf resultstep compare s and s to check if s can provide accu.
perf result?
yes nostart a workflow of metior .step compute the perf result and and their perc.
diff from s and s .
inputs s s perf test datae max allowed errorcl conf levelstep is e l and h e ?step block bootstrap s and s to establish the ci for with cl cl.
let this ci be l h s cannot provide accu.
result.s can provide accu.
result.yes no b metior s stop criterion.
fig.
.
overall workflow of using metior to conduct a performance test.
maximum allowed error denoted by e and a confidence level denoted by cl .
metior uses block bootstrapping to determine if the performance results obtained from sand s primehave a maximum possible change difference larger than e withcl confidence.
if the maximum possible change difference is less than e then metior deems that scan provide a performance result with less than e error under cl confidence.
this stop criterion is based on the following intuition if both sands primecan provide accurate performance results with less than e error then the maximum difference between the performance results obtained from sands prime should usually be smaller than e .
figure 6b gives the steps performed by metior s stop criterion.
these steps correspond to the internal operations of step in figure 6a.
the first step of this stop criterion step in figure 6b is to calculate the performance results and prime fromsands prime.
let the percentage difference of and primebe prime .
in step metior calculates the ci of withcl confidence.
this ci represents the maximum possible difference between the performance resultsobtained from sands primewithcl possibility.
this ci is calculated using the comparison block bootstrapping with thefollowing procedure .
first two resamples s ands prime are resampled from sands primeusing block bootstrapping.
as stated above the block sizes are automatically selected basedon the data of sands prime.
second the percentage difference between the performance results mean or percentile ofs ands prime is computed.
third the above resampling is performed times providing differences i.e.
... .
fourth the s are sorted and the center cl of the sorted list then gives the ci of with cl confidence.
let the ci of be l h .
in step metior checks if the conditions e l andh e are true.
if both conditions are true then the maximum possible differencebetween the performance results of sands primeis less than e and the performance result of sis considered to be accurate bymetior .
hence the test can be stopped.
otherwise the test continues to step in figure 6a.
c. low cost test execution in metior because cloud performance fluctuates over time we choose to conduct the performance test in small intervals periods of executions rather than a specific number of executions.
thatis in figure 6a metior executes the aut for one day in step1 and .
moreover our analysis on bootstrapping in section iishows that continuous executions usually have similar perfor mance.
therefore there is no need to continuously execute theaut.
instead the aut can be executed intermittently whilestill providing good coverage of performance fluctuations.
ascloud usage cost is charged in terms of seconds or minutes intermittently execution can reduce the number of executionsand the cloud usage expenditure.
nevertheless the interval length and intermittent frequency must be carefully chosen to obtain accurate performanceresults.
prior work employed a similar periodical and intermit tent execution strategy .
however the prior work did notprovide a systematic approach to determine the interval lengthand intermittent frequency.
here we employed a data drivenapproach.
to determine the interval length we evaluated the optimal block sizes for the performance tests data of ftand ima in section ii using the aforementioned automatic block size selection technique.
the largest block size we found was24 hours of executions.
therefore to ensure the executioncount in step is indeed significantly large we set the intervallength to be one day so that step can provide at least onenew block of data.
to determine the intermittent frequency we again used the performance test data from section ii todetermine how many executions can be discarded from the testdata without significantly reducing the autocorrelation acf .we discovered that it needed at least four executions per hourto maintain an acf similar less than .
smaller to theoriginal data.
we used .
as the threshold because less than .
acf is considered as no correlation .
in summary based on the above analysis in step and of metior we choose to execute the aut intermittently times per hour and repeat hourly for one day.
iv .
e xperimental ev aluation this section describes the experimental evaluation of metior which answers the following two research questions what is the accuracy of the performance results obtainedmetior ?
what is the cost of applying metior ?
a. experiment setup benchmarks and clouds.
for this evaluation we used the performance data sets from pt4cloud .
pt4cloud datasets provide the performance data of executing six benchmarkson two public clouds aws and chameleon continuouslyfor eight weeks.
table ii gives the details of these sixbenchmarks and table iii gives the types and counts of the six 671table ii benchmarks used in the ev aluation .
benchmark domain type of perf.
origin ft hpc execution time npb ep hpc execution time npb jpetstore jps web throughput j2ee ycsb db throughput ycsb tpc c db throughput oltpbench inmem analy.
ima ml execution time cloudsuite table iii vm configurations from chameleon c and aws a used in the ev aluation .
config.
vm cnt x vm type cores vm m e m v m c s x small 2gb c m x medium 4gb c l x large 8gb a s x m5.large 8gb a m x m5.xlarge 16gb a l x m5.2xlarge 32gb virtual machine vm configurations used in pt4cloud.
each benchmark was executed on all six vm configurations exceptfor benchmarks ft ep and ima which could not be executed on the small vms of chameleon i.e.
c sm in table iii due to inefficient memory.
here a pair of benchmark and vmconfiguration is called a benchmark configuration.
in total all benchmark configurations from pt4cloud were evaluated.
evaluation methodology.
we partitioned the week performance data for each benchmark configuration into twoparts.
the first part contained week of data and was usedto conduct performance tests whereas the rest week datawere used as ground truth.
the week data have a largenumber of performance data points with each data pointcontains the performance execution time or throughput ofone execution.
from each data point a performance test couldbe conducted simulated .
for instance starting from a datapoint a performance test with metior can be simulated by continuously reading in data points until metior s stop criterion deems that the performance test can be stopped.
after theperformance test is stopped the data points read during thetest can then be used to calculate the performance results.because the week data of each benchmark configurationcontain hundreds or thousands of data points repeating thesimulated performance test starting from every data point ledto hundreds or thousands of simulated performance tests foreach benchmark configuration allowing a thorough evaluation.
in this evaluation the performance tests were used to obtain performance results as the mean and ile of the executiontime or throughput reflecting the average and tail performance.the maximum allowed errors i.e.
the e in figure were set to be and .
the confidence level i.e.
thecl in figure was chosen to be .
baselines.
as stated above the last week data of each benchmark configuration were used to obtain the ground truthperformance.
for cloud performance testing the ground truthshould be the performance results obtained from extremelylong performance tests that can truly cover all performancefluctuations.
our current performance results showed that theperformance results from or more weeks of executionsare usually stable enough to be used as ground truth.
besidesthe ground truth we also compared metior with three stateof the art cloud performance testing methodologies.
the firstmethodology basicbt used the basic bootstrapping .
thesecond methodology cov uses the changes in the coefficientof variation of the performance data as the stop criterion .the third methodology is pt4cloud .
metrics.
the percentage error of each performance result obtain with metior perf metior is calculated by comparing it with the ground truth performance perftrue using the following equation err perf metior perftrue perftrue .
because large numbers of performance tests were conducted using metior for each benchmark configuration we report the percentage of tests that provided performance results withless than the maximum allowed error.
for example if themaximum allowed error is then we report the percentageof the performance tests that indeed provided performanceresults with less than error.
moreover recall that in thispaper we call a performance test result as accurate if it indeed has an error less than the maximum allowed error.
open data.
our data and source code are available at b. accuracy evaluation with maximum allowed error accuracy of metior figure gives the percentage of the metior performance tests that provided accurate mean performance i.e.
with less than error .
as figure shows of metior s tests conducted on aws provided mean performance with less than error.
on chameleon exceptfor benchmark configurations of jps c s and tpcc c m metior ensured more than of the tests were accurate for all benchmark configurations.
the benchmark performance onchameleon had larger fluctuations than aws making it moredifficult to obtain accurate results.
nonetheless even for jpsc sand tpcc c m the tests could still provide performance results with low error.
for jps c s the largest error among all the tests conducted for it was only .
.
2for tpcc cm the largest error was .
.
figure 9a gives the averagepercentages of metior s tests for mean performance that were accurate.
overall of the tests on chameleon and of aws tests had less than errors.
of all tests on twoclouds had less than errors.
figure gives the percentage of the metior performance tests that provided accurate ile performance i.e.
with lessthan error .
again of metior s tests conducted on aws provided ile performance with less than errors.on chameleon except for the configurations of ycsb c s 2because hundreds or thousands of tests were conducted for each benchmark configuration it is impossible to provide the error for each test in figure and figure .
due to space limitation we also cannot provide themax and average error for tests of each benchmark configuration.
672ft c l ft c m ep c l ep c m ima c l jps c l jps c m jps c s tpcc c l tpcc c m ft a l ft a m ft a s ep a l ep a m ep a s ima a l ima a m ima a s jps a l jps a m jps a s tpcc a l tpcc a m tpcc a s ycsb a l ycsb a m ycsb a s 0102030405060708090100percentage basicbt cov pt4cloud metior fig.
.
percentages of the tests that provided mean performance results with less than errors.
ft c l ft c m ep c l ep c m ima c l ima c m jps c l jps c m jps c s tpcc c l tpcc c m tpcc c s ycsb c l ycsb c s ft a l ft a m ft a s ep a l ep a m ep a s ima a l ima a m ima a s jps a l jps a m jps a s tpcc a l tpcc a m tpcc a s ycsb a l ycsb a m ycsb a s 0102030405060708090100percentage basicbt cov pt4cloud metior fig.
.
percentages of the tests that provided ile performance results with less than errors.
a c t a c t a c t 5060708090100percentage basicbt cov pt4cloud metior a tests for mean performance.
a c t a c t a c t 405060708090100percentage basicbt cov pt4cloud metior b tests for ile performance.
fig.
.
summary of the percentages of the performance tests that were accurate.
results are reported for three and maximum allowederrors on aws a chameleon c and all tests t .
and ycsb c l metior ensured more than of the tests were accurate for all benchmark configurations.
similar to the mean performance metior s performance results still had low errors for ycsb c s and ycsb c l the maximum errors of the tests conducted for ycsb c s and ycsb c l were only .
and .
respectively.2figure 9b gives the average percentages of metior s tests for ile performance that were accurate.
overall of the chameleon tests and ofaws tests had less than errors.
of all tests on twoclouds had less than errors.
note that for mean performance results the tests for five benchmark configurations could not stop with just week ofdata.
for ile performance results the performance testsfor ycsb c m could not stop with week of data.
these benchmark configurations are not shown in figure andfigure but are discussed in the next section.
unstoppable benchmark configurations for the mean performance for five benchmark configurations ima c m tpcc c s ycsb c l ycsb c m and ycsb c s metior deemed that all their performance tests should not stop i.e.
could not provide accurate results with just week data.
forthe ile performance metior deemed that all the tests of ycbs c m should not stop with just week data.
therefore the results of these benchmark configurations are not shown infigure and figure .
note that as pt4cloud only provided8 weeks of performance data it was also impossible for usto continue the tests without running into ground truth data.therefore we terminated the tests once all week data wereused and report these tests as unstoppable.
for three benchmark configurations ima c m tpcc c s and ycsb c m week data indeed could not provide mean or ile performance with less than error.
for ycsb c l and ycsb c s although week data could provide accurate performance results their performance data had high varia tions.
these high variations resulted in wide cis for their meanor ile performance making it impossible to conclude thatthe performance results from week data were accurate with95 confidence.
therefore we concluded that metior indeed should not stop the tests and behaved as expected for thesebenchmark configurations.
these unstoppable tests also reflectthe difficulty of conducting cloud performance testing underlarge performance fluctuation and show the importance ofexploring reliable cloud performance testing methodologies.
comparison with pt4cloud the percentages of the accurate tests for pt4cloud are also given in figure and figure .
as the figures show except for the above fourconfigurations metior had more accurate tests than pt4cloud for all other benchmark configurations.
when testing for themean of ima c l and ile of tpcc c m pt4cloud was slightly better than metior with less than more accurate tests.
when testing for the mean of tpcc c m and ile ofycsb c l p4cloud had and more accurate tests than metior .
nonetheless when testing for the mean of tpccc m the average and maximum errors of the tests conductedby pt4cloud and metior were similar the average errors for pt4cloud and metior were and and the maximum errors for pt4cloud and metior were .
and .
.
2for the ile tests of ycsb c l the average errors of pt4cloud and metior were both whereas pt4cloud maximum error .
was higher than metior .
.
metior s similar or even better average or maximum errors show that althoughmetior had fewer accurate tests for these two configurations the accuracy of individual tests of metior was still similar to those of pt4cloud and metior s tests also had errors very close to the maximum desired error.
moreover metior was more accurate for all other benchmark configurations.
figure also gives the overall accuracy of pt4cloud which shows that pt4cloud s overall accuracy was similar orworse than metior .
especially when testing for the ile on chameleon with max error metior has more accurate tests than pt4cloud.
pt4cloud stopped the test with theanticipation that the distribution i.e.
most of the percentiles had less than error.
however it did not imply that the meanor every percentile also had less than error.
therefore theerrors for some point estimates may still be larger than causing p4cloud to have lower accuracy than metior .
note that for several benchmark configurations pt4cloud could not stop the tests with week of data in addition tothose discussed in section iv b2 .
the pt4cloud bars of theseconfigurations are omitted in figures and .
comparison with basicbt method figures and also show the percentages of tests that had less than errors for basicbt .
as both figures show basicbt hadlower or similar accuracy than metior for every benchmark configuration.
in several cases basicbt was significantly lessaccurate than metior such as jps c l and tpcc c l. figure gives the overall accuracy of basicbt.
on aws basicbt had performed reasonably well with of allbasicbt s tests had less than errors.
nonetheless it wasstill lower than the of metior .
moreover the large performance fluctuation on chameleon had made it particu larly difficult for basicbt to stay accurate.
on chameleon of basicbt s tests for mean and of basicbt stests for ile had less than errors whereas ofmetior s tests on either mean or ile were accurate.
as analyzed in section ii basicbt has two issues of incompleteperformance fluctuation coverage and not considering internaldata dependency causing the relatively low accuracy.mean ile average0123 .
.
.
.
.
.04normalized executionsbasicbt cov metior pt4cloud fig.
.
the number of executions conducted by evaluated testing methodologies normalized to metior .
comparison with the cov method figures and also show the percentages of tests that had less than errors for the cov method .
overall cov had the lowest accuracyamong the three cloud performance testing methodologies.
onaws of cov s tests could provide accurate means or90 iles.
on chameleon of cov s tests could provideaccurate means and of its tests could provide accurate90 ile performance.
the cov method was not designed toobtain accurate performance results with a maximum desirederror.
instead it was designed to detect if the performanceresults from microbenchmarks were stable enough.
hence because of having a different design goal cov s accuracywas lower than the other methodologies in this evaluation.
c. testing cost for max allowed errors to obtain the mean performance metior conducted executions per test on average.
for the ile performance metior conducted .
executions per test on average.
these executions roughly translate into three days of execution pertest.
as we used existing data sets we do not have the awsbills for these tests.
therefore we estimated the testing costbased on the test execution time and aws s cloud usage rates chameleon is a free research cloud without charges .
theestimation shows that metior s cost for one test on aws ranged from .
to .
with an average cost of .
.
figure compares the average number of executions per test required by the evaluated performance testing method ologies.
as figure shows the number of executions re quired by pt4cloud was .
times of those used by metior on average.
this high number of executions was becausept4cloud was designed to obtain performance distributions.when a whole distribution is deemed accurate i.e.
with lessthan error by pt4cloud most of the percentiles of theperformance distribution had about error which requiredmany more tests than just obtaining one accurate mean orpercentile.
consequently pt4cloud incurs unnecessary costsfor performance tests that only need to obtain point estimates and its cost may be prohibitively high when a cloud applicationhas a large number of inputs that need to be tested.
note that metior s reduction in execution count does not only imply less monetary cost it also indicates a reduction in temporal cost and hence a faster development deployment cycle.
basicbt and cov required fewer executions than metior as shown in figure .
specifically the average executioncounts per test for basicbt were to of metior s execution counts and cov s execution counts were about 674ofmetior .
however as basicbt and cov had lower accuracy than metior these fewer executions did not indicate a cost reduction but indicated that basicbt and cov stopped their tests too early before obtaining accurate performance results.
d. sensitivity to maximum allowed errors we also evaluated metior with and maximum allowed errors.
the evaluation results are summarized in figure .
as figure shows metior s retained its accuracy when the maximum allowed errors were increased.
on aws of metior s tests were still accurate.
on chameleon more than of metior s tests were accurate when the maximum error was and more than of metior s tests were accurate when the maximum error was .
the accuracy of pt4cloud basicbt and cov was also improved under and maximum errors.
however pt4cloud still had lower overall accuracy than metior .
moreover the numbers of executions per test of pt4cloud werestill .
and .
times more than metior for than maximum errors.
basicbt still struggled on chameleon andhad considerably lower accuracy than metior even with larger maximum errors.
the accuracy of cov was even lower thanbasicbt as it is not designed to obtain performance withspecified maximum errors.
v. c ase study and application of metior a case study.
to demonstrate the usefulness of metior to cloud researchers and practitioners we applied metior to a cloud optimization technique cherrypick which selectsthe best vm configuration for a cloud application by predictingits performance on different vm configurations.
cherrypickemploys gaussian process gp to make the prediction which requires a training data set which consists of theperformance of the application running in other vms.
here we applied metior to obtain accurate performance results as the training sets to show that metior can improve cherrypick s prediction accuracy.
more specifically cherrypick was used to predict the performance of the six benchmarks in table ii when they wereexecuting on the aws m5.xlarge configuration i.e.
am .
the training data sets were composed of performance data points for five vm configurations including m5.large a s m5.2xlarge a l m5.large m5.large and m5.xlarge.
the original cherrypick techniques asked for data points for each vm configuration in the training set.however for the metior enhanced cherrypick the data points for each vm configuration must be many enough to providean accurate mean using the metior methodology.
that is metior was used to obtain accurate mean performances for each vm configuration then the acquired mean performancesof each vm configuration were used as training data sets forcherrypick to make predictions.
new tests were conductedfor the vm configurations i.e.
m5.large m5.large and m5.xlarge that are not included in the pt4cloud data sets.
for a thorough evaluation performance predictionsjps ft ep ima ycsb tpcc avg020406080100perc error original metior enhanced fig.
.
average prediction errors for the original cherrypick and metior enhanced cherrypick.
were made using the original and metior enhanced cherrypick.
then for each performance prediction its percentage error was computed by comparing the prediction with the groundtruth performance which was the same five week performanceused in the previous evaluation.
the mean absolute percentageerrors mape are then reported in figure .
note that thiscase study was only conducted on aws as chameleon onlyoffered three vm configurations cherrypick requires morethan configurations .
as figure shows metior enhanced cherrypick had higher accuracy than the original cherrypick for every bench mark.
on average metior enhanced cherrypick s prediction error was .
less than the original cherrypick.
moreover metior enhanced cherrypick had more stable predictions i.e.
less variation in prediction accuracy .
the main benefit ofmetior is that it can provide more accurate training data sets which contained many more samples than the originalcherrypick.
note that simply adding more training data donot always increase cherrypick s accuracy because without areliable performance testing methodology it is unclear howmany more training data should be added to ensure goodaccuracy.
the authors of cherrypick also noted that gpwas considered as just accurate enough to separate fast vmconfigurations from slower ones.
nonetheless a later studyshowed that a gp model with better accuracy could improvethe accuracy of identifying better performing vms .
thehigher accuracy could improve cherrypick s ability at vmconfiguration optimization .
these results illustrate that byproviding more reliable training sets and accurate performanceresults metior is valuable for both cloud research and practice.
application of metior.
the necessity of accurate cloud performance results was documented by prior cloud perfor mance test studies .
in cloud deployments a key step is to select the proper vm configuration thatmeets the performance requirement .
the selection ofauto scaling policies also requires determining a proper vmconfiguration that meets the performance requirement as thescaling target .
the most reliable way to determineif a vm configuration meets a performance requirement isperformance testing.
for cloud research obtaining accurateperformance results is also the fundamental requirement.
ac curate performance results are required to evaluate new cloudsystem application designs and develop new optimizationtechniques as shown with the case study .
675vi.
t hreats to validity execution environment changes.
metior assumes that the execution environments including the cloud hardware infrastructure and the statistical behavior of multi tenancy remain unchanged during the performance test and after thedeployment.
in our experience the hardware infrastructure atchameleon and aws remained unchanged for years and themulti tenancy behaviors were also consistent within at leasta year.
therefore performance tests conducted with metior with only a few days or weeks of executions can accuratelyprovide the performance of cloud deployments within at leastone year.
however if the execution environment changes newperformance tests should be conducted.
nonetheless the cloud execution environment does experience long term e.g.
after multiple years changes.
therefore new performance testing methodologies are required to tacklethis long term performance change.
we plan to redesignmetior into cloud apis to allow it to detect performance variations caused by execution environment changes.
other cloud applications test inputs and cloud service providers.
although we strive to provide a comprehen sive evaluation the exact accuracy of metior may change with the cloud applications performance test inputs and the cloudservice providers.
nonetheless we expect metior s behavior to be generally consistent over a variety of cloud applications inputs and cloud services.
moreover metior is used to obtain the accurate performance of a cloud application given one testinput.
metior does not aim at determining what test inputs should be included in the performance tests.
vii.
r elated work cloud performance testing.
performance testing is a fundamental task in computer science .
jain documentedthe methodology of using ci for performance measurement indetail .
maricq et al.
recently improved this methodologyto cloud computing by employing the basic bootstrapping .wang et al.
also employed basic bootstrapping in cloudperformance testing .
however as shown in this paper the basic bootstrapping based testing methods had loweraccuracy partially due to overlooking internal data depen dency.
pt4cloud was a performance testing technique forobtaining performance distributions of cloud applications .our work is inspired by pt4cloud especially in the use ofperiodical and intermittent executions.
however pt4clouddetermined the parameters of interval length and intermittentfrequency empirically whereas metior employed a systematic approach to select these parameters.
moreover as shownin section iv b3 pt4cloud had higher errors and highercosts than metior .
laaber et al.
proposed a stop criterion for executing microbenchmark in the cloud using the coefficientof variation .
however as shown in section iv b3 thisstop criteria had lower accuracy as it was not designed toobtain performance results with a maximum allowed error.alghmadi et al.
proposed a stop condition for performancetesting by determining the repetitions in performance data .as shown in prior work this stop condition was not suitablefor testing performance in cloud computing .
duet bench marking is a technique to compare the performance of twocloud system designs or optimizations .
while thistechnique provides accurate comparisons it was not designedto determine the exact performance of a cloud application.
other related work cloud performance prediction models were built to predict a cloud application s performance on avm configuration or cloud service to aid cloud resource allo cation .
these models used performance testingresults as their training data.
some studies also predicted andestimated non cloud software performance with models andsimulators .
as shown with our case study section v metior can provide more accurate cloud performance results as reliable training and testing data sets to facilitate the devel opment of performance modeling and simulation.
there werealso studies on test inputs generation and prioritization forperformance testing .
several studies alsoinvestigated performance change identification in configurablesystems .
these studies are orthogonal to metior a s metior focused on providing accurate performance results for any test inputs and or configurations.
viii.
c onclusion and future work this paper addressed the cloud performance testing problem.
we first conducted an analysis to show that the basicbootstrapping could not always provide accurate performanceresults due to the overlooked internal dependency with cloudperformance data.
we then present metior a reliable automated performance testing methodology using block boot strapping which considers the internal dependency of cloudperformance data.
to improve performance fluctuation cov erage and further reduce testing cost metior also employed the law of large numbers and conducted tests periodicallyand intermittently.
experiment results showed that metior ensured that more than of tests had less than error.for future work we will use metior to test different types of cloud services more cloud service providers as well asadditional types of applications.
we also plan to implementmetior as a serverless api so that it can easily used by cloud practitioners for performance testing and for long termperformance fluctuation monitoring.
a cknowledgment this work was supported by the national science foundation under grants ccf ccf and cns .
the views and conclusions contained herein arethose of the authors and should not be interpreted as neces sarily representing the official policies or endorsements eitherexpressed or implied of nsf.
the authors would like to thankthe anonymous reviewers for their insightful comments.
r eferences neeraja j. yadwadkar bharath hariharan joseph e. gonzalez burton smith and randy h. katz.
selecting the best vm across multiple public clouds a data driven performancemodeling approach.
in acm symp.
on cloud computing .
stoyan stefanov.
yslow .
.
in csdn software development .
conference .
marissa mayer.
in search of a better faster strong web .
timothy zhu michael a. kozuch and mor harchol balter.
workloadcompactor reducing datacenter cost while providingtail latency slo guarantees.
in acm symp.
on cloud computing .
ming mao and marty humphrey.
auto scaling to minimize cost and meet application deadlines in cloud workflows.
inproc.
of int l conf.
for high performance computing network ing storage and analysis .
omid alipourfard hongqiang harry liu jianshu chen shivaram venkataraman minlan yu and ming zhang.
cherrypick adaptively unearthing the best cloud configurations for bigdata analytics.
in usenix symp.
on networked systems design and implementation .
f. l. ferraris d. franceschelli m. p. gioiosa d. lucia d. ardagna e. di nitto and t. sharif.
evaluating the autoscaling performance of flexiscale and amazon ec2 clouds.
inint l symp.
on symbolic and numeric algorithms for scientificcomputing .
mark grechanik qi luo denys poshyvanyk and adam porter.
enhancing rules for cloud resource provisioning via learnedsoftware performance models.
in acm spec on int l conf.
on performance engineering .
w. wang n. tian s. huang s. he a. srivastava m. l. soffa and l. pollock.
testing cloud applications under cloud uncertainty performance effects.
in int l conf.
on software testing v erification and v alidation .
yutong zhao lu xiao xiao wang bihuan chen and yang liu.
localized or architectural an empirical study of performanceissues dichotomy.
in int l conf.
on software engineering companion proceedings .
andreas burger heiko koziolek julius r uckert marie platenius mohr and g osta stomberg.
bottleneck identification and performance modeling of opc ua communicationmodels.
in proc.
of acm spec int l conf.
on performance engineering icpe .
aleksander maricq dmitry duplyakin ivo jimenez carlos maltzahn ryan stutsman and robert ricci.
taming perfor mance variability.
in usenix symp.
on operating systems design and implementation .
sen he glenna manns john saunders wei wang lori pollock and mary lou soffa.
a statistics based performancetesting methodology for cloud applications.
in proc.
of acm joint meeting on european software engineering conf.
andsymp.
on the f oundations of software engineering .
philipp leitner and j urgen cito.
patterns in the chaos a study of performance variation and predictability in public iaasclouds.
acm trans.
internet technol.
april .
alexandru iosup simon ostermann nezih yigitbasi radu prodan thomas fahringer and dick epema.
performanceanalysis of cloud computing services for many tasks sci entific computing.
ieee transcations on parallel distributed system june .
christoph laaber joel scheuner and philipp leitner.
software microbenchmarking in the cloud.
how bad is it really?empirical software engineering .
catia trubiani pooyan jamshidi jurgen cito weiyi shang zhen ming jiang and markus borg.
performance issues?
heydevops mind the uncertainty.
ieee software .
james e. ii bartlett w. joe kotrlik and chadwick c. higgins.
organizational research determining appropriate sample sizein survey research.
information technology learning and performance journal .
raj jain.
the art of computer systems performance analysis techniques for experimental design measurement simulation and modeling.
john wiley sons .
christoph laaber stefan w ursten harald c. gall and philipp leitner.
dynamically reconfiguring software microbench marks reducing execution time without sacrificing resultquality .
in proc.
of european software engineering conference and symp.
on the f oundations of software engineering .
david shue michael j. freedman and anees shaikh.
performance isolation and fairness for multi tenant cloud storage.inproc.
of usenix conf.
on operating systems design and implementation .
soumendra nath lahiri.
resampling methods for dependent data.
springer science business media .
john a gubner.
probability and random processes for electrical and computer engineers.
cambridge university press .
amazon.
amazon web services.
.
google.
.
a configurable experimental environment for large scale cloud research.
.
a colin cameron and pravin k trivedi.
microeconometrics methods and applications.
cambridge university press .
bradley efron.
the jackknife the bootstrap and other resampling plans.
siam .
a. c. davison and d. v .
hinkley.
bootstrap methods and their application.
cambridge university press .
dimitris n. politis and halbert white.
automatic block length selection for the dependent bootstrap.
econometric reviews .
d.h. bailey e. barszcz j.t.
barton d.s.
browning r.l.
carter l. dagum r.a. fatoohi p.o.
frederickson t.a.
lasin ski r.s.
schreiber et al.
the nas parallel benchmarkssummary and preliminary results.
in int l conf.
on supercomputing .
michael ferdman almutaz adileh onur kocberber stavros v olos mohammad alisafaee djordje jevdjic cansu kaynak adrian daniel popescu anastasia ailamaki and babak falsafi.clearing the clouds a study of emerging scale out workloadson modern hardware.
in int l conf.
architectural support for programming languages and operating systems .
patrick schober christa boer and lothar a schwarte.
correlation coefficients appropriate use and interpretation.
anesthesia analgesia .
m. hajjat r. liu y .
chang t. s. e. ng and s. rao.
application specific configuration selection in the cloud impactof provider policy and potential of systematic testing.
in ieee conf.
on computer communications infocom .
frederik michel dekking cornelis kraaikamp hendrik paul lopuha a and ludolf erwin meester.
a modern introduction to probability and statistics.
springer science business media .
oracle.
java platform enterprise edition java ee .
.
brian f. cooper adam silberstein erwin tam raghu ramakrishnan and russell sears.
benchmarking cloud servingsystems with ycsb.
in proc.
of acm symposium on cloud computing .
djellel eddine difallah andrew pavlo carlo curino and philippe cudre mauroux.
oltp bench an extensible testbedfor benchmarking relational databases.
proc.
vldb endow.
december .
carl edward rasmussen.
gaussian processes in machine learning.
in summer school on machine learning pages .
springer .
c. hsu v .
nair v .
w. freeh and t. menzies.
arrow low level 677augmented bayesian optimization for finding the best cloud vm.
in ieee int l conf on distributed computing systems .
maciej malawski gideon juve ewa deelman and jarek nabrzyski.
cost and deadline constrained provisioning forscientific workflow ensembles in iaas clouds.
in proceedings of the international conference on high performance comput ing networking storage and analysis .
jing jiang jie lu guangquan zhang and guodong long.
optimal cloud resource auto scaling for web applications.incluster cloud and grid computing ccgrid 13th ieee acm international symposium on pages .
m. albonico s. d. alesio j. mottu s. sen and g. suny e. generating test sequences to assess the performance of elasticcloud based systems.
in ieee int l conference on cloud computing cloud .
ali abedi and tim brecht.
conducting repeatable experiments in highly variable cloud computing environments.
in proceedings of the 8th acm spec on international conference onperformance engineering .
h. m. alghmadi m. d. syer w. shang and a. e. hassan.
an automated approach for recommending when to stopperformance tests.
in int l conf.
on software maintenance and evolution .
lubom r bulej v ojt ech hork y petr tuma franc ois farquet and aleksandar prokopec.
duet benchmarking improvingmeasurement accuracy in the cloud.
in acm spec int l conf.
on performance engineering icpe .
luke bertot st ephane genaud and julien gossa.
improving cloud simulation using the monte carlo method.
in euro par parallel processing .
steffen becker lars grunske raffaela mirandola and sven overhage.
performance prediction of component based sys tems.
in ralf h. reussner judith a. stafford and clemens a.szyperski editors architecting systems with trustworthy components pages berlin heidelberg .
springerberlin heidelberg.
catia trubiani indika meedeniya vittorio cortellessa aldeida aleti and lars grunske.
model based performance analysisof software architectures under uncertainty.
in proceedings of int l acm sigsoft conf.
on quality of software architectures .
ivan postolski victor braberman diego garbervetsky and sebastian uchitel.
simulator based diff time performancetesting.
in proc.
of int l conf.
on software engineering new ideas and emerging results .
n. siegmund s. s. kolesnikov c. k astner s. apel d. batory m. rosenm uller and g. saake.
predicting performance via automated feature interaction detection.
in int l conf.
on software engineering icse .
guoliang zhao safwat hassan ying zou derek truong and toby corbin.
predicting performance anomalies in softwaresystems at run time.
acm transactions on software engineering and methodology december .
xusheng xiao shi han dongmei zhang and tao xie.
contextsensitive delta inference for identifying workload dependentperformance bottlenecks.
in international symposium on software testing and analysis issta .
h. ha and h. zhang.
deepperf performance prediction for configurable software with deep sparse neural network.
inieee acm int l conf on software engineering icse .
yuhui lin adam barker and john thomson.
modelling vm latent characteristics and predicting application performanceusing semi supervised non negative matrix factorization.
inieee int l conf.
on cloud computing cloud .
emilio coppa camil demetrescu and irene finocchi.
inputsensitive profiling.
in proc.
of the conf.
on programminglanguage design and implementation .
sudipta chattopadhyay lee kee chong and abhik roychoudhury.
program performance spectrum.
in proc.
of acm conf.
on languages compilers and tools for embedded systems .
mark d. syer zhen ming jiang meiyappan nagappan ahmed e. hassan mohamed nasser and parminder flora.continuous validation of load test suites.
in int l conf.
on performance engineering .
cornel barna marin litoiu and hamoun ghanbari.
autonomic load testing framework.
in int l conf.
on autonomic computing .
jacob burnim sudeep juvekar and koushik sen. wise automated test generation for worst case complexity.
inproc.s of int l conference on software engineering .
pingyu zhang sebastian elbaum and matthew b. dwyer.
automatic generation of load tests.
in proc.
of int l conf.
on automated software engineering .
bihuan chen yang liu and wei le.
generating performance distributions via probabilistic symbolic execution.
in proc.
of int l conf on software engineering .
michael pradel markus huggler and thomas r. gross.
performance regression testing of concurrent classes.
in int l symp.
on software testing and analysis .
xue han tingting yu and david lo.
perflearner learning from bug reports to understand and generate performancetest frames.
in acm ieee int l conf on automated software engineering .
d. marijan and m. liaaen.
effect of time window on the performance of continuous regression testing.
in ieee int l conf.
on software maintenance and evolution icsme .
pengyu nie ahmet celik matthew coley aleksandar milicevic jonathan bell and milos gligoric.
debugging theperformance of maven s test isolation experience report.
inproceedings of the 29th acm sigsoft international sympo sium on software testing and analysis issta .
marija selakovic thomas glaser and michael pradel.
an actionable performance profiler for optimizing the order ofevaluations.
in proceedings of the 26th acm sigsoft international symposium on software testing and analysis issta2017 .
a. banerjee s. chattopadhyay and a. roychoudhury.
static analysis driven cache performance testing.
in ieee 34th realtime systems symposium .
hazem samoaa and philipp leitner.
an exploratory study of the impact of parameterization on jmh measurement resultsin open source projects.
in acm spec int l conf.
on performance engineering icpe .
hammam m. alghamdi cor paul bezemer weiyi shang ahmed e. hassan and parminder flora.
towards reducing thetime needed for load testing.
journal of software evolution and process page e2276 .
s. m uhlbauer s. apel and n. siegmund.
identifying software performance changes across variants and versions.
inieee acm int l conf.
on automated software engineering ase .
c. trubiani and s. apel.
plus performance learning for uncertainty of software.
in ieee acm 41st international conference on software engineering new ideas and emergingresults icse nier .