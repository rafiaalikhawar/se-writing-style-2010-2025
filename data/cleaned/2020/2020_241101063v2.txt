inter trans leveraging transitive intermediate translations to enhance llm based code translation 1stmarcos macedo school of computing queen s university kingston on canada marcos.macedo queensu.ca2ndyuan tian school of computing queen s university kingston on canada y.tian queensu.ca3rdpengyu nie cheriton school of computer science university of waterloo waterloo on canada pynie uwaterloo.ca 4thfilipe r. cogo centre for software excellence huawei canada kingston on canada filipe.roseiro.cogo1 huawei.com5thbram adams school of computing queen s university kingston on canada bram.adams queensu.ca abstract code translation aims to convert a program from one programming language pl to another.
this long standing software engineering task is crucial for modernizing legacy systems ensuring cross platform compatibility enhancing performance and more.
however automating this process remains challenging due to many syntactic and semantic differences between pls.
recent studies show that even advanced techniques such as large language models llms especially open source llms still struggle with the task.
currently code llms are trained with source code from multiple programming languages thus presenting multilingual capabilities.
in this paper we investigate whether such capabilities can be harnessed to enhance code translation.
to achieve this goal we introduce i nter trans an llm based automated code translation approach that in contrast to existing approaches leverages intermediate translations to bridge the syntactic and semantic gaps between source and target pls.
i nter trans contains two stages.
it first utilizes a novel tree of code translation toct algorithm to plan transitive intermediate translation sequences between a given source and target pl then validates them in a specific order.
we evaluate i nter trans with three open llms on three benchmarks i.e.
codenet humaneval x and transcoder involving six pls.
results show anabsolute improvement of .
to .
in computation accuracy ca for i nter trans over direct translation with attempts.
the best performing variant of i nter trans with the magicoder llm achieved an average ca of .
.
on three benchmarks.
i. i ntroduction automatically translating source code between different programming languages pls can significantly reduce the time and effort required for software development teams.
in the literature researchers have proposed various automated code translation methods.
data driven learning based approaches have shown impressive improvements over traditional rule based methods .
unlike rulebased approaches which rely on handcrafted rules and program analysis techniques learning based methods can auto matically learn syntactic and semantic patterns from largescale code repositories.
large language models llms represent the most advanced learning based approaches developed in recent years and have demonstrated promising results across various software engineering tasks .
pre trained on vast amounts of code across dozens of pls and text data and equipped with billions of parameters llms can be applied directly to code translation without the need for task specific continuous training finetuning.
this would eliminate the need for costly and timeconsuming processes involved in collecting training datasets and developing specialized models for code translation.
however recent studies have shown that the performance of llm based automated code translation particularly with open source llms is still far from the production level with correct translations ranging from .
to .
.
these studies found that many errors in llm generated code translations stem from the models lack of understanding of syntactic and semantic discrepancies between source and target languages which can vary significantly across different pairs.
for instance of the errors in translating from c to go are due to syntactic and semantic differences while only .
of such errors occur when translating from c to c .
this variation is intuitive as certain pls naturally share more similarities in syntax and semantics than others.
a similar phenomenon has been observed in machine translation for human languages where translating between certain languages is easier than others .
to improve translations for challenging language pairs a common strategy is to use parallel corpora with a pivot bridge language .
in fact traditional statistical machine translation between non english languages such as french to german often involves pivoting through english .
this approach remains effective with the rise of multilingual neural machine translation models.
for instance in a recent work by meta training language pairs were collected based on linguistic families and bridgearxiv .01063v2 nov 2024languages facilitating translation across numerous language pairs without exhaustively mining every possible pair.
inspired by this idea this paper explores the potential of leveraging transitive intermediate translations from a source pl into other pls before translating to the desired target pl an idea not previously explored in the field of automated code translation.
for example to translate a program written in python to java we might first translate it from python to c and then from c to java as illustrated in figure .
this process is done through prompting without additional training data thanks to code llms that are pre trained on text and code across multiple pls and naturally possess multilingual capabilities.
while this idea is inspired by machine translation its potential in the inference stage of llm based translation approaches has not been explored.
despite the conceptual simplicity of the idea a major challenge to address is the choice of the number and type of intermediate language s since the optimal choice might be different for each pair of pls or even each pair of code snippets.
the idea of utilizing existing pls as bridges is different than earlier work transcoder ir a non llm learningbased method that enhances source code pairs by incorporating their corresponding low level language agnostic compiler intermediate representations ir such as llvm irs into the training dataset.
instead of relying on one unified ir to bridge any pair of cross pl translations we systematically explore different potential transitive intermediate translations using multiple existing pls.
inter trans our novel llm based code translation approach that enhances source target translations via transitive intermediate translations operates in two stages.
in the first stage a method called tree of code translations toct generates a translation tree containing all potential translation paths for a specific source target pl pair conditioned to a set of pre defined intermediate pls and the maximum number of intermediate translations to be explored.
in the second stage translation paths are turned into llm prompts that are executed in a breadth first order.
i nter trans then uses a readily available test suite to validate whether the generated translation to the target language is correct enabling early termination of translation path exploration if a successful path is found before completely exploring the translation tree.
to evaluate the effectiveness of i nter trans we conducted experiments using three code llms code llama magicoder and starcoder2 on translation problems sourced from three datasets i.e.
codenet humaneval x and transcoder .
each translation problem aims to translate a program writing in a source pl to a target pl.
these problems involve different sourcetarget pl pairs across six languages c javascript java python go and rust.
our results show that i nter trans consistently outperforms direct translation i.e.
without intermediate language translation with attempts achieving an absolute computational accuracy ca improvement of .
to .
median .
across the three llms and datasets.
through ablation studies we analyzed the effects ofvarying the number and selection of intermediate languages on inter trans s performance.
generally increasing the number of intermediate translations enhances ca though the benefits taper off after three translations.
similarly incorporating more intermediate languages is advantageous but gains slow after including three languages.
the effectiveness of specific intermediate pls varies across translation pairs with notable patterns observed in translations from c python to java via rust and from rust to go via c .
the main contributions of this paper are as follows we present the first study demonstrating that intermediate translations based on existing pls can enhance the performance of llm based code translation.
we propose toct a novel planning algorithm designed to explore intermediate translations effectively.
we also introduce i nter trans an llm based code translation approach that uses toct and is orthogonal to existing approaches for code translation.
we conducted a comprehensive empirical study to evaluate inter trans .
our results highlight the effectiveness of inter trans in enhancing llm based code translation.
we also provide insights for the practical application of inter trans .
the code for implementing i nter trans the datasets and the notebooks for generating the experiment results are available at ii.
i nter trans inter trans translates programs from a source to a target language using an llm and a series of transitive intermediate translations.
the input of i nter trans includes a llm a program pswritten in a source language ls the target language lt a non empty intermediate pl set l which contains lsbut excludes lt a hyper parameter maxdepth which determines the maximum number of transitive intermediate translations.
i nter trans utilizes a readily available test suite to evaluate the accuracy of the generated program s tp written in the target language i.e.
tp pt pt pps lt s t where pps ltis the set of programs written in ltthat represent translation candidates for ps.
given a translation problem aimed at converting a source program psinto a target language lt inter trans operates in two stages.
in stage it constructs all possible translation pl paths using a novel approach called the tree of code translations toct which identifies potential sequences of transitive translations from lstoltvia intermediate languages from the set l. stage then uses the source program ps and the pl paths generated from stage to perform inferences with an llm to generate a set of target programs tpwritten inlt.
these programs each corresponding to a translation path are generated and verified sequentially against a test suite.
the algorithm terminates when a successful translation is identified indicated by a ptthat passes the test suite.
the following subsections provide detailed descriptions of each stage accompanied by a running example.fig.
running example of i nter trans with maxdepth for translating python to java showing a successful translation through c after exploring various translation paths.
red nodes represent unsuccessful translations blue nodes indicate explored translations green nodes denote successful translations and grey nodes are skipped translations.
the number along with each edge is the execution order of the translations.
a. stage generating tree of code translations toct algorithm specifies how toct creates plans translation pl paths for a given translation pl pair utilizing a set of intermediate languages.
since toct operates at the level of translation pl pairs this planning algorithm only needs to run once for all translation problems involving the same source and target languages.
in toct the intermediate language set lincludes the source language lsbut excludes the target language lt. this is because ltshould be the final target and should not occur as an intermediate step in the translation process while we should allow lsto appear in intermediate translations for cases where a source program can be simplified by translating to and from another pl .
below we use a running example shown in figure to illustrate this algorithm.
in this example we aim to translate a python program to java lsis python ltis java and we consider a maximum depth maxdepth of meaning that at most three edges can be included in a translation path.
the set of intermediate languages l includes five programming languages python rust javascript c and go.
toct see algorithm starts by enqueueing and then dequeueing the source pl yielding the current path starting from the source pl i.e.
and the current depth in our running example.
since python is not the target language and the current depth is less than the maximum depth of the algorithm continuously explores possible transitions either to an intermediate language excluding python since a pl cannot be translated to itself or directly to the target language to complete the translation path.
this results in the following paths and .
each of these new paths along with the incremented depth of is enqueued into q. continuing this process the algorithm dequeues python java i.e.
the direct translation path and since it ends with the target pl this path will be added to the final translation pl path output list.
next the algorithm dequeues and explores further transitions appending each language from the set lto the current path but excluding rust to avoid translation between the same pls.
this results in new paths like etc.
which are then enqueued with a depth of .
this process repeats for all potential paths within the specified maximum depth ensuring all possible translation paths from python to java are explored and recorded.
by the end of the algorithm the list paths will contain all feasible sequences of translations from python to java considering all given intermediate languages and the maximum depth argument.
algorithm toct path generation algorithm input ls source programming language lt target programming language maxdepth maximum depth of the tree l li a set of intermediate languages.
output all paths from lstolt initialize an empty list paths initialize a queue q enqueue intoq while qis not empty do currentpath currentdepth dequeue q currentlang last element of currentpath ifcurrentlang targetlang then append currentpath topaths else if currentdepth maxdepth then forlang lt ldo iflang currentlang then newpath currentpath enqueue newpath currentdepth intoq return paths b. stage sequential verification of toct for a specific translation problem source program the second stage of the i nter trans approach see algorithm takes the toct generated plan for the problem s source and target pl i.e.
the list paths from algorithm to determine the order of the paths that will be verified i.e.
checked if they lead to a successful translation generate the translations using an llm and a prompt template promptt and evaluate the translations to the target language using the given test suite t. to make i nter trans more efficient anearly stopping mechanism is applied lines as soon as one path successfully translates the code into lt algorithm terminates.
algorithm algorithm for executing toct generated plans input ps an input source program paths a list of translation pl paths generated by toct llm a llm that can generate code into lt l promptt a prompt template for the specific llm t a test suite for evaluating the computational accuracy of the generated translation to target pl lt. output successful translation if any from lstoltforps sortpaths by their length in ascending order forpathp paths do foredgeek pdo ifekis already processed then continue with cached output else retrieve extracted source code from ek create a new prompt using promptt perform translation using llm and the prompt extract source code from inference output iffailed extracting source code then break continue with the next path p save the extracted code for ekto cache iftarget language of ek ltthen verify this translation using the test suite t iftest suite passes then return the translation found return the translation failed following the design of toct it is common for multiple paths to share the same initial transitive translation edges.
for instance path p1 and path p2 python rust javascript java share the first translation edge .
to further improve the efficiency of i nter trans we apply memoization within each path to ensure the same edge is not computed more than once lines .
note that this optimization requires deterministic output for the same input prompt which is ensured via a fixed seed in our experiment.
only new translation edges after branching from a shared path are processed.
in other words if p1is verified first then p2 will reuse the resulting rust program saved in the memory cache to continue its unique translation to javascript.
in algorithm the input paths are first sorted by length in descending order line ensuring that the first explored path is always a direct translation from lstolt.
in the best case scenario where the llm generates a program in the target language that successfully passes the evaluation test suite the algorithm completes after exploring only this direct path.
if no direct translation is found the sorting step following path generation ensures that the algorithm maximizes the number of paths explored relative to the total translations performed.
for instance for our running example in figure the numbers along the edges indicate the sequence of steps performedfollowing algorithm for a specific ps.
the direct translation i.e.
will be verified first.
if the transferred code generated following this path fails then the path python rust java will be verified and so on until the transferred code generated by path passes the test suite t the algorithm stops and returns the successful translation.
for each edge in a translation path we first generate translated code for the target language of the previous edge line which serves as the source program of the current edge .
next we use the given llm to generate the translation output lines then extract the source code from this output line .
if the extraction is successful we then verify if it can pass the test suite t. iii.
e xperiment design we evaluate the effectiveness of i nter trans by answering the following three research questions rq1 how effective is i nter trans compared to direct translation and other baselines?
rq2 how could varying the maxdepth affect the performance of i nter trans ?
rq3 how could varying the selection of intermediate languages affect i nter trans ?
a. benchmark dataset collection and pre processing our experiment dataset consists of translation problems across source target translation pl pairs involving six pls c go java javascript python and rust.
when creating our experiment dataset we considered three existing datasets.
below we describe the creation of our experimental datasets from these sources.
transcoder the original transcoder dataset was created by manually collecting coding problems and solutions written in c java and python from geeksforgeeks .
recently yang et al.
discovered quality issues in this dataset and subsequently conducted a manual verification and curation of the dataset to ensure its correctness.
in this study we reused their cleaned version containing a total of translation problems and corresponding test suites.
we employed the full version of this dataset for comparisons with sota learning based approaches.
humaneval x humaneval x extends the python only code generation evaluation dataset humaneval with additional canonical solutions and test cases in six pls c go java javascript python and rust.
we created translation pairs for all tasks in humaneval x across the six languages resulting in translation problems.
due to computational constraints particularly required by the ablation studies performed to understand the impact of varying variables on the performance of i nter trans we randomly sampled translation problems stratified across the source target translation pairs ensuring a .
confidence level.
codenet codenet contains programs written in programming languages for learning and evaluating coding tasks and was adopted in a recent empirical study by pan et al.
on llm introduced translation bugs.
programmingtasks in codenet are verified by matching the program outputs with the expected results.
for our study we selected tasks with at least three test cases to ensure adequate test suite coverage resulting in programming tasks.
from these tasks we generated translation problems by concentrating on the six pls featured in humaneval x removing problems with a file size exceeding 1kb as a proxy for token length to prevent inputting into the prompt problems longer than the model s token limit and ensuring that each translated code snippet could be assessed using three test cases.
we created a subset of pairs from this dataset using stratified random sampling ensuring a .
confidence level.
b. selected large language models intertrans relies on an llm that understands multiple pls.
almost all recent code llms possess this multilingual capability.
we have chosen the following three instruct tuned llms over their base models as instruct tuned models are fine tuned to follow prompted instructions more effectively.
magicoder an open source collection of llms trained on 75k synthetic instruction response pairs and includes multiple model variants with different base models.
all magicoder models have around 7b parameters.
we use the magicoder sds variant .
starcoder2 an open source collection of llms offered by the bigcode project .
starcoder2 has instruction tuned versions ranging from 1b to 34b parameters.
we use the starcoder2 15b variant .
codellama an open source collection of llms offered by meta based on llama specialized in code generation with 7b 13b and 34b parameters.
we use the codellama13b variant .
we chose these models because of their proven effectiveness in code generation tasks and their open source nature which promotes accessibility and collaborative development.
additionally we prioritized models compatible with efficient inference frameworks i.e.
vllm while also ensuring they work well with platforms such as the huggingface text generation interface .
this ensures that our selected models are not only high performing but also practically feasible for widespread use in both research and industry settings.
c. compared approaches direct translation ca and ca we compare inter trans with direct translation by evaluating performance with a single attempt ca and multiple attempts ca .
for ca a single prompt is used to generate ten translation candidates.
the translation is considered successful if any of these ten attempts result in a correct translation.
comparing with ca reveals the additional opportunities inter trans discovers via toct.
since i nter trans utilizes multiple translation paths it inherently makes more than one attempt making a comparison with ca alone insufficient.
hence to find a fair number of attempts k for direct translation we analyzed how many attempts i nter trans required to achieve a successful translation across the experiments.on average .
attempts were needed with of cases successful within two attempts and less than .
requiring between and attempts.
therefore we chose ca as a stronger baseline allowing ten attempts with a high temperature setting to generate diverse variants and increase the chances of passing the test suite.
the distribution of the number of attempts made by i nter trans in our experiments is presented in the supplementary material.
non llm sota approaches transcoder is an unsupervised model pre trained with cross lingual language modeling denoising auto encoding and back translation leveraging a vast amount of monolingual samples.
transcoder ir an incremental improvement introduces the idea of using a lowlevel compiler intermediate representation ir to enhance translation performance.
in addition to transcoder s pretraining tasks transcoder ir includes translation language modeling translation auto encoding and ir generation.
transcoderst is another enhanced version of transcoder that uses automatically generated test cases to filter invalid translations improving performance.
these models are trained on only a few pls i.e.
python c and java.
gpt .
and its enhanced version gpt .
is a powerful closed llm provided by openai that is capable of code generation.
we consider the gpt .
turbo version.
unitrans with gpt .
is an enhanced version designed for code translation proposed by yang et al.
.
unitrans generates test cases to aid llms in repairing errors by integrating test execution error messages into prompts.
despite unitrans with gpt .
requiring additional program repair and extra test cases we include it as a baseline since it represents the stateof the art performance on the transcoder dataset.
d. evaluation metric similar to recent studies on llm based code translation we adopt execution based evaluation metrics i.e.
computational accuracy ca .
ca assesses whether a transformed target program produces the same outputs as the source function when given identical inputs.
ca on a benchmark is the ratio of translation problems that have correctly translated to the target language.
we choose ca over text based metrics like bleu score because llms can produce valid translations that differ from humanwritten