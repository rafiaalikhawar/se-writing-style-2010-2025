towards greener yet powerful code generation via quantization an empirical study xiaokai wei xiaokaiw amazon.com aws ai labs usasujan kumar gonugondla gsujan amazon.com aws ai labs usashiqi wang wshiqi amazon.com aws ai labs usa wasi ahmad wuahmad amazon.com aws ai labs usabaishakhi ray rabaisha amazon.com aws ai labs usahaifeng qian qianhf amazon.com aws ai labs usa xiaopeng li xiaopel amazon.com aws ai labs usavarun kumar kuvrun amazon.com aws ai labs usazijian wang zijwan amazon.com aws ai labs usa yuchen tian tiayuche amazon.com aws ai labs usaqing sun qinsun amazon.com aws ai labs usaben athiwaratkun benathi amazon.com aws ai labs usa mingyue shang myshang amazon.com aws ai labs usamurali krishna ramanathan mkraman amazon.com aws ai labs usaparminder bhatia parmib amazon.com aws ai labs usa bing xiang bxiang amazon.com aws ai labs usa abstract ml powered code generation aims to assist developers to write code in a more productive manner by intelligently generating code blocks based on natural language prompts.
recently large pretrained deep learning models have pushed the boundary of code generation and achieved impressive performance.
however the huge number of model parameters poses a significant challenge to their adoption in a typical software development environment where a developer might use a standard laptop or mid size server to develop code.
such large models cost significant resources in terms of memory latency dollars as well as carbon footprint.
model compression is a promising approach to address these challenges.
we have identified quantization as one of the most permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
compression techniques for code generation as it avoids expensive retraining costs.
as quantization represents model parameters with lower bit integer e.g.
int8 the model size and runtime latency would both benefit.
we empirically evaluate quantized models on code generation tasks across different dimensions i resource usage and carbon footprint ii accuracy and iii robustness.
through systematic experiments we find a code aware quantization recipe that could run even a billion parameter model in a regular laptop without significant accuracy or robustness degradation.
we find that the recipe is readily applicable to code summarization task as well.
ccs concepts software and its engineering software creation and management automatic programming integrated and visual development environments .
keywords quantization code generation large language models generative ai model hosting esec fse december san francisco ca usa wei and gonugondla et al.
acm reference format xiaokai wei sujan kumar gonugondla shiqi wang wasi ahmad baishakhi ray haifeng qian xiaopeng li varun kumar zijian wang yuchen tian qing sun ben athiwaratkun mingyue shang murali krishna ramanathan parminder bhatia and bing xiang.
.
towards greener yet powerful code generation via quantization an empirical study.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
https introduction in recent years ml powered code generation tools like codex github copilot amazon codewhisperer1and chatgpt2have gained significant traction.
these services aim to generate a computer program in response to a human written specification commonly called prompt as shown in figure .
such tools bring promise to significantly automate the software development process and improve developers productivity.
the backbone of ml powered code generation tools are transformer based large pretrained language model plm .
with recent rapid developments plms have exhibited superior performance in multiple code related tasks including code generation code summarization and type inference .
despite their great success there are multiple challenges and downsides associated with applying these gigantic code generation models with often tens of billions of parameters in a typical software development environment.
importance of model compression.
the need of compressed models in a typical development scenarios include hosting.
the huge number of model parameters poses a significant challenge.
for example let us consider one of the largest publicly available models codegen with up to 16b parameters.
hosting this model requires gb of memory which is impossible on a typical laptop with gb or gb ram.
an alternative is to host this model on a server equipped with gpus with sufficient memory and with model parallelism.
using ec2 pricing as reference using such models is expensive and costs around per1k queries.
furthermore sizes of plms continue to grow and further limit deployment of future and more powerful models in real life development environment.
note that although services like copilot chatgpt etc.
provide apis for code generation hosting customized model is still important for custom tasks3.
latency and user experience.
state of the art code generation models typically consist of 50transformer layers and 2b 16bparameters.
model inference serving on a single gpu machine might incur a latency of several seconds.
such a delay in response would cause a negative user experience especially for interactive code development.
carbon footprint.
recently researchers start to pay more attention to examining plms from the perspective of responsible and green ai.
the training and inference of large plms typically involve a considerable amount of co2emission.
for the co2emission of training gpt 3model 175b parameters amounts to three times that of a jet plane flight from san francisco to new york .
with increasing adoption of plms e.g.
by hundreds of thousands of software developers in the near future the carbon footprint of inference will become a bigger issue.
to address these challenges machine learning researchers have investigated different model compression techniques .
a key challenge however is to preserve the powerfulness of the gigantic models while significantly reducing the computational cost by compressing them.
addressing this challenge would be crucial to democratizing the power of ai.
in this paper we empirically investigate the impact of model compression techniques on code generation tasks.
selecting compression technique.
we focus on light weight compression techniques that do not incur additional re training cost.
also the compressed model should be run efficiently in a developer s laptop or a moderate sized server.
in such a scenario we identify the following desirable properties that a practical model compression strategy needs to satisfy minimal compression cost converting a pretrained model to a more efficient version typically involves certain processing training costs.
if the compression technique requires significant re training of the large model over substantial amounts of data it could result in undesirable environmental impacts large power consumption and carbon footprint and the cost could be prohibitive for a typical user.
high compression costs would defeat the purpose of greener ai and democratizing ai.
substantial reduction in hosting cost as state of the art models are gigantic with billions of parameters and are expected to continue growing in sizes minor reductions in size or latency would not be useful.
ideally one would expect a properly designed model compression method to bring at least improvement in key hosting metrics of size and latency.
preservation of generation capabilities the compressed model should have similar generation accuracy as the original model.
model compression at the cost of significantly degenerated predictions would not be appealing.
minimal adverse side effect in addition to preserving generation accuracy we also expect the model to not degenerate in other important aspects of generation such as weakened robustness.
model compression techniques developed by ml community such as distillation pruning and quantization aware training are often associated with large training costs.
training or finetuning large transformer models requires access to training data and substantial compute resources.
often fine tuning data for many freely available models is proprietary which prevents a compression technique which relies on fine tuning to reproduce the performance of the original models.
considering all the above factors out of many model compression options we are able to identify a compression recipe with negligible processing cost and preserved accuracy with a specific subcategory of quantization methods i.e.
post training quantization ptq .
quantization is a compression technique where the weights and activations of an ml model are converted to and computed with 225towards greener yet powerful code generation via quantization an empirical study esec fse december san francisco ca usa input prompt codetestcases figure sample prompt code and test cases taken from mbpp dataset .
given the nl prompt a code generation model aims to generate the corresponding code.
the associated test cases run the generated code to check functional correctness.
integer data types such as int8 instead of commonly used floatpoint data types such as fp32 .
as data is represented with lower bits e.g.
8or4 the model would be much smaller in size.
also most hardware types either cpu or gpu perform integer operations e.g.
multiplication at a much faster speed the quantized model would also likely to enjoy reduced computational cost and latency.
code aware quantization.
in order to quantize the model for code domain we observe the activation ranges and calibrate the quantization step sizes that minimize the mean square error on the code data.
properly designed ptq methods would require none or a relatively small amount of code data for post training calibration and experimental results show that this approach is highly effective on multiple code specific tasks.
this means one can get all the compression benefits e.g.
latency memory storage carbon emission with negligible cost while retaining the generation power of the full precision model.
our contribution can be summarized as follows we recognize the importance of model compression in the context of code generation and identify the adequacy of posttraining quantization for this purpose.
to our best knowledge this is the first attempt at compressing a state of the art code generation model.
for example the quantized model with 16b parameters could run on a personal laptop with only cpus and generate a token long prediction within 25seconds as opposed to 70seconds by the corresponding full precision model .
we perform an extensive empirical study on multiple code generation models with their quantized variations on both nlto code and code to nl tasks.
we observe comparable accuracy across multiple model types and sizes with the proposed quantization techniques.
even for the extremely large codegen 16b we can preserve accuracy with quantization.
we also experiment in different ablation settings to provide guidelines for properly employing quantization.
we present an in depth empirical analysis on the layers activations and weights of the state of the art code generation models to gain deeper insights on the effect of quantization.
this helps us understand why certain quantization methods perform better than others for code generation task.
beyond accuracy we also investigate the impact of quantization on model robustness which is often overlooked by the existing code generation literature.
we show that properly designed quantization recipe would have no adverse impact on model robustness.
relevance to software engineering.
we identify the importance and requirements of model compression techniques for a typical development environment.
we then adapt quantization a lightweight compression technique for code domain.
we empirically multi head attentionadd normfeed forwardadd normn x token embeddinglinearsoftmax positional embeddingscaled dot product attentionlinearlinearlinearkqvlinearconcat multi head attentionfigure transformer structure and multi head attention cell.
the feed forward layer and all linear layers inside multi head attention are colored in green.
we quantize all these linear layers in the network.
establish that such approach can be significantly greener without loosing power of code generation in both cpu and gpu environments.
to this end this is the first systematic study to show that a specially crafted code aware quantization technique can democratize gigantic pre trained code generation models such that even a regular software development environment with 16gb laptop can utilize their power.
background related work .
code generation with transformers recently applying transformer based pretrained language models plms to the source code generation task have drawn considerable attention and set overwhelmingly strong state of the arts in this field .
the goal is to generate complete code or code fragments given natural language or partial code as prompts.
to achieve this goal large language models are trained on humongous code corpora typically curated from open source code archives like github stack overflow etc.
the plms typically use decoder only e.g.
gpt or encoderdecoder architectures e.g.
bart t5 .
for code generation tasks decoder only models e.g.
codegen and incoder take some pre encoded code representations and learn to decode i.e.
synthesize next token sequences.
typically these models use causal language modeling i.e generate the tokens conditioned on the previous token sequences.
thus decoder only models are a natural fit for code completion tasks where the previous code 226esec fse december san francisco ca usa wei and gonugondla et al.
context is given and the model is expected to generate the next tokens.
in contrast encoder decoder based code generation models like plbart and codet5 are typically trained to learn to reconstruct the original code sequence that is corrupted using an arbitrary noise function.
therefore such models do not naturally fit the code completion tasks but are found effective when finetuned for code generation or summarization tasks.
.
model compression the large transformer models use billions of parameters and may require trillions of operations for generating code.
model compression tackles this high costs of large models to enable their wider and easier adoption.
model compression is a class of techniques designed to reduce model size i.e.
bytes required to represent the model and improve generation latency while maintaining minimum accuracy i.e.
ability to generate useful and correct code degradation.
some representative techniques include knowledge distillation.
a small student model is trained on the outputs of a larger teacher model that we want to compress .
pruning.
it constitutes a class of techniques that make the weight matrices sparse to reduce the number of parameters as many of the matrix entries will now be zeros .
quantization.
this technique uses fewer bits to represent the weights of parameterized functions .
most model compression techniques are often associated with large training costs.
for example distillation requires us to train a new model from scratch and pruning requires multiple cycles of finetuning.
training or finetuning large transformer models requires access to the training data and large compute resources.
this is often not an option for many users who typically use fundamental models that are pretrained on large training corpus by others.
therefore we study post training quantization ptq for model compression as this does not require us to train or finetune a pretrained model.
we particularly limit to bit and bit integer quantization as the hardware accelerators such as gpus tpus support and compute faster with this precision that the floating point alternatives such as bit float.
.
quantization for model compression here we describe the process of quantizing a tensor and discuss different model quantization techniques.
.
.
quantization operation.
quantization refers to the conversion of a full precision or floating point tensors to tensors with integer values.
an example of the quantization operations is depicted in figure .
given a matrix w a basic quantizer q uses scale and rounding operations to get the quantized version of the matrix q w wq sw wheresw 2b wandwq round sww here wis the quantization range and bis the bitwidth which is in case of int8 wqis the quantized integer matrix swis the activation b per tensor bit quantization a typical linear operations c per column bit quantization2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.8weight matrix w quantize weights w!
into quantize weights w!
into per tensor max abs value per column max abs value per tensor scale s per column scales s s 127 s 127 w!
rd ws w!
rdw diags figure toy example for quantizing the typical floatingpoint weight matrix a into int8 matrix using b per tensor v.s.
c per column quantization.
quantization scale and q w is the quantized approximation of the matrixw quantization noise.
we assess the quality of quantization by estimating the relative quantization noise qa defined as qa a q a a 2 a a 12s2 a a where x 2is the thel2 norm of the vector x and w sw quantization step size.
the quantization noise increases with w or decreases with sw as the approximation of the full precision parameters becomes coarser.
parameter distributionquantization step 127127mapping to integers0clipped outliers figure illustration of quantization operation showing quantization step clipping scaling and mapping.
quantization range and scale factor.
the quantization range wis the value that will be mapped to the largest representable integer in the case of int8 .
typically we set w max abs w consequently setting the scale factor sw 2b max abs w .
however having a large outlier in wwill increase wand therefore increase the quantization noise.
to avoid that some choose to clip the data by choosing w max abs w see figure where the matrix elements are clipped the sbefore the quantization operation i.e.
matrix elements ware set to wand those w are set to w. .
.
quantization techniques.
model quantization techniques can be classified based on the following methods to obtain quantized network.
this can be broadly classified into 227towards greener yet powerful code generation via quantization an empirical study esec fse december san francisco ca usa input prompt activation at layer nmax abs valuescalequantize activation into int8dynamic quantization .
.
.
.
.
10361276920activation at layer nmax abs valuescalequantize activation into int8dynamic quantization1.
.
.
.
.
.
input prompt a dynamic quantization where the clipping range is determined dynamically as the max abs value in activation tensors.
activation at layer nmax abs valuescalequantize activation into int8staticquantization1.
.
.
.
.
.
.
input prompt static bound determined by calibrationcalibration on small amount of data e.g.
5k samples minimizemseorentropyloss4.
b static quantization where the fixed clipping range is learned through calibration.
figure toy example on quantizing activations with dynamic quantization v.s.
static quantization.
post training quantization ptq ptq derives a quantized e.g.
int8 network from an existing full precision network without any training or finetuning with additional data.
since the model is not originally trained to perform inference with quantized parameters and activation models quantized by ptq tend to be more susceptible to quantization noise.
however the low costs associated with ptq make it a very popular choice for obtaining quantized models.
quantization aware training qat qat requires training the model from scratch with additional simulated quantization operations during training process to ensure the learned parameter values are quantization friendly.
this is expensive due to the potentially huge cost of training but would lead to models that potentially have higher accuracy than a ptq model.
methods to choose the activation scale.
here we choose the ranges and the values of the activations change for each example.
there are various options to choose the quantization scale parameters for activations that can be classified into see figure dynamic quantization here we determine the clip range and scale parameter s on the fly for activations in order to minimize quantization noise where possible.
one could typically use the maximum absolute value of the activation tensors as the clip range for each input.
however determining the clip range dynamically would incur an additional scanning cost to find the max value.
static quantization here we use the same pre deter mined scale through so called calibration on samples by minimizing certain loss e.g.
mse entropy between original activation and quantized activations.
static quantization might be susceptible to higher quantization noise though it would lower computational cost during inference.
quantization granularity.
as we discussed in the section .
.
choosing a large clip range accordingly small scales sa due to outliers can lead to a large quantization step which adds to quantization noise.
to avoid outliers column wise quantization scales can be used where the scales are selected based on the max value of each column instead of the entire matrix.
we can classify quantization techniques based on the granularity of the quantization scales into per tensor scales where the entire tensor uses a single scale sa and per column where each column uses a different scale.
figure illustrates the differences between the two scaling options.
here we are choosing scale values based on the maximum absolute value of the block.
choosing per column scales avoids tensor wide outliers and allows for finer quantization steps than per tensor scales.
in the rest of the paper we will primarily use ptq as this has minimal post re training cost.
we examine the accuracy of the models with dynamic and static quantization and discuss the impacts of choosing per tensor and per row scales.
we will use int8 precision for quantization as it is widely supported across all major cpus and gpus that are used today.
methodology the goal of this work is to provide an empirical and conceptual analysis of quantization techniques originally developed as a core ml technique in the context of large code generation models.
to do that we analyze the characteristics of the models using different dimensions of quantization techniques as discussed in section .
.
.
this section discusses our study methodology in detail.
.
quantized model preparation for quantization techniques we investigate both schemes of quantization dynamic and static described in previous sections and prepare the quantized models as follows.
dynamic quantization for implementation we use the native pytorch quantization4api and convert all the weight matrices in feed forward network ffn and self attention to int8 .
as explained in the previous section the min max bound of each layer s activation is determined in a dynamic manner depending on the input during inference.
the processing time needed for this scheme is minimal which typically takes 1minutes for 2b models and 4minutes for 6bmodels.
static quantization static quantization needs to determine the clipping range for activations before inference and such ranges are typically obtained from calibration by minimizing the quantization noise.
we perform the activation bound calibration with a tiny fraction 5k samples from the codesearchnet python training set.
in preliminary experiments we find mse mean squared error based loss to be most effective so we minimize 228esec fse december san francisco ca usa wei and gonugondla et al.
table details of models under investigation.
models parameterstraining costarchitecture steps epochs data approx.
compute resources plbart 140m 406m 100k gib nvidia geforce rtx ti encoder decoder code t5 60m 220m 770m gib nvidia a100 gpus encoder decoder incoder .3b .7b gib nvidia v100 gpus decoder only codegen 350m 2b 6b 16b 650k gib google s tpu v4 decoder only the mse between the quantized activations and full precision ones as the calibration.
.
study subjects studied models.
we leverage the state of the art and representative code generation models that have open sourced model checkpoints available to study the efficacy of different calibration techniques.
we aim to cover models with different sizes and backbone architectures.
in particular we focus on codegen as they open sourced models with different sizes 350m 1b 6b 16b and different language support mono v.s.
multi language generation .
additionally we also include incoder to further confirm the patterns we observe with codegen models.
we also studied two more models code t5 and plbart for code summarization task.
the statistics of these models are summarized in table .
these models represent the set of all publicly available state of the art models code generation models at the time of writing this paper.
studied tasks.
in this paper our main focus is code generation task nl to code .
further to stress test the effectiveness of quantization on other generative tasks we study code summarization task for models accuracy evaluation rq4 .
thus we study the following two tasks nl to code generation here we evaluate the models code generation ability.
a user gives a natural language prompt as input.
these are loosely defined specifications.
the model is expected to generate the corresponding code fragments.
the generated code is tested by running the test cases.
figure shows an example.
code to nl generation we further evaluate a generative model s capability on code summarization task where given the function signature and body the model generates an nl description of the function.
studied dataset we use humaneval and mbpp for evaluating the functional correctness of generated programs.
the mbpp dataset contains 974short python functions with their textual descriptions and test cases to evaluate correctness see figure .
humaneval is a similar dataset released by openai which is widely used in evaluating code generation tasks.
it contains hand written python programs associated with their natural language descriptions and test cases.
evaluation metrics generative models in nlp domain traditionally use some form of textual matching exact or fuzzy match between the generated text and ground truth and often report bleu scores.
such textual similarity is problematic for evaluating code generation tasks as the same functionality can be implemented inmany ways.
to overcome this recent papers on code generation task recommend to evaluate functional correctness by running the generated code against test cases.
here we follow a similar evaluation criterion.
each sample in our studied dataset is equipped with multiple test cases as shown in figure .
the generated code needs to pass allprovided tests to be considered as pass .
following we report pass k to estimate the model s ability to generate code that will pass .
pass kmeasures the fraction of examples that are pass by at least one of the ksolutions that the model generates.
however given the ml model is probabilistic we expect pass k to have a high variance.
to address this a standard practice is to generate n ksolutions and estimate the statistical mean of pass kfrom thesensamples i.e.
estimate the fraction of times we pass if we randomly pick ksamples from n. in this paper we use pass 1and pass 5as a metric for evaluations which is estimated by generating 10samples per problem in the dataset.
the reported accuracy pass k is averaged on all samples generated for all programs in each dataset.
to evaluate the code summarization models we use smoothed bleu score following prior works .
results we evaluate the effect of quantization across three dimensions greener accuracy and robustness for code generation tasks.
to evaluate generalizability we further evaluate quantization techniques for code summarization tasks as code summarization is a popular code related generative task where a different modality i.e.
text is generated.
in particular we aim to answer the following four research questions rq1.
how effective are quantization techniques for greener code generation models?
rq2.
can quantized models maintain the prediction power of the corresponding full precision models?
rq3.
how robust are quantized models compared to the corresponding full precision models?
rq4.
are quantization techniques effective for other code related generative tasks such as code summarization?
.
quantization for greener code generation rq1 motivation.
the heart of this paper lies on this rq i.e.
whether a quantized model can be substantially greener than its full precision counterpart.
here by green we mean less resource usage and less carbon footprint.
our use case is to facilitate a regular development environment that can benefit from such large models.
thus a full 229towards greener yet powerful code generation via quantization an empirical study esec fse december san francisco ca usa precision model can be pretrained with larger resource even at industry scale .
however a developer will be using the model in an environment which is either cpu only or contain a smaller number of gpus.
to this end this rq evaluates the model s resource usage and carbon footprint at inference time.
experimental setup.
we aim to answer rq 1by investigating quantization from a model hosting perspective with gpu or cpu as the underlying hardware.
we consider both on cloud and on device settings as both can be important use cases for code generation models.
the environment used for experiment is the following on cloud we use an aws p3dn.24xlarge instance5which have both cpus and gpus available with nvme based ssd storage.
on device we use a typical developer s laptop a macbook pro which runs macos monterey version .
with 32gb memory and m 1processor.
metrics.
we report inference latency and model storage size as primary metrics for model hosting.
based on the latency result and the specification of underlying hardware we also estimate assuming sequential prediction the potential cost6 in us and carbon emission7 ingco 2eq for evaluating the impact in terms of green ai.
table comparison on different hosting metrics between full precision and quantized int8 dynamic versions of codegen 2b codegen 6b and incoder 6b.
codegen 2b codegen 6b incoder 6b on cloud precision fp32 int8 fp32 int8 fp32 int8 storage gb .
.
.
.
.
.
latency s pred.
.
.
.
.
.
.
est.gco2eq 1k pred.
est.
pricing 1k pred.
.
.
.
.
.
.
on device latency s pred.
.
.
.
.
.
observations.
cpu based results.
in table we report on cloud and on device hosting metrics of codegen 2b 6b and incoder 6b model for generating 20tokens for each example.
as the quantization kernel in pytorch only supports cpu inference we collect all the metrics on cpus.
for both codegen 6b and incoder 6b we observe that int8 quantization reduces the model size to about offp32 counterpart and also reduces latency significantly e.g.
by about on ec2 instance and on laptop .
as the carbon emission and pricing are roughly linear w.r.t.
the runtime using a quantized model would also contribute significantly to green ai and reduced hosting cost.
with the much less stringent requirements on the underlying hardware quantization makes it possible to run large e.g.
6b code generation models on a personal laptop within a reasonable latency constraint.
such capability can be helpful for developers to get high quality code recommendation auto completion in their local environment.
5more details on the hardware specification can be found at 6based on estimate in latency and memory usage of codegen models on a nvidia a100 gpu with context lengths of tokens and generation lengths of tokens.
model size 2b 6b 16b context encoding latency ms fp32 .
.
.
.
n a fp16 .
.
.
.
.
.
int8 .
.
.
.
.
.
per token generation latency ms fp32 .
.
.
.
n a fp16 .
.
.
.
.
.
int8 .
.
.
.
.
.
peak memory usage gb fp32 .
.
oom fp16 .
.
.
int8 .
.
.
gpu based results.
to assess the impact of using int8 inference on gpus we developed an inference pipeline using int8 cuda kernels from nvidia s cutlass library.
our end to end latency measurements with a token context and token generations are reported in table for codegen models.
by comparing thefp16 andint8 implementations we observed a reduction of up to in both context encoding latency and generation latency along with a decrease in gpu memory usage.
the gains in comparison to the fp32 implementation are approximately to .
times reduction in latency and .
times more memory efficient.
this effectively results in a doubling of the inference speed and an halving of the number of required gpus for deployment.
result the quantized models have lower latency memory and storage than the corresponding full precision model.
it also has remarkably less carbon footprint.
thus it is possible to fit even a 6b parameter model within a regular laptop.
.
accuracy evaluation for code generation task rq2 motivation.
although greener a quantized model will be mostly useful if it maintains the accuracy of the original full precision model.
in this rq we evaluate the functional correctness of code generation models for full precision and their different quantized variants.
experimental setup.
we evaluate the code generation tasks using codegen and incoder quantized models with static and dynamic activation quantization.
we tested the models with per column scales and per tensor scales while quantizing the weights as well.
we report both pass and pass accuracies.
observations.
table summarizes the results.
we see accuracy gain for incoder .7b models across all the quantization settings while incoder 1b shows an average accuracy drop of .
on humaneval and .
on mbpp datasets.
codegen models show average degradation with pass 1metric on humaneval and 230esec fse december san francisco ca usa wei and gonugondla et al.
table pass k accuracy on humaneval and mbpp.
performance gains are in blue and drops in red.
full precisiondynamic quant.
static quant.
dataset model per tensor per tensor per column pass pass pass pass pass pass pass pass humaneval python incoder .3b .
.
.
.
.
.
.
.
.
.
.
.
.
.
incoder .7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
codegen 350m .
.
.
.
.
.
.
.
.
.
.
.
.
.
codegen 2b .
.
.
.
.
.
.
.
.
.
.
.
.
.
codegen 6b .
.
.
.
.
.
.
.
.
.
.
.
.
.
mbpp python incoder .3b .
.
.
.
.
.
.
.
.
.
.
.
.
.
incoder .7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
codegen 350m .
.
.
.
.
.
.
.
.
.
.
.
.
.
codegen 2b .
.
.
.
.
.
.
.
.
.
.
.
.
.
codegen 6b .
.
.
.
.
.
.
.
.
.
.
.
.
.
relativequantizationerror 350m2b6b16bmodelsize0123456per tensorper column a weight quantization noise for codegen models.
maxabsolutevalue 0layernumber24681012141618 b distributions of maximum activation value across different layers in codegen 350m model on validation data.
2468101214relativeerror 0layernumber51015202530per tensorper column c mse layer between activations from original vs. quantized model.
the error is estimated on a weight only quantized codegen 6b model generating a token on humaneval.
figure statistics impacting quantized model accuracy.mbpp datasets with both dynamic and static quantization.
however we observe and average drop in accuracy in the pass 5metrics with dynamic quantization and static per tensor quantization respectively.
with static per column quantization the average pass accuracy drop is for codegen models.
overall dynamic per tensor quantization tends to outperform static per tensor quantization by a small margin and static percolumn quantization outperforms static per tensor quantization.
this is because weight quantization.
weight distributions have a high variance within a kernel accounting for outliers that result in large quantization noise.
this is particularly an issue with increasing matrix sizes in larger models.
figure 6a shows how the quantization noise increases with model sizes with per tensor scales but not with per column scales.
this reduced quantization noise with per column scales explains why static per column setting outperforms static per tensor one.
activation quantization.
the primary challenge in activation quantization is in choosing the quantization scales.
with staticquantization we have to pick pre determined scales based on validation data.
this pre determined scale is picked conservatively on the other hand dynamic quantization allows us to adjust the scales for every input example and for every token thereby making it attractive to reduce quantization noise.
dynamic quantization will be useful if we observe high variance in the max values across different inputs tokens.
for example figure 6b shows the max value of the activation across different layers in codegen 350m.
error accumulation.
quantization noise accumulates with depth making deeper models more challenging to quantization.
figure 6c shows the relative quantization noise with model depth for codegen 6b model showing quantization error growing with depth.
we observe that a per column quantization results in smaller accumulated error with depth and b the error tends to reduce in the last few 4 layers of the model.
the latter could be due to the inherent robustness of the model.
231towards greener yet powerful code generation via quantization an empirical study esec fse december san francisco ca usa .25pass 1onhumaneval0.
.
.
.10codegen 2bcodegen 350m5001k2k5k figure execution accuracy on humaneval with codegen2b and codegen 350m per column static when they are calibrated mse loss on different amounts of data from to5k .
dotted lines denote the pass 1of corresponding fullprecision models.
.
.
ablation study.
to better understand the impact of different design choices on the model as discussed in section .
we further investigated pass 1scores for different model variations on humaneval.
size of calibration set.
here we study how the size of calibration data affects the performance of quantized models.
figure shows that the execution accuracy on both 2b and 350m models is typically stable across different sizes of calibration data.
when using only 500samples for calibration the quantized model can already learn a reasonable clipping range and achieve comparable accuracy as full precision baselines.
such calibration cost e.g.
takes a few minutes on a single cpu gpu is almost negligible compared to other model compression options such as distillation which typically requires iterating over the whole training corpus and takes weeks to finish.
impact of precision.
we experimented with using bit precision instead of the bits that we use in the rest of the paper.
the experiments with different precision settings on codegen 2b models on humaneval and the results are summarized in table .
we use the static per column quantization setting for these experiments.
with bit weights and activation w8a8 we can meet the accuracy of a full precision model on humaneval.
however this accuracy drops by with weights quantized with bits while activations remain quantized with bits w8a4 .
we find that the model does not generate any meaningful outputs when activations are quantized with bits while the weights remain quantized with 8bits w8a4 indicating that the model is more sensitive to activation quantization than those of the weights.
.
.
quantizing extremely large code generation models.
so far we have seen that appropriately designed quantization techniques could preserve accuracy for models with medium to large sizes up to 6b parameters .
now we conduct an extreme study with codegen 16b one of the largest publicly available code generation models.
from table one can observe that both dynamic and static percolumn quantization achieve competitive results compared to the original model.
for example dynamic quantized model model size table execution accuracy of codegen 2b model at different activation and weight precision settings on humaneval.
here wxay indicates x bit weights and y bit activations.
pass full precision .
.
w8a8 .
.
w4a8 .
.
w8a4 .
.
table execution accuracy on codegen 16b and humaneval.
pass full precision .
.
dynamic quantization .
.
static per column quantization .
.
17gb could achieve similar pass 5and slightly lower pass compared to the significantly more gigantic fp32 model 75gb .
result quantization models often suffer minimal accuracy drop from the corresponding full precision models making them potential design choices for implementing greener code generation models.
.
robustness evaluation rq3 motivation.
it is well known that dl models are sensitive to input perturbations .
in particular a good quantized model should not adversely impact the robustness of a model i.e.
the original full precision model s robustness should not decrease drastically after quantization.
experimental setup.
to evaluate the effect of quantization on a model s robustness we evaluate both the original and the quantized models on humaneval and mbpp dataset with perturbed inputs.
in the nlp domain researchers propose different semantic preserving perturbations to inputs e.g.
mutating words with their synonyms or character level mutations .
we adapt similar techniques in our context.
in particular we perturb the text in each prompt with three different types of perturbations respectively see table character level perturbations by changing randomly selected characters to upper cases.
word level perturbations by substituting randomly selected words with synonyms from wordnet sentence level perturbations by paraphrasing the whole text with back translation .
in specific it transforms the english docstring into german and then translates back to english.
for these three types of perturbations we use the default settings and implementations from a standard text perturbation benchmark nl augmenter .
these perturbations are designed such that the original semantics of the natural language remains unaltered .
then we measure the average pass with greedy sampling for each model on the three perturbed datasets along with the 232esec fse december san francisco ca usa wei and gonugondla et al.
table the percentage of the pass drop on the datasets with character level ch word level w and sentencelevel s perturbations of prompt compared to the unperturbed ones.
humaneval mbpp ch w s ch w s incoder .3bfp .
.
.
.
.
.
d t .
.
.
.
.
.
s c .
.
.
.
.
.
s t .
.
.
.
.
.
.7bfp .
.
.
.
.
.
d t .
.
.
.
.
.
s c .
.
.
.
.
.
s t .
.
.
.
.
.
codegen 350mfp .
.
.
.
.
.
d t .
.
.
.
.
.
s c .
.
.
.
.
.
s t .
.
.
.
.
.
2bfp .
.
.
.
.
.
d t .
.
.
.
.
.
s c .
.
.
.
.
.
s t .
.
.
.
.
.
6bfp .
.
.
.
.
.
d t .
.
.
.
.
.
s c .
.
.
.
.
.
s t .
.
.
.
.
.
fp full precision d t dynamic per tensor s c static per column s t static per tensor unperturbed ones to avoid randomness and better observe the robustness trends.
to measure the robustness of a model we compute the change in pass results between perturbed and unperturbed inputs.
for each type of perturbation we compute the percentage change across all the inputs in a dataset as pass unperturbed pass perturbed pass unperturbed.
table reports the results.
the lower the value of the better the robustness of a model.
a negative drop means the model performs better with perturbed inputs.
observations.
the results show that overall all the quantization methods including per tensor dynamic per tensor static and percolumn static have comparable robustness performance w.r.t.
the corresponding full precision model.
in certain cases in fact quantized models perform better as shown in red .
on average across all model types and perturbations full precision per tensor dynamic per tensor static and per column static quantized models have .
.
.
and .
percentage of the drops on mbpp and humaneval datasets.
models quantized with static per column overall have slightly better robustness performance compared to the ones quantized with per tensor quantized models.
we further compute per sample difference in pass result between a quantized and the corr.
full precision model using wilcoxonmann whitney test this confirms the difference between the two models is statistically insignificant.result quantization does not have any negative impact on model s robustness a quantized model reacts to perturbed inputs very similarly as the corresponding fullprecision model.
.
accuracy for code summarization rq4 motivation.
here we check whether the quantization techniques studied so far are also applicable to other code related tasks.
in particular we chose code summarization as it is reversing the modality studied so far nl for code .
experimental setup.
here we use the finetuned plbart and codet5 models on the code summarization task in python released by the authors.
since codegen is not designed to generate summaries given a code snippet we do not use it in the evaluation.
in our early experiments we evaluated incoder full precision models on this task based on the author released code but got very poor performance therefore we do not pursue the model.
observations table shows the results.
we observe almost no drop in bleu score for codet5 models with both dynamic and static quantization.
in comparison while plbart with dynamic quantization matches the full precision performance we observe a performance drop with static quantization.
to understand this performance degradation we perform a qualitative comparison between these two settings.
a few examples are provided in table .
overall we observe that plbart with static quantization generates shorter summaries that affect the bleu score.
however the generated summaries are semantically comparable to the full precision version.
result quantized models behave comparably to the corr.
full precision models for code summarization.
threats to validity the main threats to the validity of our conclusions are external relating to the generalization of our findings to both other types of compression techniques and to other ml powered code related tasks.
first as discussed in section quantization based compression techniques are mostly suitable for usecase as a typical developer may not have resources to retrain the model from scratch using other compression methods.
second we focus on mostly generative tasks and thus study code generation nl to code in detail.
to evaluate the generalizability of our findings we also investigate the effect of quantization on code summarization rq4 .
finally we have other threats including studying each of these tasks on two models and two dataset respectively.
however these are state of the art open source models and data widely studied in the literature.
we further studied the different sizes of these models.
we evaluated on perturbed data rq3 which also gives us confidence on the stability of our results.
besides all the other quantization related parameters used in the experiments are empirically evaluated.
we also report the most stringent measurement pass to reduce any measurement bias.
233towards greener yet powerful code generation via quantization an empirical study esec fse december san francisco ca usa table example impact of word level character level sentence level perturbations on full precision and per tensor dynamic quantized models.
the perturbed region is underlined .
passing all testsexamples docstringfull precision dynamic per tensor unperturbed write a python function to determine whether all the numbers are different from each other are not.
character level write a python function to determine wheth er all the numbers ar e different from each other are not.
word level write a python function to determine whether all the numbers are unlike from each other are not.
s1 sentence level write a python function to see if all numbers differ from each other.
unperturbed write a function to extract the index minimum value record from the given tuples.
character level write a function to extract the index minim um value record from the given tuples.
word level write a function to extract the index minimal value record from the give tuples.
s2 sentence level write a function to extract the index minimum dataset from the given tuples.
unperturbed write a function to print check if the triangle is equilateral or not.
character level write a function to print check if the trian gle is equilateral o r not.
word level write a function to print check if the triangle equal equilateral or not.
s3 sentence level write a function to check whether the triangle is equilateral or not.
table smoothed bleu scores for code summarization.
full dynamic static static precision per tensor per tensor per column plbart .
.
.
.
.
.
.
codet5 .
.
.
.
.
.
.
table qualitative comparisons of summaries by plbart in full precision and static quantization.
full precision static per tensor copy an entire table to a temporary file.
dump the contents of a table to a temporary file recursively make all intermediate directories and subdirectories.helper function to make intermediate dirs downloads a video by its id and title.
download by vid generate rst api documentation for a module.generate the documentation for the given module.
conclusion code generation models based on large plms have set the new state of the art in generating functionally correct code given natural language description.
however the sizes of these models could be prohibitively large e.g.
billions of parameters which can cause problems for green ai and responsible ai.
therefore developing approaches towards improving model efficiency yet preserving their powerful generation capability is of great practical importance.
in this paper we address this problem by developing a quantizationbased recipe for such models.
we demonstrate the efficacy of proposed methods in terms of greenness accuracy and robustness.
as future work we would like to investigate the efficacy of quantization for more code intelligence applications such as code search code editing and code translation.
data availability the data presented in this paper is based on open source models and on publicly available datasets and therefore reproducible.
the information required to reproduce the results are in section .
.the code used to generate the the evaluation results is available at