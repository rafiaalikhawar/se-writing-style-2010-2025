outage watch early prediction of outages using extreme event regularizer shubham agarwal adobe research bangalore india shagarw adobe.comsarthak chakraborty university of illinois urbana champaign champaign usa sc134 illinois.edushaddy garg adobe bangalore india shadgarg adobe.com sumit bisht amazon bangalore india bishts002 gmail.comchahat jain traceable.ai bangalore india chahatjain99 gmail.comashritha gonuguntla cisco bangalore india ashrithag.
gmail.com shiv saini adobe research bangalore india shsaini adobe.com abstract cloud services are omnipresent and critical cloud service failure is a fact of life.
in order to retain customers and prevent revenue loss it is important to provide high reliability guarantees for these services.
one way to do this is by predicting outages in advance which can help in reducing the severity as well as time to recovery.
it is difficult to forecast critical failures due to the rarity of these events.
moreover critical failures are ill defined in terms of observable data.
our proposed method outage watch defines critical service outages as deteriorations in the quality of service qos captured by a set of metrics.
outage watch detects such outages in advance by using current system state to predict whether the qos metrics will cross a threshold and initiate an extreme event.
a mixture of gaussian is used to model the distribution of the qos metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution.
an outage is predicted if the probability of any one of the qos metrics crossing threshold changes significantly.
our evaluation on a real world saas company dataset shows that outage watch significantly outperforms traditional methods with an average auc of .
.
additionally outage watch detects all the outages exhibiting a change in service metrics and reduces the mean time to detection mttd of outages by up to when deployed in an enterprise cloud service system demonstrating efficacy of our proposed method.
work done at adobe research india permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts computing methodologies multi task learning regularization software and its engineering cloud computing software reliability general and reference reliability performance .
keywords outage forecasting system reliability and monitoring distribution learning mixture density network acm reference format shubham agarwal sarthak chakraborty shaddy garg sumit bisht chahat jain ashritha gonuguntla and shiv saini.
.
outage watch early prediction of outages using extreme event regularizer.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction the use of cloud services for deploying applications has seen a tremendous growth.
according to a recent report about of enterprises already use cloud services.
however these cloud services with numerous components are complex and prone to failures and outages due to frequent updates changes in operation repairs and device mobility.
cloud providers offer services with specific quality of service qos requirements which are technical specifications defining various aspects of system quality such as performance availability scalability and serviceability.
these qos requirements are driven by business needs outlined in the business requirements.
any failure to meet these predefined qos standards can lead to service level agreement sla violations resulting in revenue loss and customer dissatisfaction .
a study1found that a day outage by a leading cloud provider in the us could result in billion loss.
as a result cloud system nov 2023esec fse december san francisco ca usa s agarwal s chakraborty s garg s bisht c jain a gonuguntla s saini reliability is critical for business success as outages can severely impact qos metrics resource availability latency etc.
resulting in compromised system availability and a poor user experience.
several monitoring and alerting tools refer are employed to monitor and ensure the performance of cloud services.
automating system troubleshooting has been found to improve reliability efficiency and agility for enterprises .
despite these efforts cloud systems still experience incidents and outages .
timely detection and remediation of outages is essential for reducing system downtime.
however a reactive approach to incident detection is often used in practice hindering effective outage management .
with a possible innovation in being able to predict the outages well in advance the time to detect these outages can be reduced significantly.
consider a real world scenario in figure showing the timeline of an outage caused by a flawed configuration change in a storage service.
in this scenario a am a failure sparked a sequence of problems including sql errors at am and an increase in latency that starts affecting the qos at am b .
alerts were triggered at am when latency exceeded pre defined thresholds.
it took nearly minutes from am b to realize it was a cross service issue and declare an outage at am c .
an experienced site reliability engineer sre was engaged to mitigate the issue which was resolved at am d with all services back to normal.
here the flawed change impacted several sql databases and spread to other services.
the current reactive approach relying on alerts showed significant delay in detecting the outage as seen by the ramp up in underlying metrics affecting qos between am b and am c .
this example highlights the potential to predict a substantial fraction of outages in advance by utilizing the information available during the ramp up phase.
in consideration of the strict downtime constraints with only to minutes of allowable downtime per year corresponding to the uptime guarantees of .
and .
respectively the early detection of outages even minutes in advance can result in significant benefits.
the objective of this paper is to present a comprehensive solution aimed at reducing the mean time to detection mttd through early detection of outages.
outages manifest in two major ways i as degradation in qos and other metrics ii detected only through user reports and do not manifest in observable metrics.
the first type of outages accounting for of the incidents as observed from our data see exhibit characteristics that allow for prediction.
however outage prediction in cloud systems is a complex task due to the vast number of interdependent metrics.
an sre who traditionally detect outages using rule based alerts often only have a limited view of the overall system leading to difficulties in quickly and accurately identifying issues.
such approaches rely on human knowledge and is insufficient for large scale production cloud systems which have a vast number of complex and ever changing rules.
our interviews with engineers from various service teams revealed that detection could take hours particularly in cases where there are multiple concurrent alerts.
this highlights the importance of developing a more efficient method for predicting outages in cloud systems.
previous works on failure prediction through runtime monitoring which requires a substantial amount of data from the faulty state of the system are not applicable in this scenario as figure illustration of the life cycle of an outage.
a refers to the point when the root cause of a fault occurred b represents the time when it started affecting the performance metrics.
when the metrics crossed their respective thresholds alerts fired which led to an outage being declared at c. the time between c to d is when the engineers diagnose and resolve the issue.
the plots below show the variation in the root cause metric and the qos metric at these times.
outages are rare events and hence data is not available in the faulty state in plenty.
in addition using alerts to detect outages takes a toll on mttd since they are fired after a significant ramp up in metrics has been identified.
failure detection literature from other domains are not extensible to our case since the nature and the quantity of failures is very different in an enterprise service.
our scenario has very few outages and directly extending those works fail.
in this work we propose a novel system outage watch for predicting outages in cloud services to enhance early detection.
we define outages as extreme events where deterioration in the qos captured by a set of metrics goes beyond control.
outage watch models the variations of qos metrics as a mixture of gaussian to predict their distribution.
we also introduce a classifier that is trained in a multi task setting with extreme value loss to learn the distribution better at the tail thus acting as a regularizer .
outage watch predicts an outage if there is a significant change in the probability of the qos metrics exceeding the threshold.
our evaluation on real world data from an enterprise system shows significant improvement over traditional methods with an average auc of .
.
furthermore we deployed outage watch in a cloud system to predict outages which resulted in a recall and reduced mttd by up to minutes reduction .
our major contributions can be summarized as follows we propose a novel approach outage watch to predict outages in advance which are manifested as large deteriorations in a chosen set of metrics qos reflecting customer experience degradation.
outage watch works even in the absence of actual outages in training data.
outage watch generates the probability of a metric crossing any threshold making it flexible to define the threshold unlike classification tasks.
it predicts the distribution of qos metric values in future given current system state and improves learning the tail distribution via extreme value loss to capture outages before they happen.
an evaluation of the approach on real service data shows an improvement of over the baselines in terms of auc outage watch early prediction of outages using extreme event regularizer esec fse december san francisco ca usa while its deployment in a real setting was able to predict all the outages which exhibited any change in the observable metrics thus reducing the mttd.
the rest of the paper is organized as follows.
we briefly talk about related works in section followed by the background and problem formulation in section .
in section and we outline the motivation and describe outage watch .
with section analyzing its performance we conclude in section .
related work service reliability has been a well researched area in both academia and industry .
several works have attempted to address the problems of detecting localizing and mitigating outages and failures.
alerts are often used in detecting outages which are triggered when a system fault occurs and metric values crosses a threshold.
recent approaches such as airalert fog of war and ewarn compute features based on alerts and predict outages using tree based models.
however alerts only trigger when a system is already in a critical state thus incurring low reduction in mttd.
previous research in the area of time series forecasting constitutes a relevant body of literature since changes in metric value time series can be forecasted to detect outages.
classical auto regressive models to predict future metric values have limitations that have been overcome by recent advancements in deep neural network dnn based models such as rnns lstms and grus which have proven to be more effective in modelling time series data .
empirical evidence supports the use of these deep recurrent models for time series prediction .
however they perform poorly in predicting rare events like outages due to imbalanced data also known as extreme events .
predicting extreme events remains a challenging and active area of research .
recent studies have attempted to address the challenge of forecasting extreme events in time series data through innovative dnn architectures.
employs an auto encoder for feature extraction while uses an extreme value loss evl function.
however modelling point prediction of extreme events as classification task would need re estimation of the model if the threshold definition changes for defining such events.
however outage watch sets itself apart by combining the evl function with lstm and predicting the future distribution of events using a mixture normal network thus allowing a flexible change of threshold.
in the domain of reliability engineering prior studies have concentrated on detecting anomalies in time series data for monitoring systems .
supervised anomaly detection models like are effective in predicting anomalies but they require a substantial amount of labelled data making them unsuitable for our application.
on the other hand unsupervised methods can be used to identify anomalies in data without the need for labelled data.
more recently ml based models including autoencoders and transformers are used for anomaly detection in seasonal metric time series .
however they are limited to detecting events as they happen and lack interpretability to distinguish significant service degradation.
our approach differs by predicting the probability of metric exceeding threshold through its probability distributionanalysis instead of simply detecting whether the actual metric crosses a threshold or not as in traditional anomaly detection.
in related literature of failure detection several efforts have been made to predict specific types of failures utilizing large sets of system logs and metrics .
our approach differs in predicting general incidents based on a limited set of relevant metrics determined by domain knowledge.
previous works like have used ml and dnn techniques to predict disk failures.
a comparison of disk failure prediction methods was presented in .
however they are not suitable for predicting rare and extreme outages.
our approach based on distribution forecasting is flexible and outperforms some of the baselines discussed in .
3background and problem definition in this section we briefly describe basic monitoring system concepts followed by the problem formulation.
monitoring metrics in any enterprise level service deployment reliability engineers monitor the performance of these deployed services.
various monitoring tools like grafana newrelic splunk etc.
are employed to monitor the services and collect service metrics that can be used to measure the performance of the system.
monitoring metrics provide the most granular information with these tools recording them at specific time intervals.
alerts alerts are defined on monitoring metrics.
whenever these metrics cross a certain pre defined threshold defined by the reliability engineers an alert is triggered.
they represent system events that require attention such as api timeouts exceeding response latencies service errors and network jitters.
an alert contains several fields like the alert definition time of alert which service fired the alert text description of the alert datacenter region where the alert was fired and severity level which can be high medium or low .
extreme events an extreme event is characterized by a set of monitoring metrics exceeding their respective thresholds ile metric threshold based on our data causing unusual system behaviour.
this results in multiple alerts with varying levels of severity being triggered prompting engineers to take action.
it s important to note that not all extreme events lead to outages as they may not necessarily indicate a fault in the system.
extreme events which lead to outages cause violations in service level agreements slas .
extreme events can be perceived as situations when the system starts showing signs of degradation and the metric values surpass their or ile thresholds.
in addition it is important to note that extreme events are not recorded as separate incidents.
we generate proxy labels see .
.
to identify the extreme events.
outage in general outages are declared under severe situations when extreme events persist for a long time leading to a degradation in the service quality and often leading to a customer impact.
often outages affect multiple services where a fault propagates based on service dependency.
mitigating outages require cross team collaboration.
an outage usually triggers correlated alerts and escalates from a single or a group of alerts.
not all extreme events manifest as an outage since engineers manage these systems through constant monitoring and they identify and intervene in the system during potential issues even before they escalate into full scale outage.esec fse december san francisco ca usa s agarwal s chakraborty s garg s bisht c jain a gonuguntla s saini .
problem formulation we now formally define our problem statement.
metrics mtotare continuously monitored in the system essentially forming a time series.
the task is to understand the trends in some or all of the metrics inmtotand predict an impending outage with the goal of predicting it as early as possible.
it is obvious that a change in the metrics will show up only when a fault has occurred.
thus the goal of an outage forecasting solution is to minimize the lag time between the actual occurrence of the fault and its identification as an extreme event while also minimizing false positive cases.
withtas the wall clock time the input to the outage forecasting module is a set of relevant metrics see .
.
from where wis the window length.
more details on the pre processing of metrics is elucidated in .
.
.
with ground truth labels generated based on the occurrence of extreme events the supervised ml model outage watch aims to forecast an outage by learning the distribution of the relevant metrics at a certain time in the future.
a distribution of metrics is essentially a probability density function of the metric values at a certain time.
solution motivation in this section we present the rationale behind our solution design and provide a concise overview of how it functions.
figure multiple performance metrics some of which are qos metric gets impacted during an outage.
the collective information from all these metrics help in detecting the outage.
a b and d corresponds to the time defined in fig.
.
design motivation as discussed in outage prediction models should aim to predict the probability of an outage in advance to ensure timely recovery during a fault.
one way to achieve this is to monitor the system metrics for deviations from their regular trend as these deviations are often indicative of an outage.
however the current system monitoring tools often fail to detect deviations until they surpass a specific threshold and activate an alert provided that an alert has been defined for those system metrics.
however proactive monitoring of system metrics allows for earlier and more efficient identification of outages.
this motivates the design of our proposed approach outage watch .
we have observed from the data figure that during an outage multiple metrics show disturbances and deviations from trends simultaneously.
these metrics progressively become more extreme and affect multiple interdependent services.
outage watch takes multiple monitoring metrics as input and aims to predict the futurebehaviour or change in the values of certain metrics which we refer to as quality of service qos metrics and will be further defined in .
.
.
by predicting the distribution of these metrics in advance we can compute the probability of an upcoming outage based on the likelihood of the metric value crossing a certain threshold.
these thresholds can be dynamic and may be defined by service level agreements slas on qos metrics for each service.
why do we need to learn the distribution of metrics?
outages are often scarce in any established production level service.
hence if the outage prediction task is modelled as a simple classification problem a machine learning model will tend to fail even after the imbalance in the dataset is addressed because of the extremely skewed distribution of data points coming from one class outage occurrences .
for example it is often the case that one observes only two outages over a period of months.
however one way to circumvent this issue is to design our problem as a distribution learning task.
we then have a corresponding metric value at every timestamp of the data and a learned regression model can predict a metric value at any other time stamp.
we can also use the same strategy to learn a more accurate distribution in the tail where extreme events are often manifested.
this allows the flexibility of constructing the entire distribution and using variable threshold based on service level agreements sla requirements.
we provide further technical details on how the model predicts the distribution of the relevant metrics and how it can be used to predict outages in subsequent sections.
.
architecture overview the proposed framework of outage watch comprises two main phases a metric processing phase denoted as 1in fig.
and a distribution learning phase denoted as 2in fig.
.
module first selects the relevant metrics that will be used to forecast the outages pre processes them and then generates proxy labels .
what we mean by labels here and why do we need to generate them will be elucidated in details in .
.
.
on the other hand module forecasts the outage by predicting the distribution of the relevant metrics selected from phase 1at a future time.
in order for the distribution learner to predict outages at a future timet where is the prediction look ahead the machine learning algorithm must learn from the appropriate ground truth.
thus with input data at t the ground truth is constructed such that our method can predict the distribution of the metrics at time t and hence predict potential outages.
the distribution learner framework first uses a metric encoder to encode the system state in the past trend of monitoring metrics and then the distribution of the relevant metrics is forecasted using a mixture density network mdn .
mdn aims to predict the parameters of the forecasted distribution.
additionally the framework uses a classifier as an extreme value regularizer for better learning in the tail of the distribution.
the technical details are presented in .
.
though the pipeline is trained to predict the parametric distribution of metrics inference on whether and when the outage is being detected need to be developed.
.
focuses on identifying the likelihood of outages by evaluating the probability of each of the relevant metric crossing a defined threshold.
if there is a sudden increase in the probability of exhibiting extreme values based on theoutage watch early prediction of outages using extreme event regularizer esec fse december san francisco ca usa figure overall architecture of outage watch comprising of metric processing phase and the distribution learning phase.
the distribution and label prediction generated from at time step tare evaluated against ground truth metric value and labels from a future time step t which we get from 1b and 1c respectively.
distribution learnt outage watch takes this as an indication of an outage.
a thresholding mechanism is employed on the probability value to predict an outage.
5outage watch in .
we have discussed the overall architecture of outage watch and talked about its two main components briefly.
we shall now delve into the details of each component.
.
metric processing .
.
metric selection and quality of service qos metrics fig.
.
the monitoring tools collect a large set of service metrics mtotfor a system.
however many such metrics recorded by these tools are often never used by the sres .
also storage and handling of metrics data is non scalable and gets expensive over time.
consequently we derived a condensed subset of metrics denoted asm.
in our specific scenario we filtered down the number of metrics from inmtot to using a step wise procedure.
we employed established techniques for feature selection process.
firstly features were filtered using correlation analysis and rank coefficient tests.
then time series features that were constant throughout the time series or exhibited low variance were omitted due to their limited informational value.
to refine our feature set further we incorporated domain specific knowledge retaining only those metrics that either trigger alerts or have been emphasized in previous outage analysis reports that are generated post identification and mitigation of outages by engineers.
this process yielded a focused feature set well suited for effective service monitoring and analysis.
however only a fraction of mdirectly reflects the service quality as perceived by the customer for example latency of a service number of service failure errors resource availability etc.
these metrics known as quality of service qos metricsmqosor the golden metrics are used by the sre to define outages.
these metrics are crucial to monitor because cloud service providers face revenue loss if qos is not met due to violations of service level agreements slas .
based on the alert severity used by the sre team and the sla definitions we select five golden metrics comprising of i workload ii cpu utilization iii memory utilization iv latency and v errors.
these metrics are often used for system monitoring in industries and have been utilized in prior works .
the golden signals can often refer to different metrics based on service components.
for example the latency metric refer to disk i o latency for storage service web transaction time for web services query latency for databases etc.
outage watch uses the entire set of metrics m to forecast the likelihood thatmqosmetrics will surpass a threshold in the future.
we do not specifically forecast the likelihood of metric values of m mqoscrossing the threshold since these capture small issues which propagates within the system and gets manifested into the qos metrics.
also qos metrics capture the user impact directly.
it should be noted that our choice of mqosis based on system domain knowledge which we gathered from the inputs from reliability engineers on the most important metrics that define an outage.
nonetheless our approach will work in the same way for a different set ofmqosmetrics.
.
.
pre processing fig.
.
after the selection of metrics m we handle the missing values differently for different category of metrics.
for some metrics a missing value might indicate a null value which can be replaced with a zero.
for other metrics the rows containing missing values may be dropped.
for example if there are missing values in a metric that defines an error these can be replaced with zeroes as this indicates that there were no errors in the service.
however if there are missing values in utilization based metrics it may be necessary to drop those rows as the missing values could be due to a fault with the monitoring system.
once the missing values are handled each metric miis normalized using equation .
mi mi min mi max mi min mi following this we create a time series of mmetrics with a rolling window of size w. that is for each time instant tand metric mi m we create a time series miw mi t w ... mi t wheremi t refers to the value of the metric miattthtime instant.
we thus createx m1w m2w ... m m w which forms a sequence of metric values that can be used as an input to our encoder model.
.
.
label generation fig.
.
in real world production services outages are rare due to the robust deployment architecture and constant monitoring system in place.
sres often intervene to prevent the full scale outages resulting in a rarity of such events.
however these potential issues when interventions are performedesec fse december san francisco ca usa s agarwal s chakraborty s garg s bisht c jain a gonuguntla s saini can still be considered as extreme situations see which will allows us to better understand the system s behaviour and predict critical issues in advance.
thus instead of having the time periods when an outage was actually declared as the ground truth we modify our definition of labels to the time periods of extreme events.
such modifications facilitate us in forecasting the distribution of the relevant metrics.
however the challenge of labelling the data during these extreme events remain.
to address this issue we perform the following algorithmic steps that incorporates domain knowledge to generate proxy labels for outages or extreme situations.
takew minutes windows for each of the metrics mifrom the setmqos.
select those windows where the value of microsses a percentile thresholdtfor at least fraction ofw window.
filter the previously obtained time windows by keeping only those where at least kalerts were fired in the system.
these chosen time windows serve as proxy labels for extreme events.
here we take w as minutes tas as0.
kas .
these steps indirectly incorporate domain knowledge to accurately generate labels for outages and extreme situations using alerts defined by sre.
this process not only allows us to create a denser labelling of extreme events which can aid in the prediction of potential outages or situations that could have escalated to an outage in advance but it also includes some less severe cases which can aid in model training recall.
the proxy labels serve as positive training samples for the model.
.
distribution forecasting through this module outage watch aims to learn how the qos metrics will behave in a future time to forecast the probability of an outage.
we outline the component details below.
.
.
metric encoder fig.
.
before we can learn the distribution of qos metrics we must encode the past behaviour of the service metrics which captures the system state as a latent vector representation.
metric encoder extracts information via ml technique to encode spatial as well as temporal relation between the metrics.
spatial correlation captures how each behaviour of metricmi m affects the qos metrics mqos while the temporal dependence captures the time series trend in x. though both statistical and ml based techniques have been studied in this regard it has been shown that ml based models and especially recurrent neural network rnn models outperform conventional methods in encoding a time series due to their ability to capture sequential dependencies and temporal patterns in data.
several rnn based models like long short term memory lstm or bidirectional lstm bilstm models can be used for our purpose.
based on our experimental results with various rnn architectures see .
.
we choose bilstm as the metric encoder model.
lstm uses gating mechanism to control information encoding while the hidden state ht a multi dimensional vector maintains the encoding of the input time series.
bilstms extend lstms by applying two lstms one forward and one backward to input data to capture information from both directions.
the metric encoder takesxas input and outputs a vector representation h capturing the temporal and spatial relationship of the metrics.
.
.
multi task learning.
we propose a multi task learning problem where one task is to learn the distribution of each qos metricy mqosfrom the metric encoder output while the other task classifies the metric encoder output as an outage or not.
we now describe each of the task in detail.
task distribution learning fig.
.
the first task aims to learn a parametric distribution governing the qos metrics conditional on the encoded system state representation.
more precisely given a time series of metrics xwhich was encoded to form a vector h we wish to estimate the probability of a metric y mqosgivenx p y x .
learning a distribution is essentially learning the parameters governing it.
in general the metric yis often assumed to follow a normal distribution n y since we observe limited data points in the tail of the distribution.
however in a real production system a normal distribution might underfit the actual data distribution.
often we don t necessarily have simple normal distributions.
to overcome this limitation we estimate the distribution of the qos metric via a mixture of normal distributions withcmixture components where the probability distribution of a metricygivenxis of the form p y x c c 1 c y x n y c y x c y x wherecdenotes the index of the corresponding mixture component and c yis the mixture proportion representing the probability that ybelongs to the cthmixture component n y c y c y .
it is well known in the literature that a mixture of gaussian normal distributions is capable of modelling any arbitrary probability distribution with correct choice of c c cand c .
we aim to estimate a mixture distribution for each qos metric yvia a separate mixture density network mdn that comprises a feed forward neural network to learn the mixture parameters y yand the mixing coefficient y. we have chosen c 3after experimental analysis see fig.
6a and hence each mdn has values of y y and y. the network is learnt through minimizing the negative log likelihood loss of obtaining the ground truth metric value of ygiven the mixture distribution averaged over all metrics mqos.
formally arg min l r x y rlogp y x herercorresponds to the realm of possibilities.
mdn hence learns the parameters of the distribution of qos metrics which can then be further used to compute the probability of ycrossing a certain threshold to predict outages.
task outage classification fig.
.
we have observed through experiments see .
.
that the distribution learnt by mdn performs poorly at the tail where extreme values are generally observed and can be used to forecast outages figure .
to overcome this limitation a feed forward neural network performs outage classification in a multi task setting where we predict whether an outage will happen or not from the encoded output from the metric encoder .
we use the output proxy labels generated in .
.
as a ground truth.
this module acts the extreme value regularizer.
where the intuition is that the synthetically generated proxy labels will act as aoutage watch early prediction of outages using extreme event regularizer esec fse december san francisco ca usa figure using extreme value loss in the classifier over bce can aid the distribution learner to learn a better distribution at the tail.
regularizer for better learning in the tail of the distribution.
similar to distribution learning we have separate neural networks for each qos metric inmqos.
to classify outages we have used the extreme value loss evl which is a modified form of binary cross entropy bce loss as the loss function.
evl reduces the number of false positives by assigning more weight to the penalty of incorrectly predicting outages.
evl works well with imbalanced data as we have observed through experiments.
evl can be formally defined as levl nn i 1 0 yi yilog yi 1 yi yi log yi wherenis the size of the batch yi is the ground truth value and yiis the value predicted by our model outage watch 0is the proportion of normal events in the batch and 1is the proportion of extreme events in the batch.
we use 2in the loss function for the experiments.
.
training since we want to predict the probability of an outage in advance and reduce the mttd the ground truth metric values and the proxy labels should also correspond to a future time t .
thus at a time t the metric encoder takes xwhich is a time series of all metric values from t wtotas input.
the ground truth value for task is the metric value for each qos metric yat timet and while for task we use the proxy label see .
.
computed from the qos metric values at t .
we train the entire pipeline consisting of the metric encoder mixture density network and the classifier in an end to end fashion.
it should also be noted that with a large one can aim to predict an outage well in advance but the distribution followed by the qos metric will not be accurate.
hence a careful selection of is necessary.
by our experiments see fig.
6b we show that mins works the best for our purpose.
.
inference at inference time we predict and use only the distribution of the qos metrics while excluding the classifier from our inference pipeline to predict an outage.
the distribution of the qos metrics provide us with more flexibility and enables us to define outages figure tasks performed during inference time to predict potential outages from the predicted distribution based on custom thresholds.
moreover the distribution captures the entire spectrum and specifically the tail metric values.
however the steps to predict an outage from the distribution of the qos metrics can be summarized as below.
.
.
probability computation fig.
.
we first compute the probability of an outage occurring by computing the probability that the value of the qos metric crosses a pre defined threshold.
these thresholds are generally defined by the slas that have been agreed with a particular customer.
as an example an agreement of achieving a service latency of at most milli seconds for of the times might have been signed with the cloud service provider and can be termed as an sla.
hence in this case the threshold is .
formally the probability of a qos metric value ycrossing a thresholdt and hence the probability of an outage occurring can be defined as prob outage c c 1 c .
.
outage prediction fig.
.
from the probability computed above we use a thresholding technique on prob outage to predict the outages.
we compute the threshold based on youden s j index on training data.
it is a popular thresholding technique for imbalanced data extreme events are very few as compared to theusual metric values which uses the area under the roc curve auc to compute the threshold.
on the training data containing the proxy labels and the corresponding probability of an outage occurring youden s j index tries to compute the threshold such that it increases the precision and recall.
we maintain the same threshold for all our evaluations.
implementation setup in this section we outline the experimental process and the setup we followed.
we have implemented outage watch inpython and used tensorflow2 a standard open source library to implement the ml models.
we have run outage watch on a system having intel xeon e5 v4 .3ghz cpu with cores.
source of data the data is sourced from a prominent saas enterprise offering extensive software and digital services.
it leverages amazon web services aws and microsoft azure for cloud december san francisco ca usa s agarwal s chakraborty s garg s bisht c jain a gonuguntla s saini provisions.
the software infrastructure covers diverse domains including programming languages databases aws azure docker kubernetes jenkins and more.
dataset we collect the dataset for evaluating outage watch from a real world service hosted by a large cloud based service provider.
the metrics data was obtained through a message queue pipeline deployed on the monitoring system of the service.
we have collected a total of months of metrics and outage data from the monitoring system for training and testing purposes where data from the last weeks were used for testing.
we collected metrics which was reduced to as discussed in .
.
.
outages have a widespread impact within the enterprise affecting multiple services.
since there were no outages observed during the period of the training data while one outage was observed during the period of test data we generated time periods when the extreme situations occurred see .
.
.
it amounted to around of the total training data thus exhibiting a skewed label imbalance.
model hyperparameters the implementation details for the ml models used in outage watch .
are outlined as follows.
the bilstm model in the metric encoder has hidden units h followed by a dropout layer with p .
.
regularization techniques were used while training the model to prevent overfitting.
the feed forward mixture density network mdn which models the distribution parameters of qos metrics has two hidden layers with neurons each with relu activation function in the hidden layers.
the neuron outputting the mixing factor of components use a softmax function.
the classifier feed forward network has one hidden layer with neurons with relu activation while the output layer use the sigmoid function.
we use a learning rate of .
with the adam optimizer for training.
baselines the baselines for evaluation are chosen following an approach similar to the work presented in .
we leverage some of the fundamental classification and regression techniques for outage prediction.
it includes naive bayes classifier random forests and gradient boosted decision trees.
naive bayes is a probabilistic machine learning model while the other two are ensemble methods that are constructed using a multitude of individual trees.
we implement these baselines to use them as a proxy for prior learning based outage prediction models.
we also use a bilstm classifier as a baseline which uses only classifier network on the encoded bilstm representation to predict outages.
evaluation metrics to evaluate the effectiveness of various approaches we use auc pr and f1 score.
auc pr calculates the area under the precision recall pr curve and is commonly used for heavily imbalanced datasets where we are optimizing for the positive class outage being detected only.
auc pr is computed using the probability of an outage occurring or not from equation .
also based on the probability values we use the procedure in .
.
on training data to compute a threshold for detecting outages which we use to compute the f1 score in test data.
other hyperparameters for all the experiments we choose a window3w 60mins to create a windowed time series of metric data.
on the contrary we vary the prediction look ahead from mins to mins.
in .
.
we experimentally show the optimal 3according to our empirical study over issues are triggered within hour after the impact start time.value of .
unless specified we maintain threshold t eq.
for all the experiments.
evaluation in this section we present the experimental results and aim to address the following research questions rq1 how do our design decisions align with the ablation studies performed?
rq2 how does our approach compare to the established baselines?
rq3 how does our approach perform in a real world cloud deployment scenario?
.
design choices rq1 .
.
metric encoder model.
in .
.
we claim to use bidirectional lstm bilstm as the model for metric encoder.
in this subsection we discuss the experiments conducted to determine the optimal architecture and the rationale behind using bilstm.
we conducted experiments using four different types of rnns lstm bilstm stacked lstm and stacked bilstm .
the encoded representation was then used to forecast the distribution in a multi task setting with evl in the classifier network.
the performance of each architecture was evaluated using auc pr metric.
we have experimented with varying values of min.
the results are presented in table .
table design choice comparison of different rnn architectures over different prediction windows in terms of auc.
modelprediction look ahead mins mins mins mins lstm .
.
.
.
bilstm .
.
.
.
stacked lstm .
.
.
.
stacked bilstm .
.
.
.
we see that bilstm encoder performs the best in our case for all values of .
bilstm can track a time series in the forward as well as the backward direction.
thus it can help to encode the overall variation in performance metrics as well as retain recent trends which makes bilstm an ideal choice for encoding the information in the metric time series.
.
.
multi task learning.
table illustrates that incorporating the proposed multi task learning approach improves performance compared to using only a single task classification network or mdn.
we used bilstm as the metric encoder.
we evaluate the different schemes using the auc pr metric.
for the classifier network individually as well as when evaluated in a multi task setting we employed the evl loss.
in a similar setting as of the above we perform the experiments with min.
we observe that when the bilstm encoded representation was used to learn a distribution of the qos metrics in a multi task setting learning the distribution and classifying the time periods of extreme values it performed better than when the tasks wereoutage watch early prediction of outages using extreme event regularizer esec fse december san francisco ca usa table design choice comparison of different model architectures over different prediction windows in terms of auc.
here mtl refers to the multi task learning proposed model.
modelprediction look ahead mins mins mins mins classifier .
.
.
.
mdn .
.
.
.
mtl .
.
.
.
performed individually.
this corroborates our design choice of using a multi task learning in the distribution forecasting module.
.
.
classifier network loss.
additionally we perform further experiments to show the performance enhancement of using evl loss over binary cross entropy bce loss for the classifier network in .
.
.
the results as shown in table indicate that the use of evl in conjunction with multi task learning improves performance in predicting extreme events as compared to solely using bce loss.
the metric used to evaluate the performance of the models is the f1 score and the results demonstrate that evl outperforms bce.
table design choice comparison of bce and evl loss over different prediction windows in terms of f1 score.
modelprediction look ahead mins mins mins mins outage watch with bce .
.
.
.
outage watch with evl .
.
.
.
.
.
ablations.
we perform further ablation studies to prove our parameter choices for outage watch .
we first illustrate through figure 6a that predicting the distribution of qos metrics using a mixture gaussian distribution with components performs the best for predicting the outages.
we see in the figure that with more or less components there is a drop in the overall performance.
we also conducted an analysis to determine the optimal prediction look ahead .
with large look ahead we can forecast the outage well in advance.
it however suffers in accuracy of the prediction probability since the inherent trend in the metric changes.
thus there is a trade off between and accuracy metric.
through our experiments we found that a look ahead of minutes resulted in the most satisfactory performance as the validation loss showed negligible increase before reaching a sudden jump beyond this point.
thus outage watch can forecast an outage and reduce the mttd by at least mins than the current approaches .
.
.
baseline comparison rq2 with our design choices fixed that is using a bilstm for encoding the metrics and then forecasting the distribution of the qos metrics in a multi task learning setting we compare outage watch with several baselines as described in .
we use auc pr to compare the performance and tabulate the results in table .
similar to the previous evaluations we experiment with multiple values of min.
the results demonstrate that our proposed a b figure a model performance vs number of gaussian mixture components cto predict by mdn b loss of the mdn eq.
vs prediction look ahead approach of forecasting the distribution outperforms all other techniques including traditional methods by a significant margin.
it has been shown to be a highly effective approach for predicting outages through qos metrics one key advantage of outage watch is its ability to predict the probability of an outage occurring based on any threshold t see equation since we are forecasting the distribution as opposed to just learning a classifier with the ground truth proxy labels.
on the contrary traditional methods are limited to predicting outages to a specific threshold similar to the threshold used for creating the ground truth labels in training data .
as discussed in .
.
we create proxy labels based on the threshold of i.e.
if the metric value crosses the percentile mark it is considered to be a potential extreme event.
thus the classifier network was trained using the generated proxy labels as a ground truth.
however when we evaluate the distribution forecasted by outage watch based on the probability of the metric value crossing a threshold of t andt we achieve high f1 scores as seen in table .
was maintained at minutes.
this flexibility in threshold selection is a major advantage of our proposed method and sets it apart from traditional techniques as the model need not be trained again to get the results on a different threshold.
table performance of different models over different prediction windows in terms of auc score.
modelprediction look ahead mins mins mins mins naive bayes .
.
.
.
random forest .
.
.
.
gradient boost .
.
.
.
bilstm classifier .
.
.
.
outage watch .
.
.
.
table performance of proposed model over different percentile thresholds in terms of f1 score.
modelprediction thresholds t outage watch .
.
.968esec fse december san francisco ca usa s agarwal s chakraborty s garg s bisht c jain a gonuguntla s saini figure analysis of the outage watch s performance on two real outages that happened during the deployment period.
the upper plots are the metric values which shows deviations actual value is masked while the lower plots compute the probability of the metric value exceeding a threshold of t .outage watch was able to correctly predict both the outages in advance in comparison to the current approach which is indicated by the light red rectangular shaded region .
.
deployment results rq3 we deployed outage watch in an enterprise system for months and predicted the probability of impending outages.
the overall objective of outage watch is to predict outages in timely manner thereby assisting the engineers.
from the forecasted distribution we first predict the probability of a metric value crossing the thresholdt .
we then predict potential outage situations through the thresholding technique as described in .
.
on the metric probability that crosses the threshold first.
the threshold is generated based on the weeks of training data.
we report the precision and recall of the prediction made by outage watch .
we also report the reduction in time to detect outages by the model against the current reactive approach which is used to report an outage.
in this deployment we implemented a continuous re training strategy for outage watch updating the model after every outage detected with full data for up to two days after the outage ended.
this approach was taken to ensure that the system state changes during an outage are reflected in the updated model.
our strategy balances effective model retraining with efficiency.
it s worth noting that our focus here does not encompass a robust retraining strategy targeting drift issues.
the potential for addressing data distribution changes arising from factors such as the implementation of new business functionalities could influence outage detection.
however this aspect falls outside the purview of our current work.
we present a case study for multiple outages that were flagged by the engineers during the deployment period and how outage watch performed in forecasting them.
during the deployment period a total of outages outage a b c d took place out of which outage a b andcmanifested through the qos metrics in the cloud service.
outage watch was able to accurately predict all these three outages and reduced the mean time to detection as compared to the current approach followed by the engineers.
however outage d was not evident through the qos metrics and hence it was not predicted.
we report the precision recall and the reduction in mttd foroutage a b andcin table .
for each outage we consider data from a day before and days after to report the precision.
precisionhere refers to the number of outages correctly predicted over the number of times the probability value was above the threshold for a sustained period mins .
recall refers to the number of outages predicted correctly over the total outages that could have been predicted which is .
finally mttd reduction for each outage is reported as a percentage of c time of prediction c b see figure for the notations of b c .
we observe that outage watch outperforms other baselines in terms of precision and recall.
recall is while precision is .
we also observe a large reduction in mttd4foroutage watch implies outage was not predicted .
when evl is used with outage watch precision improves.
table results for outages predicted by different models using qos metrics model precision recallreduction in mttd outage a outage b outage c naive bayes random forest gradient boost bilstm classifier bilstm mdn outage watch bce outage watch evl to provide a deeper understanding of how the system works we present two case studies of outages outage a and outage b that were successfully predicted by outage watch .
.
.
outages predicted.
an illustration of these two outages and the performance of outage watch are presented in figure .
outage a the outage was auto launched at time taby the monitoring systems due to out of heap memory issues on several app store nodes in one of the regions.
the system outage watch was able to predict the out of heap memory 4mttd improvement there is a decrease of tens of minutes particularly notable given the conventional mttd is also in the range of a few tens of minutes.outage watch early prediction of outages using extreme event regularizer esec fse december san francisco ca usa issue correctly and flagged the outage 67minutes before the outage was actually launched.
the system failed due to a fault in the event consumer queue which got stuck and was not processing since ta 100minutes in one of the regions.
outage b in another outage alerts regarding high error rates in a serviceswere fired.
the outage was auto launched attbby monitoring the high error rate.
it was found that a faulty update in one of the aws components had caused the component to fail.
that component was being used by sand therefore after the update swas unable to process incoming requests.
the issue commenced since tb 25minutes.
outage watch correctly detected this outage and reduced the mttd by minutes from the auto launched approach.
.
.
outages not predicted.
in addition we also present a case study of an outage outage d which occurred after a change was implemented that inadvertently tripped an ui feature blocking protocols from making requests.
however such outages due to ui issues are not meant to be manifested in the qos metrics.
as a result it was not detected by our model.
however this is not a false negative in our case since no changes were observed on the qos metrics and hence outage watch could not have detected the outage.
this also highlights the limitations of our proposed method as it relies on the monitoring metrics to predict outages.
.
discussion from the above results we see that outage watch performs better than the other comparable baselines as well as on real world deployment setting.
however it can be observed from table that outage watch reports multiple false positives which is dependent on the quality of threshold we choose to detect an outage.
one can circumvent this issue by having a higher threshold which might reduce the reduction in mttd according to figure .
however since we are working with the constraints of data our training data set did not have any outages and the number of extreme events were very less as compared to the test data.
this resulted in youden s index to compute a threshold lower than .
.
however with more data in the training set reflecting the extreme events thresholding model becomes more proficient to distinguish true positives from the false positives which results in a higher youden s index .
as and when data arrives youden s index model should be retrained to get an improved threshold.
hence thresholds can be re adjusted as well with the predicted distribution reducing the false positives.
however in real world scenarios the presence of false positive cases is nuanced in this context.
our analysis of model predictions along with sre input identified false positive cases where predicted extreme values didn t lead to outages.
some of these issues were resolved manually or self corrected.
consequently the occurrences of false positives are not necessarily negative indicators as they can capture mild issues that resemble potential disruptions.
however false positives are always undesirable in the workflow of sre.
while sres can get insights from false positives their presence is undesirable due to the risk of alert fatigue from frequent possibly minor predicted outputs.
to tackle this we provide the benefit of adaptable threshold selection as discussed in .
.
a detailed study on how sres can distinguish a false positive from a true positive output in their workflow remains an open question.overall outage watch proves to be very helpful for production outage management as it was able to predict the outages well in advance.
this could help in reducing the severity and consequently helps with quick mitigation of the outages.
we also highlight a limitation of the model which relies solely on monitoring data and the qos metrics and cannot predict those outages that are not indicative from these metrics.
nevertheless we believe that by incorporating additional sources of data such as log files and change details we can improve the performance of our model in detecting these types of outages.
overall our proposed method is a valuable tool for predicting outages using qos metrics and has the potential to improve system performance and reliability.
conclusion in this paper we present a novel approach outage watch for predicting outages by forecasting the distribution of the relevant qos metrics.
this approach takes a time series input of multiple monitoring metrics representing the current system state within a time window to encode the information present in the time series in a vector representation.
it then uses the encoded information to learn the distribution of the relevant metrics using a feed forward neural network.
in addition outage watch uses extreme value loss to classify the extreme events in a multi task manner which helps in learning the distribution of the metrics in the tail.
at inference time our model uses the distribution learnt to compute the probability of the metric to cross a certain threshold and then predicts outages based on a thresholding technique.
our experiments on real data show the efficacy of our method with an average auc of .
.
the applicability and robustness of our approach has been verified by deploying it on an enterprise system.
future works our future works include extending the evaluation duration on production systems to provide insights into long term performance.
few interesting modifications include automated dynamic threshold selection and providing a confidence bound for our prediction to distinguish between the true positives and the false positive predictions.
furthermore refining the definition of extreme events could enhance predictive capabilities.
incorporating diverse data sources such as log files and trace data may extend the scope of outage detection.
we also plan on exploring robust re training strategies to improve model performance in production.
lastly implementing a feedback loop or introducing human in the loop dynamics may further refine the model s predictive abilities.
data availability the metrics data used in this research is proprietary and cannot be shared due to confidentiality agreements with the enterprise service provider.
however the model code along with a sample data are made available at outage watch5.
the sample data includes the format of the input data required for the model with random metric values.
any real dataset can be pre processed to the specified format for implementing the approach.
we believe that the model code and sample data provided in the paper are sufficient for replication and working with similar datasets.
december san francisco ca usa s agarwal s chakraborty s garg s bisht c jain a gonuguntla s saini