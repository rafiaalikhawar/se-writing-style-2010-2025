multiple boundary clustering and prioritization to promote neural network retraining weijun shen state key laboratory for novel software technology nanjing university china shenweijun smail.nju.edu.cnyanhui li state key laboratory for novel software technology nanjing university china yanhuili nju.edu.cnlin chen state key laboratory for novel software technology nanjing university china lchen nju.edu.cn yuanlei han state key laboratory for novel software technology nanjing university china mg1833022 smail.nju.edu.cnyuming zhou state key laboratory for novel software technology nanjing university china zhouyuming nju.edu.cnbaowen xu state key laboratory for novel software technology nanjing university china bwxu nju.edu.cn abstract with the increasing application of deep learning dl models in many safety critical scenarios effective and efficient dl testing techniquesaremuchindemandtoimprovethequalityofdlmodels.
oneofthemajorchallengesisthedatagapbetweenthetraining data to construct the models and the testing data to evaluate them.
tobridgethegap testersaimtocollectaneffectivesubsetofinputs fromthetestingcontexts withlimitedlabelingeffort forretraining dl models.
to assist the subset selection we propose multiple boundary clusteringand prioritization mcp atechniquetoclustertestsamples into the boundary areas of multiple boundaries for dl models and specify the priority to select samples evenly from all boundaryareas to make sure enoughuseful samplesfor eachboundary reconstruction.
to evaluate mcp we conduct an extensive empiricalstudywiththreepopulardlmodelsand33simulatedtesting contexts.theexperimentresultsshowthat comparedwithstateof the artbaselinemethods oneffectiveness ourapproachmcp has a significantly better performance by evaluating the improved quality of retrained dl models on efficiency mcp also has the advantages in time costs.
keywords software testing deep learning multiple boundary neural network retraining yanhui li is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on thefi rst page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
reference format weijunshen yanhuili linchen yuanleihan yumingzhou andbaowen xu.
.
multiple boundary clustering and prioritization to promote neuralnetworkretraining.in 35thieee acminternationalconferenceonautomated software engineering ase september virtual event australia.
acm new york ny usa pages.
.
introduction deeplearning dl aimstoconstructaformofadvancedpattern classification by the use of artificial neural networks nns which learnstodistinguishbetweensamplesofdifferentcategoriesand recognizes new samples into the right categories .
along with large scale training data sets e.g.
yfcc100m and improved processing capabilities e.g.
gpus dl models have achieved more deeper layers of data transformation and surpassed the levels of human experts in manyfi elds including natural languageprocessing speechr ecognition autonomousdriving and so on.
at the same time concern has been raised at the qualityofdlmodelsinthesafety criticalapplications especially after a serial of self driving car fatalities reported to the public1 which calls for more testing effort to deal with issues on reliability of dl models.
in the angle of statistics on dl models testers evaluate the quality of dl models on the whole performance of all test samples collected from a specific application context where the faults of dl models are denoted as the wrong prediction behaviors i.e.
the predicated labels are not equal to the actual ones .
the different distributions between the training data used to construct the models and the testing data collected from real applications may result in the lower than expected performance of dl models.
for example a medical diagnostic dl model trained by the data of foreign patients is introduced to the local hospital.
this dl model maynotworkwellforthelocalpatients justsincetheyaretrained with different distribution of patient data in some aspects e.g.
the race.
asthedifferentdistributionbetweenthetrainingdataandthe testingdataisaninherentproblemintheapplicationofdlmodels 35th ieee acm international conference on automated software engineering ase oneofthemaintasksfortestersistodealwithitsadverseaffects.
in artificial intelligence ai area a common approach to adjusting dl models to deal with new data of different distributions is constructingretrainedmodelsfromcurrentmodels basedonthe theory of active learning .
specifically the neural networkarchitecturesofretrained models areinitializedasthesame asthecurrentmodel andupdatedbytrainingonadditionallabeled samples ofnew data.inspired bythis idea testers fordl models couldcollectusefulinputsfromthetestingcontextandreportthem for retraining current models.
intuitively more additional samples with label information will help retrain dl models to adjust to the testing context with more reliability.
however intherealapplication labelingthesesamplesrequires a lot of manpower which is prohibitively time consuming andexpensive .
this fact greatly weakens the possibility of employingenoughlabeledsamplesfromthetestingdata.asaresult researchers of dl testing are setting their eyes on a new goal to design and implement effective test sampling methods to save the labeling effort .
liet al.
proposedan efficienttest selection method to estimate the accuracy of dl models on operational environments.kimetal.proposedanoveltestadequacycriterion called sadl surprise adequacy for deep learning systems fortestingdlsystems whichcanguildthetestselectionfortesting or retraining dl models with higher accuracy.
based on the above considerations we focus on the problem guiding the retraining of dl models with limitedlabeling effort the testers aim to select an effective subset of testing data labelthem test them and report them which are most helpful to retraindlmodels.toaddressthis wepropose multiple boundary clustering and prioritization mcp as the affirmative answer andincarnationofthisproblem.afterrevisitingthefunctionality of dl models we observe that as a form of advanced multiple classification dl models have multiple boundaries according to differentclasses andthereliabilityofdlmodelslargelydepends on the correctness of the decision boundaries of patterns .
the differencebetweentrainingandtestingdatacouldbeconsidered as the difference of boundaries between original dl models and retrained dl models.
as a result retraining dl models converts to thereconstructionofmultipleboundaries.themainideasofour mcp algorithm are a clustering we cluster the samples of testingdataintodifferentboundaryareasaccordingtotheirfirstand second predicted classes b prioritization we specify the priority to select samples evenly from all boundary areas to make sure enough samples for each boundary reconstruction.
mcp has been evaluated on three widely used open datasets mnist cifar10 and svhn and three dl models with advanced dl networks .
in each dataset we leverage two strategies image transformation and adversarial attack with synthetic operators on generating realistic synthetic images to simulate the testing contexts with different distributions.asmentionedabove wefocusonthescenariowith thelimitedlabelingeffort i.e.
onlytheverysmallpercentoftest datacouldbelabeled.intheexperiment wefocusonfourrepresen tative labeling ratios and to check the performance of mcp with labeling percent not larger than .
to assess theperformance of mcp we introducefi ve test sampling methodsas the baselines three state of the art methods from dl testing lsa dsa at icse ces at fse one active learningmethod aal andthecommonbaselinesimplerandom selection srs .
our experimental results support the claim that mcp performs well a on effectiveness we observe that mcp performs significantly better than the other baseline methods when considering the accuracy improvementsof retraining dl models i.e.
with the highest values of accuracy improvement after retraining the dlmodels b on efficiency we observe that with the exception of randomselection otherbaselinemethodscostseveraltimesoreven hundredsoftimesmoreruntimethanmcp.thatmeansmcphas anobviousadvantageintimecosts.tosumup mcpisaneffective and efficient test sampling method for guiding the retraining of dl models .
our study makes the following contributions strategy.
this paper proposes a novel technique multipleboundary clusteringand prioritization mcp asanefficient strategy to measure the usefulness of the samples by con sidering boundary areas of multiple boundaries w.r.t.
the distribution of data in the testing context.
study.thisstudyincludesanextensiveempiricalstudyof threedlmodelsand33simulatedtestingdatasetswithmore than testing inputs.
the results of our experimentshow that mcp has a significant better performance than otherfi ve baseline methods evaluated on effectiveness and efficiency of six studied methods.
therestofthispaperisorganizedasfollows.insection2 we introducethebackgroundofourpaper includingtheprincipleof dl models and two dl testing properties.
in section we present a technical description of our algorithm.
in section we introduce ourexperimentalsettings includingsubjectdatasets dlmodels data simulation baseline methods research questions and so on.section explains experimental results and discoveries.
section6furtherdiscussessomeimportantexperimentaldetails.section and are threats to validity and related works.
section is the summary of the paper.
background in this section wefi rst introduce the principle of dl models then present two properties of dl testing.
.
the principle of dl models for a dl model m it can be considered as a neural networks implementation of a mapping function from the feature domain fd to the classification domain cd m fd cd wheretheinput x fdisamatrix vector containing high dimensional features e.g.
a gray value matrix to denote a picture and the output y cdis one predictive label in a given set of target classification labels cd c1 c2 ...cm .
the standard architecture of a deep learning software generally consists of three parts input layer hidden layer and output layer.
the input layer connects the input matrix xwith the hidden layer and its dimension is the same as the number of elementsin the input matrix.
the hidden layer can be composed of many 411layers and the structure of each layer can be freely chosen.
the outputlayerconnects thehiddenlayerand thesoftmaxlayer and its dimension is the same as the size of the target classification label set cd.
layers are connected by weight values which are obtained by training.
the softmax function takes a sequence of nvaluesy1 y2 ... ynfromoutputlayer andnormalizestheminto noutput probabilities y y ... y nfor target classification labels c1 c2 ... cn respectively y i eyi summationtext.1n j 1eyj the probability values y y ...y nsatisfy y y ... y n thefinaloutput yistheindexofthelargestprobabilityvalue y ci whereisatisfiesi argmaxi y i .
.
dl testing properties dltestingpropertiesrefertothatforwhatconditions adlmodel offers some guarantees under dl testing .
in this subsection we introduce two properties we focus on in the rest of the paper.
definition accuracy .
letmbe the trained dlmodel that we need to test.
let dtbe the testing data to evaluate m. let xbe an input of the model x dt.m x is the predicted label ofxgenerated by m andl x is the actual label.
the accuracy acc m dt ofmw.r.t.dtis defined as the probability that the predicted labels are equal to the actual labels dt summationdisplay.
x dti m x l x where the indicator function i returns if the logical expression issatisfied andreturns0ifitisunsatisfied.
accuracy isoneof the most widely used statistical indictor to measure the quality of dl models in previous studies for dl testing .
definition accuracy improvement .
letmobetheoriginal dl model which is with lower accuracy w.r.t.
the testing data dt.letmrbetheretrainedmodelsfrom mobyretrainingwith the useful subset duof samples selected from the test data dt.
the accuracy improv ement is defined as the delta of the accuracies between two dl models acc mr dt acc mo dt methodology in this section we present a technical description of our algorithm.
first weformallypresentthestudiedproblem.next wedescribe the basic idea of our algorithm.
finally we describe our algorithm in detail.
.
the studied problem basedonabovedefinitions wepresenttheproblem guidingthe retraining of dl models with limited labeling effort specifically problem.
mois tested with dt wheredtexhibits the different distribution compared with the training data of mo.
all samples s s dt o fdtare unlabeled.
given labelingefforte i.e.
esamplesoftestingdatacouldbelabeledwith e lessmuch dt ouraimistoselectthethesubset ducontaining einputs from dt label them and return them to retrain mo with an as high accuracy improvement as possible.
obviously the subset duwith more accuracy improvement would be considered as more useful.
to solve this problem we try to construct an effective and efficient algorithm for selecting du.
.
the basic idea for a classification task a good dl model obtains as accurate as possibleboundaryinformationbylearningthehiddenrulesfrom large scale training data.
in the research of ai and dl testing samples near the boundary have attracted widespread attention.
hamidzadeh et al.
proposed an instance reduction method for k nnclassificationbasedonhyperrectangleclustering inwhich theykeeponlythosenearboundary.maetal.proposedamutation testingframework deepmutationfor dlmodels andthey argued thatthesamplesneartheboundaryofdlmodelsaremorelikelytobeaffectedbythemutationoperators .kimetal.
proposed twometricslsa likelihood basedsurpriseadequacy anddsa distance basedsurpriseadequacy tomeasurehowclosetothe boundary between different classes the test samples are.
althoughthe mostboundary basedtechniquesare effectiveon selectinginformativesamplesinabovescenarios theyonlycapturethe distance relationship of test samples with the boundaries of dl models and fail to take the boundary distribution information e.g.
multipleboundariesfor multipleclassification inthe wholeunlabeledtestsetintoaccount.thismaygiverisetoimbalanceselection among boundaries.
for example figure shows an example of dl models with multiple classification i.e.
three classes class class2 diamondsolid andclass3 trianglesolid .thesolidlinesshowdecisionboundaries of the original model mo while the dashed lines show boundaries of the retrained model mr. the shapes diamondsolid and trianglesolid of samples denotetheactuallabels andthecolors red blue andgreen denote thepredictedlabelsby mo.thesamples diamondsolidand trianglesolidarecorrectly predicted2 while the others e.g.
and are wrong predicted.
as shown in figure we obtain the following observations for dl models with multiple classification they have multiple boundaries each of which may have the corresponding boundary area i.e.
the light grey ellipse areas i ii and iiirepresent the boundary areas of mo between class and class and and class and respectively.
with the different distribution of samples near it.
obviously the samples in iii are more far i.e.
with longer distances from thethe original boundary than the samples in i and ii.
generally samples near the boundary are more sensitive to thechangeoftheboundary asshowninthisexample the retrained boundary only crosses the boundary area of theoriginal boundary and hence more useful to reconstruct the retrained boundary.
however thedifferentdistributionofmultipleboundaries can not be ignored.
if we only consider the distance metric is correctly predicted as red by mo diamondsolidblue and trianglesolidgreen respectively 412original boundary between class 3retrained boundary between class original boundary between class 2retrained boundarybetween class 2originalboundarybetweenclass retrained boundary between class 3class 1class class 2iii iii figure an example of multiple classification dl models with multiple boundaries.
the samples from test data are classified into three classes by the original model mowith originalboundaries solidlines andbytheretrainedmodel mrwith retrained boundaries dashed lines .
wemayoverselectsamplesfromiandii withshorterdistance and lose sight of the samples from iii with longer distance which are also essential to reconstruct the boundary between class and .
based on the above three points our basic idea is considering retraining dl models as the reconstruction of multiple boundaries basedonmultipleclassificationandtryingtoselectsamplesevenly from multiple boundary areas to make sure enough samples for each boundary reconstruction.
.
multiple boundary clustering and prioritization basedonthebasicideamentionedabove weproposeouralgorithm namedmultiple boundary clustering and prioritization mcp for short detailed presented in algorithm .
given the originaldl model mo an unlabeled test dataset dt the label set cd c1 c2 ... cm and the labeling effort e lessmuch dt mcp comprises the following three steps step output probabilities extraction.
in this step we run the modelmoagainst the whole test dataset dtwithout knowing the actual labels of samples line .
for each sam ple sindt we obtain a sequence of output probabilities ys ys ysm which represents the probability distribution ofsfor themclasses in the given label set cd.
these output probabilities are the basis of the following clustering and prioritization.
step boundaryareaclustering.dlmodelswithmultipleclassifi cationhavemultipleboundaries.inthisstep wetrytocluster thesamplesof dtintodifferentboundaryareas whichwe consider as the confusion areas of two classes.
based on outputprobabilities foreachsample sindt wecalculatethealgorithm m ultiple boundary clustering and prioritization mcp mo dt cd e input the dl model mo an unlabeled test dataset dt the label setcd c1 c2 ... cm the labeling effort e lessmuch dt output the selected subset de dtwith de e 1initialize de eused 2initialize priority queues qij i j m i j whose elements angbracketlefts ps angbracketrighthave twofi elds the sample sand the priority ps q qij i j m i j 3fors dtdo step output probabilities extraction 4runmowithsand get output probabilities ys ys ysm 5fors dtdo step boundary area clustering 6determine which boundary area sbelongs to by calculating the first second predicted class i jofswith the max secondmax probabilities i argmax1 k m ys k j argmax1 k m k i ys k 7calculate the boundary priority ps ps ys i ys j 8insertsintoqijwith its priority ps insert qij angbracketlefts ps angbracketright 9whileeused edo step uniform priority selection 10initial a temporary priority queue qt 11forqij qdo pop the sample with minimal priority from qij angbracketlefts ps angbracketright extractmin qij insert the sample into qt insert qt angbracketlefts ps angbracketright 14if qt e eused then add all sample in qtintode de de s angbracketlefts ps angbracketright qt eused eused qt 17else whileeused edo pop the sample with minimal priority from qt angbracketlefts ps angbracketright extractmin qt add the sample into de de de s eused eused 22returnde first second predictedclass i j withthemax secondmax outputprobabilities andclusterthesample sintotheboundary area between class iandj line .
our assumption isthat moreclosethesetwoprobabilitiesare moreclose s istotheboundaryofclass iandj.therefore wecalculate the priority pas the ratio of the probability ys iof thefirst classito the probability ys jof the second class j line .
forconvenienceofthefollowingprioritization weusethe priorityqueues qijtosorttheresultsofclustering line8 which contains twofi elds the sample sand the priority ps.
step uniformpriorityselection.afterobtainingthepriorityqueues qijfor multiple boundaryareas with the given labelingefforte wetrytoevenlyselectsamplesfromeach qijtomake sure that for each boundary there are enough selected samples to reconstruct the boundary.
specifically our approach comprisesroundsofselection.innormalrounds weinitialatemporarypriorityqueue qttosortthesamplewithminimal prioritypoppedofffromnon empty qij i.e.
evenlyselect the most suitable sample from each non empty boundary 413table datasets and dnn models dataset dnn model layers parameters training acc.
testing acc.
mnist convnet .
.
cifar10 densenet .
.
svhn rcnn .
.
area line12 andaddthemintotheselectedsubset de when the labeling effort is enough line .
in the last round we will add the samples with the minimal priority in the temporary priority queue qt one by one until the labeling effort is used up line .
to sum up mcp selects almost equal sizes of ranked samples from each boundary area into the selected subset de which totals to the given labeling effort de e andfi nally returns deas the subset to be labeled.
experiment setup inthissection wepresenttheexperimentalsetuptoevaluatethe performanceofmcp.ourexperimentrunsonaubuntu18.04server with8gpucores teslav100sxm232gb .therelatedframeworks ofdeeplearningincludepython3.
.
numpy1.
.
tensorflow gpu .
.
keras2.
.
cleverhans3.
.
andopencv2.
.weprovide the replication package including detailed description of studied methods and source code online see section .
.
studied datasets and models we conduct the experiments on three widely used datasets mnist cifar10 andsvhn.mnistisadatasetof28 28grayscaleimages forhandwrittendigit from0to9 imagerecognition whichcontains60 000trainingdataand10 000testdata .cifar10dataset consistsof60 000trainingimagesand10 000testimages 32x32 color images in classes e.g.
airplane bird and ship with images per class .
svhn street view house numbers isobtainedfromhousenumbersextractedingooglestreetview images .
it can be seen as similar infl avor to mnist i.e.
the images are of small cropped digits from to but comes from a significantly harder and real world problem i.e.
recognizing digits and numbers in natural scene images .
in detail it contains digits for training and digits for testing.
forabovethreedatasets weapplydlmodelstructuresandtrain dl models with competitive accuracies as shown in table .
specifically for mnist we train afi ve layer convolutional neural network convnet model which is applied in with .
training accuracy and .
testing accuracy for cifar10 we apply the advanced dl network named densenet and train a modelwith95.
trainingaccuracyand94.
testingaccuracy for svhn we apply the advanced dl network named rcnn andtrainamodelwith99.
trainingaccuracyand96.
testing accuracy.
.
data simulation we recall the studied testing scenario the distributions of training dataset and testing dataset share common components buthave non ignorable differences.
in the experiment we leveragetable2 thedescriptionsofsyntheticoperatorsandthesimulated accuracy of simulated datasets operator descriptionsimulated accuracy mnist cifar10 svhn rotate rotate with a certain angle .
.
.
translate translate with several pixels .
.
.
shear horizontal shearing .
.
.
brightness changing brightness .
.
.
contrast changing contrast .
.
.
scale zoom out or zoom in .
.
.
fgsm fast gradient sign method .
.
.
jsma jacobian based saliency map attack .
.
.
c w carlini wagner .
.
.
bim a basic iterative method a .
.
.
bim b basic iterative method b .
.
.
a original b rotate c translate d shear e brightness f contrast g scale h fgsm i jsma j c w k bim a l bim b figure one original image from mnist and syntheticimages by our image synthetic operators two strategies image transformation and adversarial attack ongeneratingrealisticsyntheticimagestosimulatethetesting context with different distribution.
table shows the short descriptionsofthesesyntheticoperatorsusedinourexperiment.
first weapplysixdifferentrealisticimagetransformations rotate translate shear brightness contrast andscale tomimicdifferent real worldphenomena.rotate translate shear andscaleareall different types of affine transformations3 which are often applied inimageprocessingtofixdistortionsduetocameraanglevariations .besides brightnessandcontrastarebothlineartransformations.
our implementation of these transformations is based on opencv4.second weapplyfivewidelystudiedattackstrategies fgsm jsma c w bim a andbim b whoseimplementation is based on cleverhans .
figure shows an example of the applicationofsyntheticoperators oneoriginalimagelabelledas and synthetic images generated by synthetic operators respectively.
we apply synthetic operators on original testing samples to generate synthetic testing samples.
we control the distribution diversitybyadding20 realisticsyntheticsamplesgeneratedfrom each synthetic operator to replace the original samples.
for example for mnist and cifar10 with testing samples and the syntheticoperatorfgsm werandomlyextract2000 originalsamples generate2000syntheticsamplesbyfgsm and 414addthemintomnistandcifar10toreplace2000originalones.
for clarity the hybrid testing datasets are named fgsm mnist andfgsm cifar10 respectively.tosumup weconstruct33 synthetic operators original testing datasets hybrid testing dataset to simulate the testing context whose accuracies of the original models are shown in the last three columns of table .
.
baseline methods givenanunlabeledtestingdatasetandthelimitedlabelingeffort weaimtoselectasubsettolabelandretraintheoriginaldlmodel.
we want to study whether our proposed method mcp which is based on multiple boundary areas of dl models can perform well intheretrainingofdlmodels.wechoosethefollowingbaseline methodsforcomparison whicharefromthefieldofaisoftware testing and traditional ai research.
lsa dsa.recently kimetal.proposedanoveltestadequacy criterioncalledsadl surpriseadequacyfordeeplearning systems for testing dl systems.
in their study lsa likelihood based surpriseadequacy anddsa distancebasedsurpriseadequacy aretwo different metrics tomeasure sa surprise adequacy which support a sense ofhow close to the class boundary the new inputs are .
thehigher the value of sa is the corresponding test case is moresurprisetothedlsystemsundertesting.weintroduce lsa dsa as the baseline selection method which select the subset of test cases with higher corresponding sas.
aal.activelearningisaspecialcaseofmachinelearningto deal with new data of different distribution which is widely appliedinmanyapplicationdomains suchascomputervision.
li et al.
presented a novel approach aal adaptive activelearning for computer vision that combines an informationdensitymeasureandamostuncertaintymeasuretogethertoselecteffectiveinstancestolabelfortraining image classifications ceaselessly.
ces.
li et al.
proposed an efficient test selection methodbased on conditioning to estimate new models precisiononoperationalenvironments.theirresultsshowthat ces crossentropy based sampling estimator consistently outperformsothercomparedmethodsinallthestudiedcases.
sowechoosethemethodces whichisalsoappliedinthe circumstance where the testing dataset is unlabeled as a baseline for comparison.
srs.duringthesubsetselection the simplerandomsampling srs isnaturallyconsideredasabaselinemethod.srsdraws samples randomly from the testing dataset.
tosumup wecomparetheperformancesofourmethodmcp with following baseline methods lsa dsa aal ces and srs in retrainingdlmodels.thankstothegenerosityoftheauthorsof theabovebaselinemethods thesecodesareavailable online and can be obtained easily.
in order to ensure the accuracy of experimental repetition we directly invoke these original codes.
duetothespacelimitation weomitthedetailsoftheimplementationofbaselinemethodshere andgivethedetaileddescription and source code in the replication package see section .
figure the framework of retraining settings .
retraining settings figure3showstheframeworkofretrainingsettings whichcomprises the following three steps.
sampling.
asmentionedinsection3.
wefocusonthescenario withthelimitedlabelingeffort e i.e.
onlytheverysmallpartoftest datadt5couldbelabelled e lessmuch dt .intheexperiment wefocus onfourrepresentativesamplingratios1 and10 tocheckthe performance of mcp with labeling percent not larger than .
specifically for each simulated test dataset we apply mcp and the baseline methods to sample and inputs from it.
for example when focusing on the ratio of minist we select a subsetwiththe1 size oftestingdatasetaccording to mcp and other baseline methods.
retraining.
following the retraining process in sadl we retrainthedlmodelsbasedonthenewsampledsubsetwithan other epochs.
mao et al.
have observed that retraining on the combination of the training set and the new sampled subsetmay performs badly because a few new sampled data is easy tobe overwhelmed by other old training data.
besides in order to eliminatetherandomnessoftheselectionmethods e.g.
cesand srs andthetrainingprocess werepeattheprocessofretraining5 times and report retrained accuracy improvements.
evaluation.
as stated in the problem in section .
to evaluate theperformanceofourmcpmethodandotherbaselinemethods we focus on the accuracy improvement of the retrained model mr againsttheoriginalmodel moonthe33simulatedtestingdatasets dt.
thehigher improved accuracymeans the betterperformance of the test sample selection methods.
.
research questions wetrytoevaluatetheperformanceofmcponeffectivenessand efficiency.
three research questions are listed as follows.
rq1 overalleffectiveness whethermcpexceedsstateof the art test sample selection techniques in the overall datasets?
rq2 effectivenessunderdifferenttestingcontexts whether mcp exceeds state of the art test sample selection techniques under different testing contexts generated by different synthetic operators?
rq3 efficiency ismcpmoreefficientthanstate of theart test sample selection techniques?
5usually whenpeoplesaytestdata thesamplesarealreadylabeled.inthispaper dt indicates the whole set of unseen data or future data in the testing environment.
.
analysis methods to quantify the differences of accuracy performance wefi rst apply p valuesofthewilcoxonrank sumtest tocheckwhetherthe differences between mcp and baseline methods are statistically significant at the significance level of .
.
second we introduce another statistical analysis method cliff s delta .themagnitudeofdifferencesisusuallyassessedbythe thresholds .147asnegligible .
.330assmall .
.
as medium and .
as large.
third we introduce w t l to denote our method performing better than equal to or worse than the baseline methods.
for our method we mark it as a win if our method performs significantly better according to the wilcoxon rank sum test p value is less than .
and the magnitude of the difference betweentwomethodsisnon negligibleaccordingtocliff sdelta .
.
in contrast we mark it as a lose .
otherwise we mark it as a tie .
experiment results in this section we specify the results of our rqs along with our motivations andfindings.
.
rq1 overall effectiveness motivation and approach.
our scenario is to sample a subset from the unlabeled testing set by a certain ratio and then manually labeltheselectedsamplesforretraining.wewanttocheckwhether mcp can work well when the sampling ratio is very small to save label costs which is the core concern of our method.
to achieve this aim we compare the performance of mcp with the baseline methods in the overall33 simulated datasets under four sampling ratios and .
specifically in each simulated dataset we use these six studied methods to sample the simulated datasetsaccordingtothecertainsamplingratios andemploythe sampled subset to retrain the original model.
we repeat the process of retraining5 times and report accuracyimprovements by comparing original models and retrained models.
results.
figure gives an overview of mcp compared with five baseline methods.
the boxplots present the distribution of average accuracy improvements on all simulated datasets grouped by six methods under four sampling ratios and .
the green points in the boxplots represent the mean values while the black lines represent the median values.
in each subfigure the sixstudiedmethodsarerankedbythemedianvalues.thecolors of boxplots indicate the comparison results based on p values and cliff s delta see section .6for detail takingmcp denoted with red as the benchmark blue indicates that mcp is considerably better than this method i.e.
p .
and .
red indicates that there is no considerably difference between mcp and thismethod i.e.
p .
or .
and purple indicates that mcp is considerably worse than this method i.e.
p .
and .
.
figure shows that under four sampling ratios mcp has the best performance of accuracy improvements i.e.
with the highestmedianandmeanvalues.besides under3 and10 out of comparisons mcp is considerably better than all the other baseline methods.
mcp srs ces dsa aal lsaacc improvement a performance with sampling mcp dsa ces aalsrs lsaacc improvement b performance with sampling mcp aaldsa ces srs lsaacc improvement c performance with sampling mcp aaldsa ces srs lsaacc improvement d performance with sampling figure the boxplots of accuracy improvements of mcp and5baselinemethodsunder1 and10 sampling.blue color indicates that mcp is considerably better thanthis method when considering p values and cliff s delta while red color indicates that there is no considerably difference between mcp and this method.
table lists the detailed results of accuracy improvements with thetop4methodsrankedbymedianvalues.intable3 numbers are means of accuracy improvements and the best numbers are highlighted in bold.
in particular an entry with a gray background indicatesthatmcpwinswhencomparedwiththebaselinemodel according to p values p .
and cliff s delta .
.
in addition table3providesaveragerank ar ofeachmethod overallsimulateddatasets whichwellreflectshowthecorresponding method outperforms others by eliminating the influence of extremevalues.the lastlinew t lreportsthenumbers ofsimulated datasets on which mcp wins ties loses when compared with baseline models.
from table we obtain the following observations a consideringtheaveragevalues mcphasthebestaccuracyimprovementsamongthetop4methodsunderallthelabelingefforts.the line shows the relative improvement rates a b b of average accuracy improvements whencomparing mcp a with theother baselines b .
as can be seen the values range from to b highlightedbyboldfont mcpachievesthebestperformanceon nolessthan20 outof33 datasetsunderallfourlabellingefforts c in the view of ar mcp has .
.
.
and .
under and10 labellingefforts respectively whicharealso the best among the top methods.
the lower ar values show abetter applicability of mcp on different simulated datasets with 416table the overall results of average accuracy improvements in the top methods sample ratio datasetmethodmcp dsa ces srs mcp dsa ces aal mcp dsa ces aal mcp dsa ces aal rotate mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
translate mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
shear mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
brightness mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
contrast mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
scale mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rotate cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
translate cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
shear cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
brightness cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
contrast cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
scale cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rotate svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
translate svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
shear svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
brightness svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
contrast svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
scale svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fgsm mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jsma mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bim a mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bim b mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
c w mnist .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fgsm cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jsma cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bim a cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bim b cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
c w cifar10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fgsm svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jsma svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bim a svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bim b svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
c w svhn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ar1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w t l the baseline methods d the w t l result also reports that mcp performsconsiderablybetterthanalltheotherbaselinemethodson the most of simulated datasets.
all above results indicate that mcp can significantly increase the overall effectiveness of retraining dl models.
answer to rq1 we observe that mcp performs significantly better than the other baseline methods when considering the overall effectiveness of retraining dl models.
.
rq2 effectiveness under different testing contexts motivation and approach.
as mentioned in section .
we employ synthetic operators to simulate the testing contexts.
image transformation can mimic the real world phenomena e.g.
the image with cameralens distortions could besimulated by the rotate operator andtheimagewithdifferentbackgroundconditionscould be simulated by the brightness operator.
similarly adversarialattacksmimicadeliberateviciousattack forthepurposeofattackingthesystembyelaboratelyconstructing the input which is only slightly different from the original input.
in this rq we want to observe the performance of all six methodsunder different kind of synthetic samples generated from syntheticoperators andcheckwhethermcpismoreeffectivethantheother methods.
results.
figure shows the detailed results of accuracy improvements under synthetic operators.
take rotate as an example wegatheralltheresultsofthreedataset rotate mnist rotate cifar10 and rotate svhn generated by the rotate operator then compare all the average accuracy improvements of6 compared methods.
similar to figure the white points in the boxplotsrepresentthemeanvalues whiletheblacklinesrepresent themedianvalues blueindicatesourmethodisconsiderablybetterthanthecomparedmethodwhenconsideringon p valuesand cliff sdelta redmeans therearenoconsiderably differences and purple indicates that mcp is considerably worse.
as can be seen in figure we observe that a mcp has the best mean values and the median values in most i.e.
out of testing contexts b the color of boxplots shows that mcp performssignificantlybetterthanallthefivebaselinemethodsin most i.e.
6outof11 testingcontexts i.e.
alltheothermethodare coloredasblue c thoughmcpisnotthebestmethodwiththe bim boperators mcpisstillcomparabletotheothermethods i.e.
therearenoconsiderablydifferencesbetweenmcpandtheother baseline methods with red colors.
rotate translate shear brightness contrast scale mcplsadsaaalcessrsmcplsadsaaalcessrsmcplsadsaaalcessrsmcplsadsaaalcessrsmcplsadsaaalcessrsmcplsadsaaalcessrs 50510acc improvement a performance under image transformation fgsm jsma c w bim a bim b mcplsadsaaalcessrs mcplsadsaaalcessrs mcplsadsaaalcessrs mcplsadsaaalcessrs mcplsadsaaalcessrs 250acc improvement b performance under adversarial attack figure results of accuracy improvements under different simulated testing contexts generated by synthetic operators.
answer to rq2 we observe that in most testing contexts mcp has the best performance of mean and median values and is considerably better than all other methods.
.
rq3 efficiency motivation and approach.
in the previous rqs we have observed that the subset selected by mcp outperforms all the compared baseline methods considering the accuracy improvementsafter retraining.
as selecting the subset of testing dataset is time consuming in this rq we want to check whether our method has obvious advantages in efficiency i.e.
with lower time cost .
result.table4showsthetotaltimecostsofall6methodswhen sampling10 6imagesfrom11simulateddatasetsgeneratedfrom three original testing dataset from mnist cifar10 and svhn respectively.forexample showsthatthetimecostis28seconds whenwesample10 inputsfromallthe11mnistsimulatedtest datasetsbyourproposedmethodmcp.fromtable4 weobserve that with the exception of the random method srs other methods cost several times at least times just as .
or even hundreds of times more time than our method.
answertorq3 ourmethodisnotonlyeffectiveinretraining dlmodelswithhigheraccuracyimprovements butalsohasadvantages in time costs.
6as10 isthemaxsamplingratioweconsiderinthispaper wemeasuretheefficiency with sampling.table the time costs second of all methods when sampling from all simulated datasets dataset mcplsadsaaalces srs mnist 629na .
cifar10 21214783na170 .
svhn 102na1 .
when the time cost is larger than hours we denote it as na.
discussion we further discuss the aims and results of our method in the following three aspects.
.
performance under different original accuracies asshown intable ourstudied modelsperform on33simulated testing dataset with different original accuracy from .
to .
.
in this subsection we try to evaluate the effects of original accuracies on the performance of our studied methods.
we sort all the simulated datasets according to the original accuracies and group theminto four parts thefirst quarter thesecondquarter the third quarter and the last quarter e.g.
thefi rst quarter contains the of datasets with the lowest original accuracies.
figure shows the detailed results of accuracy improvements of thesixstudiedmethodsunderfouroriginalaccuracyclasses.similar to figure and in figure red means there are no considerably differences and blue indicates our method is considerably better than the compared method.
fromfigure6 weobservethatourmethodcanwinalltheother methods in quarters the second the third and the last.
in the first quarter our method wins two of the baseline methods and do first quarter second quarter third quarter last quarter mcplsadsaaalcessrs mcplsadsaaalcessrs mcplsadsaaalcessrs mcplsadsaaalcessrs 1001020acc improvement figure6 resultsofaccuracyimprovementsunderfouroriginal accuracy classes not lose to any method.
in addition considering the mean value whitepointsinthebox ormedianvalue blacklinesinthebox our method still ranksfi rst in all quarters.
.
retraining vs. finding faults inthepoint of viewoftraditionalsoftwareengineering testingcanbeconsideredasameanstoachievereliability .themaincontent oftestingistogenerateorselecterror triggeringinputs i.e.
test cases tofindthefaultsofsoftware.ascanbeseen findingfaults is stillone of the coretargets of current dltesting .
in a traditional software fi nding faults are actionable for the developers of the software since they are actually helpful to improve the reliability of the software for each fault the developers canlocatethe buggy code and fixit.
compared with the traditional software dl models follow a data driven programming paradigm wherethecoreunderlyinglogicofdlmodelsisobtainedviathe distribution of training data under the neural network architecture e.g.
layer numbers and neuron numbers .
the fundamental differences between dl models and traditional software have weakened the actionability offi nding faults in the following two respects difficultto locatethefaultsofdlmodels.thecomplexstructure of dl models obviously diminishes the probability to locate the buggy part of the neural network architecture in dl models.
to our knowledge there are no obvious connectionsbetweentheerroroutputsandsomespecificareasof the neural network.
difficult to fixthe faults of dl models.
as a result of the statistical nature dl models can not guarantee to predictcorrectly when dealing with a single input which meansalthough the developers know the bugs they may fail to retrain dl models tofi x it.
the above limitations bring us back to the angel of statistics on dl testing e.g.
retraining dl models which may be more actionable to improve the reliability of dl models.
.
similarities and differences with active learning herewediscussthesimilaritiesanddifferencesbetweenourmethod mcp in dl testing and ai methods in active learning.
both methods aim to save labeling costs and select the unlabeled instances withhigherprioritytolabel whichassumeaninstancewithhigher priority is more effective to train dl models.
itisworthemphasizingthatourmethodmcphassomeinherent differences with methods in active learning application scenario.
active learning is usually focusing ontrainingscenarios inwhichthereisapoolofunlabeled instances and active learning methods select samples totrain dl models.
meanwhile our method focuses on testingscenarios inwhichtheunlabeledtestdatasetfromthe testing contexts shares common components but have nonignorable differences with training dataset used to train dl models.
training frequency.
in the training scenario developers couldtraindlmodelsin manyroundswith tinyincrements of labeled samples recommended by active learning methods.
on the contrary there is an obvious division of laborbetween the developers and the testers in software engineering.
our method mcp is designed as one timefeedback technologyfor testers toreport enoughefficient samplesto developers for retraining.
threats to validity four aspects may become the threats to validity of this paper.
first the selection of dl models could be a threat to validity.
in thispaper wetrytoalleviatethisissuebyapplyingthreefamousdl network structures and training three dl models with competitive prediction accuracies.
second selection of subject dataset could be a threat to validity.
we use three famous multiple classification datasets and have generated as many simulated datasets as possible eliminating some contingency.inthefuturewewillevaluateourtechnologyonmany different kinds of image datasets as well as some natural language datasets in the form of speechand text.
third the simulation of data distribution shift could also be a threattovalidity.bothadversarialexamplesandtransformation generated examples in the synthesized test cases may to some degree representrobustnessissues.moresimulationmethodsare needed to verify our results.
lastbutveryimportantthreatishowwecanguaranteethatour synthetic dataset does not deviate much from the original training dataset.
we have manually checked the synthetic datasets.
for each simulated dataset containing synthetic images we have manually checked pictures to affirm that human can recognize the synthetic images the same as the original one.
related work in this section we propose three aspects of the related work.
testingdlsoftware.
more and more researchers are focusing on the testingof well trained dl software artifacts.many conventionalconceptsortechniquesinnormalsoftwaretestinghavebeen tuned and applied to dl software testing.
419in deepxplore pei et al.fi rstly proposed neuron activation coveragetoevaluatetestadequacyindlsoftware.indeepgauge ma et al.
introduced kinds of multi granularity coverage criteria which are deemed to reflect behaviors of dl software in finer granularity.
in deepct ma et al.
proposed a combinatorialtesting ct coverage guidedtestgeneration techniquewhich contains several ct criteria formed for dl systems.
besides indeeptest metamorphictestingwhichcanrelieve oracle problem has been applied in dl software to generate novel test cases.
deeproad uses the technology of gan to generate self drivingimagesinvariousweathercircumstancesfordltesting automatically.
deephunter is a fuzzing testing framework guided by coverage which can generate abundant test cases for detecting potential faults of general service dl software.
transre pair combines mutation with metamorphic testing to test and renovate the consistency of machine translation software.
new perspectives and newfi ndings have also been applied to dl software testing.
deepbillboard is a systematic real world testingapproachforpracticalself drivingsystemsbygeneratingre silient and robust printable adversarial billboards.
deepimportance isasystematictestingframeworkcontainingatestadequacy criterion motivated by importances for dl systems which can establish a functional layer wise understanding of the dl systemcomponents and assess the semantic diversity of a test suite.
in deepinspect chen et al.
presented a testing method to detect category based confusion and bias errors not per image violations in dnn driven imageclassification software automatically.
other works about therealistic failure repair and deployment of dl software which are related to platforms frameworks andmodelparameters structures arealsopublishedbyresearchers in this area.
improvingdlsoftware.
theexistingstudiesaboutimproving dl software are mainly from the aspects of training dataset refinement and weight values of dl software.
to alleviate the lackingof representative training data researchers proposed some data augmentation anddatagenerationtechniques .withhelpof these technologies mode can generateand select new traininginputsbyconductingmodelstatedifferentialanalysis.gaoet al.proposedatechnology thatre purposesthetechnologyof fuzzing testing based on mutation to augment the training data of dnns with the aim of strengthening the robustness of dl models.
in addition to augmenting training data some researchers directly updatetheundesirableweightvalues.apricot canimprovedl modelsiterativelybyanovelweight adaptationapproachwhich generates many reduced dl models rdlm providing insights on theadjustmentmagnitudeanddirectionoftheweightstohandle the misclassified test cases.
activelearning.
aimingtoreducelabelingeffortintraininga goodclassifier the techniqueofactivelearninghasbeenappliedin controlling the labeling process in deep learning.
as traditional active learning are ineffective when applied to deeplearninginthesettingofbatchtraining ozanetal.
definedthisproblemascore setselection i.e.
choosingasetofpointscompetitive for the remaining points.
their experiments show that the proposed approach obviously surpasses existing approachesin image classification experiments by a wide margin.
but core set selection would be prohibitively expensive to apply in deeplearning because they depend on feature representations that need to be learned.
cody et al.
showed that they could greatly improvethecomputationalefficiencybyusingasmallproxymodel to perform data selection for active learning.
experimental results show that this selection via proxy svp approach can give an orderofmagnitudeimprovementindataselectionruntimewithout harming thefi nal accuracy of the target.
william et al.
comparedensemble based approacheswithmonte carlodropout and geometric approaches for active learning with cnn classifiers and high dimensionaldata.
theyfound thatensemble basedmethodsshow better performances and lead to more correct predictive uncertainties whicharethebasisformanyactivelearningalgorithms.
conclusion based on the statistically orientated nature the quality of dl models is evaluated statistically on the performance of all testing inputs collected from a specific application context.
the lower thanexpected performance of dl models is inherently the result of differentdistributionsofthetrainingdataandthetestingdata.oneofthemaintasksoftestersfordlmodelsistocollectusefulinputs fromthetestingcontextandreportthemtothedevelopersforretraining.the factof expensivelabelingcost greatlyweakens thepossibility of employing enough labeled inputs to retrain the dl models.
to overcome it we propose multiple boundary clustering and prioritization mcp a technique to cluster the samples into the boundaryareas of multipleboundaries fordl modelsand specify the priority to select samples evenly from all boundary areas to makesureenoughusefulsamplesforeachboundaryreconstruction.
the resultsof the experimentsdemonstrate that ourmethod mcp is very effective in the retraining of dl model.
at the same time the time costs of mcp are also lower than baseline methods.
in the future we will explore whether our mcp works well insuchtestingscenario thedistributionsoftrainingdatasetand testing unseen datasetdifferalot e.g.
theaccuracyofthednn model under retraining on the testing dataset is very low.
we want to know whether the low accuracy will affect the reliability of predictive probabilities and then affect the selection of the testing subset.
repeatability weprovidealldatasetsandpythoncodeusedtoconductthisstudy at