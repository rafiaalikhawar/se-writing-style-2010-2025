code comment inconsistency detection and rectification using a large language model guoping rong nanjing university nanjing china ronggp nju.edu.cnyongda yu nanjing university nanjing china yuyongda smail.nju.edu.cnsong liu nanjing university nanjing china smail.nju.edu.cnxin tan nanjing university nanjing china smail.nju.edu.cn tianyi zhang nanjing university nanjing china smail.nju.edu.cnhaifeng shen southern cross university gold coast australia haifeng.shen scu.edu.aujidong hu zhongxing telecom equipment xi an china hi.jidong zte.com.cn abstract comments are widely used in source code.
if a comment is consistent with the code snippet it intends to annotate it would aid code comprehension.
otherwise code comment inconsistency cci is not only detrimental to the understanding of code but more importantly it would negatively impact the development testing and maintenance of software.
to tackle this issue existing research has been primarily focused on detecting inconsistencies with varied performance.
it is evident that detection alone does not solve the problem it merely paves the way for solving it.
a complete solution requires detecting inconsistencies and more importantly rectifying them by amending comments.
however this type of work is scarce.
in this paper we contribute c4rllama a fine tuned large language model based on the open source codellama.
it not only has the ability to rectify inconsistencies by correcting relevant comment content but also outperforms state of the art approaches in detecting inconsistencies.
experiments with various datasets confirm that c4rllama consistently surpasses both post hoc andjust in time cci detection approaches.
more importantly c4rllama outperforms substantially the only known cci rectification approach in terms of multiple performance metrics.
to further examine c4rllama s efficacy in rectifying inconsistencies we conducted a manual evaluation and the results showed that the percentage of correct comment updates by c4rllama was .
and .
injust in time andpost hoc respectively implying c4rllama s real potential in practical use.
index terms code comment inconsistencies detection rectification large language model i. i ntroduction most code related activities need program understanding.
for example when updating code maintainers need to understand the logic of the original code.
when developing code in teams team members need to understand each other s code.
the role of comments which are widely present in code is to assist readers in better understanding the code .
if a comment is consistent with the code snippet it intends to annotate it serves its purpose well.
otherwise code comment inconsistency cci would occur which is detrimental to the understanding of code but more importantly it negatively corresponding authors yongda yu and jidong hu.impacts the development testing and maintenance of software.
common causes of cci include developers skill or time constraints developers forgetting to update the comments when making changes to the code and developers failing to complete modify the code in accordance with the requirement specifications presented in the comment .
table i lists some examples of cci.
it is worth noting that these examples only intend to show different forms of cci.
they may seem minor however inconsistencies in real software projects can contain longer comments and more complex code structures creating barriers to code understanding and further reducing the understandability and maintainability of code .
developers often rely on comments to understand the main functionality and interface specifications of external code libraries.
cci can lead to misunderstandings about the code s functionality and introduce defects during subsequent development .
the cost of fixing such defects is often high because they require developers to understand the source code but they may also require costly communication with the code developers.
in a teamwork environment frequent cci can erode developers trust in comments and a lack of confidence may make developers ignore comments or even abandon updating them altogether leading to the failure of the entire code comment paradigm.
to address the cci issues existing research has been primarily focused on detecting inconsistencies using either the post hoc or the just in time approach.
the post hoc approach aims to detect cci issues in an existing code version while thejust in time approach aims to detect cci issues before they are committed.
detection techniques include rule based i.e.
with predefined rules or patterns or learningbased i.e.
extracting features by analyzing code comments and related code and then learning the differences between inconsistent code and comments in the extracted features .
rule based techniques are easy to implement however they often fail to detect implicit or semantic inconsistencies due to the limited coverage of rules.
moreover it is generallytable i cci examples code comment explanation read bandwidth to intermediate memory in gb per second.
double intermediate read gb per sec read bandwidth to intermediate memory in gb per second double intermediate write gb per sec from the name of the method it looks like it should be write instead of read .
it looks very much like when the author copied the code and comment he forgot to change the comment at the same time.
checks if one of the graphs from unsupported graph type and throws illegalargumentexception if it is.
the current unsupported types are graphs with multipleedges.
param graph1 param graph2 param g throws illegalargumentexceptiontprotected protected static void assertunsupportedgraphtypes graph g throws illegalargumentexception ... it looks like the two parameters graph1 and graph2 have been merged into parameter g in the new version of the code but the corresponding comment has not been changed which can obviously be misleading.
returns the line number in the xml data where the exception occurred.
if there is no line number known is returned.
public int getlinenr return this.linenr it s not easy to tell if the comment correctly describes the functionality and the code doesn t give the full implementation or if the new version of the code ensures that there is no return value.
but in either case the code here is inconsistent with the comment.
challenging to formulate a rule set that can cover a wide spectrum of situations and as such some inconsistencies may be misjudged or cannot be detected.
while learning based methods can detect inconsistencies that are difficult to express in rules they require a large amount of labeled data to train the models and the model performance depends heavily on the quality of the training data .
the performance of existing models varies and is generally not high.
it is evident that detecting inconsistencies alone does not solve the cci problem it merely paves the way for solving it.
a complete solution requires detecting inconsistencies and more importantly rectifying them by suggesting revisions of incorrect comments.
however research on the latter is scarce.
to the best of our knowledge there are only two pieces of published work i.e.
the studies conducted by panthaplackel et al.
and dau et al.
.
in this paper we propose c4rllama a fine tuned large language model based on the open source codellama .
c4rllama not only has the ability to rectify inconsistencies by correcting relevant comment content but also outperforms contemporary state of the art methods for detecting inconsistencies.
we have conducted extensive experiments and a manual evaluation to gauge c4rllama s effectiveness and efficacy in detecting and rectifying cci.
the main contributions of this work are as follows.
we contribute to learning based cci detection techniques by exploiting a general purpose large language model.
we design a targeted loss function for fine tuning codellama so that c4rllama can achieve improved ccidetection performance and more importantly rectify cci by amending comments.
we introduce a manual evaluation method to gauge c4rllama s efficacy in amending inconsistent comments as a complement to the experimental evaluation method that cannot consider the actual semantic meaning as it relies on text similarity based metrics.
the rest of the paper is organized as follows.
section ii introduces some related work.
section iii describes the steps to design and implement c4rllama followed by the evaluation process and results in section iv.
section v discusses some considerations in c4rllama followed by the validity risks pertinent to the study in section vi.
section vii concludes the paper with a summary of future work.
ii.
r elated work cci detection is presently a vibrant field of research with a variety of methods being proposed .
these methods predominantly fall into two categories rule based and learning based.
however the rectification of cci issues has been relatively underexplored in the literature.
a. rule based cci detection rule based methods detect inconsistencies between code and comments by employing predefined rules or patterns.
these rules may involve checking the consistency of parameter names return values exceptions and so forth between the code and its comments.
this method is relatively straightforward to implement and does not require training data.
however its limitations lie in the restricted coverage of the rules which often fail to detect certain implicit or semantic inconsistencies.
for instance tcomment is a rule based method that identifies inconsistencies by testing the specifications in javadoc comments.
to be specific it generates random tests through javadoc comments to test whether the code implementing the method is consistent with the content of the comments with the main focus being on null values or exception handling.
however it falls short of detecting whether the descriptions in the comments align with the code s other important behavior.
similarly smartcoco is a rule based method that detects inconsistencies between code and comments in smart contracts.
it employs constraint propagation and binding techniques to first extract constraints from code and comments then propagate them to variables and functions and finally verify their compliance with the binding condition.
gao et al.
utilize a rule based method to discern the relationship between comments and code.
they identify a set of rules to determine whether a code modification impacts the validity of a todo comment and if it does the comment is removed.
b. learning based cci detection learning based methods use machine learning or deep learning techniques to detect inconsistencies between code and comments.
this is achieved by utilizing neural networks orlanguage models to ascertain the consistency between code and comments.
such methods may be able to detect inconsistencies that are difficult to articulate as rules.
however they necessitate a substantial volume of annotated data for model training and the model s performance is heavily reliant on the quality of the training data while the interpretability of the model may be poor .
for example rabbi and siddik use a bi directional recurrent neural network to encode code and comments separately and then employ a fully connected layer to compute their similarity.
a binary classification loss function is used to train the model to determine whether the code and comments are consistent.
the work conducted by steiner and zhang uses a pretrained language model bert to detect code and comment inconsistencies.
it adopts a long text processing technique longformer to process code and comments that exceed the bert limit and then designs a binary loss function to train the model to determine whether the code and comments are consistent.
panthaplackel et al.
uses a deep neural network to detect whether comments need to be updated when the code changes.
it aligns the semantics of code and comments through an attention mechanism and then introduces a multicategorical loss function to train the model to determine whether comments need to be kept updated or deleted.
c. detection timing post hoc vs.just in time researchers have found that the consistency of code and comments may be closely related to code changes .
for example when the functionality or logic of the code changes the corresponding comments usually need to be updated accordingly otherwise inconsistency occurs.
the coevolutionary relationship between code and comments was explored by fluriluri et al.
who conducted experiments on three different open source software systems and found that code and comments rarely co evolve .
thus existing research investigates different timing of detecting cci problems.
for example some studies focus on the detection of cci in existing source code repositories aka post hoc detection while others focus on the detection of cci that occur immediately after code changes aka just in time detection .
these two different modes of detecting cci have little impact on the rule based approach.
however for the learning based approach as the diff information indicating the difference between the two versions can often be put into the training data it may have some positive impact on the final performance of the model.
d. pre trained models in se tasks pertinent to natural and programming languages pre trained language models can learn universal language representations through pre training on large scale corpora thereby achieving better performance on downstream tasks through fine tuning .
many researchers are trying to amalgamate natural language nl and programming language pl to achieve mutual conversion between nl and pl.
for example codebert demonstrates superior performance incode search and document generation tasks by jointly training pl and nl.
codereviewer designs targeted pre training tasks for nl and pl in code review scenarios achieving good performance in pl review and nl review comment generation.
with the development of large language models llms researchers have begun to use them to automatically generate pl or nl.
for example codellama achieves the most advanced performance in open source models in multiple code benchmark evaluations by pre training and instruction finetuning on general text and code data.
wizardcoder enhances the performance of large model code generation by applying code evol instruct technology generating higher quality datasets through self instruct and instruction finetuning .
to generate nl geng et al.
uses a small amount of context learning to make the large model codex perform better in the field of multi intent comment generation than the most advanced supervised learning methods.
the work carried out by liang and huang introduces two different types of transformer encoders that learn the nonfourier and abstract syntax tree ast structure relative position representation of the source code thereby improving code semantics and syntax learning to be superior to other deep learning based models from multiple metrics.
in the field of code review large models also have played a role.
for example lu et al.
use large language models and parameter efficient fine tuning peft technology to automate the code review process framework maintaining a fairly high code review performance while reducing time and space costs.
in essence the majority of the research mentioned above involves a unidirectional transformation between pl and nl either converting pl into nl or vice versa.
the focus of our work is on the matching problem of nl and pl followed by rectifying inconsistent comments should the cciissue exist which is significantly different from the aforementioned studies.
in this regard we have found a limited number of studies.
the work conducted by panthaplackel et al.
uses gru which is not a pre trained model to detect and resolve cci.
owing to temporal constraints this work did not employ large language models such as llama and gpt.
another work is docchecker which uses a pre trained language model unixcoder to detect and resolve cci.
however docchecker only deals with the summary information which is merely one of many types of code comment information.
iii.
m ethodology in this section we will detail the methodology for building c4rllama .
as shown in fig.
the process mainly consists of constructing a large model training dataset defining optimization tasks for cci detection and rectification and implementing c4rllama by using low parameter finetuning methods to fine tune the pre trained base large language model.codes comments dataset chain of thought cot detection revision detection task classification revision task generation optimization tasks specialized loss function label smoothing technique codellama c4rllamafine tune lora construct define training dataset post hoc just in time codes comments train constructing the training dataset fine tuning codellama defining optimization tasks fig.
.
the process of constructing c4rllama a. problem statement forcci we adhere to a concept that has been consistently employed across multiple studies .
the primary aim of our research is to harness the power of a large language model to understand textual data which includes both code and its corresponding comments.
this understanding enables us to identify inconsistencies between the two and subsequently amend the comments to rectify these inconsistencies.
it is important to note that the comments discussed in this paper differ from code summarization which is a one way transformation from pl to nl.
in contrast our work involves two way consistency detection assuming the existence of both code and natural language.
in many cases the content of comments interspersed within lines of code does not precisely reflect the content encapsulated in most code summaries.
in fact the code summaries in the dataset used in our study represent only one type of comment.
to facilitate a lucid and precise depiction of our approach we initially define a set of symbols as illustrated in table ii.
table ii notation definition cn comments of the nth version mn code implementation of the nth version i true false judgement result of the code comment consistency ins instructions for the detection and revision to cci issues r revision of the comments to resolve cci issues b. constructing the training dataset to ensure the validity of comparison with existing studies we also use the dataset widely applied in several studies on thecci topic .
curated from oss projects the dataset consists of data items each of which contains a pair of consecutively submitted comments cand code m denoted as c1 m c2 m which includes comment elements such as return param and summary .
based on an underlying assumption that all consistency issues will be rectified promptly a code change does not cause a consistency problem c1 c2 conversely if the change raises a consistency problem c1 c2.
however to match the input data format requirements of codellama we need to do some preprocessing of the data.
first we consider adopting chain of thought cot in the training process.
cot has been proven to be effective in improving the performances of large language models .
its core principle lies in decomposing complex tasks and generating results step by step through recursion thus avoiding overly complex reasoning processes.
to enable cot we first transform the data using a llama template shown in table iii.
an example of using the template to process the original data is shown in table iv where we present the preprocessing results for both the post hoc and the just in time modes respectively.
we use a zero shot prompting strategy which is more comparable to the fine tuning technology i.e.
to solve the problems without providing examples.
besides as we want to support both modes of cci detection and rectification at the same time we need to further process the dataset to construct the training dataset.
it is important to note that our processing of the training data only changes the format of the data to cater to codellama and adds nothing new to the dataset.
post hoc post hoc targets source code and comments that already exist in the repository and aims to detect whethertable iii llama template s y s system prompt system prompts for service providers to define model roles or restrictions which we do not need to use.
sys inputs the input i.e.
the prompt triggers the model to perform consistency check as well as the source co de and the corresponding comment outputs the output i.e.
the result of consistency check and the revised comment if inconsistency occurs.
table iv data processing example post hoc example sys sys is the given code consistent with the corresponding summary?
code public static jsonelement parse inputstream is string encoding throws converterexception return parse grailsioutils.tostring is encoding summary parses the given json and returns ether a jsonobject or a jsonarry the given code is inconsistent with the corresponding summary.
the corresponding summary to the given code parses the given json and returns either a jsonobject or a jsonarray just in time example sys sys did the changes cause any issues with consistency in the summary?
changes keep private static format get sample fo rmat format container format format sample format if container format null return sample format keep end delete int width container format .
width ?
format .
no value container format .
width int height container format .
height ?
format .
no value container format .
height delete end keep return sample format .
copy with c ontainer info container format .
id container format .
bitrate keep end insert cont ainer format .
i nsert end keep width keep end insert contai ner format .
insert end keep height container format .
language keep end summary derives a sample format corresponding to a given container format by combining it with sample level information obtained from a second sample format.
the given changes is consistent with the corresponding summary.
outputs inputs revision outputs data type outputs label of detection result label of detection result inputs data type data type data type there is a cci issue.
the task is thus defined as follows first the code and its comments are analyzed to determine whether there is a cci issue denoted as ins c m i i true false .
considering the dataset characteristics we chose to use m2asmandc1ascas a way to implement the judgment of whether the code modification is consistent with the original comment.
i.e.
when c1 c2 it is determined thati false meaning there is no cci issue conversely when c1 c2 it is determined that i true meaning there is one cci issue.
next when i true a revision to addresscci is required denoted as ins c m i r. in our dataset the revision result is denoted as r c2 c1 c2 .
based on the above definition we have developed python scripts to process the original dataset to construct the dataset for cci detection and revision in post hoc mode.
just in time just in time targets the code commit scenario aiming at detecting cci issues before the code is committed to a repository.
the task is defined as follows first determine whether there is a cci issue by analyzing code changes and their comments denoted as ins c diff mn mn i i true false .
in the dataset of this study we chose to use m1asmn m2as mn and c1asc as a way to implement the determination of whether a change triggers cci issues when c1 c2 it is determined that i false meaning there is no cci issue conversely when c1 c2 it is determined that i true meaning there is a cci issue.
next if there is a cci issue i.e.
i true a revision to the corresponding comment is required denoted as ins c diff mn mn i r. for this dataset the revision results in r c2 c1 c2 .
similarly we also process the original dataset and construct the dataset for training c4rllama injust in time mode.
c. defining optimization tasks to enable cot in model training and inference the tasks that come first in the cot significantly affect the results of the subsequent tasks .
for this the cci detection which is essentially a judgment task may significantly affect the performance of subsequent cci rectification tasks.
we therefore custom design a loss function to increase the weight of the judgment task to highlight the importance of the accuracy of the judgment task as follows lid li nx i 1logp xi x i where represents the weight of the code comment consistency judgment task usually takes the value of .
and p xi x i denotes the probability distribution of token xi predicted by the model based on the input sequence x i which is a commonly used method to calculate the loss in fine tuning large models.
we note that the original dataset has a small number of labeling issues which also have been confirmed by other researchers .
to mitigate the noise brought by mislabelling we employ the label smoothing lsm technique and define the following sub loss function for the cci detection task li log p i ins c m k where is the degree of smoothing of categorical labels generally taken as .
and p i ins c m denotes the probability distribution of judging the consistency of the comments and code implementation given the model directives the comments and the code implementation.
kis the length of the word list of the large language model.
the methodmitigates the data noise problem by preventing the model from giving overly deterministic answers to noisy data.
d. fine tuning codellama fine tuning can significantly enhance the ability of large language models to solve problems for specific tasks.
we also use the llama template as shown in table iii for training and inference.
large language models typically have a large number of parameters and fully fine tuning them tends to require significant computational resources .
therefore we adopt a low parameter fine tuning strategy to fine tune codellama a highly regarded model within the open source community.
codellama is built upon the llama2 model and utilizes code data for complementary pre training which has demonstrated state of the art performance across numerous code benchmark evaluations and is used as a base model for fine tuning in a variety of software engineering tasks .
specifically we chose the lora method as the low parameter fine tuning scheme for the follow up task.
lora can achieve similar results to full parameter finetuning by using only one thousandth to one ten thousandth of the original model parameter for fine tuning.
lora assumes that the parameter changes during the fine tuning phase have a low intrinsic rank allowing the parameter changes to be decomposed into the product of low rank matrices i.e.
w w0 w w0 ba.
here w represents the fine tuned model parameters w0is the set of pre trained model parameters wis the change in model parameters after fine tuning b rd r a rr k with dandk being the dimensions of the model parameters and satisfying r min d k .
during training the original pre trained parameter set w0is frozen and does not participate in gradient updates only bandaare updated.
since the number of parameters in the low rank matrices is much smaller than that of the original model matrix it allows for fine tuning the large model with a minimal number of parameters.
to further reduce the training cost and improve the convergence speed we use the lion evo lved s ign m omentum optimizer .
the finetuning of the model was performed on a100 40gb graphics cards training was performed using bf16 precision and the hyperparameters were set as shown in table v. table v training hyperparameters param lora r lora dropout learning rate batch size epoch value .
1e iv.
e valuation in this section we evaluate our approach c4rllama for cci detection and rectification against several state of the art approaches we have identified.
the entire evaluation process is depicted in fig.
aiming to answer the following two research questions rq1 how does c4rllama perform on the cci detection tasks for both the post hoc and the just in time modes?
rq2 how does c4rllama perform on the cci rectification tasks for both the post hoc and the just in time modes?
from a practical point of view a good cci solution should first perform well in detecting cci issues followed by its ability to provide accurate revision to reduce developers efforts in rectifying these cci issues.
a. experimental settings as the existing studies are predominantly on cci detection and very few have addressed cci rectification we use different evaluation settings for rq1 and rq2 in our experimental design.
cci detection task evaluation rq1 we utilized the widely used dataset from previous work as our training and testing dataset and also adopted the metrics applied in these studies for evaluating c4rllama s accuracy performance including precision recall f1 score andaccuracy.
to ensure a fair and consistent evaluation we established specific selection criteria for the benchmark approaches.
if retraining was necessary we mandated that these approaches utilize the identical dataset as employed in our study.
additionally we required the algorithm or code to be either provided or publicly accessible.
in cases where retraining was not required we opted for commercially available large language models specifically chatgpt and gpt4 recognized for their robust performance.
consequently we identified the following approaches for benchmarking.
codebert bow build on codebert this approach can support both post hoc andjust in time tasks with relatively good performance.
seq graph hybrid three new baseline approaches proposed by panthaplackel et al.
to detect cci issues which are distinguished by different encoding methods for different contents i.e.
comment ast tree respectively.
seq encodes comments using the gru network graph encodes comments using the ast tree and hybrid is computed using a multi head attention mechanism combining the above two encoders.
bert longformer steiner et al.
used bert and longformer to detect cci.
by utilizing the feature of longformer s ability to handle longer sequence lengths to reduce the information loss it achieves state of the art performance for the cci detection tasks.
docchecker proposed by dau et al.
it achieves fairly good performance by supplementary pre training based on unixcoder .
as the paper only provides performance evaluation on the post hoc task we only compare it on the post hoc task.
chatgpt gpt as the most famous large language models that can effectively accomplish a variety of software engineering tasks we use shot prompt engineering to drive them and realize the cci detection task.c4rllama benchmark approaches detection task evaluation revision task evaluation dataset choice metrics choice benchmark choice result evaluation dataset c4rllama resultresearch questions detection result precision recall f1 score accuracy revision result xmatch bleu gleu meteor sari human evaluation to revision result human evaluation successfully fixed unsuccessfully fixed fig.
.
the process of evaluate c4rllama cci rectification task evaluation rq2 as previously discussed research on the rectification of cci issues is notably sparse.
we identified only two studies that provide solutions forcci rectification.
however docchecker utilizes a different dataset in the cci rectification task leaving us with only one benchmarking study namely the work conducted by panthaplackel et al.
.
the evaluation metrics used in our study were exactly derived from panthaplackel et al.
s work i.e.
xmatch bleu gleu meteor and sari .
while the rest are commonly used metrics xmatch tests the extent to which two texts are identical.
the test dataset was constructed by following the baseline approaches i.e.
by copying consistent comments when there is no cci issue and using corrected comments generated by the rectification approach when there is a cci issue.
we use human evaluation as complementary evidence to further assess the efficacy of the revision content which is mainly done by evaluating the semantics to see whether the modification has really solved thecci issue.
there are only two types of evaluation results successfully fixed and unsuccessfully fixed according to the following criteria successfully fixed requires that the resulting new comment is semantically related to the corresponding code and does not contain explicit errors otherwise it is considered unsuccessfully fixed .
the criteria provide a clear basis to determine whether an incorrect comment had been correctly rectified which is fairly achievable by senior students majoring in software engineering.
we then randomly selected entries at a confidence level with a confidence interval range of less than to serve as the dataset for human evaluation.
b. evaluation results we present the evaluation results in this subsection.
rq1 detecting cci issues results are presented in table vi and table vii.
it is clear that our proposedtable vi results for post hoc cci detection .
approach precision recall f1 accuracy codebert bow .
.
.
.
seq .
.
.
.
graph .
.
.
.
hybrid .
.
.
.
bert .
.
.
.
longformer .
.
.
.
docchecker .
.
gpt .
turbo .
.
.
.
gpt preview .
.
.
.
codellama 7b .
.
.
.
c4rllama approach significantly outperforms various previous approaches both in post hoc and just in time modes.
specifically in post hoc mode our fine tuned 7b model achieves f1andaccuracy of .
and .
respectively which are .
and .
better than longformer the stateof the art approach.
it is worth mentioning that chatgpt and gpt4 do not seem to show an advantage in detecting cci issues which to a fair degree confirms the importance of fine tuning to improve the performance of large language models on specific tasks.
meanwhile in just in time mode our method c4rllama also performs well with f1andaccuracy reaching .
and .
respectively which are .
better than graph feature and .
better than seq feature two state of the art approaches.
chatgpt and gpt4 have very high recall s in this mode which however is dragged down by the lower precision hence their f1is only about .
and their accuracy is only around .
in particular it should be noted that to obtain better model performance in just intime mode seq graph and hybrid all add extra tags to the code diff information and these tags change the original structure and content of the diff which we suspect degradestable vii results for just in time cci detection .
approach precision recall f1 accuracy codebert bow .
.
.
.
seq .
.
.
.
graph .
.
.
.
hybrid .
.
.
.
seq features .
.
.
.
graph features .
.
.
.
hybrid features .
.
.
.
bert .
.
.
.
longformer .
.
.
.
gpt .
turbo .
.
.
.
gpt preview .
.
.
.
c4rllama .
.
.
.
c4rllama with standard diff .
.
.
.
as a variation the features are derived from their prior work which typically includes linguistic e.g.
pos tags and lexical e.g.
comment code overlap features.
we use the standard formatted diff to evaluate c4rllama .
the performance of the general purpose large language model.
therefore we processed the diff information to restore its original form and re evaluated c4rllama and the results are shown in the bottom row of table vii.
in the case of using the standard format diff the c4rllama s metrics for detecting cci injust in time mode improved significantly with its f1and accuracy remarkably reaching .
and .
respectively which largely corroborates the previous speculation that the special diff format drags down the c4rllama s performance.
the performance of an llm is largely influenced by the data in the pre training .
the justin time dataset uses a customized special format of diff while the post hoc dataset uses the regular diff format.
as codellama does not reveal the details of its pre training data we can only speculate that its training data used the regular instead of the special format of diff leading to the performance discrepancy between the two datasets.
rq2 rectifying cci issues the c4rllama approach also exhibits a distinct advantage in rectifying cci issues as depicted in table viii.
in panthaplack et al.
s work the researchers employed diverse strategies for model training.
these included a pre training strategy encompassing solely positive examples and a joint training strategy incorporating both positive and negative examples.
from the results in table viii it is evident that the c4rllama approach emerges as the superior performer in both post hoc andjustin time modes on all four metrics including bleu gleu sari andmeteor reflecting the clear advantage of the large language model in resolving cci issues.
in addition akin to cci detection the format of the diff continues to exert a significant influence on the output results as the c4rllama approach performs much better with the standard formatted diff information.
the sole metric where the c4rllama approach falls short of other approaches is xmatch.
this can be primarily attributed to the fact that large language models excel at content generation whereas the xmatch metric requiresexact content matches.
consequently it is unlikely that the c4rllama based on the large language model could exhibit an advantage in the xmatch metric even if it yields reasonable revisions.
it should be noted that metrics such as bleu gleu sari meteor and xmatch are based on the level of text similarity.
they are not the best for evaluating the efficacy of rectifying cci issues which apparently can only revise comments in the form of natural language.
in the first example in table i a correct change may take many forms such as write to write or it is to write .
in turn even if with the original incorrect form i.e.
keep read unchanged the text similarity metrics may still be very high.
this inevitably results in the need to understand and analyze the semantic meaning of the revised comment to determine whether the cci issue has been appropriately solved.
to this end we performed a human evaluation of the results created by c4rllama to rectify cci based on sampling.
the result is depicted in fig.
.
we can observe that in post hoc mode c4rllama can successfully fix of cci issues by revising the comment content while injust in time mode the success rate is .
indicating c4rllama s high potential for practical applications.
we performed a deep analysis of the cases that were determined to be unsuccessful fixes and we identified several situations that are worth noting for future work.
first there are some cases where c4rllama only rephrased the inconsistent comment.
although issues such as misspellings were properly rectified the semantic meaning was still incorrect.
this seems to imply that the 7b model we were using in this study has room for improvement in terms of correctly understanding user intent.
second some cci issues involve external information e.g.
even changes in requirements which are rather difficult to resolve.
nevertheless we believe that cci rectification still has a chance to preserve some additional considerations of the original developers at the time of developing the code with respect to other entities associated with the current block of code compared to the use of code summarization techniques for comment like generation and thus it is worthwhile to explore better solutions to the cci issue.
table viii results on cci revision .
approach xmatch bleu gleu sari meteor jointly trained seq .
.
.
.
.
jointly trained graph .
.
.
.
.
jointly trained hybrid .
.
.
.
.
pretrained seq .
.
.
.
.
pretrained graph .
.
.
.
.
pretrained hybrid .
.
.
.
.
c4rllama post hoc .
.
.
.
.
c4rllama just in time50.
.
.
.
.
c4rllamajust in time with standard diff .
.
.
.
.
we use the standard formatted diff to evaluate c4rllama .
uni00000013 uni00000015 uni00000013 uni00000017 uni00000013 uni00000019 uni00000013 uni0000001b uni00000013 uni00000014 uni00000013 uni00000013 uni00000008 uni00000003 uni00000034 uni00000058 uni00000044 uni0000004f uni0000004c uni00000057 uni0000005c uni00000003 uni00000035 uni00000044 uni00000057 uni00000048 uni00000033 uni00000052 uni00000056 uni00000057 uni00000003 uni0000004b uni00000052 uni00000046 uni0000002d uni00000058 uni00000056 uni00000057 uni00000010 uni0000004c uni00000051 uni00000010 uni00000057 uni0000004c uni00000050 uni00000048 uni00000019 uni00000018 uni00000011 uni00000013 uni00000016 uni00000018 uni00000011 uni00000013 uni00000018 uni00000018 uni00000011 uni0000001c uni00000017 uni00000017 uni00000011 uni00000014 uni00000036 uni00000058 uni00000046 uni00000046 uni00000048 uni00000056 uni00000056 uni00000049 uni00000058 uni0000004f uni0000004f uni0000005c uni00000003 uni00000029 uni0000004c uni0000005b uni00000048 uni00000047 uni00000038 uni00000051 uni00000056 uni00000058 uni00000046 uni00000046 uni00000048 uni00000056 uni00000056 uni00000049 uni00000058 uni0000004f uni0000004f uni0000005c uni00000003 uni00000029 uni0000004c uni0000005b uni00000048 uni00000047fig.
.
human evaluation of the results on cci revision.
v. d iscussion in this work we fine tuned a large language model i.e.
codellama to detect and rectify cci issues and conducted empirical studies on publicly available datasets to confirm the validity of our proposed c4rllama approach.
in this section we discuss some considerations in our work so far.
a the reasons for the better performance of c4rllama the task of code comment consistency checking necessitates a comprehensive understanding of both the code and comment content as well as the ability to discern discrepancies between them.
this requires the applied approach to possess extensive knowledge to perform these complex logical comparisons.
large language models pre trained on vast amounts of data have the potential to excel in this task by gaining a profound understanding of both code and language .
during the fine tuning phase we devise training algorithms that concentrate a portion of the model s capabilities on the task of cci detection and rectification thereby enhancing the large language model s capabilities.
as some studies revealed that noisy data cannot be completely eliminated from datasets curated from open source software oss projects the loss function we designed in c4rllama helps to mitigate the negative impact of this noisy data thereby improving the final performance in terms of cci detection and rectification.
b other implications of resolving cci issues since most large language models including those oriented towards code generation do not disclose the details of the dataset used for training there is currently no information on whether comments are included along with the code fed to the model when it is pre trained .
however many studies have confirmed that for large language models data quality is a key factor in model performance .
the inclusion of a substantial number of inconsistent comments in the code used for training these models could invariably compromise their performance.
in light of this the research presented in this paper could offer valuable insights for enhancing the quality of code used in training large language models for coderelated tasks particularly from the standpoint of eliminating inconsistent comments.
c future improvements although c4rllama has performed quite well according to the results of this paper there are still a lot of directions to be explored and problems to besolved.
first of all c4rllama currently only uses the model of codellama 7b.
however many studies have shown that the amount of model parameters has a significant impact on the resultant model performance .
therefore it is worth using a large language model with more parameters for the base model.
also as new and more advanced large language models continue to be released it is promising that these more capable models can yield better results.
secondly the data used in this paper is more likely to be examined for just a single comment at a time which is relatively less difficult.
in real world scenarios there are often interlinear comments code documents and other scenarios with complex semantics which require the model to fully understand the semantics of the code and natural language and even include complex logical and mathematical transformations that exist between some of the documents and the code.
although a large language model presents the potential to deal with these types of problems a lot of exploratory work is still needed.
d exploring more proactive ways to solve the comment issue comment is undoubtedly helpful to code readers and is hence one of the integral software artifacts.
in this paper as well as in previous related studies we note that an ex post facto approach to cci issues has generally been taken i.e.
waiting until an cci issue occurs which is the case even with the just in time mode except that it has not been committed to the repository of the code and then trying to figure out how to correct an error that has already occurred thus attempting to match the code with improved comments.
however the emergence of the large language models and in particular the emergence of multiple programming assistants geared towards programming based on large language models prompt us to wonder if there are more proactive ways to deal with the challenge of commenting code.
for example training a large language model to generate comments directly while programming is a reasonable way for the programmer to pick a more appropriate comment.
then by integrating this feature in ides and using c4rllama with the just in time mode it is clear that most of the cci issues can be avoided.
this paper reveals to a large extent that large language models have impressive capabilities of understanding both source code and natural language and in this sense a more proactive means of coping with comments is already emerging.
what is lacking may be their practical use which should be explored as a future research direction.
vi.
t hreats tovalidity this section discusses some of the factors that could potentially impact the findings and conclusions of our study.
the claim regarding large language model we do not intend to stir up controversy and in fact there are no wellrecognized criteria or definitions for distinguishing large language models.
in this paper we refer to the general purpose large language models represented by chatgpt and llama.
these large language models are significantly different from previous models such as bert and longformer in terms of model structure and the number of parameters and it is alsoevident that the performance of large models is significantly improved by the phenomenon of emergence due to a large number of parameters .
the limited dataset this study employs a dataset originally compiled by a prior researcher that exclusively contains data in the java language.
the dataset is limited to three specific data types relevant to comments summary param and return .
consequently the task of cci detection is inherently constrained by these types of comment data to a certain extent.
it is widely acknowledged that the performance of machine learning models is largely dictated by the quality and relevance of the data they are trained on.
while large language models can achieve a degree of generalization across different programming languages due to pre training they still require training on meticulously curated data e.g.
data in the relevant language specific comment types etc.
to yield optimal results.
this underscores the importance of careful data preparation in the pursuit of more desirable outcomes in machine learning tasks.
the definition of consistency inconsistency we take the same treatment of consistency inconsistency in our work as in previous studies i.e.
since the datasets provided in these studies are labeled and already have a clear distinction between consistent and inconsistent items we directly follow these labels to determine whether there is a cci issue.
however we also found that there are naturally multiple ambiguous interpretations of so called consistency and there are clearly differences in the magnitude of the impact of the cci issue.
we find that these phenomena have not been addressed in a targeted manner in existing research so far and undoubtedly have some implications for the conclusions of related studies e.g.
do some of the inconsistencies similar to those in table i really need to be addressed?
.
this paper is no exception.
nevertheless as we have made great efforts to ensure the validity of comparisons we believe the conclusions of this paper are still correct.
however from the perspective of guiding practice a more nuanced distinction between cci issues may be needed.
the baseline labeling issue regarding dataset the dataset from previous researchers used in this study assumed that the collected items were relatively well maintained i.e.
that all cci issues were fixed in a timely manner a requirement that the researchers who provided the dataset also acknowledged would be difficult to fully satisfy in reality.
this may have led to a small number of untimely modifications being recognized in the dataset as not having cci issues.
and we also found that some of the cci rectifications were not fixing code comments but rather fixing spelling.
these facts could have a negative effect on the performance of c4rllama regarding accuracy.
however from our sampling results this type of phenomenon does not occur frequently so we believe this risk is manageable.
unable to replicate original algorithm for comparative evaluations owing to the absence of certain information in the original dataset e.g.
ast tree we were unable to replicate the algorithms in study with complete accuracy.however we believe this does not affect the comparative evaluation presented in vi vii and viii as the data is presumed to represent the optimal performance of the benchmark methods in their respective studies.
despite this it did prevent us from conducting a human evaluation to compare the effectiveness of the cci issue rectification between our c4rllama method and the benchmark methods .
consequently it remains an open question as to whether the rectification results of the c4rllama method at the semantic level outperform those of the benchmark methods no comparison with code summarization techniques code summarization enables developers to quickly understand a given implementation by generating natural language from the given code implementation.
ideally developers can avoid cci issues by using code summarization to generate various comments after modifying the code implementation.
alternatively after identifying the cci issue the developer can simply delete the corresponding comment and use the content generated by the code summarization as the new code comment.
however considering that code summaries and comments do present not exactly the same content and our dataset is specific to code comments.
as such it is impossible to conduct a fair comparison.
therefore we did not analyze code summaries comparatively during the evaluation process.
exclusion of gpt series in cci rectification comparative study in our comparative study of cci rectification we did not include the gpt series of large language models.
this omission does not preclude the potential of these models to outperform others in addressing cci issues.
however it is important to note that the gpt series has demonstrated suboptimal performance in detecting cci issues.
it is generally more practical for practitioners to first detect cci issues before attempting to rectify them.
therefore the effectiveness of the gpt series in cci revision remains an open question.
constraints to the large language model as the training of large language models consumes a lot of resources we only carried out the related work described in this paper on codellama one of the most representative code large language models.
also due to resource constraints we only used a model of 7b parameters.
in fact there are also several large language models oriented to coding tasks each of which also has several different parameter size settings.
all these factors obviously have an impact on the results of our study.
limited by time and gpu resources we were not able to conduct experiments on each of them.
we recognize this validity risk and will always adopt an open mind to try different large language models at the right time.
on the other hand as one of the most well known open source large language models the llama series has been widely studied so the c4rllama approach based on codellama is laid on a solid foundation and the results deserve to be generalized.
errors from human evaluation despite using clear criteria and consistency checks to ensure the accuracy of human evaluations errors in manual evaluation are still possible which may to a certain degree affect the results.
we mitigated this risk by employing evaluators with a background insoftware engineering ensuring they have the necessary expertise to enact the criteria when determining the consistency of rectified comments and the corresponding source code.
besides this risk is also largely mitigated by the high degree of consistency between the results of human evaluation and objective evaluations based on multiple metrics as shown in table viii.
vii.
c onclusion and future work in this study we present a novel approach c4rllama for the detection and rectification of cci.
leveraging the power of large language models we fine tune our model based on codellama to achieve state of the art performance on both tasks.
specifically c4rllama successfully detects approximately of cci issues with over half of these issues being automatically and correctly rectified irrespective of whether the model operates in post hoc orjust in time mode.
this research represents an early attempt to utilize a large language model to address cci issues.
our findings underscore the immense potential of large language models in resolving cci issues while also highlighting areas for further enhancement.
for instance one promising avenue for future research involves fine tuning a large language model to directly generate code comments.
additionally exploring the performance boundaries of c4rllama by employing a more powerful large language model as a base could prove beneficial.
such an approach may significantly automate the resolution of cci issues thereby making it a practical solution for large scale applications.
artifacts in order to facilitate the verification or replication of our work we provide the dataset algorithmic code as well as associated instructions used in the study as detailed in the following url.