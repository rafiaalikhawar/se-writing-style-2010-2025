time travel testing of android apps zhen dong national university of singapore zhen.dong comp.nus.edu.sgmarcel b hme monash university australia marcel.boehme monash.edu lucia cojocaru politehnica university of bucharest lucia.cojocaru stud.acs.upb.roabhik roychoudhury national university of singapore abhik comp.nus.edu.sg abstract androidtestingtoolsgeneratesequencesofinputeventstoexercise the state space of the app under test.
existing search based techniquessystematicallyevolveapopulationofeventsequences so as to achieve certain objectives such as maximal code coverage.
the hope is that the mutation of fit event sequences leads to the generationofevenfittersequences.ho wever theev olutionofevent sequences may be ineffective.
our key insight is that pertinent app stateswhich contributed to the original sequence s fitness may not bereachedbyamutatedeventsequence.theoriginalpaththrough the state space is truncated at the point of mutation.
inthispaper weproposeinsteadtoevolveapopulationofstates which can be captured upon discovery and resumed when needed.
thehopeisthatgeneratingeventsonafitprogramstateleadsto the transition to even fitter states.
for instance we can quickly deprioritize testing the main screen state which is visited by most eventsequences andinsteadfocusourlimitedresourcesontesting more interesting states that are otherwise difficult to reach.
wecallourapproach time traveltesting becauseofthisability totravelbacktoanystatethathasbeenobservedinthepast.we implementedtime traveltestingintotimemachine atime travel enabled versionof thesuccessful automated android testingtool monkey.inourexperimentsonalargenumberofopen andclosed sourceandroidapps timemachineoutperformsthestate of theart search based model based android testing tools sapienz and stoat both in terms of coverage achieved and crashes found.
introduction android app testing has been gaining in importance.
in there is a smart phone for every third person .
billionusers while app revenue will double from us to billion .1the number of bugs and vulnerabilities in mobile apps are growing.
in .
ofmobileappscontainedatleastonehigh risksecurity flaw .theandroidtestingmarketisalsoexpectedtodoublein five years from us .
billionin to us .
billionin .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn ... .
a excerpt of the map of maridia where pink marks explored rooms.
b samus discovering the spazer weapon figure super metroid on an android emulator toillustratethechallengesofexistingandroidtestingtools take forexamplesupermetroid oneofthebestgamesforthe nes gaming console now available for android.
supermetroid is playedonalargemapofroomsthatcanbeexploredinanyorder.
by pushing the right buttons on the controller the main character samusmovesfromoneroomtothenext findingsecretsandgaining in strength by fighting enemies.
today android app testing is like playing a game of super metroid albeit withoutthe ability to save after important milestones and to travel back in time when facing the consequences of a wrong decision.
one possible approach is to generate a single very long sequence of events in a random fashion .
however the testing tool may ultimatelygetstuckindeadends.forinstance samusmayfallinto pits or get lost in a particularly complex part of the labyrinth.
this problemisovercome onlypartially byrestartingthe androidapp because i we must start from the beginning ii there is no clean slate e.g.
database entries remain and iii how to detect when we are stuck isstill an open question.
forandroid testing the ability tosaveandtravelbacktothemostinterestingstatesgoesalong way towards a more systematic exploration of the state space.
anotherandroidapptestingapproach istoevolveapopulation of event sequences in a search based manner.
in each iteration the fittest event sequences are chosen for mutation to generate the next generation of event sequences.
an event sequence is mutated by adding modifying or removing arbitrary events.
however this approach does not allow for systematic state space exploration by traversingthevariousenabledeventsfromastate.if eiinthesequencee angbracketlefte1 ... ei ...en angbracketrightismutated thenthesuffixstarting inei 1maynolongerbeenabled.forinstance whensamusstands next to an enemy or a ledge after event ei 1and the event eiis turnedfromapressofthe buttontoapressofthe button samus may be killed or get stuck.
the remaining events starting fromei 1becomeimmaterial roomsthatwerereachedby emay not be reached by its mutant offspring.
ieee acm 42nd international conference on software engineering icse in this paper we propose instead to evolve a population of states which can be captured upon discovery and resumed when needed.
by capturing and resuming an app s states we seek to achieve a systematicstatespaceexploration withoutgoingtotheextentof exhaustive exploration as in formal verification .
due to the ability totravelbacktoanypaststate wecallthisastime traveltesting.
our novel time travel testing approach systematically resets the entire system the android app and all of its environment to the mostprogressivestatesthatwereobservedinthepast.a progressive stateis one which allows us to discover new states when different input events are executed.
once the tool gets stuck it goes back in time and resumes a progressive state to execute different events.
weimplementtime traveltestingforandroidappsinto timemachine3a time travel enabled variant of the automated android testing tool monkey .
in our example one can think of timemachine as an automatic player that explores the map of super metroidthroughveryfastrandomactions automaticallysavesafter important milestones and once it gets stuck or dies it travels back to secret passages and less visited rooms seen before in order to maximizethe coverageofthe map.comparedto toolsthatevolve event sequences such as sapienz timemachine does not mutate thesequence prefix which is required to reach the fittest most progressive state and instead generates only the sequence suffix startingfromthatstate.comparedtotoolsthatgenerateasingle very long event sequence such as monkey or stoat timemachine automatically detects when it gets stuck i.e.
there is a lackofprogress andresumesthatstateforfurthertestingwhichis mostpromisingforfindingerrors.inourexperimentswithsapienz stoat andmonkeyonbothopen sourceandclosed sourceandroid apps timemachine substantially outperformed the state of the art in terms of both coverage achieved and errors found.
timemachine canbeseededwithasetofinitialeventsequences.
atthebeginningofatestingsession timemachine takesasnapshot ofthestartingstate.duringtestexecution timemachine takesa snapshotofeveryinterestingstateandaddsittothe statecorpus travels back to the interesting state and executes the next test.foreachtransitionfromonestatetoanother timemachine also recordstheshortesteventsequence.ifnoinitialtestsetisprovided timemachine only adds the starting state to the state corpus.
timemachine isanautomatictime travelling enabledtestgenerator for android apps that implements several heuristics to choose the most progressive state from the state corpus to explore next.
intuitively a statereachingwhichcovered newcodeandthathas beendifficulttoreachhasmorepotentialtotriggernewprogrambe havior.
timemachine dynamicallycollectssuchfeedbacktoidentify themostprogressivestate.
timemachine identifiesaprogressive state as one which itself was infrequently visited andtheknearest neighbors4were visited relatively infrequently.
ourexperimentsdemonstrateasubstantialperformanceincrease over our baseline test generation tool monkey extended withsystem level event generator of stoat .
given the apps in the androtest benchmark our time travel strategy enables the baseline tool to achieve .
times more statement coverageand to discover .
times more unique crashes.
given apps 3named after the celebrated fictional work by h.g.
wells more than a century ago.
4theknearest neighbors are states reachable along at most kedges.in the benchmark of industrial apps around more methods are covered on average and .
times more unique crashes are discovered.
our time travel strategy makes timemachine so efficient thatitoutperformsthestate of the arttestgeneratorssapienz and stoat both in terms of coverage as well as errors found detecting around .
times more unique crashes than the next best test generator.
timemachine tested the top most popular apps from google play and found unique crashes.
in summary our work makes the following contributions we propose time travel testing for android which resumes the most progressive states observed in the past so as to maximize efficiency during the exploration of an app s state space.theapproachidentifiesandcapturesinterestingstates as save points detects when there is a lack of progress and resumesthe mostprogressivestates forfurther testing.for instance it can quickly deprioritize the main screen state whichisvisitedbymostsequences andresume testdifficultto reach states.
we propose several heuristics that guide execution to a progressive state when progress is slow.
we implement the time travel testing framework and an automated feedback guided time travel enabledstatespace explorationtechniqueforandroidapps.theframeworkand testing technique are evaluated on both open source and closed sourceandroidappbenchmarks aswellastop popular apps from google play.
we have made our timetravel android app testing tool timemachine publicly available on github time travel framework we design a general time travel framework for android testing which allows us to save a particular discovered state on the fly andrestoreitwhenneeded.figure2showsthetime travelinfrastructure.
the android app can be launched either by a human developeroranautomatedtestgenerator.whentheappisinteracted with the state observer module records state transitions and monitors the change of code coverage.
states satisfying a predefinedcriteriaaremarkedas interesting andaresavedbytakinga snapshot of the entire simulated android device.
meanwhile the framework observes the app execution to identify when there is a lack of progress that is when the testing tool is unable to discover any new program behavior over the course of a large number of state transitions.
when a lack of progress is detected the framework terminates the current execution selects and restores the most progressive one among previously recorded states.
a more progressivestate isonethatallowsustodiscovermorestatesquickly.
when we travelback to the progressive state an alternative event sequence is launched to quickly discover new program behaviors.
theframeworkisdesignedaseasy to useandhighly configurable.
existing testingtechniques canbe deployedon the frameworkby implementing the following strategies specifyingcriteria whichconstitute an interesting state e.g.
increases code coverage.
only those states will be saved.
specifyingcriteriawhichconstitute lackofprogress e.g.
whentesting techniques traverse the same sequence of states in a loop.
providinganalgorithmtoselectthemostprogressivestatefor time travelling when a lack of progress is detected.
482automated test generators e.g.
monkeystate identificationcoverage monitorstate recorder state manager snapshot creator lack of progress detection snapshot restorersnapshot poolinteresting state detection progressive state selectionstate transition state grapha snapshot android os state observer a snapshot developer figure time travel framework.
modules in grey are configurable allowing users to adjust strategy according to scenarios.
.
taking control of state stateidentification.inordertoidentifywhatconstitutesastate our frameworkcomputesanabstractionofthecurrentprogramstate.
aprogramstateinandroidappisabstractedasanapppagewhich isrepresentedasawidgethierarchytree non leafnodesindicate layout widgets and leaf nodes denote executable or displaying widgets such as buttons and text views .
a state is uniquely identified bycomputingahashoveritswidgethierarchytree.inotherwords when a page s structure changes a new state is generated.
tomitigate thestate spaceexplosion problem weabstract away values of text boxes when computing the hash over a widget hierarchy tree.
by the above definition a state comprises of all widgets andtheirattributes inanapppage.anydifferenceinthosewidgets orattributevaluesleadstoadifferentstate.someattributessuchas text boxvaluesmayhavehugeorinfinitenumberofpossiblevalues that can be generated during testing which causes a state space explosion issue.
to find a balance between accurate expressiveness ofa stateand statespace explosion we ignoretext box values for stateidentification.ourpracticethataguistateisdefinedwithoutconsideringtext boxvaluesisadoptedbypreviousandroidtesting works as well .
state saving restoring.
we leverage virtualizationto save and restoreastate.ourframeworkworksontopofavirtualmachine where android apps can be tested.
a virtual machine vm is a softwarethatrunsafullsimulationofaphysicalmachine including the operating system and the application itself.
for instance a vm withanandroidimageallowsustorunandroidappsonadesktop machinewhererelatedhardwaresuchasthegpsmodulecanbe simulated.
app states can be saved and restored with vm.
our framework records a program state by snapshotting the entire virtual machine state including software and emulated hardware inside.
states of the involved files databases third party libraries and sensors on the virtual device are kept in the snapshot so that the state can be fully resumed by restoring the snapshot.
this overcomes the challenge that a state may not be reached from the initial state by replaying the recorded event sequence due to state change of background services.
.
collecting state level feedback toidentifywhetherastateis interesting ourframeworkmonitors thechangeincodecoverage.wheneveranewstateisgenerated code coverage is re computed to identify whether the state haspotential to cover new code via the execution of enabled events.
ourframework supportsbothopen source andclose sourceapps.
for open source apps we collect statement coverage using theemma coverage tool .
for closed source industrial apps we collectmethodcoverageusingtheellacoveragetool .forclosedsource apps statement coverage is difficult to obtain.
our framework uses a directed graph to represent state transitions where a node indicates a discovered state and an edge representsastatetransition.eachnodemaintainssomeinformation about the state whether there is a snapshot only states with snapshotscanberestored howoftenithasbeenvisited howoften ithasbeenrestored andsoon.thisinformationcanbeprovided totestingtoolsorhumantesterstoevaluatehowwellastatehas been tested and to guide execution.
methodology we develop the first time travel enabled test generator timemachine forandroidappsbyenhancingandroidmonkey with our framework.
timemachine s procedure is presented in algorithm .
timemachine s objective is to maximize state and code coverage.
timemachine startswithasnapshotoftheinitialstate lines .
for each event that monkey generates the new state is computed and the state transition graph updated lines .
ifthe state isinteresting sec.
.
a snapshot of the vm is taken andassociatedwiththatstate lines10 .ifmonkeyisstuckand no more progess is made sec.
.
timemachine finds the most progressivestate selectfitteststate sec.
.
andrestoresthe associated vm snapshot lines .
otherwise a new event is generated and loop begins anew lines .
.
identifying interesting states timemachine identifies an interesting state based on changes in guiorcodecoverage line10inalgorithm1 .thefunctionisinteresting state returnstrueif stateisvisitedforthefirsttime and when statewas first reached new code was executed.
483algorithm time travel testing timemachine .
input androidapp sequence generator monkey statecurstate launch app save vm snapshot of curstate interesting states states curstate state transition graph stategraph initgraph curstate for each eventeinmonkey.generateevent do iftimeout reached then break end if prevstate curstate curstate executeevent app e stategraph updategraph prevstate curstate ifisinteresting curstate stategraph then save vm snapshot of curstate states states curstate end if ifisstuck curstate stategraph then curstate selectfitteststate states stategraph restore vm snapshot of curstate end if end for output state transition graph stategraph the intuition behind our definition of interesting states is that theexecutionofnewcodeprovidestheevidencethatafunctionality thathasnotbeentestedbeforeisenabledinthediscoveredstate.
morenewcode relatedtothefunctionalitymightbeexecutedbyexploringthisstate.forinstance supposeclickingabuttononscreen s1leads to a new screen s2 from where a new widget is displayed increasing code coverage .
the new widget comes with its own event handlers that have not been executed.
these event handlers canbecoveredbyfurtherexploringscreen s2.thisheuristicnot onlyaccuratelyidentifiesaninterestingstate s2inthiscase but also significantly reduces the total number of saved states since only interesting states are saved during testing .
.
identifying lack of progress thetestingprocesscanstayunprogressivewithoutdiscoveringany new program behavior for quite some time.
as reasonsfor monkey getting stuck we identified loops and dead ends.
loops.aloopisobservedwhenthesamefew high frequency states are visited again and again.
to easily perform routine activities app pages are typically organized under common patterns e.g.
fromthemainpageonecanreachmostotherpages.thisdesignleadstoaphenomenonwhererandomeventstendtotrigger transitions to app pages which are easy to trigger.
moreover apps often browse nested data structures it is difficult to jump out from them without human knowledge.
for example let us consider the anymemo app a flashcard learning app we tested.
monkey clicks a button to load a csvfile and arrives at an app page that browsessystemdirectories.itkeepsonexploringdirectoriesand cannotleavethisapppageuntilitfindsa csvfiletoload orbypressing the back button many times in a row .
in our experiments monkey could not jump out of the loop within events.algorithm detecting loops and dead ends isstuck .
input queue length l input lack of progress threshold maxnopro ress input max.
top most frequently visited states input max.
proportion of repeated plus frequent states fifoqueue empty queue of length l nopro ress events since last state transition procedure isstuck state curstate graphstategraph prevstateid queue.top ifprevstateid curstate .idthen nopro ress nopro ress else queue.push curstate .id nopro ress end if ifnopro ress maxnopro ress then return true detect dead ends end if ifqueue.length lthen nrepeated countmaxrepeatedstates queue nfrequent countfreqstates queue stategraph if nrepeated nfrequent l then return true detect loops end if end if return false deadends.
adeadendisastatewhichisdifficulttoexit.some pagesrequirespecificinputswhichareveryunlikelytoberandomly generated.monkeycanbetrappedbythemandcankeepongenerating events without making any progress .
for instance consider anapppageinanymemo whereaformneedstobefilledand submitted.
yet the submit button is located at the bottom of the page and does not even appear on screen.
monkey would need to correctly fill in certain parameters scroll all the way to the bottom andthengeneratea click eventonthebuttontotransitiontoexit the page.
this is quite unlikely.
monkey gets stuck in a dead end.
when timemachine gets stuck the most progressive state is traveled back to lines in algorithm .
the function isstuckissketchedinalgorithm2andrealizesaslidingwindowalgorithm.
firstly four parameters must be specified which are explained later.therearetwoglobalvariables aqueueof specifiedlengthl and a counter which keeps track how often the same state has been observed lines .
given the current app state and the state transition graph if the current state is the same as the previous statetheno progresscounterisincremented lines4 .otherwise the counter is reset lines .
if the counter exceeds the specified maximum maxnopro ress then a dead end is detected lines .ifthefixed lengthqueueis filledandtheproportionof easy states in the queue surpasses the specifiedthreshold then a loop isdetected.twokindsofstatesinthequeueareconsidered easy states occurring multiple times in the queue and states among the top percentage of the most frequently visited states.
484algorithm selecting the next state selectfitteststate input path length k procedure selectfitteststate states stategraph bestfitness for each stateinstatesdo statefitness paths all paths in stategraph of length kfromstate for each pathinpathsdo for each nodesinpathdo statefitness statefitness f s see eq.
end for end for statefitness statefitness paths ifstatefitness bestfitness then beststate state bestfitness statefitness end if end for returnbeststate .
progressive state selection in order to select a state to travel back to once monkey isstuck we assign a fitness to each state which evaluates its potential to trigger new program behavior lines in alg.
.
the fitness f s of a state sis determined by the number of times the state has been visited and the number of interesting states generated from it.
concretely the fitness function is defined as f s f0 r w s p v s w s wherev s isthenumberoftimesstate sisvisitedand w s isthe number of interesting states generated from state s ris a reward of finding an interesting state and pis a penalty of transiting to astatethathasalreadybeendiscovered f0istheinitialvalue.in timemachine the initial value of an interesting state is set as timesofthatofanuninterestingstate and raswellas paresetas0 .
.
whenastateisrepeatedlybeingvisitedandnointerestingstates are discovered its fitness keeps on being reduced due to penalty p so that other state will be selected and restored eventually.
maximizing benefit of time travel.
the definition of state fitness in equation does not account for the fact that events executedonthatstatemayquicklytriggeradeparturefromthat state again advancing throughunprogressive states.
to maximize benefit of time travel we develop an algorithm that selects the statewithahigh fitnees neighborhood i.e.
thestatewhichhas neighboring states which also have a high fitness.
algorithm3outlinestheprocessofselectingthemostprogressive state for time travel.
it takes as input the interesting states thathaveanassociatedvmsnapshotandthestatetransitiongraph thatismaintainedbyourtime travelframework.thenumberof transitions kwhichdeterminesastate s neighborhood mustbe specified by the user.
in our experiments we let k .
for each interesting state timemachine computestheaveragefitnessofa state identificationadb daemon adb server cov.
data collectorguided event generator virtualbox managermonkey uiautomator sys event generatorcoverage monitortimemachineandroid virtual machine android os docker container host os state corpus vm controller figure architecture of timemachine implementation.
stateinthe k neighborhoodofthestate.thestatewiththemaximumaveragestatefitnessinits k neighborhoodisreturned.the k neighborhood ofstateareallstates sinstategraph thatarereachable from statealong at most ktransitions.
the fitnessf s of a statesis computed according to equation .
with this algorithm monkeynotonlytravelsintimetothestatewiththehighestfitness value but also continues to explore states with high fitness values withinktransitions which maximizes the benefit of time travel.
implementation our time travel framework is implemented as a fully automatedapp testing platform which uses or extends the following tools virtualbox the python library pyvbox for running and controllingtheandroid x86os androiduiautomator for observingstatetransitions andandroiddebugbridge adb for interacting with the app under test.
figure gives an architectural overview of our platform.
components in grey are implemented by us while others are existing tools that we used or modified.
forcoveragecollection ourframeworkinstrumentsopen source apps using emma statement coverage and closed source apps using ella method coverage .
ella uses a client server model sendingcoveragedatafromtheandroidostothevmhostviaa socket connection.
unfortunately this connection is broken every time a snapshot is restored.
to solve this issue we modified ella to save coverage data on the android os to actively pull as needed.
ontopofthetimetravelframework weimplement timemachine.
to facilitate the analysis of all benchmarks we integrated timemachine withtwoandroidversions.
timemachine workswiththe mostwidely usedversion androidnougatwithapi25 android .
.
however to perform end to end comparison on androtest benchmark we also implement timemachine on android kitkat version with api android .
.
the publicly available version of sapienz a state of the art practice baseline for our experiments islimitedtoandroidapi19andcannotrunonandroid .
.
to collect state level feedback we modified androidmonkeyanduiautomatortomonitorstatetransitionaftereach eventexecution.
timemachine alsoincludes asystem level event generatortakenfromstoat tosupportsystemeventssuchas phone calls and smss.
empirical evaluation inourexperimentalevaluation weseektoanswerthefollowing research questions.
485rq1howeffectiveisourtime travelstrategyintermsofachieving more code coverage and finding more crashes?
we compare timemachine tothebaselineintowhichitwasimplemented.
rq2howdoestime traveltesting i.e.
timemachine compareto state of the arttechniquesintermsofachievedcodecoverage and found crashes?
rq3howdoestime traveltesting i.e.
timemachine performon larger real world apps such as industrial apps and top apps from google play?
.
experimental setup to answer these research questions we conducted three empirical studies on both open source and closed source android apps.
study1.
toanswerrq1 weevaluate timemachine andbaseline toolsonandrotest andinvestigatehowachievedcodecoverage andfoundfaultsareimprovedbyusingthetime travelstrategy.we choseandrotestappsassubjectsbecauseandrotesthasbecome a standard testing benchmark for android and has been used to evaluate a large number of android testing tools .itwascreatedin2015bycollectingandroidappsthat have been used in evaluations of android testing tools.
timemachine applies time travel strategy to a baseline tool the baseline tool is monkey extended with stoat s system level event generator.
to accurately evaluate effectiveness of time travel strategy we set monkey extended with the system level event generator fromstoatasbaseline called ms .wechosemsinsteadofmonkey asabaselinetooltomakesurethattheimprovementachievedby timemachine completelycomesfromtime travelstrategy notfrom system event generation.
we also implement another variant of monkey as baseline to evaluate effectiveness of heavy components such as state saving andrestoringonenhancingatesttechnique.thisvariantapplies onlythelackofprogressdetectioncomponentofourtime travel strategywithoutstatesavingandrestoringcomponents.whenlack ofprogressisdetected itsimplyrestartstestingfromscratch i.e.
re launching app under test without resuming states called mr .
intimemachine parameters l maxnopro ress forisstuck inalg.2aresetto10 .
and0.
respectively.thesevalues were fixed during initial experiments of two authors with three appsfromandrotest anymemo bites acal .weexecutedthese apps with monkey for many rounds and recorded relevant data such as the number of state transitions when a loop was observed andthenumberofexecutedeventswhenmonkeyjumpedoutfrom a dead end.
based on observed data and authors heuristics we cameupwithseveralgroupsofvaluesandevaluatedthemonthese threeapps andeventuallychoseabovedataasdefaultparameter values.
in the evaluation timemachine used the default values for all the three studies.
baseline tool ms and mr use the same parameter values as in timemachine.
study .
toanswerrq2 weevaluate timemachine andstateof the art app testing tools on androtest and compare them interms of achieved code coverage and found crashes.
for state of the art tools we chose monkey sapienz andstoat .
monkey is an automatic random event sequence generator fortesting android apps and has been reported to achieve the best performance in two works .
sapienz and stoat are the mostrecenttechniquesforandroidtesting.thesetestingtoolshavealso beenadequatelytestedandarestandardbaselinesintheandroid testingliterature.tohaveafaircomparison alltechniquesusetheir default configuration.
study .
to answer rq3 we evaluate timemachine baseline toolsandallstate of the arttechniquesonlargereal worldandroid apps and investigate whether they have a consistent performance on both closed source and open source androidapps.
in thisevaluation we use industrialapps as subject apps.
industrialapps wasabenchmarksuitecreatedin2018toevaluatetheeffectiveness ofandroidtestingtools onreal worldapps.theauthorssampled 68appsfromtop recommendedappsineachcategoryongoogle play andsuccessfullyinstrumented41appswithamodifiedver sion of ella .
in our experiment we chose to use the original version of ella and successfully instrumented apps in industrial app suite.
on this benchmark we could not compare with sapienz because the publicly available version of sapienzis limited to an older version of android api .
tofurtherinvestigatetheusabilityof timemachine weevaluate timemachine on top popular android apps from google play andinvestigatewhether timemachine caneffectivelydetectcrashes in online apps i.e.
those available for download from google play at the time of writing.
following the practice adopted by some previous authors of applying the technique to top popular apps on google play we focus on analyzing detected crashes by timemachine and do not compare timemachine with state of thearttechniquesonthisdataset.top 100popularappswerecollected bydownloadingthemosthighlyrankedappson googleplayand instrumenting them withour coverage tool ella until we obtained apps that could be successfully instrumented by ella.
procedure.
tomitigateexperimenterbiasandtoscaleourexperiments wechosetoprovidenomanualassistanceduringtesting in all studies.
for all test generators the android testing is fully automatic.
none of the test generators is seeded with an initial set of event sequences.
the testing process is automatically started after installation.
all data are generated and processed automatically.
we neither provide any input files nor create any fake accounts.
each experiment is conducted for six hours and repeated five times totalling cpu hours .
year .
to mitigate the impact of random variations during the experiments we repeated each experiment five times and report the average.
in comparison the authors of sapienz report one repetition of one hour while the authors of stoat report on five repetitions of three hours.
we chose atimebudgetofsixhoursbecausewefoundthattheasymptotic coverage was far from reached after three hours in many apps i.e.
no saturation had occurred .
coverage crashes .wemeasurecodecoverageachievedand errorsdiscoveredwithinsixhours.to measurestatementormethod coverage weuseemmaandella thesamecoveragetoolsthatare usedinsapienzandstoat.to measurethenumberofuniquecrashes detected we parse the output of logcat 5an adb tool that dumps alogofsystemmessages.weusethefollowingprotocoltoidentify a unique crash from the error stack taken from su et al.
removeallunrelatedcrashesbyretainingonlyexceptions containing the app s package name and filtering others .
486table results from androtest open source apps .
subjectstimemachine baselines state of the art cov cra state coverage crashes coverage crashes ms mr ms mr st sa mo st sa mo a2dp aagtl aarddict acal addi adsdroid agrep aka alarmclock alogcat amazed anycut anymemo autoanswer baterrydog battery bites blockish bomber book cat cdt dalvik exp dialer2 dac fileexplorer fbubble gestures hndroid hotdeath importcont jamendo k9mail lnm lpg lbuilder manpages mileage mnv mirrored multisms munchlife myexp mylock nectroid netcounter pwmg pwm photos qsettings rmp ringdroid sanity soundboard smt spritetext swiftp syncmypix tippytipper tomdroid translate triangle wchart whams wikipedia wordpress worldclock yahtzee zooborns ave sum giventhe relatedcrash information extractonly thecrash stackandfilteroutallinformationthatisnotdirectlyrelevant e.g.
the message invalid text... .
computeahashoverthesanitizedstacktraceofthecrash toidentifyuniquecrashes.differentcrashesshouldhavea different stack trace and thus a different hash.
time in minutesstatement coveragetm ms mr figure progressive statement coverage for timemachine tm andbaselinetoolson68benchmarkapps.msindicates monkey extended with stoat s system level generator and mrindicatesmonkeywiththeabilitytorestartfromscratch when lack of progress is detected.
executionenvironment.
theexperimentswereconductedon two physical machines with gb of main memory running a64 bit ubuntu .
operating system.
one machine is powered byanintel r xeon r cpue5 2660v4 .00ghzwith56cores while the other features an intel r xeon r cpu e5 v3 .60ghzwith40cores.toallowforparallelexecutions werunour systemindocker v1.
containers.eachdockercontainerrunsavirtualbox v5.
.
vmconfiguredwith2gbramand2cores for the android .
and cores and 4gb ram for android .
.
we madesurethateachevaluatedtechniqueistestedunderthesame workload by running all evaluated techniques for the same app on the same machine.
.
experimental results .
.
study effectiveness of time travel strategy.
table shows achieved coverages and found faults by each technique on android apps.
the highest coverage and mostfound crashes are highlighted with the grey color for each app.
the results of timemachine and baseline techniques are shown in column timemachine and baselines .recallthatmsindicates monkeyextendedwithstoat ssystem leveleventgenerator and mrindicatesmonkeywiththeabilitytorestarttestingfromscratch when lack of progress is detected.
comparisonbetween timemachine andms.
timemachine achieves54 statementcoverageonaverageanddetects199unique crashes for benchmark apps.
ms achieves statement cov erage on average and detects unique crashes.
timemachine covers1.15timesstatementsandreveals1.73timescrashesmore than ms. to further investigate these results figure presents achieved code coverage over execution time for all apps.
as we cansee timemachine hasachievedhighercoveragefromaround the 20th minute onwards finally achieving more statement coverage at the end of execution time.
figure presents the box plots ofthefinalcoverageresultsforappsgroupedbysize of app where x indicates the mean for each box plot.
we see that coverage improvement is substantial for all four app size groups.
487tm ms mr tm ms mr tm ms mr tm ms mr a 1k apps b 1k 3k apps c 3k apps d all apps tm ms m k a mr tm ms m b 1k 3k a mr tm ms m k mr a r tm ms mr r figure statement coverage achieved by timemachine tm ms and mr. a 1k apps b 1k 3k apps c 3k apps d all appstm st sa mo k a tm st sa mo b 1k 3k a o d all a tm st sa mo tm st sa mo tm st sa mo figure statement coverage achieved by timemachine tm stoat st sapienz sa and monkey mo .
ourtime travelstrategyeffectivelyenhancestheexistingtesting technique ms by achieving .
times statement coverage and detecting .
times crashes on benchmark apps.
comparison between timemachine and mr. mr achieves statementcoverageonaverageanddetects45uniquecrashes for68benchmarkapps.
timemachine achieves1.15timesstatement coverage and .
times unique crashes more than mr. similarly figure4andfigure5shows timemachine coversmorecodeina short time and substantially improves statement coverage for all fourappsizegroupscomparedtomr.thisshowsthatitisnotsufficienttosimplyrestartanappfromscratchwhenlackofprogressis detected though mr improves monkey by statement coverage monkey s statement coverage is shown in the third subcolumn of column state of the art of table .
state saving and restoring as well as other components substantially contribute to enhancing testing techniques it is not sufficienttosimplyrestartappfromscratchwhenlackofprogress is detected.
.
.
study testing effectiveness.
theresultsofstate of the arttechniquesareshownincolumn state of the art oftable1 st sa andmoindicatestoat sapienz and monkey respectively .
as can be seen timemachine achieves thehigheststatementcoverageonaverage andisfollowedby sapienz stoat andmonkey .figure6also shows that timemachine achieves the highest statement coverage for all15199 tm st199 tm sa1129199 tm mo st sa9140 st mo8121 sa mo figure7 comparisonoftotalnumberofuniquecrashesfor androtestapps.thedarkgreyareasindicatetheproportion of crashes found by both techniques.
four app size groups.
timemachine detects the most crashes as well followed by stoat sapienz and monkey .
thebetterresultsfrom timemachine canbeexplainedasfollows state level feedback accurately identifies which parts in app are inadequatelyexplored.moreoveraninadequatelyexploredstatecan bearbitrarilyanddeterministicallylaunchedforfurtherexploration via restoring a snapshot.
existing techniques typically observe programbehavioroveraneventsequencethatoftenisverylong and goes through many states.
coverage feedback of an individual state is unavailable.
so our time travel framework enhances app testing by providing fine grained state level coverage feedback.
timemachine achievesthehigheststatementcoverageanddetectsthemostcrasheson68benchmarkappscomparedtostateof the arttechniques.promisingly ourtime travelframework hasapotentialtoenhancestate of the artapptestingtoolsto achieve better results.
tostudyperformanceacrossapps foreachtechniqueunderevaluation we compute the number of apps on which the techniqueachieves the best performance.
in terms of statement coverage timemachine achieves the best performance on apps followed bysapienz 19apps stoat 11apps andmonkey 1app .fordetected crashes timemachine achieves the best performance on apps.
for stoat sapienz and monkey there are and 4apps respectively.
we further perform a pairwise comparison of detectedcrashesamongevaluatedtechniques.asshowninfigure7 thereislessthantenpercentoverlapbetweenthecrashesfoundby timemachine andstoat or timemachine andsapienz respectively.
the overlap with monkey is reasonably high.
about of unique crashes found by monkey are also found by timemachine h o w ever timemachine foundmanynewcrasheswhicharenotfound by monkey.
this analysis shows that timemachine can be used together with other state of the art android testing techniques to jointly cover more code and discover more crashes.
timemachine complements state of the art android testing techniques interms ofthe abilityto discovermore crashesand cover more code.
488table results from closed source industrial apps.
subject coverage crashes state name method tmst mo ms mr tmst mo ms mr tm autoscout24 besthairstyles crackle duolingo esfile explorer evernote excel filtersfor selfie flipboard floor plan creator fox news g.p.
newsstand golauncher z goodrx ibispaint x linecamera marvel comics match merriam webster mirror mybaby piano officesuite onenote pinterest quizlet singing speedometer spotify tripadvisor trivago watchespn wattpad webtoon wish word yelp zedge ave sum .
.
study closed source apps.
table shows results of closed source benchmark apps.
it is clear that timemachine achieves the highest method coverage and the most found crashes among all evaluated techniques.
comparedtobaselinemsandmr timemachine improvesmethod coverage to from and respectively.
note that the improvement of to is considerable since each app has methods on average and around to more methods are coveredforeachapp.intermsofnumberofcrashesfound timemachine detects .
times and .
times crashes more than ms and mr respectively.
ms detects crashes and mr detects crashes.
compared to state of the art techniques timemachine substantially outperforms them on both method coverage and the number of found crashes.
stoat achieves method coverage and detects 3crashes.monkeyachieves15 methodcoverageand20crashes.
unexpectedly stoat demonstrated the worst results worse than monkey.acloserinspectionrevealedthatthesereal worldappsuse complexuicontainers e.g.
animations whichposedifficultiesfor stoat to build a model.
certain app pages might be missed entirely becausetheeventhandlersassociatedwiththosewidgetscannotbetriggered.however both timemachine andmonkeyovercomethis issueduetotheir independence fromanyuimodels.wereportedthis matter to the authors of stoat who confirmed our insight.
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
figure statistics of tested apps from google play.
ourtime travelstrategysubstantiallyimprovetheexistingtechnique i.e.
ms bycoveringaround900moremethodsanddiscovering1.5timesmorecrashes.
timemachine alsooutperforms state of the art techniques stoat and monkey in terms of both method coverage and the number of found crashes.
outoftop 100instrumentedappsfromgoogleplay wesuccessfully tested apps.
the remaining apps kept crashing due to a self protection mechanism thoughtheyweresuccessfullyinstrumented .
as shown in figure the tested apps are quite diverse beingselectedfrommorethan10categories.itcomesasnosurprise thatthemajorityofthemarerankedwithover4stars andarebeing actively maintained.
inthe87apps wefound137uniquecrashes.theseareallnonnative crashes i.e.
their stack traces explicitly point to the source lineofthepotentialfaultsinthetestedapp.thedetected137crashes were caused by kinds of exceptions shown in figure .
the most common type is nullpointerexception.
intotal timemachine detects137uniquecrashesin25oftop100 google play apps.
.
.
analysis on state identification and time travel.
state identification.
the evaluation shows gui layout is appropriate to identify an app state.
this state abstraction generates acceptablenumberofstatesforanapp atthesametimesufficiently capturesfeaturesofastate.asweseecolumn state intable1 and table more states are discovered in apps with rich functionalityandlessstatesarediscoveredinsimpleapps.forinstance app anymemo with plentiful activities has found states and app frozenbubblewithfewactivitieshas15foundstates.similarresults can be found for industrial apps.
note that no login is providedin the evaluation such that timemachine might identify a small number of states for some large scale apps like k9mail.
gui layout sufficiently captures features of an app state.
on average 85statesarefoundinanopensourcebenchmarkapp and states are found in an industrial benchmark app.
frequency.
an automatic android testing tool that generates a verylongeventsequence likemonkey maygetstuckinloopsor dead ends.
we measure the number of times our time travelling infrastructureisemployedtounderstandhowoftenmonkeygets stuck.
in six hours over all runs and apps monkey gets stuck with a mean of .
and a median of .
times.
as we can see inthe box plots of figure there are generally less restores as the 4891k 3k apps 1k apps 3k apps all apps restores figure boxplots.
depending on app size how often does timemachine travel back in time?
apps get bigger.
this is due to an increasing state space.
a greater proportion of random actions lead to yet undiscovered states such thatprogressismaintained.
smallerappshaveasmallstatespace andmonkeymayrunmorequicklyintoloopsofstatesthathave already been observed.
on average timemachine travels about times per hour back to more progressive states that were observed in the past because monkey gets stuck in a loop or dead end.
cost.thecostoftime travelisacceptable.
timemachine spends around seconds taking a snapshot and seconds restoring a snapshot on an android7.
virtual machine with gb memory.
a snapshottakesaround1gbdiskspace.forlargescaleindustrial appsintheevaluation asessiontypicallygenerateslessthan100 snapshots.
so timemachine is able to run on a desktop in term ofconsumedstoragespace.besides sinceonesnapshotisstored foreach interesting state storagecanbepotentiallyreducedby re configuring the definition of interesting .
thisisareasonablecostforreachingaparticularstateinadeterministic way for android testing especially for large scale apps.
to reach a deep state a human tester may need to perform dozens of events and repeat them many timesdue to non determinism of androidapps.thisisevenmoredifficultforanautomatedtestgeneratorsbecauseittypicallyrequiresgeneratingaverylongevent sequenceautomatically.thesetoolsthusspendmoretimereaching hard to reach states than timemachine which makes reachability easier by recording and restoring snapshots of interesting states.
thecostoftime travellingisacceptable andalsoreasonablefor testing stateful programs.
threats to validity we adopted several strategies to enhance internal validity of our results.
to mitigate risks of selection bias we chose apps in a standardtestingbenchmarkwhichhasbeenusedinpreviousstudies .
in order to put no testing tool at a disadvantage weuseddefaultconfigurations providedtheexactsame starting condition and executed each tool several times and under the same workload.
to identify unique crashes we followed the stoatprotocol andalsomanuallycheckedthecrashesfound.
to measure coverage we used a standard coverage tool.we realise that our results on stoat versus sapienz and those reported in the stoat paper are vastly different.
we checked withtheauthorsofstoat onthismatter.theauthorsofstoat explain the disparity i by additional files they provided to the android device via sdcard and ii running of experiments at their end on a different machine intel xeon r cpu .50ghz cores 32gb ram with hardware acceleration.
additionally wetooktwomeasurementstoruleoutcrashesthat mightbecausedbyourtechniqueitself i.e.
artificialcrashes .first timemachine inserted a delay of ms between two events to avoidcrashesduetomonkeygeneratingmanyeventsinaextremely short time.
second we manually checked stack traces to filter out crashes due to state restoring issues such as inconsistent states.
finally ourtechniquetestsappsinavirtualmachineinstalled withandroid x86osanddoesnotsupportphysicaldevicesyet.for appsinteractingwitharemoteserver ourtechniquesaves restores only app states without considering remote server states.
related work the stream of works most closely related to ours is that of timetravel debugging .
time travel debugging allows the user to step back in time and to change the course of events.
theusercannowaskquestions suchas whatifthisvariablehad a different value earlier in the execution ?
now time travel testing has a similar motivation.
the tester can test the state ful app for various alternative sequences of events starting in any state.
this work was originally inspired by existing work on coveragebased greybox fuzzers cgf .
a cgf started with a seed corpus of initial inputs generates further inputs by fuzzing.
if ageneratedinputincreasescoverage itisaddedtotheseedcorpus.
similar to cgf our time travel enabled test generator maintains a statecorpus with states that can be restored and fuzzed as needed.
search based.
the most closely related automatic android test generationtechniquesemploysearch basedmethods.maoetal.developedamulti objectiveautomatedtestingtechniquesapienz .
sapienz adopts genetic algorithms to optimize randomly generated teststomaximizecodecoveragewhileminimizingtestlengths.evodroid thefirstsearch basedframeworkforandroidtesting extractstheinterfacemodelandacallgraphfromappundertest andusesthisinformationtoguidethecomputationalsearchprocess.
search basedapproachesareeasytodeployandhaveattracteda lot of attention from industry e.g.
sapienz has been used to test differentkindsofmobileappsatfacebook.ourwork timemachine takes a search based approach as well but instead of a population of input sequences it evolves a population of app states.
our work proposes a new perspective of app testing as state exploration and providesafeedback guidedalgorithmtoefficientlyexploreanapp s state space.
random.
oneofthemostefficientapproachesfortestingandroid apps is the random generation of event sequences .
apps are exercised by injecting arbitrary or contextual events.
monkey isgoogle sofficialtestingtoolforandroidapps whichis built into the android platforms and widely used by developers.
monkeygeneratesrandomusereventssuchasclicks touches or gestures aswellasanumberofsystem levelevents.dynodroid employs a feedback directed random testing approach with two 490strategies biasedrandom favors events related to the current context and frequency is biased towards less frequently used events.
although random testing has gained popularity because of its ease of use it suffers from an early saturation effect i.e.
it quickly stops making progress e.g.
no new code is executed after certainnumberofeventexecutions.fromthispointofview our work powers random testing with the ability to jump to a progressive state observed in the past when there is no progress.
thus an early saturation can be avoided.
model based.
another popular approach of android apps testing is model based testing.
app event sequences are generated according to models which are manually constructed or extracted from project artefacts such as source code xml configuration files and ui runtime state.
ape leverages runtime information to evolve an initial gui model to achieve more precise models.
stoat assignswidgetsinaguimodelwithdifferentprobabilities of being selected during testing and adjusts them based on feedback such as code coverage to explore uncovered models.
androidripper usesadepth firstsearchovertheuserinterfaceto buildamodel.a3e exploresappswithtwostrategies targeted exploration whichprioritizesexplorationofactivitiesthatarereachablefromtheinitialactivityonastaticactivitytransitiongraph and depth first exploration which systematically exercises user interfaceindepth firstorder.droidbot orbit andpuma use static and dynamic analysis to build basic gui models fromapp under test on top of which different exploration strategies canbedeveloped.model basedapproacheshaveattractedagreat dealofattentioninthisfieldbecausetheyallowtorepresentapp behavior as a model on which various exploration strategies can be applied.
however complex widgets e.g animation commonlyusedinmodernappsposedifficultiesonmodelconstruction leading to an incomplete model.
combining model based approaches with othertechniquessuchasrandomtestingcanbeapromisingoption for android apps testing.
learning based.
adifferentlineofworkusesmachinelearningtotestandroidapps.liuetal.
useamodellearnedfrom amanuallycrafteddataset includingmanualtextinputsandassociated contexts to produce text inputs that are relevantto the current context during app testing.
for instance it would use a nameforanexistingcitywhengeneratinganinputforasearchbox ifthereisanearbyitemlabeled weather.wuji employsevolutionaryalgorithmsanddeepreinforcementlearningtoexplore the state space of a game under test.
swifthand uses machine learning to learn a model of the user interface and uses this model todiscoverunexploredstates.thetechniquebydegottetal.
leverages reinforcement learning to identify valid interactions for a gui element e.g.
a button allows to beclicked but not dragged and uses this information to guide execution.
humanoid takes manualeventsequencesandtheircorrespondinguiscreenstolearn a model and uses the model to predict human like interactions for an app screen.
machine learning is typically applied to resolve specificchallengeintheeventsequencegeneration suchasgenerating contextual text inputs or identifying possible types of input events thatcanbeexecuteduponaguielement.incontrast ourworkfeaturesafullyautomatedandroideventsequencegenerator.some components in timemachine such as identifying an interesting state might benefit from machine learning since learning basedapproacheshaveshowntobeeffectiveforsimilarissues.itisworth exploring this direction in future work.
program analysis based.
several existing approaches employ program analysis when testing android apps.
acteve uses symbolic execution to compute enabled input events in a given app stateand systematically exploresthe stateby triggering these events.
synthesise leverages concolic execution and program synthesistoautomaticallygeneratemodelsforandroidlibrarycalls.
crashscope combines static and dynamic analysis to generate an event sequence that is used to reproduce a crash.
similarly intellidriod uses static and dynamic analysis to generate an event sequence thatleads execution to aspecified api invocation.
thor executesanexistingtestsuiteunderadverseconditionsto discoverunexpected appbehavior.such programanalysisprovides detailed information about the app which can help to guide testsequence generation.
at the same time intrinsic limitations ofprogram analysis such as poor scalability create an impedimentto easy and widely applicable automation.
in contrast our work timemachine requireslittleinformationfromtheappundertest and is widely applicable to variety of android apps as shown by our experiments.
conclusion androidapptestingisawell studiedtopic.inthispaper wedevelop time travelapptestingwhichleveragesvirtualizationtechnology toenhancetestingtechniqueswiththeabilitytocapturesnapshots of the system state state of the app and all of its environment.
capturingsnapshotsfacilitatesourtime travel enabledtestingtool to visit arbitrary states discovered earlier that have the potential totriggernewprogrambehavior.state levelfeedbackallowsour technique to accurately identify progressive states and travel to them for maximizing progress in terms of code coverage and state space exploration.
our time travel strategy enhances a testing technique monkey extended with stoat s system level generator by achieving .15times more statement coverage and discovering .
times morecrashes on the androtest benchmark.
moreover timemachine outperformsotherstate of the arttechniques sapienzandstoat byachievingthehighestcoverageonaverageandthemostfound crashes in total.
on large industrial apps timemachine covers around more methods on average and discover .
times more unique crashes over the baseline tool at the same time outperformsstate of the arttechniquesaswell.ourtool timemachine also reveals a large number of crashes owing to a wide varietyof exceptions nine different kinds of exceptions in real life top popular apps from google play.