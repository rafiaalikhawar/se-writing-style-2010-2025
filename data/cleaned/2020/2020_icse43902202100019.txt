studying test annotation maintenance in the wild dong jae kim nikolaos tsantalisy tse hsun peter chen jinqiu yangy software peformance analysis and reliability spear lab concordia university montreal canada ydepartment of computer science and software engineering concordia university montreal canada fkdongja nikolaos peterc jinqiuyg encs.concordia.ca abstract since the introduction of annotations in java the majority of testing frameworks such as junit testng and mockito have adopted annotations in their core design.
this adoption affected the testing practices in every step of the test lifecycle from fixture setup and test execution to fixture teardown.
despite the importance of test annotations most research on test maintenance has mainly focused on test code quality and test assertions.
as a result there is little empirical evidence on the evolution and maintenance of test annotations.
to fill this gap we perform the first fine grained empirical study on annotation changes.
we developed a tool to mine commits and detect instances of test annotation changes from open source java projects.
our main findings are test annotation changes are more frequent than rename and type change refactorings.
we recover various migration efforts within the same testing framework or between different frameworks by analyzing common annotation replacement patterns.
we create a taxonomy by manually inspecting and classifying a sample of test annotation changes and documenting the motivations driving these changes.
finally we present a list of actionable implications for developers researchers and framework designers.
index terms software quality empirical study annotation software evolution i. i ntroduction modern software systems are becoming more complex due to the ever growing demands from customers.
to ensure that the software quality remains on par with consumer expectations testing has become a pivotal role in software development.
developers rely on testing to verify the quality of every code change and provide an indication on whether the software can be released to production .
to increase the effectiveness of testing developers need to maintain and improve test code continuously.
similar to source code test code may also contain design issues that hinder the quality.
for example prior studies have found that the results of some test cases can be unreliable i.e.
flaky tests due to bugs in test code .
to that end developers have begun to notice a recurring design problem in test code and coined the term test smells as an indicator of design problems in tests.
since its inception researchers have shown that test smells are prevalent in software systems negatively affect software maintainability and comprehension and may impact software quality in terms of post release defects .
the introduction of annotations in java has driven annotation as a critical component of the many java based frameworks influencing how developers to design and imple ment software.
even in software testing frameworks such as junit testng and mockito have all adopted annotations as critical ingredients in test design and implementation.
a prior study has found that junit4 is one of the most widely utilized testing frameworks for java based systems and test annotations e.g.
test are also one of the most widely used annotations in java development.
table i provides an overview of the commonly used test annotations in junit4.
although the test annotations may be different across testing frameworks e.g.
testng or junit5 in general they provide similar functionalities.
despite the importance of test annotations most prior research on test maintenance has only focused on general test design and test assertions and has not considered the peculiarity of test annotations.
therefore in this paper we present the first empirical study on how developers leverage test annotations in the wild to maintain the high quality of test code e.g.
readability test flakiness test performance obsolete test .
we first extended the state ofthe art refactoring mining tool refactoringminer .
to detect annotation additions removals and modifications.
we study the collected annotation changes both quantitatively and qualitatively by answering three research questions rq1 how common are test annotation changes?
test annotation changes are .
more common than regular test refactorings such as renames and type changes.
despite their popularity there is negligible tool support e.g.
antipattern detection annotation change suggestions and annotation api usages for test annotation changes.
rq2 how are test annotations changed in the wild?
we quantitatively study frequent test annotation changes.
we find that developers update test annotations more frequently than annotation additions and removals.
moreover test annotation migration across testing frameworks and within a different version of the same framework is also common.
rq3 why do developers change test annotations?
we qualitatively uncover test annotation usage and misusage and how developers bypass the limitations of test annotations.
our findings highlight potential future directions on helping developers improve test maintenance and detect potential issues in test code.
in summary our findings provide actionable implications for three groups of audiences researchers we open an avenue for further research622021 ieee acm 43rd international conference on software engineering icse .
ieee table i abrief overview on commonly used junit4annotations .
annotations annotation location description of commonly used junit annotation rule field rule provides a mechanism to enhance tests by running some code around a test case execution which is similar to fixture and teardown.
parameterize field method test case annotated with parameterize can be invoked by using a predefined input i.e.
parameterized test inputs and expected output.
test method test indicates that the annotated test code should be executed as a test case.
test takes optional parameters such as timeout to indicate that the test should finish within a given time or exception to indicate that the test should throw an exception.
before after method before indicates that the annotated test code should be executed as a precondition before each test case i.e.
database setup .
similarly after indicates the execution of the annotated test code as a postcondition after each test case.
beforeclass afterclassmethod beforeclass and afterclass are similar to before and after annotation types but indicate the annotated test code to only execute once i.e.
before or after the test class is invoked .
ignore method class ignore indicates that the annotated test case should not execute.
category method class category provides a mechanism to label and group tests giving developers the option to include or exclude groups from execution.
test timeout x method class a test will fail if its execution takes longer than the value x specified in timeout.
directions on detecting misuses and test smells related to test annotations and their relation with other aspects of software development i.e.
quality maintainability and performance improvements .
we also highlight potential directions on automated test code refactoring by leveraging test annotations.
developers our findings reveal the usage of test annotations in an ad hoc manner by some developers.
a number of test cases is temporarily disabled until a fix is found however the disabled tests are not re enabled after the fix.
moreover in several cases developers are unaware of the features offered by testing frameworks and thus apply suboptimal custom solutions.
these findings indicate the need to educate developers with the best testing practices and provide recommendation tools to help developers apply the appropriate test annotations where needed.
framework designers we find cases where developers try to bypass the current limitations of test annotations i.e.
fixture configuration and provide suggestions for framework designers on improving the flexibility of test annotations.
paper organization.
section ii surveys related work.
section iii discusses our extension on refactoringminer .
and provides preliminary results.
section iv presents our quantitative analysis results and section v presents our qualitative analysis results.
section vi summarizes the implication of our findings.
section vii discusses threats to validity.
section viii concludes the paper.
ii.
r elated work in this section we discuss prior research in the areas of test maintenance and evolution java annotation usage and test framework usage.
test maintenance and evolution.
researchers have been studying testing practice especially on test quality and evolution.
pinto et al.
studied how the test code evolves in practice for more effective automatic program repair.
zaidman et al.
studied how the test code co evolves with production code.
bavota et al.
found that test smells are prevalent in software systems and may hinder test comprehension and maintenance.
ma ayan et al.
quantitatively studied the language features of junit assertions and identified opportunities for improvement.
researchers further correlated assertions with defects and test suite effectiveness .
our work is the first to study the maintenance of test annotations and document test annotation usages misuses and limitations.empirical studies on java annotations.
rocha et al.
mined open source java systems to investigate annotation usage empirically.
similarly dyer et al.
analyzed 31k open sourced projects to analyze how various java language features are adopted by developers including the adoption of java annotations.
their study shows that java annotations are very commonly adopted.
parnin et al.
used opensource java projects to study the adoption of java generics and contrast it with the adoption of annotations.
this study shows that a champion leads annotation adoption in a development team.
yu et al.
performed the first largescale empirical study about java annotation usage evolution and impact without focusing on test related annotations.
in contrast to previous studies on java annotation usage we perform the first in depth study on how test annotations from various frameworks are utilized and maintained and derive the first taxonomy of annotation changes.
moreover our study yields actionable implications for researchers developers and testing framework designers to further expand and improve test annotation practices.
testing framework usage.
researchers have been investigating how developers use various testing and mocking frameworks in open source systems.
zaraouili and mens performed one of the first studies on the evolution of the testing frameworks .
their study shows that junit was the most prominent testing library while many libraries are used simultaneously e.g.
powermock mockito and easymock complement each other.
researchers have conducted quantitative studies to understand the adoption of mocking frameworks in mocking file dependencies and in general use .
more recently spadini et al.
performed a comprehensive study on how and why developers use mocking in test code and how mocking evolves over time.
different from prior work we zoom in and perform an in depth analysis at a more finegrained level i.e.
annotation changes at commit level to understand how test annotations are evolved and maintained in practice.
iii.
m ethodology refactoringminer extension.
in order to detect annotation additions removals and modifications we extended the stateof the art refactoring mining tool refactoringminer .
.
we selected this tool for the following reasons it operates at commit level allowing us to obtain annotation changes at the finest granularity level of software evolution i.e.
commits .
it can detect refactoring operations allowing us to include annotation changes for refactored program elements i.e.
methods with changes in their signatures moved renamed classes and fields in addition to non refactored program elements.
this makes our dataset more complete and our findings more reliable.
it has the highest precision .
and recall among other refactoring mining and ast diff tools allowing us to have an accurate dataset of annotation changes with a very small number of false positives and false negatives.
it has the fastest execution time among other refactoring mining tools allowing us to scale up our data collection for the entire commit history of large projects with over 20k commits.
using the refactoringminer api we obtain the pairs of program elements i.e.
type method and field declarations which have been matched between the currently analyzed commit and its parent in the directed acyclic graph that models the commit history of git based version control repositories.
the pairs of matched program elements may have identical signatures e.g.
a pair of methods with identical names parameter and return types or may have different signatures due to refactoring operations e.g.
r ename method change parameter type add delete parameter .
java annotations are used in three different forms marker annotations without member value pairs typename.
normal annotations with a list of member value pairs typename name1 value1 name2 value2 .. where names are simplename ast nodes and values areexpression ast nodes.
single member annotations with a single member value typename expression where the member name is omitted i.e.
foo bar is equivalent to the normal annotation foo name bar .
we consider two annotations as equal if they have the same typename and the same member value pairs regardless of their order.
let us assume that for a given pair of matched program elements apis the annotation set of the program element in the parent commit and acis the annotation set of the matched program element in the child commit.
then the added annotations are computed as a acn ap ac .
the removed annotations are computed as a apn ap ac .
the pairs of modified annotations are computed as a f ap ac jap2ap ac2ac ap typename ac typename ap membervaluepairs ac membervaluepairsg.
to evaluate the precision and recall of our refactoringminer extension we extended the oracle used in which contains true refactoring instances found in commits from open source github projects with instances of annotationtable ii precision and recall of our extended version of refactoring miner .
change type tp fp fn precision recall add method annotation .
.
remove method annotation modify method annotation add parameter annotation remove parameter annotation modify parameter annotation add field annotation .
remove field annotation modify field annotation add class annotation remove class annotation modify class annotation overall .
.
additions removals modifications for four different program elements namely type method field and parameter declarations.
to compute precision an author of the paper manually validated annotation change instances reported by our refactoringminer extension.
to compute recall we need to find all true instances of annotation changes.
we followed the same approach as in by executing a second tool namely gumtree and considering as the ground truth the union of the true positives reported by refactoringminer and gumtree.
gumtree takes as input two abstract syntax trees e.g.
java compilation units and produces the shortest possible edit script to convert one tree to another.
we used all insert anddelete edit operations on annotation ast nodes to extract annotation changes and report them in the same format used by refactoringminer.
table ii shows the number of true positives tp false positives fp and false negatives fn detected missed by our refactoringminer extension.
the overall precision is .
and the recall is .
.
studied systems.
we choose the studied systems by following three selection criteria.
first we selected the top java projects on github ordered by popularity i.e.
stargazer count .
we also made sure that the repositories are not forks.
second we discarded projects that are below percentile in terms of size i.e.
lines of code repository popularity i.e.
stars and the number of commits.
finally we discarded inactive repositories that did not have any commits in .
we ended up with systems i.e.
druid hadoop cassandra storm flink hbase camel hive openfire ambari orientdb and kafka.
these studied systems cover different domains ranging from distributed databases stream processing frameworks message brokers and groupchat servers.
table iii shows an overview of the studied systems.
our study focuses on test annotation usage but there may be some non test related annotations in test classes.
hence we set off to understand what are the common testing frameworks or libraries from which test related annotations are used.
we mined all annotation usages in the versions released in and respectively for the studied systems.
in particular we analyzed all test files that have a .java extension and est s as prefix or suffix in their name.
we manually verify the build configuration files e.g.
maven or gradle build file of the studied systems to use the default heuristic 64table iii an overview of the studied systems from to2020 .
systems total test loc no.
test method no.
test class !
!
!
ambari 125k !
.8k !
!
camel 562k!787k !
!
cassandra .3k !189k !
!
druid 45k!307k !
!
flink 79k!437k !
!
hadoop 480k!914k !
!
hbase 185k!359k !
!
hive 124k!323k !
!
ignite 261k!99k !
!
kafka .9k!191k !
!
openfire .2k!
.3k !
!
storm .7k!47k !
!
total 1916k !3939k !
!
table iv use of annotations from different frameworks in testcode released in and respectively .
framework type frequency proportion testing framework !
.
!
junit !
!
testng !
.
!
mocking framework !
!
mockito !
!
powermock !
!
easymock !
!
java lang annotations !
!
custom annotation !
.
!
spring framework !
!
other libraries !
!
e.g.
google javax specified by maven gradle plugin to identify test files.
after collecting the annotation usages we manually study them and identify their corresponding framework.
table iv summarizes the annotation usage of different frameworks in the studied systems.
we find that junit annotations are the most commonly used annotations in test code accounting for of the mined annotations in and in .
testng is a less commonly used testing framework accounting for .
of the mined annotations in and in .
our finding shows that developers in the studied systems are migrating away from testng.
we also found annotations from frameworks used for test mocking i.e.
mockito powermock and easymock the spring framework and other non test related libraries.
the annotation usage of testing frameworks such as junit and mocking frameworks increases significantly over the years.
however their percentages decrease due to the increasing use of custom annotations.
in section vi we discuss some of the custom annotations related to testing.
to collect annotation changes we run our refactoringminer extension on every commit that modified at least one java test file between for the studied systems.
we only keep changes on test related annotations i.e.
from junit testng and mocking frameworks and discard the rest.
in total we mined test related annotation changes in the commit history of the studied systems from to .
iv.
a q uantitative study on testannotations in this section we conduct a quantitative study to understand the prevalence of test annotation usages and change patterns.
in particular we answer two research questions table v mined testcode changes in c ommits .
field method class total num.
per per per commits commit commit commit annotation changes .
.
.
refactoring .
.
.
percentage .
.
.
.
.
.
.
.
rq1 how common are test annotation changes?
as a stepping stone to understanding how developers leverage annotations we examine how frequently test annotations are changed compared to common source code changes i.e.
renames and type changes at the same program element level.
rq2 how are test annotations changed in the wild?
we examine what are the common test annotation change patterns in the wild.
studying predominant annotation changes reveals frequent maintainability activities developers perform through test annotation changes as software evolves.
such insights act as stepping stones for our subsequent qualitative study on the motivations and challenges behind test annotation changes.
rq1 how common are test annotation changes?
we study how frequently developers change test annotations.
to provide some comparative statistics we show the prevalence of test annotation changes compared to common source code transformations i.e.
renames and type changes at the same program element level i.e.
class method and field declaration .
in particular we compare test annotation changes at the method level with rename method and change return type at field level with rename field and change field type and at the class level with rename class.
such a comparison is attainable because all compared changes are performed on the same kind of program elements.
we used the tool implemented by ketkar et al.
to detect renames and type changes.
ketkar et al.
report an average precision of .
and a recall of .
for type change detection and an average precision of and recall of for rename detection which is very close to the precision recall values reported in table ii allowing for a fair comparison annotation change and refactoring practices.
table v compares the prevalence of test annotation changes with that of the refactoring changes in test code from the commits.
as shown in table v the number of test annotation changes is comparable to the number of test refactorings i.e.
.
difference .
test annotation changes are performed at a method and class level more than renames and type changes i.e.
.
and .
more respectively.
however at the field level test annotation changes occurred less than renames and type changes.
despite the popularity of test annotation changes little tool support exists for test annotations compared to common code transformations such as renames and type changes.
we find that much fewer commits modify test annotations than those that perform renames and type changes.
out of the commits commits modify test annotations and perform code transformations such as renaming and type changes.
once normalized by the number 65table vi quantitative analysis top three highest frequency of annotation addition removal and modification .
addition freq.
removal freq.
modification freq.
field level mock rule mock rule mock parameter parameter classrule parameterized method level ignore ignore test before before parameterized after beforeclass parameters class level runwith ignore category category runwith runwith ignore category preparefortest table vii quantitative analysis of annotation replacements .
granularity annotation changes freq.
total field level .
junit rule classrule junit4!junit5 classrule rule !
registerextension classrule!
container rule!
tempdir method level junit beforeclass afterclass !
before after before after!
beforeclass afterclass before after!
after before parameters parameterized timeout x in test !
timeout beforeclass!
afterclass junit4!junit5 before!
beforeeach after!
aftereach beforeclass!
beforeall afterclass!
afterall ignore!
disabled testng!junit beforemethod aftermethod !
before after test enabled false !
ignore beforetest!
before custom junit testtag!
category .
category performancetest.class !
performancetest2 testng beforetest !
beforeclass beforemethod .
aftertest!
afterclass aftermethod afterclass!
aftermethod aftermethod!
beforemethod beforeclass!
beforemethod junit4!
springboot category!
integrationtest .
class level .
junit4!junit5 ignore!
disabled .
runwith!
extendwith ignore!
category junit4!
springboot category!
integrationtest total of commits test annotation changes at class and method level are performed much more frequently than renames and type changes.
this shows that test annotation changes at class and method levels are more concentrated in fewer commits suggesting that annotations may be associated with dedicated maintenance activities.
in rq3 we will further discuss ways annotations are utilized in the maintenance of test code.
test annotation changes are comparable to renames and type changes at the same program element level and are even more frequently applied at the method and class level.
despite the popularity there is currently negligible tool support i.e.
antipattern detection annotation change suggestions and annotation api usages for test annotation changes.
rq2 how are test annotations changed in the wild?
we present the quantitative analysis on test annotation change patterns from two aspects.
first we present the rawchange patterns based on three types of changes addition removal and modification.
the three types of changes are the direct output from our refactoringminer extension.
table vi lists the top three annotation changes per change type at three program levels i.e.
field class and method .
we observe that modification has a strong prevalence at a method and class level across the three types of changes.
developers frequently update parameters of the test e.g.
timeout x and category annotations.
in general we notice that developers frequently change the test annotations from mocking frameworks i.e.
add mock and runwith of powermockrunner of mockitojunitrunner .
considering the low prevalence of mocking frameworks in tests around as shown in table iii this suggests that the mocking frameworks are frequently updated as code evolves.
we also find that developers frequently add and modify parameter parameterized and runwith of parameterized annotations suggesting that expanding test input and diversifying test execution settings is commonly leveraged to facilitate code evolution.
we also observe a high prevalence of adding and removing the ignore annotation at both method and class level i.e.
disabling and enabling test cases and test classes .
this suggests that technical debt may occur in test code evolution.
lastly we observe a large number of modifications for the category annotation at the class level to organize test classes into groups and a diverse set of fixture additions and deletions i.e.
before after beforeclass .
furthermore we perform an in depth analysis to reveal a composite change pattern i.e.
an annotation replacement.
a replacement x!
y occurs when annotation x is removed and y is added on the same program element and commit.
annotation replacements may happen within one testing framework or between different testing frameworks.
replacements show that as software evolves the original test annotation or framework does not satisfy the testing needs.
therefore developers may look for alternatives.
however such alternatives are not directly provided or are hard to achieve in the current framework.
hence developers need to compromise with workarounds or adopt another framework.
mining annotation replacements is straightforward based on the output of our refactoringminer extension.
specifically for each commit we match pairs of removed and added annotations on the same program elements i.e.
fields methods and classes and ensure that these pairs involve different annotation types.
in total we mined replacements from all mined annotation changes.
table vii shows the frequencies of different replacement patterns.
most of the replacements are at the method level and of the replacements are switching between junit fixtures.
for example developers replace beforeclass afterclass with before after or vice versa to configure the setup and tear down phases at the test class or test case level.
similarly developers replace rule with classrule at field level to expand the impact of a rule to the entire class.
at all program element levels we notice that many replacements occur due to migrations i.e.
between different testing frameworks or from junit4 to 66junit5.
interestingly we observe a few replacements between different frameworks which are not due to migrations.
developers may find a similar test annotation in springboot more suitable for a particular development need than the junit category annotation.
another common case is to replace custom annotations with the junit ones.
developers may define custom annotations for particular needs as junit may not yet support the desired features or developers may not be aware of such support by junit.
when the developers become aware of the unused junit features or the desired features are shipped in the next junit releases they tend to replace their custom annotations.
our study shows that developers modify test annotations more frequently compared to additions and removals.
annotations from mocking frameworks are commonly changed despite their low prevalence.
further analysis of annotation replacements shows that they commonly occur for migrating to newer framework versions or other frameworks.
v. a q ualitative study on testannotations in this section we conduct a qualitative study to understand the reasons that developers change annotations by answering the following research question rq3 why do developers change test annotations?
our goal is to provide suggestions to researchers practitioners and framework designers on opportunities to improve test annotations.
we derive a taxonomy of test annotation changes representing distinctive test maintenance efforts.
we believe our taxonomy will provide insights on the maintenance of test annotations and how to improve test quality.
rq3 why do developers change test annotations?
we manually study and understand the reasons that developers change test annotations.
our manual study is composed of the following phases phase i we use stratified random sampling with a confidence level and confidence interval to acquire samples from the test annotation changes identified by our refactoringminer extension.
we adopted stratified random sampling to sample each studied system independently to reduce sampling error when a sub population within the overall population varies .
phase ii to create the taxonomy for the test annotations we first classified the changes at a high level based on the annotation type e.g.
ignore .
then the first two authors of the paper a1 and a2 independently derived an initial list of the reasons behind annotation changes by manually inspecting the relevant commit messages test source codes and bug reports.
phase iii authors a1 and a2 unified the derived reasons and compared the assigned reason for each annotation change.
any disagreement was discussed until reaching a consensus.
the inter rater agreement of the coding process has a cohen s kappa of .
indicating almost a perfect agreement level .table viii shows the derived taxonomy of the reasons that developers changed the junit annotations upper half of the table and the annotations from other frameworks lower half of the table that we found in the sample.
to encourage the replication of our results we have made the dataset available.
junit test annotation changes ignore .
ignore is the most frequently changed test annotation mostly added .
developers often use this annotation to temporarily disable the execution of tests when there are software bugs or flaky tests.
developers may also bypass test failures caused by recent code changes during a feature addition that breaks a test.
for example in hive 7f4a3e17 the developer ignored the failing test code due to breaking changes during feature addition to pass the test temporarily.
other instances of adding ignore are due to dependencies with external libraries.
for example developers disabled a test while waiting for a new software version e.g.
jdk update .
however developers may also add ignore to replace automated testing with manual testing when the test code requires manual startup.
although developers frequently ignore tests to facilitate maintenance difficulties this practice may become ad hoc and affect code quality.
we found instances where developers use ignore to pass failing tests without fixing the issue in the code.
for example in druid da32e1ae the developer disabled a test due to unknown failure.
later on the bug persisted but the test was enabled and the issue was closed.
similarly in camel 8ba68e34 the developer disabled a test due to external dependencies during feature addition.
however the ignored test is never enabled.
we further conducted an exploratory investigation to see whether the ignored flaky tests in table viii are fixed and later enabled.
we find that developers often do not find the root cause of test flakiness and ignore the test in the entirety of software evolution indicating that ignored tests persist and are often forgotten.as ignore becomes a common way to bypass challenges in test maintenance it may become ad hoc and a source of technical debt.
test timeout x .
our analysis reveals innovative uses of timeout and maintenance problems of such uses due to software s ever evolving nature.
we find that timeout is employed to achieve various goals i.e.
detecting deadlocks e.g.
hbase 2428c5f and performance regressions e.g.
hadoop f131dba8 and providing meaningful debugging information when accessing external resources.
while timeout is an effective tactic to serve the aforementioned purposes we observe that developers constantly need to increase the timeout threshold even by removing the use of timeout entirely to avoid test failures and to accommodate the evolving software development.
for example once the running environment changes e.g.
to a slower cluster or platform test execution may become slower and lead to timeout errors.
developers need to increase the timeout threshold to avoid test failures.
67table viii qualitative analysis taxonomy of annotation changes .
annotation type motivation frequency junit test annotations changes ignore bugs adding ignore to bypass test failure caused by bugs in test source code.
flaky test adding ignore to disable flaky tests.
external dependency adding ignore when an external dependency needs to be manually configured e.g.
database or the developers wait for a new release of an external dependency to resolve an issue e.g.
jvm .
feature addition improvement adding ignore to disable tests that are related to incomplete features code changes.
timeout relax timeout increasing test timeout thresholds to accommodate slow cluster slow machine or slow tests.
deadlock detection adding test timeout to help detect deadlocks i.e.
if tests do not finish within the specified time there may be a deadlock .
external resource retrieval adding test timeout to complement tests that retrieve an external resource.
for example without a timeout the test may fail due to nullpointerexception and suppress the actual fault e.g.
resource unavailability .
perf.
regression detection adding test timeout to ensure that the test finishes on time for detecting performance regression.
ad hoc timeout removal removing test timeout completely as tests become too slow instead of relaxing the timeout.
fixture reset fixtures replacing beforeclass with before to reset fixtures for each test case e.g.
for bug fixing or test case isolation .
improve test speed replacing before with beforeclass to improve test time by removing unnecessarily repeated fixture initialization.
inflexible configuration removing or changing fixtures since they are not configurable per test case e.g.
before method runs for every test case while beforeclass runs only once before a test case .
maintainability adding fixtures to remove duplicate initialization in the test code for better maintainability.
category test prioritization adding category to group tests based on their speed size to detect failures more quickly e.g.
run faster tests first .
ignore tests adding category in addition to ignore to organize ignored tests for future maintainability.
parameterized increase test coverage adding or changing parameterized to increase coverage for failing corner cases or newly added features.
refactor test code adding parameterize to refactor tests to improve maintainability e.g.
share common test inputs or test code .
parallelize tests adding parameterized and use a thread pool to run tests in parallel and speed up test execution.
add debugging messages changing parameterized parameter to include optional messages for improved debugging.
slow test removing parameterized to improve test execution time.
in junit4 parameterized is limited to class level.
if only subsets of tests use parameterized annotations then it may increase test execution time.
expected exception adjusting exception handling changing between different exception handling mechanisms i.e.
junit3 try with fail junit4 rule junit5 assertions.assertthrows junit4 expectedexception or even custom expected exception test driven development adding expected exception to complement test driven development by making the test pass with known exceptions until the feature implementation is done.
exception too general changing expected exception from a general exception type to a more specialized one.
for example expectedexception genericexception can pass the expected exception test but does not provide details about the actual exception type.
rule refactor via rule adding built in or custom e.g.
extract duplicate fixture rule to improve test code maintainability.
fixed test order adding fixmethodorder from junit4 to enforce deterministic test orders and fix flaky tests.
other types of test annotations changes migration junit4 tojunit5 migration manual migration from junit4 tojunit5 e.g.
one package at a time resulting in sparse junit5 adoption.
typically migrated annotations are related to fixtures i.e.
before to beforeeach and sometimes from runwith to extendwith or from category to tags.
junit3 tojunit4 migration automated migration from junit3 tojunit4 using tool support.
testng tojunit5 remove testng in favour of junit5 due to its popularity.
mocking namespace error in mocking mocking frameworks such as powermock utilize an independent classloader which may cause namespace error i.e.
class not found .
mock usage adding powermock to mock final utility and abstract class.
mock vsinjection removing mocking and use dependency injection instead.
custom retry on failure exception implementing custom annotations to retry test on failure junit4 .
developers should leverage junit5 repeatedtest experimental implement custom annotation to categorize tests during feature addition to detect untested code instead of using junit4 category.
mixed framework usage using dependson in testngusing testng in addition to junit because testng provides the dependson annotation which enforces test ordering.
however developers can leverage junit4 fixmethodorder to avoid using multiple frameworks.
using test group x in testngusing testng in addition to junit because testng provides the group option inside test annotation which categorizes tests.
however developers can leverage junit4 category or junit5 tag to avoid using multiple frameworks.
by product changing adding removing test annotation due to feature deletion or test code relocation.
another example is that detecting performance regressions may become flaky as code evolves i.e.
the execution time comes closer to the timeout threshold and leads to unstable test results in different runs.
to avoid flakiness developers may increase the timeout threshold.
this shows that timeout may not be suitable for performance testing and a framework e.g.
openjdk jmh that allows more sophisticated settings e.g.
repeated runs warm up iterations should be leveraged.
developers use test timeout x to detect concurrency issues or performance regressions however they may also relax remove the timeout when performance regressions occur.
fixtures .
.
our fixture change analysis reveals the inherent difficulties and error proneness in maintaining the balancebetween a clean test environment and minimized test execution time.
to minimize test execution time developers may continuously refactor test initialization code i.e.
extracting duplicate initialization code in a separate method with fixture annotations or changing a fixture method from before to beforeclass druid da32e1ae .
however some fixture code e.g.
resetting shared variables if incorrectly placed in a class fixture i.e.
beforeclass may violate the test independence assumption introduce bugs in test code and produce unstable test results i.e.
flaky test .
therefore developers may perform changes to execute such a fixture code for each test method repeatedly e.g.
ambari 7153112e .
interestingly we noticed that developers expressed performance concerns on such changes and sometimes they were even uncertain about whether such changes would completely fix the buggy 68test.
hence developers may benefit from having a detection tool that helps them determine the trade off e.g.
a searchbased approach .
moreover we observe that developers need to perform workarounds due to the limitations of expressing fixtures in junit.
in particular developers may remove junit fixture to resort to a direct call to the parameterized helper methods increasing redundancies.
this happens when there is a need to adopt different fixtures for each test method.
however junit does not support tailoring fixture i.e.
all the test cases in one test class share the same fixture.
another limitation is the lack of fine tuning junit fixtures based on the test cases.
for example in updateactiverepoversiononstartuptest ambari 2700bd125f developers replaced junit before with a parameterized helper method to conditionally configure the cluster in the fixture based on test cases.
we find that developers performed such workarounds due to junit limitations complicate test fixture and may increase test maintenance overhead.developers are concerned about the tradeoff between minimizing test code duplication and minimizing test execution time.
furthermore the lack of configuration capabilities in junit fixtures causes developers to perform workarounds increasing technical debt and maintenance overhead.
category .
we find that developers mostly add category to categorize tests for test prioritization.
in hbase the category groups test based on timeout thresholds.
developers acknowledge that categorizing based on timeout can improve regression testing practice by running faster tests first i.e.
the ones with smaller timeout threshold to detect bugs quickly.
we also find that developers add category to complement ignored tests for better maintainability.
for instance a failingtest category can indicate ignored tests due to test failure.
developers may also use category to specify whether certain tests are integration tests or unit tests.developers customize category to assist diverse maintenance needs.
we find that categorization based on test execution time can be useful for efficient regression test analysis and can also help detect failed or ignored tests for more reliable maintainability.
parameterized .
.
we find that most changes to parameterized align to its regular use i.e.
increase the flexibility of managing test inputs.
developers may add more corner cases to the input using parameterized after adding new features or fixing bugs to avoid regression.
we also find that developers may refactor the test code using parameterized.
we find cases where developers use parameterized to reuse common test input arguments in different test cases.
for example in a test from hadoop ad1b988a8286 developers use parameterized to remove multiple subclasses that share the same code with only differences in test input.
we also find that developers may use parameterized to run tests with different inputs in parallel to speed up test execution.
however one limitation of parameterized is that it can only be applied at the class level.
therefore if only a subset ofthe test methods needs the parameterized annotation there may be additional setup overhead that slows down the test.
finally the junit4 parameterized class contains an optional string pattern that helps decorate test results with additional messages for improved debugging.
including such messages is considered the best practice by some developers as we found in our manual study.
nevertheless most of our manual study samples do not leverage this best practice and thus we believe users of junit4 can benefit from knowing about it.
parameterized is used to enable test code to take arguments and refactor test code and test input duplications.
moreover developers can improve the test execution speed of a parameterized test by using parallel programming.
expected exception .
.
developers sometimes need to test if the code would throw a specified exception for erroneous behaviours.
over the junit history encoding expected exceptions in test cases can be achieved differently i.e.
trywith failin junit3 rule and expectedexception in junit4 and assertthrows in junit5.
on the one hand we find that new junit releases gradually adapt to the increasing need for handling expected exceptions elegantly i.e.
a specialized annotation expectedexception instead of the workaround trywith fail and an improved annotation assertthrows to overcome the limitation of expectedexception.
on the other hand we find that developers may not be fully aware of new features in their adopted junit version.
for example we find cases that developers may migrate back to junit3 to use the trywith failmechanism because expectedexception is limited in providing comprehensive error messages.
such a backward migration is not needed since developers could leverage rule from junit4 or migrate to junit5 assertthrows.
in other cases we find that developers tried to customize their test code to handle expected exceptions but later migrated to a more framework dependent pattern as described above.
our findings suggest a potentially ill defined knowledge of how to handle expected exceptions in practice.
developers could benefit from having an expected exception recommendation tool to reduce test maintenance overhead.
we also find that developers may use expected exceptions to facilitate test driven development.
namely the expected exception avoids test failures while developers actively work on implementing the features.
finally we find a misuse of the expected exception associated with the exception type.
developer discussions reveal that the use of general excepted exception types e.g.
java.lang.exception could hide the actual faults because the test will still pass if an unexpected sub type exception is thrown.
therefore one way to solve this issue is to use specific exception types instead of a general exception type.there is a diverse way to handle expected exception in test code.
developers sometimes are unaware of which mechanism to utilize and what mechanisms may be available in their adopted junit version.
rule .
.
junit4 introduced rule to provide a flexible 69mechanism to enhance tests by running code around a test execution similar to before and after.
in alignment with the regular use of the rule we find that developers often refactor duplicate test code using rule to improve maintainability and readability.
we also found three common uses of built in rules namely the timeout temporaryfolder and testname rules .
in these cases developers preferred using these built in rules to simplify test code.
future studies may consider using various annotations to help refactor test code.
developers utilize rule to remove duplicate code in test fixtures.
moreover we find that developers can benefit from using built in rules to simplify their test code.
fixmethodorder .
.
prior studies found that one of the root causes of flaky tests is test order dependencies.
junit4 provides fixmethodorder to allow a deterministic test execution order.
for example we find incamel 4e7ec8f79b6 that since jdk7 does not preserve test execution order developers fixed test flakiness using fixmethodorder.
hence there is a future research opportunity on automated test fixing by applying such annotations.
developers apply fixmethodorder to ensure a deterministic test execution order and avoid dependency related flaky tests.
other types of test annotations changes migration .
.
in the migration from junit3 to junit4 we find that developers use an automated tool that applies migration in the entire codebase.
however the migration from junit4 to junit5 is applied manually and slowly e.g.
one package at a time .
there are many migrations that started over one year ago i.e.
before but the issue reports still remain open today.
we believe migrations to junit5 are intentionally manual because some annotations such as rule are removed in junit5.
moreover some annotations are renamed and may further cause confusion to developers e.g.
before is renamed to beforeeach .
therefore developers could benefit from having an automatic junit5 migration tool.
finally we find some changes where developers migrate from testng to junit5 due to the popularity of junit.
we find that the migration from junit4 to junit5 is done manually and slowly.
to help developers utilize the new features in junit5 future studies should further investigate migration patterns for junit5 in order to assist developers with automated migration.
mocking .
.
a junit class annotated with runwith indicates that the junit framework invokes a specified class using a developer specified test runner instead of running the default runner.
an issue with using mocking runners is that these mocking frameworks utilize an independent class loader which sometimes causes namespace error due to conflicting classes.
to resolve the issue developers add powermockignore to defer loading the conflicting classes.
finally for one instance we find that developers decided to remove mocking and use dependency injection instead.developers often use mocking for external dependencies.
however they may encounter issues related to namespace conflicts.
custom annotation and mixed framework usage .
.
we found cases where developers created customized annotations to repeat the test execution upon failure e.g.
for detecting flaky test or to indicate that a test is experimental.
however junit5 provides a new annotation repeatedtest and both junit4 and junit5 provide annotations e.g.
category to categorize tests.
we also find cases where developers added annotations from testng even though developers were already using junit which provides similar annotations.
the findings may indicate that sometimes developers might not know the functionality that is provided by testing frameworks and may use customized annotations and increase maintenance costs.developers resort to customized annotations due to the lack of awareness about the features offered by testing frameworks suggesting the need for annotation recommendation tool support.
by product .
.
we find that developers may modify test annotations due to a feature removal or test refactoring e.g.
relocate the annotation to another test file .
vi.
i mplications and future work based on our empirical findings we present actionable implications and future work for three groups of audiences researchers application developers and testers and framework designers.
a. researchers r1 test annotation is an integral part of test design and implementation.
future studies on test maintenance and refactoring should consider the peculiarity of test annotations.
as we find in rq1 test annotation changes are frequent in test maintenance and developers often use test annotations to improve test maintenance.
as we found in rq3 developers use various test annotations such as fixtures or rule to remove duplication in test code.
based on our manual study such refactorings are commonly performed but there is a lack of tool support.
therefore future refactoring studies may want to design test code refactoring techniques that leverage such test annotations.
r2 developers may use test annotation for ad hoc fixes which may affect test maintenance or even code quality.
future studies are needed to study such impact and provide research solutions.
in rq3 we find that developers may use test annotations for ad hoc fixes e.g.
adding ignore to failing tests or increasing the timeout threshold in test timeout x without finding the root cause .
although the fixes make the tests pass the underlying issues remain unsolved which may cause more severe issues in the future.
moreover as we found in rq2 there are much more additions of ignore than removals which indicates that many tests get disabled without being re enabled.
future studies are needed 70to study the prevalence of such technical debt in ad hoc test fixes and their potential consequences.
r3 there are future research opportunities on detecting misuses of test annotations.
recently test smell detection starts to receive interest from both the academia and industry due to its practicality .
however most prior studies only consider test smells related to test code yet as we found in rq3 there exist many test annotation misuses.
for example we found that developers use suboptimal ways to handle expected exceptions or parameterized in tests e.g.
tests expecting generic exceptions or not recording error messages when using parameterized .
furthermore there are also possible test annotation misuses related to fixtures e.g.
using before without considering its performance impact .
our study highlights and opens an avenue for future research to better detect test smells and improve test quality.
r4 future research is needed to provide automated test grouping and better utilization of test annotations to reduce test execution overhead.
to reduce test execution overhead prior studies have proposed various test selection and prioritization techniques .
in rq3 we find that developers use category to group and execute small tests first e.g.
run faster tests first to detect failures early .
future studies may consider integrating their techniques with test annotations for better research adoption.
developers may also use parallelization to speed up test execution e.g.
using parameterized with thread pools .
however we find that junit5 provides a new annotation execution executionmode.concurrent which allows parallel test execution.
future study is needed to assist developers to automatically adopt such annotations and improve test execution time without causing concurrency issues or flaky tests.
b. application developers and testers a1 developers need better education about the capabilities of testing frameworks.
as found in a prior study due to differences in background some developers may not be familiar with specific frameworks.
in rq3 we also have similar observations with testing frameworks.
even though junit is the most commonly used framework in java we find that some developers do not fully utilize test annotations.
for example some developers were unsure which way they should use to handle expected exceptions and some developers created custom test annotations even though junit already provides the same functionality.
a prior study found that there is often a champion who first adopts new features in a framework and helps the team with adoption .
we recommend that developers follow similar procedures and dedicate at least one team member to gain expertise in testing frameworks and help the development team utilize testing frameworks.
c. framework designers f1 framework designers need to provide better flexibility in their apis.
in rq3 we find some cases where developersneed to find workarounds or adopt other testing frameworks to bypass some inflexibility in junit.
for example before is executed for every test case in the class but developers may only want before to be executed for a subset of the test cases.
in this case developers removed junit fixture annotations and replaced them with a direct call to the helper method increasing redundancies.
in other cases fine tuning fixtures based on test cases also enforced developers to use a parameterized helper method over junit before.
finally we also uncovered some limitations with parameterized in junit4 although it can remove test code and test input duplication and improve test coverage.
firstly the junit4 parameterized class only works at class level and cannot be configured at method level.
thus for the specified test inputs the test runner will execute every single test case even if not all test cases utilize the input.
hence we believe that software engineering researchers may work with framework developers and identify possible issues that framework users encounter and improve the framework accordingly.
f2 better annotation support targeting specific testing issues e.g.
flaky tests .
we find that junit provides annotations such as fixmethodorder to resolve issues related to order dependent flaky tests.
similarly one of the customizations we find is retrying on test failure.
however junit5 now provides a new annotation repeatedtest to help developers detect test flakiness more easily.
to this end with the recent research advances in the detection of flaky tests we believe that incorporating more support in a practical framework such as junit will help developers quickly address test flakiness without resorting to other specialized tools that are difficult to adopt in practice.
vii.
t hreats to validity internal validity.
our findings depend on the accuracy of our tool to mine annotation changes from the commit history.
we mitigate this threat by validating our tool thoroughly.
the extension of refactoringminer .
detects annotation changes with a .
precision and .
recall.
external validity.
we study systems that are all open source implemented in java so the result may not be generalizable to all systems.
to minimize the threat we follow a set of criteria to select systems that are popular on github large in scale and actively maintained.
the studied systems cover various domains and are frequently used in commercial settings.
construct validity.
we conduct a manual study to understand the reasons behind test annotation changes.
due to the large number of changes we take a statistically significant sample.
there may be bias or misidentification in our manual study on characterizing test annotation changes.
thus two authors independently examined all available software artifacts and discussed them until the agreement is made.
we do not claim to find all usage and misusage patterns and the limitations of test annotations.
however we show the existence of such patterns and identify further research opportunities.
thus fu71ture work should survey developers based on recent annotation changes to gain additional insights.
viii.
c onclusion this paper presents the very first empirical study on annotation changes in java tests to fill the knowledge gap regarding the evolution and maintenance of tests since prior studies focused mainly on the test code and ignored the test annotations.
our study reveals many interesting findings with actionable implications test annotation changes are more common than test refactorings.
despite that there is very limited tool support for migrating test annotations to newer framework versions or different frameworks and automating common annotation change patterns within the same framework version.
test developers are sometimes unaware of the features provided by testing frameworks and thus apply alternative suboptimal solutions.
there is great need for tool support to detect the misuse or lack of use of annotations and recommend appropriate test annotations.
test developers are forced to apply workarounds to overcome the current limitations of testing frameworks.
framework designers need to be aware of these workarounds to improve the design and flexibility of test annotations.