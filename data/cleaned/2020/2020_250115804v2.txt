arxiv .15804v2 jun 2025codeimprove program adaptation for deep code models ravishka rathnasuriya university of texas at dallas usa ravishka.rathnasuriya utdallas.eduzijie zhao university of pennsylvania usa tez seas.upenn.eduwei yang university of texas at dallas usa wei.yang utdallas.edu abstract leveraging deep learning dl based code analysis tools to solve software engineering tasks is becoming increasingly popular.
code models often suffer performance degradation due to various reasons e.g.
code data shifts .
retraining is often required to address these issues but frequent model updates are costly in labeling and deployment.
in this paper we explore an alternative solution adapting the program inputs to the code models.
this can be achieved by two steps input validation that focuses on identifying whether an input is an out of scope input program that are beyond a model s handling capability and input adaptation that adapts out of scope inputs to become in scope inputs.
validating program input is challenging as current techniques focus on continuous inputs such as image data and fail with discrete inputs like code data which have unique characteristics and are processed differently by deep learning models.
adapting out of scope programs is also challenging due to their vast search spaces.
therefore in this paper we propose codeimprove which distinguishes outof scope from normal inputs and converts such out of scope inputs back to in scope inputs through program transformation.
in particular we propose a validity score metric to identify out of scope inputs and leverage genetics algorithms to apply semantic preserving program transformation to convert out ofscope inputs to in scope inputs.
our experimental results show codeimprove can enhance upto .
of accuracy and .
of relative improvements in three code models on two se tasks.
additionally our input validation is promising in detecting outof scope inputs auc score of .
.
index terms input validation program transformation i. i ntroduction in the field of software engineering code analysis tools play a crucial role in addressing critical tasks such as vulnerability detection defect prediction and clone detection .
code analysis tools often suffer performance degradation due to various reasons .
traditionally the primary approach to overcoming these obstacles has been to refine the tools to handle various challenging cases which might involve updating their versions or incorporating new heuristics.
however this process can be complex and resource intensive.
recently researchers have identified an alternative strategy adapting the inputs to the tools .
this approach has gained traction especially in scenarios where refining the tools becomes impractical.
this adaptation of inputs serves as an corresponding author.
at the time of submission zijie zhao was serving as an intern at the university of texas at dallas usa.effective strategy to circumvent the limitations of tool modification ensuring that code analysis tools remain effective even for cases they were initially unable to address before the adaptation of inputs.
while existing input adaptation efforts have primarily focused on traditional symbolic reasoning based tools there is a growing need for developing input adaptation strategies for deep learning based code analysis tools models .
this need arises from the challenges and high costs associated with improving these models to address their limitations.
commonly improving these tools involves retraining and replacing the underlying deep learning models .
such processes not only lead to increased labeling and computing efforts but also risk reducing the models generalization capabilities alongside potential compatibility and version control issues.
additionally model replacement may need the creation of new architectures and datasets which incurs substantial costs related to rebuilding and redeploying the systems.
to address the challenges an alternate cost effective solution is to adapt the program inputs to learning based tools without altering the original tool.
this strategy of input adaptation is particularly beneficial in scenarios like agile development where the code base rapidly changes as it maintains the model s applicability without the need for frequent retraining.
additionally exploring different input adaptation techniques is more resource efficient than the continuous retraining of a model to discover the optimal approach.
the process of adapting an input to a code model generally involves two key steps input validation that aims at identifying out of scope inputs that fall outside the model s capacity i.e.
inputs prone to being mishandled and input adaptation where the out of scope inputs are converted by semantic preserving transformations to become in scope inputs that are within the model s handling capabilities i.e.
inputs that the model is likely to process correctly .
the process of validating and adapting inputs for code models encompasses distinct challenges.
first in terms of input validation it is difficult to create a metric that accurately predicts the likelihood of a model generating a correct or incorrect output for a given input.
the machine learning community has developed some methodologies known as uncertainty metrics to estimate a model s level of uncertainty fora specific input.
however these metrics which are primarily designed for image data often fall short when applied to code data in identifying out of scope inputs.
the root of this issue lies in the nature of image data which is typically continuous and dense with fewer semantic nuances allowing for smooth gradients that make the uncertainty metrics more reliable.
in contrast code data is inherently discrete structured and filled with abstractions such as control flows and data dependencies which can result in abrupt changes in the model s output and make the uncertainty less predictable.
moreover while image data mainly undergoes noise corruption or compression as matrix transformations code data faces changes in syntax and programming paradigms.
our preliminary study section iii suggests that these essential distinctions pose a substantial challenge to the direct application of standard uncertainty metrics to code data.
second once specific inputs have been identified for adaptation transforming these inputs into the model s handling scope is also challenging.
first navigating the vast search spaces involved in code transformation is a complex task.
considering the numerous possible transformations that can be applied to code and the fact that many of these transformations can be applied repeatedly to produce various forms of code the resulting array of potential variations creates a complex landscape for exploration.
moreover any modifications made during the program transformation process must maintain the original program functionality and semantics of the code.
therefore efforts to modify the code to enhance model compatibility must be carefully balanced to avoid unintentionally changing its fundamental meaning or functionality thus posing a dilemma between improving model performance and preserving code semantics.
to address these challenges we propose codeimprove figure the first techniques for input validation and input adaptation of code inputs.
for input validation we identified that existing uncertainty metrics misrepresent the model s handling capability on code inputs leading to overconfident predictions for out of scope code inputs section iii .
we observe that the relevance of different aspects of the input such as structural information or variable names can shift dynamically across the model s layers.
traditional uncertainty metrics which typically focus on the outputs of the final few layers fail to capture this layer by layer processing.
based on such observation we propose a dropout based sub model generation dsmg approach to find an optimal hidden state representation that accurately identifies in scope versus out of scope inputs.
by analyzing sub models derived from the original dl model codeimprove can delve into how inputs are processed at each layer.
dsmg allows codeimprove to generate sub models that provide deeper insights into the transformation of inputs through the network.
codeimprove utilizes the confidence levels of these sub models predictions as a new metric for assessing the validity of inputs offering a more reliable measure that captures the complexities of code input processing in dl models.
following input validation codeimprove employs adapta tion by evolutionary search aes .
we develop a list of basic semantic preserving transformations and leverage dsmg s validation score as a guiding metric to combine these basic transformations into a composite transformation that effectively covert the input from being out of scope to in scope.
we evaluated our technique with pre trained transformerbased language models on software engineering tasks such as vulnerability detection and defect prediction.
our experimental results report promising results and show codeimprove can enhance .
of absolute accuracy and .
of relative improvements in three code models on two code tasks.
notably our validity score computation that validates out ofscope inputs obtained promising results auc score of .
.
we summarize our contributions in this paper below novel perspective .
we propose a novel perspective of differentiating out of scope from in scope inputs as well as adapting these out of scope inputs to become in scope inputs.
to the best of our knowledge our novel perspective is the first attempt to adapt inference time inputs for deep code models through program transformation.
tool implementation .
we implement codeimprove following the novel perspective by implementing a sub model generation technique from the original code model for code data designing a validity score metric to distinguish out of scope inputs from in scope inputs utilizing the generated sub models and designing a genetic algorithm based technique to adapt out of scope inputs to become in scope inputs by applying program transformation.
comprehensive evaluation .
we conducted an extensive study on three popular pre trained models and two codebase tasks demonstrating the effectiveness and efficiency of codeimprove s input validation and input adaptation on test data.
public artifact .
we release all experimental data and source code at the project github repository for future research practical use and experiment replication.
ii.
b ackground a. problem definition given a code model mand an input code snippet x the class with the highest probability is the final prediction result ofmforx denoted as y m x .
during deployment ensuring the correctness of every prediction is challenging.
thus the objective is to enhance model performance on test inputs through code adaptation.
the validation metric v m x evaluates m s uncertainty on input xto determine whether the input is in scope or outof scope.
if v m x is less than the predefined threshold c it indicates uncertainty and xrequires adaptation.
otherwise thexis considered in scope.
the set of transformations t t1 t2 .
.
.
t n refers to a sequence of code transformation operators applied to the out of scope input x resulting in a modified input x .
let yrepresent the ground truth of xand lety m x be the prediction result after adapting xtox viat.
2the goal is to compute vand apply tsuch that the loss function of the adapted prediction l y is smaller than the original loss l y .
we aim to find vandtto make l y l y where y m t1 t2 .
.
.
t n x ifv m x c y ifv m x c the loss l y is characterized by the distance from the ground truth y l y y y this can be generalized to any distance metric e.g.
l1 l2 l to accommodate for any se task.
the challenge is to develop an effective validation metric v oracle problem and a determine a sequence of transformations t search problem that adapt out of scope inputs.
b. oracle problem developing v during deployment determining whether a prediction is correct without manual analysis is challenging.
an effective validation metric vis needed to automatically guide mto accurately make decisions thus reducing false positives.
a substantial progress has been made in this direction such as handling uncertainty deep emsemble and cross layer dissection .
c. search problem transformation sequence t beyond validation the process of adapting the out of scope inputs to become in scope inputs necessitates efficient search algorithms to explore program syntax for potential transformations.
the goal of search techniques is to optimize the code transformations while preserving the program semantics.
search techniques like random search hill climbing search and genetics algorithms offer solutions for code transformations.
the search strategy begins with a set of candidate solution s generated by applying semantic preserving code transformation.
these candidates are then evaluated using vto select the most promising one.
the search algorithm iteratively refines the candidates until a termination criterion is reached or an optimal solution is found.
iii.
p reliminary study on input validation for code models in our preliminary study we assess the applicability of existing uncertainty metrics within the realm of code models aiming to identify a dependable threshold score that can differentiate in scope and out of scope program inputs.
we perform the study on a comprehensive set of metrics from the existing literature.
our goal is to answer the research question how effective are the existing uncertainty metrics in distinguishing in out of scope program inputs?a.
experimental method and setup uncertainty metrics our study includes eight different uncertainty metrics.
we utilize vanilla that computes maximum softmax probability as the confidence.
temp scale is a post hoc calibrated confidence metric applied to the validation set where the bfgs optimizer is used to train a calibration temperature with a learning rate of .
.
our study includes confidence based uncertainty metrics such as least confidence which calculates the difference between confidence and the most confidently predicted label margin confidence which determines the difference between the top two most confident softmax predictions and ratio confidence which computes the ratio between the top two most confident softmax predictions.
we also include uncertainty metrics that were designed using information theory .
entropy computes the average amount of surprise uncertainty for a given outcome.
predictive entropy quantifies the uncertainty associated with the outcomes for a given set of observed inputs.
mutual information measures the amount of information obtained from one random variable given another using entropy and conditional entropy.
montecarlo dropout mcd quantifies uncertainty by averaging the logits over multiple dropout samples.
deep ensembles de quantifies uncertainty by averaging the outputs from multiple independently trained models with different initial seeds.
both employ the sampled winning score sws as the primary uncertainty metric.
dataset and models to evaluate the effectiveness of detecting out of scope data through uncertainty quantification we consider two code tasks and two associated datasets i.e.
defect prediction with codechef dataset and vulnerability detection on devign dataset on three pre trained models.
more information on datasets and subject models is explained in section v b. table i auc comparison on distinguishing out of scope inputs for selected uncertainty metrics experiment vulnerability detection defect prediction codebert roberta graphcodebert codebert roberta graphcodebert vanilla .
.
.
.
.
.
temp.
scaling .
.
.
.
.
.
predicitive entropy .
.
.
.
.
.
entropy .
.
.
.
.
.
mutual information .
.
.
.
.
.
least confidence .
.
.
.
.
.
ratio confidence .
.
.
.
.
.
margin confidence .
.
.
.
.
.
mcd .
.
.
.
.
.
de .
.
.
.
.
.
evaluation metric we used the area under curve auc based on true positive rate tpr and false positive rate fpr data to measure how effective a technique is in distinguishing an out of scope inputs from in scope inputs.
auc quantifies the probability that a positive example receives a higher predictive score than a negative sample.
for example a random classifier yields an auc of .
while a perfect classifier achieves an auc of .
to compute the auc scores we define positive and negative samples based on the correspondence between predicted outputs and ground truth labels.
a positive sample indicates correct predictions in scope labeled as while a negative 3sample signifies misclassified out of scope labeled as .
for instance given a well trained model f an input pair x y has ground truth if f x is an exact match of y and otherwise.
during evaluation each uncertainty method predicts a score reflecting the model s capability in handling the input.
b. results and analysis table i shows auc scores for the evaluated uncertainty metrics.
the majority of these metrics exhibit auc scores close to .
akin to what one would expect from a random classifier with none surpassing an auc score of .
.
from these results it is inferred that the uncertainty metrics under consideration are ineffective at distinguishing between inscope and out of scope code inputs.
fig.
overview of codeimprove iv.
d esign of codeimprove figure provides an overview of codeimprove.
codeimprove consists of two main technical phases input validation phase to detect out of scope inputs from normal inputs and input adaptation phase to transform out of scope inputs to inscope inputs.
a. input validation the goal of this phase is to design a guiding metric that distinguishes between in scope and out of scope test data for a trained dl model and its associated dataset.
our preliminary investigation section iii demonstrates that traditional uncertainty metrics are inadequate for software engineering se tasks motivating the need for a more granular approach to understanding input behavior.
we aim to identify an optimal hidden state representation that distinguishes between in scope and out of scope inputs.
code inputs fed into the dl model s layers often experience shifting focus across various aspects such as from structural information to variable names.
existing uncertainty metrics which rely on outputs from the final few layers fails to capture the this dynamic layerwise processing.
to bridge this gap we explore sub models extracted from original model to gain deeper insights into the processing of inputs.
these sub models enable us to measure confidence at multiple levels forming the basis for a more reliable input validation metric.
a key motivation for our approach is the need to compute epistemic uncertainty which arises from the model s lack ofknowledge about certain inputs and reflects how confident the model is when making predictions.
accurately estimating epistemic uncertainty requires introducing model variance which captures the variability in predictions when slight changes are applied to the model s architecture or processing.
existing approaches such as dropout based techniques or ensemble methods aim to create model variance but face significant limitations in the context of code data.
these methods do not investigate layerwise processing ignoring the dynamic transformations that inputs undergo at different model layers nor do they explore hidden layer representations which are crucial for understanding how the model processes structured data like code.
most importantly they are primarily designed for continuous input spaces like images where smooth gradients and variations can be leveraged for uncertainty estimation.
our approach introduces sub models to address these limitations.
these sub models are generated by selectively extracting representations from intermediate layers of the original model and introducing architectural diversity inspired by subensemble methods.
each sub model independently processes the same input providing a detailed view of how the original model transforms inputs across layers.
the key insight is that the degree of agreement among sub model predictions correlates strongly with the trustworthiness of the model s overall prediction.
when sub models produce consistent predictions it indicates that the input is in scope.
conversely significant disagreement among sub models suggests the input is likely out of scope.
training sub models focuses on capturing layer specific uncertainties by freezing earlier layers and retraining only the dense layers which act as classifiers for each task.
this approach enhances the signal strength for determining model confidence.
additionally studies have shown that shallower layers in code models often outperform deeper ones for specific tasks making the sub model strategy particularly effective for code input validation.
by dissecting predictions layer by layer sub models can pinpoint nuances in data representation resulting in more accurate identification of inscope and out of scope inputs.
sub model generation.
figure shows the overview of our dropout based sub model generation dsmg .
dsmg constructs diverse sub models to capture the hidden state representations of the original model enabling a more detailed analysis of input processing.
each sub model consists two components the first part is inherited from the original dl model containing all structures and parameter values from the first layer up to an intermediate layer k for sub model k and the second part is a newly trained dense layer linking layer k to the output customized for the specific software engineering se task.
to generate first part dropout based hidden representations are extracted from each layer introducing controlled randomness to highlight distinct processing behaviors.
to generate second part the dense layer is trained independently for each sub model using cross entropy as the loss function 4which has demonstrated effectiveness for classification tasks in deep neural networks .
dropout regularization is applied during training to prevent overfitting and enhance generalization.
notably this sub model generation process is performed offline tailored to the se task at hand and does not affect the model during deployment.
dropout based layerwise hidden representation.
in dsmg hidden state representations are generated by selectively choosing specific layers and applying dropout to nodes within these selected layers.
dropout randomly omits a subset of nodes in a chosen layer during sub model generation while retaining the remaining active nodes.
this approach introduces diversity along two axes the depth of layers considered and the variation induced by dropout.
such diversity enables dsmg to effectively capture the model s processing of different input patterns and structures providing a richer understanding of its behavior.
this enhanced representation equips codeimprove to better identify and differentiate in scope inputs from outof scope inputs.
original dl modelsub model1sub model2original weightsnewly trained weightskeep the first k layers then train a dense layerwith dropout fig.
overview of sub model generation sub model validity measurement.
equation outlines the computation of the validity score which relies on understanding the processing of inputs across layers.
for a given input xfed into a dl model mwith nlabels mpredicts x as label lx.
let submodel kbe the softmax probability vector associated with the sub model k. to calculate the validity score we differentiate between two scenarios lxbeing a correct prediction or an incorrect one.
for correct predictions we employ the best versus second best bvsb strategy which measures the difference between the highest predicted probability labeled as lx and the secondhighest predicted probability labeled as ls .
this difference reflects the confidence of the sub model with a larger difference indicating higher confidence.
for incorrect predictions the bvsb strategy instead compares the actual highest predicted probability labeled as lh with the probability assigned to the original model s predicted label lx highlighting the uncertainty between the sub model s confidence and the incorrect prediction.
this approach formulated in equation ensures that the validity score effectively captures the reliability of the sub model s predictions.
once each sub model computes the respective validity score for each input we utilize the dissector s approach i.e.
weight growth types to compute the validity for the whole dl model equation .validityscorek lx submodel k submodel k b submodel k lxwith the highest probability submodel k b submodel k otherwise b submodel k submodel k submodel k iflxis correct submodel k submodel k iflxis incorrect .
final score lx pn k 1v alidityscore k lx submodel k weight kpn k 1weight k b. input adaptation given an out of scope input the goal of this phase is to covert the input to become an in scope input.
the challenge lies in exploring the large search space of possible program modifications while preserving semantic correctness.
to tackle this we define a set of semantic preserving transformations.
guided by the dsmg validation score section iv a transformations are iteratively combined and refined to align inputs with the model s capabilities.
this process formalized as adaptation by evolutionary search aes efficiently optimizes the transformations to achieve the best solution.
algorithm high level aes algorithm input pre trained model m testing dataset t out of scope input ids ids maximum iterations max iter mutation rate rate fitness threshold threshold output new test program dataset n 1n 2foreach sample intdo ifsample.id isnotinidsthen n.append sample.code continue i bestcandidate sample.code fitness while i max iterandfitness threshold do initial pop genpop bestcandidate fitness fitness initial pop new pop select initial pop pop evolve new pop rate fitness fitness pop bestcandidate select best pop i i iffitness threshold then n.append bestcandidate else n.append sample.code 21return n program transformations.
to construct the set of basic program transformations we considered all common kinds of code structures including loop structures branch structures 5table ii list of code transformation no transformation operator description changename function name and variable name renaming changefor the for loop is transformed into a while loop.
changewhile the while loop is transformed into a for loop changedo the do loop is transformed into a while loop.
changeifelseif transformation of if elseif to if else changeif transformation of if else to if elseif changeswitch transformation of the switch statement to the if elseif statement.
changerelation transformation of relational expressions e.g.
a b to b a .
changeunary modifications to unary operations e.g.
i to i i changeincrement modifications to incremental operations e.g.
i to i i .
changeconstant modifying constant e.g.
changedefine modifications to variable definitions e.g.
int b to int b b .
changeaddjunk insert junk code that will never be executed.
e.g.
if printf changeexchangecod exchange the order of statements without dependencies e.g.
declaration statements changedeletecomments deleting statements that print debugging hints and comment.
e.g.
printf operator and expression changes table ii .
all transformations are carefully compiled to ensure that the adapted program not only undergoes transformation but also upholds the semantics of the original code.
our list of transformation operators is designed to be task preserving by ensuring that the functionality and behavior of the code remain unchanged.
while there is potential to broaden this list of transformations codeimprove already shows significant improvement over existing techniques based on the current list.
due to the space limitation we list all these specific atomic operations at our project homepage .
then we illustrate how to apply these operators to search for the best solution that deep code models can adapt.
adaptation by evolutionary search one of the primary requirements before applying transformation operators is to identify syntactic features i.e.
the places of code fragments applicable for transformation.
finding appropriate syntactic features is essentially an optimization problem.
codeimprove addresses this problem by counting the number of code fragments present in each operator for a given code snippet.
for example if there are four identifiers in a source code snippet then the count of the operator is k .
after identifying the number of syntactic features to be transformed codeimprove needs a transformation strategy to generate a diverse pool of candidates.
codeimprove achieves this by implementing a genetic algorithm based strategy comprising initialization a fitness function crossover and mutation operators and termination criteria to guide the transformation process.
algorithm shows the overview of our transformation strategy.
the inputs for the aes algorithm are the trained dl model m the test data used to evaluate the m s performance the identified out of scope data the maximum number of iterations that the aes should evolve for and the required fitness score that the solution of aes should achieve.
the output of our genetic algorithm is a new dataset that includes the transformed source code.
to avoid randomness when applying transformations codeimprove transforms all kcounts of features in each operator.
initially codeimprove creates a starting population byapplying operators to each out of scope input line with each individual in this population representing a potential solution.
the fitness of each individual determined by the dsmg s validation score section iv a reflects how closely a solution approaches the problem s target with higher scores indicating better candidates line .
then the genetic algorithm iterates through cycles of evaluation selection and reproduction.
in the selection phase the top of candidates are chosen based on their fitness scores line .
during the reproduction phase codeimprove performs genetic operators i.e.
crossover and mutation to generate new solutions line .
during crossover for each candidate codeimprove applies a sequence of transformations.
then we add the new samples to our population.
for each crossover variant we mutate the syntactic feature with a random transformation to maintain a diverse population.
the algorithm terminates when the candidate code has reached a higher validation score based on our guiding metric or the fixed number of generations has reached.
in the end the algorithm returns the solution with the highest fitness value line .
next we will describe the experimental evaluation.
v. e valuation we conducted all our experiments on a server equipped with an intel xeon e5 cpu and eight nvidia ti gpus.
we set up .
and .
as the threshold based on our validation score on detecting out of scope inputs for vulnerability detection and defect prediction tasks respectively on all subjects.
we set up maximum iteration to three and different crossover rates i.e.
.
.
.
and when applying sequences of transformation operators on each subject matter described in section v. a. research questions rq1.
overall performance what is the overall performance of codeimprove?
rq2.
input validation how effective is the out of scope program data detection?
rq3.
input adaptation how effective to convert out ofscope data to become in scope data?
6table iii effectiveness of codeimprove experiment modelvulnerability detection defect prediction a p r f1 ri csr mcr a p r f1 ri csr mcr original set upcodebert .
.
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
inputreflectorcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
codeimprovecodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
rq4.
setting sensitivity how sensitive is codeimprove s performance under different experimental setups?
rq5.
semantic preserving does program transformations of codeimprove preserve semantics?
rq6.
runtime overhead what is the overhead of codeimprove in adapting a program to dl models?
b. subjects datasets and tasks codeimprove is evaluated on two code based classification tasks vulnerability prediction with devign dataset and defect prediction with codechef dataset .
although this study used two datasets codeimprove applies to all code model based tasks with plans for broader future evaluations.
forvulnerability detection the devign dataset consists of functions extracted from ffmpeg and qemu open source c projects.
the functions are labeled as containing vulnerabilities or being clean with vulnerable and clean samples.
the dataset is split into train validation test sets with sizes .
for defect prediction the codechef dataset includes c c functions from the codechef platform .
samples are labeled with categories such as no defect wrong output timeout error or runtime error with no defect wrong output timeout error and runtime error samples.
the dataset is divided into train validation test sets with sizes .
models we employed state of the art pre trained models namely codebert roberta and graphcodebert which have been widely utilized in previous studies .
these models were fine tuned on our tasks using the corresponding datasets adhering to the recommended settings proposed in previous literature .
hyperparameters were set to match the original configurations.
codeimprove is designed for use with all types of code models.
our study includes a diverse range of tasks pretrained models and class numbers ensuring a comprehensive evaluation of codeimprove s performance.
c. evaluation metrics we use a diverse set of metrics to measure codeimprove s effectiveness for our six rqs.
theaccuracy a is the proportion of correctly classified samples out of all samples.
the precision p is the percentage of correctly predicted positive samples out of all positive predictions.
the recall r measures the percentage of correctlypredicted positive samples that were retrieved out of all actual positive samples.
the f1 score f1 is the harmonic mean of precision and recall.
relative improvement ri quantifies the accuracy improvement relative to the difference between training and test accuracy.
correction success rate csr is the ratio of successfully corrected mispredictions to the total identified mispredictions.
mis correction rate mcr measures the negative effect caused which is the ratio of correct predictions changed to mispredictions to the total number of correct predictions in the test set.
correction validation rate cvr is the ratio of successfully validated mispredictions to the total possible mispredicted inputs to be validated.
mis correction validation rate mvr is the ratio of correct predictions validated as mispredictions to the total number of correct predictions in the test set.
auc score evaluates the effectiveness of the input validation process.
transformations per second tps measures the rate of transformations codeimprove can apply per second.
next we will describe the results of the experiments.
vi.
r esults and analysis we report and analyze experimental results and answer the preceding research questions in turn.
a. rq1 overall performance of codeimprove baseline codeimprove is the first technique to improve the code model s performance through program transformations.
thus we cannot find direct baselines for comparison.
to address this we draw inspiration from a technique in the image domain for comparative analysis i.e.
the inputreflector iref that detects deviating inputs and substitutes them with the most similar sample from the training set.
we are unable to include codedenoise as a baseline due to the evaluation methods of this work which splits the test set into two subsets as t1 and t2 and subsequently assesses results on t2 set.
however the splitting criteria of datasets is not provided in the project website.
moreover the work is similar to the adversarial style which denoises the program identifiers and corrects the program with the supervision of the model s predictions.
codeimprove demonstrates better performance compared to codedenoise where it only fixes .
of inputs while codeimprove fixes .
for defect prediction task on the codebert model.
7process iref utilizes two models the siamese network and the quadruple network to detect deviating inputs and repair them.
during training these models rely on three datasets the original set a transformed set human recognizable and an extremely transformed set human unrecognizable .
these transformed sets are created by applying varying degrees of transformation to the original data.
however for code data generating such datasets is challenging as transformations do not follow a continuous degree like in image data.
therefore we utilized only two sets the original set and a transformed version.
we adapted the iref loss functions for these two sets and fed hidden layer outputs from the original model into the siamese and quadruple networks.
to repair out of scope inputs iref searches for the most similar data in the training set and exchanges their labels.
in contrast codeimprove applies all semantic preserving transformations during the crossover step.
effectiveness is evaluated using metrics such as accuracy precision recall f1 score ri csr and mcr.
result table iii presents the comparison between codeimprove and iref illustrating that codeimprove consistently outperforms iref.
notably we observe the following codeimprove consistently achieved the best model improvements with upto .
in accuracy .
in precision .
in recall and .
in f1 score on all the subjects codeimprove is capable of correcting around .
to .
of the mispredicted inputs on both vulnerability detection and defect prediction tasks codeimprove shows notable ri improvements ranging from .
to .
particularly excelling with roberta models techniques designed for image data e.g.
iref fail in the context of code.
iref negatively impacts performance especially for codebert as it focuses on out of distribution inputs rather than inputs prone to misprediction within the same distribution.
this limitation arises from iref s inability to effectively handle the syntactic and semantic similarities between transformed and original code and codeimprove introduces minimal negative effects with only .
of correct predictions misclassified in the worst case.
codeimprove successfully adapts out of scope inputs to in scope inputs as demonstrated in table iii.
rq1 what is the overall performance of codeimprove?
codeimprove was effective in adapting out of scope inputs for both se tasks on three subject models with higher accuracy precision recall f1 score csr and ri.
table iii .
b. rq2 effectiveness of out of scope data detection baseline we compared codeimprove s dsmg with the dissector .
additionally we evaluated the dsmg approach with the uncertainty metrics in our preliminary study section iii vanilla temperature scaling predictive entropy entropy mutual information least confidence ratio confidence and margin confidence monte carlo dropout and deep ensemble.process we compute the auc score cvr and mvr to evaluate the effectiveness of out of scope data detection on all baseline approaches.
table iv effectiveness of input validation experiment modelvulnerability detection defect prediction cvr mvr auc cvr mvr auc vanillacodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
temp.
scalingcodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
predictive entropycodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
entropycodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
mutual informationcodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
least confidencecodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
ratio confidencecodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
margin confidencecodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
mcdcodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
decodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
dissectorcodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
codeimprovecodebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
table v effectiveness of sub model decomposition experiment modelvulnerability detection defect prediction auc auc hidden statescodebert .
.
roberta .
.
graphcodebert .
.
codeimprovecodebert .
.
roberta .
.
graphcodebert .
.
results table iv shows the results of out of scope data detection.
based on the results we observe that codeimprove achieved higher auc scores across all models and tasks i.e.
auc .
.
while dissector performs better than other uncertainty metrics codeimprove still surpasses it in auc scores codeimprove demonstrates a higher cvr across all subjects detecting up to .
of out ofscope inputs for codebert on vulnerability detection tasks and consistently outperforming other methods in cvr the mvr on codeimprove is lower than other approaches concluding that codeimprove is better at differentiating in scope inputs.
mvr for defect prediction task shows .
.
and .
for codebert roberta and graphcodebert models mcd and de average predictions over multiple forward passes through the same network.
this approach limits diversity in the predictions which may contribute to their poorer performance in auc compared to codeimprove and other uncertainty metrics did not produce promising results on auc cvr or mvr.
for example predictive entropy obtained a cvr of .
and an mvr of .
which are not significant indicators of effective performance.
8comparison of using sub models vs. hidden state outputs from the original model sub models are trained using the layerwise hidden states of the original model but it is also possible to directly access these hidden states for detecting out of scope inputs.
we conducted an experiment comparing trained sub models with the direct use of hidden states.
due to the high dimensionality of the hidden states we applied a linear transformation and subsequently applied equations .
table v presents the statistics of the auc comparison between these two methods.
based on the results in table v we observe that training sub models auc .
.
outperforms direct use of hidden states auc .
.
across both software engineering tasks.
these results signify the performance of trained sub models for out of scope input validation.
we summarize several factors contribute to the lower effectiveness of directly using hidden states ineffective feature utilization and transformation effective input validation requires a mapping between the feature space and the class space.
without training a dense layer this process would merely reduce dimensions without learning this mapping.
training a dense layer allows it to learn the most relevant features from the hidden states and establish an accurate mapping from the feature space to the class space.
this reduces significant information loss and enhances overall performance and lack of adaptability trained dense layer in sub models can adapt to the characteristics and distribution of the training data making them more effective for each se task.
without training the model lacks this adaptability resulting in poor effectiveness.
rq2 how effective is the out of scope program data detection?
codeimprove effectively distinguishes out of scope from in scope inputs auc .
.
cvr .
.
mvr .
.
and is more suitable than existing techniques for various se tasks.
c. rq3 effectiveness of search strategies to adapt out ofscope inputs.
baseline we employed two search strategies namely random search codeimprove rand and hill climbing algorithm codeimprove hc .
codeimprove rand applies random transformations until identifying the optimal candidate.
codeimprove hc follows the principles of the hill climbing algorithm.
process for codeimprove rand transformation operators are applied randomly until the algorithm identifies the best candidate.
in codeimprove hc the process starts with an initial solution obtained through a random transformation.
the algorithm then iteratively applies a single transformation operator to improve the fitness score.
once the current solution surpasses the fitness threshold the algorithm terminates having reached the optimal solution.
to ensure fairness all techniques are limited to transformations per solution.
in codeimprove each candidate solution undergoes all operators during the crossover phase.results table vi compares the three approaches highlighting the following key observations codeimprove obtained the best accuracy across all subjects up to .
while both codeimprove rand and codeimprove hc did not achieve the performance of codeimprove up to .
although codeimprove hc and codeimprove rand improve the model performance we find that these search algorithms stop at the local minima i.e.
once the algorithm identifies a better candidate the process terminates .
however codeimprove will evolve for multiple generations i.e.
in our case is three to find the best candidate in terms of correcting mispredictions codeimprove performs the best i.e.
csr up to .
although codeimprove rand and codeimprovehc shows lower values of mcr note that its csr values are really low therefore unable to correct mispredictions in a large scale and in conclusion codeimprove is a stable approach to adapt program inputs.
rq3 how effective to convert out of scope data to become in scope data?
codeimprove is better at correcting out of scope inputs compared to other search algorithms such as random search and hill climbing.
d. rq4 influence of hyper parameters process we studies the influence of number of transformation operators n applied during the crossover for each candidate.
we investigated the effectiveness and efficiency of codeimprove under different settings i.e.
n .
we applied variable renaming and one random transformation operator from table ii for n operators for n operator for n and operators for n .
results table vii show the results of codeimprove under the hyper parameter settings of nin terms of csr mcr and ri.
we observed that as n increases more mispredicted inputs can be corrected resulting in a larger ri.
meanwhile more correctly predicted inputs are identified as mispredicted ones due to more transformations leading to slightly higher mcr.
when n for codebert on vulnerability detection task codeimprove achieved a higher csr i.e.
.
than n i.e.
csr of .
however the ri was lower than n due to higher mcr value.
therefore it is necessary to maintain the balance between csr and mcr during the transformation.
moreover codeimprove has achieved greater performance on all cases for n indicating its practical applicability.
rq4 how sensitive is codeimprove s performance under different experimental setups?
codeimprove is effective when different number of transformation operators are applied during the crossover i.e.
ri shows .
.
for vulnerability detection and .
.
for defect detection tasks .
9table vi effectiveness of search strategy for input adaptation experiment modelvulnerability detection defect prediction a p r f1 ri csr mcr a p r f1 ri csr mcr codeimprove randcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
codeimprove hccodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
codeimprovecodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
table vii sensitivity study experiment modelvulnerability detection defect prediction ri csr mcr ri csr mcr n 1codebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
n 5codebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
n 10codebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
n 15codebert .
.
.
.
.
.
roberta .
.
.
.
.
.
graphcodebert .
.
.
.
.
.
e. rq5 semantic preservation in codeimprove s program transformation process the objective of this rq is to examine whether the adapted programs maintain the semantics of the original inputs.
we investigate the effectiveness of applying semantic preserving program transformations to adapt out of scope inputs.
based on our investigation we provide an example in figure .
additional examples are on our project website due to space restrictions .
figure illustrates how codeimprove revises a misprediction.
as illustrated in figure 3a the codebert model incorrectly predicts the input to no defect although the ground truth label is actually wrong output .
during the validation phase codeimprove identifies this out of scope input with a validity score of .
significantly lower than our threshold of .
.
to adapt this input codeimprove applies semantic preserving transformations.
these transformations include splitting lines changing code order splitting declarations and separating variable assignments at line .
additionally the relational and incremental operators were altered in lines and .
these changes create a syntax shift that affects the model s interpretation resulting in different embeddings.
the transformed version is shown in figure 3b.
after these transformations the model correctly predicts the label as wrong output with an improved validity score of .
.
rq5 does program transformations of codeimprove preserve semantics?
codeimprove can generate semantic preserving program transformations.
f .
rq6 overhead of codeimprove process to apply codeimprove in real time we compute the overhead of applying transformations for an input.
we calculate the tps which measures the number of transformations int main int t scanf d t while t int n c i min scanf d n int a for i i n i scanf d a min a for i i n i if a min min a for i i n i c c min printf d n c 12345678910111213141516171819ground truth label wrong outputprediction result no defect a before transformation int main int t scanf d t while t int c int n int i int min c scanf d n int a for i n i i i scanf d a min a for i n i i i if a min min a for i i n i c c min printf d n c 123456789101112131415161718192021222324ground truth label wrong outputprediction result wrong output b after transformation fig.
an example of a m c transformation applied per second by codeimprove.
this metric is averaged across all nvariants of codeimprove studied under rq4.
additionally we evaluate both offline and online overhead.
the offline stage involves the sub model training while the online stage measures the time required to adapt an input with codeimprove.
table viii shows the statistics of tps and figure shows the time overhead of codeimprove.
table viii tps of codeimprove experimentvulnerability detection defect prediction codebert roberta graphcodebert codebert roberta graphcodebert overhead .
.
.
.
.
.
based on table viii codeimprove is capable of applying transformations at a rate of .
tps to .
tps for each se task across all code models.
figure 4b confirms that codeimprove takes approximately .92s to .4s to adapt an input across all models.
we plan to further minimize the transformation times in future work.
moreover codeimprove is more efficient and offers a practical scalable solution to enhance model performance without the significant cost and time investment required by traditional methods such as retraining and replacement.
it is important to note that the sub model training procedure is treated as an offline stage minimizing its impact on overall performance.
figure 4a shows that training a sub model takes around 900s to 940s for the vulnerability detection and 1200s to 1250s for the defect prediction across all models on a machine with an nvidia geforce gtx gpu.
this process only needs to be done once and incurs significantly lower costs compared to regular retraining or fine tuning.
10codebert roberta graphcodebert020040060080010001200overhead s vulnerability detection defect prediction a offline time overhead in second codebert roberta graphcodebert0102030405060overhead s .
.
.
.
.
.
vulnerability detection defect prediction b online time overhead in seconds fig.
time overhead of codeimprove rq6 what is the overhead of codeimprove in adapting a program to dl models?
codeimprove was highly efficient in adapting an out ofscope input through semantic preserving program transformations in real time .2tps .04tps .
vii.
t hreat analyses and limitations our selection of the two subject datasets namely devign and codechef with their associated code models might threaten the external validity of our experimental conclusions.
we tried to alleviate this threat by following efforts the two datasets are very popular and have been widely used in relevant research their associated dl models are commonly used in se tasks these datasets and models differ from each other by varying topics labels from two to four and model accuracies from .
to .
which make these subjects diverse and representative.
therefore our experimental conclusion should generally hold although specific data could be inevitably different for other subject.
threats to external validity may arise from the techniques selected for experimental comparisons including uncertainty metrics and dissector .
due to inherent differences existing methods for image data selfchecker and inputreflector cannot be directly applied to code data.
therefore we chose dissector as a baseline for input validation as it identifies out of scope inputs similarly to our focus.
additionally we sampled a subset of uncertainty metrics to assess their effectiveness in identifying out of scope inputs .
our internal threat mainly comes from the lack of ground truths for distinguishing out of scope inputs from in scope inputs limitations on applying codeimprove in different subjects and applying different program transformation rules.
we used mispredictions and correct predictions to simulate out ofscope inputs and in scope inputs respectively.
such estimation can be rough however the logic may holds rq2 .
regarding the lack of subject matter our assumption was to investigate how our propose technique performed on different se tasks and code models.
therefore we select a subset of se tasks and code models rq1 rq3 and rq4 .
in future we plan to extend and apply codeimprove on other subjects matter e.g.
clone detection functionality classification etc.
.
codeimprove uses semantic preserving transformation rules but we plan to add more and update our website with new results.
dueto the diversity of our subjects we believe our conclusions are generally applicable and codeimprove can support future research.
viii.
r elated work input validation for deep learning model.
several work has been proposed to compute the trustworthiness of a dl model.
hendrycks et.
al propose a baseline for detecting misclassified samples.
propose re calibration of probabilities on a held out validation set.
proposed dissector to validate inputs by crossing layer dissection.
proposed selfchecker by leveraging kernel density estimation kde .
confidnet is designed to learn the confidence criterion using true class probability for predicting failures.
inputreflector identifies failure inducing inputs on image data.
in addition to the aforementioned techniques on code data we show that existing uncertainty metrics do not perform promising results on code data.
related work on code inputs.
several techniques for generating adversarial code to challenge these models have been proposed in recent years.
tian et al.
claim they designed a code difference guided generation technique which can improve the efficiency further.
codedenoise is the most advanced technique to improve the performance of deployed models without retraining however it can only relieve the noise introduced by different identifier names and consequential mispredictions.
in contrast codeimprove leverages unique transformation operators.
to the best of our knowledge codeimprove is the first attempt to use the inference time program transformation technique to enhance the performance of code models ix.
c onclusion this paper proposes codeimprove for validation and adaptation of out of scope inputs to become in scope inputs for code models.
codeimprove employs a validity score metric by leveraging dropout based sub model generation dsmg technique to distinguish out of scope inputs from in scope inputs and applies semantic preserving program transformations on these inputs.
experimental evaluation confirmed that codeimprove s effectiveness highlighting its potential for broader application scenarios in input validation and adaptation.
data availability our artifacts are available at containing datasets code to reproduce the results in this paper.
our project website is available at