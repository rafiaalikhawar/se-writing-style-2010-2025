mocksniffer characterizing and recommending mocking decisions for unit tests hengcheng zhu southern university of science and technology shenzhen china zhuhc2016 mail.sustech.edu.cnlili wei the hong kong university of science and technology hong kong china lweiae cse.ust.hkming wen huazhong university of science and technology wuhan china mwenaa hust.edu.cn yepang liu southern university of science and technology shenzhen china liuyp1 sustech.edu.cnshing chi cheung the hong kong university of science and technology hong kong china scc cse.ust.hkqin sheng webank co ltd shenzhen china entersheng webank.com cui zhou webank co ltd shenzhen china cherryzzhou webank.com abstract inunittesting mockingispopularlyusedtoeasetesteffort reduce test flakiness and increase test coverage by replacing the actual dependencies with simple implementations.
however there are no clearcriteriatodeterminewhichdependenciesinaunittestshould be mocked.
inappropriate mocking can have undesirable consequences under mocking could result in the inability to isolate the classundertest cut fromitsdependencieswhileover mocking increases the developers burden on maintaining the mocked objectsandmayleadtospurioustestfailures.accordingtoexisting work various factors can determine whether a dependency should be mocked.
as a result mocking decisions are often difficult to makeinpractice.studiesontheevolutionofmockedobjectsalso showed that developers tend to change their mocking decisions of the studied mocked objects were introduced sometime afterthetestscriptswerecreatedandanother13 oftheoriginally mocked objects eventually became unmocked.
in this work we aremotivatedtodevelopanautomatedtechniquetomakemockingrecommendationstofacilitateunittesting.westudied10 testscriptsinfouractivelymaintainedopen sourceprojectsthat usemockedobjects aimingtocharacterizethedependenciesthat thisworkwasconductedwhenhengchengzhuwasavisitingstudentathkust the hong kong university of science and technology .
the first two authors contributed equally to this work.
yepang liu and shing chi cheung are corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september virtual event australia copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
mocked in unit testing.
based on our observations on mocking practices we designed and implemented a tool mocksniffer toidentifyandrecommendmocksforunittests.thetoolisfully automatedandrequiresonlythecutanditsdependenciesasinput.
it leverages machine learning techniques to make mocking recommendations by holistically considering multiple factors that can affect developers mocking decisions.
our evaluation of mocksnifferon ten open source projects showed that it outperformed three baseline approaches and achieved good performance in two potential application scenarios.
ccs concepts generalandreference empiricalstudies softwareand its engineering software maintenancetools softwaretesting and debugging .
keywords mocking unit testing recommendation system dependencies acm reference format hengchengzhu liliwei mingwen yepangliu shing chicheung qin sheng andcuizhou.
.mocksniffer characterizingandrecommending mockingdecisionsforunittests.in 35thieee acminternationalconference onautomatedsoftwareengineering ase september21 virtual event australia.
acm new york ny usa pages.
.
introduction unittestinghasbeenwidelyadoptedtoassurethequalityofprogramunits namelyclasses bytestingtheminisolation.inpractice aclassundertest cut iscommonlycoupledwithotherclassesin aprogramoritsreferencedlibraries.theseclassesarethedependencies of the cut and often participate in its unit tests.
mocking isadefactomechanismtoisolatethecutfromitsdependencies in a test by simulating the behaviors of the dependencies using 35th ieee acm international conference on automated software engineering ase mockedobjects .itwasreportedthat23 ofthejavaprojects with test scripts use mocking .
to conduct effective unit testing using mocking developers first needtomakemockingdecisions i.e.
decidingwhichdependencies should be mocked.
however it is non trivialto make proper mocking decisions.
a study showed that developers may change their initial mocking decisions during development .
of their studied mocked objects were introduced sometime after the test scripts were created.
another of the originally mocked objects eventuallybecameunmocked.thissuggeststhattheoriginalmocking decisions were later considered improper by the developers.
makingpropermockingdecisionsischallengingbecause mocking decisions are unlikely to be made by considering only a single factor.
in practice developers may need to consider multiple factorsholisticallytomakeamockingdecision.
mockingdecisions are usually context aware.
developers can make different mocking decisions for the same dependency when testing different cuts according to the different usage scenarios of the dependency.
inappropriatemocking decisionscan leadtoundesirable consequences.
on the one hand dependencies that should be mockedcan be left unmocked in test scripts.
such under mocking could result in the inability to isolate the cut from its dependencies which can seriously affect the efficiency of unit testing.
the developers fixed this issue by mocking the real object .
on the otherhand dependenciesthatdonotneedmockingcanbemocked by developers.
such over mocking increases developers burden on maintaining the mocked objects since they need to keep the behaviorsofthemockedobjectsconsistentwiththerealimplementations.inconsistenciesbetweenthemockedobjectsandthereal implementationscancausespuriousfailuresintesting.forexample in issue of project flink the method getid in mocked executionvertex returnsanullvalueandthuscausednullpointer exceptions npes in test executions.
however in the real implementation of executionvertex the method getid will never return null.inthiscase thefailurecause dbynpesdoesnotreveal arealbug.developersreplacedthemocked executionvertex with a real one to fix this issue.
the evolution of the project code can furtherexacerbatetheproblemsincedevelopersneedtoupdatethe mocked objects to catch up with the code evolution.
if too many dependenciesaremocked itwouldbedifficultforthedevelopers to update the mocked objects in time.
given the challenges in making proper mocking decisions studies were conducted to find out the factors that affect mocking decisions.
for example mostafa et al.
pointed out that production classes are more frequently mocked than library classes.
spadini et al.
categorized mockedobjects and found thatclasses dealing withexternalresourcesareoftenmockedbydevelopers.marriet al.
revealedthatfilesystemapiscanbemockedtofacilitate unittesting.
thesestudies investigatedthe mockingpractices and identifiedhigh levelandintuitivefactorsthatcanaffectmocking decisions.inaddition thesefactorsareallgenerictocutswithout considering their interactions with the dependencies.
with suchadvice it is still difficult for developers to make proper mocking decisionswhen writingtest scripts.researchers havepointedout the need for automated mock recommendation techniques .
yet noneoftheexistingworkhasproposedsuchatechnique.in fact thehigh levelandproject genericcharacteristicsofmockeddependenciesidentifiedbythesestudiescannoteffectivelyguide the design of automated mock recommendation techniques.
this motivates us to conduct an empirical study to characterize mocked dependencies at the code level by analyzing api usages data flows control flows etc.
we aimed to identify those characteristics that can be automatically extracted via code analysis so that wecanleveragethemtobuildautomatedmockrecommendation techniques.
inour empiricalstudy weanalyzed 846test scripts offourlarge scaleopen sourceprojects suchashadoop .when conducting the empirical study we not only studied the characteristics of the dependencies themselves but also investigated their interactions with the cuts in different test cases.
we made several important observations that were not captured by existing studies.
specifically weidentifiedtencharacteristicsofmockedobjectsat thecodelevel.wefoundthatallofthetencharacteristicscanaffect mocking decisions yet none of them is the sole determining factor.
this provides evidence for the fact that mocking decisions are madebyconsideringmultiplefactors andthusweshallholistically consider different factors to recommend mocking decisions.
we alsoobservedthatcontext awarefactors whichcapturetheinteractionsbetweendependencies andcuts arethe mostrelevantto the mocking decisions.
this indicates that an automated mocking decision recommendation technique should be context aware i.e.
considering the interactions between the dependencies and the cuts.
basedonourempiricalfindings wefurtherproposedatechnique mocksniffer torecommendmockingdecisionsinunittesting.
mocksniffermakescontext awarerecommendationsfordependencies of cuts in unit testing.
it takes a cut and its dependencies as inputandoutputsarecommendedmockingdecisionforeachofthe input dependencies.
it also holistically combines various factors to suggest mocking decisions by leveraging machine learning techniques with features formulated from our empirical study findings.
mocksniffer learns the knowledge of making mocking decisions from existing mocking practices andleveragesthe knowledge to recommend future mocking decisions.
inourevaluation wetrainedandtested mocksniffer with546k dataentriesofmockedandunmockeddependenciesextractedfrom ten open source projects.
we compared the performance of mocksnifferwith the generic mocking decision strategies adopted in existingstudies.ourresultsshowthat mocksniffer whichperforms context aware mocking recommendations can significantly out perform the baseline methods as shown by the mann whitney u test .wealso evaluated mocksniffer undertwo potentialapplication scenarios for mature projects train mocksniffer with dataextractedfromhistoricalreleasesofthesameprojecttoconduct cross versionmockingrecommendation and fornewprojects trainmocksniffer with data extracted from other projects to conduct cross project mocking recommendation.
our evaluation results showed that mocksniffer achieved good performance in both of these two application scenarios it achieves an average f1 score of .
and .
for the two application scenarios respectively.
to summarize this paper makes three major contributions we conducted an empirical study based on test scripts of fourlarge scaleopen sourceprojectsanddisclosedtencode levelcharacteristicsofthemockingpracticesofreal worlddevelopers.
we also validated our findings in a large scale dataset consisting 4371public int countfiles production code try return filemanager.scan .length catch ioexception e return 9public void test1 test script filemanager mgr mock filemanager.class when mgr.scan .thenreturn new file undertest cut new undertest mgr assertequals cut.countfiles 1516public void test2 test script filemanager mgr mock filemanager.class when mgr.scan .thenthrow new ioexception undertest cut new undertest mgr assertequals cut.countfiles listing example usage of mocked objects of354kmockedandunmockeddependencies.inourstudy we observedmockingdecisionsareaffectedholisticallybyvarious factors among which contextual features play an important role.
wedesignedandimplemented mocksniffer thefirstautomated techniquetorecommendmockingdecisionsforunittesting.our evaluationof mocksniffer onopen sourceprojectsshowedthat mocksniffer significantlyoutperformedthemockingstrategies adoptedbyexistingstudiesandachievedgoodperformancein two potential application scenarios.
inourstudy wehavegeneratedalargelabeleddatasetconsisting of546kdataentriesoftestcases dependencies andcuts.we releasedthisdatasetforpublicaccesstofacilitatefutureresearch background in unit tests mocked objects help decouple a cut from its dependencies.
mocked objects are usually created by leveraging a mockingframework.take test1 inlisting1asanexample.the production code at line involves disk i o. to save effort in setting up the environment for testing at line developers create a mocked filemanager objectusingmockito apopularmocking framework.
then the mocked object mgrdirectly returns a file arraywithoutaccessingthedisk atline11 .thisalsospeedsuptest executionsasdiski ocanbeslow.similarly theproductioncode at line is not executed unless exceptions occur at line .
to emulatetheexceptionalscenarios in test2 developersconstruct a mocked filemanager object and make it throw an exception directly when the method scan is invoked.
apart from mocked objects created with mocking frameworks we also observed that developers can construct mocked objects by creatingdummyclassesthatextendtheconcerneddependencies.
for example in the test script of project hbase developers created a class keyproviderfortesting which is a subclass of the production class keyprovider .
they mentioned in the document that theclassistoreturnafixedsecretkeyfortesting.instancesofsuch classescreatedinthetestscriptsservethesamepurposeasthose mocked objects created with mocking frameworks.
mockinghasbeenwidelyusedinunittestgenerationtechniques.
for example arcuri et al.
enhanced evosuite b yl e v e r aging mocking to increase code coverage and reduce flaky tests.alshahwanetal.proposedautomock toimprovetheperformanceoftestcasegenerationbymockingtheenvironment.till mannetal.
proposedasymbolic execution basedtechniqueto generatemockedobjectsforunittesting.althoughthesestudies foundthatmockingcanfacilitatetestgeneration theyalsoreported that generated test cases with mocked objects can introduce spurioustest failures i.e.
falsealarms .
theunderlying reasonis that there is no reliable mechanism to help decide which dependencies to mock during test generation.
when making mocking decisions these existing techniques have to resort to simple rules e.g.
all databaseandfilesystemrelateddependenciesshouldbemocked .
such mocking decisions contradict with the practices of real world developers who oftenmock onlya smallportion ofdependencies e.g.
file system related dependencies are not always mocked and may result in substantial false alarms .
to reduce such falsealarms existingstudiesproposedseveralstrategies such as confining the values thatcan be returned by method calls on mocked objects.
although these strategies can help reduce false alarms it would be better to mock only when necessary.
in the following sections we will study the mocking practices of developers and figure out the factors that can affect mocking decisions.
data collection inordertounderstand themockingpracticesadoptedbydevelopers weconstructed a dataset byextracting cuts their dependencies and developers mocking decisions from open source projects.
this dataset will enable us to study whether developers share similarpracticeswhenmakingmockingdecisions.inthissection we present its construction process in detail.
.
data representation each entry in our dataset is a tuple t cut d l wheret represents the test case cutrepresents the class under test d representsthedependency aclassusedin t butnotcut andl mocked unmocked isalabelthatrepresentswhether dismocked inthetestcase t.forexample testcachingkeyprovider.testkeyversion cachingkeyprovider keyprovider mocked is adataentryextractedfromhadoop .itmeansthatdevelopers mocked the dependency keyprovider in the test case testcachingkeyprovider.testkeyversion forcut cachingkeyprovider .
such a data entry not only captures the developers mocking decisiononadependencybutalsolinksdependenciestocuts.we include the links in our dataset because the mocking decisions for the same dependency can vary across cuts .
.
subjects and data extraction for constructing the dataset we selected four actively maintained open sourceprojects.table1showstheinformationaboutthese projects.theseprojectsallusemockito ordefinedummyclasses to construct mocked objects.
as we can see from the table theprojects are large scale.
in the following we explain our data extraction procedure.
existing studies leveraged static analysis to identify mocked objectsintestcases.however suchidentificationmaybeimprecise.
forexample developersofhadoopcreatedafactorymethod to create mocked objects of eventwriter .
depending on the value of 438table selected projects project version files loc data entries mocked hadoop .
.
.6m camel .
.
.3m 864hbase .
.
738k 093storm .
.
282k total .9m the field mockhistoryprocessing in the test class the test cases mayormaynotcreateamockedobjectof eventwriter .itisdifficultforstaticanalysistopreciselyinferwhethersuchobjectsin each test case are mocked or not.
in our work to obtain a more precise dataset we leveraged dynamic analysis to identify mocked objects and extract data entries.
we explain the main ideas below.
for each test case t we first infer its class under test i.e.
the cut usingnamingheuristics.accordingtocommonlyadopted naming conventions the name of a test class typically contains the name of the cut e.g.
testmyclass is a test class for myclass .
hence wecananalyzetheclassnameof ttoinfercut.next to analyze whether a dependency dis mocked in t. in this paper we regard the non cut objects created during test case execution andpassedtothe cut directlyorindirectlyastestdependencies.
suchobjectsareusuallypassedviamethodcallsonthecutand its dependencies.
therefore we instrumented all method call sites intto log the exact type of each reference type argument and the type of the corresponding formal parameter.
we observed that mocked objects created with popular mocking frameworks e.g.
mockito easymock have special type names.
for example when using mockito the type names of the mocked objects areintheform foo mockitomock xxx where xxxisahashcode.
therefore afterexecuting t wecananalyzetheloggedinformation to infer whether an argument is a mocked object i.e.
determine the labell via checking its type name and obtain the dependency d which is the type of the corresponding formal parameter.
as mentioned in section developers may construct mocked objects by themselves rather than using a mocking framework.
to include such mocked objects in our dataset we also considered objects as mocked ones if they are instances of classes that are definedintestscripts and extendaclassintheproductioncode.
in this case dependency dis identified as the production class being extended i.e.
keyprovider in the example in section .
while the above approach may miss some test dependencies e.g.
thosespecifiedviaconfigurationfilesorassigneddirectlyto apublicfield ithelpeduscollect354kdataentriesfromthefour open sourceprojectsafterrunning50ktestcases.suchcollected data entries are already sufficient for our empirical study.
empirical study to identify the factors that can affect the mocking decisions we conducted an empirical study on the projects based on the dataset extracted section3 .weaimedtoderiveasetofrulestocapture the characteristicsof themocked objects viaanalyzing theircode 1wealsocheckedthepatterninothermockingframeworksinourimplementation.
we skip the details due to page limit.patterns.
such rules can further guide us to design automated techniques to help developers make mocking decisions.
.
setup we adopted a two stage scheme when conducting the empirical study.inthefirststage wemanuallyinspectedasmallsubsetofthe datasettodevisecode levelcharacteristicsofthemockedobjects.in the second stage we formulated the code level characteristics into rules and conducted an automated validation of these rules with a large scale dataset aiming to validate the derived characteristics.
stage characteristics identification.
inthefirststage we manuallyinspected100dataentriesinthedatasettoidentifycharac teristicsofthedependenciesthataremockedbydevelopers.specifi cally werandomlysampled25dataentrieslabeledasmockedfrom each of the four projects.
for each sampled entry we analyzed the sourcecodeofthedependency thecut andthetestscriptfrom which the data entry is extracted.
we inspected the data entries using the open coding method to identify common code level characteristics e.g.
dataflow controlflow andapiusage ofthe mocked dependencies and their interactions with the cuts.
althoughthesamplingsizeissmall ouridentifiedcharacteristicscan cover .
on average of the mocking cases as shown in our evaluation see the recall of baseline in table .
stage2 large scalevalidation.
inthesecondstageoftheempirical study we further validated thecode level characteristics of mocked dependencies identified in stage using the entire dataset.
specifically basedonthemanually identifiedcharacteristics we formulatedseveralrulestoautomaticallyidentifythedataentries that exhibit the characteristics.
for each of the rules we applied ittoall354kdataentriesinourdatasettoobtainasubsetofdata entries that match this rule.
for each of the subsets we computed its mock ratio i.e.
the proportion of entries labeled as mocked.
we compared the mock ratio of each subset with the mock ratio of the entire dataset which is .
.
the larger the difference in the mock ratio themorelikelythecorrespondingcode levelcharacteristic can affect mocking decisions.
.
results following the process described above we made five observationsand formulated ten rules i.e.
code level characteristics in stage .
inthefollowing wewilldiscussourobservationsandtheformulated rules.
we will also present the mock ratio obtained in stage for eachdata entrysubset thatmatches eachrule.
themock ratio is presented in the brackets after each rule.
observation classes related to environment or concurrency are often mocked.
in of the manually inspected entries the dependencies invoke apis related to concurrency networking diski o orapisprovidedbyonlineservices e.g.
amazon aws microsoftazure .theseapiscanbeslowtoexecuteorexhibit inconsistent behaviors across different test runs.
we formulated the following rules based on this observation.
rule .
referencing environment dependent or concurrentclasses .
.
classesmatchingthisrulecallapisrelated totheenvironmentorconcurrency.wemanuallybuiltalistof suchapis atclasslevel injdk includingthoserelatedtonetworking diski o concurrency database etc.wefoundthatthese 439apisarefrequentlyusedinmockedclassesbutinfrequentlyused in unmocked ones.
this rule matches the data entries where the dependency