togll correct and strong test oracle generation with llms soneya binta hossain department of computer science university of virginia sh7hv virginia.edumatthew b. dwyer department of computer science university of virginia matthewbdwyer virginia.edu abstract test oracles play a crucial role in software testing enabling effective bug detection.
despite initial promise neural methods for automated test oracle generation often result in a large number of false positives and weaker test oracles.
while llms have shown impressive effectiveness in various software engineering tasks including code generation test case creation and bug fixing there remains a notable absence of large scale studies exploring their effectiveness in test oracle generation.
the question of whether llms can address the challenges in effective oracle generation is both compelling and requires thorough investigation.
in this research we present the first comprehensive study to investigate the capabilities of llms in generating correct diverse and strong test oracles capable of effectively identifying a large number of unique bugs.
to this end we fine tuned seven code llms using six distinct prompts on a large dataset consisting of java projects.
utilizing the most effective finetuned llm and prompt pair we introduce togll a novel llm based method for test oracle generation.
to investigate the generalizability of togll we conduct studies on unseen large scale java projects.
besides assessing the correctness we also assess the diversity and strength of the generated oracles.
we compare the results against evosuite and the state of the art neural method toga.
our findings reveal that togll can produce .
times more correct assertion oracles and .
times more exception oracles than toga.
regarding bug detection effectiveness togll can detect unique mutants that evosuite cannot which is ten times more than what toga can detect.
additionally togll significantly outperforms toga in detecting real bugs from the defects4j dataset.
i. i ntroduction software dominates almost every aspect of our lives including safety critical domains such as healthcare and autonomous transportation.
even seemingly minor software bugs can cause large scale system outages security breaches and loss of lives .
thus ensuring software reliability through rigorous testing and bug detection is paramount.
test oracles the foundation of effective testing play a pivotal role in the early detection of software bugs .
typically a test suite consists of a set of test cases where each test is composed of a test prefix that exercises a certain part of a program and a test oracle verifies whether the executed behavior matches the expected behavior .
test oracles can be categorized into two main types assertion oracles which judge the correctness of the output state of the program and exception oracles which judge whether erroneous input states are detected by the sut.the efficacy of test oracles is determined by their ability to detect bugs.
for effective bug detection test oracles should becorrect andstrong .
a test oracle is considered correct if it aligns with the expected program behavior thereby avoiding false alarms.
additionally a test oracle is deemed strong if it can detect deviations from intended program behavior.
just because a test oracle is correct does not mean it is strong as demonstrated in .
researchers have investigated the application of natural language processing nlp and pattern matching techniques for generating test oracles from code comments and natural language documentation .
these approaches can generate both assertion oracles which verify that a program s actual output matches its expected output and exception oracle which capture the program s anticipated exceptional behaviors during testing.
recent advancements have seen the application of neural techniques for generating test oracles utilizing transformer based models that learn from the method under test mut and developer written test cases .
building upon these methods the development of toga a neural based test oracle generator outperformed prior work in detecting real faults.
however a recent study shows that this state of the art neural based test oracle generation method when given a test prefix the mut and its docstring generates assertion oracles only of the time.
moreover when it does generate assertion oracles are false positives and for exception oracles the false positive rate is .
despite significant research focused on generating test oracles automated generation of correct and strong test oracles has proven challenging and the state of the art suffers from generalizability issues high false positive rates and limited bug detection effectiveness .
these challenges emphasize the ongoing need for further investigation marking it as a significant and open research problem.
large language models llms have shown promise in automating diverse software engineering tasks such as code completion program comprehension automated repair and test generation .
most prior work on ml based test generation has focused on the generation of test prefixes inputs to the system under test with the goal of yielding substantial test coverage.
codamosa uses llms to escape coverage plateaus in search based test input generation and find it can improve coverage on small to 1arxiv .03786v2 dec 2024codeparrot 110m fine tuned llmsfinetuning sf110 training datasetsoracleeval25 dataset inferencetest oracle llms fine tuning on test oracle generation taskunseen inference with togll correctness evaluation1 strength evaluationp6fig.
.
a general overview of our research approach to investigating llm based automated oracle generation.
moderate python programs.
a recent attempt to use llms to generate test prefixes for large scale java programs was not very successful it achieved only statement coverage on large scale java projects compared to over for evosuite s search based approach.
the llm based testpilot approach generates both test prefixes and test oracles for javascript.
it produces better coverage than stateof the art search based javascript test generation technique on a variety of small programs.
with regard to test oracles testpilot was shown to generate significant numbers of non trivial assertions that are distinct from those generated by other methods but this work is limited to javascript and did not evaluate oracles correctness diversity and strength aspects thoroughly.
in this paper rather than task the llm with generating both test prefix and test oracle as testpilot did we decouple these problems.
we utilize state of the art test generation methods such as evosuite to create test prefixes and concentrate on leveraging llms for test oracle generation.
initially we fine tune seven code llms with sizes ranging from 110m to .7b parameters employing six prompt formats that vary in the information they incorporate about the method under test mut its documentation and the test prefix.
employing the most effective model and prompt pair we introduce our llm based method togll.
it is important to note that togll generated oracles are not regression oracles.
regression oracles are generated based on the executed program behavior.
for example evosuite executes the code collects all actual results values knows the types and then generates oracles based on those.
this is why regression oracles are not capable of detecting bugs in current program versions.
togll in contrast does not execute the code and is not aware of the test execution results.
it uses javadoc documentation mut mut signature and test prefix to generate oracles.
some of the prompts see table i do not even use mut p1 p2 at all and some p3 p4 only use mut signature not the entire implementation.
to evaluate togll s generalizability on unseen data we conduct a comprehensive study on real world large scale java projects and compare the correctness of the generated oracles with those produced by the sota neural method toga and evosuite.
subsequently we assess the strength of the generated oracles through a large scale mutant detection study and a real bug detection study highlighting theunique advantages of togll generated oracles over alternative methods.
to the best of our knowledge this represents the first extensive study to explore the application of llms in test oracle generation examining seven code llms six prompts and three large datasets while evaluating multiple aspects of the generated oracles including correctness diversity and bug detection effectiveness.
in summary the main contributions of the paper lie in constructing a large dataset from large scale java projects to fine tune code llms aiming to investigate their performance in generating effective test oracles.
evaluating the generalizability of fine tuned llms in generating effective test oracles on unseen projects and compare them with sota methods.
evaluating the impact of different contextual information on oracle generation accuracy through six specially designed prompts.
demonstrating that well tuned and prompted llms can generate correct strong and diverse test oracles with much lower false positive rates than sota methods and can detect large number of unique bugs neither detected by evosuite nor sota neural method.
releasing all datasets fine tuned models and code to enable replication of our study and the community to build on the work.
this work offers a promising starting point for llm based test oracle generation identifies directions for future improvement of such methods and thereby has the potential to lead to impactful new capabilities for software engineers.
ii.
a pproach figure presents an overview of our research approach.
step of figure depicts fine tuning a collection of llms with different prompts for test oracle generation task we discuss this in section ii a. step of figure 1selects the best performing fine tuned model and prompt from step to define togll and perform oracle inference on the unseen data.
step assesses the correctness of togll generated oracles through test execution we discuss this in section ii b1.
finally step assesses the strength of togll generated oracles using mutation testing and real bug detection from defects4j we discuss this in section iii d and iii e. 2a.
supervised fine tuning this section covers the datasets prompts and llms that are fine tuned for the test oracle generation task.
sf110 dataset we construct this dataset using the sf110 evosuite benchmark which comprises opensource java projects with over java classes sourced from the sourceforge repository.
this benchmark is widely recognized for unit test generation.
to construct our dataset we utilize the test cases generated by evosuite for each project.
given that a single test case can contain multiple assertion oracles or even a combination of assertion and exception oracles we process each test case.
this involve decomposing them into individual test cases ensuring that each contains only a single assertion or exception oracle.
for each decomposed test case we extracted the method under test mut along with its associated documentation to prepare the dataset.
the dataset is comprised of tuples pi mi di oi where piis the test prefix miis the mut and diis the docstring if available and oiis the ground truth test oracle either assertion or exception.
this dataset is used to finetuning the llms and has been partitioned into three distinct subsets for training for validation and another for testing.
prompt designing even though llms are powerful they are not specifically designed and pre trained for test oracle generation.
thus fine tuning with effective prompts is necessary for optimal performance.
we have designed six different prompts each tailored to a specific use case scenario.
prompt p1 consists of only the test prefix prefix .
prompt p2 additionally includes the mut documentation strings prefix doc .
the separator token is different based on different llms and their respective tokenizer.
prompt p3 instead of docstrings includes the mut signature prefix mutsig .
prompt p4 includes both mut signature and docstrings in addition to prefix prefix doc mutsig .
prompt p5 includes the code for the entire mut prefix mut .
prompt p6 adds the docstring to p5 prefix doc mut .
these prompts are designed to cover diverse real use case scenarios.
for example when documents are not available p1 p3 and p5 can be used.
when the mut implementation is not available p1 p4 can be used and when it is available p5 or p6 can be used.
designing these prompts allows us to assess their relative accuracies and the impact of different information on the oracle generation performance.
code llms decoder only llms are most suitable for code generation tasks .
smaller models are cost effective produce a lower carbon footprint and can be more easily fine tuned with limited gpu resources.
therefore in our study we prefer decoder only llms that are pre trained on various code generation tasks within the software development lifecycle sdlc smaller in size publicly available on hugging face and well documented for fine tuning.
we chose seven llms because they are pre trained on multiple programming languages while meeting all the above criteria.
codegpt it is a gpt style language model with 110m parameters.
we choose to fine tune this model for two main reasons.
firstly both our training and unseen inference datasets came from java projects and this model is also pretrained on java programming language.
secondly this model is pre trained on the code completion task which aligns with our designed prompts.
we fine tune the microsoft codegptsmall java model from hugging face.
codeparrot in our study we fine tune the 110m parameters model which was pre trained for the code generation task on nine different languages java javascript php python c c go ruby and typescript.
we have fine tuned the codeparrot codeparrot small multi model from hugging face.
our experimental study suggests that despite its small size the fine tuned codeparrot model generalizes strongly on unseen data and can outperform larger models with billions of parameters.
codegen codegen is a family of autoregressive language models with four different trainable parameter sizes 350m 2b 6b 16b.
in this paper we have fine tuned 350m and 2b multi variant models that are pre trained on dataset that encompasses six programming languages c c go java javascript and python.
because of their great capability in comprehending and generating codes we fine tune them for test oracle synthesis task.
polycoder polycoder is a family of large language models with three trainable parameter sizes .4b .7b and 16b.
this model was trained on gb of code across programming languages.
in this paper we have fine tuned .4b and .7b model for the test oracle generation task.
phi phi is a transformer language model with .
billion parameters initially pre trained on python code from a variety of data sources for the task of python code generation.
despite its primary training on python our study observed that it performs on par with other models when fine tuned on java code for generating test oracles.
b. unseen inference oracleeval25 dataset to investigate the generalizability of the fine tuned models we utilize the dataset described in comprising large scale industrial standard java projects.
of these originate from the apache commons proper while the remainder are sourced from github.
these latter projects are characterized by large codebases with multiple modules with up to 10k active users per project and are frequently employed in software engineering research for the evaluation of test cases and test oracles .
given their broad coverage of domains and significant variation in developer metrics the artifacts selected for this study provide a robust framework for assessing fine tuned model s ability to generalize across a spectrum of real world 3projects.
overall the dataset consists of 271k source lines of code sloc 331k lines of javadoc 1214k test suite sloc and .5k test cases.
similar to the sf110 dataset each input sample of this dataset contains the test prefix mut and docstring.
we refer to this dataset as oracleeval25.
togll from the fine tuning results we identify codegen 350m and codeparrot 110m as the top two models due to their high accuracy.
we also select the three bestperforming prompts p4 p5 and p6.
as different fine tuned models may generalize differently to unseen data and due to the differences between datasets different prompts may work better.
for this reason we conduct a small scale study performing unseen inference with these two models and three prompts on projects from the oracleeval25 dataset.
our results suggest that even though codegen 350m is three times larger than codeparrot 110m codeparrot 110m fine tuned with p6 performs better on the unseen project than codegen on several metrics total input processed compilation error and false positive count.
furthermore the inference time with codegen350m is longer than codeparrot.
consequently we define togll as a fine tuned codeparrot 110m operating on p6.
we perform rest of the study with togll on the unseen oracleeval25 dataset ii b1.
baseline toga is a neural method for both assertion and exception oracle generation .
toga utilizes codebert model as backbone and fine tuned for the oracle generation task.
toga takes the test prefix generated by evosuite its associated mut and docstring if available and generates either an assertion or an exception oracle.
an example of evosuite generated test cases with assertion and exception oracle is shown in figure .
the prefix part is marked with yellow color.
the first test prefix involves pushing the number onto the stack followed by popping this value from the stack.
for this scenario an assertion oracle is anticipated to validate that the value retrieved is indeed .
the second test attempts to pop from an empty stack expecting the mut to throw an exception.
if no exception is thrown the test should fail.
we selected toga as our baseline method for several reasons toga represents the sota neural method having surpassed specification search and neural based techniques by detecting bugs in defects4j toga utilizes the same set of information as p6 which is our most comprehensive prompt thereby providing a fair basis for comparison toga has been previously evaluated on the oracleeval25 dataset which is also employed in our study ensuring consistency in our comparative analysis .
metrics with togll we generate test oracles for all projects from the oracleeval25 dataset.
this dataset framework includes all necessary artifacts for executing the generated test oracles when integrated with the respective test prefixes.
consequently we integrate the generated oracles into the test cases and execute the complete test suites.
we calculate accuracy as the success rate which denotes the percentage of non empty test oracles that were successfully passed during test execution indicating their alignment with public void testpopemptystack stack integer stack new stack try stack.pop fail catch emptystackexception e test passed public void testpushandpopstack stack integer stack new stack stack.push integer val stack.pop assertequals integer.valueof val test prefix with exception oracletest prefix with assertion oraclefig.
.
test cases with assertion and exception oracles.
the test prefix part is marked with yellow color.
expected program behavior.
we compute the success rate using equation successrate p t tce tfp tem t where tis the number of total test prefixes in project p tceis the number of compilation errors tfpis the number false positive tests and temthe number of empty oracles.
we compute the successrate metric for both assertion and exception oracles predicted by togll and compare them to those of our baseline toga.
to assess the bug detection effectiveness of the togllgenerated oracles we perform both mutation testing and real bug detection study on defects4j.
mutation testing introduces small changes or mutations to the source code to create numerous slightly altered versions of the software called mutants .
studies showed that there is a statistically significant correlation between mutant detection and real fault detection .
a test suite s ability to detect these mutants can be a direct measure of its bug detection effectiveness .
therefore we compute the mutant killing score as a metric to assess and compare the bug detection effectiveness of togll generated oracles with both evosuite and toga.
additionally we also record unique mutants killing score which indicates the unique capability of each oracle generation method.
besides the mutation study we also perform real bug detection study on the defects4j dataset and compare the results with the toga baseline.
iii.
e xperimental study to assess the efficacy of llm generated test oracles this study investigates several key areas effectiveness of different code llms in generating test oracles the impact of different prompts on the oracle generation accuracy the ability of finetuned models to generalize to unseen data comparative accuracy among different oracle types diversity of the generated oracles and the overall efficacy of these oracles in detecting bugs.
to this end we answer the following five research questions.
4table i test oracle generation performance of llm s on different prompts .
top two accuracies for each prompt are shown in bold .
the last row shows prompt wise average .
code llm p1 p2 p3 p4 p5 p6 prompt details codegpt 110m .
.
.
.
p1 prefix codeparrot 110m .
.
.
.
.
.
p2 prefix doc.
codegen 350m .
.
.
.
.
p3 prefix mutsig.
polycoder .4b .
.
.
.
.
.
p4 prefix doc.
mutsig phi .3b .
.
.
.
.
.
p5 prefix mut codegen 2b .
.
.
.
.
p6 prefix doc.
mut polycoder .7b .
.
.
.
avg .
.
.
rq1 what llm and prompting approaches are effective for test oracle generation?
increasing context in the prompt improves accuracy of oracle generation up to for a range of llms but smaller llms meet or exceed the performance of larger llms when fine tuned.
rq2 how well does togll i.e.
fine tuned model generates correct test oracles for unseen projects?
togll generates up to .
times the number of correct test oracles compared to prior neural based approach.
rq3 how diverse are llm generated assertions relative to those generated by evosuite?
togll generated oracles vary substantially in terms of the specific assert statements used as well as in the variables and expressions