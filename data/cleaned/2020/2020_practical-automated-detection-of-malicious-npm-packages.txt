practical automated detection of malicious npm packages adriana sejfia sejfia usc .edu university of southern california los angeles usamax sch fer max schaefer github .com github oxford uk abstract thenpmregistryisoneofthepillarsofthejavascriptandtypescript ecosystems hosting over .
million packages ranging from simpleutilitylibrariestocomplexframeworksandentireapplications.
each day developers publish tens of thousands of updates as well as hundreds of new packages.
due to the overwhelming popularityofnpm ithasbecomeaprimetargetformaliciousactors who publish new packages or compromise existing packages to introducemalwarethattamperswithorexfiltratessensitivedata fromuserswhoinstalleitherthesepackagesoranypackagethat transitively dependsonthem.defendingagainstsuchattacksis essential to maintaining the integrity of the software supply chain but the sheer volume of package updates makes comprehensive manual review infeasible.
we present amalfi a machine learning basedapproachforautomaticallydetectingpotentiallymalicious packages comprised of three complementary techniques.
we start withclassifierstrainedonknownexamplesofmaliciousandbenign packages.
if a package is flagged as malicious by a classifier wethen check whether it includes metadata about its source repos itory and if so whether the package can be reproduced from itssource code.
packages that are reproducible from source are not usually malicious so this step allows us to weed out false positives.
finally we also employ a simple textual clone detection technique toidentifycopiesofmaliciouspackagesthatmayhavebeenmissed by the classifiers reducing the number of false negatives.
amalfiimproves on the stateof the art in that itis lightweight requiring only a few seconds per package to extract features and run the classifiers andgivesgoodresultsinpractice runningiton96287 package versions published over the course of one week we were abletoidentify95previouslyunknownmalwaresamples witha manageable number of false positives.
ccs concepts security and privacy malware and its mitigation.
keywords supply chain security malware detection this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
reference format adriana sejfia and max sch fer.
.
practical automated detection of malicious npm packages.
in 44th international conference on software engineering icse may21 pittsburgh pa usa.
acm newyork ny usa pages.
.org .
.
introduction npm1is a system for publishing and consuming software packages for javascript and typescript.
while initially closely associated with the node.js platform2and back end javascript applications it isnotarchitecturallytiedtonode.js andhasalsofoundwidespread use with web applications and on other platforms.
the core concept of npm is the package registry which is a databaseofjavascriptpackageswithassociatedmetadata.while someorganizationsandenterpriseshosttheirownregistries byfar thebest knownregistryisthepublicnpmregistry accessiblevia the npm website which also provides facilities for browsing and searchingforpackages aswellasviewingtheirmetadata.inthis paper we exclusively concern ourselves with the public registry.
asofearlyseptember2021 npm spackageregistryhostsover .7millionpackages.someoftheseareprivatepackagesthatare only accessible to specific users or organizations but most of them are public and it is these public packages that are our focus.
over the course of a single week developers publish around publicpackageversions includingbothnewpackagesandupdated versionsofexistingpackages.historicversionsofapackageremain availableontheregistryunlesstheyareexplicitlyremovedeither by the package maintainer or by npm staff allowing dependent packagestorelyonspecificolderversionsofapackage forexample to make use of an api that has been removed in the latest version.
most developers interact with the registry through a commandline interface such as the npm cli3or yarn 4which can be used to download a particular version of an existing package and install it locally ortopublishanewpackageoranewversionofanexisting package to the registry.
when installing a package version the package manager will first recursively install the dependenciesofthatpackage unlesstheyarealreadyinstalled downloadthe tarballcontainingthepackagefromtheregistry unpackthetarballin the installation directory and finally run any installation scripts specifiedbythepackage.thesescriptsarefree formshellscripts that typically perform setup tasks such as downloading additional artifacts not bundled with the package itself.
due to the transitive natureofpackageinstallation popularpackagesaredownloaded very frequently for example the chalkpackage5 which offers .com .org .npmjs .com package npm .npmjs .com package yarn ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa adriana sejfia and max sch fer supportforcoloringterminaloutput wasdownloadedalmost89 million times a week at the time of writing.
publishing a new package or package version is the dual to this process anyone authenticated through the npm website can create a new package thereby becoming its maintainer and maintainers canpublishnewversionsatanytimebysimplyuploadingatarball totheregistry.whilepackagescanindicatearepositoryhosting theirsourcecode thisinformationisoptional.itisthemaintainer s responsibility to run any necessary build steps such as compiling typescript to javascript bundling and minifying etc.
before publishing the registry simply hosts the tarball and is largely agnostic to its content.
theoverwhelmingpopularityofnpmandthecentralroleitplays in the software supply chain for javascript and typescript which inturn areamongthemostwidely usedprogramminglanguagesat present has long made it a favorite target for attackers attempting topublishmaliciouspackageversionsthattamperwithorexfiltrate datafromthemachinestheyareinstalledon performparasitical computations such as bitcoin mining or other malicious activities.
recent examples include high profile incidents such as the eslint scope compromise 6where attackers managed to steal thecredentialsofamaintainerofapopularpackage allowingthem to publish a new malicious version of the package that uploaded user credentials to a server upon installation the event stream backdoor 7where social engineering techniques were used to gain maintainer status and then launch a similar attack and a steady stream of smaller incidents since then.
the malicious package versions were quickly removed by npm staff from the registry upon detection but in the case of eslint scope andevent stream notbeforebeinginstalledseveral million times.
while the number of affected users is much smaller in most cases the frequency with which such incidents occurstillposesasignificant dangertothesoftwaresupplychain both in terms of concrete damage to its users and in terms of reputationaldamagethatcouldpotentiallyimpedetheflourishingnot just of npm but also the wider open source ecosystem.
addressing this problem at a fundamental level would arguably requiresignificantchangestonpm perhapsincludingamoresecurepackage publishingmodeltopreventmaliciouspackagesfrom reaching the registry in the first place and or the node.js platform perhaps with access control enforcement to limit the damage a malicious package can do when it is installed.
our aim in this paper is more modest without making any changes to the fundamentals of npm we want to detect potentially malicious package versions as quickly as possible and report them to a human auditor for take down.
tobepracticallyuseful then ourapproachhastosatisfyatleast three requirements it has to be automated since the sheer number ofpackagesrendersmanualauditsinfeasible efficienttokeepup withthespeedat whichnewversionsarepublished and accurate to avoid flagging benign packages or missing malicious ones.
we achieve this by combining three complementary techniques into one system which we call amalfi .org blog postmortem for malicious package publishes .io blog a post mortem of the malicious event stream backdoor 8short for automated malicious package finder .
machine learning classifiers trained on labelled examples of maliciousandbenignpackages utilizingfeaturesthatrecord changes in the apis the package uses as well as package metadata extracted using a lightweight syntactic scan areproducer thatrebuildsapackagefromsourceandcompares the result with the version published in the registry aclonedetector thatfinds near verbatimcopiesofknown malicious packages.
our feature selection discussed in more detail below is motivated by the observation borrowed from garrett et al.
that maliciouspackagestendtomakeuseofdistinctivecapabilitiesof the javascript language such as runtime code generation the underlyingplatform suchasaccesstothefilesystemorthenetwork and the npm package manager such as install scripts .
while none of these features are dead giveaways by themselves in combination they are worthy of closer inspection especially if a package suddenlystartsusingcapabilitiesithasneverusedbefore.forexample the above mentioned eslint scope package uses runtime code generation andan install scriptin its malicious version .
.
capabilities it had never used before.
bytrainingonacorpusofmaliciousandbenignpackagesprovided to us by npm our classifiers learn to distinguish typical and therefore most likely harmless feature changes from atypical and therefore suspicious ones.
the choice of classifiers is constrained by the small size of the corpus which contains fewer than samples weexperimentedwiththreedifferenttechniques decision trees naive bayesian classifiers and one class svms.
toeliminatefalsepositives weborrowanotherinsightfromthe literature malicious package versions tend not to have theirsourcecodepubliclyavailable inordertoavoiddetection.
consequently being able to reproduce a package version from its source code is a good indicator that it is benign.
as has been notedpreviously evenperfectlybenignpackagesmayfailto reproduceforavarietyofreasons butthisisacceptableinourcase since we are only using this criterion to filter out benign packages erroneously flagged as malicious not to detect new ones.
finally wenotethatattackersoftenpublishmultipletextually identical copiesof oneand thesame maliciouspackage underdifferentnames.however sincepackagemetadatamaybedifferent our classifiers sometimes fail to spot these copies.
we use a simple clone detector that hashes the contents of a package minus the package name and version which are always unique to eliminate this source of false negatives.
an overview of how these different components work together can be seen in figure .
tomotivate ourapproachmore carefully we showtwotypical examplesofmaliciouspackagesinsection2anddiscusshowwe detect them.
section provides details on feature selection and extractionfortheclassifier aswellasanoverviewofthereproducer andclonedetector.weevaluateamalfiinsection4inalarge scale experimentonnewlypublishednpmpackagestodemonstrateits ability to find previously unknown malicious packages and in a cross validation experiment to evaluate precision and recall.
we discuss the results in section survey related work in section and outline conclusions and future directions in section .
9in fact we are not aware of a single counterexample to this rule.
1682practical automated detection of malicious npm packages icse may pittsburgh pa usa figure overview of amalfi in summary the significance of our contributions is as follows we present amalfi an automated approach for detecting malicious npm packages that uses a novel combination of techniquesandshowssolidresultsinpractice identifying95 previously unknown malicious packages.
among the three differentclassifiersweconsider thedecisiontreeperforms best though the others also contribute findings.
amalfi is efficient usually taking only a few seconds per package to extract features and run the classifier.
we also showthatretrainingtheclassifiersischeap thusallowing continuous improvements to be made as more and more results are triaged.
thefalse positiverate whileinitiallyquitehigh dropssignificantly as the classifiers are retrained on more data with fewer than one in a thousand packages being flagged spuriously.across validationexperimentonourtrainingsetalso showsthatthedecisiontreeachievesover40 recall suggesting that its false negative rate is reasonable.
supplementary materialsincludingexperimentaldataandresultsarepubliclyavailableat .org .
zenodo .5908852and .com githubnext amalfi artifact.
while the building blocks we use have been proposed before thenoveltyofourapproachliesintheircombination andamore thorough exploration and evaluation of the design space.
background to set the scene we discuss two representative examples of realworld malicious package versions that were manually detected and removed from the registry and then explain how we could have identified them automatically.
forthepurposesofthispaper wedefinea maliciouspackageversionto be a specific version of an npm package that contains code thatimplementsmaliciousbehaviorincluding butnotlimitedto exfiltratingsensitiveorpersonaldata tamperingwithordestroying data orperforminglong runningorexpensivecomputationsthat are not explicitly documented.
in particular we consider a package version to be malicious even if the malicious code it contains is disabled or broken.
moreover in line with npm s acceptable contentpolicy10weincludeinourdefinitionmaliciousbehaviorthatis ostensiblydoneforresearchpurposes.forbrevity wewilloftenuse the term malicious package the version part being understood.
.npmjs .com policies open source terms acceptable contentfromanattacker sperspective therearethreestepstodelivering malware through npm publish a malicious package version get users to install it and get them to run the malicious code.
theeasiestwaytogoabout istopublishacompletelynew package.aclassicwayofachieving inthisscenariois typosquatting whereby the name chosen for the new package is very similar to the name of a popular existing package a user who accidentallymisspellsthenameofthepopularpackagewillthenend upinadvertentlyinstalling themaliciouspackageinstead.
amore sophisticated approach is dependency confusion the attacker identifiesdependenciesonapackagehostedinaprivateregistry and then publishes a malicious package with the same name and a higher version number on the public npm registry clients of the private package may then end up installing the malicious package instead.finally therehavebeencasesofattackerspublishingan initially benign and useful package getting it added as a dependencytoapopulartargetpackage andthenpublishingamalicious version .
analternative morelaboriousstrategy toachieve isfor the attacker to compromise an existing popular package by gaining maintaineraccess forexamplebystealingmaintainercredentialsor bysocialengineeringasdescribedinsection1 andthenpublishing a new malicious version of that package.
in this case is easy sincethepackagealreadyhasmanyuserswhowill eitherexplicitly or implicitly upgrade to the malicious version.
finally a common tactic to achieve in either scenario is to useinstallationscriptswhich asexplainedabove arerunduring installationandcanexecutearbitrarycode.however thecommands run by installation scripts are by default logged to the console increasingtheriskofdetection.henceamorecarefulattackermay instead choose to hide their malicious code in some frequently executed bit of functionality in the main body of the package.
a typical example of a package employing typosquatting is mogodb a putative typo for the highly popular mongodbpackage whichiscurrentlyseeingaroundtwomillioninstallationsperweek.
two versions of mogodb numbered .
.
and .
.
were published within less than a millisecond of each other on august and identified as malicious and taken down a few minutes later.
as shown in figure a the package.json manifest file of the package registers a postinstall script to be run after package installation which executes the test.js script included in the package.thatscript showninfigure2 b harveststhehostnameof themachineonwhichthepackagewasinstalled andsendsitofftoa remotehostcontrolledbytheattacker.whiletheinformationbeing stoleninthiscaseisnothighlysensitive thisisclearlymalicious behavior.
even before studying the package implementation in detail a humanauditormightnoticefeaturesofthepackagethatmakeit seemworthyofcloserscrutiny suchasthepresenceofapostinstall script theusageofthepackages osandrequest andmostofall the extremelyshort timespan betweenthe publication ofthe two versions.
while the former two features are not by themselves suspicious theircombinationwitheachotherandwiththethird feature strongly suggests a malicious package.
atypicalexampleofacompromisedpackageis jasmin aw eb frameworkthatwasmoderatelypopularatonepointbuthasnot seenactivedevelopmentinanumberofyears.versions0.
.1and 1683icse may pittsburgh pa usa adriana sejfia and max sch fer name mogodb version .
.
scripts postinstall node test.js ... ... a package.json varremote host varhost require os .hostname 3require request remote ?h host function b test.js figure malicious code in mogodb .
.
simplified varremote host for varform ofdocument.forms for varelement ofform.eleme nts if element.type password form.addeventlistener submit function vardata .map function elt return elt.name elt.value .join document.cookie varenc encodeuricomponent btoa data this.action remote ?data enc break figure malicious code inserted into file component.js of jasmin .
.
simplified .
.2ofjasminarebenign butversion0.
.
presumablypublished byamaliciousactor containsthecodeshowninfigure3 which traverses all forms contained in an html document looking for passwordfields andoverridestheir submithandlertoharvest the contentofthesefieldsandsendthemtoanattacker controlledhost.
in this case there are no particularly suspicious features of the package that might draw the attention of a human auditor dealing with password fields encoding data using encodeuricomponent for transmission and accessing http cookies are all relatively innocent capabilities and are often used together.
what is immediatelysuspicious however isthatnoneofthesethreecapabilities wereusedinthepreviousversionof jasmin.moreover theupgrade from0.
.2to0.
.3isaminorversionupgrade whereonewouldnot expectmajornewfeaturesthatmightrequiresuchnewcapabilities to be introduced.
these examples and others like them suggest that a machinelearningbasedapproachmightbeabletodetectmaliciouspackages based on high level features like usage of particular apis platform capabilities and package metadata and in particular how these featureschangebetweenversions withouttheneedfordeepsourcecode analysis.
our approach having motivated what kind of features are interesting for automatedclassification wenowdescribeourfeaturesetinmoredetail andthenexplainhowtoextract single versionfeatures describing one package version as well as change features capturing the difference in features between two versions.
next we discuss our choice of classifiers and their training regimen.
finally we give somemoredetailsabouttheothertwocomponentsofourapproach the reproducer and the clone detector.
.
feature set based on manual inspection of known examples of malicious packages wedeterminedelevenfeaturesofinterest.nineofthemare single version features that can be extracted from the contents of a single package version while the other two intrinsically involve two versions of a package.
thesingle versionfeaturesareasfollows wherewegrouprelated features into categories and provide examples of each access to personally identifying information pii creditcard numbers passwords and cookies access to specific system resources a file system access reading and writing files b process creation spawning new processes c network access sending or receiving data use of specific apis a cryptographic functionality b data encoding using encodeuricomponent etc.
c dynamic code generation using eval function etc.
use of package installation scripts presenceofminifiedcode toavoiddetection orbinaryfiles such as binary executables the remaining two features concern two versions of a package and are thetime between publicationof the two versions and the typeofupdateinsemantic versioningterms major minor patch build or pre release .
the motivation for considering time between updates is that malicious package versions often exhibit unusual update patterns such as multiple versions published in very rapid succession as seen in the mogodbexample in section or a new version being published after years of inactivity which might suggest an accounttakeover .theupdatetype ontheotherhand candeterminewhether achange insomeother featureissuspicious ornot as explained above.
these two features along with the changes in the values of the ninesingle valuedfeaturesbetweenonepackageversionandthe previous version constitute our feature set.
inordertoaccommodatethefirstversionofapackageaswell we introduceapseudo updatetyperepresentingfirstversions consider theirtimebetweenupdatestobezero andtakethevaluesofthe single versionfeaturestobetheremainingchangefeatures.this enablesustonotonlydetectmaliciousupdates whereapreviously benign package becomes malicious but also packages that were malicious from the start.
1684practical automated detection of malicious npm packages icse may pittsburgh pa usa .
feature extraction tocomputethefirstfourcategoriesofsingle versionfeatures we parseeachjavascriptandtypescriptfileinthepackageusingtreesitter.11wethenusetree sitterastqueriestolookforsyntactic constructs corresponding to the features such as string literals containing the keyword password for pii access imports of the fs module for file system access and calls to evalandfunction for dynamic code generation.
similarly to check for the presence of installation scripts we parsethepackage.json fileandlookfordefinitionsof preinstall install andpostinstall properties.
minifiedorbinaryfilestendtohavehigherentropythanplain sourcecode sowecomputetheshannonentropyofallfilescontainedinthepackageandusethe averageandstandarddeviation of the entropy across all files as features.
to compute change features we use the publication timestamps provided by the npm view time command to obtain the time between updates in seconds.
we rely on an off the shelf semanticversioninglibrarytodeterminetheupdatetypeand foreachgiven version determine the previous version in chronological order.
finally wesimplysubtractthevaluesofthesingle versionfeatures across the two consecutive versions.
.
classifier training ourchoiceofclassifiersisdictatedbythecorpusoflabelledtraining datawehaveavailable.sincemaliciouspackagesaretakendown by npm immediately upon discovery most known examples of maliciouspackagesarenolongeravailableforinspection.however npm kindly agreed to make their archive of malicious package versionsdetectedupto29july2021availabletousforthepurposes of this study.
out of these packages are malicious versions of otherwisenon maliciouspackages i.e.
compromisedpackages.we added to the original dataset the benign versions of the same packagespublishedbythesamedate yieldingabasiccorpusof1790 labelledsamplesofmaliciousandbenignpackageversions.since thegoalof amalfiistodetectmaliciouspackages thebasiccorpus oversamples malicious packages i.e.
it contains more malicious packages than we would expect from a similarly sized random sample of npm packages.
this is a common strategy in learningbased approaches .
it is worth emphasizing that while compromised packages have a much bigger potential impact on the npm ecosystem they occur so rarely that there simply is not enough data to make them the sole focus of our study.
anecdotally however compromised and malicious packages use similar techniques to carry out attacks meaningthat theyshare features whichenables amalfi todetect both types of malicious packages.
sincethenumberofmalicioussamplesissmallercomparedto the total number of package versions in our dataset and on npm we had to use learning algorithms that handle imbalanced data well.further duetothenoveltyofthefeaturesinourapproach we soughtalearningalgorithmthatallowedustoanalyzetheimportanceofthefeaturesweselected.intheend thelearningalgorithms thatsatisfiedtheconstraintsweredecisiontrees naivebayesian classifiers and one class support vector machines svms .
we .github .io tree sitter picked the first one due to its ability to explain which features impactthefinaldecision andthetwolatteronesbecauseoftheir versatilitywhendealingwithimbalanceddatasetsasseeninanomaly detection work.
totraintheclassifiers weusethe sklearnlibraryforpython.
for the decision tree we use information gain as the split criterion.
for the naive bayesian classifiers we use the bernoulli variant which can only deal with boolean features so we omit the discrete features entropy average and standard deviation as well as update time andcollapsetheotherstoavalueof1ifthefeatureispresent and otherwise.
for the svm we choose a linear kernel and train onlyonbenignexamples sincethetaskofthisclassifieristodetect outlier versions that are noticeably different from the benign ones.
wedeterminedthe parameterofthesvm whichapproximates the number of expected outliers by conducting a leave one out experimentonourbasiccorpus.theexperimentshowedthatoptimal precisionandrecallareattainedfora valueof0.
meaningthat theclassifier expectsabout oneina thousandpackage versionsto be malicious.
.
reproducer and clone detector asexplainedinsection1 thereproducertakesagivenpackageversion and then attempts to rebuild the package tarball from source.
this is a heuristic process that may fail for a variety of reasons whilepackagescanspecifytheurloftheirsourcerepositoryin theirpackage.json file this information is optional and many packagesdonotprovideit ortherepositoryisnotpubliclyaccessible.packageversionscanalsospecifythegitshaofthecommit they were built from but again this information is optional.
while therearepopularconventionsforcreatingbranchesortagswith names reflecting the package version they correspond to many packages do not follow these conventions making it impossible to determine the correct commit.
the build commands to run to produce the package from its source are likewise not prescribed.
finally manypackagesneglecttospecifythepreciseversionofthe build tools such asthe typescript compiler they relyon leading to seemingly random differences between the reproduced package and the original.
for all of these reasons the success rate of the reproducer is low in practice as we shall see but it still serves a useful purpose as an automated false positive filter.
thethirdcomponentof amalfiisasimpleclonedetectorthat computes an md5 hash of the contents of a package tarball and compares it to a list of hashes of known malicious packages.
when computingthehash weignorethepackagenameandversionspecifiedinthe package.json file sincethesearealwaysuniqueand wouldcausespuriousmisses.nootherattemptatfuzzymatching is made so only verbatim clones are detected.
evaluation whilemotivatingandpresentingthedetailsofourapproachabove we have informally argued that its design makes it practically useable anduseful.
wewill nowback upthese claimswith anexperimentalstudy whichaimstoanswerthefollowingthreeresearch questions 12thesourcecodeofourclassifier trainingscriptsandthelistofpackagesinthebasic corpus are included in the supplementary materials.
1685icse may pittsburgh pa usa adriana sejfia and max sch fer date versions decision tree naive bayes svm clones t p f p t p f p t p f p t p july july july august august august august table results from experiment ndenotes tps contributed by the clone detector nfps eliminated by the reproducer.
rq1does amalfi find malicious packages in practice?
rq2is it accurate enough to be useful?
rq3is training and classification fast enough to be useable?
to answer these questions we conducted two experiments one experimentonalargesetof newlypublishedpackageversionsto assessperformance andonesmallerexperimentonlabelleddatasets to assess accuracy.
we will describe the experiments below.
.
experiment classifying newly published packages thiswas alarge scale experimentdesigned tosimulate arealistic scenario for automated malware detection in which we applied amalfi to all new public package versions published on the public npm registry over the course of a single randomly chosen week from july to august .
on the first day we trained our three classifiers on the basic corpusandthenusedthemtoclassifytheset n1ofallnewpackage versionspublishedthatday.additionally weranourclonedetector on the same set to find copies of malicious packages in the basic corpus acting as a fourth classifier.
this yielded a set p1 n1 of package versions flagged by at least one classifier.
we ran the reproduceronthissettoautomaticallyweedoutsomefalsepositives and manually inspected the rest.
the manual inspection was initiallyconductedbybothauthors witheachauthorexamining roughlyonehalfoftheflaggedversions.thepackageversionsthat were found to be malicious by one author were afterwards verified by the other author.
finally we reported the verified malicious packages to the npm security team.
all of them were subsequently taken down meaning that the npm security experts agreed with ourassessment.assuch weareconfidentthatourmanuallabeling of malicious packages is highly accurate.
the manual inspection resulted in a partitioning of p1into two setstp1andfp1of true positives i.e.
genuine malicious packages foundbytheclassifiers andfalsepositives i.e.
benignpackages falselyflaggedasmalicious .asalaststep werantheclonedetector againtofindadditionalcopiesofpackagesin tp1thatweremissed by the classifiers and added them to tp1.
on the second day we retrained the classifiers on the basic corpusaswellastheset n1triagedthepreviousday adding tp1 to our set of labelled malicious packages and everything else that is n1 tp1 to the set of benign packages.
in other words for the purposes of this experiment we assumed that any package notflagged by any of the classifiers was benign.
this is not truein general but the enormous number of new package versions publishedeachdaymadeitinfeasibletoinspectthemall andsince weexpectthenumberofmaliciouspackagesonanygivendayto be low it is not an unreasonable approximation to the unknown ground truth.
asonthefirstday wethenappliedtheclassifierstotheset n2of packagespublishedthatday ranthereproducerontheresultingset p2 manually inspected the rest and ran the clone detector to mop upanythingthatwasmissed yieldingaset tp2ofnewlyidentified maliciouspackages.onthethirdday weretrainedtheclassifiers using the basic corpus as well as both n1ann2 and so forth for each subsequent day.
the intuition here is that we want to mimic a usage pattern whereresultsfromtheclassifiersareinspectedbyahumanauditor and the classifiers are then retrained with the additional ground truth obtained in this way.
.
experiment classifying labelled data whilethefirstexperimentcanprovideinsightintotheperformance ofourapproachunderreal worldconditionsandinparticularits false positive rate it cannot tell us much about false negatives.
henceweranasecondexperiment measuringtheprecisionand recall of amalfi on the basic corpus.
its small size prevented us from separating it in a train and test fashion so instead we performed a10 fold cross validationexperiment repeatedlytraining theclassifierson90 ofthecorpusandmeasuringprecisionand recallontheremaining10 .giventheimbalanceinourdataset we usedstratifiedsamplingtomaintainthedistributionofmalicious and benign versions for each fold.
furthermore wealsomeasuredprecisionandrecallonalabelled datasetfromrecentworkbyduanetal.describingtheirmaloss system .
this dataset had some overlap with our basic corpus which we removed leaving only the unique data points for this experiment.
also the dataset initially only contained malicious packages to balance it we followed the same strategy as for the basiccorpusandaddedallbenignversionsofthecontainedpackages.intheend thisyieldedadatasetwith372packageversions out of which were malicious.
basedontheresultsfromthesetwoexperiments wewillnow answer the research questions posed above.
13thelistofpackagesconsideredinthisexperimentandtheresultsoftheclassification are included in the supplementary materials.
14detailed results for this experiment are included in the supplementary materials.
1686practical automated detection of malicious npm packages icse may pittsburgh pa usa .
rq1 practical performance on newly published packages the results of the first experiment are presented in table .
the tablecontainsthedateforwhichwecollectedthepackageversions date as well as the total number of versions published on that date versions .then foreachclassifier thetablecontainsthe numberoftruepositives tp andfalsepositives fp flaggedby theclassifier annotatedwiththenumberofadditionaltruepositives found by the clone detector n and the number of false positives eliminated by the reproducer n .
as explained above all true positives were confirmed by the npm security team.
thus forexample theentry16 2inthe tpcolumnforthe decision tree onaugust means that the classifier flagged true positive among the package versions published that day of which the clone detector found two additional copies.
the entry in the fpcolumn means that among the nine false positives itflagged onewassuccessfullyreproducedandhenceeliminated automatically.
asexplainedabove theclonedetectorisalsotreatedlikeafourth classifier.ithasnofalsepositivesandnevermissesidenticalcopies hence this column only contains a single number per day.
note again that in this column we show the number of clones found on that same day as opposed to the entries after the sign which depict the number of clones from the previous days.
the first takeaway from this table is that the number of new packagespublishedeverydayishigh butquitevariable withalmost four times as many packages being published on july a thursday than on august a sunday .
secondly we can see that all our classifiers are able to correctly classifymaliciouspackageversions withvaryingdegreesofsuccess.
the decision tree performs better than the rest especially in terms of true positives.
removing the overlap between classifiers we wereabletoidentify95previouslyunknownmaliciouspackages over the course of these seven days which is a significant number especially considering that the entire set of malicious packages detected prior to our work only contained samples.
third wenoticethatonthefirstdayallthreeclassifiersproduce anunmanageablenumberofresults.wethereforehadtomodify ourapproachandonlyexaminedasubsetofallflaggedpackages indetail assumingalltheresttobefalsepositives.thismeansthat the false positive counts for this day are likely to be overstated.
however oncethisset ofpackagesisadded tothetraining seton thesecondday thenumberofresultsdropsdramatically andbythe endoftheweekallthreeclassifiersyieldarelativelylownumber of false positives.
fourth ourresultsshowthatthereproducerhasalowsuccess rate in practice only being able to reproduce one or two packages on any given day.
however given the overall low number of alerts towards the end of the week this is still a valuable improvement.
similarly clone detection only contributes a few additional true positives each day the packages detected on august being an outlier and mostly overlapping with the results from the decision tree but it still improves the overall results.
both mechanisms are computationallyinexpensive andthusthehelptheyprovidecomes at a low cost making them worth keeping in spite of their limited contributions.
conversely this shows that our classifiers add valuebeyondapurelytextualscanlookingforverbatimcopiesofknown malware.
in summary we can answer rq1 in the affirmative amalfi does indeed detect malicious packages in practice.
further since all the packages found by amalfi and reported to npm had not been identified before we can confidently claim that our approach complements existing solutions for malicious package detection in npm.
dataset decision tree naive bayes svm prec.
recall prec.
recall prec.
recall basic .
.
.
.
.
.
maloss .
.
.
.
.
.
table results from experiment .
rq2 accuracy the results from experiment are presented in table .
the first row shows precision and recall measurements from the fold cross validation experiment on our basic corpus averaged over all ten runs.
the numbers for the svm classifier have to be interpreted with care since its parameter was fitted on this verydataset hencewehaveputtheminbrackets.allourmodels achieveveryhighprecision buttherecallofnaivebayesandsvm issomewhatpoor.thisisexpectedduetothelowpriorofmalicious packages.
thesecondrowshowsprecisionandrecallfromrunningamalfi on the maloss dataset derived from the literature as explained above.
we see that the recall is higher than with the previous row at the expense of precision.
these results point to the trade off betweenthesetwometrics butitisalsoworthpointingoutthatthe malossdatasetcontainsanumberofpackageslabeledasmalicious where npm disagreed with the authors assessment and did not take them down.
a more detailed comparison of amalfi to maloss is unfortunately not possible since they do not present statistics on false positives or performance.
the heavy weight nature of their approachanditscomplicatedsetupinvolvingasophisticatedpipeline combining static and dynamic components made it infeasible to run on our dataset.
based on these results and the false positive numbers discussed above we can give a cautiously positive answer to rq2 amalfi is reasonablypreciseanddoesnotproduceanoverwhelmingnumber ofresults makingmanualtriagingofresultsbyahumanauditor feasible.
the second experiment suggests that there may be a good numberoffalsenegatives butatleastthenumbersforthedecision tree look promising.
.
rq3 performance to characterize the performance of our approach we measured threemetrics i thetimeittakestotraintheclassifier ii thetime it takes to extract features for a package version and iii the time it takes to classify a package version.
all three measurements were obtained as a byproduct of eperiment .
1687icse may pittsburgh pa usa adriana sejfia and max sch fer figure classifier training time figure shows how long it took to train the classifiers for each dayofourexperimentasafunctionofthesizeofthetrainingset.
as can be seen training is quite fast taking no more than a few secondsdespitethesteadyincreaseintrainingdata.thesvm based classifier takes the longest time though it still seems to scale more orlesslinearlyinthetraining setsize.theothertwoclassifiersare very quick to train and it is clear that the training set sizecould be increased substantially before training time becomes a bottleneck.
tobenchmarkfeatureextraction wepost processedlogsfrom our run of experiment to measure the time it takes to extract features for a randomly chosen set of around package versions.
by and large feature extraction takes less than ten seconds and for over half the packages considered it takes less than one second.
however asingleoutlierpackagecontainingmorethan11 000files takes more than ten minutes to extract somewhat skewing the distribution for an average extraction time of six seconds.
lastly we measured the time it took to predict whether a given packagewasmalicious.foralltheclassifiers thetimeforprediction was less than a second.
based on these results we can give a positive answer to rq3 amalfi is fast enough for practical use.
.
threats to validity whiletheresultsofourevaluationareoverallverypromising there are some threats to the validity of our conclusions.
first whilethesetofpackagesweconsideredinexperiment1 was taken from the wild it may have been biased in ways that we did not anticipate and so our results may not generalize.
also the basic corpus is to some degree biased in that it contains clusters of similar malware samples resulting from copy cat campaigns.
second while we examined all packages flagged by amalfi and reportedthe true positivesto npm welimited ourselves toat mostfiveminutes inspectiontimeperpackage whichprevented detailed investigation of some of the larger ones and may have caused us to miss true positives.
conversely we may have been mistakeninlabellingsomepackagesmalicious leadingtomissed false positives butthis seems unlikely considering thatnpm have takenallreportedpackagesdown meaningthattheyagreewith our assessment.finally as noted above in our retraining step in experiment we assumed packages that were not flagged by any classifier to be benign.thisisnotasoundassumptioningeneral andmightendup increasing the number of false negatives over time.
for this reason and also to escape the slow but inexorable rise in training time suggested by figure in practice one would not want to continue retraining in this fashion indefinitely.
table suggests diminishing returns from retraining after a few days but the data is clearly too sparse to draw a definite conclusion.
discussion inthissectionwereviewthetypesofmaliciouspackagesourmodels found takeacloserlookatthemodelsthemselves andfinallytouch upontweakstoourapproachweinvestigatedbutwereshownto be unsuccessful or unnecessary.
table details the types of discrete features exhibited by the maliciouspackageswefound whilefigure5showsthedistribution of the entropy and time features of malicious and benign packages using boxplots.
all packages used installation scripts or code in their main moduletoconnecttoaremotehost withalmostallofthemsendingpii tothathostexceptforasmallhandfulofpackagesthatonlypinged thehostwithoutsendinganyinformation perhapsasaproof ofconceptorinpreparationforanactualattack.ourfeatureextractor didnotdetectthepiiaccessesthemselves pointingtoaneedtoimproveourdetectionofthisfeature butthepackagesweredetected anyway usually because of the usage of the installation scripts or network access.
this suggests that our approach is robust enough tofindvarioustypesofmaliciouspackageversions.thefactthat some features do not appear in these specific packages does not necessarilyindicatetheyareuselessorunnecessary thosefeatures attempttopaintageneralpictureofmaliciousnessinpackagesand they may prove to be useful in other batches.
a surprising observation in the data was the distribution of updatetypes themajorityofthemaliciouspackageversionswefound weremajorupdates contradictingourassumptionthatmalicious package updates tendto hide behind aminor update.
this could also mean that we missed malicious package versions representing a minor update.
thedistributionofaverageentropyvaluesshowsthatthemedian highlighted by the thick redbar is significantly higher for feature of packages file system access process creation network access data encoding use of package installation scripts update type major update type minor update type patch update type prerelease update type first table features found in the malicious packages 1688practical automated detection of malicious npm packages icse may pittsburgh pa usa maliciouspackages .
thanforbenignpackages .
suggesting they are more likely to contain minified code or binary files in line with our expectation.
thetime betweenupdatesalso follows a rather expected distribution with malicious packages exhibiting a shortermedian timebetween updates .02s thanthebenignones .
s .
figure entropy left and time right value distributions next wetookalookatthegeneratedclassifiersandhowtheir predictionsoverlap.figure6showsasummaryoftheresultsonthe packages flagged by the classifiers the remaining five having beenflaggedonlybytheclonedetector .whilethedecisiontree takes the lion s share each individual algorithm makes its own contribution suggesting that a combination of all three might be a good choice in practice.
sincethedecisiontreeclassifiersaretheonesthatfacilitateinterpretationwetookalookatthefeaturestheyusetomakedecisions.
we noticed that the classifiers for july to july examine all features except the one representing uses of cryptographic functionality andtheremainingfourclassifiersfromaugust1onwards employ all eleven features suggesting that there is not much redundancy in our feature set.
figure6 overlapamongthethreedifferentclassifiersonthe malicious packages they flaglastly we tried out several tweaks that ultimately did not prove successful.
the literature often recommends using random forest classifiers instead of plain decision trees but we did not find them to provide any advantage in our setting.
we also investigated booleanizing features for the decision tree and one class svm but in both cases this led to worse performance in the latter case increasing the rate of false positives by more than .
related work ourworkhasconnectionswithfourdifferentresearchareas which we survey briefly malicious package detection proper malware and anomaly detection more generally package registry security and security implications of code reuse.
malicious packagedetection.
previousworkinthisareacanbe broadly divided into four categories general purpose maliciouspackage detection approaches using machine learning o rp r o gram analysis techniques for rebuilding packages from source and finally work that specifically targets typosquatting .
garret et al.
s work on detecting malicious npm packages using machine learning techniques is very closely related to our work.
they use a k means clustering algorithm to identify anomalous and hence suspicious package updates.
likeus theycollectfeaturesforpackageupdates notjustsingle package versions and the set of features they consider overlaps to some extent with ours as shown in table .
in particular they also consider access to system resources dynamic code generation and use of installation scripts.
we additionally consider access to pii and several specific apis as features which they do not though to some extent this is covered by their feature recording added dependencies.
their feature set also does not directly model the presenceofminifiedcodeorbinaryfiles thoughagaintheydohave a more general feature for added code that is similar in spirit.
they do not consider update type a feature instead accounting for their different characteristics by training separate models for each type ofupdate.finally theydonothaveanyfeaturecorrespondingto our time between updates.
our work is larger in scope than theirs considering three different kinds of classifiers instead of just one and complementing the classifiers with package reproduction and clone detection.
their evaluation on a set of package updates suggests that their approachleadstomanymorealertsthanours flagging539updates aspotentiallysuspicious theydidnottriagetheresultsindetail so itisunknownwhethertheysucceededinfindingmaliciouspackages.ourexperimentscoveramuchlargersetofpackages andwe haveshown thatwe candetecta significantnumber ofpreviously unknown malicious packages.
duan et al.
study recent examples of supply chain attacks pinpointingrootcausesandclassifyingattackvectorsandmalicious behaviors.
based on their results they built an analysis pipeline leveraging acombination of staticand dynamic program analysis techniquestodetectmaliciouspackagesacrossthreedifferentpackageregistries npm pypi 15andrubygems16 .inalarge scaleexperiment covering more than one million packages their approach .org .org 1689icse may pittsburgh pa usa adriana sejfia and max sch fer category feature amalfi garrett et al.
access to pii check access to system resourcesfile system access check check process creation check check network access check check use of specific apiscryptographic functionality check data encoding check dynamic code generation check check use of installation scripts check check presence of minified code and binary files check time between updates check update type check added dependencies check added code check table comparison of features considered in our models and those of garrett et al.
identified previously unknown malicious packages of them on npm.
they do not provide precise statistics on false positives or on the performance of their approach.
on the whole our goals differ from theirs where they aim to provide a comparative framework for the security of registries we specifically focus on finding malicious npm packages.
they update theirdetectionrulesmanuallyasresultsareassessed whileourclassifierscanberetrainedwithoutfurthermanualeffortbeyondthe assessment of results itself.
our approach seems simpler and more lightweight than theirs not requiring potentially expensive deep static analysis or potentially dangerous code execution for dynamic analysis.17nevertheless we manage to find more malicious packages on a smaller set than they do.
pfretzschneretal.
proposetheuseofstaticanalysistodetectusesofjavascriptlanguagefeaturesthatcanmakeapackage vulnerable to interference from a malicious downstream dependency.theypresentfourdifferentattackscenariosinvolvingglobal variables monkey patching and caching of modules though they did not find real world examples of such attacks.
ohm et al.
propose a dynamic analysis for observing and measuring the creation of artifacts during package installation as a way of detecting malicious packages.
while this is a promising researchdirectionandpotentiallypracticallyveryuseful approaches relying on code execution inherently tend to be more heavyweight and harder to scale than our lightweight feature extraction.
at the shallower end of the analysis spectrum tools like microsoft application inspector18and ossgadget19offer regularexpression based scanning as a way of quickly detecting various typesofpotentialmalware includingmaliciouspackages.however these tools tend to be very noisy in practice and produce many false positives precluding large scale usage.
severalresearchershaveproposedcheckingfordifferencesbetween packages hosted on registries and their purported source codeasawayofdetectingmalware.goswamietal.
reportthat 17whilethereproducerdoesexecutecode itonlyrunsbuildscripts whichareless likely to be malicious.
.com microsoft applicationinspector .com microsoft ossgadgetthisisdifficultfornpmpackagesduetomanyirrelevantbutnonmalicious differences an experience that tallies with ours.
vu et al.
studythesameproblemforpypi andsimilarlyconclude that non reproducibility by itself is a weak indicator of maliciousnessandneedstobecombinedwithothertechniquestobecome effective which is what we have done in this work.
forthespecificproblemofdetectingtyposquatting vuetal.
propose using edit distance as a metric for finding packages whose nameisverysimilartoanother whiletayloretal.
employa combinationoflexicalsimilarityandpackagepopularity.ourwork doesnot specificallyfocuson typosquatting but maystill be able to identify such packages from other criteria.
anomalydetection.
malicious packagedetectionisaparticular instance of the more general problem of malware detection which in turn is often phrased in terms of anomaly detection where malware is characterized as anomalous outliers in a larger set of benign samples.
this framework has been brought to bear in a widevarietyofcontexts includingdetectinganomalouscommits ongithub aswellasmaliciouswebsites binaries and mobile apps .
applying machine learningtechniques in thesedomains often faces the problem of imbalanced datasets just like in our case.
this line of research pointed us to the advantages of using decision trees naive bayes andone classsvms asthebase algorithms for our models.
more complicated models based on neuralnetworks werenotsuitableforourproblemgiventhe relatively small dataset in our p ossession.
ho wever with more and better data this could be an avenue for future research.
package registry security.
the security mechanisms of the packageregistriesandtheimpactofmaliciouspackagesontheseregistrieshavealsobeenstudiedextensively.ohmetal.
sstudy explores the forms attacks can have on different registries.
others have focused on specific registries such as pypi or npm .ononehand mechanismstounderstandtheimpactof maliciouspackageshavebeenproposed .ontheotherhand studieshavealsofocusedonwaysregistriescanmitigatetheimpact ofmaliciouspackages .comparedtotheselonger termsolutions 1690practical automated detection of malicious npm packages icse may pittsburgh pa usa which are of course well worth investigating our focus is on short term mitigation measures that do not require any changes to the registries themselves as explained in section .
code reuse and security.
our work touches upon the risks of codereuse whichhasrecentlyseenafairamountofinterestinthe research literature.
wang et al.
investigate third party library usage in java with particular focus on the problem of outdated libraries that may be lacking recent bug fixes.
they find that maintainers of client projectsareoftenunwillingtoupdatetomorerecentlibraryversions even when alerted to severe bugs in the version they depend on.
prana et al.
report similar results from a study covering vulnerabledependenciesinjava python andruby.interestingly they conclude that different levels of development activity project popularity and developer experience do not affect the handling of vulnerable dependency reports.
mirhoseini et al.
find that automated upgrade pull requests improve the situation to some extent although they can also have the adverse side effect of overwhelming maintainers with upgrade notifications.forthecaseofmaliciousnpmpackages thisisless of a problem since they are taken down upon discovery and hence can no longer be depended on.
gkortzisetal.
specificallyexaminetherelationshipbetween softwarereuseandsecurityvulnerabilities.asonemightexpect theyfindthatthelargeraprojectthemorelikelyitistobeaffected bysecurityvulnerabilities andsimilarlythatprojectswithmany dependencies are more exposed to security risks.
while their work focusses on vulnerable code as opposed to malware it stands to reason that similar correlations exist in the latter case.
tomitigatethisproblem koishybayevetal.
proposeastatic analyzercalledmininodethateliminatesunusedcodeanddependencies from node.js applications thereby reducing their attack surface.
conclusion we have presented amalfi an approach to detecting malicious npm packages based on a combination of a classifier trained on known samples of malicious and benign npm packages a reproducerforidentifyingpackagesthatcanberebuiltfromsource anda clonedetectorforfindingcopiesofknownmaliciouspackages.the classifier works on a set of features extracted using a light weight syntactic analysis including information about the capabilities the package makes use of and how these change between versions.
we have presented an evaluation of our approach employing threedifferentkindsofclassifiers decisiontrees naivebayesian classifiers and svms.
in our experiments all three techniques succeeded in detecting previously unknown malicious packages with the decision tree outperforming the other two though each classifier contributed unique results.
while all three classifiers producefalsepositives theirprecisioncanbeimproveddramatically through continuous retraining as past predictions are triaged.
we havealsoshownthattraining featureextraction andclassification are very fast suggesting that amalfi is practically useful.
forfuturework weareplanningoninvestigatingdeeperfeature extractionthatgoesbeyondthepurelysyntacticapproachwehave used so far perhaps employing light weight static analysis.
wewould also like to experiment with more advanced clone detection approaches to identify similar but not textually identical copies of maliciouspackages.anotherareaworthexploringwouldbehowto combine results from multiple classifiers perhaps in the form of a rankingofresultsthatcouldaidinmanualtriaging.finally itwould beveryinterestingtoapplyourtechniquestootherecosystemssuch aspypiorrubygems whichalsosufferfrommaliciouspackages in much the same way as npm.