ast trans code summarization with efficient tree structured attention ze tang state key laboratory for novel software technology nanjing university nanjing china qq.comxiaoyu shen alexa ai amazon berlin germany gyouu amazon.comchuanyi li jidong ge state key laboratory for novel software technology nanjing university nanjing china lcy gjd nju.edu.cn liguo huang department of computer science southern methodist university dallas texas usa lghuang lyle.smu.eduzhelin zhu bin luo state key laboratory for novel software technology nanjing university nanjing china zzl luobin nju.edu.cn abstract code summarization aims to generate brief natural language descriptions for source codes.
the state of the art approaches follow atransformer basedencoder decoderarchitecture.asthesource code is highly structured and follows strict grammars its abstract syntax tree ast is widely used for encoding structural information.however astsaremuchlongerthanthecorresponding source code.
existing approaches ignore the size constraint and simply feed the whole linearized ast into the encoders.
we argue thatsuchasimpleprocessmakesitdifficulttoextractthetrulyuseful dependency relations from the overlong input sequence.
it also incurssignificantcomputationaloverheadsinceeachnodeneeds to apply self attention to all other nodes in the ast.
to encode the ast more effectively and efficiently we propose ast transin this paper which exploits two types of node relationships in theast ancestor descendantandsiblingrelationships.itapplies the tree structured attention to dynamically allocate weights forrelevant nodes and exclude irrelevant nodes based on these tworelationships.wefurtherproposeanefficientimplementationto supportfastparallelcomputationfortree structureattention.on thetwocodesummarizationdatasets experimentalresultsshow thatast transsignificantlyoutperformsthestate of the artswhile being times more efficient than standard transformers1.
work done before joining.
1all thecodes and dataare availableat trans.git permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
concepts softwareanditsengineering documentation computing methodologies natural language generation.
keywords tree based neural network source code summarization acm reference format ze tang xiaoyu shen chuanyi li jidong ge liguo huang and zhelin zhu binluo.
.ast trans codesummarizationwithefficienttreestructured attention.
in 44th international conference on software engineering icse may21 pittsburgh pa usa.
acm newyork ny usa pages.
introduction thesummaryofsourcecodeisabriefnaturallanguagedescription explainingthepurposeofthecode .thecodetobesummarized can be with different units.
in this work we focus on summarizing the subroutines or defined methods in a program.
previousstudieshaveshownthatsuchashortdescriptioncan assistprogramdeveloperstoquicklydigestthecodewithouttraversing over it themselves .
nonetheless maintaining high quality codesummariesrequiresexpensivemanuallaborinreality.inmany projects these summaries are often mismatched missing or outdatedwhichslowdownthedevelopingprogress .automatic code summarization can greatly save developers time by avoiding writing such summaries manually for every single code snippet.
the traditional methods utilized handcrafted rules like software word usage model swum or stereotypes to synthesize the code summaries.
however when identifiers or methods arepoorlynamed theycannotextractaccuratekeywordstoproducegoodsummaries.someusedinformationretrieval ir techniques tominesummariesfromsimilarexistingcodebanks which unfortunately cannot generalize to unseen code snippets with different functions.
recently withthedevelopmentofopensourceplatformssuchas github more and more data for code summarization can be easily extractedfromonlineresources.data drivenstrategiesbasedon ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ze tang xiaoyu shen chuanyi li jidong ge liguo huang and zhelin zhu bin luo astcodereturn if x else return x itself.
orelse nameload x body constant compare constant lt nameload x ifexpreturnsummary figure example of code ast summary triples.
we mainly need tounderstandtheancestor descendentandsiblingrelationshipsin the ast to generate a summary.
neural networks start to raise more and more attention .
current state of the arts all follow the transformer based encoder decoder architecture and can be trained end to end with code summary pairs.
since the source code is highlystructuredandfollowsstrictprogramminglanguagegrammars a common practice is to also leverage the abstract syntax tree ast tohelptheencoderdigestthestructuredinformation.
theastisusuallylinearizedbydifferentalgorithmslikepre order traversal structure basedtraversal sbt andpathdecomposition thenfedintotheencoder.severalworksalsoproposed architectures specific for tree encoding like tree lstm .
however the linearized asts as containing additional structuredinformation aremuchlongerthantheircorrespondingsource code sequence.
some linearization algorithms can further increase the length.
for example linearizing with sbt usually makes the size times longer.
this makes the model extremely difficult to accurately detect useful dependency relations from the overlong input sequence2.moreover itbringssignificantcomputationaloverhead especially for state of the art transformer based models where thenumberofself attentionoperationsgrowsquadraticallywith thesequencelength.encodingastswithtree basedmodelslike tree lstm will incur extra complexity because it needs to traverse the whole tree to obtain the state of each node.
in this work we assume that the state of a node in the ast is affectedmostbyits ancestor descendentnodes whichrepresent the hierarchical relationship across different blocks and sibling nodes which represent the temporal relationship within one block.
we show an example of code summarization in figure .
as can be seen we need the ancestor descendent relationship to understand thehigh levelprocedure andthesiblingrelationshiptounderstand thelow leveldetailswithinablock.capturingthesetworelationships are enough for producing the summary and modelling the full attention among all nodes is unnecessary.
based on this intuition we propose ast trans a simple variant ofthe transformer modeltoefficientlyhandle thetree structured ast.ast transexploitsancestor descendantandsiblingrelationship matrices to represent the tree structure and uses these matrices to dynamically exclude irrelevant nodes in different selfattentionlayers.theabsolutepositionembeddingfromtheoriginal transformer is replaced with relative position embeddings defined 2indeed encoding the overlong ast with sbt even underperforms directly encoding the source code when using transformer with relative position embeddings .bythetworelationshipmatricestobettermodelthedependency.
we further describe several implementations of the proposed asttrans andhave acomprehensiveanalysisof theircomputational complexity.
in short the contributions of this paper are as below we propose ast trans that can efficiently encode long ast sequences withlinear complexity incontrast with thequadratic complexity of the standard transformer.
weperformacomprehensiveanalysis withboththeoretical andempiricalevidences onthecomputationalcomplexity of different implementations.
wevalidateourproposedmodelontwodatasetsofjavaand python.experimentalresultsshowthatast transoutperforms the state of the arts by a substantial margin.
we compare representative methods for ast encoding and discuss their pros and cons.
paperorganization theremainder ofthispaperisorganized as follows.
section presents background knowledge on the transformerandast.section3elaboratesonthedetailsofast trans section presents its different implementation and the complexity is analyzed in section .
section explains the experimental setup andanalyzestheresults.section7discussesthreatstovalidity.section surveys the related work.
finally section concludes the paper and points out future research directions.
background transformer.
thetransformerarchitecturewasinitiallyproposed forneuralmachinetranslation .itconsistsofmulti headstacked encoder and decoder layers.
in each encoder stack the inputs first flow through a self attention sublayer and then are fed into a position wise feed forward network followed by a layer normalization.
the decoder has a set of the cross attention layers to help the decoderfocusonrelevantpartsoftheinputsequence.thetransformer architecture removes the recurrence mechanism in favor of theself attention.aseachwordinasentencesimultaneouslyflows throughtheencoderanddecoderstack themodelitselfdoesnot have any sense of the word order.
therefore a position embedding is added to each word embedding to inform the order information.
abstract syntax tree ast .
an abstract syntax tree ast uniquely represents a source code snippet in a given language andgrammar .theleavesofthetreeareterminals usuallyreferring to variables types and method names.
the non leaf nodes arenon terminalsandrepresentarestrictedsetofstructuresinthe programming language e.g.
loops expressions and variable declarations.forexample infigure1 variables suchas nameload x arerepresentedasterminalsofast.syntacticstructures suchas compare are represented as non terminals.
since the variable and method names can be rather freely defined directly processing the sourcecodecanbechallenging.itscorrespondingast duetoits strictstructure oftenservesassubstitutewhenencodingthesource code.
ast trans this section details our proposed ast trans.
for an ast it will be firstly linearized into a sequence.
then the ancestor descendent and sibling relationships among its nodes will be denoted through authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ast trans code summarization with efficient tree structured attention icse may pittsburgh pa usa table linearized ast of the tree in fig with pot sbt and pd.
methods linearized ast sequence potreturn ifexp compare nameload x lt constant body constant orelse nameload x sbt return ifexp compare constant constant lt lt nameload x nameload x compare bod y constant constant bod y orelse nameload x nameload x orelse ifexp return pdpath1 path1 lt compare constant path2 nameload x compare constant path3 path3 constant compare ifexp body constant ... twospecificmatrices.basedonthematrices wereplacethestandardself attentionwithtree structuredattentiontobettermodel these two relationships.
irrelevant nodes are dynamically ruled outto reducecomputationalcost.wewill firstintroducedifferent linearizationmethods section3.
thenexplaintheconstruction of two relationship matrices section .
and finally present the tree structure attention to utilize the matrices section .
.
.
ast linearization inordertoencodethetree shapedast itfirstneedstobeconverted into a sequence with a linearization method.
there are the three most representative linearization methods used in current works pre order traversal pot it visits the tree nodes with preordertraversal.sequencesobtainedbypre ordertraversal are lossy since the original asts cannot be unambiguously reconstructed back from them.
structure based traversal sbt it adds additional brackets toindicatetheparental descendentrelationshipsuch thateachsequencecanbeunambiguouslymappedbackto the ast but it also doubles the size of the linearized sequence.
path decomposition pd it represents the ast by concatenatingthepathbetweentworandomleafnodes.thetotal numberofpathscanbetoolargeforcomputingandtherefore random sampling is needed .
table shows the ast in figure linearized with the above three different methods.
for pot and sbt the linearized trees can be directly fed into the encoder.
for pd the average total numberofpathscanbeover200 concatenatingthemalltotrain is infeasible .
in practice mean pooling is run over the states of each path such that each path has one unique representation.
the decoder only attends to these unique representations of paths instead of specific nodes within paths.
this can affect the model when copying user defined names in leaf nodes is needed.
we adopt the simplest pot linearization for our model.
we show that it has already achieved sota results and more complex linearizationmethodslikesbtdonothelp.pddoesnotapplytoourmodelsinceittreatsonepathasawhole.wewillshowinsection6.
that this leads to poor performance in code summarization.
.
relationship matrices wedefinetwokindsofrelationshipsbetweennodesinthetreethat wecareabout ancestor descendant a andsibling s relationships.
theformerrepresentsthehierarchicalinformationacrossblocks andthelatterrepresentsthetemporalinformationwithinoneblock.
figure example of generating position matrices for ancestordescendent a and sibling relationship s .
position matrix generatedfromthelinearrelationshipisusedinstandardtransformers.
specifically twonodeshavetheancestor descendantrelationshipif thereexistsadirectedpathfromrootnodethatcantraversethrough them.
two nodes have the sibling relationship if they share the same parent node.
we use two position matrices an nandsn nto represent theancestor descendentandsiblingrelationshipsrespectively.
n is the total number of nodes in ast.
we denote the ith node in the linearized ast as ni.aijis the distance of the shortest path between niandnjin the ast.
sijis horizontal sibling distance betweenniandnjintheastiftheysatisfythesiblingrelationship.
if one relationship is not satisfied its value in the matrix will be infinity.notethatweconsidertherelativerelationshipbetweentwo nodes which means aij ajiandsij sjiif a relationship exists between niandnj.
formally we use spd i j andsid i j to denote the shorted pathdistanceandhorizontal siblingdistancebetween niandnj in the ast.
the values in the relationship matrices are defined as aij braceleftbiggspd i j if spd i j p otherwise sij braceleftbiggsid i j if sid i j p otherwise pis a pre defined threshold and nodes with relative distance beyondpwill be ignored.
we hypothesize that precise relative distanceisnotusefulbeyondacertainrange.itcanbothconstrainthecomputationcomplexitywithinaconstantrangeandsavememory space for storing the relative position embeddings.
figure shows anexampleofgeneratingmatrix aands incomparisonwiththe position matrix generated from a linear relationship which is used in standard transformers.
in the next section we will introducehow to use these two matrices to dynamically incorporate such relationship information through a tree structured attention.
.
tree structured attention tree structured attention is built on the standard self attention with relative position embeddings and disentangled attention.
it replacestherelativepositionembeddingsderivedfromthelinear relationship into the two matrices derived from the tree structure.
self attention.
standard self attention transforms the input sequence x x1 ... xn xi rdwhichstandsfortheembedding ofni intoasequenceofoutputvectors o o1 ... on oi rd .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ze tang xiaoyu shen chuanyi li jidong ge liguo huang and zhelin zhu bin luo the single head self attention can be formulated as ij q xi k xj intercal d oi n summationdisplay.
j 1 ij v xj where q k rd rmarequeryandkeyfunctionsrespectively v rd rdis a value function is a scoring function e.g.
softmax or hardmax .
relativepositionembedding.
eq2isacontent onlyattention withoutanypositioninformation.theinitialtransformermodel uses absolute position embeddings to inform about the position.
shaw et al .
proposed replacing them with relative position embeddings whichhasshownmoreeffectiveincodesummarization tasks .
the relative position i j reflects the pairwise distance between niandnj.
denotepas the max relative distance i j can be defined as i j for i j p 2pfori j p i j pothers.
in this way wecan map each relative distance intoan embedding representation.
the relative position embeddings can be added on top of eq to inform the pairwise distance.
disentangledattention.
disentangledattention usesrelative position embedding as bias in self attention process.
each word is represented using two vectors that encode its content and relativepositioninandisentangledway.theattentioncomputationisthendividedintothreeparts content to content contentto position and position to content defined as i j q xi k xj intercal bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipupright content to content q xi kp i j intercal bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipupright content to position qp j i k xj intercal bracehtipupleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipdownright bracehtipdownleft bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehext bracehtipupright position to content where qp kp r 2p mrepresentthequeryandkeyprojection matrices of relative positions.
kp i j is the i j th row of kpandqp i j isthe i j throwof qprespectively.thelasttwo items i.e.
content to position and position to content are used to measure the relative positions between a word pair.
besides for content to position computation as all possible relative positions are always in the scores of query content q x to all key positions kpcan be first computed as q x kp intercal and then gathered into with i j as index.
in this way the relative position embedding can be reused for all query contents and thus reduce the space complexity to o 2pm .
attentionwithtree structuredrelationships.
ourmethod essentially replaces i j the relative distance defined under the linear relationship with r i j whererstands for either the ancestor descendent relationship aor the sibling relationship sin the tree structure.
r i j is defined as r i j braceleftbiggrij p 1i frij 0i frij rijreferstoeither aijorsijdefinedineq1.astherearetwokinds of relationships we consider only one relationship in each head sothatitwillnotaddanyadditionalparameterontopofthestandard transformer.
haheadswilluse a i j andtherest hsheadswill use s i j .
information from the two relationships will be merged togetherthroughmulti headattention.wethenreplace i j in eq4with r i j informula5 andapplyascalingfactorof1 3don i j becauseithas3items .thefinaloutputvectoriscomputedas in eq where vprepresents the value project matrix of relative distances and vp rijis therij th row of vp.
oi j j r i j summationdisplay.
j i j 3d v xj vp rij note that we only compute the attention weights for node pairs where r i j which is similar to the idea of sliding window andcan reduce the timeand space complexityof the selfattention process.
we will discuss its implementation and analyze its complexity in sections and respectively.
efficient implementation alimitationofthefullattentionmechanisminstandardtransformers is the computational and memory cost that grows quadratically with the sequence length.
ast trans we proposed can alleviate thisproblemsincetheattentionscoresonlyneedtobecomputed for node pairs where r i j .
nevertheless a memory and computationalefficientimplementationofast transthatsupports parallelprocessingisnon trivial.theessenceofast transissimilar topreviousworksthatapplyslidingwindowstoconstraintheattentionwithinafixedrange .withslidingwindows thenode pairs in the sequence data can be planned into a linear distribution by ignoring node pairs with i j 0or2p and computed inparallel withmatrixpartitioning.however thistechniquedoes not apply to us since the position distribution of relevant nodes changeswith everytree structure whichmakesmatrix blockinginfeasible.
in this section we present the following alternative implementations of ast trans and discuss the pros and cons mask.mask out the attention scores where r i j after computing thefull attention amongall nodes.
it hasthe same quadratic time and space complexity as in the standard transformer.
loop.loopovernodepairswhere r i j 0andcomputethe attention scores.
it is memory and computational efficient but does not support parallel processing.
sparse.wecanstore rasasparsetensor st r anddeeplearningframeworks suchaspytorch canautomaticallyskipoperations withzeroelementswhenmultiplyingasparsetensorwithanormal tensor.themaskoperationcanbeoptimized forexample contentto position attention scores in eq can be computed by gathering q x kp intercalwithst r .
however it can only apply to content topositionandposition to content.forcontent to content westill havetousethe maskorloopstrategysincetheproductionoftwo sparse tensors is not directly supported.
gather with coo gc .
on the basis of sparse the contentto content computation can be optimized by additional gather operations.thecoreideaofgcistoputquery keypairsthatneedto be computed into one to one correspondence and store them as densematrices.coordinateformat coo isacommonwaytostoresparsetensors whereonlynon zeroelementsarestoredastuplesof authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ast trans code summarization with efficient tree structured attention icse may pittsburgh pa usa figure decompose the relative distance matrix rof the tree abcd with max relative distance p .
elementindicesandthecorrespondingvalues.let coorow coocol denotes the list of row column indexes and coovaldenotes the listofvaluesinthecooformatof r.wethenusethemasindexes to gather the query and key of content as qrow q x kcol k x qp val qp kp val kp bythisway eachcolumninthequerycontent qrowcorrespondsto the same column in the key content kcol.
then we can use matrix dot production to compute attention scores coo qrow kcol qrow kp val qp val kcol where indicatesdotproduction.
cooisavectorandcorresponds tothenon zerovaluesin eq.
and coocol coo .thecontent to positionorposition to contentcanbecomputedthe sameasin sparse and thetotalnumberof gatheroperationsinattentioncomputationis4timesofnon zeroelementsin r for gathering the content and for gathering the position.
gatherwithdecomposedcoo gdc .
toreducethenumber of gather operations in gc we can add a matrix decomposition operationontopofit.first wedecompose rbycoovalsuchthat eachsub matrix s rcontainsonlynode pairswiththesamerelative distances.
an example is shown in figure where the original r contains distinct values and we decompose it into sub matrices accordingly.wetransfereachsub matrix s rintoitscooformat and usecoosto indicates the sub matrix with val s. for each sub matrix coos we gather content embeddings of nodes by qrows q x coos row kcols k x coos col whereqrowsindicates the query content ordered by coosrow and kcolsrepresents the key content ordered by coos col. the attention scores can then be computed as coos qrows qp s krows kp s qp s kp s where cooscorresponds to the attention scores of node pairs in s r. note that coosis a vector of the same shape as coosrow.b y paddingall coostothesamelength theattentionscorescanbe computedinparallelandthefinalattentionscoresequaltothesum of all coos coo 2p summationdisplay.
s 1 coosthere are benefits of this approach compared with gc kpandqpcanbereused aseach qrowsandkrowshavethe samerelativedistance s.thepositionembeddingsof scanbe directly added into the content without gather operations.
only a quarter of number of gather operation is needed discussed in .
.
onlyonedotproductionisrequired asthesecond qps kps canbereusedandonly qrows qps krows kps needs to be calculated.
see appendix a for the complete algorithm.
complexity analysis inthissection wewilldiscussthebest worstandaveragecomplexity of implementations mentioned above.
we use flops floating point operations to measure the computational complexity.
the consideredoperationsincludes matrixmultiplication matrixdot production addandgatheroperationwhicharethemainoperations involved for the attention computation.
flops of these operations are listed below flops a b n m flops a c m flops a b nm2 n m flops a b intercal n flops a b whereaandbaretwomatriceswithshape a indicates gatherawithcas the index c is the number of elements in c. wewillfocusouranalysisonattentionheadsusingtheancestordescendentrelationship a butsimilarideascanbeusedtoanalyze the sibling relationship s straightforwardly.
as the complexity is related to the number of non zero elements in a denoted with a .wefirstanalyzetherangeof a thenpresentthe complexity of each implementation.
.
range of a theorem5.
.
foranydirectedtree t lete i representthenumber of paths in twith length i lrepresent the length of the longest path ing we have e e e l proof.assuming there are nnodes in the tree and the root node is at level .
define njas the number of nodes at level j. for each node at level j i fj i there exists one path of length iending with this node otherwise no such path exists.
hence e i n summationtext.1i j 1njandnj .
therefore we must have e i e i .
square theorem .
.
every tree with nnodes has exactly n 1edges.
proof.imaginestartingwith nisolatednodesandaddingedges oneatatime.byaddingoneedge wewilleither connecttwo components together or close a circuit.
since a tree is fully connectedandhasnocircuit wemustaddexactly n 1edges.
square least upper greatest lower bound.
lete ndenote the number of nodes in a tree.
we have a e e e ...e p sinceweconsiderbothpositiveandnegativedistance in a. based on the above two theorems we can have e i e i ...e i n i authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ze tang xiaoyu shen chuanyi li jidong ge liguo huang and zhelin zhu bin luo figure a in case of random trees the abscissa is the max relative distance pand the ordinate is the non zero elements in a with the unit of o n .
the coefficient decreases with growing p. a n n n ...n p n p 2p itistheleastupperboundfortheancestor descendentrelationship andisachievedonlywheneachnodehasstrictlyonechildnode.
the greatest lower bound can be achieved when the tree s depth is .
in this situation e i fori and a 3n .
average.
wecanusethepr fersequence tosimulaterandomtreessowecanestimatetheaverageof a withdifferent treestructures.thetreesize nissetintherangeof and theout degreeofeachnodeisrandomlyselectedfrom1to n controlled by the max value in pr fer sequence .
we did simulation experiments and figure shows the result.
theaverage a whenpissampledfromauniformdistributionin is1.16pn.wecanseethat thecoefficientinfigure4 graduallydecreases.forlarger p theaverage a willbemuch smaller than the upper bound of 2p n p .
.
mask loop sparse gc maskcontains matrix multiplication with in content to content 2matrixmultiplicationwith and gather operations with index shape for content toposition and position to content and add operations are used for final score computation.
the complexity is n2 2p n m2 m 2n2 n .
loopas loop only computes non zero elements in a the complexityincludes1dotproductionof a m2 m and2add operations a m andequalsto a m2 3m .
sparse s complexity is same as maskapart from the gather operationwithindexshape a thetimecomplexityforgathering sparsetensorasindexequalstothenumberofnon zeroelementsin it whichequalsto n2 2p n m2 m a n .
gcthe complexity in gc is all related to a .
it contains gather operations dot production and add operations which leads to the complexity of a m2 3m 2p nm.
.
gdc there are two implementation details in gdcto optimize the time andspacecomplexity.firstly inatree if s p thedecomposed sub matrix cooshas at most one non zero value in each row.
forexample eachnon rootnodehasexactlyoneparentnodein figure .
we can fix coosrowto and only store thecorresponding coos col.whens p astherelationshipis symmetric cooscan be represented with coo2p s. based on this when s p thequerycontentdoesnotneedtobegathered figure theoretical complexity with p m .
loop has the lowest complexity but cannot be parallelized in practice.
table statistics of java and python datasets perspectives java python of train instances of validation instances of test instances avg.
of tokens in code avg.
of nodes in ast avg.
of tokens in sbt avg.
of tokens in summary ascoosrowisthesameindexofquery andwhen s p thekey contentdoesnotneedtobegathered.hence weonlyneed 2p n gatheroperationsfromcontent.secondly paddingpositionsdonot need to be computed in dot production as the padding positionsof both qrowsandkrowsare the same.
after adding the position bias allqrowsandkrowscanbepackedbeforedotproduction then unpacked to their original length afterwards.
by this way we only need to compute related node pairs with onedot production.
in consequence the complexity of gdcincludes 2p nm gatheroperations 1dotproductionwithshape and add operations with shape which equals to a m2 m 6p nm 2p n. for better comparison we also show the theoretical complexity infigure5 underthehyper parametersin ourexperiments.ascan be seen loophas the lowest complexity but cannot be parallelized.
maskandsparsegrow quadratically with the ast size.
gdc slightly outperforms gcand has a complexity close to loop.
experiments in this section we first explain the experimental setup evaluation metrics and baseline approaches then report the main results and performablationstudies.theruntimespeedandmemor ycostof different implementations are provided for comparison.
finally we present a qualitative analysis and discuss the future directions.
.
experimental setup datasets.
experiments are conducted on the two public code summarizationbenchmarks oneinjava andtheotherinpython .
to ensure the quality of comments we filter the comments with lessthan4words constructors setters getters andtestermethods sameasinshidoetal .
.whenthecommenthastwoormore sentences onlythefirstsentenceiskeptasthedescriptionofthe authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ast trans code summarization with efficient tree structured attention icse may pittsburgh pa usa table comparison of ast trans with the baseline methods categorized based on the input type.
means implemented by ourselves.
methods inputjava python bleu meteor rouge l bleu meteor rouge l code nn code27.
.
.
.
.
.
api code .
.
.
.
.
.
dual model .
.
.
.
.
.
basetrans .
.
.
.
.
.
code transformer .
.
.
.
.
.
tree2seq ast tree .
.
.
.
.
.
rl hybrid2seq .
.
.
.
.
.
gcn .
.
.
.
.
.
gat .
.
.
.
.
.
graph transformer .
.
.
.
.
.
code2seq ast pd .
.
.
.
.
.
code2seq transformer .
.
.
.
.
.
deepcom ast sbt .
.
.
.
.
.
transformer sbt .
.
.
.
.
.
ast trans sbt .
.
.
.
.
.
transformer pot ast pot .
.
.
.
.
.
ast trans .
.
.
.
.
.
figure distribution of relative distance pin training sets method.
table shows the statistics of the datasets.
we also count the distribution of relative distances in fig .
as can be seen most ancestor descendent and sibling relationships are within the range of and respectively.
pre processing.
first wepre processthesummariesbyremoving the punctuations.
next we split multi words such as gettabletypes insummarieswithwordninja3sincetheircorresponding tokensinthesourcecodearesplittoo .wealsosplittheleaf nodesinastsintosub tokensiftheyareinformofthecamelcase orsnake case.thesplitnodesaretreatedasnewchildrenoftheoriginal parent node.
finally we reverse the children of the root nodetopreventtheimportantinformation suchasfunctionnames or parameters from being cut when the size of input ast exceeds the maximum size allowed.hyper parameters.
if not specified the maximum size of ast issetto200forallexperiments andthevocabularysizesofboth astsandcommentsaresetto30 .weuse4layersofstacked encoder decoder and set the hidden size d m .
for each attention layer we set ha andhs .
the max relative distanceforancestor descendant siblingrelationship paissetto respectively.feed forward inner layer dimensionis and theactivationfunctionisgelu .whiletraining thebatchsizeis and the maximum epochs is .
models are trained using the withlr 1e 1 .
2 .
1e label smoothing with ls .
and dropout probability of .
.
the patience in the early stopping mechanism is set to and we select the model based on the bleu in the validation set .
evaluation metrics.
we evaluate the performance with corpus bleu meteor and rouge l .
the experiments used the gpus provided by aliyun which use eflops architecture and accl .
eflops architecture improves the scalability and efficiency of commodilty clusters cow andacclbringtheperformantefficiencyofeflopsarchitecture to general cluster systems and cloud scenarios.
.
baselines wecomparetheproposedast transformerwith16baselinemethods.
they can be divided into groups based on the input type code.
modelswiththecodeasinput.ittreatscodeasplain textanddoesnotleverageasts.
code nn usedrnnwhile basetrans used the transformer.
on the basis of code nn dual model usedduallearningtotraincodesummarization and generation together.
api code used multi encoders to encode code along with the api call sequence.
to make upfor the lack of structural information code transformer additionallyaddsfourstructuredistances includingtwokindsof distancementionedinsec3.
tothecodetokensanddoesattention computation separately for each kind of distance.
differently it doesnotdistinguishembeddingsofdifferentrelationsandusessine and cosine functions to represent distance embeddings.
ast tree .
modelswiththeastasinputandencodeitwith tree specificencoders.therearetwomaintypesofsuchencoders.
oneusestree lstm suchas tree2seq andrl hybrid2seq .
rl hybrid2seq adds the code information and deep reinforcementfortraining.theothertreatstheastasgraphandencodes 4we also report the results with best meteor and rouge l in the validation set in appendix b authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ze tang xiaoyu shen chuanyi li jidong ge liguo huang and zhelin zhu bin luo it with graph neural network gnn models.
we consider three kinds of gnn models including gcn gat andgraphtransformer .
the edges fed to gnn includes the ancestordescendant and sibling edges distinguished by the edge attributes.
ast pd .
modelswiththeastlinearizedwithpathdecomposition as input.
path representation needs to be encoded fromthe nodes then the whole ast representation is encoded fromthe path representations.
code2seq is the first approach usingpd anditusedtwolstmmodelstoencodehierarchicalnetworks.forfairnessofcomparison wealsodesignanewbaseline code2seq transformer by replacing these two lstm models with the transformer.
ast sbt .
models with the ast linearized with structurebasedtraversalasinput.
deepcom isthefirstworkthatuses ast sbt as input which encodes it with lstm.
we design a new baselinetransformer sbt that encodes ast sbt with the transformer.
ast trans sbt is ourproposed modelthat inputs sbt with relationship matrices.
ast pot .
modelswiththeastlinearizedwithpre ordertraversal as input.
transformer pot is the standard transformer architecture with ast pot as input and ast trans is our proposed model with tree structured attention.
all transformer based models are based on the relative position embeddingswith disentangledattentionmentioned insection3.
withthe same number of parameters.
the same hype parameters are used through the way for a fully fair comparison.
.
main results the main result of ast trans and the baselines are presented intable .ast trans outperforms all the baselines on all the three metrics.specifically itoutperformsthebestbaselineby3.
.
inbleu .
.08inmeteorand0.
.04in rouge lonthe java and python datasets respectively.
code vs ast tree vs ast linearized .
apart from asttrans onbothtwodatasets usinggnnstoencodeast tree achieved the best results.
the reason is that the ast has both structural and semantic information and the other two input types both lose part ofthestructuralinformation.allthreevariantsofgnnsachieve similarresultsandoutperformthetree lstminencodingtheast tree .
comparedwithtaking thelinearized astasinput models onlyusing thecodeperformbetter onthejavadataset butworseon thepythondataset.thiscouldberelatedtothecodelength.ascode and corresponding asts in python are relatively shorter encoding asts is more effective than in the java dataset.
therefore models using linearized asts with the help of additional structural information are able to outperform models using only the code.
ast pd vs ast sbt vs ast pot .
among three linearization methods when using the transformer encoder decoders ast sbt performs the best on the java dataset and ast pot performs the best on the python dataset.
ast sbt and ast pot both have theirownadvantages.ast sbt maintainsmorestructuralinformation than ast pot while ast pot has the shortest length 5theresultsofbasetrans inthepythondatasetarelowerthanreportedinthepaper .
bleu .
meteor and .
rouge then we set max relative distance pto kept thesame asoriginalpaper andget .
.
bleu .
.
meteor .
.
rouge l.thisreductionmaybebecausethatweadditionallysegment multi words in comments.table ablation study on ast trans with without aands.
model dataset bleu meteor rouge ast trans w o a java47.
.
.
ast trans w o s .
.
.
ast trans .
.
.
ast trans w o a python34.
.
.
ast trans w o s .
.
.
ast trans .
.
.
table ablation study on haandhson java dataset.
hahsbleu meteor rouge l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
among these three linearization methods.
using the ast pd as inputleadstopoorperformanceonbothdatasets.therearetwomain reasons.
on the one hand ast pd method was first proposed for methodnamecompletion.methodnamesaremuchshorterthanthe code summaries and do not include many details.
pd linearization extractsfeaturesfrompaths whichaggregateshigh levelcharactersbutignoresthedetailedinformationinthenode.however codesummarizationrequiresmoredetailedinformationinthecodesuchasthetypeofthereturnvalue whichisstoredintheleafnodes.ontheotherhand code2seq transformer usesahierarchicalnetwork and the amount of trained parameters is much larger.
it is thereby harder to converge than transformer sbt and transformer pot .
impact of relationship matrix r.we compared the performanceofthreekindsofinputswithorwithouttherelationmatrix r code transformer vs basetrans ast trans sbt vs transformer sbt and ast trans pot vs transformer pot .
results show thataddingrimprovesthe performance forall these inputsand asttrans pot performs the best.
this is because code transformer ignores non leaf node information and ast trans sbt stores duplicate information resulting in too long sequence length.
asttrans pot maintains a short sequence length without losing necessary structural or semantic information.
ast trans vs gnn.
ast trans outperforms gnns the bestperformed baseline model in both datasets.
with the help of rela tionship matrix ast trans includes additional relative distanceinformation.nodescanperceiveinformationfromits p distance neighbors at each layer.
for gnn however each node needs p hopstopropagateinformationfromtheseneighbors.inaddition ast transusesmulti headmechanismtocomputedifferentrelationshipsindifferentheads whileallrelationships distinguishedbyedgeattribute arecalculatedtogetheringnns.ast transalsouses extra feed forward layers and residual connections in the encoder which could help improve the model generalization.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ast trans code summarization with efficient tree structured attention icse may pittsburgh pa usa table ablation study on paandpson java dataset.
papsbleu meteor rouge l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table ablation study on the number of layers on java dataset.
numbleu meteor rouge l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ablation studies we conducted ablation studies on four hyper parameters use of eachrelationship numberofheadsusedforancestor descendant ha and sibling relationships hs max relative distance pand the numberoflayers.ineverystudy apartfromthehype parameter that needs to be analyzed we keep the rest settings unchanged.
use of two relationships .
we verified the impact of using ancestor descendantorsiblingrelationshipseparatelyintable4.
resultsshowthattheperformanceisachievedwhenusingthem all.however usingoneoftherelationshipsalonecanalreadyachieve close results and outperform all previous baselines.
number of attention heads .
we change the number of heads usedfortheancestor descendantrelationship hafrom0to8andfix thetotalnumberofheadsto8.ascanbeseemfromtable5 thebest performance is obtained with ha andhs but there is no significantdifferenceamongallcombinationsof haandhs.ev en when one relationship is missing ha 0o rhs the effects are still marginal.
however when both relationships are removed ha hs the performance drops a lot.
we conjecture that this phenomenonisrelatedtothecharacteristicsofast.knowingabout one relationship can help the model guess the other relationship properly.forexample thenode compare canbethechildnodeof whileexp ifexp or switchexp etc but when it isthe sibling ofnode case itcanonlybethechildofnode switchexp .the information about its parent can be guessed in attention computationwithitssibling case .similarly node namestore canonly appear on the left side of a statement and nodes with the same parentasitmustbeitsrightsiblings.messagesofthesesiblingscan be passed to namestore through their common parent.
however there are many cases that the guess will not be successful.
for example statements a bandb ahave the same child nodes and canonly bedistinguished bysibling relationship whilestatementsa b a b b aandb b a a b aonly differ in ancestor descendant relationship.
it could be that the testset does nothaveenoughhardexamplesthatneedthisfine graineddistinction or the current metrics are not enough to reflect the difference.
figure7 runtimeandmemorycostoffiveimplementationswith batch size .
the cost of the mask implementation is equal to the standardtransformer whichgrowsquadraticallywiththeastsize.
maxrelative distance weanalyze theimpactof themaxrelativedistance pintable6.accordingtotable6 theout degreeand depthofmostnodesinastisin and .therefore the max relative distance of ancestor descendant pa and sibling relation ps areselectedfrom and respectively.results showthatastherelativedistancegrows theperformanceimproves too suggesting a wider view of nodes in ast relationships is helpful.
however the improvement is marginal and even with p the model performance can already outperform all other baselines.
thismightbeascribedtothemulti layerstackedencoders.even forp longer distance nodescan still be attended to indirectly on upper layers.
in practice pcan be set as a hyperparameter to balance the performance efficiency trade off.
number of layers finally we perform ablation study by varyingthenumberoflayers andtheresultsarepresentedintable7.
in ourexperiments weobserve thata deepermodel morelayers performs better but the improvement saturates after layers.
.
complexity analysis in fig we analyzed the rum time and memory usage of different implementationsmentionedinsection4.differentfromthetheoretical complexity which analyze the attention computation in isolate operationsingpucanbecomputedinparallel andthereareother factors e.g.
decoder parameters dependent libraries vocabulary embeddingsthatallneedmemoryusage.therefore theneedfor computingattentionscoresisonlyonepartofitandleadstothegap between fig and where the difference across implementations infig7ismuchlarger.nevertheless thetrendstaysthesame.time and memory usage of gdcandgcboth scale linearlywith the ast size while the cost of maskandsparsegrowsquadratically.
even with the batched parallelism in gpus the implementation ofmaskandsparseare still slower than gdcandgcwhile requiring significantly more memory cost.
gdcis faster and with less memory usage than gc.
the main reason is that gdcuses one quarter of gather operations compared with gc.loopshows alineargrowthinmemoryusagewithastsize butitstimecost is much higher as it does not support parallel operations.
whenthe ast size grows further we can expect the difference across implementations will become larger and larger.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ze tang xiaoyu shen chuanyi li jidong ge liguo huang and zhelin zhu bin luo figure heatmaps of relative position representations.
x axis is the relative position representation and the y axis is the relative positions.
the variance for the sibling relation s is much larger than that for the ancestor descendent relation a .
.
visualization and qualitative analysis visualization.
we further visualize the relative position representationsofancestor descendant a andsibling s relationshipsin fig .
as can be seen the variance of relative position embeddings insis much larger than in a. it implies that our model is not sensitivetotherelativedistancebetweenancestoranddescendantnodes as the embeddings are almost the same regardless of the positions.
in contrast the variance for sibling nodes is relatively large and the modelcan distinguishthe siblingnodes withdifferent relative distances.inaddition therelativeembeddingsin aaredemarcated between theupper and lower part suggestinga clear distinction betweenancestoranddescendantnodes.itshowsthatourmodel pays more attention to direction rather than distance in a. it is likely thattheexactdistancebetweensiblingnodesaremoreimportant than that between ancestor descendant nodes in asts.
qualitativeanalysis.
weprovideacoupleofexamplesforqualitativeanalysisintable8.itcanbeobservedthatast transgenerates theclosestsummarytothereference andlackof aorshurtsthe qualityofsummarization.inthefirstcase thekeyinformationis the connection between the sibling nodes method call addall andparameter actions .bothast transandast transw o a generates the summary as a batch add operation while ast trans w osmisunderstandsitas addsanaction .onthecontrary the meaningofthethirdcaseistogetjobbythetagfirstthendelete it.
the order of execution is controlled by the ancestor descent relationship themethodcall get isthechildnodeof delete and ast transw o ajustignoresthe get operation.thesummariesof ast transw o aandw osarebothcorrectinthesecondcase.the statements of the second case are relatively simple and ignoring the order of statements will not affect the function comprehension.
threats to validity therearethreemainthreatstothevalidityofourevaluation.firstly many public datasets are proposed to explore code summarization.table qualitative examples.
public quickactionview addactions collection action actions checkshown mactions.addall actions return this ast trans w o s adds a sub action to the menu ast trans w o a adds the given actions to the list of actions ast trans adds a collection of actions to the quick action view human written adds a collection of actions to the quick action view public java.lang.object newinstance object o newinstanceimpl if o null throw new instantiationexception return o ast trans w o s creates a new object initialized to the string object ast trans w o a returns a new instance of the object class ast trans returns a new instance of the object human written creates a new instance of a class def job delete by tag tag job.objects.get tag tag .delete return job get by tag tag isnone ast trans w o s delete a job and return tag ast trans w o a delete a job objects ast trans delete a job based on its tag human written deletes a job entry based on its tag we select two widely used ones to evaluate the proposed asttransformer but they may not be representative of other programming languages.
secondly to ensure a fair comparison as much as possible we build baselines on the top of the same transformer architecture.thearchitectureandhyperparameterchoicemightbe sub optimalforcertainapproaches6.finally therewillbeacertain gapbetweentheautomaticevaluationandthemanualevaluation ofthesummarizationresults.weselectthreedifferentautomatic evaluation methods to avoid bias as much as possible.
related works code summarization.
most approaches on code summarization frametheproblemasasequencegenerationtaskanduseanencoder decoderarchitecture.theonlydifferencebetweenitandtraditional machine translation is that programming languages are unam biguous and follow rigid grammar rules.
most approaches eithertreat the source code as natural language i.e.
a sequence of tokens without specified structures or utilize its structural information with the help from asts or other parsed forms.
to encodethe code sequence there exist many encoder architectures likecnn rnn and the transformer .
to leverage the tree structured ast tree based models such as recursive nn tree lstm andtree transformer areused toencodeastdirectly.astreeisaspecialkindofgraph graph based approaches canalsobeusedtoencodeasts.someworks alsocombinethecodetokensequencewiththeastandobserve improvement .ourapproachonlyneedsthelinearizedast 6nevertheless ast trans performs best among all reported results on both datasets.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ast trans code summarization with efficient tree structured attention icse may pittsburgh pa usa andcanbebuiltuponthetransformerarchitecture.moreimportantly itrestrictstheattentionrangeandmakesitpossibletoencode very long ast sequences.
tree basedneuralnetworks.
theexistingtree basedneuralnetworkscanbegroupedintotwocategoriesdependingontheirinputs the models that directly take the tree as input .
these models are strongly coupled with the tree structure and the calculation process needs to be performed simultaneously with the tree traversal.
since trees generally have different shapes by nature parallization of training these models is non trivial.
the modelsthattakethesequence s extractedfromthetreeasinput suchasthesampledpathsinthetree thetraversalsequence with tree positionalembedding or the structurebased traversal sbt sequence .
taking sampled paths as input is with a certain degree of randomness and instability and the method of tree positional embeddingignores the concept ofpaths in the tree all nodes even if not related will participate in the calculation together .ourmethodimprovesfromthesetwomethods which can be guaranteed that each node exchanges message on and only on all paths containing it.
conclusion in this paper we present ast trans which can encode asts effectively for code summarization.
in ast trans each node only pays attention to nodes which share the ancestor descendent or sibling relationshipswithit.itbringstwobenefits themodelisgiven aninductivebiasandwillnotgetlostintheoverlongastsequence and it can reduce the computational complexity from quadratic tolinear.thelattermakesitpossibletoencodelongcodesequence e.g.
a whole file which is prohibitively expensive for standard transformers.weconductcomprehensiveexperiments showing that ast trans achieve sota results on two popular benchmarks while significantly reducing the computational cost.
we believe the basic idea of ast trans can also be applied in otherstructureddatalikedatadependenceandcontrolflowgraphs.
thecodeismadepubliclyavailabletobenefittherelevantresearch.
in future work we plan to improve ast trans by incorporating more features of the code snippet such as api sequence and node type into the self attention mechanism.