log parsing with generalization ability under new log types siyu yu school of computer and electronic information guangxi university chinayifan wu peking university chinazhijing li school of data science the chinese university of hong kong shenzhen cuhk shenzhen china pinjia he school of data science the chinese university of hong kong shenzhen cuhk shenzhen chinaningjiang chen school of computer and electronic information guangxi university chinachangjian liu school of computer and electronic information guangxi university china abstract log parsing which converts semi structured logs into structured logs is the first step for automated log analysis.
existing parsers are still unsatisfactory in real world systems due to new log types in new coming logs.
in practice available logs collected during system runtime often do not contain all the possible log types of a system because log types related to infrequently activated system states are unlikely to be recorded and new log types are frequently introduced with system updates.
meanwhile most existing parsers require preprocessing to extract variables in advance but preprocessing is based on the operator s prior knowledge of available logs and therefore may not work well on new log types.
in addition parser parameters set based on available logs are difficult to generalize to new log types.
to support new log types we propose a variable generation imitation strategy to craft a novel log parsing approach with generalization ability called log3t.
log3t employs a pretrained transformer encoder based model to extract log templates and can update parameters at parsing time to adapt to new log types by a modified test time training.
experimental results on benchmark datasets show that log3t outperforms the state of theart parsers in terms of parsing accuracy.
in addition log3t can automatically adapt to new log types in new coming logs.
ccs concepts computing methodologies neural networks software and its engineering software maintenance tools .
keywords log parsing self supervised test time training generalization ningjiang chen is the corresponding author of this work.
e mail chnj gxu.edu.cn permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
reference format siyu yu yifan wu zhijing li pinjia he ningjiang chen and changjian liu.
.
log parsing with generalization ability under new log types.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction nowadays with the proliferation of continuously running systems such as cloud service systems users are provided with a wide range of software services.
despite considerable efforts dedicated to ensuring system reliability these systems remain vulnerable to failures.
in order to effectively monitor and analyze system performance logs play a crucial role as they provide valuable insights into system behavior error messages and operational activities .
however the huge log volume of such systems brings challenges to manual reading.
for instance alibaba inc. can generate about million logs per hour .
to cope with the huge log volume a lot of work has gone into automated log analysis to help operators diagnose failures such as log based anomaly detection and failure prediction .
log parsing serves as a fundamental step in automated log analysis as it transforms semi structured logs into structured ones.
log parsers parse logs into log templates by using wildcards to replace predicted variables and keep predicted constants .
for instance log connection from .
.
.
will be parsed into log template connection from because .
.
.
is predicted to be generated according to the system context.
techniques in existing log parsers can roughly be categorized into heuristic clustering frequent item extraction and deep learning .
for example drain uses a fixed depth tree to group logs with the same first n words.
logram uses n gram dictionary to convert each word in the log into gram gram and then classifies words according to the frequency of these items.
although the superiority of existing log parsers has been illustrated in their papers the performance is far from satisfactory when applying them in practice which is mainly caused by new log types in new coming logs i.e.
new patterns of existing log templates or new log templates.
in the real world the system s available logs usually do not contain all log types of the system because many system esec fse december san francisco ca usa siyu yu yifan wu zhijing li pinjia he ningjiang chen and changjian liu.
figure continuous emergence of new log types within days of mac states are infrequently activated and evolving systems frequently update logging statements to bring new log types to the log parser .
as shown in figure an average of .
new log types appear in mac dataset s logs every day.
we summarize the challenges posed by new log types as follows invalid prior knowledge.
to avoid parsing errors and improve efficiency most log parsers need regular expression filters abbreviated as regex filter in the following which are designed by operators according to their prior knowledge of the available logs of the system to convert some variables into in the preprocessing stage .
however these regex filters designed on the prior knowledge of available logs cannot avoid new parsing errors caused by new log types.
consequently log parsers would become ineffective and inefficient when new log types appear.
labor intensive model tuning.
for unsupervised approaches e.g.
heuristic based ael and frequent item extraction based logram the model parameters e.g.
frequency threshold need to be continuously adjusted to adapt to new log types otherwise the parsing accuracy will drop a lot.
for example we applied ael s best performing parameters in the first half of the bgl benchmark dataset to the second half and the resulting accuracy is .
lower than the best.
for supervised approaches e.g.
uniparser new log types may be generated by log statements with completely different styles in untriggered system components so they may not be in the same distribution as historical logs i.e.
training set .
these new log types require laborious labeling work to retrain the model to generalize.
violation of underlying assumptions.
some log parsers do not work well enough when some new log types arrived due to the violation of their underlying assumptions.
for example drain drain and spine consider logs with the same first nwords should belong to the same template.
however logs starting with variables are common in practice such as cupsd startup succeeded and sm client startup succeeded from linux logs.
in addition drain lenma and ael assume logs belonging to the same template should have the same length.
however there are various lengths in logs belonging to the same template like en0 support ed channels and en0 supported channels from mac.
these assumptions neglect the existence of two log types i.e.
logs starting with variables and various lengths in logs belonging to the same template .
to address these challenges we incorporate the idea of testtime training to propose a novel log parsing approach supportingnew log types called log3t.
log3t mainly consists of offline pretraining and online parsing.
during offline pre training we pretrain a model with labeled historical logs.
during online parsing log3t extracts the few words that are most likely to be constants.
subsequently the logs are partitioned into log groups based on these words.
whenever a new log is added to a log group its representative template is updated accordingly.
in addition we applied the idea of test time training to online parsing which can enable model parameters to be automatically updated in the online parsing stage.
to evaluate the performance of log3t we conducted comprehensive experiments on benchmark datasets .
the experimental results demonstrate that log3t achieves the best average grouping accuracy and edit distance across benchmark datasets.
log3t exhibits the best edit distance on out of the benchmark datasets while its grouping accuracy outperforms all other log parsers on out of the benchmark datasets.
compared with the log3t model trained only on historical logs modified test time training can help the model achieve better parsing accuracy under new log types.
the contribution of this paper can be concluded as follows we propose log3t the first log parser that has generalization ability under new log types without human intervention.
its core techniques are as follows.
we propose a imitation variable generation strategy to generate labeled imitated logs to pre train model to accurately extract variables in different log types requiring no prior knowledge.
we apply modified test time training which can automatically update model parameters to avoid labor intensive model tuning when new log types occur.
extensive experiments have been conducted on benchmark datasets to evaluate log3t.
compared to existing log parsers log3t achieves the best average ga .
and edit distance .
across all benchmark datasets.
the implementation of log3t is publicly available for better reproducibility and further research.
the rest of the paper is organized as follows.
section introduces the preliminaries of this paper.
section describes the details of log3t.
section presents our experimental results.
we discuss the limitations and future work in section and section introduces related works of this paper.
finally section concludes this paper.
preliminaries .
log parsing as shown in figure logs are generated by logging statements e.g.
loginfo print in the source code and consist of log header e.g.
time pid log level info and log content e.g.
block broadcast 0 piece0 stored as bytes in me mory estimated size .0b free .
kb .
log content is composed of constant words e.g.
block and variable words e.g.
.
.
.
log parsing parses semi structured logs into structured logs by keeping predicted constant words and replacing predicted variable words with wildcards and the parsing result is called log template like log template block stored as bytes in memory estimated size b free kb .
426log parsing with generalization ability under new log types esec fse december san francisco ca usa figure an illustrative example of log parsing .
tokenization the preprocessing step of log parsing involves tokenizing the input log to a set of tokens and then predicting their corresponding classifications.
most log parsers use delimiters to split log and achieve tokenization.
however this tokenization methods can cause certain log parsers to fail when encountering new tokens.
for example nulog needs to establish a vocabulary for all the tokens obtained from historical logs because it needs to vectorize logs using the index values of the tokens in the vocabulary.
when new log types emerge there may be new tokens not in the old vocabulary making them difficult to be vectorized.
wordpiece.
in natural language processing there is a widely used tokenization technology wordpiece which can improve the generalization ability of vocabulary.
wordpiece is a subtokenlevel tokenization technology introduced by google aiming to address the issue of out of vocabulary oov tokens.
the basic idea of wordpiece is to divide the text into smaller meaningful subtoken units instead of traditional word units.
wordpiece can use approximately subtoken units to represent all words.
for example there is a word jk2 init in the log not in the vocabulary it can be represented by combining the subtoken units j k in it from the vocabulary.
.
regular expression filter most log parsers need regular expressions to pre parse common variable anderror prone variable words into wildcards to avoid parsing errors.
common variables such as block ids and ip addresses.
researchers commonly assume that the patterns of this category of variables remain consistent.
thus they can easily set up regex filters to pre parse all common variables.
for example the regular expression r .
d ?
is used in 2k proxifier dataset to match urls uniform resource locators or domain names including port numbers where .
captures subdomains or subcomponents of a domain captures the last component of a domain and d ?
captures the port number.
error prone variables.
this category of variables is prone to causing parsing errors in log parsers.
researchers design regex filters to pre parse these variables based on parsing errors caused by them in the historical logs.
for example the regex filter r d ssec is used to transform patterns like sec into .
the words sec is used to record the duration of a lifecycle.
however in general words representing lifecycles are in the formatof where there is no space between the characters.
the presence of a space in sec leads to varying log lengths after tokenization while log parsers like drain brain drain spine ael lenma assume that logs belonging to the same template have the same length.
therefore such a filter can prevent these log parsers from making errors.
without these regex filters the log parser would generate a substantial number of parsing errors and become ineffective.
for example after removing the regex filters the group accuracy of spell decreases from .
to .
.
therefore when new log types appear the effectiveness of a log parser may decrease if the regex filters are not updated or adapted to the new log types regex filters designed to filter common variables may fail to match new patterns of the common variables in new logs.
for example in the original proxifier dataset there are a substantial number of urls and domain names that do not conform to the regular expression pattern r .
d ?
which is set based on 2k sampled dataset such as wpad and dshytnh .
the patterns of the words that lead to the same parsing errors can vary.
for example in the variable position of en0 supported channels different contexts may generate a different number of variables separated by spaces such as and .
regular expression r d ssec is unable to solve this issue of various length in this case.
different datasets require significant variations in regex filters.
considering that an operator typically maintains multiple systems and components with different log types it can be laborious to set different regex filters for each component and system.
for example the regex filters used by drain in android and spark are r r .
r b ?
?
d b b0 b b b and r d .
d r b ?b b r .
respectively indicating that they are completely different.
in summary the superior performance of current log parsers is largely attributed to the use of regex filters.
however when new log types appear regex filters based on historical logs may become ineffective.
therefore an effective log parser that does not require regex filters is needed in the industry.
.
test time training the challenge of labor intensive model tuning arises due to the distribution shift of new log types compared to the available logs i.e.
training and test data come from different distributions .
the performance of predictive models is often unsatisfactory when training and test data come from different distributions.
a lot of efforts have been dedicated to solving the problem such as domain generalization unsupervised domain adaptation and test time training .
the reason for adopting test time training in this paper is that it does not require new logs and historical logs to possess common features.
instead it utilizes a self supervised task to capture the features present in the new logs.
we will provide a concise overview of these concepts as follows domain generalization studies the setting where a meta distribution generates multiple environment distributions some of which are available during training source while others are used for testing target .
in log parsing it is challenging to determine whether logs from different systems belong to the same 427esec fse december san francisco ca usa siyu yu yifan wu zhijing li pinjia he ningjiang chen and changjian liu.
figure standard version of test time training figure workflow of log3t meta distribution or if newly generated logs resulting from software changes belong to the same meta distribution as historical logs.
unsupervised domain adaptation studies the problem of distribution shifts when an unlabeled dataset from the test distribution target domain is available at training time in addition to a labeled dataset from the training distribution source domain .
however the limitation of the problem formulation is that generalization might only be improved for this specific distribution which can be difficult to anticipate in advance.
in log parsing evolving systems make it hard to anticipate distributions.
test time training can provide generalization under distribution shifts using a self supervised auxiliary task to update model parameters at test time where self supervised learning studies how to create labels from the data such as context prediction and rotation prediction .
figure shows the standard version of test time training.
specifically at training time we jointly optimize the loss of the main task lossm and the loss of the self supervised auxiliary task lossa .
at testing time we will update the shared parameters sand the parameter aof the auxiliary task by optimizinglossa.
for instance in the case of the main task being object recognition when confronted with images belonging to a new distribution online the model is updated by restoring rotated new images.
this approach enables the model to capture features present in the new images thereby facilitating the generalization of the main task to these new images.
our approach in this section we introduce the proposed log parser log3t in detail.
as shown in figure log3t consists of offline pre training and online parsing.
in the offline pre training stage we formulated figure an example of tokenization the log parsing to a binary classification task utilizing labeled historical logs to train a transformer encoder.
in the online parsing stage we extract the most likely constant words from the newcoming logs based on the probabilities assigned to the words by the model.
subsequently we partition the new coming logs into different log groups based on these constant words.
each log group has its representative log template.
to enhance the model s ability to generalize to new log types we further developed a version of online parsing that incorporates test time training.
in the following sections we will provide a detailed description of log3t.
.
offline pre training in the offline pre training stage historical logs are used to pretrain a transformer encoder.
we introduce offline pre training stage through three main components .
tokenization .
context features integration .
loss function.
tokenization.
we first split the up coming logs into separated words using delimiters such as comma and space.
then we use wordpiece to tokenize each word into sub tokens.
as shown in figure we design a fixed length window for each word to pad sub tokens.
for example the word config is tokenized into con fi and g the remaining positions will be filled with .
this fixed length window facilitates the model to identify which sub tokens belong to the same word.
before entering the model each token needs to be embedded into a vector by an embedding layer.
the embedding layer is a fully connected network and its dimension is based on a vocabulary.
context features integration.
with the help of context we are more likely to accurately identify whether the word is variable.
therefore in order to integrate context features to identify whether the word is variable the model we use needs to be able to integrate context features well.
compared with rnns recursive neural networks that can only consider adjacent contexts transformer can integrate context features without restrictions.
thus we use transformer encoder based model to predict the variable word.
loss function.
as a binary classification task variable or constant we will feed the output of the model into a sigmod function and then compute the binary cross entropy loss.
the detailed loss function is as follows loss ni i yilnxi yi ln xi 428log parsing with generalization ability under new log types esec fse december san francisco ca usa whereyiis the label of this sub token xiis the predicted probability of this sub token assigned by the model and nis the number of sub tokens of this log.
if a word is labeled as variable all of the word s sub tokens labels yiwill be and if it is not the label yi will be .
.
online parsing in the online parsing stage the model takes the up coming log as input and outputs the probability after sigmod layer referred to as variable probability hereafter .
then a word is regarded as a constant if its variable probability is lower than a predefined probability boundary.
however establishing a probability boundary to extract all the constant words in the log is challenging as this probability boundary may vary across different logs.
therefore this paper adopts a partial constant based approach for online parsing which can only utilize partial constants to accurately group logs.
each log group has a representative log template which will be checked for updates when new logs are added.
online parsing can be concluded as three main steps partial constant words extraction online log grouping and log template updating.
partial constant words extraction.
in this step we will extract top n words from the log in ascending order of variable probability.
the value of n is typically set to be between and and it is determined by operators and related to the average length of available logs of the system.
figure provides a detailed illustration of an example for this step.
for example if the parameter n is set to ask and delete will be considered as partial constant words of log5.
in our implementation if a log contains identical words these words will be assigned different tags for distinction.
as an example in figure the0 represents the first occurrence of the word the inlog .
in addition pure numbers will not be extracted as constant words.
the predicted variable probability of a word is defined as the maximum variable probability value among its sub tokens.
for example 20kb is a word in the log then wordpiece divides it into two sub tokens and kb the predicted variable probability of the is .
and the predicted variable probability of the kb is .
.
therefore the predicted variable probability of the word 20kb is .
.
online log partition.
in this step each log will find its log group based on the extracted partial constant words.
determining the log group based solely on the presence of these partial constants is insufficient as an example in figure log1andlog2have the same partial constant words but they do not belong to the same log template.
thus we need to conduct further comparisons between the input log and additional features of the existing log groups to accurately identify the appropriate log group.
as shown in figure four modules word comparison order comparison length comparison and consecutive variable detection are employed to match existing log groups for the input log.
word comparison is used to find which existing log templates contain these extracted words.
as shown in figure the constant words extracted from log2are source and destination .
therefore the matched log template for log2 should include the words source and destination .
order comparison is used to check if the partial constant words in the same order as they do in the log template.for instance in log1andlog2 the same extracted words source and destination have different orders source destination inlog1and destination source inlog2.
in log parsing this module is more suitable compared to position indexing.
it accommodates situations where a single position yields a varying number of variables.
for example inlog4andlog5 the word delete will have different position indexes but the order of their constant words remains consistent.
length comparison is used to check whether the length of the input log matches the length of the log template.
consecutive word detection is used to check if there are consecutive variables generated at a single variable position.
we assume that consecutive variables generated from a single position have the consistent pattern and use regular expressions to represent this pattern.
will be used to replace letter ?
d will be used to replace consecutive numbers and special characters are reserved.
if multiple words are successfully matched these words will be considered as consecutive variables at a single variable position.
if the match is successful the input log will be added to the matched log group and proceed to the next step of log template updating.
if multiple templates are matched the proportion of tokens that are identical to the input log will be used to determine the final template.
however if the match fails the input log will form a new log group where the log template will be the log itself.
in addition we used the example shown in figure to provide an easy understanding of each module in the log grouping process.
figure illustrates the process of finding a appropriate log group forlog5within the existing log groups.
the word comparison module initially identifies that only group2 s log template contains the words ask and delete .
then the order comparison module verifies that the order of these two words in log5and group2 s log template is consistent.
next the length comparison module detects a discrepancy in the length between group2 s log template andlog5.
as a result the consecutive word detection module is utilized to check whether log5contains consecutive variables generated from a single position.
in this case there are multiple words between ask and delete inlog5 letters special characters and numbers of the first word blk 422 will be transformed into the corresponding regular expression ?
d .
since the regular expression can match all the remaining words blk 422 and blk 354 can be considered as consecutive variables at a single position and log5can be considered as belonging to group2.
log template updating.
once a new log is added to the log group the log template of that group will be checked for the updates.
in each log group the consecutive variables generated from a single variable position are considered as one variable position ensuring alignment of all constant words in the columns.
once the addition of a new log leads to different words appearing in a column the corresponding word in the log template will be replaced with the wildcard at the respective position.
we formally present the online parsing algorithm in algorithm .
we define a log set as s l1 l2 ...lm wherelrepresents a log message and mrepresents how many log messages this set has.
429esec fse december san francisco ca usa siyu yu yifan wu zhijing li pinjia he ningjiang chen and changjian liu.
figure illustration of extracting partial constant words from the log in the log stream.
figure the workflow for online log grouping figure illustration of how log5in figure matches with the corresponding log group from the existing log groups.
the input of algorithm is the log l and the extracted partial constant words pc wherew represents the word in l nrepresents how many words lhas p represents how many constant words extracted rk k .
algorithm implements the four modules of log partition word comparison line order comparison line length comparison line consecutive variable detection line and log template updating line .
the time complexity of the consecutive variable detection module is the highest within the log partition process being o mn as it requires traversing every word in the log line .
log template updating involves checking if there are different words in each aligned column with a time complexity of o mn .algorithm online parsing input logl partial constant words pc existing template et output updatedet 1matched 2fortpinetdo ifpcnot intpthen continue else order forwrkinpcdo order.append tp.index wrk end iforder.is ascending then check whether the position indices stored in the order are in ascending order.
iflen tp len l then pos positions of words in tp that are not present in l .
posconsecutive consecutive values in list pos .
merge the words with the same pattern that exist at the positions indexed in posconsecutive within log l .
iflen tp len l then matched.
append tp else continue end else matched.
append tp end else continue end end 27end 28final template template from list matched with the highest ratio of identical words to those in log l .
29return final template.update l figure modified version of test time training in log3t .
online parsing with test time training in this work we take advantage of test time training to generalize under new log types.
to design a self supervised auxiliary task for log parsing we proposed an imitation variable generation strategy to create labeled imitated logs from the new log types.
these imitated logs can be used to train the model and enable it to learn the features specific to the new log types.
430log parsing with generalization ability under new log types esec fse december san francisco ca usa imitation variable generation strategy.
for each logging statement the generated constants will not change while the generated variables will change according to the system contexts.
therefore we consider that the most crucial characteristic of variable positions compared to constant positions is their ability to generate distinct words.
we imitate the variable generation by replacing randomly selected words in the new logs with historical variable words.
to enrich the historical variable word repository we create imitated variables.
specifically we extract the pattern for each type of historical variable words as regular expressions to generate imitated variables.
will be used to replace consecutive letters ?
d will be used to replace consecutive numbers and special characters are reserved.
each regular expression generates a set of randomly words that conform to that regular expression which are used as imitated variables.
for example the real variable word blk 5646 could generate imitated variable words like adc 5951 and lop 6484 .
modified test time training in log3t.
as shown in figure the standard test time training follows a branching structure where the main task and the auxiliary task are entirely different.
for instance in the test time training framework composed of object recognition and rotation prediction during the test time there is no labeled test data to train the model for accurate identification of new images.
however the self supervised rotation prediction task can provide the model with labeled data.
by training the model on labeled rotated images researchers consider that the bottom features learned by the model from labeled rotated images can be transferred to the main task.
consequently the main task and the auxiliary task share certain bottom parameters.
in the online parsing of log3t with test time training both the main task and the auxiliary task are binary classification tasks of words in the logs where the difference is that the auxiliary task uses imitated logs instead of real logs.
in log3t unlike standard test time training the main task and auxiliary task share the same optimization objective which is to train the model for accurate word classification.
as the imitated logs possess the crucial characteristics of real logs we consider them as labeled real logs.
therefore as depicted in figure the model parameters updated using imitated logs can be fully utilized for online parsing of new logs.
evaluation in this section we evaluate our approach by answering the following research questions rq1 is the parsing accuracy of log3t better than existing log parsers without the help of prior knowledge?
rq2 can test time training help log3t generalize under new log types?
rq3 to what extent does each component of log3t improve performance in log3t?
rq4 can the efficiency of log3t meet the requirements for log parsing in practice?table data statistics and configuration dataset data size messages delimiter threshold epoch hdfs .47gb apache .96mb zookeeper .95mb mac .09mb healthapp .44mb android 183mb openstack .01mb bgl .76mb proxifier .42mb linux .25mb hpc .00mb hadoop .61mb windows .09gb thunderbird .60gb spark .75gb openssh .02mb .
experimental setting dataset.
our experiments are conducted on the most widely used benchmark datasets from logpai.
the entire dataset includes distributed system logs hdfs zookeeper spark hadoop openstack supercomputer logs hpc bgl thunderbird operating system logs mac linux windows mobile system logs android healthapp server application logs apache openssh and standalone software logs proxifier more detail statistics about the benchmark dataset is available in table .
and each benchmark dataset provides a 2k sampled dataset where logs are labeled with the log template they belong to as the ground truth.
the researchers corrected some errors in the ground truth and based on that we further corrected some minor errors.
evaluation metrics.
we used grouping accuracy abbreviated as ga in the following and edit distance also known as levenshtein edit distance to evaluate the log parser s parsing accuracy.
for ga a log is considered correctly parsed if and only if its log template corresponds to the same group of logs as the ground truth does.
for example if we parse the normal log sequence e1 e2 e2 as we get ga because the second and third messages are not grouped.
ga can evaluate whether a log parser can facilitate downstream tasks e.g.
log sequence anomaly detection because only logs belonging to the same template are considered to be the same log key we can get the real log key sequence.
for example incorrect parsing result will be considered as an anomaly log sequence because its pattern does not align with the normal log sequence pattern .
edit distance was proposed by nedelkoski to address the limitations of ga which only evaluates whether logs are parsed into the same template without considering the distance between the extracted template and the ground truth.
edit distance is used to evaluate the template extraction in terms of string comparison.
log parser selection for comparative experiments.
we conducted comparative experiments by selecting all the online log parsers from logpai namely drain spell lenma and shiso along with the latest open source log parsers logram and nulog.
logram generates n gram dictionaries for all logs finds highfrequency n gram combinations then determines log variables and constants.
drain is an online log parser and employs a fixeddepth tree structure to assist in dividing logs into different groups.
431esec fse december san francisco ca usa siyu yu yifan wu zhijing li pinjia he ningjiang chen and changjian liu.
table group accuracy and edit distance of log parsers on benchmark datasets with and without regular expression filters.
the higher the ga the better and the lower the edit distance the better.
for worse performance.
dataset lenma shiso spell drain logram nulog log3t rex use ga ed ga ed ga ed ga ed ga ed ga ed ga ed ga ed ga ed ga ed ga ed ga ed hdfs .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bgl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hpc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
apache .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
healthapp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mac .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
proxifier .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
zookeeper .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
thund... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
spark .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
android .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
linux .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hadoop .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
openstack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
windows .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
openssh .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rex impact .
.
.
.
.
.
.
.
.
.
spell uses the longest common sub sequence algorithm to extract log templates.
shiso used a tree form structure to guide the parsing process where each node was correlated with a log group and a log template.
lenma is a clustering based log parser and focuses on the word length feature and converts the log into a vector of the number of word letters.
nulog utilizes masked word recovery for pre training a transformer model.
during the parsing process each word is masked once and the probability of correctly recovering the masked word by the model is used to determine whether the masked word is a variable.
implementation and configuration.
all experiments are conducted on a gpu server with a100 sxm4 80gb and cuda .
.
we implement log3t based on python .
and pytorch .
.
we set the initial learning rate to .
and use adam optimizer.
the thresholds refer to section .
we used for each dataset are shown in table .
the parsing accuracy of the sampled dataset was obtained when only the variables in the first logs were labeled.
all the imitated variables are generated based on first logs.
.
rq1 the parsing accuracy of log3t to answer rq1 we followed the guidance in and collected parsing accuracy for log3t and other log parsers on 2k sampled datasets.
in our reproduction the optimal parameters for nulog on the benchmark datasets are not fully available as the authors only provides optimal parameters for datasets.
we made efforts to modify the parameters of nulog for the remaining datasets however we cannot guarantee that these parameters will lead to the best performance for nulog on these datasets.
in addition we found that the parameters provided in the logram code repository resulted in poor performance leading us to believe that the uploaded parameters by the authors were incorrect.
we madeefforts to modify the logram s parameters to achieve the better performance.
the epochs for the log3t model on each dataset are shown in table .
we evaluated the ga and edit distance of all the log parsers on benchmark datasets both with and without their standard regex filters.
we did not record parsing accuracy for nulog without regex filters because it is challenging to apply nulog to online parsing scenarios with new tokens.
refer to section .
table shows the experimental results.
log3t achieves the best ga on out of the benchmark datasets and the best edit distance on out of the benchmark datasets.
log3t also achieves the best average ga .
and edit distance .
across all benchmark datasets.
as indicated in table s regex impact row we can conclude that regex filters have an impact on the effectiveness of all the log parsers.
in terms of average ga across the benchmark datasets spell without regex filters exhibits a decrease of compared to spell with regex filters while the edit distance increases by .
removing all regex filters results in an average ga drop of and an average edit distance increase of .
across the benchmark datasets for the log parsers.
in this paper we consider regex filters as barriers to applying log parsers to new log types.
however the parsing accuracy of existing online parsers heavily relies on these regex filters.
answer to rq1 log3t without regex filters is effective and outperforms existing online log parsers.
the effectiveness of existing online parsers tends to decrease when regex filters are removed which poses challenges for their application to new log types.
432log parsing with generalization ability under new log types esec fse december san francisco ca usa a hdfs b hpc c healthapp d spark e linux f openssh figure group accuracy of log3t without test time training and with test time training when only one batch of data is available as historical data in each subplot left batch size right batch size .
rq2 model generalization to answer rq2 we simulated real world scenarios to input new log types into the log3t model and collected ga. specifically we partitioned the benchmark dataset into multiple equally sized batches in chronological order.
the first batch served as the system s available logs while the subsequent batches are considered as newly arrived logs to input to the model.
the ga for each batch was calculated based on the current batch and all previous batches.
for example if the batch size is when processing the third batch the ga we collected was based on the first logs.
this approach is adopted because the definition of ga requires all logs belonging to the same log template to be grouped together.
thus assessing ga within a single batch holds no meaningful significance.
in our implementation we collected the ga of the log3t model trained on the first batch when parsing subsequent batches the ga of the log3t with test time training and the ga of the log3t model trained with all logs.
batch size is set as and .
the experimental results are shown in figure .
the experimental results indicate that the improvement brought by test time training varies across different categories of datasets.
for simple datasets where variables are mostly pure numbers models trained with the first batch of logs can achieve the similar performance to models trained with all logs as well as models with test time training.
this is because log3t does not consider pure numbers as extracted constants for subsequent log partition.
for example in datasets such as hdfs apache and proxifier over of the variables are pure numbers the three curves withttt withoutttt trained with all logs overlap completely.
in datasets like healthapp spark over of the variables are pure numbers.
there is a slight performance gap approximately .
.
between a model trained without testing time training and a model trained using all logs.
testing time training can help bridge this gap.
for the remaining datasets with much less pure numbers variables models trained with the first batch of logs perform worse compared to models trained with all logs while test time training can provide a ga improvement to the model.
in datasets like openssh linux android over of the variables are notpure numbers.
models trained with the first batch of logs show approximately lower ga compared to models trained with all logs.
test time training can improve the model s ga by around .
in the linux dataset with batchsize set as models trained with the first batch and models with test time training achieve the same performance.
this is because the log types that can be correctly parsed by log3t are predominantly included within the first logs.
figure displays that certain model without test time training also experience an increase in accuracy in subsequent batches.
this is connected to the distribution of logs within the dataset that are easy to parse as a similar trend is observed for model trained on all logs.
in addition to the model parameters the adjustment of the hyperparameter threshold refer to section .
also links to whether the log parser extensive effort under new logs types .
therefore we conducted a sensitivity analysis on the threshold.
we set the threshold values uniformly to and respectively and 433esec fse december san francisco ca usa siyu yu yifan wu zhijing li pinjia he ningjiang chen and changjian liu.
figure sensitivity analysis on threshold collected the average ga and edit distance of log3t on benchmark datasets.
as shown in figure log3t with different threshold values consistently achieves an average ga of .
or higher across the benchmark datasets.
furthermore it maintains an average edit distance ranging from to .
the experimental results indicate that log3t is not significantly affected by the threshold.
answer to rq2 log3t can utilize our modified testtime training to automatically update model parameters to generalize to new log types.
.
rq3 effectiveness of each component to answer rq3 we conducted four evaluation experiments.
firstly we collected predicted variable probabilities generated by the log3t model for constant and variable words in each log to examine if there are noticeable differences in their variable probabilities.
secondly we compared the ga obtained based on random word order original word order and word order sorted by the log3t model to check whether the log3t model can extract constant words first.
thirdly we compared the ga obtained from training the log3t model using only real variables with the ga obtained from training the log3t model using both imitated and real variables to evaluate the effectiveness of imitated variables on the model s performance.
finally we evaluated whether the modified test time training contributes more to the performance of log3t compared to standard test time training.
table presents the average predicted variable probabilities generated by the log3t model for constant words and variable words in each log from the benchmark datasets.
the average predicted variable probabilities for constant words across the benchmark datasets is .
while the average predicted variable probabilities for variable words is .
.
there is a significant difference.
table presents the ga achieved by different methods for word order in the logs.
keeping the word order unchanged yields a ga of .
.
after randomly shuffling the word order in each log from the benchmark datasets five times the average ga obtained is .
.
significant improvement in ga is observed when using log3t model for word ordering reaching a ga of .
.
table presents the average ga achieved by the log3t model trained with imitated logs generated based on real variables from the first logs in the 2k dataset compared to the log3t model trained with imitated logs generated based on real and imitated variables.
compared to the log3t model trained with imitated logs based on real variables utilizing both real and imitated variables in imitated logs generation resulted in a ga improvement of .
.table parsing accuracy for different word order on bechmark datasets random order original order log3t average ga .
.
.
table variable probabilities assigned by log3t to constants and variables on datasets dataset constant variable difference hdfs .
.
.
hadoop .
.
.
spark .
.
.
zookeeper .
.
.
bgl .
.
.
hpc .
.
.
thunderbird .
.
.
proxifier .
.
.
windows .
.
.
linux .
.
.
android .
.
.
healthapp .
.
.
apache .
.
.
openssh .
.
.
openstack .
.
.
mac .
.
.
average .
.
.
table parsing accuracy and variable probabilities for different word extractions methods on datasets training method average ga probability difference with real variables .
.
with real and imitated variables .
.
furthermore the difference in predicted variable probabilities between constants and variables was expanded by .
with the inclusion of imitated variables.
figure illustrates the integration of standard test time training into the log3t model wherein the classifier of the model is configured with non shared parameters while the attention block is configured with shared parameters.
we set the batch size to and compare the final ga of all batches achieved by two versions of testtime training on the benchmark datasets.
the log3t model using standard test time training achieved an average ga of .
while the log3t model using the modified test time training achieved an average ga of .
across benchmark datasets.
answer to rq3 the predicted variable probabilities generated by the log3t model can be utilized to distinguish between constants and variables leading to good parsing accuracy.
the training strategy of log3t which incorporates imitated variables also contributes to its effectiveness in log parsing.
.
rq4 efficiency of log3t to answer rq4 we verified whether the log parsing speed of log3t is higher than the log generation rate of large scale systems in the real world.
figure shows the running time of log3t on 434log parsing with generalization ability under new log types esec fse december san francisco ca usa figure illustration of standard test time training used in the log3t model for comparative experiments shared parameters attention block non shared parameters classifier figure running time of log3t facing different log volume different volumes of bgl datasets.
we designed batch processing for log3t to parse logs.
experimental results show that log3t takes around 80s to process 100k logs when the batch size is set to .
the computational power of the graphic processing unit is an important factor in determining the training and inference time of deep learning based approaches.
during parsing if we set batch size to log3t takes seconds to process 100k logs.
according to the positive correlation between computational power and the efficiency of log3t we believe that having more computational power within the company can indeed result in higher efficiency for log3t.
the aforementioned example of log generation speed of alibaba inc. requires the log parser to process 100k logs in less than seconds.
answer to rq4 the parsing speed of log3t is capable of matching the log generation rate of large scale systems in the real world.
discussion log parser whose source code is hidden.
spine is an efficient and scalable log parser and it can evolve from human feedback.
in order to improve the efficiency of spine logs with the same first n words are grouped together when creating the initial groups.
however logs starting with variables are common in practice.
drain can adaptively generate delimiters but still does not support logs starting with variables.
uniparser is not open sourced and the technical detail of uniparser is not described well in their paper so it is hard for us to reproduce their experimental result e.g.
the characters they used to cover the most of tokens formed by their combinations whether the parser uses tokenization filters .
both uniparser and drain lack generalization ability.
threats to validity.
we have identified the following major threats to validity.
.
external validity.
threats to external validity relate to the generalization of experimental results that is the effectiveness of log3t in the benchmark datasets does not necessarily guarantee its effectiveness in an industrial setting.
.
constructvalidity.
researchers have found that the performance of log parsers on existing metrics does not necessarily correlate with their usefulness for downstream tasks .
therefore in the future new metrics may be required to evaluate log parsers .
related work automated log parsing.
automated log parsing has replaced manual log reading and the manual setting of regular expressions through source code .
the main approaches of automated log parsing include heuristics clustering frequent item extraction and deep learning .
for unsupervised methods such as clustering and frequent item extraction their parameters that require prior knowledge to set are difficult to modify under new log types.
lpv uses skip grams to convert logs into vectors which are then grouped according to the similarity of vectors.
brain is a latest unsupervised approach incorporating heuristic rules and frequent item extraction.
in recent years supervised log parsing has gradually developed logppt achieved impressive performance by utilizing a small number of samples to provide prompts for fine tuning a large language model.
researchers have confirmed that chatgpt performs log parsing well but the efficiency is difficult to match the real world system log generation rate.
model generalization.
in real world applications deep learning models often run in non stationary environments where the target data distribution continually shifts over time e.g.
system logs .
domain generalization methods study the learning problem on multiple data domains where the source domain is available and target domain is unavailable.
domain adaptation methods study continual data drifts in dynamic environments.
test time or online domain adaptation can improve the target model performance with a small training cost.
in addition researchers handled the poor performance on new domains before and during adaptation.
there are many excellent studies in this field and in the future we may continue to explore the integration of other generalization techniques into log parsing conclusion in this paper we propose log3t the first log parsing approach with generalization ability under new log types without human intervention.
log3t incorporates the idea of test time training to automatically update model parameters to adapt to new log types.
extensive experimental results demonstrate that log3t is effective and can generalize to new log types.
the generalization ability of log parsers under new log types is a promising research direction which we will further explore in the future.
acknowledgement this work was supported by the national natural science foundation of china no.
no.
and the nanning science and technology project no.
.
data availability codes and data of log3t can be found at .
435esec fse december san francisco ca usa siyu yu yifan wu zhijing li pinjia he ningjiang chen and changjian liu.