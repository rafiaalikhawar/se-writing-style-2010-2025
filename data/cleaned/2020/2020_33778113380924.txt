caspar extracting and synthesizing user stories of problems from app reviews hui guo secure computing institute north carolina state university raleigh north carolina hguo5 ncsu.edumunindar p. singh secure computing institute north carolina state university raleigh north carolina mpsingh ncsu.edu abstract a user s review of an app often describes the user s interactions with the app.
these interactions which we interpret as mini stories are prominent in reviews with negative ratings.
in general a story in an app review would contain at least two types of events user actions and associated app behaviors.
being able to identify such stories would enable an app s developer in better maintaining and improving the app s functionality and enhancing user experience.
we present caspar a method for extracting and synthesizing user reported mini stories regarding app problems from reviews.
by extending and applying natural language processing techniques caspar extracts ordered events from app reviews classifies them as user actions or app problems and synthesizes action problem pairs.
our evaluation shows that caspar is effective in finding actionproblem pairs from reviews.
first caspar classifies the events with an accuracy of .
on manually labeled data.
second relative to human evaluators caspar extracts event pairs with .
precision and .
recall.
in addition we train an inference model on the extracted action problem pairs that automatically predicts possible app problems for different use cases.
preliminary evaluation shows that our method yields promising results.
caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering.
acm reference format hui guo and munindar p. singh.
.
caspar extracting and synthesizing user stories of problems from app reviews.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction application distribution platforms such as apple app store and google play store provide critical pathways for users to provide feedback to app developers in the form of ratings and reviews .
developers must pay close attention to such post deployment user feedback because it contains important information such as feature requests and bug reports .
not surprisingly given the explosive increase in the number of reviews and the demands for permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
productivity user reviews have attracted much research interest of late .
however current approaches focus on arguably the more superficial aspects of reviews such as their topics and the reviewer s sentiment for an app or a feature.
some of these studies target the classification and collection of whole reviews that describe app problems.
such endeavors presume that developers would read and understand the collected full reviews to identify useful insights which is time consuming and error prone.
in contrast we observe that reviews often carry deeper knowledge than is traditionally mined.
such knowledge would be valuable if it were extracted and synthesized.
specifically we have found that a user s review of an app often tells a mini story about how the user interacted or attempted to interact with the app.
this story describes what function the user tried to bring about and how the app behaved in response.
definitions.
we define a user story as a sequence of ordered events that a user reports regarding his or her interaction with an app.
an event in a story is a part of a sentence that describes a single action.
we use the term event phrase to talk about an event as it is represented in language.
investigating stories present in app reviews has major implications for software engineering.
these stories not only serve as de facto deployment reports for an app but also express users expectations regarding the app.
in our study we consider an app problem story as a sequence of ordered events that happen in a use case where the app violates the user s and possibly the developer s expectations.
a story of interest in this study includes at least two types of events user actions and app problems.
an app problem is an undesirable behavior that violates a user s expectations.
in particular when a review gives a negative rating the stories within it contain rich information regarding app problems.
these app problems when reported on and sometimes ranted about by users call for a developer s immediate attention.
negative reviews tend to act as discussion points and if left unaddressed can be destructive to user attraction and retention .
a user action event describes what action the user took when interacting with the app often indicative of user expectations.
user actions in app problem stories depict the scenarios where app problems occur.
example shows one star review for the weather channel app1from apple app store in this and other examples all underlining is added by us for clearer illustration .
ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea hui guo and munindar p. singh the review in example contains a pair of ordered events a user action trying to scroll through cities and the app s problematic behavior in response app hesitating.
example username1 somebody messed up!
horrible.
what on earth were these people thinking.
i m going to look for another weather app.
ithesitates when itrytoscroll thru cities.
i m so irritated with this fact alone that i m not going to waste my time explaining the other issues.
we define an action problem pair as such a pair of events in which an app problem an event follows or is triggered by a user action an event .
such event pairs are mini stories that describe where and how the app encounters a problem.
therefore these pairs can yield specific suggestions to developers as to what scenarios they need to address.
combining user actions with app problems makes the problems easier to understand.
for example consider the following reviews for the fitbit app2from apple s app store example username2 app crashing app keeps crash ingwhen igoandlogmyfood.
not all the time but at least a crashing session a day.
username3 app full of bugs theappcrashes freezes andmiscalculates caloriesconstantly.
the only reason i still own a fitbit is the website.
both reviews report the problem of app constantly crashing.
however the first review which includes the user action i.e.
logging food is more informative than the latter.
many users describe their actions when they report problems in app reviews.
we found from a manual annotation of one star reviews see section for more details that reviews .
mentioned an app problem of which .
described the associated user actions.
of course some app problems may occur without users actions.
although such reviews may mention serious problems that need a developer s attention they do not provide insightful information for a developer to address those problems.
event extraction and synthesis.
extracting and synthesizing actionproblem pairs from app reviews is a challenging task.
first extracting the targeted events i.e.
user actions and app problems is nontrivial because user provided text is not well structured and are often riddled with typos and grammatical errors.
second users may not describe the events of their interaction with apps in a sequential order.
determining the temporal or causal links between events can be difficult.
we present caspar a method for extracting and synthesizing stories of app problems as action problem event pairs from app reviews.
caspar addresses the following main research question how effectively can we extract and synthesize app problem stories as action problem pairs from app reviews?
manually reading negative reviews to identify reports of app problems is time consuming.
automatic extraction and synthesis of such reports can save time for analysts of app reviews.
to answer rqextract we investigate the performance of caspar in classifying events as user actions or app problems and identifying action problem pairs compared to human annotators.
to the best of our knowledge caspar is the first work on app reviews that focuses on the user app interaction stories that are told in app reviews.
event inference.
we consider a possible enhancement of the extraction of user actions and app problems from text automatically learn the relation between user actions and app problems and infer relevant app problems corresponding to a user action from this link.
this type of linking and inference may potentially help developers preemptively handle possible problems in different use cases especially where user actions are known but the problems have not yet been reported.
further developers and analysts would not need to limit their analysis to extracting information from a limited set of reviews for one target app but instead would be able to leverage reviews for all apps with similar functionalities.
so would be particularly helpful for the less popular apps that might each garner only a few reviews regarding app problems.
therefore we ask the following research question rqinfer how effectively can an event inference model infer app problems in response to a user action?
caspar includes a preliminary investigation of rqinfer .
we evaluate the effectiveness of caspar s tentative solution in linking user actions and app problems as well as inferring relevant app problems that may happen after a user action.
contribution.
we introduce and provide the first solution to the research problem of identifying and analyzing user reported stories.
caspar adopts natural language processing nlp and deep learning and brings the investigation of app reviews down to the event level.
instead of generating a selective set of full reviews caspar yields high quality pairs of user action and app problem events.
moreover by linking app problems and user actions caspar can infer possible problems that correspond to a use case.
a crucial meta requirement in app development is to avoid such problems.
our reusable contributions include a method for extracting and synthesizing stories describing app problems as action problem event pairs from app reviews a resulting dataset of collected event pairs and a tentative method and preliminary results for event inference.
by presenting caspar we emphasize the importance of analyzing user reported stories regarding the usage of a specific app.
organization.
the rest of the paper is organized as follows.
section describes the related studies on the analysis of app reviews and event inference.
section introduces the targeted data and our method in caspar.
section demonstrates the results of our method.
section concludes with a discussion of the merits and limitations of caspar and directions for future work.
629caspar extracting and synthesizing user stories of problems from app reviews icse may seoul republic of korea related work analyzing informative reviews and prioritizing feedback have been shown to be positively linked to app success .
recent work on analyzing app reviews mostly involves generic nlp techniques.
in particular it does not address the tasks of extracting and analyzing stories in app reviews and applying event inference on those stories.
we now introduce the related work in app review analysis and event inference and story understanding.
.
information extraction from app reviews app reviews include valuable information for developers.
pagano and maalej report on empirical studies of app reviews in the apple store.
they identify topics in user feedback in app stores by manually investigating the content of selected user reviews.
pagano and maalej also find that a substantial fraction of the reviews specifically .
of reviews with one star ratings include the topics of shortcoming orbug report which could be mined for requirements related information.
previous studies on information extraction from app reviews emphasize the classification of reviews as a way of combing through a large amount of text and reducing the effort required for analysis.
maalej and nabil classify app reviews according to whether or not they include bug information requests for new features or simply praise for an app.
based on maalej and nabil s classification method dhinakaran et al .
investigate active learning to reduce manual effort in annotation.
panichella et al .
classify user reviews based on a taxonomy relevant to software maintenance and evolution.
the base categories in their taxonomy include information giving information seeking feature request and problem discovery.
the problem discovery type of app reviews describe app issues or unexpected behaviors.
by applying this classification panichella et al .focus on understanding the intentions of the authors of reviews.
chen et al .
employ unsupervised techniques for identifying and grouping informative reviews.
their framework helps developers by prioritizing and presenting the most informative app reviews.
guzman et al .
investigate user feedback on twitter to identify and classify software related tweets.
they leverage decision trees and support vector machines svms to automatically identify relevant tweets that describe bugs shortcomings and such.
with the amount of available app reviews increasing reading through entire reviews become impractical.
to reduce the time required by developers recent research targets certain topics and investigates user reviews on the sentence level.
iacob and harrison retrieve sentences that contain feature requests from app reviews by applying carefully designed rules such as keyword search and sentence structures.
they specify these rules based on an investigation of the ways users express feature requests through reviews.
di sorbo et al .
summarize app reviews by grouping sentences based on topics and intention categories.
developers can learn feature requests and bug reports more quickly when presented with the summaries.
kurtanovi and maalej classify reviews and sentences based on user rationale.
they identify concepts such as issues and justifications in their theory of user rationale.
using classification techniques kurtanovi and maalej synthesize and filter rationale backed reviews for developers or other stakeholders.
.
event inference and story understanding we recognize that app reviews contain user app interaction stories related to user experience.
a story in the sense of natural language processing is a sequence of events.
research on the topics of event inference and story understanding involves understanding the relations between events as well the structure of events in a sequence.
these two topics have gained prominence in information extraction because they can be applied to many tasks including question answering storytelling and document summarization.
previous work on these topics targets sources of well edited text such as news articles books movie scripts and blogs.
caspar is an approach for event inference and story understanding on app reviews which are generally casually produced.
event inference involves understanding relations between events.
the extraction of temporal relations between events has garnered much attention.
mani et al .
apply rules and axioms such as the existence of marker words like before andafter to infer temporal relations between events.
mirroshandel and ghassem sani extract temporal relations of events from news articles with carefully engineered features.
they adopt basic features of events such as tense polarity and modality as well as extra event event features such as the existence of prepositional phrases.
ning et al .
propose facilitating the extraction of temporal relations with a knowledge base of such relations collected from other available large sources of text such as news articles.
they claim that extraction of temporal relations can be more effective if the extraction systems understand how events usually happen.
many studies have endeavored to extract causal relations based on events temporal orders.
beamer and girju propose the concept of causal potential as a measure of the strength of the causal relation between a pair of events.
two events tend to be causally related more strongly if they occur more frequently in one order than the reverse order.
hu and walker extract temporal relations of actions from action rich movie scripts and infer their causality.
based on similar ideas hu et al .
extract and infer fine grained event pairs that are causally related from blogs and film descriptions.
zellers et al .
provide swag a dataset of multiple choice questions composed by event pairs extracted from video captions.
the swag task is given the first event to select the second event from four choices based on commonsense inference.
studies of event relation extraction on text with lower quality such as tweets and online reviews are lacking.
in caspar we focus on event pairs describing app user interactions where a user action may trigger but not necessarily be the cause of an app problem.
story understanding investigates longer sequences of events.
one important task in story understanding is to infer an event that has been held out from the story .
the story cloze test is a popular event inference task based on a high quality collection of five sentence stories extracted from personal weblogs about everyday life.
specifically it calls for an inference model that infers the last event the ending based on four preceding events.
deep learning is a popular family of techniques in event inference and story understanding.
long short term memory lstm networks are a type of recurrent neural networks that yield superior performance on sequences of data such as text.
srinivasan 630icse may seoul republic of korea hui guo and munindar p. singh et al.
target the story cloze test using a straightforward bidirectional lstm model to determine whether an event is random or the correct ending of a story.
we learn from their insights when building and training the event inference model in caspar.
bert is a pretrained language representation model that can be finetuned to achieve state of the art performances in numerous nlp tasks including the event inference in swag.
we borrow insights from bert when adopting pairs of sentences as input.
method caspar s methid consists of three steps.
first caspar extracts events from targeted app reviews and order them based on heuristics as well as more sophisticated natural language processing nlp techniques part of rqextract .
second caspar synthesizes actionproblem pairs by classifying the events and keeping the ordered pairs of user action and app problem events part of rqextract .
third caspar trains an inference model on the extracted event pairs and infers app problem events given the user actions rqinfer .
figure shows an overview of caspar.
app reviewsevent extraction and ordering ordered events action problem p airsaction problem p air synthesist rain classifier for synthesis t rain problem inference model query user actionproblem inferenceapp problemsextraction synthesis inference figure an overview of caspar.
for event extraction caspar takes a corpus of targeted app reviews and produces a list of ordered events for each review.
for synthesis of event pairs caspar requires a dataset of events labeled with event types to train its event classifier.
.
dataset targeted app reviews in the present study we collected reviews including text and star ratings received by apps from the period of to by crawling the app reviews pages on apple app store.3table lists the counts of reviews with different ratings.
number of reviews grouped by ratings.
rating count an app problem indicates a deviation from the reviewer s expectations.
app problems are prevalent in reviews with negative ratings.
most reviews with bug reports and without praise are associated with one star ratings .
in this study we focus only on app reviews with the most negative ratings i.e.
one star ratings.
we focus on action problem event pairs each of which comprises an expected user action and an app problem that indicates a deviation from expected app functionality.
the app problem is related to the user action in that the former happens after or is triggered by the latter.
although an app review tells a story i.e.
presents a sequence of events not all pairs of events are necessarily related temporally or causally.
to make sure that extracted events are temporally ordered and casually related we keep only the reviews that contain common temporal conjunctions including before after and when.
in addition we consider key phrases that indicate temporal ordering such as as soon as every time and then.
instead of processing all available app reviews which are a large dataset we adopt key phrase search as a heuristic and keep only the reviews that match at least one key phrase.
we borrow this insight from previous studies which have shown that such temporal markers are effective in the identification of sentence internal temporal relations .
therefore we conduct our experiments on a refined dataset of negative reviews that contain at least one key phrase.
the total number of targeted reviews is .
table lists the key phrases and the count of reviews that contain each of them.
note that some reviews contain multiple key phrases.
we targeted this list of key phrases because they are the most prominent temporal phrases in our dataset.
we excluded words that are likely to be used in other senses besides their temporal meanings such as since andas which frequently act as causal conjunctions.
.
extracting events this step extracts ordered events from these targeted reviews using nlp techniques.
we refer to an event in the text as a phrase that is rooted in a verb and includes other attributes related to the verb.
an event phrase is different from a verb phrase in that it is usually the longest phrase related to a target verb that does not include words related to other target verbs.
caspar s extraction step employs the following nlp techniques.
part of speech pos tagging.
part of speech pos tagging is a process that marks a word in a sentence with a tag corresponding to its part of speech based on its context and properties.
pos tagging is commonly provided in nlp libraries.
we leverage pos tagging to identify verbs in a sentence as each event phrase must 631caspar extracting and synthesizing user stories of problems from app reviews icse may seoul republic of korea table counts of one star reviews with key phrases.
key phrase occurrences after as soon as before every time then until when whenever while targeted reviews ithesitates whenitrytoscrollthrucitiesnsubjroot advmodadvcl auxxcomp pobj prep nsubj figure dependency parse tree for the example sentence.
contain a verb.
common pos tags for verbs include vbfor the base form vbd for past tense and vbg for gerund or present participle.
dependency parsing.
dependency parsing is the process of analyzing the grammatical structure of a sentence.
for each word in the sentence a dependency parser identifies its head word and how it modifies the head i.e.
the dependency relation between the given word and its head.
the dependency relations identified in a sentence define a dependency based parse tree of the sentence.
event extraction from a sentence.
to identify an event phrase rooted in a certain verb we find the subtree rooted on this verb in the dependency based parse tree.
note that a sentence may include multiple verbs and some of the verbs may belong to the same event.
beginning from a dependency parse we consider only verbs that are parsed as root advcl adverbial clause modifier or conj conjunct .
we choose these dependency relations because they are good indicators of events.
a verb parsed as advcl is typically the root verb of an adverbial clause that starts with one of the key phrases which describes a separate event from the main clause.
aconj verb is often the root of an event phrase in a list of events.
since the dependency tree rooted in root covers all the words in a sentence we extract the root event phrase from words that are not incorporated in any other event phrases.
we remove punctuation marks at both ends of an event phrase.
figure shows the dependency parse tree of the underlined sentence in example .
we consider two verbs hesitates root andtry advcl .
in this parse tree all words are in the subtree rooted onhesitates whereas the phrase when i try to scroll thru cities is in the subtree rooted on try.
therefore two events are extracted from this sentence it hesitates andi try to scroll thru cities.event extraction from a review.
we take the following steps to extract ordered events from each review.
find and keep key sentences i.e.
sentences that contain the key phrases and collect the sentences surrounding them one preceding sentence and one following sentence if any.
extract event phrases from each key sentence and its surrounding sentences.
order event phrases in each key sentence using heuristics.
collect other event phrases in the original order in which they appear in the text.
the heuristics we adopt to order the events are shown in table where e1 e2indicates that e1happens before e2.
table heuristics for events in a complex sentence.
sentence structure event order e1 before until then e2 e1 e2 e1 after whenever every time as soon as e2e2 e1 e1 when e2 e1 e2 if verb of e1is vbg e2 e1 otherwise in the case of e1 when e2 e2happens first most of the time.
however consider the key sentence in example for snapchat4 .
example username4 virus i love snapchat.
use it often.
but snapchat gave my phone a virus.
soiwasusingsnapchat todaywhen allofasuddenmy phone screen turned blue andthen myphone shut offfor7 hours.
hours.
so i had to delete snapchat because it was messing up my iphone 5c.
we add the heuristic that in e1 when e2 where e1is continuous i.e.
the verb in e1is marked as vbg by the pos tagger e1occurs before e2.
note that the key phrases are not included within any event phrase.
instead we label the events based on their positions relative to the key phrases.
for example if an event appears in an adverbial clause that starts with when it is labeled a subclause event and the event outside of this subclause is labeled main.
events that do not appear in a key sentence are labeled surrounding.
we keep these labels as context information to make the events easier to understand.
there is one key sentence the underlined sentence in example section .
thus we keep three sentences and extracts four events from them.
we order the events extracted from the key sentence based on the heuristic for when.
table shows the ordered list of extracted events.
.com us app snapchat id447188370 632icse may seoul republic of korea hui guo and munindar p. singh table events extracted from example .
id label event phrase e1surrounding i m going to look for another weather app e2subclause when i try to scroll thru cities e3main it hesitates e4surrounding i m so irritated with this fact alone .
.
.
.
synthesizing event pairs this step classifies the extracted events into user actions app problems or neither and synthesizes action problem pairs.
we define user actions as what the users are supposed to do to correctly use the app typically from an anticipated use case.
we define app problems as the unexpected and undesirable behaviors of an app in response to the user actions including the lack of a correct response that are not intended by the app developers.
in negative reviews users sometimes complain about the designed app behaviors which we do not classify as problems.
accordingly we disregard the types of event phrases shown in table without checking the context the reviews from which they are extracted .
event phrases that fall into these categories are labeled neither.
table types of event phrases we classify as neither.
event phrase type example .incorrectly extracted verb phrases see section .
.users affections or personal opinions toward the app it has made the app bad ms onedrive is superior .app behaviors that are designed by the developers i guess you only get of the levels free .users observations of the developers you guys changed the news feed .
users requests of features needs the ability to enter unlimited destinations .users imperative requests for bug fixes fix the app please .users behaviors that are not related to the app i give you one star i contacted customer service .events that are ambiguous without context or too general it was optional i try to use this app manual labeling.
to create a training set for the classification we conducted three rounds of manual labeling with three annotators pseudonyms a b c who are familiar with text analysis and app reviews.
the three annotators were asked to label each event as a user action an app problem or neither as described above.
for each round we randomly selected extracted events from the results in the previous step.
in the first two rounds each annotator labeled all events in a subset followed by the annotators resolving their disagreements through discussion.
in the third round each event was labeled by two annotators and any disagreements were resolved by labeling the events asneither.
we consider this resolution acceptable as the neither events are not considered in the event inference task.
table shows the pairwise cohen s kappa for each round of manual labeling between each pair of annotators a b a c and b c for rounds and mixed for round since each event received two labels from a b b c or a c before any resolution of differences.
table pairwise cohen s kappa for manual labeling.
round count cohen s kappa a b a c b c mixed .
.
.
.
.
.
.
considering there are three classes so agreement by chance would occur with a probability of .
the results show that the annotators had moderate to good agreement over the labels before their discussions.
after excluding some events that were identified by the annotators as having parsing errors or being too short we produce a dataset that contains labeled events.
table shows the distribution of event types in this dataset.
table distribution of the manually labeled dataset.
event type count user action app problem neither total event encoding.
before performing the classification we need to convert the event phrases into vectors of real numbers.
one basic encoding method is tf idf term frequency inverse document frequency which we adopt as a baseline.
tf idf has been widely adopted in information retrieval and text mining.
however tf idf results in sparse vectors of high dimensionality and loses information from the phrase since it ignores the order in which the words appear.
to obtain results with higher accuracy we adopt the universal sentence encoder use to convert event phrases into dense vectors.
use is a transformer based sentence embedding model that leverages the encoding subgraph of the transformer architecture .
use vectors capture rich semantic information.
the pretrained use model and its variants have become popular among researchers for downstream tasks such as text classification and clustering and can achieve state of the art performance for these tasks.
classification.
we adopt support vector machines svms to classify the sentence vectors into the aforementioned three classes.
we instantiate two classifiers with probability estimates for user actions and app problems respectively since svm can be applied only on binary classification.
633caspar extracting and synthesizing user stories of problems from app reviews icse may seoul republic of korea for a given event e the first svm yields a probability u ofe being a user action and the second svm yields a probability a ofebeing an app problem.
we adopt the following formulae to convert these probability estimates into a three class probability distribution.
each tuple below is of the form pneither paction andpproblem which represent the probability estimates of event e being neither a user action or an app problem respectively.
the purpose of this exercise is to convert two probability estimates into a three class probability distribution via a continuous transformation while preserving the results of the original classifiers.
an event is classified into the class with the highest probability after this transformation.
ifu .5anda .
p e u a u a 2ua u u a 2ua a u a 2ua ifu .5anda .
p e u a u a a a ifu .5anda .
p e a u u u a u ifu .5anda .
p e .
.
u a u .
u a a .
u a in our experiments we adopted the use implementation in tensorflow hub5to encode each event phrase into a vector.
each use vector is of size .
for tf idf the minimal document frequency df adopted was resulting in vectors of size .
we adopted the svm implementation in scikit learn 6which provides an estimate of the probability of a classification.
synthesis.
upon obtaining sequences of ordered events each of which has been classified as a user action or an app problem we can extract action problem pairs by selecting user actions as well as the app problems that immediately follow them.
in such a pair we can assume the user action triggers the app problem since they happen sequentially in a story.
manual verification.
to evaluate the effectiveness of caspar in extracting and synthesizing action problem pairs we compare the performance of caspar against human annotators.
we randomly selected negative one star app reviews and asked four graduate students majoring in computer science to independently identify action problem pairs in these reviews.
each review was examined by two annotators.
the annotators achieved moderate agreement.
the cohen s kappa for the annotations was .
.
an author of this paper acted as a tiebreaker to resolve the disagreements.
for evaluation of caspar we address rqextract by reporting the accuracy arising from fold cross validation for event classification as well as the precision and recall of caspar against the manual results with disagreements resolved .
word embeddingi clicked app crashedlstm1 lstm1 lstm1 lstm1 lstm1lstm2 lstm2 lstm2 lstm2 lstm2 forward lstm layerbackward lstm layerlinear output layerlabel softmax ......... .........concatenationsfigure a bidirectional lstm network for sequence classification.
.
inferring events this step infers possible app problems i.e.
unexpected app behaviors based on an expected user action.
the purpose of this event inference task on the action problem pairs is to further investigate the relation between user actions and app problems.
such inference can potentially help developers anticipate and address possible issues to ensure app quality.
relation between events.
we can learn the relation between user actions and app problems from the collected action problem pairs.
we propose learning this relation through a classification task.
given a pair of ordered event eu ea where euis auser action andeais an app problem the classifier determines whether eais a valid follow up event toeuor a random event.
thus the classes for each entry are ordered event pair andrandom event pair.
we define this type of classification as event follow up classification.
we first convert a pair of events into a vector representation and then apply existing classification techniques on these vectors.
in addition to encoding an event into a vector using sentence encoding techniques we convert an event phrase into a list of word vectors.
converting words into dense vectors require a word embedding technique.
word embedding is the collective name for models that map words or phrases to dense vectors of real numbers that represent semantic meanings.
popular word embedding techniques include word2vec and glove .
we experiment with the following classification models.
baseline.
we adopt svm for this classification.
as a baseline we first convert each event into a vector using tf idf and then concatenate the vectors of the two events in an event pair and train an svm classifier on the concatenated vectors.
use svm.
we convert each event into a vector using use and then concatenate the use vectors of the two events in an event pair.
we then train an svm classifier on the concatenated vectors.
bi lstm network.
we apply three substeps.
one concatenate the tokens in the two events separated by a special token .
two convert the concatenated tokens into a sequence of word vectors.
three train a bidirectional lstm network for the classification of the sequences of vectors.
the structure of this network is shown in figure .
in our experiments we adopted one of spacy s pretrained statistical models for english en core web lg 7with glove vectors 634icse may seoul republic of korea hui guo and munindar p. singh trained on common crawl8data to convert each token into a vector.
each glove vector is of size .
we implemented the bidirectional lstm network using tensorflow.9the number of hidden layers is .
an adam optimizer with a learning rate of10 4is used to minimize the sigmoid cross entropy between the output and the target.
we trained the model for epochs with a batch size of one.
negative sampling.
to train the classifiers for event follow up classification we conduct negative sampling to create training sets.
negative sampling i.e.
using random examples as negative evidence is a well accepted nlp technique for scenarios where only positive examples are available.
the concept of negative sampling was first defined by mikolov et al .
for training word vectors.
in general each positive example of the context in which a word appears is explicit in a corpus.
however a negative example i.e.
a context in which a word does notappear is implicit.
negative sampling solves this problem by considering a random context as negative evidence.
negative sampling is widely used now.
for example bert adopts negative sampling for the next sentence prediction task where a random second sentence is considered as negative evidence i.e.
not the next sentence of the given sentence.
to create a dataset for training and testing a classifier we first divide the extracted action problem pairs into a training set and a testing set .
then for each user action event we add two event pairs to the dataset a positive example and a negative example.
we keep the extracted action problem pair as a positive example an ordered event pair since the included app problem event is the actual follow up event.
following negative sampling we generate a negative example a random event pair by combining the user action and a random app problem event.
the app reviews setting poses an interesting challenge for negative sampling multiple reviewers may have identified duplicate or similar app problem events.
for example the events app crashed and app freezes are common occurrences.
an extracted action problem pair includes a user action and its actual follow up app problem event but a randomly chosen app problem event is likely to be semantically similar to the latter which can impair the accuracy of the classification.
we solve this problem by choosing dissimilar events when composing our negative examples.
specifically we introduce the following strategies for negative sampling in addition to the naive random selection.
clustering.
we cluster all app problem events into two groups based on cosine similarity of their use vectors using k means implemented in scikit learn .
for each positive example i.e.
an extracted action problem pair we find the cluster to which the problem event belongs and randomly choose a problem event from the other cluster when generating the negative example.
similarity threshold.
when choosing a random app problem event we shuffle all available app problem events using the random.shuffle function in python and iterate over them.
we select the first app problem event whose similarity with the actual follow up event based on cosine similarity of the respective use vectors is below a preset threshold.
we experiment with thresholds of .
and .
.
we experimented with four negative sampling strategies completely random clustering similarity .
and similarity .
resulting in four datasets.
to understand the differences between these strategies consider the examples of table .
note that this is purely for illustration in our experiments we consider all available problem events for negative sampling.
table shows a user action event its actual follow up event and four random problem events including their clusters and cosine similarity to the actual follow up event .
table an action problem pair and four random problems.
id event phrase cluster id cos. sim.
a i play videos in fb p i have no sound .
p1there is no sound .
p2i get kicked off .
p3i m unable to play .
p4my password does n t work .
regardless of the strategy the event pair a p is kept as a positive example since pis the actual follow up event to a. to generate a negative example the completely random strategy chooses any one of a p1 a p2 a p3 and a p4 .
the clustering strategy chooses only from a p3 and a p4 the other cluster .
the similarity .5strategy chooses a p2 or a p4 and the similarity .25strategy may choose only a p4 as a negative example.
inferring app problems.
the trained classification models estimate the probability of an app problem following or being caused by a user action and therefore can be leveraged for inferring possible follow up app problems based on a user action.
for a given user action eu we rank all possible app problems eia by the model s confidences of the pair eu eia being an ordered event pair.
the top ranked app problems are treated as the results of event inference.
as we mentioned above many app problems are similar to each other for which a classifier should yield similar probabilities.
to diversify the inferred events we choose a similarity threshold and enforce that the cosine similarity between any two inferred events is below this threshold.
in a preliminary investigation of rqinfer we manually verified the relevance of the top app problem events for a user action by the trained bidirectional lstm network similarity .25as the negative sampling strategy .
we considered only app problems extracted from reviews of the same app to generate inferred problems.
we chose a similarity threshold of .75to diversify the inferred events.
we asked three graduate students majoring in computer science to independently label each of events based on whether it is possible that it follows or is triggered by the user action.
to answer rqinfer we report the ratio of the relevant app problems in the top ranked inferred events based on the manual verification results.
635caspar extracting and synthesizing user stories of problems from app reviews icse may seoul republic of korea results we now present the results of our experiments.
as mentioned in section all experiments were conducted on the one star reviews that contain key phrases.
.
event extraction and classification as described in section .
we trained two svm classifiers and combined their results for a three class classification.
table shows the accuracy of each classification.
table accuracy of event classification.
classification tf idf svmuse svm user actions vs. others .
.
app problems vs. others .
.
user actions vs. app problems vs. neither .
.
we then applied the better performing of the two trained classifiers use svms to the entire dataset of extracted events.
table shows the results for the events extracted from example .
table event classification for events in example .
id event phrase p1 e p2 e prediction e1i m going to look for another weather app0.
.
neither e2i try to scroll thru cities .
.
user action e3it hesitates .
.
app problem e4i m so irritated with this fact that i m not going .
.
.
.
.
neither event pairs.
each adjacent and subsequently ordered actionproblem event pair is then synthesized as a possible result.
for example e2 e3 in table is synthesized accordingly.
the total number of resulting event pairs is .
table shows additional examples with some paraphrasing to save space for the same app.
table extracted event pairs for the weather channel.
user action app problem after i upgraded to iphone this app doesn t work as soon as i open app takes me automatically to an ad you need to uninstall app before location services stops every time i try to pull up weather i get no data whenever i press play it always is blotchy when i have full bars always shows up not available i updated my app then it deleted itselfmanual verification.
we compared the performance of caspar against human annotators.
table shows two confusion matrices based on whether an event pair has been identified one for all reviews and one for reviews with key phrases.
of the randomly selected one star app reviews only contain one or more key phrases that we have adopted.
table manual verification of caspar s extraction results.
all reviews reviews w key human human id ed not id ed id ed not id ed casparid ed not id ed caspar identified action problem pairs from these reviews whereas the annotators identified .
when we consider the labels produced by the annotators as the ground truth we find that caspar has an overall accuracy of .
a precision of .
and recall of .
.
the human annotators identified app problem events from reviews of which contained the related user action events.
of these reviews that contain action problem pairs .
contain at least one key phrase.
we discuss these results in section .
.
event inference we investigated the performance of the proposed classifiers on classifying the follow up event of an event and conducted a preliminary experiment with the inference of app problems based on a user action.
event follow up classification.
as mentioned in section .
we generated four datasets based on four negative sampling strategies.
since we extracted action problem pairs in the previous step a negative sampling strategy would generate negative data points.
thus each dataset included data points for training and for testing.
table shows the accuracy of each method for event follow up classification on each dataset.
inferring app problems.
figure shows the top app problem events for the user action i try to scroll thru cities.
the inferred event it loads for what seems like forever presents the most similar meaning to the ground truth it hesitates i.e.
app pausing or not responding .
in the manual verification all three annotators labeled a1 a2 a3 a4 and a8as relevant and a5 a7 and a10as irrelevant .
they disagreed over the other two events.
.
curated dataset our entire dataset comprises one star reviews extracted events along with their predicted types events used for manual labeling with manually labeled types and collected action problem pairs.
this dataset along with our source code is available for download.
the public release of this dataset was approved by the institutional review board irb at nc state university.
10hguo5.github.io caspar 636icse may seoul republic of korea hui guo and munindar p. singh table accuracy of classification of event pairs.
classifier negative sampling strategy accuracy baseline completely random .
use svm completely random .
bidirectional lstm completely random .
baseline clustering .
use svm clustering .
bidirectional lstm clustering .
baseline similarity .
.
use svm similarity .
.
bidirectional lstm similarity .
.
baseline similarity .
.
use svm similarity .
.
bidirectional lstm similarity .
.
user action i try to scroll thru cities ground truth it hesitates inferred app problems relevant a1it says there is an error a2it loads for what seems like forever a3it tells me the info for my area is not available a4the app crashes a8it reset my home location conflicting judgments a6it rarely retrieves the latest weather without me having to refresh a9it goes to a login screen that does not work irrelevant a5the radar never moves it just disappears a7i rely heavily on it for the past month it says temporarily unavailable a10radar map is buggy weather activity stalls appears then disappears figure inferred app problem events to follow a user action threshold .
grouped by manual verification results.
conclusions and discussion we presented caspar a method for extracting and synthesizing app problem stories from app reviews.
caspar identifies two types of events user actions and app problems as well as how the specific events in a story relate.
caspar adopts heuristics and classification and effectively extracts ordered event pairs.
by extracting and synthesizing such app problem instances caspar helps developers by presenting readable reports of app issues that require their attention.
caspar extracts high quality action problem pairs with high precision.
in addition caspar trains an inference model with the extracted event pairs leveraging nlp techniques and deep learning models and inferspossible follow up app problems based on user actions.
such inference enables developers to preemptively address possible app issues which would help them improve the quality of their apps.
.
merits caspar demonstrates the following merits.
previous studies of app reviews have focused on text analysis of app reviews on the review level.
their results are collections of somewhat verbose reviews that require further manual investigation by developers which becomes impractical as the number of available reviews increases.
caspar dives deeper into a review down to the event level and can extract and synthesize succinct action problem pairs.
app problem event pairs.
by extracting and synthesizing actionproblem pairs from app reviews caspar identifies app problems that require developers attention.
each extracted event pair describes an app s unexpected behavior as well as the context the user s action in which that behavior is seen.
knowing such action pairs can potentially help developers save time and effort to improve their apps by addressing the identified problems.
caspar can be applied selectively such as to reviews for certain apps over a specified period of time so that the extracted event pairs are more valuable to a particular audience of developers.
by answering rqextract we have shown that caspar extracts targeted event pairs effectively and the classification of event types yields high accuracy.
event inference.
rqinfer seeks to establish a connection between user actions and app problems.
our preliminary solution learns the relations between the two types of events in the training set.
our experiments have shown that caspar yields satisfactory performance when determining whether an app problem is random or a valid follow up event of a user action.
with the help of this classification caspar generates relevant follow up app problems to user actions.
inferring relevant app problems based on a user action has the potential of helping developers avoid problems or failures of user experience.
caspar does not limit the event types to user actions and app problems.
a possible future direction is to investigate its effectiveness in other types of inference such as inferring user actions based on an app problem to better understand the scenarios where a certain problem is likely to occur.
.
threats to validity the first threat to validity is that our annotators may lack the expertise in the software development of ios apps.
our annotators are familiar with or experts on concepts of nlp and machine learning but they may not possess enough experience in app development in industry which may have affected their judgments about the events and what labels to assign.
second all of our labeling and training was conducted on reviews with one star ratings from apple s app store.
our work may not generalize to reviews where the descriptions of app behaviors are not limited to app problems.
third the manually labeled training set for event classification includes randomly selected events for the apps that we targeted which might not be general.
for app reviews in different genres the accuracy of the event type classification may vary.
637caspar extracting and synthesizing user stories of problems from app reviews icse may seoul republic of korea .
limitations and future work we identify the following limitations of caspar.
each limitation leads to ideas for future work to mitigate that limitation.
key phrases.
we target only those app reviews that contain selected key phrases that indicate the temporal ordering of events.
using key phrase search limits the size of the resulting dataset only action problem pairs were extracted from a total of one star reviews.
we use these key phrases because we need the extracted events to be temporally and causally related.
further investigation on how to extract related events is required.
first we can incorporate phrases such as the less prominent temporal phrases ever since andany time that indicate additional relations between events.
also it would be worth experimenting with key phrases that indicate conditional or causal relations such as if andbecause.
additional key phrases may be found in a semisupervised fashion.
second extraction techniques without reliance on key phrases may be fruitful.
such techniques include leveraging discourse relations and sentiment analysis or relying on higherlevel features such as structural and semantic correspondence with respect to various attributes such as authorship or the function and importance of a mobile app .
third event inference models that automatically learn relations between events can potentially facilitate the extraction of targeted event pairs.
text quality.
the quality of app reviews varies widely.
in addition to the possibility of being less informative or disorganized app reviews as a type of user generated text are subject to low text quality indicated by slang typos missing punctuation or grammatical errors .
caspar extracts events using a part of speech tagger and a dependency parser which may work imperfectly on such text.
during the manual verification human annotators identified event pairs that caspar is not able to parse.
for example one review says app is now crashing everyone i tap a story where the typo causes caspar to miss the event pairs in it.
caspar identifies this sentence as one event which is classified asneither since the sentence is missing a conjunction.
however human annotators can easily identify two events in this sentence.
we posit that the low quality of user generated text is the most potent reason for the low recall of caspar in extracting event pairs.
thus an important future direction is to investigate extraction methods that do not rely on the correctness of the parser employed.
manual labeling.
caspar requires a dataset of events labeled with event types and manual labeling can be time consuming.
further it seemed difficult for the annotators to achieve high agreement.
we gave the neither label to data points on which the annotators disagreed which might have affected the number of extracted event pairs but not the correctness of them.
we identify the following reasons for the disagreement among annotators.
first incorrectly extracted event phrases may cause the annotators to disagree.
second there are difficult and undiscussed cases where annotators may disagree.
for example the event switching between apps doesn t make anything faster can be interpreted as an app problem or an irrelevant event.
third events have been stripped out of context some of them may lose critical information.
for example the event reset my phoneis usually a user action but the annotators could not be sure without context.
example shows the entire review for messenger11 .
example username5 great but...... this is a great app.
butithasbeen crash ingbefore itcanload.
resetmyphone gotthenew update foriosanditjustkeeps crash ing.
not sure if i m the only one with this problem.
action problem pairs.
caspar targets only those event pairs that describe single iterations of a user app interaction.
however this type of interaction does not cover all scenarios of app problems.
future work includes the investigation of longer sequences of events in user app interaction than just a pair.
for example the review in example describes multiple user actions none of which seems to have caused the observed problem.
however this review does report a bug that requires the developers attention.
in addition to action problem pairs many app reviews describe user expectations user reactions to app problems or misuses of apps.
we leave the extraction of other forms of user app interaction to future work.
event inference.
the proposed classification of event pairs yields moderate results.
one major reason is that quite a few app problems occur multiple times.
our negative sampling strategies improved the classification results.
future work includes more sophisticated negative sampling strategies.
a second possible reason for the moderate performance is that the training set is fairly small especially for a deep learning model.
we collected event pairs for different apps which may not be large enough to train a bidirectional lstm network.
a possible direction is to apply caspar on app reviews from other app distribution platforms to extract and synthesize more event pairs.
future work includes the improvement of recall for the extraction.
third we simplified event inference to event follow up classification which limits the inference to app problem events that have been reported.
to fully infer follow up events of user actions we may need to build more sophisticated inference models such as sequence to sequence models .
we leave the investigation such models to future work.
in sum this paper is a demonstration of the knowledge we could potentially mine from natural language artifacts such as reviews which knowledge is not fully taken advantage of in software engineering.
the area of natural language processing has advanced beyond simple text classification and topic modeling with the aid of deep learning techniques.
the prospects are great for further investigation of natural language techniques customized to software engineering settings.