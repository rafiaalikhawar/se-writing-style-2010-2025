collaboration challenges in building ml enabled systems communication documentation engineering and process nadia nahar nadian andrew.cmu.edu carnegie mellon university pittsburgh pa usashurui zhou university of toronto toronto ontario canada grace lewis carnegie mellon software engineering institute pittsburgh pa usachristian k stner carnegie mellon university pittsburgh pa usa abstract theintroductionofmachinelearning ml componentsinsoftware projects has created the need for software engineers to collaborate withdatascientists and otherspecialists.
while collaboration can always be challenging ml introduces additional challenges withitsexploratorymodeldevelopmentprocess additionalskills and knowledge needed difficulties testing ml systems need for continuousevolution and monitoring and non traditional quality requirements such as fairness and explainability.
through interviews with practitioners from organizations we identified key collaboration challenges that teams face when building and deploying ml systems into production.
we report on common collaboration points in the development of production ml systems forrequirements data andintegration aswellascorresponding teampatternsandchallenges.wefindthatmostofthesechallenges centeraroundcommunication documentation engineering and process and collect recommendations to address these challenges.
acm reference format nadianahar shuruizhou gracelewis andchristiank stner.
.collaborationchallengesinbuildingml enabledsystems communication documentation engineering and process.
in 44thinternational conference on software engineering icse may pittsburgh pa usa.
acm newyork ny usa 13pages.
introduction machine learning ml is receiving massive attention and funding inresearchandpractice itisachievingincredibleadvances surpassing human level cognition in many applications but it is widely acknowledged that moving from a prototyped machine learnedmodel to a production system is very challenging.
for example venturebeat reported in that percent of ml projects fail and gartner claimed in that percent do not make it from prototype to production .
while traditional software projects are already complex failure prone and require a broadrange of expertise the introduction of machine learning raises this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
qqhu jurxsv 5hvsrqvlelolw dwd roode srlqw 6riwz qj dwd 6flhqwlvw qg xvhu qwhju 3urgxfw 0rgho 7hdp rypw folhqw e f3 d surgxfw slsholqh lqihuhqfh prgho prqlwru 0rgho 7hdp 3urgxfw 7hdp e3 d lqiudvwu 3urg uhtxluhphqwv qwhjudwlrq 3xeolf gdwd surgxfw slsholqh lqihuhqfhprgho qg xvhu 0rgho uht 7udlqlqj gdwd qwhju 2ujdql dwlrq 2ujdql dwlrq figure structure of two interviewed organizations further challenges requires additional expertise and introduces additionalcollaborationpoints.
technical aspects such as testing ml components misuse of ml libraries engineering challenges for developing mlcomponents andautomatinglearning and deployment processes for ml components havereceivedsignificantattentioninresearchrecently.however humanfactorsofcollaborationduringthedevelopmentofsoftware products supported by ml components ml enabled systems for short have receivedless attention includingthe needtoseparate and coordinate data science and software engineering work to negotiateanddocumentinterfacesandresponsibilities andtoplanthe system soperationandevolution.yet thosehumancollaboration challengesappeartobemajorhurdlesindevelopingml enabled systems.
in addition past work has mostly been model centric focusedonchallengesoflearning testing orservingmodels but rarely focuses on the entire system i.e.
the product with many non ml parts into whichthe model is embedded as a component whichrequirescoordinatingandintegratingworkfrommultiple experts or teams.
tobetterunderstandcollaborationchallengesandavenuestowardbetterpractices weconductedinterviewswith45participants ieee acm 44th international conference on software engineering icse icse may21 pittsburgh pa usa nadianahar shuruizhou gracelewis and christian k stner contributing to the development of ml enabled systems for production use i.e.
not pure data analytics early prototypes .
ourresearch question is what are the collaboration points and correspondingchallengesbetweendatascientistsandsoftwareengineers?
participants come from organizations from small startups to large big tech companies and havediverse roles in theseprojects includingdatascientists softwareengineers andmanagers.during our interviews we explored organizational structures e.g.
see figure interactions of project members with different technical backgrounds and where conflicts arise between teams.
while some organizations have adopted better collaboration practices than others many struggle setting up structures processes andtoolingforeffectivecollaborationamongteammembers with different backgrounds when developing ml enabled systems.
tothebestofourknowledge andconfirmedbythepractitioners we interviewed there islittle systematic orshared understanding ofcommoncollaborationchallengesandbestpracticesfordeveloping ml enabled systems and coordinating developers with very different backgrounds e.g.
data science vs. software engineering .
we find that smaller andnew to mlorganizations struggle more buthave limited advice to draw from for improvement.
threecollaborationpoints surfacedasparticularly challenging identifyinganddecomposingrequirements negotiatingtraining data qualityand quantity and integrating data science and software engineering work.
we found that organizational structure teamcomposition powerdynamics andresponsibilitiesdiffer substantially but also found common organizational patterns at specific collaboration points and challenges associated with them.
overall ourobservationssuggestfour themesthatwouldbenefit frommoreattentionwhenbuildingml enabledsystems groupinvest insupporting interdisciplinaryteams toworktogether including educationandavoidingsilos file textpaymoreattentiontocollaboration points and clearly document responsibilities and interfaces cogsconsider engineeringwork asakeycontribution totheproject and 5investmore into process and planning.
in summary we make the following contributions we identify three core collaboration points and associated collaboration challenges based on interviews with practitioners triangulated with a literature review we highlight the different ways in whichteamsorganize butalsoidentifyorganizationalpatternsthat associate with certain collaboration challenges and we identify recommendations to improve collaboration practices.
state of the art researchers and practitioners have discussed whether and how machine learning changes software engineering with the introduction of learned models as components in software systems e.g.
.tolaythefoundationforourinterview study and inform the questions we ask we first provide an overview of the related work and existing theories on collaboration in traditional software engineering and discuss how machine learningmaychangethis.
collaborationinsoftwareengineering.
mostsoftwareprojects exceed the capacity of a single developer requiring multiple developers and teams to collaborate work together and coordinate aligngoals .collaborationhappens acrossteams ofteninamoreformal and structured form and withinteams where familiarity withotherteammembersandfrequentco locationfostersinformal communication .atatechnicallevel toallowmultipledevelopers to work together abstraction and adivide and conquer strategy are essential.
dividing software into components modules functions subsystems andhidinginternalsbehind interfaces isakey principle of modular software development that allows teams to divide work and work mostly independently until the final system is integrated .
teamswithinanorganizationtendtoalignwiththetechnical structure of the system with individuals or teams assigned to components hencethe technicalstructure interfacesand dependenciesbetweencomponents influencesthepointswhereteams collaborate and coordinate.
coordination challenges are especially observedwhenteamscannoteasilyandinformallycommunicate often studied in the context of distributed teams of global corporations andopen source ecosystems .
more broadly interdisciplinary collaboration often poses challenges.
it has been shown that when team members differ in their academicandprofessionalbackgroundsandpossessdifferentexpectations on the same system communication cultural and methodicalchallengesoftenemergewhenworkingtogether .key insights are that successful interdisciplinary collaboration depends on professional role structural characteristics personal characteristics and a history of collaboration specifically structural factors such as unclear mission insufficient time excessive workload and lackof administrative support are barriers to collaboration .
thecomponent interfaceplaysakeyroleincollaborationasa negotiation and collaboration point.
it is where teams re negotiate how to divide work and assign responsibilities .
team members oftenseek informationthat maynot becaptured ininterface descriptions asinterfacesarerarelyfullyspecified .inanidealizeddevelopmentprocess interfacesaredefinedearlybasedon what is assumed to remain stable because changes to interfaceslaterareexpensiveandrequiretheinvolvementofmultiple teams.inaddition interfacesreflect keyarchitecturaldecisions for thesystem aimed to achieve desired overall qualities .
inpracticethough theidealizeddivide and conquerapproach followingtop downplanningdoesnotalwaysworkwithoutfriction.
notallchangescanbeanticipated leadingtolatermodifications andrenegotiationofinterfaces .itmaynotbepossibleto identify how to decompose work and design stable interfaces until substantial experimentation has been performed .
to manage negotiate andcommunicatechangesofinterfaces developershave adopted a wide range of strategies for communication often relying on informal broadcast mechanisms to share planned or performed changes with other teams.
softwarelifecyclemodels alsoaddressthistensionofwhen and how to design stable interfaces traditional top down models e.g.
waterfall plan software design after careful requirements analysis the spiralmodel pursuesarisk firstapproachinwhichdevelopersiteratetoprototyperiskyparts whichtheninformsfuture systemdesigniterations agileapproachesde emphasizeupfront architectural design for fast iteration on incremental prototypes.
thesoftwarearchitecturecommunityhasalsograppledwiththe questionofhowmuchupfrontarchitecturaldesignisfeasible practical ordesirable showingatensionbetweenthedesire 414collaborationchallengesin building ml enabled systems communication documentation engineering and process icse may pittsburgh pa usa for upfront planning on one side and technical risks and unstable requirements on the other.
in this context our research explores howintroducing machinelearning intosoftwareprojects challenges collaboration.
softwareengineeringwithmlcomponents.
inaml enabled system machine learning contributes one or multiple components toalargersystemwithtraditionalnon mlcomponents.wereferto thewholesystemthatanenduserwoulduseasthe product.insome systems thelearned modelmaybearelativelysmallandisolated additiontoalargetraditionalsoftwaresystem e.g.
auditpredictionintaxsoftware inothersitmayprovidethesystem sessentialcore withonlyminimalnon mlcodearoundit e.g.
asalesprediction systemsendingdailypredictionsbyemail .inadditiontomodels an ml enabled system typically also has components for training and monitoring the model s .
much attention in practice recentlyfocusesonbuildingrobustml pipelines fortrainingand deploying models in a scalable fashion often under names suchas aiengineering sysml and mlops .inthis work we focus more broadly on the development of the entire ml enabled system including both ml and non ml components.
compared to traditional software systems ml enabled systems require additional expertise in data science to build the models and mayplaceadditionalemphasisonexpertisesuchasdatamanagement safety andethics .inthispaper weprimarilyfocuson therolesof softwareengineers anddatascientists whotypicallyhave different skills and educational backgrounds data science education tends to focus more on statistics ml algorithms andpracticaltrainingofmodelsfromdata typicallygivenafixeddataset notdeployingthemodel not buildingasystem whereas softwareengineeringeducationfocusesonengineeringtradeoffs with competing qualities limited information limited budget and the construction and deployment of systems.
research shows that software engineers who engage in data science without further educationareoftennaivewhenbuildingmodels andthatdata scientistsprefertofocusnarrowlyonmodelingtasks butare frequentlyfacedwithengineeringwork .whilethereisplenty ofworkonsupportingcollaborationamongsoftwareengineers and more recently on supporting collaboration among datascientists wearenotawareofworkexploringcollaborationchallengesbetween these roles which we do in this work.
the software engineering community has recently started to exploresoftware engineering for ml enabled systems as a research field withmanycontributionsonbringingsoftware engineering techniques to ml tasks such as testing models and ml algorithms deployingmodels robustnessand fairnessofmodels lifecycles formlmodels andengineeringchallengesorbestpracticesfordeveloping ml components .
a smaller body of workfocusesontheml enabledsystembeyondthemodel such as exploring system level quality attributes requirements engineering architecturaldesign safetymechanisms and user interaction design .in this paper we adopt this system wide scope and explore how data scientists andsoftware engineers work together to build the system with ml and non mlcomponents.
research design because there is limited research on collaboration in building mlenabledsystems weadoptaqualitativeresearchstrategytoexplore collaborationpoints andcorresponding challenges primarilywith stakeholderinterviews.weproceededinfoursteps weprepared interviewsbasedonaninitialliteraturereview weconducted interviews we triangulated results with literature findings and wevalidatedourfindingswiththeinterviewparticipants.we baseourresearchdesignon straussiangroundedtheory which derives research questions from literature analyzes interviews with open and axial coding and consults literature throughout the process.
in particular we conduct interviews and literature analysis in parallel with immediate and continuous data analysis performingconstantcomparisons andrefiningourcodebookand interview questions throughout the study.
step1 scopingandinterviewguide.
toscopeourresearchand prepare for interviews we looked for collaboration problems mentionedinexistingliteratureonsoftwareengineeringforml enabled systems sec.
.
in this phase we selected papers opportunistically through keyword search and our own knowledge of the field.
we marked all sections in those papers that potentially relate to collaboration challenges between team members with differentskillsoreducationalbackgrounds followingastandardopen codingprocess .eventhoughmostpapersdidnottalkabout problems in terms of collaboration we marked discussions thatmay plausibly relate to collaboration such as data quality issues between teams.
we then analyzed and condensed these codes into nineinitialcollaborationareasanddevelopedaninitialcodebook andinterview guide provided in appendix of arxiv version .
step interviews.
we conducted semi structured interviews with45participantsfrom28organizations each30to60minutes long.allparticipantsareinvolvedinprofessionalsoftwareprojects using machine learning that are either already or planned to be deployedinproduction.intable1 weshowthedemographicsof theinterviewparticipantsandtheirorganizations.detailscanbe foundin the appendix of our arxiv version .
wetriedtosampleparticipantspurposefully maximumvariation sampling to cover participants in different roles types of companies andcountries.weintentionallyrecruitedmostparticipants from organizations outside of big tech companies as they representthevastmajorityofprojectsthathaverecentlyadopted machine learning and oftenface substantially different challenges .
where possible we tried to separately interview multiple participantsindifferentroleswithinthesameorganizationtoget different perspectives.
we identified potential participants through personalnetworks ml relatednetworkingevents linkedin and recommendationsfrompreviousintervieweesandlocaltechleaders.
we adapted our recruitment strategy throughout the research based on our findings at later stages focusing primarily on specific roles and organizations to fill gaps in our understanding until reachingsaturation.forconfidentiality werefertoorganizations bynumberand toparticipantsbypxy wherexreferstothe organization number and ydistinguishes participants from the same organization.
415icse may21 pittsburgh pa usa nadianahar shuruizhou gracelewis and christian k stner table participant and company demographics type break down participantrole ml focused se focused management operations domainexpert other participantseniority years of experience or more years under years companytype bigtech nonit mid sizetech startup consulting companylocation north america southamerica europe asia africa we transcribed and analyzed all interviews.
then to map challenges to collaboration points we created visualizations of organizationalstructureandresponsibilitiesineachorganization we showtwoexamplesinfigure1 andmappedcollaborationproblems mentionedintheinterviewstocollaborationpointswithinthese visualizations.
we used these visualizations to further organize our data in particular we explored whether collaboration problems associatewithcertaintypes of organizational structures.
step triangulation with literature.
as we gained insights from interviews we returned to the literature to identify related discussions and possible solutions even if not originally framed in termsofcollaboration totriangulateourinterviewresults.relevant literature spans multiple research communities and publication venues including machine learning human computer interaction software engineering systems and various application domains e.g.
healthcare finance and does not always include obvious keywords simply searching for machine learning research yields a far too wide net.
hence we decided against a systematic literature review and pursued a best effort approach that relied on keyword search for topics surfaced in the interviews as well as backward andforwardsnowballing.outofover300papersread weidentified as possibly relevant and coded them with the same evolving codebook.thecompletelistcanbefoundinourarxivversion .
step4 validity check with interviewees.
for checking fit and applicability as definedby corbin and strauss and validating our findings we went back to the interviewees after creating a full draft of this paper.
we presented the interviewees both a summary and the full draft including the supplementary material along withquestionspromptingthemtolookforcorrectnessandareas of agreement or disagreement i.e.
fit and any insights gained fromreadingaboutexperiencesoftheothercompanies roles or findings as a whole i.e.
applicability .
ten interviewees responded withcommentsandallindicatedgeneralagreement someexplicitly reaffirmedsomefindings.
weincorporatedtwominorsuggested changesabout details of two organizations.
threatstovalidity andcredibility.
ourworkexhibitsthetypical threats common and expected for this kind of qualitative re search.
generalizations beyond the sampled participant distribu tion should be made with care for example we interviewed fewmanagers no dedicated data experts and no clients.
in several organizations we were only able to interview a single person giving us a one sided perspective.
observations may be different inorganizationsinspecificdomainsorgeographicregionsnotwell represented in our data.
self selection of participants may influenceresults forexampledevelopersingovernment relatedprojects more frequentlydeclined interviewrequests.
asdescribed earlier we followed standard practices for coding and memoing but as usual in qualitative research we cannot entirely exclude biases introduced by us researchers.
diversity of org.
structures throughout our interviews we found that the number and type of teams that participate in ml enabled system development dif fers widely as do their composition and responsibilities powerdynamics and the formality of their collaborations in line with findingsbyahoetal.
.toillustratethesedifferences weprovide simplified descriptions of teams found in two organizations in figure1.weshowteamsandtheirmembers aswellastheartifactsfor whichtheyare responsible suchas whodevelopsthe model who builds a repeatable pipeline who operates the model inference who is responsible for or owns the data and who is responsible for the final product.
a team often has multiple responsibilities and interfaceswithotherteamsatmultiplecollaborationpoints.where unambiguous we refer to teams by their primary responsibility as product team ormodel team.
organization3 figure1 top developsanml enabledsystemfor agovernmentclient.theproduct healthdomain includinganml modeland multiplenon ml components is developedbya single person team.the team focuseson traininga model first before buildingaproductaroundit.softwareengineeringanddatascience tasks are distributed within the team where members cluster into groupswithdifferentresponsibilitiesandroughlyequalnegotiation power.
a single data scientist is part of this team though they feelsomewhatisolated.dataissourcedfrompublicsources.the relationshipbetweentheclientanddevelopmentteamissomewhat distant and formal.
the product is delivered as a service but the teamonly receives feedback when things go wrong.
organization7 figure1 bottom developsaproductforin house use qualitycontrolforaproductionprocess .asmallteamisdevelopingandusingtheproduct butmodeldevelopmentisdelegated to an external team different company composed of four data scientists of which two have some software engineering background.
theproductteaminteractswiththemodelteamtodefineandrevise modelrequirementsbasedonproductrequirements.theproduct teamprovidesconfidentialproprietarydatafortraining.themodel team deploys the model and provides a ready to use inference api totheproductteam.therelationshipbetweentheteamscrosses company boundaries and is rather distant and formal.
the product teamclearlyhas thepower in negotiations between the teams.
thesetwoorganizationsdifferedalongmanydimensions andwe foundnoclearglobalpatternswhenlookingacrossorganizations.
nonethelesspatternsdidemergewhenfocusingonthreespecific collaborationaspects as we will discuss in the next sections.
collaboration point requirements and planning in an idealized top down process one would first solicit product requirements and then plan and design the product by dividing work 416collaborationchallengesin building ml enabled systems communication documentation engineering and process icse may pittsburgh pa usa into components ml and non ml deriving each component s requirements specifications from the product requirements.
in this process collaborationisneededfor productteamneedstonegotiateproductrequirementswithclientsandotherstakeholders product team needs to plan and design product decomposition negotiating with component teams the requirements for individual components and product project manager needs to planand manage the work across teams in terms of budgeting effort estimation milestones and work assignments.
.
commondevelopment trajectories feworganizations ifany followanidealizedtop downprocess and it may not even be desirable as we will discuss later.
while we did notfindanyglobalpatternsfororganizationalstructures sec.
there are indeed distinct patterns relating to how organizations elicitrequirementsanddecomposetheirsystems.mostimportantly we see differences in terms of the orderin which teams identify product and model requirements model firsttrajectory 13ofthe28organizations focus on building the model first and build aproductaroundthemodellater.intheseorganizations product requirements are usually shaped by model capabilities after the initial model has been created rather than being defined upfront.
inorganizationswithseparatemodelandproductteams themodel teamtypically startsthe project and the product team joins later withlow negotiating power to build a product around the model.
product first trajectory in13organizations models are built later to support an existing product.
in these cases a product often already exists and product requirementsarecollectedforhowtoextendtheproductwithnew ml supportedfunctionality.here the modelrequirementsarederived from the product requirements and often include constraints on model qualities such as latency memory and explainability.
parallel trajectory two organizations follow no clear temporalorder model and product teams work in parallel.
.
product and model requirements wefoundaconstanttensionbetweenproductandmodelrequirements in our interviews.
functional and nonfunctional product requirements set expectations for the entire product.
model requirementssetgoalsandconstraintsforthemodelteam suchas expected accuracy and latency target domain and available data.
product requirements require input from the model team group 5 .acommonthemeintheinterviewsisthatitisdifficultto elicitproductrequirementswithoutagoodunderstandingofmlcapabilities which almost always requires involving the model team and performing some initial modeling when eliciting product requirements.
regardless of whether product requirements or model requirementsareelicitedfirst datascientistsoftenmentionedbeing faced with unrealisticexpectations about model capabilities.
participants that interact with clients to negotiate product requirements which may involve members of the model team indicatethattheyneedtoeducateclientsaboutcapabilitiesofmltechniquesto setcorrectexpectations p3a p6a p6b p7b p9a p10a p15c p19b p22b p24a .thisneedtoeducatecustomersaboutmlcapabilitieshasalsobeenraisedintheliterature .for many organizations especially in product first trajectories the model team indicates similar challenges when interacting with theproductteam.iftheproductteamdoesnotinvolvethemodel teaminnegotiatingproductrequirements theproductteammay notidentifywhatdataisneededforbuildingthemodel andmay commit to unrealistic requirements.
for example p26a shared for thisproject wantedtoclaimthatwehaveno false positives and i was like that s not gonna work.
members of the model team often report lack of ml literacy in members of theproductteam andproject managers p1b p4a p7a p12a p26a p27a and a lack of involvement e.g.
p7b the decidedwhat typeof datawould makesense.
ihad nosay onthat.
.
usually the product team cannot identify product requirements alone instead product and model teams need to interact to explore whatis achievable.
in organizations with a model first trajectory members of the modelteamsometimesengagedirectlywithclients andalsoreport having to educate them about ml capabilities .
however when requirements elicitation is left to the model team members tend to focusonrequirementsrelevantforthemodel butneglectrequirements for the product such as expectations for usability e.g.
p3c s customers werekindofhappywiththeresults but weren thappy with the overall look and feel or how the system worked.
several researchpaperssimilarlyidentifiedhowthegoalsofdatascientistsdivergefromproductgoalsifproductrequirementsarenotobviousat modeling time leading to inefficient development worse products or constant renegotiation of requirements especially .
modeldevelopmentwithunclearmodelrequirementsiscommon file text .
participantsfrom model teams frequently explain how they are expected to work independently but are given sparse model requirements.
they try to infer intentions behind them but are constrained by having limited understanding of the product that the model will eventually support p3a p3b p16b p17b p19a .
model teams often start with vague goals and model requirements evolve over time as product teams or clients refine their expec tations in response to provided models p3b p7a p9a p5b p19b p21a .especiallyinorganizationsfollowingthe model firsttrajectory model teams may receive some data and a goal to predictsomething with high accuracy but no further context e.g.
p3a shared thereisn talwaysanactualspecofexactlywhatdatathey have whatdatatheythinkthey regoingtohaveandwhattheywant themodeltodo.
severalpaperssimilarlyreportprojectsstarting withvaguemodel goals .
eveninorganizationsfollowinga product firsttrajectory product requirementsareoftennottranslatedintoclearmodelrequirements.
for example participant p17b reports how the model team wasnot clear about the model s intended target domain thus could notdecidewhatdatawasconsideredinscope.asaconsequence modelteamsusuallycannotfocusjustontheircomponent buthave to understand the entire product to identify model requirementsin the context of the product p3a p10a p13a p17a p17b p19b p20b p23a requiringinteractionswiththeproductteamoreven bypassingtheproductteamtotalkdirectlytoclients.thedifficulty of providing clear requirements for an ml model has also beenraised in the literature partially arguing thatuncertaintymakesitdifficulttospecifymodelrequirements 417icse may21 pittsburgh pa usa nadianahar shuruizhou gracelewis and christian k stner upfront .ashemoreetal.reportmappingproduct requirements to model requirements as an open challenge .
providedmodelrequirementsrarelygobeyondaccuracyand data security cogs file text .requirements given to model teams primarilyrelatetosomenotionofaccuracy.beyondaccuracy require mentsfordatasecurityandprivacyarecommon typicallyimposed by the data owner or by legal requirements p5a p7a p9a p13a p14a p18a p20a b p21a b p22a p23a p24a p25a p26a .literature also frequently discusses how privacy requirements impact and restrict ml work .
we rarely heard of any qualities other than accuracy.
some participantsreportthatignoringqualitiessuchaslatencyorscalabilityhasresultedinintegrationandoperationproblems p3c p11a .ina fewcasesrequirementsforinferencelatencywereprovided p1a p6a p14a andinonecasehardwareresourcesprovidedconstraints on memory usage p14a but no other qualities such as traininglatency modelsize fairness orexplainabilitywererequiredthat couldbe important for product integration and deployment.
whenprompted veryfew ofourinterviewees reportconsiderations for fairness either at the product or the model level.
only two participants from model teams p14a p22a reported receiving fairnessrequirements whereasmanyothersexplicitlymentioned thatfairnessisnotaconcernforthemyet p4a p5b p6b p11a p15c p20a p21b p25a p26a .thelackoffairnessandexplainabilityrequirements is in stark contrast to the emphasis that these qualities receive in the literature .
recommendations.
ourobservationssuggestthatinvolvingdata scientists early when soliciting product requirements is important group and that pursuing a model first trajectory entirely without consideringproduct requirements is problematic 5 .
conversely model requirements are rarely specific enough to allow data scientiststoworkin isolationwithoutknowingthebroadercontext of the system and interaction with the product team should ideally be planned as part of the process.
requirements form a key collaboration point between product and model teams which should be emphasized even in more distant collaboration styles e.g.
outsourcedmodeldevelopment .thefeworganizationsthatusethe paralleltrajectory reportfewerproblemsbyinvolvingdatascientistsinnegotiatingproductrequirementstodiscardunrealisticonesearly on p6b .
vogelsang and borg also provide similar recommendationstoconsultdatascientistsfromthebeginning tohelpelicit requirements .
while many papers place emphasis on clearly definingml use cases and scope several others mention how collaboration of technical and non technical stakeholders suchas domain experts helps .
mlliteracyforcustomersandproductteamsappearstobeimportant group .
p22a and p19a suggested conducting technical ml trainingsessionstoeducateclients similartrainingisalsouseful for members of product teams.
several papers argue for similar trainingfornon technicalusersof ml products .
most organizations elicit requirements only rather informally and rarely have good documentation especially when it comesto model requirements.
it seems beneficial to adopt more formal requirementsdocumentationforproductandmodel file text asseveral participants reported that it fosters shared understanding at this collaborationpoint p11a p13a p19b p22a p22c p24a p25a p26a .checklists could help to cover a broader range of model quality requirements such as training latency fairness and explainability.
formalismssuchasmodelcards andfactsheets couldbe used as a starting point for documenting model requirements.
.
project planning ml uncertainty makes effort estimation difficult group .
irrespective of trajectory participants p3a p4a p7a b p8a p14b p15b c p16a p17a p18a p19a b p20a p22a c p23a p25a mentioned that the uncertainty associated with ml components makes it difficult to estimate the timeline for developing an ml component and by extension the product.
model development is typically seen as a science like activity where iterative experimentation and exploration is needed to identify whether and how a problem can be solved rather than as an engineering activity that follows a somewhat predictable process.
this science like nature makes itdifficultfor themodel team to set expectations or contracts with clientsortheproductteamregardingeffort cost oraccuracy.while data scientists find effort estimation difficult lack of ml literacy in managers makes it worse p15b p16a p19b p20a p22b .
teams report deploying subpar models when running out of time p3a p15b p19a orpostponingorevencancelingdeployments p25a .
these findings align with literature mentioning difficulties associatedwitheffortestimationformltasks andplanning projectsinastructuredmannerwithdiversemethodologies with diverse trajectories and without practical guidance .
generally participantsfrequentlyreportthatsynchronization betweenteamsischallengingbecauseofdifferentteampace different development processes and tangled responsibilities p2a p11a p12a p14 b p15b c p19a see also sec.
.
.
recommendations.
participants suggested several mitigation strategies keepingextrabuffertimesandaddingadditionaltimeboxesforr dininitialphases p8a p19a p22b c p23a 5 continuouslyinvolvingclientsineveryphasesothattheycanunderstand the progression of the project and be aware of potential missed deadlines p6b p7a p22a p23a group .fromtheinterviews wealso observe the benefits of managers who understand both software engineeringandmachinelearningandcanalignproductandmodel teamstoward common goals p2a p6a p8a p28a group .
collaboration point training data data is essential for machine learning but disagreements and frustrations around training data were the most common collaboration challenges mentioned in our interviews.
in most organizations theteam that is responsible for building the model is not the team that collects owns and understands the data making data a key collaboration point between teams in ml enabled systems development.
.
commonorganizationalstructures weobservedthreepatternsarounddatathatinfluencecollaboration challengesfrom the perspective of the model team provided data the product team hastheresponsibilityofprovidingdata tothemodelteam org.
.theproductteamistheinitialpoint 418collaborationchallengesin building ml enabled systems communication documentation engineering and process icse may pittsburgh pa usa of contactfor all data relatedquestions from themodel team.
the product teammay ownthe dataor acquireit froma separatedata team internalorexternal .coordinationregardingdatatendsto be distant and formal and the product team tends to hold more negotiationpower.
external data the product team doesnothavedirectresponsibilityfor providingdata butinstead themodel teamreliesonexternaldataproviders.
commonly the model team i uses publicly available resources e.g.
academicdatasets org.
or ii hiresathirdpartyfor collectingorlabelingdata org.
.intheformercase the model team has little to no negotiation power over data in the latter it can set expectations.
in housedata product model and data teams are all part of the same organization and the model team relies on internal data from that organization org.
.
in these cases both product and modelteamsoftenfinditchallengingtonegotiateaccesstointernal dataduetodifferingpriorities internalpolitics permissions and security constraints.
.
negotiatingdata qualityandquantity disagreementsandfrustrationsaroundtrainingdatawerethemost common collaboration challengesin our interviews.
inalmost every project data scientists were unsatisfied with the quality andquantity of data they received at this collaboration point in line with a recent survey showing data availability and management to be the top ranked challenge in building ml enabled systems .
provided and public data is often inadequate file text group .in organizations where data is provided by the product team the model team commonly statesthatit is difficult to getsufficient data p7a p8a p13a p22a p22c .
the data that they receive is often of low quality requiringsignificantinvestmentindatacleaning.similarto the requirements challenges discussed earlier they often state that the product team has little knowledge or intuition for the amount and quality of data needed.
for example participant p13a stated that they were given a spreadsheet with only rows to build a modelandp7areportedhavingtospendalotoftimeconvincing theproductteamoftheimportanceofdataquality.thisalignswith past observations that software engineers often have little appreciation for data quality concerns and that training datais often insufficient and incomplete .
when the model team uses public data sources its members also have little influence over data quality and quantity and report significant effort for cleaning low quality and noisy data p2a p3a p4a p3c p6b p19b p23a .
papers have similarly questioned the representativeness and trustworthiness of public training data as nobody gets paid to maintain such data .
training servingskew isacommonchallengewhentrainingdata is provided to the model team models show promising results but do not generalize to production data because it differs from providedtrainingdata p4a p8a p13a p15a p15c p21a p22c p23a .
our interviews show that this skew often originates from inadequate training data combinedwithunclearinformationaboutproductiondata andthereforeno chancetoevaluatewhetherthetrainingdataisrepresentativeof production data.
dataunderstandingandaccesstodomainexpertsisabottleneck file text 5 .existingdatadocumentation e.g dataitemdefinitions semantics schema isalmostneversufficientformodelteams to understand the data also mentioned in a prior study .
in the absence of clear documentation team members often collect informationandkeeptrackofunwrittendetailsintheirheads p5a knownasinstitutionalortribalknowledge .dataunderstandinganddebuggingofteninvolvemembersfromdifferentteamsand thuscausechallengesat this collaboration point.
model teams receiving data from the product team report strugglingwithdataunderstandingandhavingadifficulttimegetting helpfromtheproductteam orthedatateamthattheproductteam works with p8a p7b p13a .
as the model team does not have directcommunicationwiththedatateam dataunderstandingissuesoftencannotberesolvedeffectively.forexample p13areports ideally forusitwouldbesogoodtospendmaybeaweekortwowith one person continuously trying to understand the data.
it s one ofthe biggest problems actually because even if you have the person if you re not in contact all the time then you misinterpreted some thingsandyoubuildonit.
thelownegotiationpowerofthemodel teamin these organizations hinders access to domain experts.
modelteamsusingpublicdatasimilarlystrugglewithdataunderstanding and getting help p3a p4a p19a relying on sparse datadocumentation or trying to reach any experts on the data.
forin houseprojects inseveralorganizationsthemodelteam reliesondatainshareddatabases org.
collectedby instrumentinga productionsystem butsharedby multipleteams.
several teams shared problems with evolving and often poorly documenteddatasources asparticipantp5aillustrates can have features features.
and no one really cares.
theyjustdumpfeaturesthere.
ijustcannottrack10 000features.
modelteamsfacechallengesinunderstandingdataandidentifyingateamthatcanhelp p5a p25a p20b p27a aproblemalsoreported in a prior study about data scientists at microsoft .
challengesinunderstandingdataandneedingdomainexperts are also frequently mentioned in the literature as is the danger of building models with insufficient understanding of the data .
although we are not aware of literature discussing the challenges of accessing domain experts papers have shown that even when data scientists have access effective knowledge transfer is challenging .
ambiguitywhenhiringadatateam file text .whenthemodelteam hiresanexternaldatateamforcollectingorlabellingdata org.
the model team has much more negotiation power over setting data quality and quantity expectations though kim et al.
report that model teams may have difficulty gettingbuy in from the product team for hiring a data team in the first place .
our interviews did not surface the same frustrations as withprovideddataandpublicdata butinsteadparticipantsfrom these organizations reported communication vagueness andhidden assumptions askeychallengesatthiscollaborationpoint p9a p15a p15c p16a p17b p22a p22c p23a .
forexample p9arelated how 419icse may21 pittsburgh pa usa nadianahar shuruizhou gracelewis and christian k stner differentlabellingcompaniesgiventhesamespecificationwidely disagreed on labels when the specification was not clear enough.
wefoundthatexpectationsbetweenmodelanddatateamsare often communicated verbally without clear documentation.
as a result the data team often does not have sufficient context to understand what data is needed.
for example participant p17b states data collectors can t understand the data requirements all the time.
because when a questionnaire is designed the overview of the project is not always described to them.
evenif we describe it they can t always catch it.
reports about low quality data from hired data teams have been also discussed in the literature .
need to handle evolving data cogs group .in most projects models need to be regularly retrained with more data or adapted to changes in the environment e.g.
data drift which is a challengefor manymodel teams p3a p3c p5a p7a b p11a p15c p18a p19b p22a .whenproductteamsprovidethedata theyoften haveastaticviewandprovideonlyasinglesnapshotofdatarather than preparing for updates and model teams with their limited negotiationpowerhaveadifficulttimefosteringamoredynamic mindset p7a b p15c p18a p22a asexpressedbyparticipantp15c people don t understand that for a machine learning project data has to be provided constantly.
it can be challenging for a model teamto convincethe productteam toinvest incontinuousmodel maintenanceand evolution p7a p15c .
conversely if data is provided continuously most commonly withpublicdatasources in housesources andowndata teams model teams struggle with ensuring consistency over time.
data sources can suddenly change without announcement e.g.
changes toschema distributions semantics surprisingmodelteamsthat makebutdonotcheckassumptionsaboutthedata p3a p3c p19b .
forexample participantsp5aandp11areportsimilarchallenges with in house data where their low negotiation power does not allow them to set quality expectations but they face undesired and unannounced changes in data sources made by other teams.
most organizations do not have a monitoring infrastructure to detect changesin data quality or quantity as we will discuss in sec.
.
.
in houseprioritiesandsecurityconcernsoftenobstructdata access 5 .
in in house projects we frequently heard about the productormodelteamstrugglingtoworkwithanotherteamwithin the same organization that owns the data.
often these in house projects are local initiatives e.g.
logistics optimization with more or less buy in from management and without buy in from other teamsthathavetheirownpriorities sometimesotherteamsexplicitly question the business value of the product.
the interviewed model teams usually have little negotiation power to request data especiallyifitinvolvescollectingadditionaldata andalmostnever get an agreement to continuously receive data in a certain format quality orquantity p5a p10a p11a p20a b p27a alsoobservedin studies at microsoft ing and other organizations .
for example p10a shared we wanted to ask the data warehouse team to and it was really hard to get resources.
they wouldn t do that because it was hard to measure the impact our in house project hadonthebottomlineofthebusiness.
modelteamsinthese settingstend to work with whatever data they can get eventually.security and privacy concerns can also limit access to data p7a p7b p21a b p22a p24a especially when data is owned by a team in a different organization causing frustration lengthynegotiations and sometimesexpensive data handling restrictions e.g.
no use of cloud resources for model teams.
recommendations.
data quality and quantity is important to model teams yet they often find themselves in a position of low negotiationpower leadingtofrustrationandcollaborationinefficiencies.modelteamsthathavethefreedomtosetexpectationsand hiretheirowndatateamsarenoticeablymoresatisfied.whenplanningtheentireproduct itseemsimportanttopayspecialattention tothiscollaborationpoint andbudgetfordatacollection accessto domain experts or even a dedicated data team 5 .
explicitly planningtoprovidesubstantialaccesstodomainexpertsearlyin theproject was suggested as important p25a .
we found it surprising that despite the importance of this collaborationpointthereislittlewrittenagreementonexpectations and often limited documentation file text even when hiring a dedicateddatateam instarkcontrasttomoreestablishedcontractsfor traditionalsoftwarecomponents.notallorganizationsallowthe moreagile constantclosecollaborationbetweenmodelanddata teams that some suggest .
with a more formal or distant relationship e.g.
across organizations teams without buy in itseems beneficial to adopt a more formal contract specifying data quantity and quality expectations which are well researched inthe database literature and have been repeatedly discussed inthe contextof ml enabledsystems .this has also been framed as data requirements in the software engineering literature .whenworkingwithadedicateddatateam participants suggested to invest in making expectations very clear forexample byprovidingprecisespecificationsandguidelines p9a p6b p28a running training sessions for the data collectors and annotators p17b p22c andmeasuringinter rateragreement p6b .
automatedchecksarealsoimportantasdataevolves cogs .for example participantp13amentionedproactivelysettingupdata monitoringtodetectproblems e.g.
schemaviolations distribution shifts atthiscollaborationpoint apracticesuggestedalsointhe literature andsupportedbyrecenttooling .
the risks regarding possible unnoticed changes to data make it important to consider data validation andmonitoring infrastructure asakeyfeatureoftheproductearlyon cogs 5 as also emphasized by several participants p5a p25a p26a p28a .
collaboration point product model integration as discussed earlier to build an ml enabled system both ml components and traditional non ml components need to be integrated anddeployed requiringdatascientistsandsoftwareengineersto worktogether typicallyacrossmultipleteams.wefoundmanyconflicts at this collaboration point stemming from unclear processes and responsibilities as well as differing practices and expectations.
.
commonorganizationalstructures we saw large differences among organizations in how engineering responsibilitieswereassigned mostvisibleinhowresponsibility formodel deployment and operation is assigned which typically 420collaborationchallengesin building ml enabled systems communication documentation engineering and process icse may pittsburgh pa usa involves significant engineering effort for building reproducible pipelines api design or cloud deployment often with mlops technologies.
we found the following patterns shared model code in some organizations themodelteamisresponsible only for model develop ment and delivers training code e.g.
in a notebook or modelfiles to the product team the product team takes responsibility for deployment and operation of the model possibly rewriting the training code as a pipeline.
here the model team has little or no engineeringresponsibilities.
modelasapi inmostorganizations out of the model teamisresponsiblefordeveloping anddeployingthemodel.hence themodel teamrequires substantialengineering skillsin addition to data science expertise.
here some model teams are mostly composedofdatascientistswith littleengineeringcapabilities org.
someconsistmostlyofsoftwareengineerswhohave picked up some data science knowledge org.
and others have mixed team members org.
.
thesemodelteamstypicallyprovideanapitotheproductteam orreleaseindividualmodelpredictions e.g.
sharedfiles email org.
or install models directly on servers org.
.
all in one if only few people work on model and product sometimes a single team or even a singleperson sharesallresponsibilities org.
.itcanbeasmallteamwithonlydatascientists org.
ormixedteamswithdatascientistsandsoftware engineers org.
.
wealsoobservedtwooutliers onestartup org.
hadadistinct model deployment team allowing the model team to focus on data science without much engineering responsibility.
in one large organization org.
an engineering focused model team model as api was supported by a dedicated research team focused on data scienceresearch with fewer engineering responsibilities.
.
responsibilityandculture clashes interdisciplinary collaboration is challenging cf.
sec.
.
we observed many conflicts between data science and software engineer ingculture madeworsebyunclearresponsibilitiesandboundaries.
team responsibilities often do not match capabilities and preferences cogs .
when the model team has responsibilities requiringsubstantialengineeringwork weobservedsomedissatisfaction when its members were assigned undesired responsibilities.
data scientists preferred engineering support rather than needing to do everything themselves p7a b 13a but can find it hard toconvince management to hire engineers p10a p20a p20b .
for examplep10adescribes iwasstrugglingtochangethemindsetof the team lead convincing him to hire an engineer...i just didn t want this to be my main responsibility.
especially in small teams data scientists report struggling withthe complexity of the typical ml infrastructure p7b p9a p14a p26a p28a .incontrast whendeploymentistheresponsibilityofsoftware engineersintheproductteamorofdedicatedengineersin all inoneteams some of those engineers report problems integrating themodelsduetoinsufficientknowledgeonmodelcontextordomain and the model code not being packaged well for deployment p20b p23a p27a .
in several organizations we heard about softwareengineersperformingmltaskswithouthavingenoughml understanding p5a p15b c p16b 18b 19b 20b .mirroringobservations from past research p5a reports there are people who are ml engineers at but they don t really understand ml.theywereactuallysoftwareengineers...theydon tunderstand .
they just copy paste code.
siloing data scientists fosters integration problems group 5 .
weobserveddatascientistsoftenworkinginisolation knownas siloing inalltypesoforganizationalstructures evenwithinsingle small teams see sec.
and within engineering focused teams.
insuch settings datascientists oftenworkin isolationwithweak requirements cf.sec.
.
withoutunderstandingthelargercontext seriouslyengagingwithothersonlyduringintegration p3a p3c p6a p7b p11a p13a p15b p25a whereproblemsmaysurface.
forexample participantp11areportedaproblemwhereproduct and model teams had different assumptions about the expected inputs and the issue could only be identified after a lot of back and forthbetween teams at a late stage in the project.
technicaljargonchallengescommunication group .
participants frequentlydescribedcommunicationissuesarisingfromdiffering terminologyusedbymembersfromdifferentbackgrounds p1a b p2a p3a p5b p8a p12a p14a b p16a p17a b p18a b p20a p22b p23a leadingtoambiguity misunderstandings andinconsistent assumptions on top of communication challenges with domain experts .p1breports therearealotofconversations inwhichdisambiguationbecomesnecessary.weoftenusedifferent kindsofwordsthatmightbeambiguous.
forexample datascientists may refer to prediction accuracy as performance a term many software engineers associate with response time.
these challenges can be observed more frequently between teams but they even occur within a team with members from different backgrounds p3a c p20a .
code quality documentation and versioning expectationsdiffer widely and cause conflicts group cogs .
many participants reported conflicts around development practices between data scientists and software engineers during integration and deployment.
participants report poor practices that may also be observed in traditionalsoftwareprojects butparticularlysoftwareengineers expressedfrustrationininterviewsthatdatascientistsdonotfollowthesamedevelopmentpracticesorhavethesamequalitystandards whenitcomestowritingcode.reportedproblemsrelatetopoor code quality p1b p2a p3b p5a p6a b p10a p11a p14a p15b c p17a p18a p19a p20a b p26a insufficient documentation p5a b p6a b p10a p15c p26a and notextendingversioncontroltodataandmodels p3c p7a p10a p14a p20b .
in two shared model code organizations participants report having to rewrite code from the data scientists p2a p6a b .
missingdocumentation for ml code and models is considered the causefordifferentassumptionsthatleadtoincompatibilitybetween ml and non ml components p10a and for losing knowledge and 421icse may21 pittsburgh pa usa nadianahar shuruizhou gracelewis and christian k stner eventhemodelwhenfacedwithturnover p6a b .recentpapers similarlyholdpoor documentation responsible for team decisions becoming invisible and inadvertently causing hidden assumptions .hopkinsandboothcalledmodelanddata versioning in small companies as desired but elusive .
recommendations.
many conflicts relate to boundaries of responsibility especially for engineering responsibilities and to different expectations by team members with different backgrounds.
better teamstend to define processes responsibilities and boundaries more carefully 5 document apis at collaboration points between teams file text and recruit dedicated engineeringsupport for model deployment cogs but also establish a team culture with mutual understanding and exchange group .
big tech companies usually have more established processes and clearer responsibility assignments than smaller organizations and startups that often follow ad hoc processes or figure out responsibilities as they go.
theneedforengineeringskillsformlprojectshasfrequently been discussed but our interviewees differ widely in whether all data scientists should have substantial engineering responsibilities or whether engineers should support datascientistssothattheycanfocusontheircoreexpertise cogs .
especially interviewees from big tech emphasized that they expect engineering skills from all data science hires p28a .
others emphasizedthatrecruitingsoftwareengineersandoperationsstaffwith basic data science knowledge can help at many communication and integration tasks such as converting experimental ml codefor deployment p2a p3b fostering communication p3c p25a and monitoring models in production p5b .
generally siloing data scientists is widely recognized as problematic and many interviewees suggest practices for improving communication group such as trainingsessionsforestablishingcommonterminology p11a p17a p22a p22c p23a weekly all hands meetings to present all tasks andsynchronize p2a p3c p6b p11a andproactivecommunicationtobroadcastupcomingchangesindataorinfrastructure p11a p14a p14b .
this mirrors suggestions to invest in interdisciplinary training andproactive communication .
.
qualityassurance for model and product during development and integration questions of responsibility forqualityassurancefrequently arise oftenrequiring coordinationandcollaborationbetweenmultipleteams.thisincludesevaluating components individually including the model as well as their integrationand thewholesystem oftenincludingevaluating and monitoringthe system online in production .
model adequacy goals are difficult to establish file text group .offline accuracy evaluation of models is almost always performed by themodelteamresponsibleforbuildingthemodel thoughoften theyhavedifficultydecidinglocallywhenthemodelisgoodenough p1a p3a p5a p6a p7a p15b p16b p23a .
as discussed in sec.
and sec.
model team members often receive little guidanceonmodeladequacycriteriaandareunsureabouttheactual distribution of production data.
they also voice concerns aboutestablishing ground truth for example needing to support data for different clients and hence not beingable to establish offline measuresformodelquality p1b p16b p18a p28a .asqualityrequirementsbeyondaccuracyarerarelyprovidedformodels modelteamsusuallydonotfeelresponsiblefortestinglatency memory consumption orfairness p2a p3c p4a p5a p6b p7a p14a p15b p20b .whereasliteraturediscussedchallengesinmeasuringbusinessimpactofamodel andbalancingbusinessgoals with model goals interviewed data scientists were concerned about this only with regards to convincing clients managers or product teams to provide resources p7a b p10a p26a p27a .
limited confidence without transparent model evaluation file text .participants in several organizations report that model teams donotprioritizemodelevaluationandhavenosystematicevaluationstrategy especiallyiftheydonothaveestablishedadequacy criteria they try to meet performing occasional ad hoc inspections instead p2a p15b p16b p18b p19b p20b p21b p22a p22b .
without transparency about their test processes and test results other teams voiced reduced confidence in the model leading to skepticismto adopt the model p7a p10a p21b p22a .
unclear responsibilities for system testing 5 .
teams often strugglewithtestingtheentireproductafterintegratingmland non mlcomponents.modelteamsfrequentlyexplicitlymentioned thattheyassumenoresponsibilityforproductquality including integration testing and testing in production and have not been involved in planning for system testing but that their responsibilities end with delivering a model evaluated for accuracy p3a p14a p15b p25a p26a .
however insev eralorganizations productteams also did not plan for testing the entire system with the model s and at most conducted system testing in an ad hoc way p2a p6a p16a p18a p22a .recentliteraturehasreportedasimilarlackof focus on system testing in product teams mirroring also a focus in academic research on testing models rather than testing the entire system .
interestingly some established software developmentorganizationsdelegatedtestingtoanexistingseparate qualityassuranceteamwithnoprocessorexperiencetestingml products p2a p8a p16a p18b p19a .planning for online testing and monitoring is rare 5 cogs group .
duetopossibletraining servingskewanddatadrift literature emphasizestheneed foronline evaluation .withcollectedtelemetry onecanusually approximate both product and model quality monitor updates and experiment in production .
online testing usually requires coordinationamongmultipleteamsresponsibleforproduct model andoperation.weobservedthatmostorganizationsdo notperform monitoringoronlinetesting asitisconsidereddifficult inaddition to lack of standard process automation or even test awareness p2a p3a p3b p4a p6b p7a p10a p15b p16b p18b p19b 25a p27a .only11 out of organizations collected any telemetry it is most established in big tech organizations.
when to retrain models isoftendecidedbasedonintuitionormanualinspection though many aspire to more automation p1a p3a p3c p5a p10a p22a p25a p27a .
responsibilities around online evaluation are often neitherplanned nor assigned upfront as part of the project.
most model teams are aware of possible data drift but many do nothaveanymonitoringinfrastructurefordetectingandmanaging drift in production.
if telemetry is collected it is the responsibility of the product or operations team and it is not always accessible to the model team.
four participants report that they rely on manual feedback about problems from the product team p1a p3a p4a 422collaborationchallengesin building ml enabled systems communication documentation engineering and process icse may pittsburgh pa usa p10a .
atthe sametime others reportthat productand operation teams do not necessarily have sufficient data science knowledge to provide meaningful feedback p3a p3b p5b p18b p22a .
recommendations.
qualityassuranceinvolvesmultipleteams andbenefitsfromexplicitplanningandmakingitahighpriority 5 .whiletheproductteamshouldlikelytakeresponsibilityfor productqualityandsystemtesting suchtestingofteninvolvesbuilding monitoring and experimentation infrastructure cogs which requiresplanningandcoordinationwithteamsresponsibleformodel development deployment andoperation ifseparate toidentify therightmeasures.modelteamsbenefitfromreceivingfeedback ontheirmodelfromproductionsystems butsuchsupportneeds to be planned explicitly with corresponding engineering effort assigned and budgeted even in organizations following a model first trajectory.wesuspectthateducationaboutbenefitsoftestingin production and common infrastructure often under the label devops mlops can increase buy in from all involved teams group .
organizations thathave establishedmonitoring and experimentationinfrastructure strongly endorse it p5a p25a p26a p28a .
definingclearqualityrequirementsformodelandproductcan helpall teamstofocus theirquality assuranceactivities cf.
sec.
file text .evenwhenitischallengingtodefineadequacycriteriaupfront teams can together develop a quality assurance plan for model and product.participants andliterature emphasizedtheimportance of human feedback to evaluate model predictions p11a p14a which requires planning to collect such feedback 5 .
system and usability testing may similarly require planning for user studies withprototypes and shadow deployment .
discussion and conclusions throughourinterviewsweidentifiedthreecentralcollaboration points where organizations building ml enabled systems face substantialchallenges requirementsandprojectplanning training data and product model integration.
other collaboration points surfaced but were mentioned far less frequently e.g.
interaction with legal experts and operators did not relate to problems betweenmultipledisciplines e.g.
datascientistsdocumentingtheir work for other data scientists or mirrored conventional collaboration in software projects e.g.
many interviewees wanted to talk about unstable ml libraries and challenges interacting with teams building and maintaining such libraries though the challenges largelymirrored those of library evolution generally .
data scientists and software engineers are certainly not the first to realize that interdisciplinary collaborations are challenging and fraught with communication and cultural problems yet it seemsthatmanyorganizationsbuildingml enabledsystemspay littleattentionto fostering better interdisciplinary collaboration.
organizationsdifferwidelyintheirstructuresandpractices and someorganizationshavefound strategies that work for them see recommendation sections .
yet we find that most organizations do notdeliberatelyplantheirstructuresandpracticesandhavelittle insight into available choices and their tradeoffs.
we hope that this work can encourage more deliberation about organization and processatkeycollaborationpoints and serveasastartingpoint forcatalogingand promoting best practices.beyond the specific challenges discussed throughout this paper we see four broad themes that benefit from more attention both in engineering practiceandin research groupcommunication manyissuesarerootedinmiscommunication between participants with different backgrounds.
to facilitate interdisciplinarycollaboration educationiskey includingmlliteracy for software engineers and managers and even customers but also training data scientists to understand software engineering concerns.
the idea of t shaped professionals deep expertise inonearea broadknowledgeofothers canprovideguidancefor hiringandtraining.
file textdocumentation clearlydocumentingexpectationsbetween teamsisimportant.traditionalinterfacedocumentationfamiliar to software engineers may be a starting point but practices for documenting model requirements sec.
.
data expectations sec.
.
and assured model qualities sec.
.
are not well established.
recent suggestions like model cards and factsheets a r e a good starting point for encouraging better more standardized documentation of ml components.
given the interdisciplinary nature at these collaboration points such documentation must be understoodbyallinvolved theoriesof boundaryobjects may helpto develop better interface description mechanisms.
cogsengineering with attention focused on ml innovations many organizations seem to underestimate the engineering ef fort required to turn a model into a product to be operated and maintainedreliably.arguablyadoptingmachinelearningincreases software complexity andmakes engineeringpractices suchasdataqualitychecks deploymentautomation andtestingin production even more important.
project managers should ensure that the ml and the non ml parts of the project have sufficient engineeringcapabilitiesandfosterproductandoperationsthinking from the start.
5process finally machinelearningwithitsmorescience like process challenges traditional software process life cycles.
it seems clear that product requirements cannot be established without involving data scientists for model prototyping and often it may be advisable to adopt a model first trajectory to reduce risk.
but while a focus on the product and overall process may cause delays neglectingitentirelyinvites thekind ofproblems reportedby our participants.
whether it may look more like the spiral model or agile more research into integrated process life cycles for mlenabled systems covering software engineering and data science isneeded.