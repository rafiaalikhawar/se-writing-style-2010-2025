diversity drives fairness ensemble of higher order mutants for intersectional fairness of machine learning software zhenpeng chen xinyue li jie m. zhang federica sarro yang liu nanyang technological university singapore peking university china king s college london united kingdom university college london united kingdom china singapore international joint research institute china zhenpeng.chen ntu.edu.sg xinyueli stu.pku.edu.cn jie.zhang kcl.ac.uk f.sarro ucl.ac.uk yangliu ntu.edu.sg abstract intersectional fairness is a critical requirement for machine learning ml software demanding fairness across subgroups defined by multiple protected attributes.
this paper introduces fairhome a novel ensemble approach using higher order mutation of inputs to enhance intersectional fairness of ml software during the inference phase.
inspired by social science theories highlighting the benefits of diversity fairhome generates mutants representing diverse subgroups for each input instance thus broadening the array of perspectives to foster a fairer decision making process.
unlike conventional ensemble methods that combine predictions made by different models fairhome combines predictions for the original input and its mutants all generated by the same ml model to reach a final decision.
notably fairhome is even applicable to deployed ml software as it bypasses the need for training new models.
we extensively evaluate fairhome against seven state of theart fairness improvement methods across decision making tasks using widely adopted metrics.
fairhome consistently outperforms existing methods across all metrics considered.
on average it enhances intersectional fairness by .
surpassing the currently best performing method by .
percentage points.
index terms machine learning intersectional fairness input mutation output ensemble i. i ntroduction machine learning ml software plays a critical role in high stakes human related decisions such as hiring criminal sentencing and loan application .
in this context fairness of ml software holds increasing significance as biases in these systems can perpetuate inequalities and harm historically marginalized groups .
unfair ml software can lead to ethical reputational financial harm and legal consequences if it violates anti discrimination laws .
indeed fairness has long been acknowledged as a foundational requirement for ml software aiming at mitigating bias and discrimination tied to protected attributes such as sex race and age.
from the software engineering se perspective unfairness in ml software can be considered as software fairness bugs .
this has led to a growing corresponding author jie m. zhang jie.zhang kcl.ac.uk .body of se studies aimed at improving ml software fairness also referred to as bias mitigation .
software users inherently belong to multiple intersecting identity groups defined by various protected attributes.
these intersections yield varied experiences of discrimination among different subgroups.
for instance black women might encounter biases stemming from both sexism and racism.
this emphasizes the importance of intersectional fairness which measures fairness among subgroups formed by the combination of multiple protected attributes .
notably intersectional fairness has been encoded in legal regulations and thus highlights the compelling need for software researchers and engineers to consider multiple protected attributes simultaneously .
compared to fairness concerning individual protected attributes which oversimplifies the complex realities faced by software users intersectional fairness is considered a more challenging task .
recent se studies have introduced advanced bias mitigation methods capable of dealing with multiple protected attributes and improving intersectional fairness.
notable examples include fairsmote which pre processes training data to mitigate bias across protected attributes and then trains a fairer model maat which trains distinct models to optimize fairness for each protected attribute individually and then combines them and fairmask which trains individual extrapolation models based on training data to modify protected attributes in inputs for fairer outcomes.
in this paper we introduce fairhome a novel ensemble approach using higher order mutation of inputs to improve intersectional fairness of ml software during the inference phase.
in the social science domain it is widely acknowledged that increasing diversity can foster fairness in decisionmaking .
drawing inspiration from this fairhome generates diverse mutated inputs from various subgroups for each given input by applying higher order mutation across multiple protected attributes thereby enriching the decisionmaking process with diverse perspectives.
this approacharxiv .08167v1 dec 2024is particularly effective in addressing intersectional fairness which inherently involves numerous subgroups thus ensuring diversity within the generated input set.
then fairhome combines the predictions generated by the ml software for the original input and its mutants to reach the final decision.
unlike conventional ensemble methods including stateof the art ones in fairness research that combine predictions from different models fairhome combines predictions generated by the same ml model.
this unique characteristic enables fairhome to bypass the need for creating new models rendering it even applicable to already deployed ml software.
to evaluate fairhome we conduct a large scale empirical study which compares it with seven bias mitigation methods across decision making tasks using six intersectional fairness metrics.
additionally given that bias mitigation often decreases ml performance e.g.
accuracy we comprehensively evaluate the fairness performance trade off achieved by fairhome using an advanced benchmarking tool with fairness performance measurements.
the evaluation results demonstrate the effectiveness of fairhome in improving intersectional fairness showcasing its ability to consistently outperform all methods across each metric considered.
on average across all metrics fairhome improves intersectional fairness by .
marking a .
percentage point improvement over the currently best performing method.
additionally fairhome achieves this notable fairness improvement with only minimal reductions in ml performance ranging from .
to .
depending on the metric considered.
evaluation using the advanced benchmarking tool reveals that fairhome surpasses all existing methods in fairness performance trade off.
to summarize this work offers the following contributions introduction of fairhome a novel ensemble approach using higher order mutation of inputs to significantly enhance intersectional fairness in ml software.
large scale empirical study an extensive evaluation of fairhome against state of the art techniques across decision making tasks employing intersectional fairness metrics and fairness performance measurements.
open resources public access to all our data and code to encourage replication and facilitate further research.
ii.
p reliminaries a. fairness in ml software this paper focuses on fairness of ml classification the most extensively studied topic in software fairness research .
in this context ml software assigns input instances with favorable or unfavorable labels.
central to discussions of fairness is the concept of protected attributes which are sensitive characteristics such as sex race age religion and disability status.
the population is divided into groups based on these protected attributes commonly known as privileged and unprivileged groups.
privileged groups historically enjoy certain advantages whileunprivileged groups face disadvantages or discrimination.
in practice ml software frequently exhibits bias by more often assigning favorable outcomes to members of privileged groups and unfavorable outcomes to those in unprivileged groups .
this predicament has spurred researchers to advocate for the principle of group fairness.
this principle aims to ensure that decisions made by ml software do not unfairly benefit or harm any specific population group.
to achieve this group fairness requires that the probability of receiving favorable labels remains equal between privileged and unprivileged groups or that the model s performance remains consistent across these groups .
group fairness has garnered substantial attention within the software fairness literature due to its alignment with legal regulations .
intersectional fairness is a critical facet of group fairness which measures fairness across subgroups defined by the simultaneous presence of multiple protected attributes.
it is often used interchangeably with subgroup fairness in the literature .
these subgroups are vulnerable to discrimination stemming from the convergence of various unprivileged groups within them making intersectional fairness essential.
we adopt two intersectional fairness criteria worst case intersectional fairness also called min max intersectional fairness and average case intersectional fairness .
worst case intersectional fairness quantifies the maximum disparity among subgroups i.e.
the difference between the subgroups with minimum and maximum discrimination while average case intersectional fairness calculates the averaged differences between each subgroup and the entire population.
additionally we consider three widely adopted group fairness metrics spd statistical parity difference aod average odds difference and eod equal opportunity difference .
based on the two intersectional fairness criteria and three group fairness metrics we have six intersectional fairness metrics wc spd wc aod wceod for worst case and ac spd ac aod ac eod for average case .
we assume the presence of dprotected attributes denoted as a1 a2 ... a d. each of these attributes divides the population into various groups.
we define a subgroup sga1 a2 ... a das the collection of individuals resulting from the intersection of members belonging to groups ga1through gad.
formally this is expressed as sga1 a2 ... a d ga1 ga2... gad.
in this context building subgroups aims to find all possiblesga1 a2 ... a dby considering all possible combinations of values for the protected attributes a1 a2 .
.
.
a d. for example consider two protected attributes sex with groups gsex male female and race with groups grace white non white .
the subgroup set sg includes four subgroups sg white male white female non white male non white female .
we use yand yto represent the actual decision label and the predicted decision label respectively with denoting the favorable label and denoting the unfavorable label.
building upon these concepts we can compute the worst case intersectional fairness metrics as follows.
wc spd calculates the maximum difference across subgroups in achieving favorable outcomes max s sgp min s sgp .
wc aod calculates the maximum of the average difference in false positive and true positive rates across subgroups.
max s sg p p min s sg p p .
wc eod calculates the maximum difference across subgroups in true positive rates max s sgp min s sgp .
average case intersectional fairness metrics calculate the difference between each subgroup and the entire population and then average these differences .
for instance ac spd computes the favorable rate for each subgroup and the entire population and then averages the differences between each subgroup s rate and that of the population.
detailed equations are omitted due to the page limit.
b. related work fairness improvement also known as bias mitigation has garnered growing attention in the research community.
from the se perspective it aims to address fairness bugs and ensure that software aligns with fairness requirements thereby emerging as a focal point of discussion in the se community .
bias mitigation methods are commonly classified into three types pre processing in processing and post processing .
pre processing methods mitigate bias within training data preventing its amplification during the training phase and promoting fairness in ml models.
in processing methods employ optimization strategies to reduce bias during model training.
post processing methods adjust outcomes of ml models to make them fairer.
researchers have also been exploring the combination of strategies from different categories.
for example chakraborty et al.
combined a pre processing strategy called situation testing and an in processing technique that concurrently optimizes fairness and ml performance.
bias mitigation for multiple protected attributes is important as studies have highlighted intersectional fairness issues in practice.
for example buolamwini and gebru conducted an empirical study of commercial gender classification systems and found that darker skinned females are most frequently misclassified.
however recent research has pointed out that existing bias mitigation methods primarily focus on individual protected attributes.
to alleviate this limitation se researchers have proposed techniques that can handle multiple protected attributes simultaneously.
notable examples include fairsmote maat and fairmask .
additionally there are techniques from the ml community.
for instance kang et al.
framedintersectional fairness improvement as a mutual information minimization problem using a generic end to end algorithmic method to address it.
wang et al.
conducted an empirical study of five intersectional fairness improvement methods from the ml community identifying gry as the most effective.
gry improves intersectional fairness by modeling it as a two player zero sum game with the learner as the primal player and the auditor as the dual player.
fairhome sets itself apart from these advanced methods by its ability to operate without the creation of new models.
this advantage is particularly relevant for already deployed ml software where modifying models may not be feasible.
in comparison fairsmote generates instances to balance the distribution of training data across class labels and protected attributes and trains new models based on the augmented data.
maat creates fair models through sampling training data and combines them with models optimized for ml performance.
like fairhome it is an ensemble method but it adheres to the traditional ensemble paradigm that combines multiple models.
fairmask modifies inputs by using training data to train extrapolation models that adjust protected attributes in input instances.
differently fairhome operates without the need for creating extrapolation models.
gry is an inprocessing method that needs to retrain models.
additionally considerable research efforts have been devoted to the evaluation of bias mitigation methods.
for instance hort et al.
provided a unified perspective by integrating fairness and ml performance and gauging their interplay.
they introduced a benchmarking approach named fairea which establishes a unified fairness performance tradeoff baseline for comparing bias mitigation methods.
using fairea chen et al.
compared existing methods in tradeoff between intersectional fairness and ml performance and found that rew maat and fairmask can achieve the best results.
the three methods are all included as our baseline methods.
in this paper we also employ fairea for fairnessperformance trade off evaluation.
iii.
o ur approach a. fairhome in a nutshell from the se perspective fairhome follows the input debugging paradigm .
this paradigm suggests that software issues with processing inputs may not always necessitate modifications to the software itself instead adjusting the inputs can also resolve these issues.
figure illustrates the overview of fairhome.
it is inspired by the widely recognized social science insights that increased diversity can foster fairness in decision making .
specifically fairhome aims to enhance intersectional fairness by diversifying the inputs used in the decision making process of the inference phase.
this is achieved by generating diverse mutants which represent different subgroups for each input instance.
fairhome employs higher order mutation based on multiple protected attributes to create these mutants.
it then aggregates the decisions of the ml software for the original input and all its mutants to make the final decision.fig.
overview of fairhome.
this marks a departure from traditional ensemble learning which focuses on the ensemble of different models to enhance prediction.
instead fairhome enhances the prediction fairness through the ensemble of outputs from the same ml model.
b. input generation fairhome employs higher order mutation to generate input mutants from an original input xfor the ml software sml.
this aims to diversify subgroup representation in the decisionmaking process thereby addressing intersectional fairness.
to ensure clarity in the subsequent description we begin by defining the necessary notations.
the input for sml comprises nattributes collectively represented as a a1 a2 ... a n categorized into dprotected attributes a1 a2 ... a d denoted as p and n dnon protected attributes ad ad ... a n denoted as n .
assuming each attribute aibelongs to a valuation domain ii the overall input domain of smlisi i1 i2 ... in.
the detailed higher order mutation process is as follows defining mutation operator a mutation operator is defined as a transformation rule that generates a mutant from the original instance .
in our approach we define the mutation operator to alter the value of each protected attribute ai p within its valuation domain ii while keeping the values of non protected attributes constant.
this operation is grounded in the widely adopted causal fairness principle which directly modifies protected attributes and assesses the impact of this modification on the prediction .
the mutation operator enables the generation of diverse mutants from a single input instance by applying it across multiple protected attributes.
generating input mutants our input generation process is referred to as higher order mutation in se since it allows for applying the mutation operator to an input instance multiple times targeting multiple protected attributes.
specifically our higher order mutation entails the use of on protected attributes to produce any mutant x ofxthat satisfies two key conditions there exists at least one protected attribute a pfor which the value in x differs from that in x i.e.
a p xa x a .
for each non protected attribute q n the value in x matches that in x i.e.
q n x q x q .our objective is to exhaustively explore the input domain ito identify all instances that satisfy these conditions.
we define the input domain for protected attributes using all possible value combinations from the training data ensuring compliance with input constraints.
this approach ensures the validity of mutants and keeps the number of possible mutants manageable limited to combinations present in the training data.
this results in a set of mutants mrepresenting diverse subgroups by varying protected attributes.
the number of generated mutants isqd i ii which represents all possible combinations of protected attribute values excluding the combination present in the original input instance.
for example with two protected attributes sex male female and race white non white we have four subgroups white male white female non white male and non white female.
to produce a fair result for a non white female our approach uses inputs for all four subgroups.
prior research has identified correlations between protected attributes and non protected features suggesting that mutating non protected features associated with protected attributes could further enhance fairness.
however this approach requires learning feature correlations.
following the try withsimpler se practice we do not adopt this mutation strategy as the default in fairhome.
nevertheless the effectiveness of this alternative strategy compared to fairhome s default is analyzed in section v d. c. output ensemble fairhome aggregates outputs of the ml software smlfor the original input and its mutants to produce the final decision without analyzing output differences.
since we focus on ml classification for each input sml produces a probability vector with each element representing the probability of classification into a category.
smlmakes the final decision for each input based on its probability vector.
we introduce three commonly used ensemble strategies to aggregate outputs majority vote averaging and weighted averaging.
the details of each strategy are as follows majority vote the majority vote strategy selects the decision that receives the most votes as the final decision.
since intersectional fairness typically involves at least two protected attributes each with at least two values there are at least three mutants in addition to the original input making the majority vote strategy applicable.
let r0represent the decision made bysmlfor the original input x and r1 r2 ... r m denote the decisions for each mutant.
the strategy assigns a final decision as unfavorable if more than of the decisions r0 r1 r2 ... r m are unfavorable and favorable otherwise.
averaging the averaging strategy uses the output probability vectors of the original input and its mutants to determine the final decision.
let p0represent the probability of the original input being classified as favorable and p1 p2 ... p m denote such probabilities for each mutant.
the strategy computes the mean of p0 p1 p2 ... p m .
if the final probability is below the decision is unfavorable otherwise it is favorable.weighted averaging this strategy uses probability vectors to determine the final decision by calculating the weighted average of the probabilities p0 p1 p2 ... p m .
the weight wi for each probability piis pi which reduces the impact of predictions near the decision boundary known to be more prone to bias .
prediction probabilities close to indicate a higher risk of bias so assigning lower weights to these instances minimizes their impact on the ensemble outcome potentially enhancing fairness.
the weighted average is computed as w p m i 0wi pip m i 0wi.
ifwis below the final decision is unfavorable otherwise it is favorable.
following the try with simpler se practice we use majority vote as the default strategy of fairhome due to its simplicity and reliance solely on decision information.
nevertheless all the strategies are evaluated in section v e. iv.
e valuation setup a. research questions rqs rq1 can fairhome improve intersectional fairness without largely compromising ml performance?
this rq examines fairhome s impact on intersectional fairness and ml performance considering the trade off between them .
rq2 how effectively does fairhome improve intersectional fairness compared to existing methods?
this rq compares the effectiveness of fairhome against existing bias mitigation methods in enhancing intersectional fairness.
rq3 how does fairhome balance intersectional fairness and ml performance compared to existing methods?
this rq compares fairhome with other bias mitigation methods by assessing their fairness performance trade off.
rq4 how effective is fairhome when it also mutates features correlated with protected attributes?
this rq compares the default fairhome with its variant that also mutates non protected features correlated with protected attributes.
rq5 how do different ensemble strategies affect fairhome?
this rq compares the common ensemble strategies for fairhome and evaluates their effectiveness.
rq6 what is the contribution of different mutants?
this rq compares the effectiveness of fairhome with two approaches one using only mutants involving a single protected attribute and the other using only mutants involving multiple protected attributes.
rq7 how does fairhome affect group fairness regarding single protected attributes?
this rq investigates whether fairhome negatively impacts group fairness for single protected attributes.
b. experimental methodology step .
design of bias mitigation tasks we use bias mitigation tasks for the study achieved through a combination of six benchmark datasets and four ml models.
benchmark datasets we select six real world decision problems using datasets widely adopted in fairness research adult compas default german mep15 and mep16 .table i presents an overview of these datasets.
as highlighted in prior research existing datasets primarily include two protected attributes.
the protected attributes for the adult default mep15 and mep16 datasets are specified by chen et al.
while those for the compas and german datasets are specified by zhang et al.
.
to our knowledge our study employs the largest number of datasets in intersectional fairness studies.
these datasets are representative for two key reasons they span diverse domains such as finance social and medical applications where fairness is crucial and they cover sex race and age which are demonstrated to be the most commonly considered protected attributes .
ml models we use four common types of ml models that are demonstrated to be most widely explored in fairness research lr logistic regression rf random forest svm support vector machine and dnn deep neural network .
lr rf and svm have prominent application in fairness critical decision problems dnn due to its significance in modern decision making scenarios also garners substantial attention from fairness researchers .
for lr rf and svm we adopt the configurations from recent software fairness papers for dnn we adopt a network architecture extensively used for these datasets which features a fully connected network having five hidden layers with and units respectively.
for each of the six benchmark datasets we train the four types of ml models for bias mitigation resulting in tasks.
step .
selection of baseline methods we select seven bias mitigation methods for comparison covering both widelyadopted and cutting edge techniques.
first based on a recent survey we select the three most popular bias mitigation methods rew reweighting adv adversarial debiasing and eop equalized odds post processing .
second we include three recently proposed methods in se known for their effectiveness in handling multiple protected attributes fairsmote maat and fairmask .
finally we consider gry an approach identified as the most effective in a recent empirical study on intersectional fairness in the ml community.
our selection represents a diverse range of bias mitigation techniques including pre processing in processing and postprocessing methods.
a brief description of each method is provided below rew is a pre processing method that assigns different weights to training data for each group label combination thereby ensuring fairness.
adv is an in processing method that employs adversarial techniques to minimize the influence of protected attributes in predictions while concurrently maximizing prediction accuracy during model training.
eop is a post processing method that uses a linear program to determine probabilities for altering output labels in order to optimize equalized odds.
fairsmote identifies the largest subgroup in training data and generates samples for other subgroups to achievetable i benchmark datasets.
name size protected attributes favorable label description adult sex race income 50k predicting whether an individual s annual income surpasses 50k compas sex race age no recidivism predicting criminal defendant recidivism default sex age default predicting whether a customer will default on payment german sex age good credit classifying an individual as a good or bad credit risk mep15 sex race utilizer predicting healthcare utilization using survey data from mep16 sex race utilizer predicting healthcare utilization using survey data from balanced sample counts and equitable favorable rates among subgroups.
it also excludes ambiguous training data through situation testing.
maat aims to improve fairness performance tradeoff of ml software.
to achieve this it develops fairnessoptimized models for individual protected attributes through training data sampling and then combines their outputs with a performance optimized model.
fairmask uses training data to learn individual extrapolation models that predict each protected attribute using other features.
then it applies these extrapolation models to reassign protected attributes in input data.
gry formulates intersectional fairness improvement as a two player zero sum game between the learner primal player and the auditor dual player .
step .
analysis of fairhome s effect this step addresses rq1 .
we evaluate the effect on intersectional fairness using six metrics described in section ii a wc spd wcaod wc eod ac spd ac aod and ac eod.
for each metric lower values indicate a higher degree of fairness.
fairness metrics alone can sometimes indicate fairness even if a model is cumulatively biased across all subgroups.
to address this we complement our evaluation with comprehensive ml performance metrics.
if a model exhibits cumulative bias e.g.
consistently low performance across all subgroups its overall performance will be low.
additionally bias mitigation often comes at the cost of ml performance .
therefore it is crucial to also consider the impact on ml performance when evaluating fairhome.
we use five commonly used ml performance metrics accuracy precision recall f1 score andmcc matthews correlation coefficient .
for each metric higher values correspond to better ml performance.
a brief description of these ml performance metrics is provided as follows.
accuracy indicates the overall correctness of an ml model s predictions.
precision measures the model s accuracy in predicting a specific target class.
recall measures the model s ability to correctly identify all instances of a given target class.
f1 score represents the harmonic mean of precision and recall.
for precision recall and f1 score we follow previous work to employ the macroaverage value across favorable and unfavorable classes to comprehensively account for both.
this involves computing the metric for each class and then averaging the results.
mcc is chosen due to its suitability for imbalanced datasets which are common in benchmark datasets used for fairness research .
this choice addresses the concern that accuracy the most widely used metric in fairness research might not adequately reflect performance in imbalanced class distributions .
step .
comparison of intersectional fairness this step addresses rq2 .
we compare the effectiveness of fairhome and existing methods in enhancing intersectional fairness.
first we compute the enhancements achieved by each method over the original models and compare these enhancements.
additionally we conduct an in depth comparison of the fairness metric values obtained by fairhome and existing methods through a win tie loss analysis .
specifically for each existing method we compare the fairness metric values obtained by fairhome and it across the taskfairness metric combinations.
following prior research we employ the mann whitney u test to assess the statistical significance of differences in fairness metric values between two methods.
the null hypothesis is that there is no difference in fairness metric value distributions between fairhome and the existing method while the alternative hypothesis posits a significant difference.
in scenarios where fairhome and the existing method exhibit fairness results with statistically significant differences as indicated by a twotailed p value .05from the mann whitney u test if fairhome yields lower fairness metric values indicating higher fairness we label it as a win if the existing method produces lower fairness metric values we label fairhome as a loss.
conversely for scenarios where the two methods do not show statistically significant differences we classify the comparison outcome as a tie.
step .
comparison of fairness performance trade off this step addresses rq3 .
bias mitigation often leads to reduced ml performance .
if we assess fairness improvement and ml performance loss separately it becomes uncertain whether enhanced fairness results from a mere sacrifice in performance .
moreover comparing bias mitigation methods while considering these two factors separately can be challenging.
to address this problem hort et al.
proposed the use of a unified fairness performance trade off baseline created through their fairea approach as a benchmark for comparing different bias mitigation methods.
to construct the trade off baseline fairea generates a sequence of mutated models by gradually substituting an increasing portion of the original model s predictions with the majority class prediction from the dataset.
this process enhances fairness by uniformly reducing predictive performance across different subgroups.
fairea expects that any reasonable bias mitigation method should outperform these naive mutated models.
therefore it provides the trade off attained by thesemodels as the unified baseline for the research community to evaluate bias mitigation methods.
the trade off baseline categorizes bias mitigation methods into five levels of trade off effectiveness .
a method is classified as win win trade off if it enhances both ml performance and fairness compared to the original model.
conversely if a method decreases both it is categorized as lose lose trade off.
in cases where a method enhances ml performance but decreases fairness it is considered inverted trade off.
additionally there are two other levels of tradeoff where methods decrease ml performance but enhance fairness.
specifically if a method achieves a superior tradeoff compared to the baseline it is classified as good trade off otherwise it is categorized as poor trade off.
among all five trade off types win win and good trade offs indicate that the method surpasses the trade off baseline constructed by fairea.
the trade off analysis covers a total of fairnessperformance measurements since we consider six intersectional fairness metrics and five ml performance metrics.
we use fairea to construct the trade off baseline for each pairing of bias mitigation task and fairness performance measurement.
we employ these trade off baselines to comprehensively evaluate the trade off effectiveness of fairhome and existing bias mitigation methods.
step .
evaluation of mutation strategies this step addresses rq4 .
prior research has identified correlations between protected attributes and non protected features indicating that mutating non protected features associated with protected attributes could further help improve fairness.
thus we introduce fairhome1 a variant of fairhome for comparison.
unlike fairhome which mutates only protected attributes fairhome1 extends mutations to correlated nonprotected features.
following prior work we use linear regression models to learn these correlations training models for each non protected feature using protected attributes as predictors.
these models determine the adjustments for nonprotected features when protected attributes are mutated.
step .
evaluation of ensemble strategies this step addresses rq5 .
we compare the three ensemble strategies provided in section iii c in terms of intersectional fairness and fairness performance trade off effectiveness.
step .
analysis of mutant contribution this step addresses rq6 .
we compare the intersectional fairness and fairness performance trade off effectiveness of using only mutants involving a single protected attribute and using only mutants involving multiple protected attributes.
step .
analysis of group fairness this step addresses rq7 .
we evaluate the effect of fairhome on group fairness for individual protected attributes.
for instance with the adult dataset we assess group fairness regarding sex and race separately.
we use spd aod and eod as metrics to measure group fairness for each attribute.
the mann whitney u test is also used in this step to ensure statistical significance.
to ensure the reliability of our results all the experiments in steps to are repeated times.v.
r esults a. rq1 effect of fairhome rq1 investigates the dual effect of fairhome on intersectional fairness and ml performance.
we apply fairhome across tasks comprising six datasets and four ml models.
given that our experiments are repeated times we compare the mean metric values over these iterations for both the original models and those applied fairhome.
table ii shows the results.
we observe that fairhome enhances intersectional fairness indicated by lower fairness metric values compared to the original models in out of task fairness metric combinations accounting for .
of scenarios.
regarding ml performance we observe a decrease caused by fairhome which can be attributed to the well recognized fairness performance trade off .
however this decrease in ml performance is significantly outweighed by the substantial improvement in intersectional fairness achieved by fairhome.
for clarity we calculate the mean intersectional fairness and ml performance metric values across the tasks for both the original models and fairhome.
subsequently we compute the absolute and relative changes induced by fairhome as depicted in table iii.
table iii reveals that fairhome enhances intersectional fairness by .
to .
relative changes across different fairness metrics.
in contrast ml performance experiences a slight decrease ranging from .
to .
relative changes across various ml performance metrics.
finding fairhome largely enhances intersectional fairness at a minimal cost to ml performance.
specifically it improves intersectional fairness by .
to .
across different fairness metrics while only reducing ml performance by .
to .
depending on the ml performance metric considered.
b. rq2 comparison of intersectional fairness rq2 evaluates the intersectional fairness achieved by fairhome compared to existing bias mitigation methods.
due to the page limit statistical results are presented here omitting detailed fairness metric values for each method and task which are accessible in our repository .
for each method we calculate the mean absolute and relative improvements in intersectional fairness across tasks compared to the original models.
results in table iv show that fairhome achieves the most substantial enhancement in intersectional fairness as indicated by the largest decrease in fairness metric values across all evaluated metrics.
specifically for wc spd wc aod wc eod ac spd ac aod and ac eod fairhome enhances fairness by .
.
.
.
.
and .
respectively.
on average across all metrics fairhome improves intersectional fairness by .
.
in contrast among existing methods fairmask achieves the highest improvement in intersectional fairness table ii rq1 comparative analysis of fairness and ml performance between the original models and fairhome.
scenarios where fairhome enhances intersectional fairness indicated by lower fairness metric values are shaded in grey.
fairhome improves intersectional fairness in out of task fairness metric combinations accounting for .
of scenarios.
datasetlr rf ws wa we as aa ae acc p r f1 mcc ws wa we as aa ae acc p r f1 mcc adultoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compasoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
defaultoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
germanoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mep15original .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mep16original .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
datasetsvm dnn ws wa we as aa ae acc p r f1 mcc ws wa we as aa ae acc p r f1 mcc adultoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compasoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
defaultoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
germanoriginal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mep15original .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mep16original .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ws wa we as aa ae acc p r and f1 denote wc spd wc aod wc eod ac spd ac aod ac eod accuracy precision recall and f1 score respectively.
table iii rq1 mean intersectional fairness and ml performance metric values achieved by the original models and fairhome across tasks.
fairhome enhances intersectional fairness indicated by decreased fairness metric values by .
to .
across various metrics while it minimally impacts ml performance which decreases by only .
to .
depending on the metric analyzed.
original fairhomeabsolute changerelative change wc spd .
.
.
.
wc aod .
.
.
.
wc eod .
.
.
.
ac spd .
.
.
.
ac aod .
.
.
.
ac eod .
.
.
.
accuracy .
.
.
.
precision .
.
.
.
recall .
.
.
.
f1 score .
.
.
.
mcc .
.
.
.
with an enhancement of .
on average across fairness metrics .
percentage points lower than fairhome.
we further conduct a detailed comparison of fairness metric values obtained by fairhome and existing methods using the win tie loss analysis outlined in section iv b4.
table v shows the results organizing the win tie loss outcomes based on the metrics and also providing an overall summary of the comparison results.
from the last row we find that fairhome outperforms all considered existing methods in terms of scenario wins.
this indicates that fairhome achieves more wins than losses when compared with any of the methods across the six intersectional fairness metrics.
for example fairhome surpasses fairmask in scenarios while fairmask does notoutperform fairhome in any scenario.
furthermore when examining each metric individually we observe that fairhome consistently outperforms all considered existing methods.
finding fairhome consistently outperforms existing bias mitigation methods in enhancing intersectional fairness across all evaluated metrics.
on average across all metrics fairhome improves intersectional fairness by .
which is .
percentage points higher than that of the currently best performing method.
c. rq3 comparison of fairness performance trade off rq3 employs fairea a state of the art benchmarking tool designed to evaluate fairness performance trade off to compare the trade off effectiveness of fairhome with existing bias mitigation methods.
each method is applied to tasks with fairness performance measurements and each experiment is repeated times.
as a result there are a total of24 400mitigation cases for each method.
using fairea we classify the effectiveness of each method on each case into various effectiveness levels described in section iv b5 and subsequently obtain the effectiveness level distributions for each method.
figure illustrates the effectiveness level distributions.
overall fairhome demonstrates the best fairnessperformance trade off among all the methods.
specifically fairhome outperforms the trade off baseline constructed by fairea i.e.
achieving win win or good trade off in .
of cases.
in contrast existing methods achieve this in onlytable iv rq2 mean absolute and relative improvements in intersectional fairness i.e.
decreased fairness metric values across tasks.
we find that fairhome consistently outperforms existing bias mitigation methods in enhancing intersectional fairness across all evaluated metrics.
methodwc spd wc aod wc eod ac spd ac aod ac eod abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
rew .
.
.
.
.
.
.
.
.
.
.
.
adv .
.
.
.
.
.
.
.
.
.
.
.
eop .
.
.
.
.
.
.
.
.
.
.
.
fairsmote .
.
.
.
.
.
.
.
.
.
.
.
maat .
.
.
.
.
.
.
.
.
.
.
.
fairmask .
.
.
.
.
.
.
.
.
.
.
.
gry .
.
.
.
.
.
.
.
.
.
.
.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
table v rq2 comparative analysis of intersectional fairness between fairhome and existing bias mitigation methods across tasks and six fairness metrics.
the numbers of fairhome s win tie loss scenarios are shown.
for each metric fairhome consistently outperforms all considered existing methods in terms of scenario wins.
metric rew adv eop fairsmote maat fairmask gry wc spd wc aod wc eod ac spd ac aod ac eod overall fig.
rq3 effectiveness level distributions of fairhome and existing methods in fairness performance trade off.
overall fairhome achieves the best trade off with .
of mitigation cases falling in win win or good trade off.
.
to .
of cases.
furthermore fairhome exhibits the fewest mitigation cases falling into the lose lose trade off accounting for .
of mitigation cases.
in contrast existing methods suffer from lose lose trade off in .
to .
of cases.
finding according to a state of the art benchmarking tool with fairness performance measurements fairhome outperforms existing bias mitigation methods in fairness performance trade off.d.
rq4 evaluation of mutation strategies rq4 evaluates the impact of mutating not only protected attributes but also features correlated with them within the framework of our approach.
we refer to this as fairhome1.
first we evaluate the mean intersectional fairness improvements achieved by fairhome and fairhome1 across tasks.
table vi presents the results.
while fairhome1 demonstrates improved intersectional fairness through lower wc ac spd values compared to fairhome it also results in higher wc ac aod and wc ac eod values.
this suggests that fairhome1 could exacerbate differences in error rates across subgroups indicated by increased wc ac aod and wc ac eod likely due to unintended over adjustments or noise introduced during the mutation of features correlated with protected attributes.
second we compare the effectiveness of the fairnessperformance trade off between fairhome and fairhome1.
for ease of illustration we follow previous work to measure trade off effectiveness based on the proportion of mitigation cases where each method surpasses the tradeoff baseline constructed by fairea i.e.
falling in win win or good trade off .
overall fairhome surpasses this baseline in .
of cases while fairhome1 does so in .
showcasing fairhome s superior trade off between intersectional fairness and performance.
this outcome aligns with expectations as fairhome1 not only exhibits higher wc acaod and wc ac eod values but also tends to introduce additional noise that adversely affects ml performance.
considering the drawbacks of fairhome1 as evidenced by the results and its requirement for learning feature correlations we do not use it as the default strategy of fairhome.
finding extending mutations to both protected attributes and their correlated features leads to a poorer fairness performance trade off and decreased intersectional fairness across wc ac aod and wc ac eod.
e. rq5 evaluation of ensemble strategies rq5 evaluates three commonly used ensemble strategies within the framework of our approach majority vote the default fairhome averaging fairhome2 and weighted averaging fairhome3 .table vi rq4 mean absolute and relative intersectional fairness improvements achieved by fairhome and fairhome1 across tasks.
fairhome1 achieves lower wc ac spd but higher wc ac aod and wc ac eod than fairhome.
methodwc spd wc aod wc eod ac spd ac aod ac eod abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
fairhome1 .
.
.
.
.
.
.
.
.
.
.
.
table vii rq5 mean absolute and relative intersectional fairness improvements achieved by fairhome fairhome2 and fairhome3 across tasks.
overall they achieve similar intersectional fairness improvements.
methodwc spd wc aod wc eod ac spd ac aod ac eod abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
fairhome2 .
.
.
.
.
.
.
.
.
.
.
.
fairhome3 .
.
.
.
.
.
.
.
.
.
.
.
first we evaluate the intersectional fairness achieved by the three methods.
table vii illustrates their mean enhancements across tasks.
we find that they achieve similar fairness improvements.
moreover when compared with the fairness improvements of state of the art methods as listed in table iv all three ensemble strategies exhibit superior results.
second we evaluate the fairness performance trade off effectiveness of the three strategies.
we measure this using the proportion of mitigation cases where each method surpasses the trade off baseline constructed by fairea i.e.
win win or good trade off .
we find that fairhome fairhome2 and fairhome3 outperform the baseline in .
.
and .
of cases respectively with a negligible .
difference.
compared to state of the art methods which range from .
to .
as shown in figure all three strategies surpass existing methods in fairness performance trade off.
in summary the three ensemble strategies yield comparable results.
given that the majority vote requires only decision information and is simpler we adhere to the try with simpler se practice to adopt it as the default ensemble strategy.
finding our approach consistently outperforms existing methods in both intersectional fairness and fairnessperformance trade off under different ensemble strategies majority vote averaging and weighted averaging.
f .
rq6 contribution of different mutants rq6 compares the effectiveness of three approaches using only mutants that mutate a single protected attribute fairhome4 only mutants that mutate multiple protected attributes fairhome5 and a combination of both fairhome .
we use the compas dataset for this rq as it includes three protected attributes.
for datasets with only two protected attributes we can generate only one mutant involving mutating multiple attributes making majority voting impractical for fairhome5 with only two outputs.
table viii shows the improvements achieved by each model across four tasks on the compas dataset.
fairhome5 shows greater intersectional fairness improvements compared to fairhome4.
when comparing fairhome5 to fairhome fairhome5 achieves an average relative improvement of .
across six metrics while fairhome achieves .
.
therefore fairhome performs the best overall.
additionally we assess the fairness performance trade off effectiveness of the three strategies by examining the proportion of cases where each method surpasses the trade off baseline set by fairea i.e.
win win or good trade off scenarios .
the results show that fairhome fairhome4 and fairhome5 exceed the baseline in .
.
and .
of cases respectively.
this further indicates that using only mutants involving multiple protected attributes outperforms using only mutants involving a single attribute with the best results achieved by combining both types i.e.
fairhome .
finding using mutants involving multiple protected attributes enhances intersectional fairness more than using only single attribute mutations with the best results achieved by combining both types i.e.
fairhome .
g. rq7 effect on group fairness rq7 assesses the impact of fairhome on group fairness regarding single protected attributes while enhancing intersectional fairness.
we evaluate scenarios combining single attribute tasks from table i e.g.
adult sex models and group fairness metrics i.e.
spd aod and eod .
according to the mann whitney u test fairhome significantly improves group fairness regarding single attributes in scenarios and decreases it in .
in comparison fairmask the best baseline approach identified in rq2 table v and rq3 figure significantly improves such fairness in scenarios and decreases it in .
fairhome improves group fairness for single attributes while enhancing intersectional fairness through its comprehensive approach to bias mitigation.
by generating mutants representing all possible subgroups it effectively addresses biases in both single and intersectional attributes.
consequently fairhome s thorough consideration of both single and combined attribute effects leads to simultaneous improvements in group fairness and intersectional fairness.table viii rq6 mean absolute and relative intersectional fairness improvements achieved by fairhome fairhome4 and fairhome5 across four tasks on the compas dataset.
overall fairhome performs the best.
methodwc spd wc aod wc eod ac spd ac aod ac eod abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
abs.
rela.
fairhome .
.
.
.
.
.
.
.
.
.
.
.
fairhome4 .
.
.
.
.
.
.
.
.
.
.
.
fairhome5 .
.
.
.
.
.
.
.
.
.
.
.
finding in addition to improving intersectional fairness fairhome significantly enhances group fairness for single protected attributes in out of scenarios.
vi.
d iscussion a. advantages of fairhome effective.
the main goal of fairhome is to improve intersectional fairness.
our results in rq2 indicate that fairhome outperforms state of the art methods resulting in notable enhancements in intersectional fairness.
balanced.
it is important for bias mitigation methods to strike a balance between fairness and ml performance.
our results in rq3 showcase that fairhome outperforms state of the art methods by offering a superior trade off between intersectional fairness and ml performance.
non disruptive.
fairhome operates solely on inputs during the inference phase ensuring seamless implementation without disruption to existing training data processing or necessitating model changes.
even for deployed ml software applying fairhome requires minimal development or deployment efforts as engineers can easily modify software inputs.
lightweight access to training data.
unlike state of the art methods like fairsmote maat and fairmask which require access to the entire training dataset fairhome accesses only the protected attributes to acquire their possible values reducing the risk of inadvertently exposing private information within the training data.
no need for training new models.
in an era where sustainable and green se practices are increasingly emphasized in both research and industry fairhome stands out by eliminating the necessity of training new models.
moreover we compare the time cost of different methods.
the experiments are executed on ubuntu .
lts with 128gb ram a .
ghz intel xeon e5 v3 dual cpu and two nvidia tesla m40 gpus.
fairhome s mutation takes an average of .
seconds across all tasks.
in comparison existing methods that we consider take between .
and .
seconds.
b. threats to validity construct validity.
the measurement of fairness and ml performance poses a potential threat to the construct validity.
to address this concern we use six intersectional fairness metrics extensively adopted in the literature alongside five standard ml performance metrics.
additionally we conduct trade off analysis using a set of fairness performance measurements the most extensive in the fairness literature.internal validity.
to ensure the accuracy of our results we carefully replicate existing bias mitigation methods for comparative analysis.
to mitigate the impact of randomness on our results we conduct repetitions for each experiment.
due to the page limit we often present average level statistical results which may obscure variations and outliers while omitting detailed values for all scenarios.
the comprehensive results are available in our repository .
external validity.
to address potential concerns regarding external validity we use bias mitigation tasks for evaluation.
these tasks cover six well studied decision problems and four types of ml models in the fairness literature.
when selecting existing methods for comparison we consider both widely used methods and recent methods.
our selection encompasses pre processing in processing and post processing methods.
vii.
c onclusion this paper introduces fairhome a novel ensemble approach using higher order mutation to improve intersectional fairness of ml software during the inference phase.
fairhome mutates protected attributes within an input instance to generate diverse inputs from various subgroups.
these mutants are then combined with the original input to make the final decision.
an extensive evaluation across widely adopted decision tasks demonstrates the power of fairhome in surpassing state of the art bias mitigation methods thereby propelling intersectional fairness to a new height.
moreover fairhome achieves the best trade off between intersectional fairness and ml performance.
viii.
d ata availability we have provided a replication package including all the datasets code and intermediate results of our work.
acknowledgment this research is supported by the national research foundation singapore and dso national laboratories under the ai singapore programme aisg award no aisg2 rp by the national research foundation singapore and the cyber security agency under the national cybersecurity r d programme ncrp25 p04 taicen and by the national research foundation prime minister s office singapore under the campus for research excellence and technological enterprise create programme.
any opinions findings conclusions or recommendations expressed in this paper are those of the authors and do not reflect the views of the national research foundation singapore or the cyber security agency of singapore.