planning for untangling predicting the difficulty of merge conflicts caius brindescu oregon state university corvallis or usa brindesc oregonstate.eduiftekhar ahmed university of california irvine irvine ca usa iftekha uci.edu rafael leano oregon state university corvallis or usa leanor oregonstate.eduanita sarma oregon state university corvallis or usa anita.sarma oregonstate.edu abstract merge conflicts are inevitable in collaborative software development and are disruptive.
when they occur developers have to stop their current work understand the conflict and the surrounding code and plan an appropriate resolution.
however not all conflicts are equally problematic some can be easily fixed while others might be complicated enough to need multiple people.
currently there is not much support to help developers plan their conflict resolution.
in this work we aim to predict the difficulty of a merge conflict so as to help developers plan their conflict resolution.
the ability to predict the difficulty of a merge conflict and to identify the underlying factors for its difficulty can help tool builders improve their conflict detection tools to prioritize and warn developers of difficult conflicts.
in this work we investigate the characteristics of difficult merge conflicts and automatically classify them.
we analyzed conflicts across java projects and found that merge conflict difficulty can be accurately predicted auc of .
through machine learning algorithms such as bagging.
ccs concepts software and its engineering software reliability software maintenance tools software design tradeoffs .
keywords merge conflict difficulty prediction merge conflict resolution empirical analysis acm reference format caius brindescu iftekhar ahmed rafael leano and anita sarma.
.
planning for untangling predicting the difficulty of merge conflicts.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
introduction version control systems vcs have made large teams possible enabling thousands of developers to contribute together in building open source software oss and proprietary software and technologies.
however despite the introduction of new sophisticated distributed version control systems the basic protocol of using vcs still remains the same code in private workspaces and synchronize periodically.
one challenge with this coordination protocol is merge conflicts.
merge conflicts occur when developers modify the same lines of code simultaneously.
research shows that merge conflicts are prevalent about of all merges end up in a merge conflict .
merge conflicts have an impact on the code quality and are disruptive to the development workflow.
to resolve a merge conflict a developer has to stop what they are and focus on the resolution.
resolving a conflict requires the developer to understand the conflicting changes and craft a solution that satisfies both sets of requirements driving the change.
this disruption to the workflow can be compounded if conflict resolution requires additional expertise .
these factors can prompt developers to postpone the conflict resolution or kick the can further down the road.
in fact a study by nelson et al.
found that .
of developers have deferred at least once when responding to a merge conflict.
however the later a conflict is resolved the harder it is to recall the rationale of the changes.
which makes the resolution process that much more difficult .
as aptly put by a participant from the study by nelson et al.
deferring a merge conflict simply kicks the can down the road or off a cliff .
typically resolving the conflict only gets more difficult as time passes.
however sooner or later the conflict has to be resolved.
to do so developers follow a process with four distinct resolution phases as illustrated in figure .
developers alternate between clean and conflicting states of code.
beginning from the development stage developers create an awareness of conflicts within the codebase either passively when they face a conflict during a merge or by proactively monitoring ongoing changes.
once aware developers begin planning for a resolution to fix the conflict.
and finally developers evaluate the effectiveness of their deployed resolutions returning to planning if the resolution fails .
ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea caius brindescu iftekhar ahmed rafael leano and anita sarma clean conflicting awareness3 planning evaluation4 resolution1 development figure model of developer processes for managing merge conflicts from nelson et al.
several research works exist to support parts of the conflict resolution process.
for the development and awareness phase developers can benefit from workspace awareness tools .
when working on the resolution phase developers can utilize different semi automated merge techniques such as unstructured merge structured merge semanticsbased merge and hybrid merge .
the evaluation phase has support through existing vcs e.g.
git svn tfs cvs and continuous integration systems e.g.
jenkins travis ci teamcity .
none of these works however support the planning phase of merge conflict resolution.
we aim to close this gap and help developers plan their conflict resolution by predicting the difficulty of a merge conflict.
our work can facilitate planning of conflict resolution in several ways.
it can help developers plan when to resolve the conflict if the conflict is simple they can resolve it instantaneously otherwise they may need to allocate a longer resolution time period who to resolve it with if a conflict is difficult they may need to coordinate a collaborative merge how much to review or test the merged code if a conflict is difficult it may behoove the developers to more rigorously review and test the merged changes.
in this work through a large scale empirical investigation we analyze what makes a merge conflict difficult and whether we can predict the severity of a conflict from its underlying changes.
more formally we aim to answer the following research questions rq1 how well can we predict the difficulty of merge conflicts?
rq2 what makes a merge conflict difficult?
rq3 how portable is our conflict difficulty prediction model?
to answer these research questions we mine the characteristics of merge conflicts from java projects in github.
to enable a prediction of merge conflict resolution we gather a total of process and code related metrics such as the conflicting lines of code differences in abstract syntax trees ast cyclomatic complexity cc etc.
we use metrics that are available as the conflict develops i.e.
before the developer merges their changes therefore enabling developers lead time for their planning.
our results show that we can predict the difficulty of merge conflicts accurately an auc of .
.
knowing which conflicts are difficult can help developers plan their conflict resolution.
our work also serves as a baseline prediction model for further research.
related work merge conflicts are a common side effect of concurrent development .
while the use of version control systems flags divergent changes and prevents one change from overwriting another they cannot always automatically resolve conflicts.
researchers have tried different approaches to help developers deal with conflicts early detection of conflicts merging assistance and prevention of conflicts.
.
early detection conflicts tend to grow over time.
therefore early detection helps limit the files and the size of the changes involved in a conflict.
workspace awareness tools monitor ongoing work to detect emerging conflicts.
the goal of these tools is to catch the conflicts early so that it is easier to resolve them.
biehl et al.
propose fastdash which identifies developers modifying the same file and notifies them about potential merge conflicts as they emerge.
hattori and lanza propose syde a tool that analyzes changes made to the source code at the level of ast operations.
syde detects conflicts by comparing the ast tree operations.
guimaraes and silva introduce wecode which continuously merges changes in the background to detect merge conflicts.
tools such as palantir and crystal proactively detect both merge conflicts as well as semantic conflicts the latter being conflicts that are revealed by failed builds or tests.
servant et al.
propose casi a tool that allows developers to visualize the code that their changes impact with the aim of detecting semantic conflicts.
while these tools notify developers of emerging conflicts they do not provide any assistance in resolving them.
our focus is on predicting the difficulty of merge conflicts so that developers can prioritize their resolution efforts.
.
merging assistance another major thread in merge conflict research is support for the resolution of merge conflicts.
mens provides a comprehensive survey on the state of the art merging techniques.
apel et al.
propose a new merging approach called semi structured merging.
this technique considers the syntactical structure of the code that is to be merged.
lippe and van oosterom also propose a new merging technique operational merging which considers the changes that were done to the code in addition to the end result.
while we do not attempt to provide resolution support to developers our work may help developers choose one resolution strategy over other based on the difficulty of the conflict.
.
prevention finally another way to deal with conflicts is to prevent them from occurring in the first place.
kasi and sarma try to avoid merge conflicts altogether by scheduling tasks in a way that minimize the probability of conflict.
wloka et al.
introduce safecommit.
it uses a static analysis approach to identify changes that can be committed safely i.e.
they do not cause any of the tests to fail.
this allows developers to cherry pick the commits that are safe to commit and avoid conflicts .
dewan and hedge propose a new development process that allows developers to synchronously collaborate on the conflicting code and solve the conflicts before 802planning for untangling predicting the difficulty of merge conflicts icse may seoul republic of korea finishing the task.
le enich et al.
conducted a survey of developers and inferred indicators to predict the number of merge conflicts.
then they analyzed open source projects found that none of these indicators suggested by the participating developer has a predictive power concerning the frequency of merge conflicts.
while some conflicts can be prevented others are bound to occur.
for example kasi and sarma in their approach have to relax some constraints to allow some conflicts to occur when the space becomes too constrained.
le enich et al.
and cavalcanti et al.
examined and projects respectively to compare semistructured and unstructured merge techniques in terms of how many conflicts they report.
both studies found that semi structured merge techniques can reduce the number of conflicts by approximately half but not eliminate them.
our work can be useful as a guide for which constraints or conflicts can be relaxed.
.
conflict difficulty different studies have investigated ways to measure the amount of effort required to resolve a conflict.
resolution time varies significantly across projects and ranged not in hours but in days and it can be used as a proxy to measure the difficulty of conflicts difficult conflicts take more time to solve .
the orion approach by prudencio et.
al tried to minimize the number of files to belocked using the actions applied in the file and the committed actions.
their end goal was to minimize the number of conflicts that would occur at the cost of reduced development concurrency.
mckee et al.
and nelson et al.
interviewed developers and then performed a follow up survey with developers to build a detailed understanding of developer perceptions regarding merge conflicts.
they found among other things that complexity of the conflicting lines of code and file as a whole number of loc involved in the conflict and developers familiarity with the lines of code in conflict all impact how difficult developers find a conflict to resolve.
menezes et al.
used number of conflicting chunks to determine patterns that occur in merge conflicts.
all in all none of these related works deals with the main purpose of the our work the prediction of difficulty of potential merge conflicts in order to help developers prioritize merge conflicts to inspect accomplish more things given tight schedule and not waste reviewing effort on trivial merge conflict resolutions.
methodology to predict the difficulty of conflict resolutions we collect a representative corpus of merge conflicts to be examined by four different machine learning classifiers.
we use the following process during our study a we collect a sample of java projects hosted on github b we filter projects that do contain merge conflicts are inactive or toy projects c we extract the relevant attributes needed for merge conflict analysis by conducting a literature survey d we label a subset of merge conflicts manually e we conduct supervised training with four machine learning classifiers f we compare the results from each of the classifiers g we perform feature selection and h we repeat steps e and f for cross project merge conflict difficulty prediction.
we describe each of these steps in further detail in the following subsections.
.
project selection criteria we made our project selections to be applicable to the requirements of the four machine learning classifiers and to be representative of code developed in the real world.
therefore we only select active open source projects from github.
we opted to select projects that use the same programming language to control for languagespecific differences in the lines of code loc metrics program dependencies and construct comparability.
we use java as our language of choice for two reasons java is one of the most popular languages according to the number of java projects hosted on github and the tiobe index and there are more mature analysis tools available for java as compared to other programming languages.
we began by selecting java projects returned by github search mechanism without any filtering criteria.
from these projects we eliminate projects that were forked copies of other projects to prevent skewed results leaving projects in the end.
since some projects do not compile either due to syntax build errors or missing dependencies we eliminated an additional projects.
after filtering our corpus contained java projects we were successfully able to compile and run.
we follow the guidelines presented by kalliamvakou et al.
for mining git repositories.
we removed projects that were too small fewer than files or fewer than lines of code and those that were not active in the past months.
we also removed projects that do not contain merge conflicts.
these selection criteria were required since there is a long tail of small and short lived projects on github including trial projects projects with a single author or projects with no parallel development history which did not have any merge commits.
we focus on collaborative software development for this work and we therefore remove projects that are not collaborative in nature.
our final corpus had projects.
table provides a summary of project statistics for these projects including number of lines total number of commits total number of merges total number of merge conflicts number of developers and number of days that project has existed on github as of march .
table mined projects statistics dimension max min average std.
dev.
line count loc .
.
total commits .
.
total merges .
.
total conflicts .
.
developers .
.
duration days .
.
we also manually categorized the domain of the projects by looking at the project description and using the categories used by souza et al.
.
table has the summary of the domains of the projects and their percentage of representation within our corpus.
.
conflict identification we queried the repository of the projects from which we extracted commits.
this included merge commits.
since 803icse may seoul republic of korea caius brindescu iftekhar ahmed rafael leano and anita sarma table distribution of projects by domain domain percentage development .
system administration .
communications .
business enterprise .
home education .
security utilities .
games .
audio video .
git only stores information of commits but does not record instances of merge conflicts we identified merge conflicts by following branch merges and analyzing the commits involved as shown in figure .
first we identified merges as commits with two or more parents such as commit ab.
then we merged the parents of that commit anandbn using the git merge command.
if the merge was unsuccessful then abwas marked as a merge conflict m .
using this approach we identified merge conflicts.
we consider a merge conflict to be an instance when running git merge failed because of concurrent changes in the or more branches being merged.
a conflict can have multiple conflicting files each with multiple conflicting chunks.
for the purposes of this research we focused on conflicts that occurred in source code files .java .
figure identifying conflicts in git merge ab has two parents an bnthat cannot be merged automatically.
.
data collection once we identified a merge conflict we extracted additional information relevant for the analysis of the conflict such as the authors involved in the commits the commit message the files that were edited the changes that were made by using the git diff command and so on.
this was done for the parent commits anand bn as well as all commits on either branch back to the base commit i.e.
the last shared parent commit .
that is from all commits a1to anandb1tobn in figure .
performance of any prediction is dependent on the features used.
therefore we wanted to use a comprehensive set of features.
in order to get an overview of the current state of the art research on merge conflict difficulty and metrics used we conducted a literature survey.
we targeted all full conference and journal papers related to merge conflict difficulty from to inclusive that appeared in top software engineering venues icse fse ase icsm icsme msr esem tse tosem.
starting from the proceedings we searched for a set of keywords including merge conflict difficulty merge conflict resolution effort etc.
we found only papers and we analyzed what metrics were used in the studies.based on the metrics reported in the identified papers and also based on intuition we obtained factors for each conflict which we list in table .
we grouped these factors into five unique dimensions size complexity diffusion development pattern and comment .
we gathered these factors from either the git repository or we derived them by analyzing the source code when the factors are related to the process and code metrics characterized in numerical form .
since certain metrics are calculated at different levels of granularity e.g.
complexity metrics are calculated at the method level we aggregate all factors to a per conflict measurement using the average.
in table the subscript is the operation used for aggregation.
for example for cyclomatic complexity ccsum is the sum of the cyclomatic complexities of all the files modified in both branches.
we calculated file based metrics using all modified files and conflicting files.
size metrics use loc as the unit of measurement.
we also include branch and author commits patterns.
they refer to the temporal order in which commits are ordered in separate branches.
the branch pattern reflects how commits are interleaved between branches.
for example in figure commit a1in branch a followed by commit b1in branch b which yields the pattern ab.
we continue building the pattern until we reach the final abaabbba .
we then collapse identical letters yielding the final pattern ababa .
we collapse the letters because we are interested in the interleaving and not the total number of commits.
a longer pattern means that the commits were more interleaved tangled .
similarly author pattern shows how commits were interweaved between different authors.
in figure we have authors john j alice a and oscar o .
applying the same rules as the branch pattern we end up with jaoaj.
our rational for using these patterns as features in our analysis is that the more interleaved tangled a development patterns is the more difficult it should be to untangle it when resolving the merge conflict.
figure example of calculating the branch and author patterns for a merge commit.
time flows from left to right and the arrows point to a commits parent s .
.
training data labeling before training the classifiers we first manually labeled conflicts as either severe ortrivial based on their difficulty of resolution.
from the pool of conflicts we extracted and labeled a random sampling of conflicts approximately of all conflicts .
this random sample represents conflicts in distinct projects out of the total of .
in order to validate our evaluation the first two authors independently labeled conflicts based on their difficulty.
in order to 804planning for untangling predicting the difficulty of merge conflicts icse may seoul republic of korea table collected process and product metrics category metric description source sizenumber of authors number of different authors involved in all commits a1..an and b1..bn.
number of edits the number of edits of each file loc size in lines of code loc of commit a loc a b loc b loc dif f loc differences between commits anandbn complexityccsum the total sum cyclomatic complexity of all files modified in both branches ccmax the maximum cyclomatic complexity of all files modified in both branches ccav the average cyclomatic complexity of all files modified in both branches dev.
patternpattern branch length of commit pattern between branches pattern author length of author pattern between branches diffusionfiles java total number of java files modified in branches a b files total number of files modified in branches a b dependenc ysum total dependencies sum of all files modified in both branches dependenc ymax the maximum dependencies of all files modified in both branches dependenc yav the average dependencies of all files modified in both branches astdif f the number of differences of the ast trees between all modified files in both branches comment comments all the comments from commits a1.
.
.an and b1.
.
.bn.
evaluate the difficulty the authors recreated each conflict and attempted the resolution.
the authors used the time required to solve the conflict as well as the cognitive load in order to classify the merge conflict as difficult or severe ortrivial.
in order to validate the resolution we compared our merge conflict resolution with the one that was checked in the version control system.
we considered the developer s merge conflict resolution as the oracle as it is most likely to be correct given their in depth knowledge of the code.
in all cases our resolution was functionally equivalent to the developer s resolution.
listing shows part of the author s resolution for a severe merge conflict which is functionally identical to the developers resolution1.
we checked for agreement by using cohen s kappa and we achieved an irr of .
.
the two authors then independently coded the rest of the conflicts in order to build the training set.
.
feature selection to select the appropriate metrics we carry out an analysis of potential multicollinearity between the metrics.
previous research demonstrated that many process and source code metrics are correlated both with each other and with lines of code loc .
ignoring such correlations would lead to increased errors in the estimates of model performances and increased standard errors of the predictions .
we checked for multicollinearity using the variance inflation factor vif of each predictor in our model for our data set.
vif describes the correlation between predictors.
a vif score between and indicates moderate correlation with other factors and these selected the predictors are with vif .
this step was necessary since the presence of highly correlated factors forces the estimated regression coefficient of one variable to depend on other predictor variables that are included in the model.
out of factors had vif so we ended up using the remaining factors for building the classifiers.
we further investigate the resulting eleven factors in their effectiveness in predicting the difficulty of a merge conflict and report the results in section .
we find that prior work has also used a subset of these factors as a proxy for difficulty of a conflict which is encouraging for our own work.
.
machine learning we trained and tested our sample using different machine learning techniques support vector machines svm logistic regression multi layer perceptron perceptron and bayes network bayesnet .
for all techniques we used a fold cross validation on our sample of labeled conflicts.
we used a wide range of learning techniques to reduce the risk of dependence on a particular algorithm or implementation.
.
.
bayesnet.
we use the bayes network algorithm as we expected features to not be independent and bayesian network does not have assumptions regarding independence.
for example loc dif f andfiles number of files share a relation as editing more files will also increase the loc edited.
also bayesian networks can represent richer models compared to naive bayes classifiers.
we used the simpleestimator to calculate the conditional probabilities used by the bayes algorithm.
finally bayesnet uses a hill climbing greedy algorithm for evolving combined with a k2 search algorithm to create its network.
we configured with a batch size of .
.
.
binomial logistic regression.
for the binomial logistic regression we started with a full model using all attributes from conflicts.
this was followed by a model selection using akaike information criterion aic which estimates the information loss between models in comparison to the original.
it ultimately selects 805icse may seoul republic of korea caius brindescu iftekhar ahmed rafael leano and anita sarma f a ct o ry byte x i d g l o b a l i d f a c t o r y c r e a t e x i d g l o b a l i d f a c t o r y .
.
.
no c o n f l i c t s i n t h e s e l i n e s txmanager new readonlytxmanager xadatasourcemanager x i d g l o b a l i d f a c t o r y l o g g i n g .
getmessageslog readonlytxmanager .
c l a s s listing authors resolution of a severe merge conflict.
in this example the developers made two concurrent refactorings to the readonlytxmanager constructor one of the refactorings introducing a new parameter.
the best model based on both the fit of the model and the information lost.
we then use the selected attributes to build the new final model on which we evaluate.
.
.
support vector machine svm .
based upon an assumption that conflicts would be linearly separated across factors we selected svm.
our svm uses the standard radial bases function rbf kernel and for the other parameters we performed a grid search to choose the best classification found.
this configuration result had a 000cache size a size a gamma of .
a c of and one maximum iterator.
.
.
multi layer perceptron.
we used a multi layer perceptron to see if it could leverage hidden relationships not explored in the other algorithms.
we configured our perceptron with a .
learning rate a .
momentum and epochs.
the perceptron would terminate its validation testing after not being able to reduce its error times in a row.
.
.
bagging.
we also used auto weka for identifying the best classifier which automatically searches through the joint space of weka s learning algorithms and their respective hyperparameter settings to maximize performance using sequential model based optimization a bayesian optimization method .
though there is one python based implementation called auto sklearn we chose auto weka because it comprises a larger space of models and hyperparameters compared to auto sklearn.
this ended up identifying bagging bootstrap aggregating as the best technique.
bagging is an ensemble based approach that uses multiple models to fit the bootstrap samples generated from the original data and then uses voting for classification.
repeated incremental pruning to produce error reduction ripper was identified as the base learner accessible at weka.classifiers.rules.jrip by autoweka.
these results were generated by running auto weka with random seed for hours.
.
evaluation we report the standard precision recall and auc area under the receiver operating characteristic curve to asses the performance of the prediction models because it is independent of prior probabilities .
also auc is a better measure of classifier performance than accuracy because it is not biased by the size of test data.
moreover auc provides a broader view of the performance of the classifier since both sensitivity and specificity for all threshold levels are incorporated in calculating auc.
other work related to prediction have used auc for comparison purposes .
we list the formula used for calculating precision recall and f measure below.
the auc curve is created by plotting the recall against the false positive rate fpr at various threshold settings.
we list the formula of fpr also.
precision p a measure of whether the severe predictions were actually difficult.
precision tp tp fp recall r a measure of the percentage of severe instances that the approach managed to correctly predict.
recall tp tp fn false positive rate fpr a measure of the ratio of the number ofsevere conflict wrongly categorized and the total number of actual severe conflicts.
fpr fp fp tn .
cross project prediction cross project prediction is important for some projects specially projects that do not have historical data to perform any significant training.
hence we investigated whether it is feasible to perform cross project training following the method used by rahman et al.
.
we do so by training models on one project and testing on all other projects ignoring time ordering.
results here we discuss the results of our study by placing them in the context of three research questions which investigate the the ability to predict the difficulty of a conflict rq1 factors that are useful in determining the difficulty of a conflict rq2 and whether we can perform cross project merge conflict difficulty prediction rq3 .
.
rq1 how well can we predict the difficulty of merge conflicts?
to answer this research question we trained four different machine learning algorithms bayes network bayesnet logistic regression support vector machine svm and multi layer perceptron perceptron .
we also used auto weka which automatically searches through the joint space of weka s learning algorithms and their respective hyperparameter settings to maximize performance and identify the best classifier.
the algorithm with the best performance according to table is bagging bootstrap aggregating with ripper as the base learner which has the highest auc .
.
bagging is closely followed by bayes network which still performs better than chance with .
auc.
on the other hand 806planning for untangling predicting the difficulty of merge conflicts icse may seoul republic of korea table performance of the classifiers.
bagging has the highest auc at .
.
precision recall auc svm .
.
.
l.r.i0.
.
.
perceptron .
.
.
bayes network .
.
.
bagging .
.
.
ilogistic regression svm performs worst .
auc .
table shows the results in terms of precision recall and auc.
observation the difficulty of resolving merge conflicts can be predicted with an auc value of .
when using a bagging classifier.
additionally we use our bagging model on the full corpus of conflicts to see the characteristics of the merge conflicts those that are predicted as severe ortrivial.
the model identifies of the conflicts as severe and the rest are classified as trivial.
in our context precision shows how well we correctly predict severe conflicts recall shows how many of the severe conflicts we are able to find.
we posit that in our scenario precision has a higher priority than recall.
this is because incurring more false positives is likely to make the developer to distrust the tool.
as bagging outperformed all other classifiers we report the precision recall and auc of bagging separately for the two classes severe and trivial in table .
table results of the bagging classifier per class class precision recall auc severe .
.
.
trivial .
.
.
observation difficult merge conflicts can be predicted with a precision of .
when using a bagging classifier.
.
rq2 which characteristics of merge conflicts are associated with its difficulty?
in section .
we show that it is possible to predict the difficulty of a merge conflict with high accuracy.
our next step is to understand what are the characteristics of difficult conflicts.
for this purpose we use feature subset selection fss .
specifically we use wrapper based methods which considers the selection of a set of features as a search problem.
different combinations of features are prepared evaluated and compared to other combinations .
wrapper methods are also able to detect the possible interactions between features.
in this technique a predictive model is used to evaluate the combinations of features and a score is assigned based on accuracy of the model.
we used ripper as the predictive model for feature selection since it was identified as best classifier for predicting merge conflict difficulty as explained in section .
.
using fss we obtain a set of ten factors from our initial set of .
the ten selected factors encompass all the four metric categories complexity diffusion size and development pattern to which theoriginal factors belonged see table .
all these factors can be calculated before the conflict occurs.
this suggests that each of these categories are relevant in predicting difficult merge conflicts even before the developer faces the conflict.
table presents additional information about these ten factors.
two of these factors include complexity metrics such the ccsum and the ccav referring to the mean and the sum of the cyclomatic complexities of all files modified in both branches.
this suggests that the complexity of the code is an important metric that affects the difficulty in resolving a conflict.
however calculating complexity metrics require specialized standalone analysis tools.
so in the worst case developers therefore have to guestimate the complexity of the code based on their own experience.
table feature selection results sorted based on relative importance category metric fsshuman perceived importance complexity ccsum diffusion dependenc ymax diffusion dependenc yav complexity ccav diffusion astdif f size loc dif f size loc size number of authors 8not mentioned dev.
pattern pattern branch 9not mentioned dev.
pattern pattern author 10not mentioned observation we identify a subset of ten factors that include complexity diffusion size and development pattern that can predict the difficulty of merge conflicts.
table also shows that development process related metrics are less influential number of authors pattern branch andpattern author compared to code related metrics ccsum dependenc ymax dependenc yav ccav astdif f loc dif f and loc .
this led us to investigate whether there is any difference between these two types of metrics when it comes to predicting merge conflict difficulty.
we perform this analysis since both process and product metrics have known differences in prediction capability in the context of defect prediction .
we built the same set of classifiers shown in table once using only the process related metrics and once using only code related metrics.
tables and shows the results in terms of precision recall and auc.
surprisingly both process and code related metrics have similar prediction capabilities auc values of .
vs. .
unlike defect prediction where process related metrics were found to be more powerful .
in the last column of table we report the factors that mckee at al.
identify as factors used by software practitioners to gauge merge conflict difficulty.
they identified these factors through a 807icse may seoul republic of korea caius brindescu iftekhar ahmed rafael leano and anita sarma table performance of the classifiers built using only process related metrics number of authors pattern branch and pattern author .
precision recall auc svm .
.
.
l.r.i0.
.
.
perceptron .
.
.
bayes network .
.
.
bagging .
.
.
ilogistic regression table performance of the classifiers built using only code related metrics ccsum dependenc ymax dependenc yav ccav astdif f loc dif f andloc .
precision recall auc svm .
.
.
l.r.i0.
.
.
perceptron .
.
.
bayes network .
.
.
bagging .
.
.
ilogistic regression survey of software practitioners.
except for the complexity category none of the other top features mentioned by practitioners are in the top features identified by fss and vice versa.
clearly there is a disjoint between the human perceived features and machine learned features.
we discuss this further in section .
observation we identify differences between human perceived features and machine learned features for predicting the difficulty of merge conflicts for example diffusion which is perceived as unimportant by developers but picked up as important by our model.
.
rq3 is cross project training possible to predict difficult merge conflicts?
in rq1 we tested our models using a fold cross validation with all our training data conflicts .
however some projects may not have historical data to perform any significant training.
crossproject prediction has been investigated in other areas of software engineering such as defect prediction .
however to the best of our knowledge no one has investigated the applicability of cross project merge conflict difficulty prediction.
we followed the method used by rahman et al.
to perform cross project merge conflict difficulty prediction.
we train models on one project using our best algorithm bagging using ripper and test on all other projects.
figure shows the portability of models across projects for different sets of evaluation metrics precision recall f measure and auc .
performance clearly degrades in cross project settings in comparison to bagging using ripper algorithms performance of .
auc shown in table .
figure precision recall f measure and auc for crossproject training.
observation using cross project training it is possible to build model that can be applied to an individual project with better than chance results auc .
on average .
discussion centrality matters the goal of our study is to investigate whether it is feasible to predict the difficulty level of a merge conflict by using automated machine learning techniques.
one factor that emerged as relevant is dependenc y. a file that has high dependenc yis likely to be highly coupled with other parts of the code and therefore has high centrality.
this is problematic for two reasons.
first as the file is central it has more reasons to change.
for example a class with multiple functionalities i.e.
a god class is more likely to be changed for any kind of modification of the software.
second the more a file gets changed the more it s likely to be involved in a merge conflict.
moreover the conflict is likely to contain disparate changes.
this presents a challenge as the developer has to understand all the changes involved before resolving the merge conflict.
both our machine learning classifier and developers agree that this is a factor that determines the merge conflict resolution difficulty.
tangled changes in our analysis we find that pattern author is a significant factor in predicting merge conflict difficulty.
a reason for this might be that the more authors that are involved in the development process the more disparate the conflicting changes are because each developer is likely working on a different functionality.
so whenever a conflict occurs the developer has to understand the broader context of the changes before attempting to resolve the conflict.
similarly a longer branch pattern pattern branch means that the changes are more tightly tangled.
this makes the changes more difficult to untangle when resolving the conflict.
interestingly while 808planning for untangling predicting the difficulty of merge conflicts icse may seoul republic of korea our classifier identified these as important factors the developers did not.
this is an indication that developers are not aware that this could be a potential pain point.
size does matter our classifier also identified the size difference loc dif f between the two branches as a relevant factor for conflict difficulty prediction.
this is intuitive as the more lines are changed the harder it is to understand the changes that were made in that change set .
this in turn makes it more difficult to the place that change set in the context of the rest of the code base.
when dealing with the difference in terms of ast nodes astdif f this becomes even more important as the ast node difference is more likely to highlight semantic changes.
in this case both the classifier and the developers agree that the size difference between the two branches is an important factor in determining the merge conflict difficulty.
it s complicated it s well known that code with higher cyclomatic complexity is more difficult to understand.
therefore it s not surprising that our classifier identified ccsum as a significant factor for identifying the difficulty of a merge conflict.
another aspect is that code with high cyclomatic complexity is usually indicative of a complicated control structure.
therefore conflicts in that area are more likely to be semantic in nature.
prior research has shown that areas of code affected by such conflicts are more likely to be buggy .
in this case both the classifier and the developers agree that this is an important factor.
learn from others our final observation is that models are portable between software projects.
this is an indication that the factors that contribute toward merge conflict difficulty are more or less project independent.
therefore our technique can be applied to new projects or to projects with little development history and still prove beneficial to developers.
implications our findings have multiple implications for tool builders researchers and practitioners.
.
researchers our model for predicting merge conflict difficulty achieved an auc value of .
when predicting the minority class which is a high value compared to a baseline of random classification .
however there is still room for improvement.
the research community should focus on identifying different types of factors social product and process and investigate their effect on overall prediction accuracy with the goal of improving the overall prediction accuracy.
our results inform future research by providing insights into the factors that are associated with the difficulty of a merge conflict.
projects share similar features as demonstrated by the moderate performance of cross project merge conflict difficulty prediction.
this also indicates that the prediction process can be bootstrapped even for project that lacks history.
we recommend that researchers should also focus on identifying the best project selection criteria for bootstrapping cross project prediction.
for bootstrapping we should use similar projects.
however what constitutes as a similarity metric between the project in this context is still an open research question.
our results show that cyclomatic complexity is the most important metric when predicting difficulty.
so projects with similar cyclomatic complexity should could used forbootstrapping.
however further investigation is required to make any conclusive or definitive remarks.
we also found that auto weka which automatically searches through the joint space of weka s learning algorithms and their respective hyperparameters helped us to identify the best classifier and increased the auc value from .
to .
.
our finding is inline with the findings of other researchers who have shown the benefit of parameter optimization in improving classifier performance.
therefore we recommend that researchers using machine learning classifiers should seriously consider parameter optimization to ensure the best performance of the classifiers.
.
tool builders when looking at the types of factors that make a merge conflict difficult we identified categories relating to the complexity of the code complexity extent of the change diffusion and the length of branch pattern development pattern .
the complexity anddiffusionmetrics are already used by researchers and tool builders for merge conflict prediction.
however we are the first to associate the length of a branch pattern to merge conflicts and their difficulty.
further research can help identify threshold of branch pattern after which a merge conflict becomes severe.
tool builders can use such thresholds as a criteria to filter and prioritize the notifications about potential conflicts.
this will not only help users manage the information load but also will have impact on the quality of the final product .
.
developers our results indicate that the more tangled a piece of code is the more difficult it will be to resolve a conflict related to that code.
so developers can use the length of pattern author andpattern branch in deciding merge conflict resolution strategy.
moreover it s more likely that a code with bigger pattern length has diverged a lot from the initial point and has become difficult for any individual to understand completely.
in such cases it will be more productive to do a collaborative merge instead of a developer performing the merge by herself.
we also found that all ten significant factors in determining merge conflict difficulty can be collected even before the merge conflict actually occurs.
current awareness tools are already collecting these information when predicting emerging conflicts.
therefore without further overhead awareness tools can use our model to predict the difficulty of the emerging conflict which can be then used as a prioritization criteria when notifying users.
tools can also recommend developers who are most suited to resolve a conflict based on the historical data of merge resolutions.
the rationale would be that developers with more experience of resolving difficult conflicts in the past are suitable candidates for resolving a severe conflict as compared to someone who lacks the experience.
threats to validity our research findings may be subject to the concerns that we list below.
we have taken all possible steps to neutralize the impacts of these possible threats but some couldn t be mitigated and it s possible that our mitigation strategies may not have been effective.
809icse may seoul republic of korea caius brindescu iftekhar ahmed rafael leano and anita sarma our samples have been from a single source github.
this may be a source of bias and our findings may be limited to open source programs from github.
however we believe that the large number of projects sampled more than adequately addresses this concern.
another threat to our findings is that git tracks the version history of a project as it occurred but it also allows history rewriting using the rebase command.
it is known that some development teams use rebase instead of merge to reintegrate branches which means that may have missed merges in our analysis and the number of merges we analyzed is a lower bound as compared to the actual total number of merges in the projects.
although we use factors spanning across six categories there are likely other features that we did not measure.
for example we suspect that the design patterns of a program might influence the likelihood of a conflict resolution being difficult.
we plan to expand our metric set to include additional categories in future work.
our training and testing data had to be manually labeled since this information is not currently available in cm systems or issue trackers.
therefore our labels may not accurately represent the real merging difficulty because of lack of domain expertise.
our labeling process included inter rater reliability to prevent individual bias and to reduce this threat.
additionally the experience and familiarity with the source code and the project can make a conflict difficult to resolve for one developer but simple for another.
as we had multiple researchers and we also had a high inter rater agreement we assume this should minimize the aforementioned threat.
another threat would be that we excluded non source files from our manual analysis e.g.
configuration xml file etc.
but changes to non source files can have impact on the program s execution if these files are involved in the build deploy process or for code generation and ultimately make the merge resolution difficult.
conclusions and future work in this empirical study the first of its kind we investigated the different aspects that can impact the difficulty level of resolving merge conflicts.
we evaluated five different classification techniques from different families and identified bagging using ripper as the base learner to be the best model with an auc of .
.
we also identified a set of ten metrics that are most influential while predicting the difficulty level of a conflict which include metrics about the complexity of the code the size of the change and development pattern etc.
we also found that there is a disconnect between the factors developers use to gauge the difficulty of a conflict and the factors our automatic classification technique identified as important.
finally we showed that we are able to perform cross project merge conflict difficulty prediction with median auc of .
.
therefore our results show that we can bootstrap prediction in projects with no labeled data or only small amount of history by training on other projects.
our study opens a new avenue in software engineering research related to predicting the difficulty level of a merge conflict and help developers plan the merge conflict management process efficiently.
we also provide actionable implications for researchers tool builders and practitioners to harness the results of our study.
in future work we hope to explore whether these factors can beseamlessly merged into tools or techniques to assist developers workflows.