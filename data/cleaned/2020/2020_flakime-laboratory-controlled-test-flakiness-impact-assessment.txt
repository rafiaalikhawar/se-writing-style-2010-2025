flakime laboratory controlled test flakiness impact assessment maxime cordy snt university of luxembourg luxembourg maxime.cordy uni.lurenaud rwemalika snt university of luxembourg luxembourg renaud.rwemalika uni.luadriano franci snt university of luxembourg luxembourg adriano.franci uni.lu mike papadakis snt university of luxembourg luxembourg michail.papadakis uni.lumark harman meta platforms inc. and university college london united kingdom mark.harman ucl.ac.uk abstract much research on software testing makes an implicit assumption that test failures are deterministic such that they always witness the presence of the same defects.
however this assumption is not always true because some test failures are due to so called flaky tests i.e.
tests with non deterministic outcomes.
to help testing researchers better investigate flakiness we introduce a test flakiness assessment and experimentation platform called flakime.
flakime supports the seeding of a controllable degree of flakiness into the behaviour of a given test suite.
thereby flakime equips researchers with ways to investigate the impact of test flakiness on their techniques under laboratory controlled conditions.
to demonstrate the application of flakime we use it to assess the impact of flakiness on mutation testing and program repair the prapr and arja methods .
these results indicate that a flakiness is sufficient to affect the mutation score but the effect size is modest while it reduces the number of patches produced for repair by up to of repair problems a devastating impact on this application of testing.
our experiments with flakime demonstrate that flakiness affects different testing applications in very different ways thereby motivating the need for a laboratory controllable flakiness impact assessment platform and approach such as flakime.
acm reference format maxime cordy renaud rwemalika adriano franci mike papadakis and mark harman.
.
flakime laboratory controlled test flakiness impact assessment.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction test flakiness is the property of a test case and system under test that the test can pass on one occasion yet fail on another without the tester changing anything other than the fact that the test is executed on two different occasions.
this behaviour has a number .
this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
causes such as non determinism in the system under test instability in the infrastructure that provides the test environment and variability in the results produced by services and components upon which the system under test depends.
flakiness has a profound impact on all applications of software testing because it increases test signal uncertainty the tester can never be sure that a failure is genuine and this may waste effort investigating false positives or lose important signals from switching off flaky tests .
companies such as google and facebook have highlighted the problem of test flakiness indicating that it is one of their primary concerns for software testing.
some companies have also launched specific challenges to the research community to tackle this problem .
flakiness impacts each form of testing in different ways.
e.g.
in mutation testing the mutation score will vary dependent on flakiness confounding this variability with the influence of the quality of the test that the score seeks to assess.
in automated program repair the certainty we have that a repair is correct will be affected by flakiness as will be the ability of the repair technique to localise the point at which to attempt a patch.
indeed it has been argued that allforms of testing need to be reformulated to take account of flakiness in order to find techniques that can cope well in the presence of unavoidable flakiness .
this means that testing techniques need to be re investigated under flakiness conditions to assess their robustness on varying degrees of flakiness.
in order to address the problems posed by flakiness researchers need ways to investigate the impact of flakiness.
naturally studies should be conducted on real world systems to explore this impact .
however researchers also need the ability to experiment with flakiness in laboratory controlled conditions.
such laboratory control would circumvent the limited number availability and reproducibility of flaky test datasets and allow researchers to report results on the impact of varying degrees of flakiness on the test techniques they propose and introduce.
to address this need we introduce flakime a tool that allows researchers to seed a controlled degree of flakiness into a given test suite and system under test.
flakime equips researchers with a laboratory controllable environment in which they can simulate a rich set of flakiness scenarios and conditions.
specifically flakime can ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa maxime cordy renaud rwemalika adriano franci mike papadakis and mark harman be seamlessly integrated into any java project as a maven plugin without requiring any modification to the code of the project trigger flaky failures during test executions at any point during these executions rely on user defined flakiness prediction models to associate tests with a likelihood to be flaky control the degree of injected flakiness via a parameter that defines the probability threshold above which execution of any flaky test results in a flaky failure.
this paper introduces flakime and illustrates its application to the assessment of flakiness impact on mutation testing implemented in pit and generate and validate automated program repair g v apr more precisely on the prapr and arja repair methods.
to demonstrate the capability and the usefulness of flakime we combine it with a state of the art flakiness prediction approach that identifies tests that could be flaky based on their code similarity to real flaky tests.
that is the approach predicts the probability that a test is flaky based on the terms tokens this test contains.
previous studies have demonstrated that these vocabularybased approaches can effectively predict flaky tests with an f1 score greater than .
in flakime we use this flakiness prediction approach to build a theoretical model of flakiness that identifies candidate tests to inject realistic flakiness into.
we therefore obtain the following benefits each test has a different probability to trigger a flaky failure.
tests that execute similar pieces of code have a dependent probability to trigger a flaky failure.
flaky failures can occur at any location in the tests and this occurrence depends on the code of the test.
there is to date no evidence that this model matches the actual behaviour of real flaky tests and for this reason experimental results obtained with flakime may differ from results that would be obtained with real flaky tests.
flakime enables laboratory controlled experimentation with extremes and therefore it is not directly concerned with the specific settings characteristics of a particular environment context.
as such we give experimental controllability over the degree of flakiness so that researchers can stress test their techniques and study their limits.
it remains important to validate research techniques in practice through empirical studies with real flaky tests.
what flakime offers is the capacity to conduct complementary experimental analyses under broader and more controllable settings than what real but rare flaky tests enable.
previous research on flakiness has helped to identify the main causes of flakiness and has introduced techniques to either reduce or ameliorate its effects.
however hitherto no systematic way of evaluating the sensitivity of arbitrary software testing problems to flakiness has been introduced.
we fill this gap and report results on the use of our flakime platform to yield insights on two software testing problems.
thus our key contribution is the evidence that the use of flakime leads to interesting and actionable results.
specifically we use flakime to perform two laboratorycontrolled experiments on 1we will refer to g v apr simply as automated program repair .
mutation testing we show how flakime allows us to investigate the sizeof this effect.
in particular using flakime we can reveal that a a small amount of flakiness can affect the mutation score of flakiness failures yields mutation score variations between to b when the degree of flakiness increases the mutation score follows an asymptotic growth where additional increases in the degree of flakiness have a rapidly diminishing impact on mutation testing.
c taken together with the above findings we form a takehome message for mutation testing researchers flakiness is a potential problem but with limited effect.
although researchers should always take into account the flakiness effects on the mutation scores they report the results suggest that it is not sufficient to poison the well .
automated program repair for the application of repair we found that the impact of flakiness is stronger than it is on mutation testing.
specifically we show that the same degree of flakiness has different effects on different systems.
this indicates that research on automated repair needs to analyze how sensitive to flakiness their test suites and subjects are.
our results show that flakime can be used to pre select suitable subjects and to validate the key decisions made by the studied techniques.
more precisely we show that a deterministic repair is increasingly affected by the number of tests covering the produced patches.
in our studied projects only of patches remain proposed by the techniques as a result of flakiness.
b non deterministic repair experiences a drop in the number of patches produced by to100 with the worst case total failure drop occurring for of the programs studied.
exploiting knowledge about the non flaky failures mitigates this effect yielding between to times more patches and even allowing the repair of cases that were before hindered by flakiness.
related work previous work on test flakiness has primarily focused on identifying its causes.
luo et al.
were the first to propose a formal classification of the root causes of test flakiness.
relying on this definition recent studies highlight various degrees of flakiness in industrial code bases e.g.
of flaky failures at google .
other studies have been conducted addressing non deterministic system such as machine learning models different programming languages or automatically generated tests .
the follow up work aimed at the automated identification of flaky tests and the development of tools that identify and or remove flaky tests such as ifixflakies idflakies rootfinder deflaker shaker flash or the work of malm et al.
which proposes to insert delays in tests to increase their robustness.
the presence of flaky tests degrades the effectiveness of test suites.
therefore flaky tests should be fixed controlled early ideally as soon as flakiness is introduced .
facing the need to detect 983flakime laboratory controlled test flakiness impact assessment icse may pittsburgh pa usa flaky behavior before it manifests itself researchers started investigating flakiness prediction models based on supervised learning.
king et al.
rely on a bayesian network model defined over code metrics related to flakiness.
similarly flast uses code metrics to feed a k nearest neighbours model.
recent work has shifted from code metrics to vocabulary based approaches.
pinto et al.
followed by haben et al.
and camara et al.
developed random forest models that learn from test code tokens in order to predict new flaky tests based on how similar their vocabulary is to known flaky tests.
finally approaches like flakeflagger and the classifier presented by lampel et al.
rely on dynamic features observed at runtime instead of the static features used in the other studies.
with flakime researchers have now the ability to leverage such flakiness prediction models in order to control the degree of flakiness present in test suites and assess the sensitivity of testing techniques to this flakiness.
the inherent non determinism of flaky tests combined with the specific environmental conditions under which flakiness manifests itself makes it inherently difficult to collect flaky test datasets and even more difficult to consistently reproduce flaky failures.
as a result few studies have analysed the effects of flakiness on software testing techniques.
mutation testing.
shiet al.
evaluated the effects that nondeterministic coverage in test executions can have on mutation score and proposed a way to reduce this problem.
the study shows that the mutation score can vary up to when ignoring nondeterminism.
these findings are based on in vivo observations of non determinism in open source software projects.
none of the projects they studied exhibited flaky behavior in the test outcome only in the test coverage.
this highlights the difficulties to construct flaky test datasets suitable for research and to reproduce real flakiness occurrences.
therefore we complement such studies with flakime and its capability to produce in vitro laboratory controlled flakiness.
with flakime we simulate test failures with different flake rates and observe the asymptotic growth in mutation score as the flake rate increases.
our results corroborate shi et al.
s general finding that a small amount of non determinism can affect the mutation score.
our novel finding is that the effect of flakiness saturates rapidly when the flakiness rate increases.
automatic program repair.
qinet al.
have analyzed the impact of specific causes of flakiness on apr.
like our study they rely on the defects4j dataset as a source of buggy programs the most established dataset in apr.
however their analysis is limited to flaky failures caused by the use of different jdk versions.
their results show that a few flaky failures .
of all test executions negatively impact the suspicious statement localization in more than of the programs and reduce the repair capabilities of the apr tool by up to .
by contrast our study introduces a broader set of flakiness instances in defect4j tests based on how similar their code is to flaky tests observed in the field.
we investigate in depth how the rate of occurrence of flaky failures affects each step of the apr process from fault localization to patch validation.
while qin et al.
observe negative effects these effects are somehow limited apr can still fix bugs .
our novel finding is that scarce flakiness occurrences can have a profound effect and even annihilate the capacity of apr methods.
we also identify the key components that are affected and suggest mitigation strategies.fault localization.
vancsics et al.
have studied the effects of flakiness on fault localization.
they seeded flakiness in tests suites by making tests randomly flake with a uniform distribution whereas we use calibrated prediction models from the literature and a varying flakiness rate in order to observe trends and extremes.
their observations are aligned with ours flakiness has a significant impact and falsely alters the ranking of suspicious statements.
they also show that different ranking formulae are impacted differently.
in our study we do not observe the effects of flakiness on fault localization in isolation but within the broader use case of apr.
we further demonstrate that the impact of flakiness on fault localization is the main factor explaining the reduced effectiveness of apr and propose mitigation strategies accordingly.
finally our platform is generic and enables experimentation on different testing techniques including but not limited to fl.
test selection prioritization.
leong et al.
has reported that flaky tests significantly mislead the test selection algorithms used in google s continuous integration environment.
lam et al.
have conducted a focused study on the impact of test order dependency a specific form of flakiness.
though our study focuses on mutation testing and apr flakime can also support sensitivity studies on other testing techniques through the seeding of varying degrees of flakiness.
flakime flakime2is a flexible tool through which researchers can simulate a rich set of scenarios and conditions for flakiness.
it is distributed as a maven plugin.
hence testers can seamlessly integrate flakime into any java project as part of the maven configuration without requiring any modification to the code of the project or its environment.
flakime instruments the bytecode of the tests by introducing a payload that can trigger flaky failures at any test execution point i.e.flake points.
flakime therefore introduces test failures but does not make some tests incorrectly pass.3these test failures are introduced at specific test code locations named flake points .
concretely a flake point can be any location that corresponds to the end of a basic control flow block i.e.
a sequence of contiguous instructions that does not contain jump branch instructions except the last one .
the probability that a flaky failure occurs at a specific flake point depends on a user defined flakiness prediction model and a real valued parameter named the nominal flake rate .
flakime implements a test instrumenter that injects probable program failures at any flake point during test execution.
to simulate a flaky failure flakime raises an unchecked exception thereby causing the test to fail.
the exception is guarded by a probabilistic condition which depends on the probability of the test execution to flake at this point.
to qualify the probabilistic condition that guards the unchecked exception we rely on three concepts the test flakiness probability pf lakiness represents the likelihood that some part of test code is flaky.
this value is determined by the flakiness prediction model integrated into flakime.
3we will consider extending flakime with this capability in the future.
984icse may pittsburgh pa usa maxime cordy renaud rwemalika adriano franci mike papadakis and mark harman the nominal flake rate frn represents the probability that the execution of a flaky test results in a flaky failure.
this rate is a parameter that researchers can set and control in their experiments to conduct sensitivity analyses.
the effective flake rate fre is the actual probability for the execution of any test to result in a flaky failure.
it is defined asfre frn pf lakiness .
therefore the higher the user sets the nominal flake rate the higher number of flaky failures this user will observe.
in addition to the nominal flake rate that flakime users control entirely the users can easily plug in their own flakiness prediction model.
for instance such models can learn from previously observed flaky tests to predict the likelihood that unseen test instances are flaky .
flakime can benefit from studies characterizing flakiness to build specific environmental perturbations that mimic real world flakiness as it has been observed in the field.
there currently exists no perfect model of real flaky tests in the literature.
hence the realism of the flakiness that flakime injects is bound to the evidence that the integrated model matches real flaky tests.
hence researchers should use flakime with the model that best reflects their environmental settings and working assumptions and be aware that the generality of their conclusions is limited to the validity of these working assumptions in the real world.
we discuss this further in section threats to validity.
in our study we take advantage of a generic state of the art approach that predicts which tests could be flaky based on their test code tokens similarity with previously observed real flaky tests .
the advantage of this model is that it applies to pieces of code of any length.
it can therefore be used to define dependent probabilities on multiple flake points of the same test.
also the model being based on code vocabulary tests that execute similar pieces of code have a dependent likelihood to be flaky.
more specifically the approach of pinto et al.
uses a set of flaky tests extracted from open source projects to train a classifier.
they represent a training sample with its binary label test being flaky or not and its token feature vector i.e.
for each token contained in any of the flaky tests whether the token is present.
to compute this vector the approach tokenises the code of the test removes a predefined list of stop words and builds a boolean vector such that each element in the vector indicates the presence or absence of the corresponding token in the test.
with this encoding pinto et al.
trained a supervised classification model to classify tests as flaky or not thus based on the terms tokens this test contains.
among the all classification models they have considered in their empirical evaluation pinto et al.
have demonstrated that a random forest classifier yielded the best prediction performance the highest precision and f1 score .
in flakime we leverage the approach of pinto et al.
that predicts the probability that a test is flaky in order to determine the flakiness probability pf lakiness of a test based on the terms it contains.
to use this approach in our study we have extended the initial training set with the token of our test subjects i.e.
the project under flakiness simulation and we trained from scratch the random forest following pinto et al.
s protocol .
then we use the model to determine the test flakiness probability at any flake points in the tests of the project under study.
more precisely we use themodel to compute the probability that a given test is flaky.
we then spread the probability mass over all flake points in the test based on the vocabulary of the code that precedes each flake point.
hence flake points lying at statements found in many flaky tests will receive a higher probability than flake points that include no risky operations.
figure shows the distribution of the test flakiness probability that we obtain for our studied projects see section .
.
we observe that the shape of the distribution changes for each project.
when compared to chart figure 1a the project time figure 1d has a median flakiness probability of0.45while the former has a median value of .
.time exhibits less variance across tests and is on average more prone to flakiness than chart .
it is noteworthy that these probability values do not correspond to the actual probability that tests executions flake but they rather serve as a means of distinguishing the inherent proneness of different tests to be flaky.
flakime weights each of these probability values with the nominal flake rate thereby allowing users to control the effective flake rate.
flakime is designed to be an extensible tool that can be used by researchers to test novel methods under different assumptions.
thus we design flakime so that it can easily incorporate new flakiness prediction models to account for such assumptions.
to do so we integrate the concept of flakikess model which is an abstract java class that can be extended to easily integrate additional models.
this abstract class contains three methods preprocess which is run before the instrumentation and provides information about the tests to be instrumented geteffectiveflakerate which returns the effective flake rate given a test method and a line number and postprocess which allow for eventual cleaning or any other post processing operation.
to ease the integration of other methods models the framework provides static information about the test code and an interface to integrate modules assigning the flakiness probabilities.
the impact of flakiness on software testing techniques .
mutation testing mutation testing determines the proportion of mutants artificially injected defects causing tests to transition from passing to failing.
starting from a test suite t1 ... t nthat passes on the original program p mutation testing generates mutants m1 ... m kby altering the syntax of pand then runs the test suite on the mutants.
the test suite t1 ... t nkills a mutant mjif the execution of at least one test onmjfails.
the mutation score number of mutants killed divided by the number of mutants is a frequently used metric for measuring test thoroughness .
one can see this as an n k matrix where each cell is related to a test tiand mutant mjpair and denotes whether tikilledmjor not.
in the absence of flakiness such a matrix is determined by the tests and the mutants.
however in the presence of flakiness things change arbitrarily a flaky test that passes on the original program can fail on a mutant leading to a kill.
thus running the test suite with flakime results in swapping the status of some mutants.
the probability of swapping in this case is equivalent to the effective probability that the concerned tests flake.
hence failed flaky 985flakime laboratory controlled test flakiness impact assessment icse may pittsburgh pa usa .
.
.
.
.
.
probability0100200300number of t ests a chart .
.
.
.
.
.
probability0100200300number of t ests b lang .
.
.
.
.
.
probability0100200300number of t ests c math .
.
.
.
.
probability0500100015002000number of t ests d time figure distribution of test flakiness probability assigned by the vocabulary model employed in flakime to each test subject.
test executions artificially inflate the mutation score causing an overestimation of the test suite s fault revealing potential.
.
program repair automated program repair aims at generating patches modifications of the software code that fix bugs for programs with bugs witnessed by failing test cases.
in this line of work effectiveness is measured by the number of valid patches generated within a given time limit.
a valid patch is one that compiles and passes all tests including the initially failing tests that witnessed the bug .
since the validity of patches is determined by the test case results it is interesting to see the extent to which flakiness can impact patch selection.
in other words we would like to check the sensitivity of repair methods to noisy signals caused by flakiness.
flakime impacts this validity check by making a test fail randomly.
in the repair process such a patch could be mistakenly discarded.
an increasing number of repair methods have appeared over the years .
we select two recent methods that exhibit fundamental differences of approach for greater diversity.
these are practical program repair prapr and automated repair for java programs arja .
prapr uses mutation testing to generate patches while arja uses genetic programming.
another key difference between the two approaches lies in their usage of fault localization fl .
where prapr only uses fl to prioritise the order in which the patches are generated arja uses it to identify the most suspicious statements to repair.
.
.
deterministic mutation based repair prapr .
unlike arja prapr requires the user to specify the failing test s that witness the bug that is to be repaired.
then it applies fault localization fl to associate an estimated degree of suspiciousness to the statements covered by the failing tests.
the most suspicious statements are targeted first so as to increase the likelihood of finding a good fixing point early.
however all statements are investigated given sufficient time.
to repair the program prapr applies a predefined set of mutation operators on each of the covered statements.
more suspicious statements are mutated first.
this process results in a deterministic set of patches defined by the mutation operators and the mutable locations.
to validate the patches only tests covering the mutated statements as well as the tests given as input are executed against the mutants.
the patch candidates mutant programs for which all tests pass constitute the resulting set of valid patches.
.
.
genetic programming based repair arja .
arja is a genproglike tool.
it generates a population of patches that evolve over a predefined number of generations.
arja does not ask the userto specify the initially failing tests.
instead it runs the entire test suite applies fl and considers alltests that failed to retrieve the set of suspicious statements.
then it discards the statements with suspiciousness values below a predefined threshold and collects the statements that i are covered by at least one test covering the suspicious statements ii have some dependency with the suspicious statements.
a patch is formed by altering these statements.
arja uses the nsga ii genetic algorithm to evolve the patches using a fixed size population and by producing patches for a fixed number of evaluations.
to evaluate a patch arja runs the initially failing tests and all other passing tests that cover suspicious statements not discarded during filtering .
if no tests fail including the initially failing ones the patch is considered valid.
arja generates the same number of candidate patches over different runs.
however the patches will differ due to the randomness in the evolution of the population.
hence the number and content of valid patches vary from one run to another.
flaky tests may affect the initial test suite run impacting the fl estimates suspiciousness scores .
this can have a double effect change the patch search space and alter the number of tests to be used for patch validity check.
variations in these numbers provide a coarse view of the extent to which the use of flakime has reshaped the search space and affects the likelihood of finding a valid patch.
during patch validation flaky test failures might occur and add noise to the signal.
not only does this cause the algorithm to discard valid patches but it also perturbs its learning potentially hindering its effectiveness.
.
suspicious statement selection suspicious statement selection in many repair techniques including arja relies on spectrum base fault localization sbfl .
given a set of statements s1 ... s s and a test suite t1 ... t n sbfl assigns a suspiciousness score to each statement based on the number of failing and passing tests covering them.
it does this by building an n smatrix where each cell records whether a particular test covers a particular statement.
then it runs all tests and keeps a record of those that pass and those that fail.
based on this it computes a suspiciousness score for every statement.
for instance ochiai the metric used by prapr and arja assigns to any statement sthe score sf q sf nf sf sp wheresfis the number of failing tests covering the statement s nfis the number of failing tests and spis the number of passing tests covering s. compared to a non flaky test suite with clearly identified failing tests flakime affects the ochiai score of all statements because tests 986icse may pittsburgh pa usa maxime cordy renaud rwemalika adriano franci mike papadakis and mark harman sometimes fail instead of pass.
this increases the values nfandsf if the failing flaky tests cover s .
thus flakiness can either increase or decrease the ochiai score of the statements.
when a surrounding repair method discards statements based on their suspiciousness as in the case of arja such differences can largely affect the search space and reduce the effectiveness of the repair.
experimental setup .
research questions our first question concerns mutation testing and examines quantitatively the extent to which mutation score can be inflated by test flakiness rq1 how much does flakiness artificially inflate the mutation score of given test suites?
to answer this question we check the effect of flakiness on the mutation scores of the whole projects test suites first then on randomly chosen samples.
our interest is on the divergence of those scores under different degrees of flakiness.
in automated program repair the observable effect of flakiness is to reduce the number of generated valid patches.
we want to check the sensitivity of repair methods on flakiness.
thus we ask rq2 how sensitive to flakiness is the effectiveness of program repair at generating valid patches?
we study this question on two state of the art automatic program repair tools leveraging different strategies namely prapr and arja.
while our goal is not a comparison of the tools we aim to highlight the ways flakiness affects different approaches and from there to suggest mitigation strategies.
thus we divide rq2 into the following subquestions how much does flakiness decrease the number of valid patches produced by deterministic mutations?
how much does flakiness decrease the number of valid patches produced by genetic programming?
going a step further we also investigate the way prapr and arja use spectrum based fault localization sbfl .
prapr applies sbfl only as a prioritisation step to minimise the time to generate the first valid patch.
ultimately it considers only the suspicious statements covered by the failing tests that the user provided.
conversely arja uses sbfl to define its search space discarding less suspicious statements .
this has three consequences a the search space encompasses more candidate patches that do not fix the bug since they target wrong statements reducing de facto the effectiveness of arja b the faulty statement may be removed from the search space making it impossible to generate a valid patch c the number of tests both failing and passing executed to validate candidate patches is increased.
consequently we also investigate the scenario where no flaky failures occur during sbfl.
in this case the suspicious statement search space is not compromised which could help to alleviate the effects of flakiness.
to this end we seek to answer rq3 does making fault localization target real failing tests improve the robustness of program repair against flakiness?
to complement our analysis we study the sensitivity of sbfl to flakiness with respect to the selection of suspicious statements.we aim to measure the number of faulty statements that remain selected as the flakiness rate increases and the number of nonfaulty statements that are kept out of the search space.
hence we are interested at rq4 how much does flakiness hinder the ability of thresholdbased suspicious statement selection?
overall the aim of our study is to demonstrate that flakime can yield interesting insights on the techniques behaviours and robustness.
our goal is to show that the sensitivity of some decisions and method characteristics to flaky tests can be exposed through the lens of flakime which paves the way for the study and design of mitigation strategies.
.
nominal flake rate the nominal flake rate frnis the parameter that we can set in our experiments to control the effective rate freat which flaky failures occur.
in practical applications fredepends on environmental factors and conditions that vary significantly and is therefore hard to generalize outside a particular context.
for example industrial evidence shows the existence of test suites involving flaky tests which differs significantly from open source projects .
however these studies do not report on the rate at which these tests flake.
because of this we consider it essential and insightful to experimentally analyse the sensitivity of testing techniques to varying degrees of flakiness.
we therefore consider a range offrnfrom .
to .
we stop at .
for arja because this repair method could not produce patches above this rate .
.
third party tools to study the effect of flakiness on mutation testing we use the open source tool pit with its default operator set.
the mutation scoremsof a test suite ton a program pcan be expressed as ms p t k m e where k is the number of mutants killed m is the total number of mutants and e is the number of equivalent mutants.
we ignore the analysis of equivalent mutants since they do not impact our analysis and use a simplified mutation score measure ms p t k m which is independent of the equivalent mutants.
thus as flakiness is introduced only the number of killed mutants k influences the mutation score.
to analyze the sensitivity of automated program repair we select two tools leveraging different strategies prapr and arja.
prapr is available as a maven plugin and as a docker image4.
we use the maven plugin in conjunction with the flakime plugin to perform our experiment.
arja is retrieved from github5and modified to print additional statistics related to its execution and notify the system which step it is performing sbfl or patch validation .
to account for random variations introduced by flakiness and by the patch generation process in the case of arja we execute runs for each degree of flakiness.
this number of repetitions is a compromise between statistical relevance and computation cost.
987flakime laboratory controlled test flakiness impact assessment icse may pittsburgh pa usa .
test subjects defects4j is a set of real bugs harvested from java projects.
it is one of the most popular datasets in evaluating automatic program repair techniques including prapr and arja.
in our study we consider the bugs for which the techniques succeeded.
an important success metric here is the ability of the techniques to generate genuine patches semantically equivalent to the developers patch .
thus for prapr we picked the buggy versions from five projects originating from the defects4j dataset for which prapr produced at least one genuine patch.
we discarded the buggy programs for the closure project because prapr requires more than 64gb of ram to repair them .
for arja we consider buggy versions from two programs of defects4j for which the tool generated at least one genuine patch reported in arja s supplementary material6 and for which we could successfully generate valid patches using the default settings of the tool .
unfortunately we could not generate valid patches for some programs due to differences in the tool configurations and or infrastructures.
nevertheless to increase diversity we also considered projects for which arja could generate valid but not necessarily genuine patches i.e.
patches that pass all tests.
for mutation testing we consider the latest releases non buggy of four projects that we use in the repair experiments time lang charts and math.
we choose these projects to maintain a certain consistency across our experiments.
results .
rq1 mutation testing we investigate the effect of flakiness on the mutation score by injecting flaky failures with a nominal flake rate ranging from .
to .
.
we repeat the experiment times for each rate .
figure 2a shows the average mutation score overall runs.
we observe that mutation score increases more steeply at lower flakiness rates.
this indicates that flaky tests even at a small degree are sufficient to introduce noise although the effect remains modest overall.
one reason leading to this fast increase comes from tests with high flakiness probability.
these tests tend to fail as soon as flakiness is introduced.
this effect is project dependent and bounded by the number of mutants that survive and are covered by the flaky tests hence the asymptotic behaviour.
however the total increase of mutation score remains low even when the degree of flakiness increases.
for instance when increasing the nominal flake rate from .0to0.
the mutation score of time raises from .
to up to .
increase of .
while the score of lang increases from .
to up to .
increase of .
.
figure 2b shows the standard deviation of the mutation score as the nominal flake rate increases.
without flakiness the outcome of the tests is deterministic and therefore we observe a standard deviation of zero.
when flakiness occurs the standard deviation remains low with average values ranging from .
to .
.
figure 2c allows us to better observe the difference in mutation score that each nominal flake rate induces.
we see that the median values for a nominal flake rate of .1range between .
lang and .
time .
the values for a nominal flake rate at .5ranges impact of flakiness on the valid patches generated by prapr.
p and v denote the number of all patches and valid patches originally generated by prapr.
tv is the average number of tests covering a valid patch.
vf is the average number of valid patches in the flaky case where f .
.
.
.
.
is the nominal flake rate.
bug p v tv v0.
v0.
v0.
v0.
v0.
math .
.
.
.
.
.
math .
.
.
.
.
.
math .
.
.
.
.
.
math .
.
.
.
.
.
math .
.
.
.
.
.
time .
.
.
.
.
.
time .
.
.
.
.
.
lang .
.
.
.
.
.
mock.
.
.
.
.
.
.
mock.
.
.
.
.
.
.
chart .
.
.
.
.
.
chart .
.
.
.
.
.
chart .
.
.
.
.
.
chart .
.
.
.
.
.
chart .
.
.
.
.
.
chart .
.
.
.
.
.
from .
lang to .
time .
we observe that the smallest degree of flakiness already results in about half of the total increase of mutation score i.e.
at .
nominal flake rate and that mutation score plateaus from thereon.
a small amount of flakiness can affect the mutation score less than .
nominal flake rate inflates the score by yet this effect diminishes as the flakiness degree increases.
this results in an asymptotic growth with the mutation score plateauing around .
.
rq2 effectiveness of program repair .
.
deterministic mutation technique.
to evaluate the impact of flakiness on prapr we replicate its original experiments .
we retrieve the set of patches pgenerated by prapr and the set of valid patches v as well as the set of covering tests tvfor each mutant.
then we re execute prapr and report the number of valid patches for different nominal flake rates i.e.
.
.
.
.
and .
.
even though the results of prapr execution are deterministic the validity of a patch may vary from one execution to the other because of the injected flakiness.
to alleviate any bias due to this randomness we execute prapr times for any flake rate and report the average number of valid patches over these runs.
table shows the results.
the number of valid patches is reduced by17.
math to time when a nominal flake rate of0.1is introduced.
when the degree of flakiness is increased to .
the number of patches drops under of the original number for all projects and prapr generates no valid patch for out of the projects.
the initially large number of valid patches generated for math i.e.
patches combined with the low number of covering tests explains the low impact of flakiness on this project.
on the contrary for time prapr initially generates a 988icse may pittsburgh pa usa maxime cordy renaud rwemalika adriano franci mike papadakis and mark harman .
.
.
.
.
.
nominal flake rate020406080100ms time lang charts math a mutation score .
.
.
.
.
.
nominal flake rate0.
.
.
.
.20standard deviation time lang charts math b standard deviation .
.
.
.
.
nominal flake rate2468difference time lang charts math c difference figure impact of flakiness on the mutation score.
figure 2a shows the msof the complete test suites while figure 2b shows the standard deviations of ms. figure 2c shows the difference in msthat flakiness introduces.
low number of valid patches while the number of covering tests is high tests .
in this case the introduction of flakiness completely annihilates prapr s patch generation capabilities.
overall we observe that if the number of covering tests is important the number of valid patches decreases faster.
the decrease however remains relatively slow when compared to the results of arja see section .
.
.
although prapr relies on sbfl to locate suspicious statements the tool takes as input a set of initially failing tests.
during the evaluation of the suspicious statements any statement which is not covered by any initially failing test is discarded.
thus the set of suspicious statements is bounded by the set of statements covered by all initially failing tests.
consequently the maximum number of added tests that are introduced during the patch validation steps are those with an intersection in terms of coverage with the initial failing tests.
the chances for at least one test to generate a flaky failure remain lower as the total number of executed tests remains low.
in an attempt to better analyze the benefits of knowing the real failing tests we ran prapr without this prior knowledge.
in this case prapr runs first a fault localization procedure where it executes all tests.
if flaky failures occur they will augment the set of failing tests that prapr considers during repair.
when running under these settings for all the buggy versions contained in the dataset prapr failed to generate any valid patches as soon as the slightest flakiness is introduced.
this is because prpar repairs single faults and narrows its search to statements covered by all failing tests.
if two failing tests have a disjoint coverage which can happen if one such test is flaky then prapr will generate no patch.
as the degree of flakiness of the test suite increases the effectiveness of deterministic mutation based repair techniques decreases.
the number of generated valid patches drops by to for a low degree of flakiness and exhibits a decrease of to when a high degree of flakiness is injected.
.
.
genetic programming based technique.
we first run arja on each unmodified buggy program times and analyze the number .
.
.
.
.
nominal flake rate050100150number of valid patcheslang lang lang math math math math math math math math 98figure sensitivity of arja s effectiveness to flakiness.
the effect of flakiness is case dependent and has disastrous consequences.
fewer valid patches are generated as the nominal flake rate increases.
the most affected bugs are those that arja can hardly fix even in the absence of flakiness.
of valid patches.
then we repeat the same experiment while introducing flakiness.
we assign each of these tests with a nominal flake rate ranging from .00to0.
we stop there because at this point arja fails to generate any patch for all projects.
for each project we perform runs of arja on each resulting flaky program.
figure shows the number of valid patches generated by arja as the nominal flake rate increases.
the results reveal that flakiness has disastrous effects on all projects and drastically reduces the number of valid patches generated even more so as the nominal flake rate increases.
when we consider a .05nominal flake rate the average number of patches generated drops by more than for all projects.
math5 math math and math see their score reduced by i.e.
arja produces no valid patch for these projects.
in math and math the number of generated patches drops from an average of .
and generated patches respectively to less than patch.
for math this average number drops from .
to .
.
arja still produces valid patches for lang lang and 989flakime laboratory controlled test flakiness impact assessment icse may pittsburgh pa usa lang .
math exhibits the smallest decrease from .
to .
valid patches that is .
overall while the impact of flakiness varies a lot from one buggy program to the other the most negative scenarios tend to occur in programs for which arja could hardly generate a valid patch already in the non flaky variants.
the decrease in the effectiveness of genetic programmingbased program repair due to flaky failures is dramatic for all bugs in our dataset.
with a nominal flake rate of .
the number of generated patches drops to zero for all projects.
.
rq3 targeted fault localization in program repair as a first step to investigate a mitigation strategy for the corruption of arja s search space due to flakiness we retrieve additional information from our rq2 experiments.
the failing and the passing tests that cover one or more suspicious statements identified by sbfl determine the set of statements considered to produce patches.
their number is thus an indication of both the size of the search space and the risk of discarding a valid patch due to flakiness.
figure shows the number of those tests for all projects and nominal flake rates averaged over runs.
as the nominal flake rate increases the number of failing tests increases almost linearly figure 4a .
interestingly as soon as flakiness is introduced the number of executed passing tests may not only increase but also decrease figure 4b .
we explain this potential decrease by the fact that ochiai the statement suspiciousness formula used by arja depends on the number of the failing tests that cover the statement and the total number of failing tests.
hence a flaky failure can decrease the suspiciousness of a statement if the corresponding flaky test does not cover the statement.
if the suspiciousness score of the statement goes below the predefined threshold used by arja then the statement is ignored during patch generation.
this can then result in a reduction in the number of executed passing tests.
we investigate the impact of flakiness on the suspiciousness score in more depth in rq4.
we notice a general trend where the number of executed tests increases with the nominal flake rate.
this is especially true e.g.
for math which sees the number of tests executed explode after a nominal flake rate of .
.
these observations shed some light on the results of rq2 we suspect that the number of patches generated dramatically decreases because of the larger number of flaky tests executed.
we therefore pursue our investigation by studying the practical benefits of making sbfl target the real failing test cases.
we conduct controlled experiments where we compare the number of valid patches produced by arja in the previous flaky case where sbfl is applied as is and a new case where no flakiness is injected during fault localization thereby leaving the search space untouched.
so allows us to discard any suspicious statements and tests that are artificially added due to flakiness occurring during sbfl under the same nominal flake rate.
as before we run arja times on each variant of each buggy program.
becausethe patch search space is not compromised we expect to observe improvements in the number of valid patches.
figure shows the number of valid patches in the new case targeted and in the previous case non targeted with a nominal flake rate of .
.
we observe a clear improvement in the targeted case.
arja is even able to generate valid patches for the four programs math math math and math it could not repair in the non targeted case.
for the remaining programs the median number of valid patches is multiplied by a factor ranging from math to math .
a wilcoxon signed rank test with .05reveals that the differences are statistically significant p value .
these results show the importance of identifying the failing test cases on which to apply fault localization in order to avoid corrupting both the patch search space and the validation process.
the fault localization step of program repair is particularly vulnerable to flakiness.
an efficient strategy to mitigate the effect of flakiness on apr is therefore to make sure that the repair targets the real failing tests.
applying this strategy has allowed the generation of patches for four more programs and yields up to times more valid patches.
.
rq4 suspicious statement selection to evaluate how the alterations in the suspiciousness scores impact the set of statements selected by repair methods we record for each unmodified buggy program the statements retained by arja after filtering.
as recommended in the original paper all statements with an ochiai score below .
are discarded.
taking the set of selected discarded statements as the ground truth we compute their counterparts in flaky variants of the programs.
we then define the resilience of the ochiai score against flakiness as its capability to preserve the original set of suspicious statements.
we measure this resilience using the standard metrics of accuracy precision and recall.
accuracy indicates the percentage of statements that remain in their class selected or discarded a coarse grained view of how much the sets of statements are altered.
precision measures the percentage of selected statements in the flaky case that indeed had to be selected were selected in the non flaky case .
thus the lower the precision the more the patch search space is increased with patches that do not target the real buggy statements.
recall measures the percentage of real suspicious statements that remain selected in the flaky case.
a lower recall means a higher risk of discarding the real buggy statements.
we compute the accuracy precision and recall for different flaky failure rates ranging from .
non flaky case to .
in steps of .
.
for each rate we repeat the experiment times.
figure shows the results for different nominal flake rates.
in figure 6a we observe that the accuracy tends to slightly decrease with a higher rate of flaky failures but remains high for all projects over for a nominal flake rate of .
.
indeed most statements are not covered by any failing test in the non flaky program and remain so in the flaky cases.
flakiness has however a drastic impact on precision and recall with an amplitude that is project dependent.
as soon as a small 990icse may pittsburgh pa usa maxime cordy renaud rwemalika adriano franci mike papadakis and mark harman .
.
.
.
.
nominal flake rate0255075100125150175200number of tests executed a number of initially failing tests executed .
.
.
.
.
nominal flake rate0100200300400500600700800number of tests executed b number of initially passing tests executed .
.
.
.
.
nominal flake rate0100200300400500600700800number of tests executedlang lang lang math math math math math math math math c total number of tests executed figure number of tests failing passing and total covering one or more suspicious statements and executed by arja for a nominal flake rate ranging from .
to .
and across runs for each rate.
test flakiness creates discrepancies in the test results that are executed against candidate patches.
this has a double effect waste of computation resources and a higher risk of altering important test signals.
lang 20lang 22lang 39math 22math 39math 5math 50math 53math 58math 70math 98050100150200number of valid patchesnon targeted targeted figure number of generated valid patches obtained when no flaky failures occur during the fault localization step of arja targeted and flaky failures can arise at any step non targeted with a nominal flake rate of .
.
degree of flakiness .
is introduced some projects see their precision and recall drop dramatically e.g.
math which sees its precision drop to .
and its recall to .
math falls to a precision of .
and to a catastrophic recall .
math sees both precision and recall fall to about .
when the nominal flake rate is set to .
all projects see their precision drop by or more while the recall remains more casespecific.
the ochiai score of the lang projects better resist flakiness and manages to keep acceptable precision and recall at the lower flakiness rates.
the precision exhibits low values less than around at a nominal flake rate of .
.
at .
the values of recall for lang and lang suffer a significant drop and it is only around .28that the recall of lang drastically drops.
finally on math and math the ochiai based selection offers acceptable recall until around a nominal flake rate of .
where the recall drops below .
we conclude that the slightest degrees of flakiness i.e.
.
can reduce the precision and recall of the threshold based suspicious statement selection by up to .
this shows that the adopted threshold is yet another factor contributing to arja s loss of effectiveness.
as shown by our results the potential benefits of thisthreshold reducing the number of tests to execute must be balanced with the risk of executing flakiness which can dramatically reduce the performance of program repair.
lowering the threshold may help but still necessitates a clear a priori knowledge of the particular flaky failure rate.
the smallest degree of flakiness is sufficient to disrupt threshold based suspicious statement selection.
we found that both precision and recall can drop by more than .
without user feedback sbfl cannot target real failing tests so the use of the threshold should be avoided.
threats to validity the most important threat to validity regards the realism of the injected flakiness and the lack of validation of flakime against the actual behaviour exposed by real flaky tests.
the results of performing mutation testing or program repair with real flaky tests may therefore differ from the results observed with flakime.
while the experimental analysis that flakime enables is important to stress test the research techniques and study their limits empirical analysis involving real flaky tests remains essential to validate these research techniques in practice.
it is to be noted that flakime opens the possibility for mitigating this threat and conducting empirical studies through its calibration of its behavior to a particular context.
this can be done by incorporating a context specific flakiness prediction model e.g.
to simulate particular causes or instances of flakiness.
nevertheless the primary contribution of this paper is the experimental analysis of the impact of flakiness as enabled by flakime.
our results corroborate and expand the findings of other studies conducted on real world flakiness .
threats to construct validity are related to how we inject flaky failures.
given that flakime alters the execution of tests and not the source code we cannot systematically control the execution flow of the program under test.
one of the major consequences of this limitation regards flakes that instead of creating flaky failures cause it to pass.
because in its current implementation flakime introduces flaky failures it cannot simulate this behaviour.
previous studies 991flakime laboratory controlled test flakiness impact assessment icse may pittsburgh pa usa .
.
.
.
.
.
nominal flake rate0.
.
.
.
.
.
.
.000accuracy a accuracy .
.
.
.
.
.
nominal flake rate0.
.
.
.
.
.0precision b precision .
.
.
.
.
.
nominal flake rate0.
.
.
.
.
.0recalllang lang lang math math math math math math math c recall figure performance of suspicious statement selection as the flaky failure rate increases.
have shown that this phenomenon can occur in reality and decrease the mutation score although to a limited extent.
moreover the mutation testing tool we use pit computes test coverage once and for all on the basis of the original program and does not execute the tests not covering the mutated statements.
thus later changes of coverage due to flakiness remain undetected by pit and do not affect the mutation score.
nevertheless flakime can support a plethora of scenarios such as the above ones providing experimental control on flakiness.
we therefore expect that future work will further alleviate such threats by considering further test suites projects characteristics and flakiness injection methods .
a threat to the internal validity is due to the flakiness prediction model that we used to determine the probability of tests to be flaky.
this model relies on the vocabulary of tests and do not include any dynamic information such as coverage.
although one could use better predictors based on the code under test these are unlikely to affect our results given the study of haben et al.
that found negligible improvement on the use of the code under test.
finally a threat to the external validity which hinders the ability to generalize our results regards the selection of the projects in this study.
we mitigate this threat by selecting projects from the well known benchmark defects4j and ensure that the flakiness probability distribution present in the test subjects figure presents differences.
furthermore all the projects taken in our analysis are written in java but we do not have any evidence that different languages would yield significantly different results.
ultimately although laboratory controlled conditions inevitably differ from reality to some extent they can still lead to interesting insights that remain applicable in the real world such as the mitigation strategies that our study has allowed us to reveal.
we believe that one of the main strengths of flakime remains its flexibility to integrate different flakiness models i.e.
the set of methods and conditions that determine how flakime injects flakiness in programs and tests.
we entrust future users of flakime with the task of designing appropriate models for the specific flakiness phenomena that these researchers investigate and do so with the awareness that any model they build comes with inevitable threats to validity and should therefore be used under specific and validated working assumptions.
to ease this endeavor additionally to the flakime maven plugin we provide a full replication package.
conclusion we presented a test flakiness platform flakime that allows experimenting with laboratory controlled test flakiness.
flakime is customizable and can simulate a wide range of flakiness related conditions and scenarios.
to demonstrate the utility of flakime we performed laboratory controlled experiments to assess the impact of test flakiness on mutation testing and automated program repair.
interestingly we showed that putting flakiness under laboratory control adds a new dimension to software testing studies which is the simulation of a world where some tests exhibit nondeterministic behaviour and are considered as potentially flaky.
such a world allows establishing a better understanding of the effects of flakiness and paves the way for developing robust against flakiness test techniques.
for instance we demonstrated that mutation testing a popular test assessment metric is impacted by flaky tests i.e.
mutation score is inflated by approximately depending on the degree of flakiness.
this effect is however small as the introduced noise is similar among all cases making the metric relatively stable.
in program repair our results showed that the fault localization step is particularly sensitive to test flakiness.
such sensitivity can have disastrous effects on patch generation.
thus to make program repair techniques robust to flaky tests one should revisit the key decisions and assumptions made during fault localization.
for example in a scenario where some real failing tests are specified as inputs a tailored fault localisation procedure that considers only these tests helps to prevent the corruption of the patch search space as well as useless runs of the candidate patches with flaky test cases.
with flakime as a framework to conduct controlled and fine grained experiments researchers can further analyze mitigation techniques to improve the resilience of software testing techniques under flakiness.
acknowledgement this work is supported by the luxembourg national research funds fnr through the core project grant c20 is testflakes.
mark harman s scientific work is part supported by european research council erc advanced fellowship grant number evolutionary program improvement epic which is run out of university college london where he is part time professor.
he is a full time research scientist at meta.
992icse may pittsburgh pa usa maxime cordy renaud rwemalika adriano franci mike papadakis and mark harman