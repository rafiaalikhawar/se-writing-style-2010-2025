bigdataflow a distributed interprocedural dataflow analysis framework zewen sun nanjing university china sunzew smail.nju.edu.cnduanchenxu nanjing university china mf1933108 smail.nju.edu.cnyiyu zhang nanjing university china zhangyy0721 smail.nju.edu.cn yun qi nanjing university china mf20330058 smail.nju.edu.cnyueyangwang nanjing university china smail.nju.edu.cnzhiqiang zuo nanjing university china zqzuo nju.edu.cn zhaokang wang nanjing university china wang.zk foxmail.comyueli nanjing university china yueli nju.edu.cnxuandongli nanjing university china lxd nju.edu.cn qingda lu alibabagroup unitedstates qingda.lu alibaba inc.comwenwen peng alibabagroup china wenwen.pww alibaba inc.comshengjianguo baidu research unitedstates guosj vt.edu abstract apart from forming the backbone of compileroptimization static data f lowanalysishasbeenwidelyappliedinavastvarietyofapplications suchasbugdetection privacyanalysis programcomprehension etc.despiteitsimportance performinginterprocedural data f low analysis on large scale programs is well known to be challenging.
in this paper we propose a novel distributed analysis framework supporting the general interprocedural data f low analysis.
inspired by large scale graph processing we devise a dedicated distributedworklistalgorithmtailoredforinterproceduraldata f low analysis.
we implement the algorithm and develop a distributed frameworkcalledbigdata f lowrunningonalarge scalecluster.the experimental results validate the promising performance of bigdata f low it can f inish analyzing the program of millions lines of code in minutes.
compared with the state of the art bigdata f low achieves muchmore analysisefficiency.
ccs concepts software and its engineering general programming languages theoryofcomputation programanalysis computing methodologies distributedalgorithms.
keywords interproceduraldata f lowanalysis distributedcomputing alsowithstatekeylaboratoryfornovelsoftwaretechnologyatnanjinguniversity.
corresponding author.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forpro f itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe f irstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspeci f icpermission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
format zewensun duanchenxu yiyuzhang yunqi yueyangwang zhiqiang zuo zhaokang wang yue li xuandong li qingda lu wenwen peng and shengjian guo.
.
bigdata f low a distributed interprocedural data f lowanalysisframework.in proceedingsofthe31stacmjointeuropean software engineering conference and symposium on the foundations of softwareengineering esec fse december3 sanfrancisco ca usa.
acm newyork ny usa 13pages.
introduction data f lowanalysisisatechniqueforstaticallygatheringprogram information at program points along the program s control f low.
besidesformingthebackboneofcompileroptimization ithasbeen adopted in many other signi f icant application areas including bug detection security vulnerability discovery privacy analysis program testing debugging etc.
in a data f low analysis a separate data f low fact is maintained at each program point under the control f low graph cfg representation.
based on the effect of each statement a transfer function is applied to transformthe data f low factaccordinglyalong the cfg.
the transformation process is performed iteratively via a worklist algorithm until a f ixed point is reached meaning that all the data f low facts are unchangedanymore.
challenges.
despite its importance performing interprocedural data f low analysis on large scale systems code is well known to be challenging.
first as modern real world programs are usually of largescale likemillionlinesofcode maintainingsolutionsatall program points with limited memorycan hardly be scalable.
even worse forcertainanalysis thedata f lowsolutionmaintainedateach pointitselfishighlyspace intensive.althoughpriorworkattempts to adopt sparse representations the huge memory consumption still severely limits the scalability.
as evidenced by recentstudies theanalysisoversparsevalue f lowgraph can easily exceed hundreds of gigabytes showing the memory consumptionafactualbottleneck.second thecomputationof f lowsensitive analysisrequires updating the data f low factwith respect 1431esec fse december3 san francisco ca usa zewensun et al.
toeachstatementalongthecfgbyperformingthetransferfunction.
the process is highly computation intensive because the amountoftransferfunctionexecutedisatleastlinearinthenumber ofprogramstatements whichislarge scalegiventhemodernlargesizesoftwareunderanalysis thecomputationofeachtransfer function is perhaps expensive as well.
for instance in the f lowsensitive pointer aliasanalysis thedata f lowfact at eachprogram point should capture the aliasinformation among allthe variables in the entire program.
updating variable relations by each transfer function consumeshigh cpucycles.
state of the art.
toaccelerateinterproceduraldata f lowanalysis afewattemptstodistribute parallelizethecomputationhave been made.
for distributed approaches garbervetsky et al.
presented a distributed worklist algorithm on the basis of the actor model.howeverasstatedexplicitlyintheirpaper itcannotsupport the standard data f low analysis due to the lack of f low ordering betweenactors.albarghouthietal.
parallelizedthedemand driven top down analyses based on mapreduce paradigm.
they only targetedveri f icationandsoftwaremodelcheckingwithoutsupporting data f lowanalysis.bigspa supportsthedistributedaccelerationforcflreachability basedanalysis .unfortunately alot of data f low analyses e.g.
cache analysis and numerical analysis do not belong to this category.
greathouse et al.
proposed scalabledata f lowanalysis.however theyfocusedondynamicanalysisratherthanstaticanalysis.inbrief thereexistnodistributed systemssupportingstaticdata f lowanalysis.
asforparallelapproaches leeandryder exploitedalgorithmicparallelismtoacceleratedata f lowanalysis.rodriguezetal.
proposedanactormodel basedparallelalgorithmforinterprocedural f initedistributivesubset ifds analysis .moreover some researchers alsostudiedparallelalgorithmsforpointeranalysis.
note that the above approaches only support speci f ic analysis rather than the general class of data f low analyses.
more importantly theyrelyheavilyonmemoryforcomputation.thereisno doubt that they can rarely scale to large systems such as linux kernel .recently zuoetal.
developedchianina asingle machine based analysis framework which can scale general data f low analysis to millions lines of code.
unfortunately due to the involvement of disks it readily takes hours or even days to f inishtheanalysisforlarge scaleprograms.suchinefficiencycan hardly meet the requirement of quick analysis response usually in minutes in the modern continuous integration and deployment ci cd pipelines .
ourwork.
withtheadventofcloudcomputing thelarge scale distributed cluster of commodity computers has become prevalent.
itnotonlyofferspowerfulcomputingcapability butnowadayscan beeasilyaccessiblebyasingledeveloper.exploitingcloudresources for static analysis would be the promising breakthrough point for achieving both signi f icant scalability and efficiency.
however as mentioned earlier there exists no distributed system running on a cluster whichcan support the general data f low analysis.adapting theexistingparallelalgorithms suchaschianina todistributedenvironmentisnon trivial.parallelalgorithmsonlyfocusoncomputation on shared memory which lacks the consideration of partitioning task dispatching fault tolerance and efficient communications between cluster nodes.
none of the existing parallel approachescan directly do it without re designing and re implementing the system.
in this work we propose a novel system that can leverage large scaledistributedcloudresourcestoscaleandacceleratethe general class of interprocedural data f low analyses.
in particular itonlytakesminutestoanalyzetheprogramsofmillionslinesof code providedthat acluster of 125commodity pcs.
inspiredbylarge scalegraphprocessing werevisit thetraditionalworklistalgorithmfromtheperspectiveof distributed vertex centriccomputationmodel anddeviseadedicateddistributed worklistalgorithmtailoredforinterproceduraldata f lowanalysis.
weimplementthedistributedalgorithmatopthegeneraldistributed graphprocessingplatform i.e.
apachegiraph anddevelop a framework named bigdata f low running on the cloud so as to take full advantage of the modern distributed computing resources.
the underlying platform i.e.
apache giraph provides the basic functionalitiestosupportreliableandrobustdistributedprocessing includinginput partitioning task dispatching cross nodecommunications andfaulttolerance.bigdata f low asagenericframework provides severalapistospecifythetransferfunctions andmerge operatorsimilartoothermonotonedata f lowframeworks thusalleviatingtheburdenofimplementingvariousclientanalyses.
by f illing these apis users canreadily implement a particular data f lowanalysisontop of bigdata f low.
contributions.
the contributionsare listedas follows we devise an optimized distributed vertex centric computation model to accelerate staticdata f low analysis by leveraginglarge scale cloud resources.
we develop and implement a distributed data f low analysisframeworkcalledbigdata f lowrunningonareal world cloud whichprovidesavarietyofhigh levelapistoeasily implement clientdata f lowanalyses.
we evaluate the performance and scalability of bigdata f low over large scale real world software systems e.g.
firefox and linux kernel .
the experimental results validate the promisingperformanceofbigdata f low itcan f inishanalyzingthe program of millions linesof code inminutes.
outlines.
the rest ofthepaper isorganized asfollows.
2gives the necessary background of data f low analysis and distributed graphprocessing.
3presents the distributed worklistalgorithms proposed followedbytheimplementationdetailsofbigdata f low in .
we discuss the programming model provided by our framework to implement various client analyses in .
6describes the empiricalevaluationofbigdata f lowintermsofperformanceand scalability.wegivecertaindiscussionsin 7andreviewtherelated work in .finally 9concludes.
background .
intraproceduraldata f lowanalysis data f low analysis is a technique for gathering program informationwithrespecttovariousprogrampointsalongprogram f lows.a clientdata f lowanalysiscanusuallybeformulatedasaninstanceof themonotonedata f lowanalysisframework whichconsists of the analysis domain including operations to copy and merge domain elements and the transfer functions over domain elements with respect to each type of statement in the control f low graph 1432bigdataflow a distributedinterproceduraldataflowanalysis framework esec fse december3 san francisco ca usa cfg .
an iterative worklist algorithm then takes as input an instance of the monotone framework performs the transfer function for each program statement iteratively along the cfg and computesa f ixedpointastheanalysisresult .algorithm 1shows the worklist algorithm for forwardanalysisindetail.
foreachstatement u1d458inthecfg twoelements in u1d458andout u1d458 representtheincomingandoutgoingdata f lowfacts respectively.
at each merging point of cfg in which case a node u1d458has multiple predecessors u1d45d u1d45d u1d45f u1d452 u1d451 u1d460 u1d458 the incoming data f low fact in u1d458of node u1d458is the combination of all the outgoing facts out u1d45d shown as line4 where indicates the merge operator speci f ied by users which can be meet for must analysis or join for may analysis .
atransferfunctionforstatement u1d458thentakesasinput in u1d458and returns the new outgoing fact as shown by line .
the worklist algorithmisconductedalongthecfgtoupdatethedata f lowelementsin u1d458andout u1d458for each statement in an iterative manner until a f ixed point is reached meaning that all the data f low facts are unchangedanymore .
algorithm1 worklist algorithm for forwardanalysis 1w allthe entry statements ofthe cfg 2repeat 3remove u1d458fromw 4in u1d458 u1d45d preds u1d458 out u1d45d mergefunction 5temp in u1d458 u1d43e u1d43c u1d43f u1d43f u1d458 u1d43a u1d438 u1d441 u1d458 transfer function 6iftemp out u1d458then 7out u1d458 temp 8w w succs u1d458 9untilw .
interprocedural data f lowanalysis interprocedural data f low analysis takes into account the propagation of data f low values across multiple procedures.
contextsensitive interprocedural analysis distinguishes the distinct calls of a procedure to eliminate the invalid paths thus achieving high precision.
generally there exist two dominant approaches to contextsensitiveinterproceduralanalysis namelythe summary based or functional approach andthecloning based approach .
thesummary basedapproachcommonlyconstructsasummary transfer functionforeachprocedure.ateachcallsitewherethe procedure is invoked the analysis computes the effects of the procedurebydirectlyapplyingthesummaryfunctiontothespeci f ic inputs at the call site.
as such the re analysis of the procedure bodyisavoidedwhileenablingcontextsensitivity.however itis not possible to construct such symbolic summary functions in general.
take the pointer analysis as an example we can hardly establish a succinct summarization for each procedure since the effectsofaprocedureareheavilydependentofthealiasrelationsof theinputsateachcallsite.theevaluationofasummaryfunctionon a particular input maynotbe cheaper than reanalyzing thewhole procedure .
another option is the explicit representation a.k.a.
tabulation method or partial transfer functions .
given a f inite lattice it enumerates the summary function as input output data f lowvaluepairsforeachprocedure.theoutputvalueofasummaryfunctioncanbedirectlyexploitedwhentheidenticalinputvalueisencounteredagainforthesameprocedure.however asa largenumberofstatesneedtobemaintained thisapproachusually suffers from huge spaceconsumption.
the alternative of achieving context sensitivity is a cloningbased approach where a separate clone of the procedure body is created at each callsite .
as such each procedure is reanalyzedunder each callingcontext preventingtheanalysisfrom propagatingdata f lowvaluesalonginvalidpaths.inthiswork we adopt the cloning based approach to achieve context sensitivity.
the basicanalysis logic ofinterprocedural analysisis thesameas that of intraprocedural analysis shown as algorithm except that thecfgbecomestheinterproceduralcfg.morespeci f ically toconstruct the interprocedural cfg the cfg for each function is f irstly generated.basedonapre computedcallgraph thecfgforeach functionisclonedandincorporatedintothatofeachofitscallersby creating assignment edges to connect vertices representing formal parameters and actual arguments.
in order to achieve the sweet spot between scalability and precision we can actually perform cloningonlyatcertainlevels whichistheoreticallyequivalentto the u1d458 cfa callstringapproach .
.
vertex centric graphprocessing with the inception of pregel system vertex centric graph processingbecomesahotspotinthelarge scalegraphprocessingcommunity .
following pregel various algorithmic techniques and systems were proposed such asasynchronous model graphlab in memorydataparallelmodel graphx .peopleareable toachieveefficient scalable andfault tolerantgraphcomputing onalarge clusterofcomputers byleveragingthesesystems.
algorithm2 synchronous vertex centricgraphprocessing data a the setofactiveverticesduringprocessing 1repeat 2foreachvertex u1d458 ado inparallel doneby system remove u1d458froma doneby system perform user speci f ied logic for each vertex in particular includinggather applyandscatter 5m u1d458 gather u1d458 gathermessagesorinformation from neighbors 6d u1d458 apply m u1d458 u1d458 updatevalue ofkbased on gathered information m a scatter d u1d458 u1d458 activate newvertices and or sendout messages synchronize beforenext superstep 9synchronize doneby system 10a a doneby system 11untila algorithm 2gives the pseudo code of a synchronous vertexcentric processing algorithm.
given an initialized set of active vertices itconductsaniterative computation where eachiteration is termed as a superstep.
at each superstep all the active vertices in aare processed in a distributed and parallel way across the entire cluster.
over each active vertex u1d458 gather apply scatter a.k.a.
1433esec fse december3 san francisco ca usa zewensun et al.
gas model is performed .
at f irst the messages or informationfromitsneighborsaregathered line .attheapplyphase it updates its associated value d u1d458according to its current value and the information gathered line .
based on the newly computedvalue itupdatestheactiveverticesaccordingly and orsends necessarymessagestoitsneighbors line atthescatterphase.
beforethenextsuperstep allthemessagesgeneratedatthecurrent superstep and active vertices are synchronized lines .
the wholecomputationterminatesuntil noactive vertex isgenerated.
note that vertex centric graph processing is a programmingmodelforimplementing graphprocessingapplications.
userswritegraphalgorithmsfromtheperspectiveofvertices.they only need to specify the code executed at each vertex particularlygather apply scatterfunctions lines .theunderlying graphprocessingsystemisresponsiblefordividingtheinputlargescalegraphintomultiplepartitions loadingpartitionsintodifferent cluster nodes launching multiple threads processes to execute user de f ined code simultaneously performing necessarysynchronizations optimizingcommunicationamongnodes maintaining replicas to ensure faulttolerance etc.
in this work we take inspiration from vertex centric graph processing design and implement a distributed framework bigdata f low tailored to interprocedural data f low analysis of largescalecode.similartotheexistinggeneral purposegraphsystems bigdata f lowprovidesuser friendlyapis e.g.
mergeandtransfer functions based onwhichuserscan readilyimplementtheir own clientanalyseswithoutworryingaboutscalability.theintrinsicsystem support under bigdata f low ensures the distributed capability inlifting the sophisticatedanalysisto large scale programs.
distributed vertex centric worklist algorithm inspired by large scale graph processing we revisit the classic worklist algorithm of data f low analysis algorithm from the perspective of vertex centric computation model algorithm andaccordinglypresentour f irstdistributedworklistalgorithm i.e.
algorithm 3in .
.thisalgorithmfaithfullyfollowstheclassic worklistalgorithm andthusitiseasytounderstand however its scalability isalso limitedunder thedistributed setting.asaresult in .
wefurtherproposeanoptimizedalgorithmthatachieves betterperformance thanalgorithm 3as demonstratedin .
.
distributedworklist algorithm by directly instantiating gather apply scatter interface and other respective data structures in algorithm we devise the f irst distributed worklist algorithm for data f low analysis which is listed as algorithm .
our f irstworklistalgorithmtakesasinputalargeinterprocedural control f low graph cfg or an arbitrary sparse representation .
at the beginning all the entry vertices in the input cfgareaddedtowastheinitialactivevertices line .during each superstep the underlying system launches a large number ofthreads processestohandlethecomputationoneachvertexin parallel line .oneachvertex u1d458 allthedata f lowfactsfrom u1d458 s predecessors are f irstly gathered line .
this can be implemented bydirectlyinvokingtheexistingapisprovidedbypull basedgraphalgorithm3 distributed worklist algorithm data w the list ofallactiveverticesduringanalysis ds u1d458 out u1d45d u1d45d u1d45d u1d45f u1d452 u1d451 u1d460 u1d458 aset containingallthe data f lowfacts of u1d458 spredecessors 1w allthe entry verticesin cfg 2repeat 3foreachcfg vertex u1d458 wdo inparallel doneby remove u1d458fromw doneby system 5ds u1d458 gatherall u1d458 gatherallthe predecessors data f lowfacts 6in u1d458 merge ds u1d458 merge 7out u1d458 transfer in u1d458 u1d458 transfer ifpropagate out u1d458 out u1d458 then propagate 9out u1d458 out u1d458 10w w succs u1d458 11synchronize doneby system 12w w doneby system 13untilw systems e.g.
powergraph or designing a pulling mechanism on top of push based systems such as giraph .
next a merge function takes all the data f low facts gathered from predecessors i.e.
ds u1d458 asinput andproducestheincomingdata f lowfact in u1d458 line6 .
a transfer function is then performed to generate the new outgoing data f low fact out u1d458 line7 .
after that we check if the updateddata f lowfact out u1d458isdifferentfromthat i.e.
out u1d458 at previoussuperstep.ifso thepropagationisemployedtoupdatethe data f lowfactasthenewlycomputedvalue line .simultaneously all of u1d458 s successors are activated and put into active list w for the nextsuperstep line .
a initial state b gather and merge c transferandpropagate figure onesuperstep computation at vertex inalgo.
.
example.
figure1illustratesthecomputationprocedureatvertex 4intheabovealgorithm wheretheverticeswithyellowbackground areactive.vertices 3arethepredecessorsof and5 6areits successors.
suppose that at the beginning of a certain superstep the active vertex 4has its outgoing data f low fact out4.
predecessor1hasthenewlyupdated fact out whilepredecessors and3hold the old data f low facts out2andout3 respectively shown as figure 1a .
firstly all the predecessors data f low facts aregatheredasds4 out out2 out3 andin 4isgeneratedbymerging ds4showninfigure 1b.out 4iscomputed by performing transfer function on in .
assuming thatout is different fromout4 propagation is employed so that all the successors 5and6are markedas active shownas figure 1c.
1434bigdataflow a distributedinterproceduraldataflowanalysis framework esec fse december3 san francisco ca usa despitethattheabovealgorithmsucceedsinleveraginglargescaledistributedcomputingresourcestoacceleratedata f lowanalysis it may still suffer from poor scalability especially when analyzing large scale programs such as the linux kernel or firefox elaboratedshortlyin .asshowninalgorithm eachvertex hastocollectafullsetofdata f lowfactsassociatedwithallitspredecessors i.e.
ds u1d458 for computation.in the worst case the data f low factofeachvertexwouldbemademultiplecopieseachofwhich is sent to one of its successors.
as a result the total number of data f lowfacts heldin memoryandpassed acrossnetworks grows exponentiallywiththesizeofinterproceduralcontrol f lowgraph underanalysis.this numbercould be super largeinpracticeespecially when performing context sensitive analysis over large scale programs.
passing gathering a huge number of expensive data f low factsnotonlyexhauststhepreciousmemoryofaclusterquickly but also increases the burden of network communications leading to poor scalability.
we implemented algorithm 3as a prototype named bigdata f low classic and conducted the empirical evaluationofit.theexperimentalresultsdiscussedshortlyin 6show that bigdata f low classic works well for medium size programs but quickly runs out of memory on a worker cluster when analyzinglarge scaleprograms suchasthelinuxkernelorfirefox.
in the following .
we propose an optimized algorithm which addresses the aforementioned limitations by signi f icantly pruningawaythedatagathered thusachievingbetterscalability andperformance.
.
optimizeddistributedworklist algorithm algorithm4 optimized distributed worklist algorithm data w the list ofallactiveverticesduringanalysis m u1d458 out u1d45d pis apredecessor ofk asetcontaining the data f lowfacts of u1d458 spredecessorswhichareupdated at previous superstep 1w allthe entry verticesin cfg 2repeat 3foreachcfg vertex u1d458 wdo inparallel remove u1d458fromw 5m u1d458 gathermessages u1d458 gatherdata f lowfacts of the updated predecessors 6in u1d458 merge m u1d458 in u1d458 merge 7out u1d458 transfer in u1d458 u1d458 transfer ifpropagate out u1d458 out u1d458 then propagate 9out u1d458 out u1d458 foreachsuccessor u1d451of u1d458do sendmessages u1d451 out u1d458 send the updated data f lowfacts to successors w w u1d451 13in u1d458 in u1d458 14synchronize doneby system 15w w doneby system 16untilw asdiscussedearlier eachactivevertexrequiresthedata f lowfacts associated with all its predecessors to complete the computationintheoriginalworklistalgorithm i.e.
line4ofalgorithm 1and line6ofalgorithm .thatiswhyextensivedata f lowfactshaveto be transferred across the cluster network and then merged locally on each vertex resulting in poor scalability.
to tackle the problem we devise an optimized algorithm which prunes the data f low facts to be gathered.
in particular instead of gathering the full set of data f low facts from all the predecessors only the predecessors data f lowfactsthatarenewlyupdatedattheprevioussuperstepare passedandmerged.sinceasigni f icantportionofdata f lowfactsare not changed at one superstep the optimized algorithm can thus pruneawaymanyunnecessaryandmemory consumingdata f low factstobegathered greatlyreducingtheoverallmessagetrafficand computation cycles for merging.
we will discuss the correctness of such optimization it produces the same analysis results as the originalalgorithm andgive the formalproof shortlyin .
.
we propose an optimized distributed worklist algorithm shown asalgorithm .foreachactivevertex u1d458 onlythesetofpredecessors data f low facts which are updated at previous superstep are gathered.thiscanbeachievedviathepush basedmessagepassingmechanism.speci f ically eachvertex u1d458passivelyreceives the messagespassedtoit i.e.
m u1d458 fromitspredecessorsatprevious superstep line .
each message in fact corresponds to a data f low fact sent from one of the predecessors which is updated at the previous superstep.
subsequently the data f low facts out u1d45d m u1d458 aremergedwiththeincomingdata f lowfactof u1d458atlastsuperstep i.e.
in u1d458 togeneratethenewincomingdata f lowfact i.e.
in u1d458 line6 .we thenupdatethedata f low factaccordinglyvia atransfer function line .
next we check if the updated data f low fact out u1d458isdifferentfromthat i.e.
out u1d458 atprevioussuperstep.if so the propagation is employed to update the data f low fact as the newlycomputedvalue line .atthesametime out u1d458issent asa message to eachofitssuccessors u1d451 line11 while activating u1d451for the nextsuperstep line .
a initial state b gather and merge c transferandpropagate figure onesuperstep computation at vertex inalgo.
.
example.
figure2illustratesthecomputationprocedureatvertex 4in algorithm 4for the same example as figure .
suppose that at the beginning of a certain superstep the active vertex s predecessor1has the newly updated fact out 1and sends it as a message to4atthelastsuperstepdenotedbythedashedarrowsinfigure 2a.the messageisgatheredas m4 out andthenin 4is generatedbyincrementallymergingthedata f lowfacts in m4with in4shown as figure 2b.
finally as shown by figure 2c out iscomputedbyperformingtransferfunctionon in .assuming thatout 4isdifferentfromout4 propagationisemployedto send the newly updated out 4to all the successors 5and6while marking themas active.
1435esec fse december3 san francisco ca usa zewensun et al.
.
correctnessproofofoptimized algorithm the underlying rationale of such optimization is that the merge operation for the general monotone data f low analysis satis f ies the accumulative property.
in other words on each active vertex u1d458 mergingonlytheupdateddata f lowfactsof u1d458 spredecessorswith theoldin u1d458oflastsuperstepshouldproduceidenticalresultsto that merging the full set of data f low facts of all its predecessors.
the following theorem 1gives its formalde f inition.
theorem1 accumulative property .given an active vertex u1d458 at superstep u1d456.
let u1d45d u1d45f u1d452 u1d451 u1d460 u1d458 be the set of u1d458 s predecessors.
without loss of generality suppose at superstep u1d456 a partial set of u1d458 s predecessors i.e.
u1d443 u1d458 u1d45d u1d45f u1d452 u1d451 u1d460 u1d458 update their outgoing data f low facts while the outgoing facts of the remaining i.e.
u1d443 u1d458 u1d45d u1d45f u1d452 u1d451 u1d460 u1d458 u1d443 u1d458 stay unchanged.in u1d458andin u1d458indicate theincomingdata f lowfactof u1d458atsuperstep u1d456 1and u1d456 respectively.
theaccumulative propertyissatis f iedif andonlyif thefollowing equation holds.
in u1d458 in u1d458 u1d45d p u1d458 out u1d45d i.e.
u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d preds u1d458 out u1d45d u1d45d p u1d458 out u1d45d proof.generally there are two cases of monotone data f low analysis namely increasing analysis with the join operator and decreasing analysiswiththe meetoperator .
for case and for each predecessor u1d45d u1d443 u1d458 out u1d45d out u1d45dholds where denotes the partial order relationand isre f lexive anti symmetricandtransitiveaccording to its de f inition.
as de f ined by the operator which computes the least upper boundoftwoelementsinthelattice thefollowinginequality .
holds.
out u1d45d out u1d45d out u1d45d .
given thatout u1d45d out u1d45d for increasing analysis and out u1d45d out u1d45d isre f lexive the following can be deduced out u1d45d out u1d45d out u1d45d .
as isanti symmetric giveninequalities .1and3.2hold we can imply the following equation .
.
out u1d45d out u1d45d out u1d45d .
therefore for all u1d45d p u1d458 the following equation .4holds.
u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d out u1d45d .
becausethe operatorinmonotonedata f lowanalysisisboth associative andcommutative we can imply that u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d .
byjoining u1d45d p u1d458 out u1d45dwithbothsidesoftheequation .
we can getthe following u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d .
andfurther equation .7isdeducedsince isassociative.
u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d .
since equation .8holds u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d preds u1d458 out u1d45d .
the f inal equation .9for case isthus proved.
u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d preds u1d458 out u1d45d u1d45d p u1d458 out u1d45d .
example.
weusethevertex4infigure 2asanexampletodemonstrate the proof procedure.
assuming that u1d458 u1d45d u1d45f u1d452 u1d451 u1d460 .
given that at previous superstep predecessor updates its outgoing data f low fact thus u1d443 and u1d443 u1d45d u1d45f u1d452 u1d451 u1d460 u1d443 .
the incoming data f low fact of 4at previous and current supersteps are in4andin respectively.
for case suppose each data f low fact corresponds to a set.
the join operator indicatesthesetunion .thepartialorderrelation isset inclusion .validatingtheaccumulativepropertyspeci f ictothis example isto prove the following equation holds in in4 out giventhejoinoperator andpartialorderrelation itisapparent thatthe equation out out out 1holds according to3.1and3.
.
out out out .
out out out out2 out out out .
out out out out2 out out out .
in in4 out for case and for each predecessor u1d45d u1d443 u1d458 out u1d45d out u1d45dholds.we can followthe similar proof logic.
asthemeet operatorcalculatesthegreatestlowerboundof elements the following inequality .10holds.
out u1d45d out u1d45d out u1d45d .
given thatout u1d45d out u1d45d for decreasing analysis and out u1d45d out u1d45d isre f lexive the following can be deduced out u1d45d out u1d45d out u1d45d .
as is anti symmetric given inequalities .10and3.
the following equation .12can be concluded.
out u1d45d out u1d45d out u1d45d .
1436bigdataflow a distributedinterproceduraldataflowanalysis framework esec fse december3 san francisco ca usa therefore we can imply the following equations.
u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d p u1d458 out u1d45d u1d45d preds u1d458 out u1d45d u1d45d p u1d458 out u1d45d .
as equations .9and3.13hold for each case we ultimately complete the proofoftheorem .
implementation weimplementedbigdata f lowbyfollowingthedistributedworklist algorithm on top of apache giraph .
.
a well maintained open sourcejava implementationofpregel .
giraph replicates pregel s concepts and adds several new features to this model including master computation out of core computation and sharded aggregators etc.
in particular giraph f irstdividestheinputgraphintoanumberofpartitionsbasedon hadoopdistributed f ile system.withineachsuperstepofthebsp model giraph launches multiple workers and enables each worker to process a partition separately in a distributed way.
giraph offers multipleeffectivepartitioningschemes whichbigdata f lowdirectly adopts to achieve goodworkload balanceandscalability.
besides bigdata f low leverages two extra options offeredby giraphtorealizethepulled basedworklistalgorithm.
basiccomputation class.
basiccomputationis a general option for performing computations in giraph.
it can be used to access the graph s information suchasthesuperstepidandinformationofverticesand edges.
we extend it to distinguish analysis phase and acquire edge informationintheimplementationofbigdata f low.
broadcast class.broadcastisthesimplestwayformasternodetocommunicate with worker nodes in the scope of the entire cluster ensuring that all vertices access the same information.
bigdata f low exploits this feature to broadcast workers ofentry nodes incfg.
programming model bigdata f lowasaframeworksupportingthegeneralinterprocedural data f lowanalysis providesasetofnecessaryapistousers.users readily implement a particular client analysis based on these apis byspecifyingtheinformationofinputcfg thedata f lowequations i.e.
merge transfer andthepropagationlogic.inthefollowing we f irst discuss the crucial apis provided by bigdata f low then demonstrate how to implement a client analysis based on the apis.
.
apis givenacontrol f lowgraphorothersparserepresentation bigdata f low takes it as input and constructs the graph in memory.
during a data f low analysis each vertex in the cfg maintains a 4inlisting 1showtheabstractclassof vertexattribute which de f inestwomembers data f lowfactofabstractclass factandstatementsofclass stmts.data f lowfactdescribesthedata f lowinformation computed at each program point during analysis.
the abstract classfact line6 leavesuserstheinterfaceforspecifyingaparticular type of data f low fact in a client analysis.
stmts lines9 describesthesetofstatementsassociatedwiththevertex which determines the logic of transfer functions.
in a statement level data f low analysis data f low factis associatedwitheachstatement whereaninstanceof stmtscontainsonesinglestatement.while inabasicblock levelanalysis eachinstanceof stmtsindicatesa setofstatements inabasic block.
listing the apis.
1abstract class vertexattribute 2fact fact 3stmts stmts 6abstract class fact 8abstract class stmt 9classstmts 10stmt stmts 13interface analysis 14fact merge set fact predfacts fact oldin 15fact transfer stmts stmts fact infact 16boolean propagate fact oldfact fact newfact besidestheabovecrucialdatastructures threenecessarycomponents of data f low analysis are de f ined in the analysis interface shown as lines 17in listing .
whenever the computation on a vertex u1d458islaunched merge is f irstinvokedtotakethenewlyupdateddata f lowfactsofpredecessorstogetherwiththeoldincoming fact and produce a new incoming data f low fact for u1d458.
in general themergeoperationcanbeunionorintersectiondependingonthe speci f icclientanalysis.usersoverride merge tospecifytheexact logic.
taking the incoming data f low fact produced by merge and the statements as input transfer computes the outgoing data f low fact accordingly.
users are required to specify the particular transformation logic by overriding transfer for a particular clientanalysis.
propagate describestheconditionsforpropagating data f low facts to successors.
usually propagation is decided bythecomparisonbetweenoldfactandnewfact.useroverrides propagate to de f ineconcrete terminationcondition.
.
anexample ofaliasanalysis weuseacontext and f low sensitivealiasanalysisasanexampleto illustrate how to use the apis to implement a client analysis.
flowsensitivealiasanalysiscomputesthealiasrelationsbetweenpointer variablesateach program point.
as afundamental analysis ithas beenwidelyusedinvariousapplicationsincludingbugdetection security enforcement optimizations etc.
we adopt function cloning to achieve context sensitivity .
the input cfg to bigdata f low actually corresponds to a cloned interprocedural cfg.
taking the inlined icfg as input we f irst de f ineaparticularsubclass aliasstmt toinstantiateeachstatement for alias analysis.
its detailed implementation is omitted due to 1437esec fse december3 san francisco ca usa zewensun et al.
space limit.
stmtshasonly one stmtinstanceas we would like to analyze the alias information at the granularity of statement.
here weadopttheprogramexpressiongraph peg asadata f lowfact torepresent thealias informationateachprogrampoint.
assuch eachobjectof factisinstantiatedasa peginstance.next merge isachievedasunionoftheupdatedpegsfrompredecessorswith the old incoming fact.
within the overridden transfer edge addition and or deletion are performed on peg according to the semantics of each type of statement.
if the old peg and newly updated peg are isomorphic propagate returns false and the vertex becomes inactive.
listing2 theimplementationof f low sensitivealiasanalysis on top ofbigdata f low.
1public class aliasstmt extends stmt ... 2classaliasvertexattribute extends vertexattribute 4super 5fact newpeg 8classaliasanalysis implements analysis 9fact merge set fact predfacts fact oldin peg peg peg oldin for fact item predfacts if item null continue peg prepeg peg item peg.merge prepeg return peg 18fact transfer stmts stmts fact fact peg peg peg fact switch stmts .gettype caseload transfer load peg aliasstmt stmts break ... return peg 28boolean propagate fact oldfact fact newfact if oldfact null return true peg newpeg peg newfact peg oldpeg peg oldfact return !newpeg.consistent oldpeg as can be seen to implement a client analysis on top of bigdata f low usersonlyneedtospecifythenecessaryfunctionalities speci f ictoclient analysis without worryingaboutanyimplementation details of the underlying worklist algorithm as well as other system side optimizations.
evaluation our evaluation focuses onthe following three questions q1 what is the overall performance of bigdata f low given a richsetofdistributedcomputing resources?
.
q2 how does bigdata f low perform compared with other competitive analysissystems tools?
.
q3 whatabouttheperformanceof bigdata f lowgiventhe varyingnumbers ofcores andresources?
.
subjects.
to measure the performance of bigdata f low on scaling large programs we selected f ive real world software as the experimentalsubjects includinglinuxkernel firefox postgresql table characteristicsofsubject programs.
subject version loc functions description linux .
.5m 565koperating system firefox .
.9m 770k web browser postgresql .
.0m 30kdatabase system openssl .
.1519k 12k tlsprotocol h t tpd .
.
196k 6k web server openssl and apache httpd.
table 1lists detailed information about the subjects such as the version version the number of lines of code loc the number of functions functions and its description.
reference tools.
to validate the advantage of bigdata f low in termsofperformanceandscalabilityonlarge scaleprograms we selected the existing parallel distributed analysis systems tools as the competitors.
for parallel algorithms we chose chianina the most recent and state of the art parallel system scaling context and f low sensitive analysis to large scale c programs.
chianina is implemented in c c and leverages two level parallel computation model and out of core disk support to achieve both analysis efficiency and scalability.
we ignore other sequential analysis algorithms since it has been validated that chianina outperforms them .
for distributed work since there exist no distributed systems supporting data f low analysis we used bigdata f low classic the version implemented based on the distributed classic worklist algorithm shown as algorithm 3as the referencetool.bydefault bigdata f lowisimplementedusingthe optimizedversion i.e.
algorithm .
hardware and so f tware se t tings.
all experiments were conductedinthealibabacloudenvironment.bothbigdata f lowand bigdata f low classicaredeployedonaclusterconsistingof125elastic compute service ecs 2nodes with alibaba elastic mapreduce emr installed.eachnode inparticular ecs.r7.2xlarge isequipped with8virtualcpucoresbasedonintelxeonscalableprocessors and 64gb memory running centos .
.
the adopted emr version is3.
.0correspondingtohadoop2.
.2andgiraph1.
.
.tocomparewithchianinawhichcanonlyrunonasingle machinewith sharedmemory weusedthemostpowerfulservernodeavailablein theus virginia region i.e.
ecs.r6.26xlarge with104virtualcores 768g memory and1t ssd backedcloud disk.
client analyses.
in the experiments we implemented two client analyses namelycontext sensitive f low sensitivealiasanalysisand instruction cache analysis on top of bigdata f low bigdata f lowclassic and chianina.
the alias analysis is same as the example discussed in .
.
for cache analysis we followed the abstract model of lru caches in that adopts the set associative organization.
the con f iguration is set as cache lines with lru replacementstrategyenabled.theanalysiscomputesacachemodel at each program point and decides a cache hit ormiss.
we chose the above two analyses for several reasons both analyses are fundamental and widely used they are expensive and hardly scalable given their memory intensive data f lowfact and computeintensive transfer function they fall into the two cases of the 1438bigdataflow a distributedinterproceduraldataflowanalysis framework esec fse december3 san francisco ca usa table2 overallperformance columns paliasesand bcachedindicatethenumberofaliaspairsandthenumberofpotentially cachedmemoryblocks columns workers pmem timeandcostrepresentthenumberofworkersused thesizeofpeak memory consumed the total analysis time and the rental cost of cloud resources respectively part.
indicates the number of partitions indicates out of memory error a and b report theresults foralias andcacheanalysis respectively.
a aliasanalysis bigdata f low bigdata f low classic chianina subject paliases workers pmem timecost workers pmem timecost part.
pmem time cost linux .5b .5t16.7mins .
.4g .4hrs .
firefox .5b .2t16.5mins .
.6g .3hrs .
postgresql .0m .7g .8mins .
.7g .9mins .
.9g50.4mins .
openssl .8m .3g .5mins .
.8g .8mins .
.2g35.4mins .
h t tpd .1m .9g .8mins .
.9g .0mins .
.2g11.2mins .
b instruction cache analysis bigdata f low bigdata f low classic chianina subject bcached workers pmem timecost workers pmem timecost part.
pmem timecost linux .5b .6t44.4mins .
.4g .4hrs .
firefox .8b .4t39.0mins .
.5g .2hrs .
postgresql .4b .1t3.2mins .
.1t6.5mins .
.3g .1mins .
openssl .8b .3t6.9mins .
.5t13.4mins .
.6g .7hrs .
h t tpd .0m .3g .0mins .
.3g .6mins .
.3g18.5mins .
accumulative property in .3respectively thereby validating the proofmore comprehensively.
thecontext sensitivityisachievedviafullyfunctioncloning i.e.
cfa .
we start the cloning based upon a call graph constructed by using a lightweight inclusion based context insensitive pointer analysiswithsupportforfunctionpointers.tohandlerecursion we f irstidentifythestronglyconnectedcomponents sccs over thepre computedcallgraph.functionsnotinanysccenjoyfull contextsensitivity.whereas level 2call stringsensitivity i.e.
using top most callsites as the distinguishing context is used for those within sccs.
note that function cloning is not the core contributionofthiswork.userscanadopttheclassicalk limited context sensitivityorotherselectivecontext sensitivitytechniques .
this can be done by launching a cheap pre analysis to understand the contexts desired and then performing selective function cloning.
for each client analysis the version implemented on top of bigdata f low bigdata f low classic and chianina are identical and possessthesameanalysisprecision.wecheckedtheanalysisresults of three tools and validated they are consistent.
speci f ically we comparedthetotalnumberofaliaspairs includingbothmemory aliasandvaluealias generatedforaliasanalysis andthetotalnumber of potentially cached memory blocks for cache analysis.
the columns paliases and bcached in table 2list the exact numbers.
.
overallperformance tables2aand2bdemonstrate the performance of bigdata f low when analyzing the f ive real world subjects.
columns workers pmem andtimeindicatethenumberofworkersused oneworker correspondingtoonephysicalcore theamountofpeakmemory consumed andthe totalanalysistime respectively.
it is well known that the complexity of a particular data f low analysis is heavily dependent on many factors such as the size density structureofthecontrol f lowgraph andthesemanticsof program under analysis.
thereby it is difficult to give a generalformula that can f igure out the ideal number of workers needed.
whatwecando istoestimateanumberas smallaspossiblesoas totheanalysistaskcanbecompletedsuccessfullyandefficiently.
to this end we f irst run a small sample of the analysis e.g.
of theinputgraph onasmalltestclusterwith10nodes.basedonthe resource utilization data monitored we estimate an initial number roughly.next weruntheanalysisontheinitialnumberofworkers.
if the task fails due to insufficient memory the number of workers isdoubleduntil the analysiscan succeed.
as can be seen the peak memory consumed in both alias analysisandinstructioncacheanalysiscaneasilyreachseveralterabytes for large scale programs such as the linux kernel and firefox due to the memory intensive data f low fact and the huge number of program points.
even for the smallest subject httpd performing the context and f low sensitive analysis takes more than a hundred or even several hundreds of gigabytes.
this is consistent with the claim in that memory would be the major bottleneck for analysis to scale to large programs.
by leveraging the enormous amount of memory and computing resources in a cloud environment bigdata f low manages to analyze all the subjects successfully and efficiently.
thealias analysis canbe completedwithin minutesforallsubjects themoreexpensivecacheanalysistakesless than45 minutesfor the linux kernel with500workers.
.
comparisonwith otherframeworks given the identical version of the clientanalysis implemented we comparedbigdata f lowagainstbigdata f low classicandchianina withrespecttoperformanceandcost.columnsunderbigdata f lowclassicandchianinaintable 2aand2bshowthedetailedresults ofbigdata f low classic andchianina respectively.
chianina.
aschianinacanonlyrunonasingle machinewith sharedmemory werentedthemostpowerfulservernodewith104 virtualcores 768gmemory and1tssdavailableintheus virginia region of alibaba cloud.
in terms of analysis time chianina 1439esec fse december3 san francisco ca usa zewensun et al.
with threads takes more than hours and hours to f inish alias analysis and cache analysis over linux.
while bigdata f low completesaliasandcacheanalysiswithin20and45minutesundera cluster respectively.itshowsthatdistributedparallelismenabledby bigdata f lowindeedacceleratestheanalysissigni f icantly upto62x and12xforaliasanalysisandcacheanalysisonlinux respectively .
notethatbigdata f lowtakesmoretimeforcacheanalysisthanalias analysisonallthesubjects whereaschianinadoesnot.thiscanbe explained from two aspects.
first cache analysis is more memoryintensive than alias analysis.
the cache analysis on bigdata f low implemented in java deservedly pays more gc time.
second as observed the alias analysis running on chianina has low cpu utility due to load imbalance and excessive thread switching costs for certain subjects e.g.
linux when a large number of threads are enabledonasinglemachine.
as the computing resources used by bigdata f low and chianina are different we cannot simply derive that bigdata f low outperforms chianina.
for the sake of fairness we measured the exact amount of rental costs of cloud resources in dollars paid by bigdata f low and chianina for completing the identical analysis.
as cloud providers generally adopt auni f iedpricing strategy there is little difference in the price of nodes with similar resources across different providers.
without loss of generality we calculated the costbymultiplyingtheanalysistimeandtheofficialpay as you go hourlypriceofalibabacloudinus virginia region3.
inparticular at the time of submission each node ecs.r7.2xlarge used by bigdata f low takes .
hour.
the price of the entire cluster is .
i.e.
.
hour.thesingle ecs.r6.26xlarge servernode usedbychianinatakes .
hour.thecostcolumnsintable showthedetailedresults.ascanbeseen bigdata f lowspendslower rental costs than chianina over all the subjects except for httpd.
althoughthepriceoftheclusterusedbybigdata f low .
hour is much higher than that of the single server used by chianina .
hour bigdata f low takes much less time to f inish the analysis than chianina.
we can thus conclude that bigdata f low is able tooffer signi f icantlyhigheranalysisefficiency forlarge scaleprograms whiletaking fewer costscomparedto chianina.
regarding memoryconsumption bigdata f lowapparentlyconsumesmuchmorememorythanchianina.thereareseveralreasons.
chianinaisadisk basedsystemwherethememoryconsumptionisstronglyrestricted.itwillleveragediskstomaintain thehugeamountofdataoncethememoryconsumptionexceeds acertainthreshold.incontrast bigdata f lowprefersutilizingthe memoryoneachnodetoperformcommunicationsandaccelerate theanalysis.
bigdata f lowisimplementedinjava whilechianina is implemented inc c .
nodoubtchianinawouldhave less memory footprint than bigdata f low.
bigdata f low is running on topof giraph.toachieve faulttolerance giraphneedsto maintain extra e.g.
replicasforallthedatastored.moreover forcertain global data used in the analysis bigdata f low has to broadcast it on every node leadingto extramemory consumption.
bigdataflow classic.
as numerous redundant and expensive data f low facts were transmitted in the network and gathered at eachvertex bigdata f low classicfailedtoanalyzethelarge scale subjects in ourexperiments i.e.
linux and firefox giventhe same resources as bigdata f low.
it validates thatbigdata f low doessavememoryresources thusofferingbetterscalabilitythan bigdata f low classic.
for the analyses which both bigdata f lowclassic and bigdata f low successfully complete bigdata f low exclusively outperforms bigdata f low classic in terms of time efficiency.
this is because bigdata f low classic requires more data transferred andmergedthanbigdata f lowto accomplishthe same analysis.
.
scalability tounderstandthescalabilityofbigdata f low wemeasuredtheanalysistimeinsecondsandpeakmemoryconsumptioningigabytes forbothaliasanalysisandcacheanalysisgivendifferentnumbersof workers.figures 3and4show thedetailed performance resultsof alias analysis and cache analysis on openssl respectively where the x axis indicatesthe numberofworkers and y axisrepresents thetimeorpeakmemoryused.hereonlythedataofpostgresqlis reported.othersubjectsshowasimilar trend to that of openssl.
a time b peak memory figure the time a and peak memory b used for alias analysis on openssl with varying numberofworkers.
a time b peak memory figure thetime a and peakmemory b used for cache analysis on openssl with varying numberofworkers.
for alias analysis the time taken by bigdata f low follows a vbottompatternshownasfigure 3a.whenlessworkersareavailable i.e.
thetotalmemorycapacityjustsatis f iestheanalysis need.
with the number of workers increasing from to increased parallelismis translated to higherperformance.
therefore theoverallrunningtimeshowsadescendingtrend.however the communication cost among workers is monotonically increased withthegrowthofworkersinvolved.oncetheperformancebene f it ofparallelismisnolongersuperiortotheincreasedcommunication cost among workers time climbs steadily.
as such for the speci f ic analysis having workers provides the best trade off between parallelismbene f itandcommunicationcost leadingtotheshortest running time of all the tested parallel schedules.
it implies that in practice we can seek a sweet spot of parallelism for different 1440bigdataflow a distributedinterproceduraldataflowanalysis framework esec fse december3 san francisco ca usa subjects according to the tendency of running time as the number of workers changes.
this is particularly meaningful because cloudresourcesareondemandandchargedonactualusage and performingdata f lowanalysisonthesamelarge scaleprogram could be an iterative process as the program evolves constantly.
in terms of the peak memory usage as more threads processes consume more memory space it is not surprising that it shows an ascending trendwith the growth of workersin figure 3b.figure4 showssimilartrendsforcacheanalysis.ascanbereadfromfigure 4a with160workersavailable bigdata f lowsuccessfully f inishes thecacheanalysis.thebestperformanceisachievedgivenmore workers i.e.
.
after that point more analysis time is needed withthe increasing number ofworkers.
discussion usage scenarios.
bigdata f low offers the distributed capability in lifting sophisticated data f low analysis to large scale programs.
it s highlyvaluablefororganizationswithlargecodebasestoanalyze while often with their own cluster deployed.
in such scenarios bigdata f lowreadilyoffersbothhigh speedandscalableanalysisto ultra large scale programs.
soundness.
likeotheranalysisframeworks e.g.
sootandwala users implement a particular client analysis by specifying its correspondingmergeandtransferfunctions.itistheanalysisdeveloper s responsibilitytoensurethesoundnessoftheiranalysis bigdata f low faithfullyexecuteswhateverhasbeenimplementedbythedevelopers.
as for the underlying framework we adopted the classic worklistalgorithm anddirectlyimplementeditbasedonthe vertex centric computation model.
its soundness stays unchanged.
related work .
parallelanddistributedstatic analysis over the past decades a few attempts have been made to speed upstaticprogramanalysisbyleveragingparallelanddistributed computingfacilities.leeandryder exploitedalgorithmicparallelismtoacceleratedata f lowanalysis.rodriguezetal.
proposed aparallelalgorithmforifds baseddata f lowanalysis basedon theactormodel whichrequiresthetransferfunctiontobedistributiveovermeetoperators.nagarajandgovindarajan utilized intel threading building blocks to design a parallel f low sensitive pointer analysis algorithm.
su et al.
proposed parallel cflreachability based f low insensitivepointeranalysis.importantly all the above approaches rely heavily on memory for computation.
they can rarelyscaleto large systemssuch as linux kernel.
following the line of systemizing program analysis various systems are developed to support scalable interprocedural analysis.
graspan andbigspa scalethecontext sensitivecflreachability analysis in a single machine and distributed environment respectively.
unfortunately many data f low analyses cannotfallintothiscategory suchascacheanalysisandinterval analysis.
chianina is an out of core graph system performingthecontext and f low sensitiveanalysesinparallel.however restrictedbythelimitedparallelcomputingresourcesinasingle machine itisinefficient when analyzing large scale systemscode.fordistributedwork albarghouthietal.
tooktheinspirations frommapreduceparadigmandparallelizedthedemand driventopdown analyses suchasveri f ication and softwaremodel checking.
theyfailedtosupportdata f lowanalysis.garbervetskyetal.
recentlydevisedadistributedworklistalgorithmbasedontheactor model.however itdoesnotsupportthestandarddata f lowanalysis duetotheabsenceof f loworderingbetweenactors.christakiset al.
exploredinputsplittingstrategiestoanalyzedifferentcode pieces on parallel partitions independently.
however as stated explicitly the splitting causes analysis imprecision due to the information loss across separate partitions.
greathouse et al.
extendeddynamicdata f lowanalyseswitha novel samplingsystem to achieve low runtime overhead.
apparently they only focused ondynamic analysisratherthanstaticdata f lowanalysis.
.
vertex centric graphprocessing vertex centricmodelhasbeentightlyincorporatedintodistributed processing frameworks to tackle the challenge of large scale graph processing.
based on that pregel is the pioneering system supporting general graph applications.
pregel adopts bsp model to accelerate the intensive computation.
following the idea of pregel apachegiraph isimplementedinjavaasanopensourcesystem.
followingpregel moreadvancedvertex centricmodelsandvariants have been proposed.
graphlab supports asynchronous vertexcomputationbasedonchandy lamportsnapshotswithout haltingtheentireprogram.graphx isagraphsystembased onresilient distributeddataset a.k.a.
rdd abstraction.
notethatalltheabovegraphsystemsarededicatedtothegeneral graph applications.
none of them can directly scale the interproceduraldata f low analysiswell.asa result weproposebigdata f low the f irstdistributedsystemtailoredto data f lowanalysis.
conclusion thispaperproposesadistributedinterproceduraldata f lowanalysis framework named bigdata f low.
by leveraging the large amount of memory and cpu cores in the cloud bigdata f low greatly improves the scalability of data f low analysis for analyzing large scale programs.theexperimentsconductedinareal worldcloudenvironmentvalidatethatbigdata f lownotonlyscalescontext sensitive data f low analysis to million lines of code but also completes such analysis in a highly efficient manner.
it can be expected that we could achieve nearly on the f ly analysis of industrial scale codebasesbyleveragingmodern cloud computing facilities.
data availability bigdata f low is publicly available https f igshare.com articles dataset material fse23 zip .