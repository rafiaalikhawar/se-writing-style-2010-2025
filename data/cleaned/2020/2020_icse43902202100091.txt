it takes two to t ango combining visual and textual information for detecting duplicate video based bug reports nathan cooper carlos bernal c ardenas oscar chaparro kevin morany denys poshyvanyk college of william mary williamsburg v a usa ygeorge mason university fairfax v a usa nacooper01 email.wm.edu cebernal cs.wm.edu oscarch wm.edu kpmoran gmu.edu denys cs.wm.edu abstract when a bug manifests in a user facing application it is likely to be exposed through the graphical user interface gui .
given the importance of visual information to the process of identifying and understanding such bugs users are increasingly making use of screenshots and screen recordings as a means to report issues to developers.
however when such information is reported en masse such as during crowd sourced testing managing these artifacts can be a time consuming process.
as the reporting of screen recordings in particular becomes more popular developers are likely to face challenges related to manually identifying videos that depict duplicate bugs.
due to their graphical nature screen recordings present challenges for automated analysis that preclude the use of current duplicate bug report detection techniques.
to overcome these challenges and aid developers in this task this paper presents t ango a duplicate detection technique that operates purely on video based bug reports by leveraging both visual and textual information.
t ango combines tailored computer vision techniques optical character recognition and text retrieval.
we evaluated multiple configurations of t ango in a comprehensive empirical evaluation on duplicate detection tasks that involved a total of screenrecordings from six android apps.
additionally we conducted a user study investigating the effort required for developers to manually detect duplicate video based bug reports and compared this to the effort required to use t ango .
the results reveal that tango s optimal configuration is highly effective at detecting duplicate video based bug reports accurately ranking target duplicate videos in the top returned results in of the tasks.
additionally our user study shows that on average t ango can reduce developer effort by over illustrating its practicality.
index terms bug reporting screen recordings duplicate detection i. i ntroduction many modern mobile applications apps allow users to report bugs in a graphical form given the gui based nature of mobile apps.
for instance android and ios apps can include built in screen recording capabilities in order to simplify the reporting of bugs by end users and crowd testers .
the reporting of visual data is also supported by many crowd testing and bug reporting services for mobile apps which intend to aid developers in collecting processing and understanding the reported bugs .
the proliferation of sharing images to convey additional context for understanding bugs e.g.
in stack overflow q as has been steadily increasing over the last few years .
given this and the increased integration of screen capture technologyinto mobile apps developers are likely to face a growing set of challenges related to processing and managing app screenrecordings in order to triage and resolve bugs and hence maintain the quality of their apps.
one important challenge that developers will likely face in relation to video related artifacts is determining whether two videos depict and report the same bug i.e.
detecting duplicate video based bug reports as it is currently done for textual bug reports .
when video based bug reports are collected at scale either via a crowdsourced testing service or by popular apps the sizable corpus of collected reports will likely lead to significant developer effort dedicated to determining if a new bug report depicts a previously reported fault which is necessary to avoid redundant effort in the bug triaging and resolution process .
in a user study which we detail later in this paper sec.
iii e we investigated the effort required for experienced programmers to identify duplicate video based bug reports and found that participants reported a range of difficulties for the task e.g.
a single change of one step can result in two very similar looking videos showing entirely different bugs and spent about 20seconds of comprehension effort on average per video viewed.
if this effort is extrapolated to the large influx of bug reports that could be collected on a daily basis it illustrates the potential for the excessive effort associated with video based duplicate bug detection.
this is further supported by the plans of a large company that supports crowd sourced bug reporting name omitted for anonymity which we contacted as part of eliciting the design goals for this research who stated that they anticipate increasing developer effort in managing videobased reports and that they are planning to build a feature in their framework to support this process.
to aid developers in determining whether video based bug reports depict the same bug this paper introduces t ango a novel approach that analyzes both visual and textual information present in mobile screen recordings using tailored computer vision cv and text retrieval tr techniques with the goal of generating a list of candidate videos from an issue tracker similar to a target video based report.
in practice tango is triggered upon the submission of a new video based report to an issue tracker.
a developer would then use t ango to retrieve the video based reports that are most similar e.g.
ieee acm 43rd international conference on software engineering icse .
ieee top to the incoming report for inspection.
if duplicate videos are found in the ranked results the new bug report can be marked as a duplicate in the issue tracker.
otherwise the developer can continue to inspect the ranked results until she has enough confidence that the newly reported bug was not reported before i.e.
it is not a duplicate .
tango operates purely upon the graphical information in videos in order to offer flexibility and practicality.
these videos may show the unexpected behavior of a mobile app i.e.
a crash or other misbehavior and the steps to reproduce such behavior.
two videos are considered to be duplicates if they show the same unexpected behavior aka a bug regardless of the steps to reproduce the bug.
given the nature of screen recordings video based bug reports are likely to depict unexpected behavior towards the end of the video.
tango attempts to account for this by leveraging the temporal nature of video frames and weighting the importance of frames towards the end of videos more heavily than other segments.
we conducted two empirical studies to measure i the effectiveness of different configurations of t ango by examining the benefit of combining visual and textual information from videos as opposed to using only a single information source and ii t ango s ability to save developer effort in identifying duplicate video based bug reports.
to carry out these studies we collected a set of video bug reports from six popular apps used in prior research and defined duplicate detection tasks that resemble those that developers currently face for textual bug reports wherein a corpus of potential duplicates must be ranked according to their similarity to an incoming bug report.
the results of these studies illustrate that t ango s most effective configuration which selectively combines visual and textual information achieves .
mrr and .
map an average rank of .
a h it of .
and a h it of .
this means that t ango is able to suggest correct duplicate reports in the top of the ranked candidates for of duplicate detection tasks.
the results of the user study we conducted with experienced programmers demonstrate that on a subset of the tasks t ango can reduce the time they spend in finding duplicate video based bug reports by .
in summary the main contributions of this paper are t ango a duplicate detection approach for video based bug reports of mobile apps which is able to accurately suggest duplicate reports the results of a comprehensive empirical evaluation that measures the effectiveness of t ango in terms of suggesting candidate duplicate reports the results of a user study with experienced programmers that illustrates t ango s practical applicability by measuring its potential for saving developer effort and a benchmark included in our online appendix that enables i future research on video based duplicate detection bug replication and mobile app testing and ii the replicability of this work.
the benchmark contains video based bug reports with duplicates sourcecode trained models duplicate detection tasks t ango s output and detailed evaluation results.
ii.
t hetango approach tango detecting duplic ate scree nrecordin gs of s oftware bugs is an automated approach based on cv and tr techniques which leverages visual and textual information to detect duplicate video based bug reports.
a.tango overview tango models duplicate bug report detection as an information retrieval problem.
given a new video based bug report tango computes a similarity score between the new video and videos previously submitted by app users in a bug tracking system.
the new video represents the query and the set of existing videos represent the corpus.
t ango sorts the corpus of videos in decreasing order by similarity score and returns a ranked list of candidate videos.
in the list those videos which are more likely to show the same bug as the new video are ranked higher than those that show a different bug.
tango has two major components which we refer to as tango visand t ango txt that compute video similarity scores independently.
t ango viscomputes the visual similarity and t ango txtcomputes the textual similarity between videos.
the resulting similarity scores are linearly combined to obtain a final score that indicates the likelihood of two videos being duplicates.
in designing t ango vis we incorporated support for two methods of computing visual similarity one of which is sensitive to the sequential order of visual data and the other one that is not and we evaluate the effectiveness of these two techniques in experiments described in sec.
iii iv.
the first step in t ango s processing pipeline is to decompose the query video and videos from the existing corpus into their constituent frames using a given sampling rate i.e.
and frames per second fps .
then the t ango vis and t ango txtcomponents of the approach are executed in parallel.
the un ordered tango vispipeline is shown at the top of fig.
comprising steps v1 v3 the ordered tango vis pipeline is illustrated in the middle of fig.
comprising steps v1 v4 and v5 and finally the t ango txtpipeline is illustrated at the bottom of fig.
through steps t1 t3.
any of these three pipelines can be used to compute the video ranking independently or in combination i.e.
combining the two tango vistogether one t ango vispipeline with t ango txt which we call t ango comb or all three see sec.
iii c .
next we discuss these three pipelines in detail.
b. t ango vis measuring unordered visual video similarity the unordered version of t ango viscomputes the visual similarity svis of video based bug reports by extracting visual features from video frames and converting these features into a vector based representation for a video using a bagof visual words bovw approach .
this process is depicted in the top of fig.
.
the visual features are extracted by the visual feature extractor model v1in fig.
.
then thevisual indexer v2assigns to each frame feature vector a 958new bug report existing reportsvideo video framesfeature vectorsvisual featureextractorsimclr siftt extualextractorocr t extual documentsvisual indexerk means t extualindexerapache lucenebag of visual words indexed bag of wordsvisualencodertf idf t extualencodertf idft extual vectorsvisualvectorssimilarity computation rankingvideo ranking v1video 2video n tangovistangotxtsv2v3 t1t2t3video overlap identifierfuzzy weighted lcsv4sequentialcomparatorlcs normalizationv5videooverlap n fig.
.
the t ango approach for detecting duplicate video based bug reports.
visual word from a visual codebook and produces a bovw for a video.
the visual encoder v5 based on the video bovw encodes the videos using a tf idf representation that can be used for similarity computation.
visual feature extraction the visual feature extractor v1can either use the sift algorithm to extract features or simclr a recently proposed deep learning dl model capable of learning visual representations in an unsupervised contrastive manner.
t ango s implementation of simclr is adapted to extract visual features from app videos.
the first method by which t ango can extract visual features is using the scale invariant feature transform sift algorithm.
sift is a state of the art model for extracting local features from images that are invariant to scale and orientation.
these features can be matched across images for detecting similar objects.
this matching ability makes sift promising for generating features that can help locate duplicate images in our case duplicate video frames by aggregating the extracted features.
t ango s implementation of sift does not resize images and uses the top features that are the most invariant to changes and are based on the local contrast of neighboring pixels with higher contrast usually meaning more invariant.
this is done to reduce the number of sift features which could reach at least three orders of magnitude for a single frame and make the visual indexing v2 through k means see sec.
ii b2 computationally feasible.
the other technique that t ango can use to extract features is simclr.
in essence the goal of this technique is to generate robust visual features that cluster similar images together while maximizing the distance between dissimilar images in an abstract feature space.
this is accomplished by i generating sets of image pairs containing one original image and one augmented image and applying a variety of random augmentations i.e.
image cropping horizontal flipping color jittering and gray scaling ii encoding this set of image pairs using a base encoder typically a variation of a convolutional neural network cnn and iii training a multi layer perceptron mlp to produce feature vectors that increase the cosine similarity between each pair of image variants and decrease the cosine similarity between negative examples where negative examples for a given image pair arerepresented as all other images not in that pair for a given training batch.
t ango s implementation of simclr employs the resnet50 cnn architecture as the base encoder where this architecture has been shown to be effective .
to ensure that t ango svisual feature extractor is tailored to the domain of mobile app screenshots we trained this component on the entire rico dataset which contains over 66k android screenshots from over 9k of the most popular apps on google play.
our implementation of simclr was trained using a batch size of 792and100epochs the same hyperparameters e.g.
learning rate weight decay etc.
recommended by chen et al.
in the original simclr paper and resized images to to ensure consistency with our base resnet50 architecture.
the training process was carried out on an ubuntu .
server with three nvidia t4 tesla 16gb gpus.
the output of the feature extractor for simclr is a feature vector of size for each frame of a given video.
visual indexing while the simclr or sift feature vectors generated by t ango svisual feature extractor v1 could be used to directly compute the similarity between video frames recent work has suggested that a bovw approach combined with a tf idf similarity measure is more adept to the task of video retrieval .
therefore to transform the simclr or sift feature vectors into a bovw representation tango uses a visual indexing process v2.
this process produces an artifact known as a codebook that maps simclr or sift feature vectors to visual words which are discrete representations of a given image and have been shown to be suitable for image and video recognition tasks .
the codebook derives these visual words by clustering feature vectors and computing the centroids of these clusters wherein each centroid corresponds to a different visual word.
the codebook makes use of the k means clustering algorithm where the krepresents the diversity of the visual words and thus can affect the representative power of this indexing process.
t ango s implementation of this process is configurable to 1k 5k or 10k for the knumber of clusters i.e.
the number of visual words vw .
1k vw and 10k vw were selected as recommended by kordopatis zilos et al.
and we included 5k vw as a middle ground to 959better understand how the number of visual words impacts tango s performance.
a codebook is generated only once for a givenk however it must be trained before it can be applied to convert an input feature vector to its corresponding visual word s .
once trained a codebook can then be used to map visual words from frame feature vectors without any further modification.
thus we trained t ango s six codebooks three for sift and three for simclr using features extracted from 000randomly selected images from the rico dataset .
we did not use the entire rico dataset due to computational constraints of the k means algorithm.
after the feature vector for a video frame is passed through thevisual indexing process it is mapped to its bovw representation by a trained codebook.
to do this the codebook selects the closest centroid to each visual feature vector based on euclidean distance.
for sift this process may generate more than one feature vector for a single frame due to the presence of multiple sift feature descriptors.
in this case tango assigns multiple visual words to each frame.
for simclr t ango assigns one visual word to each video frame as simclr generates only one vector per frame.
visual encoding after the video is represented as a bovw the visual encoder v3computes the final vector representation of the video through a tf idf based approach .
the term frequency tf is computed as the number of visual words occurrences in the bovw representation of the video and the inverse document frequency idf is computed as the number of occurrences of a given visual word in the bovw representations built from the rico dataset.
since rico does not provide videos but individual app screenshots we consider each rico image as one document.
we opted to use rico to compute our idf representation for two reasons i to combat the potentially small number of frames present in a given video recording and ii to bolster the generalizability of our similarity measure across a diverse set of apps.
similarity computation given two videos t ango vis encodes them into their bovw representations and each video is represented as one visual tf idf vector.
these vectors are compared using cosine similarity which is taken as the visual similarity sof the videos svis sbov w .
c. t ango vis measuring ordered visual video similarity the ordered version of t ango visconsiders the sequence of video frames when comparing two videos and is capable of giving more weight to common frames nearer the end of the videos as this is likely where buggy behavior manifests.
to accomplish this the feature vector extractor v1is used to derive descriptive vectors from each video frame using either simclr or sift.
t ango determines how much the two videos overlap using an adapted longest common substring lcs algorithm v4.
finally during the sequential comparison process v5 tango calculates the similarity score by normalizing the computed lcs score.
video overlap identification in order to account for the sequential ordering of videos t ango employs two different versions of the longest common substring lcs algorithm.the first version which we call fuzzy lcs f lcs modifies the comparison operator of the lcs algorithm to perform fuzzy matching instead of exact matching between frames in two videos.
this fuzzy matching is done differently for simclr and sift derived features.
for simclr given that each frame is associated with only a single visual word the standard bovw vector would be too sparse for a meaningful comparison.
therefore we compare the feature vectors that simclr extracts from the two frames directly using cosine similarity.
for sift we utilize the bovw vectors derived by thevisual encoder v3 but at a per frame level.
the second lcs version which we call weighted lcs wlcs uses the same fuzzy matching that f lcs performs.
however the similarity produced in this matching is then weighted depending on where the two frames from each video appeared.
frames that appear later in the video are weighted more heavily since that is where the buggy behavior is typically occurring in a video based bug report and thus should be given more weight for duplicate detection.
the exact weighting scheme used isi m j m whereiis theithframe of video a mis the of frames in video a jis thejthframe of video b and nis the of frames in video b. sequential comparison in order to incorporate the lcs overlap measurements into t ango s overall video similarity calculation the overlap scores must be normalized between zero and one .
to accomplish this we consider the case where two videos overlap perfectly to be the upper bound of the possible lcs score between two videos and use this to perform the normalization.
for f lcs this is done by simply dividing by the of frames in the smaller video since the max possible overlap that could occur is when the smaller video is a subsection in the bigger video calculated as overlap min whereoverlap denotes the amount the two videos share in terms of their frames and min denotes the of frames in the smaller of the two videos.
for w lcs if the videos are different lengths we align the end of the shorter video to the endof the longer video and consider this the upper bound on the lcs score which is normalized as follows sw lcs overlapp1 i mini min max i max wheresw lcs is the normalized similarity value produced by w lcs overlap andmin are similar to the f lcs calculation andmax denotes the of frames in the longer of the two videos.
the denominator in eq.
calculates the maximum possible overlap that can occur if the videos were exact matches summing across the similarity score of each frame pair.
our online appendix contains the detailed f w lcs algorithms with examples .
similarity computation f lcs and w lcs output the visual similarity sscoresf lcs andsw lcs respectively.
this can be combined with sbov w to obtain an aggregate visual similarity score svis sbov w sf lcs 2or svis sbov w sw lcs .
we denote these t ango vis variations as b f lcs and b w lcs respectively.
960d.
determining the textual similarity between videos in order to determine the textual similarity between videobased bug reports t ango leverages the textual information from labels titles messages etc.
found in the app gui components and screens depicted in the videos.
tango txtadopts a standard text retrieval approach based on lucene and optical character recognition ocr to compute the textual similarity stxt between video based bug reports.
first a textual document is built from each video in the issue tracker t1in fig.
using ocr to extract text from the video frames.
the textual documents are preprocessed using standard techniques to improve similarity computation namely tokenization lemmatization and removal of punctuation numbers special characters and one and twocharacter words.
the pre processed documents are indexed for fast similarity computation t2.
each document is then represented as a vector using tf idf and the index t3.
in order to build the textual documents from the videos tango txtapplies ocr on the video frames through the tesseract engine in the textual extractor t1.
we experiment with three strategies to compose the textual documents using the extracted frame text.
the first strategy all text concatenates all the text extracted from the frames.
the second strategy unique frames concatenates all the text extracted from unique video frames determined by applying exact text matching before text pre processing .
the third strategy unique words concatenates the unique words in the frames after pre processing .
similarity computation tango computes the textual similarity stxt in susing lucene s scoring function based on cosine similarity and document length normalization.
e. combining visual and textual similarities tango combines both the visual svis and textual stxt similarity scores produced by t ango visand t ango txt respectively sin fig.
.
t ango uses a linear combination approach to produce an aggregate similarity value scomb w svis w stxt wherewis a weight for svisandstxt and takes a value between zero and one .
smaller wvalues weight svis more heavily and larger values weight stxtmore heavily.
we denote this approach as t ango comb.
based on the combined similarity t ango generates a ranked list of the video based bug reports found in the issue tracker.
this list is then inspected by the developer to determine if a new video reports a previously reported bug.
iii.
t ango sempirical evaluation design we empirically evaluated t ango with two goals in mind i determining how effective t ango is at detecting duplicate video based bug reports when considering different configurations of components and parameters and ii estimating the effort that t ango can save developers during duplicate video bug detection.
based on these goals we defined the following research questions rqs rq how effective is tango when using either visual or textual information alone to retrieve duplicate videos?
rq what is the impact of combining frame sequence and visual information on tango s detection performance?
rq how effective is tango when combining both visual and textual information for detecting duplicate videos?
rq how much effort can tango save developers in finding duplicate video based bug reports?
to answer our rqs we first collected video based bug reports for six android apps sec.
iii a and based on them defined a set of duplicate detection tasks sec.
iii b .
we instantiated different configurations of t ango by combining its components and parameters sec.
iii c and executed these configurations on the defined tasks sec.
iii d .
based on standard metrics applied on the video rankings that t ango produces we measured t ango s effectiveness sec.
iii d .
we answer rq rq and rq 3based on the collected measurements.
to answer rq sec.
iii e we conducted a user study where we measured the time humans take to find duplicates for a subset of the defined tasks and estimated the time t ango can save for developers.
we present and discuss the evaluation results in sec.
iv.
a. data collection we collected video based bug reports for six open source android apps namely antennapod apod time tracker time token tok gnucash gnu growtracker grow and droid weight droid .
we selected these apps because they have been used in previous studies support different app categories finance productivity etc.
and provide features that involve a variety of gui interactions taps long taps swipes etc.
.
additionally none of these apps are included as part of the rico dataset used to train t ango s simclr model and codebooks preventing the possibility of data snooping.
since video based bug reports are not readily available in these apps issue trackers we designed and carried out a systematic procedure for collecting them.
in total we collected videos that display distinct bugs bugs for each app and three videos per bug i.e.
three duplicate videos per bug .
from the bugs five bugs one bug per app except for droid are reported in the apps issue trackers.
these five bugs were selected because they were the only ones in the issue trackers that we were able to reproduce based on the provided bug descriptions.
during the reproduction process we discovered five additional new bugs in the apps not reported in the issue trackers one bug each for apod gnu and tok and two bugs for time for a total of confirmed real bugs.
the remaining bugs were introduced in the apps through mutation by executing mutapk a mutation testing tool that injects bugs i.e.
mutants into android apk binaries via a set of mutation operators that were derived from a largescale empirical study on realandroid application faults.
given the empirically derived nature of these operators they were 961shown to accurately simulate real world android faults .
we applied mutapk to the apks of all six apps.
then from the mutant list produced by the tool we randomly selected to bugs for each app and ensured that they could be reproduced and manifested in the gui.
to diversify the bug pool we selected the bugs from multiple mutant operators and ensured that they affected multiple app features screens.
when selecting the bugs we ensured they manifest graphically and were reproducible by manually replicating them on a specific android emulator configuration virtual nexus 5x with android .
configured via android studio .
for all the bugs we screen recorded the bug and the reproduction scenario.
we also generated a textual bug report for bugs that did not have one containing the description of the unexpected and expected app behavior and the steps to reproduce the bug.
to generate the remaining video based bug reports we asked two professional software engineers and eight computer science cs ph.d. students to replicate and record the bugs using the same android emulator based only on the textual description of the unexpected and expected app behavior.
the participants have between and years of programming experience median of years .
all the textual bug reports given to the study participants contained only a brief description of the observed and expected app behavior with no specific reproduction steps .
we opted to perform the collection in this manner to ensure the robustness of our evaluation dataset by maximizing the diversity of video based reproduction steps and simulating a real world scenario where users are aware of the observed incorrect and expected app behavior and must produce the reproduction steps themselves.
we randomly assigned the bugs to the participants in such a way that each bug was reproduced and recorded by two participants and no participant recorded the same bug twice.
before reproducing the bugs the participants spent five minutes exploring the apps to become familiar with their functionality.
since some of the participants could not reproduce the bugs after multiple attempts mainly due to bug misunderstandings and some of the videos were incorrectly recorded due to mistakes we reassigned these bugs among the other participants who were able to reproduce and record them successfully.
our bug dataset consists of crashes and non crashes and include a total of steps taps long taps swipes among other types with an average of .
steps per video.
the average video length is 28seconds.
b. duplicate detection tasks for each app we defined a set of tasks that mimic a realistic scenario for duplicate detection.
each duplicate detection task is composed of a new video i.e.
the new bug report akathe query and a set of existing videos i.e.
existing bug reports in the issue tracker akathe corpus .
in practice a developer would determine if the new video is a duplicate by inspecting the corpus of videos in the order given by t ango or anyother approach .
for our task setup the corpus contains both duplicate and non duplicate videos.
there are two different types of duplicate videos that exist in the corpus i those videos that are a duplicate of the query the same bug group and ii those videos which are duplicates of each other but are not a duplicate of the query the different bug group .
this second type of duplicate video is represented by bug reports marked as duplicates in the issue tracker and their corresponding master reports .
each non duplicate video reports a distinct bug.
we constructed the duplicate detection tasks on a per app basis using the video reports collected for each app i.e.
three video reports for each of the bugs for a total of video reports per app .
we first divided all the videos for an app into three groups each group containing videos one for each bug created by one or more participants.
then we randomly selected a video from one bug as the query and took the other two videos that reproduce the same bug as the same bug duplicate group i.e.
the ground truth .
then we selected one of the remaining nine bugs and added its three videos to thedifferent bug duplicate group.
finally we selected one video from the remaining eight bugs and used these as the corpus non duplicate group.
this resulted in a total of distinct bug reports per task two in the same bug group three in the different bug group eight in the non duplicate group and the query video .
after creating tasks based on all the combinations of query and corpus we generated a total of810duplicate detection tasks per app or 860aggregating across all apps.
we designed the duplicate detection setting described above to mimic a scenario likely to be encountered in crowd sourced app testing where duplicates of the query other duplicates not associated with the query and other videos reporting unique bugs exist in a shared corpus for a given app.
while there are different potential task settings we opted not to vary this experimental variable in order to make for a feasible analysis that allowed us to explore more thoroughly the different tango configurations.
c.tango configurations we designed t ango visand t ango txtto have different configurations.
t ango vis s configurations are based on different visual feature extractors sift or simclr video sampling rates and fps of visual words 1k 5k and 10k vw and approaches to compute video similarity bovw f lcs w lcs b f lcs and b w lcs .
t ango txt s configurations are based on the same sampling rates and fps and the approaches to extract the text from the videos all text unique frames and unique words .
t ango comb combines t ango visand t ango txtas described in sec.
ii e. d.tango s execution and effectiveness measurement we executed each t ango configuration on the duplicate detection tasks and measured its effectiveness using standard metrics used in prior text based duplicate bug detection research .
for each task we compare the 962ranked list of videos produced by t ango and the expected duplicate videos from the ground truth.
we measured the rank of the first duplicate video found in the ranked list which serves as a proxy for how many videos the developer has to watch in order to find a duplicate video.
a smaller rank means higher duplicate detection effectiveness.
based on the rank we computed the reciprocal rank metric rank .
we also computed the average precision of t ango which is the average of the precision values achieved at all the cutting points k of the ranked list i.e.
precision k .
precision k is the proportion of the top k returned videos that are duplicates according to the ground truth.
we also computed hit k akarecall rate k which is the proportion of tasks that are successful for the cut point k of the ranked list.
a task is successful if at least one duplicate video is found in the top k results returned by t ango .
we report hit k for cut points k in this paper and in our online appendix .
additionally we computed the average of these metrics over sets of duplicate detection tasks mean reciprocal rank mrr mean average precision map and mean rank rank or rk per app and across all apps.
higher mrr map and hit k values indicate higher duplicate detection effectiveness.
these metrics measure the overall performance of a duplicate detector.
we focused on comparing mrr values to decide if one tango configuration is more effective than another as we consider that it better reflects the usage scenario of t ango .
in practice the developer would likely stop inspecting the suggested duplicates given by t ango when she finds the first correct duplicate.
this scenario is captured by mrr through the rank metric which considers only the first correct duplicate video as opposed to the entire set of duplicate videos to the query as map does .
e. investigating tango s effort saving capabilities we conducted a user study in order to estimate the effort that developers would spend while manually finding video based duplicates.
this effort is then compared to the effort measurements of the best t ango configuration based on rank and hit k .
this study and the data collection procedure were conducted remotely due to covid constraints.
participants and tasks one professional software engineer and four cs ph.d. students from the data collection procedure described in sec.
iii a participated in this study.
the study focused on apod the app that all the participants had in common from the data collection.
we randomly selected duplicate detection tasks covering all apod bugs.
methodology each of the tasks was completed by two participants.
each participant completed four tasks each task s query video reporting a unique bug.
the assignment of the tasks to the participants was done randomly.
for each task the participants had to watch the new video the query and then find the videos in the corpus that showed the same bug of the new video i.e.
find the duplicate videos .
all the videos were anonymized so that the duplicate videos were unknownto the participants.
to do this we named each video with a number that represents the video order and the suffix vid e.g.
2vid.mp4 .
the corpus videos were given in random order and the participants could watch them in any order.
to make the bug of the new video clearer to the participants we provided them with the description of the unexpected and expected app behavior taken from the textual bug reports that we generated for the bugs.
we consider the randomization of the videos as a reasonable baseline given that other baselines e.g.
videobased duplicate detectors do not currently exist and the videobased bug reports in our dataset do not have timestamps which can be used to give a different order to the videos .
this is a threat to validity that we discuss in sec.
v. collected measurements through a survey we asked each participant to provide the following information for each task i the name of the first video they deemed a duplicate of the query ii the time they spent to find this video iii the number of videos they had to watch until finding the first duplicate including the duplicate iv the names of other videos they deemed duplicates and v the time they spent to find these additional duplicates.
we instructed the participants to perform the tasks without any interruptions in order to minimize inaccuracies in the time measurements.
comparing tango and manual duplicate detection the collected measurements from the participants were compared against the effectiveness obtained by executing the best tango configuration on the tasks in terms of rank and hit k .
we compared the avg.
number of videos the participants watched to find one duplicate against the avg.
number of videos they would have watched had they used t ango .
iv.
t ango sevaluation results a. rq1 using only visual or textual information we analyzed the performance of t ango when using only visual or textual information exclusively.
in this section we present the results for t ango s best performing configurations.
however complete results can be found in our online appendix .
table i shows the results for t ango visand tango txtwhen using simclr sift as the visual feature extractor and ocr as the textual extractor .
for simplicity we use simclr sift and ocr ir to refer to simclrbased t ango vis sift based t ango vis and t ango txt respectively.
the best results for each metric are illustrated in bold on a per app basis.
the results provided in table i are those for the best parameters of the simclr sift and ocr ir feature extractors which are bovw fps 1k vw w lcs fps 10k vw and all text fps respectively.
table i shows that t ango visis more effective when using simclr rather than sift across all the apps achieving an overall mrr map avg.
rank hit and hit of .
.
.
.
and respectively.
simclr is also superior to ocr ir overall whereas sift performs least effectively of the three approaches.
when analyzing the results per app we observe that simclr is outperformed by ocr ir by .
difference in mrr for apod 963table i effectiveness for the best tango configurations that use either visual simclr sift or textual ocr ir information .
app config.
mrr map rk hit hit apodsift .
.
.
.
.
simclr .
.
.
.
.
ocr ir .
.
.
.
.
droidsift .
.
.
.
.
simclr .
.
.
.
.
ocr ir .
.
.
.
.
gnusift .
.
.
.
.
simclr .
.
.
.
.
ocr ir .
.
.
.
.
growsift .
.
.
.
.
simclr .
.
.
.
.
ocr ir .
.
.
.
.
timesift .
.
.
.
.
simclr .
.
.
.
.
ocr ir .
.
.
.
.
toksift .
.
.
.
.
simclr .
.
.
.
.
ocr ir .
.
.
.
.
overallsift .
.
.
.
.
simclr .
.
.
.
.
ocr ir .
.
.
.
.
droid gnu and grow with ocr ir being the most effective for these apps.
simclr outperforms the other two approaches for time and tok by more than difference in mrr.
the differences explain the overall performance of simclr and ocr ir.
simclr is more consistent in its performance compared to ocr ir and sift.
across apps the mrr standard deviation of simclr is .
which is lower than that for sift and ocr ir .
and .
respectively.
the trend is similar for map and avg.
rank.
since the least consistent approach across apps is t ango txt in terms of effectiveness we investigated the root causes for its lower performance on time and tok.
after manually watching a subset of the videos for these apps we found that their textual content was quite similar across bugs.
based on this we hypothesized that the amount of vocabulary shared between duplicate videos from the same bugs and nonduplicate videos across different bugs affected the discriminatory power of lucene based t ango txt see sec.
ii d .
to verify this hypothesis we measured the shared vocabulary of duplicate and non duplicate video pairs similarly to chaparro et al.
s analysis of textual bug reports .
we formed unique pairs of duplicate and non duplicate videos from all the videos collected for all six apps.
for each app we formed duplicate and non duplicate pairs and we measured the avg.
amount of shared vocabulary of all pairs using the vocabulary agreement metric used by chaparro et al.
.
table ii shows the vocabulary agreement of duplicate vd and non duplicate pairs vnd as well as the mrr and map values of t ango txtfor each app.
the table reveals that the vocabulary agreement of duplicates and non duplicates is very similar for time and tok and dissimilar for the other apps.
the absolute difference between these measurementstable ii vocabulary agreement effectiveness for the best tango txt.
appvocabulary agreementmrr mapvd vndjvd vndj apod .
.
.
.
.
droid .
.
.
.
.
gnu .
.
.
.
.
grow .
.
.
.
.
time .
.
.
.
.
tok .
.
.
.
.
overall .
.
.
.
.
i.e.
jvd vndj for time and tok is .
and .
while for the other apps it is above .
we found .
.
pearson correlation between these differences and the mrr map values.
the results indicate that for time and tok the similar vocabulary between duplicate and non duplicate videos negatively affects the discriminatory power of t ango txt which suggests that for some apps using only textual information may be sub optimal for duplicate detection.
answer for rq simclr performs the best overall with an mrr and hit of .
and .
respectively.
for of apps ocr ir outperforms simclr by a significant margin.
however due to issues with vocabulary overlap it performs worse overall.
sift is the worst performing technique across all the apps.
b.rq2 combining visual and f rame sequence information toanswer rq we compared the effecti veness of the best configuration of tangowhen using visual information alone simclr bovw 5fps 1k vw and when combining visual frame sequence information i.e.
b f lcs and b w lcs .
the results are sho wn in table iii.
ov erall using tango with bovw alone is more ef fectiv e than combining the approaches t angobased on bovw achie ves75.
.
.
.
and mrr map avg.
rank hit and hit respecti vely.
using bovw and w lcs combined is the least ef fectiv e approach.
bovw alone and b f lcs are comparable in performance.
howe ver bovw is more consistent in its performance across apps .
mrr std.
deviation vs. .
and .
for b f lcs and b w lcs.
the per app results reveal that b w lcs consistently is the least ef fectiv e approach for all apps e xcept for gro w for which b w lcs performs best.
after w atching the videos for gro w we found unnecessary steps in the beginning middle of the duplicate videos which led to their endings being weighted more heavily by w lcs where steps were similar .
in contrast bovw and b f lcs gi vea lower weight to these cases thus reducing the overall video similarity.
the lo wer performance of b f lcs and b w lcs compared to bovw is partially explained by the factthat f lcs and w lcs are more restrictiv e by definition.
since they find the longest common sub strings of frames between videos 964table iii effectiveness for the best tango vis configuration using either visual information bovw or a combination of visual and frame sequence information b f lcs and b w lcs .
app config.
mrr map rk hit hit apodb f lcs .
.
.
.
.
b w lcs .
.
.
.
.
bovw .
.
.
.
.
droidb f lcs .
.
.
.
.
b w lcs .
.
.
.
.
bovw .
.
.
.
.
gnub f lcs .
.
.
.
.
b w lcs .
.
.
.
.
bovw .
.
.
.
.
growb f lcs .
.
.
.
.
b w lcs .
.
.
.
.
bovw .
.
.
.
.
timeb f lcs .
.
.
.
.
b w lcs .
.
.
.
.
bovw .
.
.
.
.
tokb f lcs .
.
.
.
.
b w lcs .
.
.
.
.
bovw .
.
.
.
.
overallb f lcs .
.
.
.
.
b w lcs .
.
.
.
.
bovw .
.
.
.
.
small variations e.g.
extra steps in the reproduction steps of the bugs may lead to drastic changes in similarity measurement for these approaches.
also these approaches only find one common substring i.e.
the longest one which may not be highly discriminative for duplicate detection.
in the future we plan to explore additional approaches for aligning the frames for example by using an approach based on longest common sub sequence algorithms that can help align multiple portions between videos.
another potential reason for these results may lie in the manner that t ango combines visual and sequential similarity scores weighting both equally.
in future work we plan to explore additional combination techniques.
answer for rq combining ordered visual information via f lcs and w lcs with the orderless bovw improves the results for four of the six apps.
however across all apps bovw performs more consistently.
c.rq3 combining visual and t extual information weinvestigated tango seffecti veness when combining visual and textual information.
w e selected the best configurations of tangovis simclr bovw 5fps 1k vw and tangotxt all text fps from rq 1based on their mrr score and measured its performance overall and per app.
weprovide the results for the best weight we obtained for t ango s similarity computation and r anking which w asw i.e.
a weight of .
and .
on t angovisand t angotxt respectiv ely.these weights were found by evaluating different wvalues from zero to one in increments of 1and selecting the one leading to the highest o verall mrr score.
complete results can be found in our online appendix .table iv effectiveness of thebesttangocomb tangovis andtangotxt.
app config.
mrr map rk hit hit apodtangocomb .
.
.
.
.
tangovis .
.
.
.
.
tangotxt .
.
.
.
.
droidtangocomb .
.
.
.
.
tangovis .
.
.
.
.
tangotxt .
.
.
.
.
gnutangocomb .
.
.
.
.
tangovis .
.
.
.
.
tangotxt .
.
.
.
.
gro wtangocomb .
.
.
.
.
tangovis .
.
.
.
.
tangotxt .
.
.
.
.
timetangocomb .
.
.
.
.
tangovis .
.
.
.
.
tangotxt .
.
.
.
.
toktangocomb .
.
.
.
.
tangovis .
.
.
.
.
tangotxt .
.
.
.
.
overalltangocomb .
.
.
.
.
tangovis .
.
.
.
.
tangotxt .
.
.
.
.
table iv shows that the overall ef fectiv eness achie vedby tangocomb is higher than that achiev ed by tangotxtand tangovis.
tangocomb achiev es .
.
.
.
and .
mrr map avg.
rank hit and hit on average.
the avg.
improv ement mar gin of tangocomb is substantially higher for t angotxt .
.
mrr map than for tangovis .
.
mrr map .
our analysis of the per app results e xplains these differ ences.
t able iv reveals that combining visual and te xtual information substantially increases the performance o verjust using one of the information types alone e xcept for the time and t ok apps.
this is because t angotxt seffecti veness is substantially lo wer for these apps compared to the visual version see t able i due to the aforementioned vocab ulary agreement.
thus incorporating the textual information significantly harms the performance of t angocomb.
a better combination of visual and textual information the results indicate that combining visual and textual information is beneficial for most of our studied apps b ut harmful for a subset time and t ok .
this is because the te xtual information used alone for time and tok leads to lowperformance.
the analysis we made for t angotxtinrq revealed that the reason for the low performance of tangotxtlies in the similar amount of vocabulary overlap between duplicate and non duplicate videos.
fortunately based on this amount of vocab ulary we can predict the performance of tangotxtfor new video based b ug reports as follo ws .
in practice the issue track er will contain reports marked as duplicates reporting the same b ugs from previous submissions of bug reports as well as non duplicates report ing unique bugs .
this information can be used to compute the v ocabulary agreement between duplicates and non duplicates which ca n be used to predict ho w well tangotxtwould perform for new reports.
965based on this we defined a new approach for t ango which is based on the vocabulary agreement metric from applied on existing duplicate and non duplicate reports.
this approach dictates that if the difference of vocabulary agreement between existing duplicates and non duplicates is greater than a certain threshold then t ango should combine visual and textual information.
otherwise t ango should only use the visual information because it is likely that the combination would not be better than using the visual information alone.
from the vocabulary agreement measurements shown in table ii we infer a proper threshold from the new t ango approach.
this threshold may be taken as one value from the interval .
.
exclusive because those are the limits that separate the apps for which t ango txtobtains low time and tok and high performance apod droid gnu and grow .
for practical reasons we select the threshold to be the middle value .
in future work we plan to further evaluate this threshold on other apps.
we implemented this approach for t ango using .
as weight and measured its effectiveness.
this approach resulted in a mrr map avg.
rank hit and hit of .
.
.
.
and respectively.
the approach leads to a substantial improvement i.e.
.
.
higher mrr map over t ango comb shown in table iv.
the results mean that the best version of t ango is able to suggest correct duplicate video based bug reports in the first or second position of the returned candidate list for of the duplicate detection tasks.
answer for rq combining visual and textual information significantly improves results for of apps.
however due to the vocabulary agreement issue across all apps this approach is similar in effectiveness to using visual information alone.
accounting for this vocabulary overlap issue through a selective combination of visual and textual information via a threshold t ango achieves the highest effectiveness an mrr map avg.
rank hit and hit of .
.
.
.
and respectively.
d.rq4 t ime saved discovering duplicates as e xpected the participants were successful in finding the duplicate videos for all tasks.
in only one task one participant incorrectly flagged a video as duplicate because it was highly similar to the query.
participants found the first duplicate video in .
seconds and watched .
videos on avg.
across all tasks to find it.
participants also found all the duplicates in .
seconds on a vg.
by watching the entire corpus of videos.
this means the y spent .
seconds in watching one video on average.
wecompared these results with the meas urements tak en from t ango sbest v ersion i.e.
selectiv e tango onthe tasks the participants completed.
t angoachiev ed a .
a vg.
rank which means that by using t ango they would only have to watch one or twovideos on avg.
to find the first duplicate.
thiswould have resulted in of the time saved.
in other w ords instead of spending seconds on avg.
finding a duplicate for a given task the participants could have spent 5seconds.
these results indicate the potential of t angoto help developers sa ve time when finding duplicates.
answer for rq on average t ango s bestperforming configuration can save .
of the time participants spend finding duplicate videos.
v. tangolimitations t hreatstovalidity limitations.
tangohas three main limitations that motivate future w ork.
the first one stems from the finding that textual information may not be beneficial for some apps.
the best t angoversion implements an approach for detecting this situation based on a threshold for the difference in vocabulary overlap between duplicate and non duplicate videos which is used for selectively combining visual or textual information.
this threshold is based on the collected data and may not generalize to other apps.
second the visual tf idf representation for the videos is based on the mobile app images from the rico dataset rather than on the videos found in the tasks corpus as it is typically done in te xt retrie val.
additionally we considered single images as documents rather than groups of frames that make up a video.
these decisions were made to improv e the generalization of t ango svisual features and to support projects that have limited training data.
third differences in themes and languages across video based bug reports for an application could ha vean impact in the performance of t ango.
we belie vethat dif ferent themes i.e.
dark vs. light modes will not significantly impact t angosince the simclr model is trained to account for such color dif ferences by performing color jittering and gray scaling augmentations.
howe ver additional experiments are needed to validate this conjecture.
f or dif ferent languages tangocurrently assumes the te xt in an application to be english when performing ocr and te xtual similarity .
therefore its detection effecti veness where the bugreports display different languages e.g.
english vs. french could be negatively impacted.
wewill in vestig ate this aspect in our future w ork.
internal construct validity .most of the mobile app bugs in our dataset were introduced by mutapk and hence potentially may not resemble real bugs.
howe ver mutapk s mutation operators were empirically deriv ed from a large scale study of real android faults and prior research lends credence of the ability of mutants to resemble real faults .
w e intentionally selected generated mutan tsfrom a range of operators to increase the di versity of our set of bugs and mitig ate potential biases.
another potential threat is related to using real bugs from issue trackers that cannot be reproduced or that do not manifest graphically.
wemitigated this threat by using a small carefully v etted subset of real bugs that were analyzed by multiple authors before being used in 966our dataset.
we did not observe major differences in the results between mutants and real bugs.
another threat to validity is that our approach to construct the duplicate detection tasks does not take into account bug report timestamps which would be typical in a realistic scenario and timestamps could be used as a baseline ordering of videos for comparing against the ranking given by t ango .
the lack of timestamps stems from the fact that we were not able to collect the video based bug reports from existing mobile projects.
we mitigated this threat in our user study by randomizing the ordering of the corpus videos given to the participants.
we consider this as a reasonable baseline for evaluating our approach considering that to the best of our knowledge no existing datasets with timestamps are available for conducting research on video based duplicate detection and no existing duplicate detectors work exclusively on video based bug reports as t ango does.
external validity.
we selected a diverse set of apps that have different functionality screens and visual designs in an attempt to mitigate threats to external validity.
additionally our selection of bugs also attempted to select diverse bug types crashes and non crashes and the duplicate videos were recorded by different participants.
as previously discussed there is the potential that t ango s different parameters thresholds may not generalize to video data from other apps.
vi.
r elated work our research is related to work in near duplicate video retrieval analysis of graphical software artifacts and duplicate detection of textual bug reports.
near duplicate video retrieval.
extensive research has been done outside se in near duplicate video retrieval which is the identification of similar videos to a query video e.g.
exact copies or similar events .
the closest work to ours is by kordopatis zilos et al.
who addressed the problem of retrieving videos of incidents e.g.
accidents .
in their work they explored using handcrafted based e.g.
surf or hsv histograms and dl based e.g.
cnns visual feature extraction techniques and ways of combining the extracted visual features e.g.
vlad .
while we do make use of the best performing model cnn bovw from this work we did not use the proposed handcrafted approaches as these were designed for scenes about real world incidents rather than for mobile bug reporting.
we also further modified and extended this approach given our different domain through the combination of visual and textual information modalities and adjustments to the ccn bovw model including the layer configuration and training objective.
analysis of graphical software artifacts.
the analysis of graphical software artifacts to support software engineering tasks has been common in recent years.
such tasks include mobile app testing developer user behavior modelling gui reverse engineering and code generation analysis of programmingvideos and gui understanding and verification .
none of these works deal with finding duplicate video based bug reports which is our focus.
detection of duplicate textual bug reports.
many research projects have focused on detecting duplicate textual bug reports .
similar to t ango most of the proposed techniques return a ranked list of duplicate candidates .
the work most closely related to t ango is by wang et al.
who leveraged attached mobile app images to better detect duplicate textual reports.
visual features are extracted from the images e.g.
representative colors using computer vision which are combined with textual features extracted from the text to obtain an aggregate similarity score.
while this is similar to our work t ango is intended to be applied to videos rather than single images and focuses on video based bug reports alone without any extra information such as bug descriptions.
vii.
c onclusion and future work this paper presented t ango an approach that combines visual and textual information to help developers find duplicate video based bug reports.
our empirical evaluation conducted on4 duplicate detection tasks created from videobased bug reports from six mobile apps illustrates that t ango is able to effectively identify duplicate reports and save developer effort.
specifically t ango correctly suggests duplicate video based bug reports within the top candidate videos for of the tasks and saves .
of the time that humans spend finding duplicate videos.
our future work will focus on addressing t ango s limitations and extending t ango s evaluation.
specifically we plan to explore additional ways to address the vocabulary overlap problem investigate the resilience of t ango to different app characteristics such as the use of different themes languages and screen sizes extend t ango for detecting duplicate bug reports that contain multimedia information text images and videos evaluate t ango using data from additional apps and assess the usefulness of tango in industrial settings.
viii.
d ata availability our online appendix includes the collected videobased bug reports with duplicates t ango s source code trained models evaluation infrastructure t ango s output and detailed evaluation results.