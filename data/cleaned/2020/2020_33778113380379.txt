dissector input validation for deep learning applications by crossing layer dissection huiyan wang cocowhy1013 gmail.com state key lab for novel software technology nanjing university nanjing china department of computer science and technology nanjing university nanjing chinajingwei xu jingweix nju.edu.cn state key lab for novel software technology nanjing university nanjing china department of computer science and technology nanjing university nanjing chinachang xu changxu nju.edu.cn state key lab for novel software technology nanjing university nanjing china department of computer science and technology nanjing university nanjing china xiaoxing ma xxm nju.edu.cn state key lab for novel software technology nanjing university nanjing china department of computer science and technology nanjing university nanjing chinajian lu lj nju.edu.cn state key lab for novel software technology nanjing university nanjing china department of computer science and technology nanjing university nanjing china abstract deeplearning dl applicationsarebecomingincreasinglypopular.
their reliabilities largely depend on the performance of dl models integrated in these applications as a central classifying module.
traditional techniques need to retrain the models or rebuild and redeploytheapplicationsforcopingwithunexpectedconditions beyond the models handling capabilities.
in this paper we take afaulttoleranceapproach dissector todistinguishingthoseinputs that represent unexpected conditions beyond inputs from normalinputsthatarestillwithinthemodels handlingcapabilities within inputs thus keeping the applications still function with expected reliabilities.
the key insight of dissector is that a dl model should interpret a within input with increasing confidence while a beyond input would probably cause confused guesses in the prediction process.
dissector works in an application specific way adaptive to dl models used in applications and extremely efficiently scalabletolarge sizedatasetsfromcomplexscenarios.
the experimental evaluation shows that dissector outperformed state of the arttechniquesintheeffectiveness auc avg.
.
and up to .
and efficiency runtime overhead only .
.
milliseconds .besides italsoexhibitedencouragingusefulnessin correspondingauthors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists r equires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul 3fqvcmjd pg orea association for computing machinery.
acm isbn .
.
.
.
and20 for imagenet .
keywords deep learning fault tolerance input validation acm reference format huiyan wang jingwei xu chang xu xiaoxing ma and jian lu.
.
dissector inputvalidationfordeeplearningapplicationsbycrossinglayerdissection.in 42ndinternationalconferenceonsoftwareengineering icse may23 seoul korea.
acm newyork ny usa 12pages.
introduction deep learning dl is assisting applications in a growing way .
in such applications dl models are typically instantiatedfromtrainingscenarios andlaterparticipateindecisionmaking as a central classifier for new scenarios.
this practice simplifiesthesoftwaredesigninspecifyinghowsoftwareshouldbehave for complex scenarios.
with this benefit dl applications are increasingly emerging for complex scenarios that are otherwise challengingfortraditionalprograms e.g.
imageclassification object identification and self driving .
however duetothedifferencebetweennewscenariosandtraining scenarios as well as the evolution of and noises in new scenarios evenasuccessfuldlapplicationcanstillencounterinputsthatarebeyonditsdlmodel shandlingcapability.thenthe consequencesareunexpectedpredictionsinthedecision making andabnormalbehaviors from the application.
one could consider replacing the concerned dl model with a newone e.g.
withdifferentgeneralizability orupgrading the model by retraining it with more data from new scenarios e.g.
criticaldata corner casedata orless biased .
oe oufsobujpobm pogfsfodf po 4pguxbsf ohjoffsjoh icse may23 seoul korea huiyan wang jingwei xu chang xu xiaoxing ma and jian lu data .
although helpful the replacing or retraining practice is non trivial and especially not preferred after the dl application s deployment e.g.
self drivingalreadyputintouse .furthermore theapplicationcouldbedisrupted fromitsnormalexecution and itself mayneed rebuilding and redeployment not to mention that such issues keep arising in future e.g.
new weather conditions obstacles or environments encountered on highways .
inthispaper weconsidera faulttolerance approach.afterapplicationdeployment wemaketheapplicationrecognizetheinputs that are beyond its dl model s handling capability and prevent themfromimpactingitsdecision making e.g.
isolatedorreferring to manual driving since the predictions for these inputs are unexpected and probably misleading or wrong.
then the application is still functionalto other inputs and behaves as originally expected withouttheneedformodelretrainingorapplicationredeployment.
thisapproachcanalsobeflexibleforcopingwithcomplexscenarios thatcan hardly be fully anticipated in advance .
ourapproachrelatestoinputvalidation ordatacleaning efforts butweargueforaddressingtheproblemfrom an application s perspective rather than focusing on the inputs themselves only.
first comparing the inputs to original training datamaynotbefeasible eitherbecauseoftheunavailabilityordue to the overwhelming runtime overhead .
second even if one could do so different dl models can be instantiated fromthe same training data by different dl algorithms with different parameters andthusthesamevalidationorcleaningtechniquecanhardlymakefilteredinputssuitthevaryinghandlingcapabilitiesofdifferent dl models.
therefore our approach has to be applicationspecific aware of its target dl model model aware as well as beingefficient incurring only marginal overhead at runtime.
besides ourapproachhastoaddressakeyproblem i.e.
telling whentheinputstoanapplicationarebeyonditsdlmodel shandlingcapability.thisischallengingbecausethereisnooracledefin ingpreciselytheboundaryofadlmodel shandlingcapability and even the specific dl algorithm used for instantiating this model does not help much on this its scope of generalization never explicitly specified .
regarding this we make two observations.
first whentheinputsarebeyondadlmodel shandlingcapability their predictionscaneasilybemisleadingorwrong leadingtoalower accuracy.
second such misleading or wrong predictions do not come with no clue instead they can be perceived during the dl model s prediction process.
basedontheaboveanalyses weinthispaperproposeanovel technique dissector tomonitoradlapplication sinputsandisolate those possibly beyond its dl model s handling capability from impactingtheapplication sdecision making.first thistechnique is model aware monitoringwhether a dl model interprets aninputwithincreasingconfidenceinitspredictiontowardsthefinal result by collecting its uniqueness evidence.
if yes dissector considerssuchinputs withinthemodel shandlingcapability named within inputs or otherwise beyondthat named beyond inputs .
in practice adversarial inputs that are dedicatedly generated forattacking dl models are one example of beyond inputs besides natural inputs collected by a physical camera e.g.
a photo image canalsobebeyondorwithinaccordingtoitstargetdlmodel shan dling capability.
second this technique is also lightweight without anyheavydatacomparisonoranalysisoperation.thisenablesittoinput layer hidden layers output layer probability vector o input figure dl model architecture validateinputsefficientlyatruntime capableforcomplexscenarios with large scaledata.
we experimentally evaluated dissector and compared it to state of the arttechniques mmutant andmahalanobis in distinguishingbeyond inputsfromwithin inputs.theexperiments reported promising results for dissector auc .
.
whileexisting techniqueswereless effective auc .
.
and .
.
respectively .
what is worth mentioning is that dissector applied successfully to large datasets like imagenet while existing techniques incurred crashing cases due to largememory issues.
besides dissector was extremely efficient at runtime .
.8millisecondspersample about45xand763xspeedup versus existing techniques respectively.
finally dissector also exhibitednicehelpindefensingagainstadversarialinputs auc .
.
and improving a dl model s actual accuracy in use e.g.
by for cifar and for imagenet .
we summarize our contributions in this paper below proposedalightweighttechniqueforautomatedvalidation of inputs to dl applications as classifiers .
evaluated our technique s performance using real world large scaledl models.
explored our technique s usefulness in relevant fields.
the remainder of this paper is organized as follows.
section introducesthedlbackground.section3presentsourdissector technique for automated within input validation for dl applications.section4experimentallyevaluatesdissector sperformance and compares it to existing techniques.
section explores dissector susefulnessinrelevantfields.finally section6discusses related work in recent years and section concludes this paper.
background dlmodelsareusedindlapplicationsmainlyfordecision making which concerns the model training andinput predication the two phases.
we below introduce the background on the two phases as well as the related dl model architecture to facilitate our subsequentdiscussions.
the model training phase takes training data e.g.
a set of image samples each with a category label like cat or dog and in stantiates trains a dl model to represent the knowledge in the data by specific dl algorithms like convolutional neural network cnn .thentheinputpredictionphasetakesthetrained dlmodel ordlmodelwhenwithnoambiguity astheknowledge to predict labels for new inputs.
to distinguish we name the inputsusedintrainingandprediction trainingsamples andpredicting samples respectively.
adlmodelisthekeyarchitectureindl whichwasinspired from human brains with millions of neurons transferring information by electrical impulses across layers .
dl models are dissector input validation for deep learning applications by crossing layer dissection icse may seoul korea sub model generator .
trained dl model training samplessub model pool onlineprediction snapshot profiler .
validity analyzer .
predicting sampleprediction snapshot1 prediction snapshot2 prediction snapshot3 prediction profilepvscoresvscore1 svscore2 svscore3 confidence score for the validity degree of the given predicting sample offline weight growth figure dissector overview similarly based on the notion of neurons which represent features learnedfromtrainingsamplesonneuronsorganizedbylayers as shown in fig.
a fully connected dnn example .
in the model training phase a dl model learns values for neuron parameters e.g.
bias and weights which represent features extracted from training samples.then in theinput prediction phase the parametervaluesareusedforcalculatinganddecidingthemostsuitable labels for inputted predicting samples.
formally given a predicting sample the dl model generates a probability vector in the formof l0 prob0 l1 prob1 ... lm probm suggesting probability probiforthissamplebeing classified into label li all probabilities summed up to one if normalized e.g.
by function softmax .
typically the label with the highest probability is decided as the final prediction result.
the dissector approach dl applications rely on their integrated dl models for input predication and decision making and thus their reliability depends largelyonthesemodels predictionaccuracies.inthissection we presentourdissectorapproachtodistinguishingbeyond inputs fromwithin inputssothattheconcerneddlapplicationscanwork with inputs within their handling capabilities.
in the following we start with an overview of dissector and then elaborate on its detailed methodology.
.
overview dissector consists of three main components namely sub model generator prediction snapshot profiler and validity analyzer as shown in fig.
.
thesub modelgenerator step1 takesatraineddlmodel mand its corresponding training samples and returns a sequence of submodels to represent different levels of knowledge in m. generally asub model kencodesthepartialknowledgefrom m sfirsttoits k th layers.
note that the sub model generation is conducted only once in an offline way.
later the generated sub models can be reused for validating predicting samples inputted to this dl model.
theprediction snapshot profiler andvalidity analyzer work together for online input validation for distinguishing beyond inputs from within inputs.
given a predicting sample the profiler step trackshowthesampleisinterpretedusingthesequenceofgeneratedsub models andmakessnapshotsatdifferentlayers.itcomposes a final prediction profile based on the collected snapshots.
then the analyzer step examines the prediction profile and expectsthatawithin inputshouldbeinterpretedbyadlmodel withincreasingconfidencetowardsitsfinalpredicationresultby collectingitsuniquenessevidence.otherwise itisconsideredtobe a beyond input.
a confidence score pvscore is calculated to indicate the likelihood the predicting sample is within the dl model s handling capability.
with the score the application built on this dlmodelcanthendecidewhetherornottoacceptthisinputted sample and its corresponding prediction result according to its domain specific requirements.
we elaborate on the three steps in turn below.
.
step1 sub model generator givenatraineddlmodel m thefirststepgeneratesasequenceof sub modelsrepresentingthegrowingpartialknowledgeencoded in this dl model.
these sub models are used later for validating whetheragiveninputisinterpretedbyadlmodelwithincreasing confidencetowards to its final prediction result.
as illustrated in fig.
each sub model kis composed of two parts one part is copied from the original dl model m first layer tointermediatelayer k withallstructuresandparametervalues inherited e.g.
neurons weights and bias and the other part is thelinksfromlayer ktotheoutputlayer withlabelsin m which are newly trained using the original training samples.
generating thefirstpartisstraightforward whilegeneratingthesecondpart needs some time depending on m s scale but it is done only once.
here weadoptalinearregression lr structureforthesecond part training as the meta model since it is one of the most widely used proved effective structures in dnn for the final prediction layer and has been widely suggested in many existing work .thisarchitecturestrictlyfollowsadlmodel stypical design.
specifically the lr structure in a sub model refers to a one layerfullyconnectedstructurewithcrossentropyasitsloss function.
a dl model can generate as many sub models as the number of its intermediate layers.
users can of course choose to generate all orsome dependingontheirtimebudgets.ourlaterexperiments generatedonlyfourtosevensub models whicharealreadyenough icse may23 seoul korea huiyan wang jingwei xu chang xu xiaoxing ma and jian lu for this layer k given dl model generated sub modelkpart newly trained lr structure using training samplespart from the original dl model figure sub model generation for achieving quite promising results.
besides we note that our dissector approachis applicable to general dl models although we discuss dnn models here cnn sub models can be similarly prepared by treating each convolutional layer as a normal layer.
.
step2 prediction snapshot profiler whenfeedinganinputtoagivendlmodel thesecondsteptracks howthedlmodelinterpretsthisinputbasedonthesub models generated in the last step.
dissector transforms the problem of validatingthisinputfortheoriginaldlmodelintothatofexamininghowthisinputis interpretedinthesegeneratedsub models andobtains asequenceof correspondingprobabilityvectors normalized by function softmax .
since these sub models represent thegrowingpartialknowledgeencodedintheoriginaldlmodel these probability vectors explain how the dl model interprets the inputlayer by layer.
wenameeachthusobtainedprobabilityvectora predictionsnapshotforthe concernedsub model.
byconnecting theseprediction snapshots in order dissector obtains the whole prediction profileforthisparticularinputgoingthroughallthesub models i.e.
snapshot ... snapshot n whichisusedforexaminingthisinput s validitydegree in the next step.
.
step3 validity analyzer based on the prediction profile obtained for the given input thethird step analyzes the validity for each snapshot in this profile snapshotvalidity andthenthevalidityforthewholeprofile profile validity so as to calculate the validity degree for the given input.
by validity degree we expect that a within input should be predictedinawaythatthedlmodelusedforpredictingthisinput shouldhaveanincreasingconfidencewhencrossingtheinputlayer through hidden layers finally to the output layer.
this is based on our observation that since a within input fits in the dl model s handlingcapability themodelshouldwellrecognizethisinputin its prediction process instead of being confused by two or more possible guesses during the prediction.
.
.
snapshotvaliditymeasurement.
foreachsnapshotinapredictionprofile dissectormeasuresitsvaliditybyanalyzingwhether andhowitscorrespondinginput sfinalpredictionresultis uniquely supported by the probability vector in this snapshot.
suppose that an input iis fed to a dl model m withmlabels andmpredictsiaslabellx.dissectorconsidershowasnapshot supportsthispredictionresult.letthesnapshotunderconsideration besnapshot k whichisaprobabilityvector takingtheformof l0 prob0 l1 prob1 ... lm probm .therearetwocases lxisalready associated with the highest probability in this vector or otherwise.
forthefirstcase dissector encouragingly measuresthesnapshot s unique support by how much lx s associated probability exceedsthesecondhighestprobabilityinthisvector.let lshthe labelhavingthebestshottoconfusetheprediction i.e.
withthe second highest probability .
intuitively the larger the difference between the probabilities for label lxandlshis themore uniquely the prediction result lxis supported in this snapshot.
forthesecondcase lxisnotassociatedwiththehighestprobability.dissector penalizingly measuresthesnapshot sunique support by how much the actual highest probability with label lh exceeds that of lxin this vector.
similarly the larger the difference between the probabilities for label lhandlxis theless uniquely the prediction result lxis supported in this snapshot.
following this intuition we measure the snapshot validity as follows let snapshot returnl s associated probability svscore k lx snapshot k snapshotk snapshotk snapshotk lxwiththe highestprobability snapshotk snapshotk snapshotk otherwise.
the snapshot validity score falls in range .
the closer it isto1 themoreuniquelythecurrentsnapshotsupportsthefinal prediction result lx.
.
.
profilevaliditymeasurement.
basedonthesequenceofcalculatedsnapshotvalidityscores dissectorthenmeasuresthevalidity for the whole prediction profile with respect to our expected increasingconfidenceforadlmodelininterpretingwithin inputs towards their final prediction results.
since each snapshot corresponds to one particular intermediate layer in the original dl model m we normalize these snapshot validity scores with increasing weights from the first intermediate layertothelastone i.e.
increasing k echoingouraforementioned increasing confidence .
therefore we measure the profile validity as follows pvscore lx summationtext.1n k 1svscore k lx snapshot k wei htk summationtext.1nk 1wei htk.
.
.
weightparametersetup.
theprecedingequationrequiresthe setup for a sequence of increasing weight values.
instead of giving ad hoc choices or tuning specially for our experimental subjects we try three popular growth types namely linear logarithmic a n d exponential in our dissectorframework.
table lists general formulas for calculating weight values with respecttothesethreegrowthtypes where xcorrespondstonumber kofthespecificlayer snapshot k andyrepresentsitscorresponding weight value wei htk.
however these formulas contain too many parameters for setup.
fortunately they are subject to reduction without losing essentials since the weight values participate in bothnumeratorsanddenominatorsoftheequation.asshownin table the reduced formulas contain one o rt w o and parameters only and in our later evaluation we would investigate the impactsof their values ondissector s performance.
dissector input validation for deep learning applications by crossing layer dissection icse may seoul korea table parameter reduction for modeling weights growthtype general formula reduced formulas para linear y ax k y x2 y x logarithmic y alogb kx c1 c2 y lnx y lnx y ln x exponential y aekx b1 b2 y e x y e x generally for a given input ia n dad lm od e l m dissector s calculated pvscorevaluerepresents i svaliditydegreewithrespect tom and thisvaluehas beennormalized to .
thecloserthe pvscorevalueistoone iismorelikeawithin inputto m within m shandlingcapability orotherwise morelikeabeyond input tom beyondm s handling capability .
evaluation inthissection weevaluatedissectorandcompareittoexisting techniques in distinguishing beyond inputs from within inputs for dl applications.
.
research questions we aim to answer the following four research questions rq1 effectiveness how effective is dissector in distinguishingbeyond inputsfromwithin inputs ascomparedtoexistingtechniques?
rq2 efficiency howefficientisdissectorinthisprocess as compared to existing techniques?
rq3 controlling factors how do dissector s model aware treatment weight growth type and internal parameters affect its effectiveness?
rq4 usefulness how does dissector help defense against adversarial inputs and improve a dl model s actual accuracy in use?
.
experimentalsubjects design and process we introduce experimental subjects design and process below.
experimentalsubjects.
weusedasexperimentalsubjectsfour popularimageclassificationdatasetsinthedlfield namely mnist cifar cifar and imagenet each associated with a stateof the art dl model as shown in table .
mnist i sa n image database for hand writing digit classification ten labels .
itcontains60 trainingsamplesand10 predictingsamples for testing.
its dl model is lenet4 .
cifar i sa n imagedatabaseforobjectrecognition alsotenlabels .itcontains 000trainingsamplesand10 000predictingsamples.itsdlmodel iswrn wrnforshort .
cifar issimilar to cifar but with labels.
it contains training samples and predicting samples.
its dl model is resnext 8x64d resnext for short .
imagenet is a much larger databaseforimagerecognition with1 000labels .itcontains1.
milliontrainingsamplesand50 000predictingsamples.weused its ilsvrc2012version and its dl model is resnet101 .
experimentaldesign.
wedesignedthefollowingtwoindependent variablesto control the experiments table descriptions for datasets and associated dl models dataset description labels samples dl model mnist digitclassification lenet4 cifar object recognition wrn cifar object recognition resnext imagenet imagerecognition resnet101 subject.we used a total of four subjects each concerning adatasetandadlmodel namely mnist lenet4 cifar10 wrn cifar resnext andimagenet resnet101.
whenwithnoambiguity werefertoeachonebythedataset name only.
technique.
wecompareddissectorwithtwostate of theart techniques closely related to input validation namely mmutant basedonmodelmutationanalysis andmahalanobis based on data distance measurement.
dissector was configured into three variants three weight growth types namely dissector linear dissector log and dissector exp.mmutant wasconfigured intofour variants four mutation operators namely mmutant gf mmutantnai mmutant ws and mmutant ns no mixture recommended .mahalanobis wasconfiguredusingitsoriginal setting .allthesetechniqueswereeitheroriginallydesignedorslightlyadaptedtoreportanormalizedscorein for a given input with a value close to suggesting more within and to more beyond .
the three techniques need some setups.
dissector needs to selectdlmodellayersforsub modelgeneration.forlenet4 we selectedalllayerssinceithasonlyfour.forcomplexmodels we selectedpartoftheirlayersforefficiency.forwrnandresnext we selected layers after each block and for resnet101 we selected layers uniformly within each convolution group i.e.
layers and101 .
mmutantneedstoconfigureamutation degree.
this parameter was set to .
for mnist and .
for cifar as suggested .
it was also set to .
for cifar as it is similar to cifar .
however mmutant failed to apply toimagenetduetoitshugemodelsizes incurring outofmemoryexception oom .
even if one could do so we estimated that it needs mutants for imagenet which would cost over 000gb ram and significant time overhead over weeks with our gpuresources .sowehadtogiveitup.
formahalanobis we followeditsoriginalsetting toselect10 samplesundertestto trainitsweightparameters andthusitseffectivenessmeasurement was based on remaining samples.
similarly mahalanobis also failed to apply to imagenet causing oomexceptions when its gda classifier analyses stored and processed intermediate resultsconcerningtraining samples.
we designed the following two dependent variables to evaluate the three techniques auc.weusedthepopularareaundercurve auc basedon truepositiverate tpr andfalsepositiverate fpr data to measure how effective a technique is in distinguishingbeyond inputs from within ones.
to do so by varying athreshold from to and comparing it to the technique s reportedscoresforitsreceivedinputs theseinputscanbe separatedinto acorresponding setof within onesand that icse may23 seoul korea huiyan wang jingwei xu chang xu xiaoxing ma and jian lu table pvscore comparison between incorrectly predicted samples and correctly predicted samples subject dataset dl model accuracyincorrectly predicted samples correctly predicted samples t test indep.
samples avg.
std.
v. med.
samples avg.
std.
v. med.
tvalue pvalue mnist lenet4 .
.
.
.
.
.
.
.
.85e cifar wrn .
.
.
.
.
.
.
.
.44e cifar resnext .
.
.
.
.
.
.
.
.
imagenet resnet101 .
.
.
.
.
.
.
.
.
1denotingthatthecorresponding pvalueis quite close to .
with the difference even smaller than sys.float info.epsilon in python.
of beyond ones both varying .
then by comparing the two sets of inputs to the ground truths of real within inputs and beyond inputs discussed later one can calculate respective tpr and fpr data and corresponding auc values areas belowthecurves thelarger themoreeffectivethetechnique is in distinguishing beyond inputs from within ones.
timeoverhead.
werecordedthetimespentbyatechnique onitsofflinework offlineoverhead andonlinework online overhead .
the offline work is the sub model preparation fordissector mutantgenerationformmutant andlayerwise gda classifier analysis for mahalanobis.
the online workisthevalidityanalysisfordissector lcranalysisfor mmutant and distancescoringfor mahalanobis.
experimental process.
we conducted all experiments on a linuxserverwiththreeintelxeone5 2660v3cpus .60ghz tesla k80 gpus and 500gb ram running ubuntu .
.
for rq1 effectiveness we calculate auc values to compare dissector withmmutant and mahalanobis for their effectiveness in distinguishing beyond inputs from within inputs.
for rq2 efficiency we measure offline and online time overheadstocomparedissectorwithmmutantandmahalanobisfor their feasibilities in the input validation.
the offline time overhead once shouldbeacceptable andtheonlinetimeoverhead repeated shouldbe marginal.
forrq3 controllingfactors westudyhowdissector seffectiveness can be affected by its internal factors.
forrq4 usefulness westudyhowdissectorcanhelpdefense againstadversarialinputsandimproveadlmodel sactualaccuracy in use.
for the former we composed a set of inputs that contain bothtrainingsamplesandadversarialonesgeneratedfromthemby a popular attacker fgsm .
we study how effective dissector isindistinguishingthesetwotypesofsamples.forthelatter we controlled the threshold to study how dissector improves a dl model sactualaccuracyinuse.wealsocalculatedthenumberof samplesthusisolated which should be acceptable.
.
experimentalresultsand analyses we report and analyze experimental results and answer the preceding four research questions in turn.
.
.
rq1 effectiveness .
asmentionedearlier calculatingauc valuesforeffectivenessmeasurementneedsthegroundtruthsof realbeyond inputsandwithin inputs.inthefollowing wefirstdiscussacandidateforsimulatingthegroundtruthsandthencalculate auc values based on it.ground truths.
we note that the ground truths of real beyondinputs and within inputs do not naturally exist because otherwise distinguishing beyond inputs from within inputs becomes a trivial thing and already solved.
regarding this we consider finding acandidate for simulating the ground truths.
according to earlier discussions beyond inputsprobablycauseadlmodeltopredict in a misleading or wrong way.
for example for the four subjects a randomguesswouldcauseanaccuracyof10 and0.
farbelowtheirdlmodels accuracies asintable3 .therefore weconsiderapropercandidateforbeyond inputslikethose incorrectly predicted samples while that for within inputs like thosecorrectly predictedsamples.furthermore ifthissimulation isreasonable dissectorshouldbeabletodistinguishthemclearly byitspvscoremeasurement.therefore westudythetwosetsof samplesin tabletable to see whether they behave as expected.
fromthetable weobservethat theincorrectly predictedsamples from all predicting samples obtained only a .
.
pvscore value avg.
.
while those correctly predicted samples obtained .
.
avg.
.
with clear differences for each subject its difference is still evident e.g.
.
vs. .
.
vs. .
.
vs. .
and .49vs.
.
with the latterclose to its model accuracy.
we usedt test to measure how significant such differences are in a statistic way.
with the null hypothesis that dissector generatedpvscorevalueswithnosignificantdifferencebetween incorrectly predictedandcorrectly predictedsamples weobtaineda series of p valueslisted in the last column of table all of which are far less than .
.
thus one can reject this hypothesis at a confidencelevel.withthisresult wewouldlaterusethetwosets ofincorrectly predictedsamplesandcorrectly predictedsamples foreachsubjecttosimulatethegroundtruthsofrealbeyond inputs andwithin inputstofacilitatetheeffectivenesscomparison.the pointisthatalthoughthissimulationisaroughestimation thelogic still holds from being a beyond input to being predicted probably incorrectly which suffices for our experimental comparisons.
auccomparison.
as mentioned earlier we measure auc valuestocomparethethreetechniques effectivenessindistinguishing beyond inputs from within inputs.
for dissector s three variants we first studied their simplest forms i.e.
dissector linear y x dissector log y lnx anddissector exp y ex .table4lists auccomparisonresults the larger the better with range .
from the table we observe that for each of the four subjects dissectoralwaysobtainedthebestaucvalues e.g.
.
.
formnist .
.8963forcifar .
.8726forcifar100 and0.
.8562forimagenet mmutantobtainedonly .
.
.
.
and0.
.7129forthe firstthree dissector input validation for deep learning applications by crossing layer dissection icse may seoul korea table4 auccomparisonamongthreetechniquesindistinguishingbeyond inputs from within inputs technique mnist lenet4cifar wrncifar resnextimagenet resnet101 dissector linear .
.
.
.
dissector log .
.
.
.
dissector exp .
.
.
.
mmutant gf .
.
.
oom mmutant nai .
.
.
oom mmutant ws .
.
.
oom mmutant ns .
.
.
oom mahalanobis .
.
.
oom thehighestaucvalueforeachsubject is made bold.
.
.
.
.
.
.
.
.
.
mnist lenet4 cifar wrn cifar resnextoverhead s dissector mmutant mahalanobia a offline time overhead comparison in second .
.
.
.
.
.
.
.
.
mnist lenet4 cifar wrn cifar resnextoverhead ms dissector mmutant mahalanobia b online time overhead comparison in millisecond figure timeoverhead comparison subjects withcleargapstodissector andcrashedwith oomexceptionsforthelargestsubjectimagenet mahalanobisobtained evenlowervalues .
.
.
forthefirstthreesubjects respectively and also failed for the largest subject imagenet the threedissectorvariantsbehavedsimilarly satisfactorily and dissector expworkedslightlybetterforimagenet forlarge subjectslikecifar 100andimagenet dissectorbehavedmore stably still above .
and .
but mmutant and mahalanobis eitherledtolargelyreducedaucvalues below0.72and0.
or crashed with oomexceptions.
therefore we answer rq1 as follows dissector was effective in distinguishing beyond inputs from within inputs auc .
.
and behaved more stably and suitably than existing techniquesfor subjectsof varying accuracies and sizes.
.
.
rq2 efficiency .
we then compare the three techniques time overheads for both offline and online work.
we reported the comparisondataforthefirstthreesubjectsinfig.
imagenetdata were either incomplete or too large to fit in the figure discussed later .fordissector wechosedissector linear y x andfor mmutant we chosemmutant nai other variantsweresimilar to the chosenonesin time overheads .
from fig.
a and fig.
b we observe that regarding the offlineoverhead mmutantalwaystookthemosttime .2seconds formnist .4secondsforcifar and3 .1secondsfor cifar whicharesignificantlymorethanwhatdissectorandtable auc comparison for studying dissector s modelaware treatment analyzingscenario applicationscenario formnist lenet4 dnn2 lenet5 lenet4 .
.
.
.
.
dnn210.
.
.
.
.
lenet5 .
.
.
.
.
forcifar wrn vgg16 densenet wrn .
.
.
.
.
vgg16 .
.
.
.
.
densenet .
.
.
.
.
forcifar resnext vgg16 densenet resnext .
.
.
.
.
vgg16 .
.
.
.
.
densenet .
.
.
.
.
forimagenet resnet101 resnet50 vgg16 resnet101 .
.
.
.
.
resnet50 .
.
.
.
.
vgg16 .
.
.
.
.
1denoting a simple two hidden layer fully connected multilayer neural network.
mahalanobiscost .
.
least and318.
least secondsfor theformer and47.
least .
and878.1secondsforthelatter regarding the online overhead mmutant again took the most time .7millisecondspersampleformnist .4milliseconds for cifar and .
milliseconds for cifar which arealso significantly more than what dissector and mahalanobis cost .
.
least and4.
least millisecondspersampleforthe former and2.
least .
and194.8millisecondsforthelatter altogether dissector took only several minutes to complete its offline preparation and several milliseconds to validate an inputat runtime which are very attractive for the more important runtime input validation mmutant had to take .
.3x time to that of dissector and mahalanobis took .
.3x time.
for the largest subject imagenet with its resnet101 model dissectorspentaround90hoursonitsofflinepreparation andneeded .8millisecondsforitsonlinevalidationpersample.90hoursare comparabletothesubject owndlmodel trainingtime .since requiredonlyonce thetimeisacceptable.dissector sonlineoverhead is still marginal several milliseconds suggesting that it is extremely stable even for complex subjects like imagenet.
on the contrary both mmutant and mahalanobis failed to apply to this subject as explained earlier.
therefore we answer rq2 as follows dissector was highly efficient with acceptable offline overhead and marginal online overhead severalmilliseconds andmuchfasterthanexistingtechniques upto .3x and to .3x speedup for offline and online work respectively .
.
.
rq3 controllingfactors .
wenextstudyhowdissector s model aware treatment validating inputs with respect to dl modelsusedinapplications contributestoitseffectiveness andhowits inherent growth type and other parameters affect the effectiveness.
model awaretreatment.
dissectorvalidatesinputsandidentifiesbeyond inputsbasedonitspreparedsub models whichare derivedfromdlmodelsusedinapplications.thus dissectoris naturally model aware and we have observed dissector s unique icse may23 seoul korea huiyan wang jingwei xu chang xu xiaoxing ma and jian lu effectiveness from this treatment earlier.
still we want to know howdissector seffectivenesswouldbecompromisedifthemodel its analysis depends on deviates from the model it is applied to.
to be specific the former model is the one from which dissector prepares sub models and the latter model is the one that defines simulates the sets of real beyond and within inputs i.e.
groundtruths .if dissector smodel awaretreatmentisnotnecessary two models being different will not cause the effectiveness largelycompromised otherwise it will.
for this part of experiments we took the best dissector exp for example.
besides for each subject we additionally associated it with two more popular dl models and thus each dataset was now withthree dl models as in table original dl model is listed at thefirstrowandfirstcolumn .notethatthesenewdlmodelsmay have different accuracies e.g.
for cifar the three models had anaccuracyof82.
.
and73.
respectively exhibiting the required diversity for experiments.
table5comparesaucvaluesamongninecombinationsforeach ofthefoursubjects inwhichthe analyzingscenario referstothe modeldissector sanalysisdependsonandthe applicationscenario refers to the model it is applied to.
from the table we observe that when the analyzing and application scenarios were the same dissector indeed always obtained the best effectiveness results diagonal values in each rectangle area marked in bold when thetwoscenariosweredifferent aucvalueswerecompromised by a varying degree of .
.
which is not small in spite of scenarios being different dissector could still obtain mostly higheraucvaluesthanexistingtechniquesforscenariosbeingthe same e.g.
betterthanthebestmmutant naiin67.
cases and better than mahalanobis in cases.
therefore weconsiderdissector smodel awaretreatmentnecessary and important for its best effectiveness.
besides even if it is compromised dissectorcould still bringsatisfactory results.
weight growth type and other parameters.
as mentioned earlier dissectorcanbeconfiguredintothreevariants withthree weight growth types namely logarithmic linear and exponential .
besides itcouldbefurthercustomizedbytwoparameters and .
previously we explored only the simplest forms for the three variants and now we study the impact of different and values from to .
table6comparestheimpactof dissector sweightgrowthtype and and parameters on its three variant families according tothesetupintable1.fromthetable weobservethat three weightgrowthtypesbehavesimilarlywithstableaucvaluesfor eachsubject .
.
.
.
and0.
.0459differences respectively althoughtheexponentialgrowthcould be a bit unstable for some cases it obtained the best results mostly with auc values highest .
.
and .
for cifar10 wrn cifar resnext imagenet resnet101 respectively and quite close only .
gap to the highest auc value .
for mnist lenet4 and we owe the results to this type s nature e.g.
moreaggressiveinthegrowthandvalidation aucvalues areallover0.
withmost around76 over0.85andupto0.
suggesting dissector s general effectiveness among a wide range of parameter values.
the exponential growth seems to more favor complex subjects i.e.
the latter three subjects .
it might be due to the concerned dltable auc comparison for the impact of weight growth type and other parameters growth type mnist lenet4cifar wrncifar resnextimagenet resnet101 lineary x0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
logarith micy lnx0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
exponen tialy ex0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
models structures.for the firstsubject mnist lenet4 itsmodel is of relatively simple structure only four layers and the linearand logarithmic growths can well model its weight with normal growthtypes.forthelatterthreecomplexsubjects theirmodels areofquitecomplexstructures e.g.
resnet101witharoundone hundredlayers thusincurringquitecomplexbehaviorsinlayerwise sample predictions and the exponential growth can better model their weights with own aggressive growth.
therefore we answer rq3 as follows dissector s model aware treatmentisnecessaryandimportantforitsbesteffectiveness besides its weight growth type and and parameters slightly affect its stableness butits effectiveness generallyholds.
.
.
rq4 usefulness .
we finally study how dissector helps byitsinputvalidationin defensing againstadversarialinputsand improving a dl model s actual accuracy in use.
defensing against adversarial inputs.
we are interested in whether dissector can also identify adversarial attacking samples by its beyond input recognition.
as mentioned earlier we usedapopularadversarialattackerfgsm togenerateadversarial attacking samples l2norm adopted and attack step set to .
based on original predicting samples from the four subjects.cleansamples wereselectedfromthepredictingsampleswhen their predictions were correct and adversarial attacking samples wereselectedfromthegeneratedoneswhentheirpredictionswere dissector input validation for deep learning applications by crossing layer dissection icse may seoul korea table7 auccomparisonamongthreetechniquesinidentifyingadversarialattackingsamples technique mnist lenet4cifar wrncifar resnextimagenet resnet101 dissector linear .
.
.
.
dissector log .
.
.
.
dissector exp .
.
.
.
mmutant gf .
.
.
oom mmutant nai .
.
.
oom mmutant ws .
.
.
oom mmutant ns .
.
.
oom mahalanobis .
.
.
oom thehighestaucvalueforeachsubject is made bold.
incorrect.thenwestudywhetherdissectorcandistinguishadversarialattackingsamplesfromcleansamplesasitdidforbeyondand within inputsin earlier experiments.
table lists dissector s auc values on this aspect and also compares it to earlier techniques mmutant and mahalanobis.
from thetable weobservethat whenmixingadversarialattacking samplesandcleansamplestogether dissectorobtainedhighest andstableaucvalues alwaysover0.9962andupto0.
inidentifying the former from them outperforming the other techniques .
.
for mmutant and .
.
for mahalanobis fortheirapplicablesubjects mmutantwas unstablewithauc values ranging in .
.
performing no better than any dissector variant dissector exp won in all cases and failedfor imagenet due to oom exceptions mahalanobis obtained betteraucvaluesthanmmutantforcifar 10andcifar but behaved worse for mnist and similarly failed for imagenet.
as a whole we consider dissector very useful auc value nearlyone inidentifyingadversarialattackingexamplesbybeyondinputs asoneofpopulardl basedsecurityapplications.weowe this to dissector s dedicated design of examining increasing confidenceforgiveninputs thusabletoidentifyadversarialonesby profilingandanalyzingtheirwholecrossing layerpredictionprocess sinceatraditionalattackcanseldomconsiderthewholedl model in a crossing layer way.
improving a dl model s actual accuracy in use.
each dl modelisassociatedwithanaccuracywhengivenasetofpredicting samplesfortesting e.g.
thoselistedintable3 .withdissector the given predicting samples are refined by isolating those beyondinputs and then the remaining ones within inputs can bring a better accuracy.to distinguish wename the former original accuracyand the latter actual accuracy in use.
we are interested in how muchdissectorcanhelpimprovetheaccuracy.notethatthisis just one possible application of dissector more discussed later .
weuse athreshold todecide whethera predictingsample with a distinct pvscore value reported by dissector is a within when above orbeyond whenbelow input.fig.5illustrateshowadl model sactualaccuracywasimprovedunderdifferentthresholds.
fromthefigure weobservethat allthreedissectorvariants linear logarithmic and exponential growths with simplest forms exhibited clear accuracy improvements with the growth of dissector sthresholdvalue forsubjectsmnistandcifar whose original accuracies were already very high .
and .
dissector scontributionstotheiraccuracyimprovementwererelatively slow but steady finally reaching for subjects cifar and imagenet whose original accuracies were somewhatlow .
and77.
dissector scontributionstotheir accuracy improvement were quite impressive finally by and even with a conservative threshold value of .
thethreedissectorvariantsrealizedasatisfactoryactualaccuracy of and respectively.
someapplicationsmightconcernthenumberofisolatedsamples asthecostforthe accuracyimprovement.sowealsoinvestigated thisissue.takedissector linear y x forexample.whenone set the threshold to .
and .
the four subjects actual accuracies could already be largely improved e.g.
cifar s accuracy improvedfrom82 to89 and96 respectively.accordingly its number of isolated samples was and .
if it was dissector exp the number of isolated samples was much less e.g.
and21 respectively.nevertheless thesubjectitselfalsoplayed animportantroleonthisnumber e.g.
formnist withthehighest original accuracy isolated samples were only for both thresholdsand all dissector variants.as a comparison when using other techniques for input validation we encountered moreisolated samples.
for example when expecting the same actualaccuracyforthecifar resnextsubject mmutant nai andmahalanobiscaused21 and47 samplesbeingisolatedbut dissector expisolatedonly9 samples.thissuggeststhatdissector s beyond input identification is more precise and thus more cost effective.
thisapplicationdoesnothavetoretraintheconcerneddlmodel andinsteadbringsatransparentaccuracyimprovement thusbeing very attractive.
the fault tolerance idea behind dissector actually maximizes the potential of the original dl model about what it can actually do.
besides the simple treatment of isolating theidentified beyond inputs one can also refer to other dl models applications orevenmanualcontrols e.g.
forself driving .this direction deserves more research efforts.
therefore weanswerrq4asfollows dissectorareusefulfor defensing against adversarial inputs and improving a dl model s actualaccuracyin use.
.
threatanalyses ourselectionofthefoursubjects namely mnist cifar cifar100 andimagenetwiththeirassociateddlmodels mightthreaten theexternalvalidity ofourexperimentalconclusions.wetriedto alleviatethisthreatbythefollowingefforts thefourdatasetsare verypopularandhavebeenwidelyusedinrelevantresearch theirassociatedmodelsarealsostate of the artdlones thesedatasetsandmodelsdifferfromeachotherbyvaryingtopics e.g.
digit image and general object recognitions labels from to scales from to samples model types e.g.
lenet4 wrn resnext andresnet101 modellayers from5 to101layers andmodelaccuracies from77.
to98.
which makethesesubjectsdiverseandrepresentative.therefore ourexperimental conclusions should generally hold although specific datacould be inevitably different for other subjects.
threats to the external validity might also come from our selected techniques for the experimental comparisons which include icse may23 seoul korea huiyan wang jingwei xu chang xu xiaoxing ma and jian lu .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1practical acuracy thresholdmnist lenet4 cifar10 wrn cifar100 resnext imagenet resnet101 a dissector linear75 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1practical acuracy thresholdmnist lenet4 cifar10 wrn cifar100 resnext imagenet resnet101 b dissector log75 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1practical acuracy thresholdmnist lenet4 cifar10 wrn cifar100 resnext imagenet resnet101 c dissector exp figure accuracy improvement under different thresholds fordissector mmutant andmahalanobis .wenotethataswearguedearlier identifying beyond inputs for a dl application should be from the perspective of the application and ought to be lightweight runtime validation.
thus existing work right focused on this problem andmeetingtherestrictioncanbelittle excludingsomeoptions e.g.
deeproad and surprise .
we selected mmutant becauseitresemblesourfocusbyidentifyingunqualifiedinputsby dlmodel mutationanalysis softwareengineering area .besides theworkwaspublishedinmay2019 astherepresentativeofthe state of the arttechniques.wealsoselectedmahalanobisbecauseit similarlyuseslayer wiseinformationtoidentifyout of distribution samples for dl modelsbased on distance measurements artificial intelligence area .
the work is also the latest dec closely related to our problem as the representative of distance based and distribution based data comparison techniques.
ourinternalthreat mainlycomesfromthe lackof groundtruths for distinguishing beyond inputs from within inputs.
we used the inputs predicted incorrectly and those predicted correctly to simulate beyond inputs and within inputs respectively.
as discussed earlier suchestimationmightberough butsincethelogic from beyond input to probably incorrect prediction holds rq1 our experimentalconclusions wouldlargely hold.to furtheralleviate this threat we conducted the experiments to study dissector s usefulness rq4 which frankly disclosed what dissector can indeedhelpdefenseagainstadversarialinputsandimproveadl model sactualaccuracyinuse evenifbasedonourgroundtruth simulation.
considering that our subjects have necessary diversities as discussed above our experimental conclusions can hold generally.still duetothegroundtruthproblem weplantovalidate dissectorin more and practical application scenarios.
dissectorapplications we discuss potential dissectorapplicationsbelow tolerating imperfect dl models.
dl models can hardly be perfect forcomplexapplicationscenarios.eveniftheyareacceptablefor now application scenarios can evolve from time to time and cause themodelstobehaveunexpectedlyunsatisfactorily asdiscussed earlier.
with an input validation wrapper like dissector a dl application built on such dl models can be substantially improved by automatically recognizing beyond inputs with respect to what these dl models actually do.
especially when the application is deployedinanunstableenvironment suchanautomatedtechnique does help in a convenient way.refining dl models.
for the case dl models themselves have to be refined dissector s identified beyond inputs form a critical set for consideration.
this set of inputs are beyonda dl model s handling capability and would probably cause misleading or incorrect predictions.thenuserscanconsiderwhetherandhowtousethem e.g.
for expanding the model s scope by retraining it with these beyond inputs orstrengtheningitsoriginalscopebykeepingthem isolated.moreissuessuchasmodelstabilityandcornercasescan alsobe taken into consideration during this refinement process.
comparing dl models accuracies.
traditionally dl models can be directly compared by their prediction accuracies with respect to givensamples from anapplication scenario.
with adissectoralike input validation wrapper the comparison can now have new considerations.
suppose that models a and b have original accuracies of and for this scenario.
with dissector their actual accuraciesinuse mightbe improvedto and instead.this calls for new research opportunities on how to understand a dl model s actual accuracy in practice.
measuring deployment suitability.
a dl application might be deployed in a complex application scenario which cannot be fully anticipatedortested.basedonhowmanyinputsareidentifiedas beyond onesaswellastheresultingaccuracy dissectorcanbe used for suggesting whether and how the concerned dl model suits its deployment.
more applications include assigning the most suitable dl model from a set of candidates to the scenario as well as balancing the assignment of multiple dl models to multiple applicationscenarios.
defensingagainstadversarialinputs.
adversarialinputscanbe substantial attacks toa dl application and thus identifyingthem is beneficial.
our evaluation shows that although most adversarial inputsbehavedlikebeyond inputs fewofthemmightstillpassthe validationand behavelikewithin ones.currently thereis strong evidence showing that taking all adversarial inputs into retraining could probably bias a dl model.
then this set of adversarial yet still within inputs becomes an interesting source for critical model improvement.
more research can be initiated on this aspect.
related work theworkstudiedinthis paperrelatestoqualityimprovementfor dlmodelsandapplications.webrieflydiscussrepresentativework in recent years on data quality assurance dl related verification testingand debugging and adversarialattack.
data quality assurance.
one line of work focuses on assuring quality data for dl models so as to improve their reliability in use.
dissector input validation for deep learning applications by crossing layer dissection icse may seoul korea somework focusedondatacleaningtechniquesinorder topreparequalifiedtrainingdataforinstantiatingmodels.other work regarded training samples as in distribution andrefinedgivennewsamplesbyidentifyingout of distribution ones as outliers a statistic way.
this line of work typically focuses ondatacharacteristics andseldomconsidersrequirementsfrom applicationsbuilton these data.
dl relatedverification.
thislineofworkattemptstoformallyverifydlmodelsfortheirquality.somework proposedtouse symbolictechniquesforabstractingtheinputspaceforadlmodel but could hardlyscale to large and complex ones.
some couldworkforrelativelylargermodels butsupportedonlyspecific neural networks.
the others targeted at securitycriticalorsafety criticaldl assistedapplications andverifiedthem forsafetyproperties.ourworkcomplementsthisline byvalidating inputsat runtime for better model performance.
testing and debugging.
this third line of work aims to generate diverse and realistic corner cases to expose possible flaws in dl models.forexample deepxplore convertedthecorner case generation problem to a joint optimization one and searched towardsamodel sdecisionboundaries.moreworkexamineddlmodelsbytesting e.g.
byimagetransformation inputfuzzing mutationtesting andmetamorphictesting .tomeasure thetestingadequacy variouscoveragecriteriawereproposed includingtheneuroncoverage sscoverage neuron and layer level coverage combination dense coverage and surprisecoverage .interestingly itwasalsoarguedthatsuch structuralcoveragecriteriafordlmodelscouldbemisleading .
finally dl models could be debugged for their flaws by analyzing biased data distribution .
our work also complements this line by runtime validation for tolerating unexpected problems after the concerned dl models have been tested and deployed.
adversarialattack.
finally proposingbetterdlmodelsagainst adversarial attacks is getting hotter.
on one hand adversarial attacking techniques were proposed to generate adversarial samples with small artificial perturbations to fool dl models e.g.
lbfgs fgsm jsma c w uni.perturbation and deepfool .
on the other hand defense techniques were also proposed to identify such fooling samples e.g.
adversarial sample detection adversarial training foveation based defense gradientregularization masking defensive distillation andgan baseddefense .theytogetherimprovedlmodelsinarecursiveway.ourworkcanalsobeusedfor identifyingadversarialattackingsamples as one of its application.
conclusion in this paper we proposed dissector for effective and efficient validation of inputs to dl applications.
dissector distinguishes beyond inputs from within inputs by collecting prediction uniquenessevidence andworksinamodel awareandlightweightway.
the experimental evaluation confirmeddissector s unique effectivenessandclear gains overexistingtechniques.dissectoralso exhibitedencouraging usefulnessindefensing againstadversarialinputsand improving a dl model actual accuracy in use.
currently dissectorhastobeconfiguredforitsgrowthtype andparameters althoughtheydonotaffectmuch .weareconsideringguidingtheconfigurationbypredictionfeedbacksofitsgeneratedsub modelstomakedissectorfullyautomated.besides dissector needs to be tested in more application scenarios for both more practical validation and more usefulness exploration.