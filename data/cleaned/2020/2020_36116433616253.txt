grace language models meetcodeedits priyanshu gupta priyansgupta microsoft.com microsoft indiaavishree khare akhare seas.upenn.edu universityof pennsylvania usayasharthbajpai ybajpai microsoft.com microsoft india saikatchakraborty saikatc microsoft.com microsoft research usasumit gulwani sumitg microsoft.com microsoft usaadityakanade kanadeaditya microsoft.com microsoft research india arjunradhakrishna arradha microsoft.com microsoft usagustavosoares gsoares microsoft.com microsoft usaashish tiwari astiwar microsoft.com microsoft usa abstract developers spend a signi f icant amount of time in editing code for a variety of reasons such as bug f ixing or adding new features.
designingeffective methodsto predictcodeeditshas been anactive yetchallengingareaofresearchduetothediversityofcodeedits andthedifficultyofcapturingthedeveloperintent.inthiswork we addressthese challengesbyendowing pre trained large language models llms with the knowledge of relevant prior associated edits whichwecallthe grace generationconditionedonassociated codeedits method.thegenerativecapabilityofthellmshelps address the diversity in code changes and conditioning code generationonprioreditshelpscapturethelatentdeveloperintent.we evaluatetwowell knownllms codexandcodet5 inzero shot and f ine tuning settings respectively.
in our experiments with two datasets gracebooststheperformance of thellmssigni f icantly enabling them to generate and more correctly edited code intop 1suggestionsrelativetothecurrentstate of the artsymbolic andneuralapproaches respectively.
ccs concepts softwareanditsengineering softwareevolution automatic programming computing methodologies arti f icial intelligence .
keywords codeediting associatededits largelanguagemodels pre trained model programminglanguageprocessing bothauthorscontributed equally to thiswork.
workdonewhile at microsoft permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forpro f itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe f irstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspeci f icpermission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
format priyanshu gupta avishree khare yasharth bajpai saikat chakraborty sumit gulwani aditya kanade arjun radhakrishna gustavo soares and ashish tiwari.
.
grace language models meet code edits.
in proceedings of the 31st acm joint european software engineering conference andsymposiumonthefoundationsofsoftwareengineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.
introduction maintainingandmodifyingexistingcodetakesupaconsiderable portionofadeveloper stimecomparedtowritingnewcode .
duetothehighcostofsoftwaremaintenance popularintegrated development environments ides have tooling to support developersastheyrefactorcode f ixdefects adaptcode to changes in the environment or add support for new or changed requirements .
one desirable feature is code edit suggestions whereinthetoolsusethelocationwherethedeveloperisediting code andthesurroundingcodecontext togeneratecandidateedits to recommend .
toautomatecodeeditsuggestions researchershaveproposed several approaches to learn edit patterns from edits in source code repositories .however theseapproachessufferfrom two key limitations they focus on individual edits and learn program transformation rules for them.
we note that edits are not performedinisolation.developersmakechangesatonelocation then jump to another and then maybe back to the f irst location to makefurtherchanges .theeditsthatdevelopersmaketothe code at different locations may not be identical but they are often interrelated.infact thenexteditoftendependsonthepreviously performed edits .
learning one step edit patterns limits the ability of these approaches to accurately predict the most likely next edit.
the symbolic program transformation rules can only slice and dice the existing code and compose its pieces to create code they cannot generate new code whose pieces do not already occurintheexistingversion.thislimitstheexpressivenessofthese approachesinterms ofthe types of editsthat they can predict.
unlike symbolic program transformation rules neural language models have the capability to generate new code that does not esec fse december3 san francisco ca usa p. gupta a.khare y. bajpai s.chakraborty s.gulwani a.kanade a.radhakrishna g.soares anda.tiwari necessarily occur in the surrounding code context.
the pre trained large language models llms like codex andcodet5 have been shown to be highly pro f icient at generating code.
in fact theyarealreadyimpactingsoftwareengineeringinsigni f icant ways e.g.
through popular code completion tools like github copilot .
however when it comes to editing code without the knowledgeofpreviousedits thesemodelsareunabletoinferthe developers intentand failto generate codethat should be used to replaceexistingcodeinthenextedit.inthiswork weexploreways topredictcodeeditsusing llms byconditioningcodegeneration on prior relevant edits.
we call such prior edits associated edits and this methodology as generation conditioned on associated code edits grace .
in recent times there have been a few attempts to leverage past history of code evolution to learn to edit code .
overwatch is a symbolic technique that mines edit sequence patterns .suchapatternisessentiallyaprogramtransformation rulewhoseapplicationisconditionedonthepriorapplicationof someotherprogramtransformationrules.beingapattern based technique overwatch suffersfromtheinabilitytogeneratenew code and it also requires signi f icant engineering effort to build the underlying symbolic pattern learning engine.
c3po is a neuralmodeltopredictthenexteditatalocation giventheedits only in the spatial vicinity of that location.
while reliance on such spatially related edits shows initial promise towards automation in code editing the hypothesis may not always hold true developers may edit two locations simultaneously that are far away from each other spatially .
in this work we attempt to relax this reliance and do not restrict associated edits to be the ones that occur in thespatialvicinityofthelocationunderconsideration.weshow that associated edits obtained from temporal history can also be useful.further the c3pomodelisacustommodelthatgenerates the edited code by copy pasting existing code fragments and is therefore unable to generate new code similar to the symbolic techniques including overwatch .
editpro is a recent neural model that aims to learn the edit process for natural language documents and code f iles.
it proposes aspecialmulti stepprocedurewherethemodel f irstpredictstokenwise editactions insert delete etc.
whichare thensubsequently applied to the code.
the edit actions requiring code generation suchasinsertandreplace requireaseparatedecodingstep.editproexperimentswithsingle lineedits whereasourdatasetscontain multi line edits.
instead of training a new type of model from scratch which can be expensive and requires a signi f icant amount ofdata graceallows ustorepurposethealready powerfulllms to generateeditedcode.
we demonstrate the bene f its of our approach in two settings zero shotsetting inwhichthellmisusedout of the boxwithout additionaltrainingbutwithaninformativepromptaboutassociated edits and f ine tuning setting in which the llm is f ine tuned on dataannotatedwithassociatededits.inbothcases ourresultsshow signi f icant bene f its of conditioning existing llms on associated edits without having to pay the price of designing and training specializedmodels from scratch.
inourexperiments weusethecode editingbenchmarksfrom overwatch andc3po.
as the baseline llms we use the codexdavincimodel in the zero shot setting and the codet5model 220m params in the f ine tuning setting.
we show thatthe use of associatededitshelpsboosttheabilityofthesemodelstopredict the next edit compared to the pre trained models used without associated edits.
in the case of codex davinci we get improvementsof17 and30 inabsoluteterms fortheoverwatchand c3po datasets and improvement of .
and .
in the case ofcodet5.wealso compare codex davinci withassociated edit prompting and the f ine tuned codet5model with the overwatch andc3pomethodsontherespectivedatasets.allourmodelssubstantially outperform these methods on their own datasets by a signi f icant margin.
our best models outperform overwatch on its dataset by .
and c3poon its dataset by .
absolute this is .
and .
relative improvement respectively.
editpro datasetandmodelhavenotbeenreleasedbytheauthorsyet therefore wewereunabletocompareagainstit.both overwatch and c3poconstruct edited code from existingor past code whereas we usellmsthatarecapableofgenerating newcode.weshowthat this makes our approach more general and we can predict code editsthat are often out of scope for theseapproaches.
in summary we make the following contributions we consider a practically important software engineering problemofpredictingcodeeditsandpropose grace anovel methodofleveragingpowerfulllmstopredictcodeedits byconditioningthemonprior edits.
through experimentation on two datasets we show that usinggracewecansubstantiallyimproveperformanceof llmsinzero shotor f ine tuningsettings.
graceis superior to the state of the art symbolic or neural methodsdesignedspeci f ically to handle code edits.
we conduct experiments to thoroughly evaluate graceand report insightsgleanedfrom them.
motivating example in this section we motivate graceby using a concrete code developmentscenario.wefurtherdiscusshowthisapproachdiffers from existing approaches.
illustrative example consider a developer refactoring code showninfigure 1aasversion u1d4631.thegoalofthedeveloperistouse serializationexception provided by the system.runtime.serialization namespace to get to version u1d4633shown in figure 1c.
let us say that the developer f irst replaces exception on line in version u1d4631with serializationexception to create version u1d4632shown in figure 1b.
this edit required to go from version u1d4631to version u1d4632is denoted as u1d6ff1 .
the developer s cursor then moves to line of version u1d4632and our goal is to predict the next edit the developer will perform to reach version u1d4633 namely the edit u1d6ff2 .
conditioning on prior edits the task of predicting the edit u1d6ff2 3is non trivial.
the code in version u1d4632has some useful information for example the code indicates that serializationexception is de f ined on line of version u1d4632but the required system.runtime .serialization namespace hasn t been imported anywhere.
this signal however is faintly present within lines of additional spatialcontextandtherelationshipbetweentheaddedexception andtherequiredimportislost.thisrelationshipisanimportant pieceofinformationthatisrequiredtoinsertthe usingstatement 1484grace languagemodelsmeet code edits esec fse december3 san francisco ca usa 1usingsystem 2usingsystem.linq 3usingsystem.text ... 250catch exception ... a version u1d46311usingsystem 2usingsystem.linq 3usingsystem.text ... catch exception catch serializationexception b version u1d4632withassociatededit u1d6ff1 21usingsystem 2usingsystem.linq using system.runtime.serialization 4usingsystem.text ... catch serializationexception c version u1d4633withtargetedit u1d6ff2 figure1 thedeveloperperformsedit u1d6ff1 2togofromversion u1d4631oftheircodetoversion u1d4632.thiseditservesasanassociatededit that helpswith predictingtheedit u1d6ff2 3needed to gofromversion u1d4632to u1d4633.
table comparisonofdifferentapproacheson theexample fromfigure technique prediction correct?
c3po noresponse alieninsertion overwatch noresponse nomatching pattern code davinci edit emptyresponse codex davinci withoutassociated edits usingsystem.net.http codet5withoutassociated edits usingsystem.threading codex davinci withassociated edits ourapproach usingsystem.runtime.serialization codet5withassociated edits ourapproach usingsystem.runtime.serialization on line of version u1d4633.our f irst key observation for improving predictionofcodeupdatesisthatitshouldbeconditionedonrelatededits from the past.
in the above scenario we want to predict the update to version u1d4632by also looking at the how version u1d4632was created from version u1d4631.
the edit u1d6ff1 2is anassociated edit .
in this example thereisjustoneassociatededit butingeneraltherecanbemultiple previous editspickedas associatededits.
therehasbeensomerecentworkonpredictingcodechanges conditioned on previous changes .
we now discuss how theseapproachesworkontheillustrativeexample.table 1shows the predictions ofvarioustechniques onthe target.
c3po c3poisapath basededitpredictionmethodthatgenerates an edit script to predict subsequent edits.
it uses a pointer network to pick valid target edits at u1d4632by attending to u1d6ff1 represented asaneditpathintheast.asthesetargeteditscanonlyreferto nodesintheastsat u1d4632and u1d6ff1 thepointernetworkdoesnothave access to the serialization token needed to be inserted on line .
therefore c3powould f ilteroutabove mentionedexampleinits training and testing pipelines categorizing it as an alien insertion .
whenc3po f inetunedonthe overwatch trainsetisusedtopredict u1d6ff2 itincorrectly suggestspickingan existing usingstatement.
overwatch overwatch is a symbolic procedure that learns abstract syntax tree transformation rules from example edit sequences in the training data and then makes predictions by applying those rewrite rules.
the above mentioned example does not match any of the 50patterns that the authors released in .
thus out of the box overwatch would not be able to provide any suggestion because of unavailability of a matching pattern for the target edit in the example.
ifwe provide enough edit sequences similar to u1d6ff1 2followed by u1d6ff2 as training data to overwatch then it might learn a few edit patterns depending on the examples itgetsandtheorderinwhichtheyaregeneralized.theonlytwo usefulpatternsthatcouldbelearnedwouldbeeither thesubstitutionof exceptionbyserializedexception isfollowedbyimporting thesystem.runtime.serialization namespace or thesubstitutionofexception by aplaceholder type is followed by importing a placeholdernamespace .
whilepattern wouldreturnthecorrectresponse itisan over f itpattern thatdoesnotgeneralizetoother changesinthesubstitutedtype.pattern istoogeneralandcannot generateaconcrete suggestiondueto the unbound placeholder.
using llms the approaches discussed above cannot generate the right predictions either when the target requires a new token c3po orwhenitcannotmatchanexistinglearnedpattern overwatch .
llms of code have emerged as competitive code completion tools that offer generative capabilities.
this leads to our second key observation llms can handle diverse editing scenariosincludingthosethatinvolvegenerationofnewtokens.
wenow discuss howthesemodels work onthe illustrative example.
llms without associated edits first let us consider how a modern code completion tool based on powerful llms will attempt to predict the new code at line of version u1d4633.
code completion tools like codex davinci look at the current snapshot ofthe codetomakepredictions.
in other words thetool will look at version u1d4632to predict version u1d4633.
when we provide code from version u1d4632tocodex davinci itcorrectlypredictsthatsomething shouldbeimported butitpredictsanincorrectnamespace.ifwe usecode davinci edit the editing variant of codex davinci thatallowsyoutoprovideinstructionsforediting theprediction continues to remainincorrect.
llms with grace following our two key observations we presenttheedit u1d6ff1 2tocodex davinci alongwithline3ofversion u1d4632thatneeds tobeupdated.now themodel successfullypredicts that the updated code would be line of version u1d4633.
we discuss the prompt designindetailinsection .
.
we foundthat this utility of associated edits for edit prediction also extends to other models a base codet5model f ine tuned to predict u1d6ff2 3using u1d4632incorrectly predicts system.threadingwhile the same model f ine tuned to additionally use u1d6ff1 2to make the prediction getsthe import right.
bybuildingacodechangepredictionmodeloveracodegeneration model we are able to extend the scope of edit predictions.
moreover weareabletoalsoperformbetterthantheexistingworks 1485esec fse december3 san francisco ca usa p. gupta a.khare y. bajpai s.chakraborty s.gulwani a.kanade a.radhakrishna g.soares anda.tiwari on the subset of the benchmarks that are in their scope.
we discuss our quantitative performance on these benchmarks compared toc3poandoverwatch in sections .
and .
respectively.
we further present aqualitative analysisofthe results insection .
.
associated code updates we de f ine the associated code update task in this section.
the associated code update task is inspired from the editcompletion task andthe editlikelihoodprediction task .
let u1d4630 ... u1d463 u1d45bbe a sequence of versions of a source code f ile.
an edit u1d6ff u1d456 u1d457is the difference between two versions u1d463 u1d456and u1d463 u1d457.
we view u1d6ff u1d456 u1d457as a function that returns u1d463 u1d457on the input u1d463 u1d456 i.e.
u1d6ff u1d456 u1d457 u1d463 u1d456 u1d463 u1d457.
furthermore an associatededit u1d456is u1d6ff u1d457 u1d458 forsome u1d457 u1d458 u1d45b.
giventhe u1d45aassociatededits 1 2 ... u1d45aandtheversion u1d463 u1d45b along with locations u1d43fin u1d463 u1d45b theassociated code update task is to predict version u1d463 u1d45bassuming only the locations u1d43fin u1d463 u1d45b 1are updated.we thus want to modelthe probability u1d443 u1d463 u1d45b u1d43f u1d463 u1d45b 1 2 ... u1d45a we nextmakeafewremarks aboutthe problem formulationabove.
first the u1d45b 1versions u1d4630 u1d4631 ... u1d463 u1d45b 2need not necessarily match the history of the underlying source code f ile.
the actual historical versions can be different and in fact in the formulation above it is notthecompleteversionsthemselves buttheedits u1d456thatareused in the prediction task.
the only version that is important here is the current version u1d463 u1d45b .
furthermore the set of u1d45apast edits need not even be theexhaustive set of alltemporally consecutive edits they could be a subset of the edits that have been performed so far.
hence u1d456doesn tnecessarily have to be u1d6ff u1d456 u1d456.
second the edits are allowed to be spatially far away from each otherandfromthetargetlocations u1d43finversion u1d463 u1d45b .whileanedit u1d456that modi f ies locations close to the targetlocations u1d43fis likely to be useful to include in the set u1d45aof edits edits farther away from u1d43f may also be relevant.we makesnoassumption onspatial locality ofeditsincontrastto the editcompletion taskin .
.
assumptions aboutsub problems our problem formulation above abstracts away three important andchallengingrelatedsub problemsthatarecrucialtobuildan end to end tool.
these three sub problems are edit localization editgranularity and associatededitsidenti f ication.theassociatedcodeupdateproblemformulationassumesthatwehave somesolution for thesethree relatedproblems.
editlocalization theeditlocalizationproblemseeksto f ind the locations u1d43fwhere the developer should make edits.
how we get these locations is dependent on the application.
for example inan ide cursor locationisagoodindicator of where the developer wants to make changes.
another option is to build a model that predictsthenext edit locationgivenpriorassociated edits.
in overwatch locations werepicked based on whethercertain learned patterns matched the code at those locations.
the patterns thatwerematchedagainstwereselectedconditionedonthepast applicationsofassociatededitpatterns.
edit granularity the edit granularity problem refers to the issueofde f iningwhatconstitutesan edit .weassumethatwehave heuristicstode f inewhenalocalcodechangequali f iesasasingleedit.
all changes between two versions that successfully parse can beusedasade f initionofasingleedit asinthework .another heuristic could be to combine all changes that occur within a small spatialvicinityofeachother inacommit as asingleedit .
associated edits the associated edits problem seeks to f ind edits from the past history that would be most useful in predicting changes at the given locations u1d43fin the current version u1d463 u1d45bof the source code f ile.
edits that are spatially close to the target locations u1d43fare likely relevant .
similarly edits that are temporally close that is edits that happened in the recent past are also likely candidates for being relevant.
we can use some combination of temporal and spatial proximity to obtain a candidate set ofrelevantedits .forpredictingupdatesonatargetlocation the temporally proximal edits can indicate the developer s editing intentandthespatially proximaleditscanassistinprovidingmeaning to the target snippet.
we can even further selectively choose fromtheeditsinthespatio temporalvicinityofthetargetlocations usingtheapproachinarecentworkthatminesrelevanteditsbased on their syntactic structure and their likelihood of occurring together .
we can use any or all of these approaches to construct the set of relevant edits.
our goal is to show that even when the relevanteditsare heuristically generated usingthemforassociated code updatepredictions can be very bene f icial.
.
related problemformulations existing auto regressive llms such as gpt3andcodex predict completions for a given prompt.
if the prompt contains the current version u1d463 u1d45b 1of the artifact then these llms predict text that is meant to be appended to u1d463 u1d45b 1to generate the new version u1d463 u1d45b.
these models rely on the text in the spatial vicinity of the changelocations u1d43fto makepredictions.
in our terminology these models aremodelingtheprobability u1d443 u1d463 u1d45b u1d463 u1d45b .this isclearlydifferent from the problem we are considering.
we demonstrate that the associated code update formulation yields a simple yet effective wayofimprovingllmperformanceonsoftwaredevelopmenttasks.
the editcompletion task in is formalized as a study of u1d443 u1d6ff2 u1d43f u1d4632 u1d6ff0 u1d6ff1 where the two given edits are edits performed in the spatial vicinity of the current location one before andoneafterthecurrentlocation.theeditcompletiontaskdoes not consider relevant edits that may be spatially distant.
our problem formulation is a generalization of editcompletion problem and in fact we use the benchmarks from for evaluation.
as discussedinthe introduction our approachesare differenttoo.
the edit likelihood prediction problem explicitly considers the study of u1d443 u1d463 u1d45b u1d4630 u1d4631 ... u1d463 u1d45b but it uses u1d443 u1d6ff u1d45b u1d45b u1d4630 u1d4631 ... u1d463 u1d45b asa waytoestimate the former.
this problemdiffers from the associated code update problem in two ways f irst it includes the sub problem of f inding the locations u1d43fthat need to be edited as part of the larger problem and second it considers the entire edit history as an ordered sequence in an auto regressive way whereas we focusonasmall setof associatededits.
exploiting associated edits wepropose grace atechniquetousepre trainedlanguagemodels for solving the associated code update problem.
there are two possible ways of using these models to perform the associated 1486grace languagemodelsmeet code edits esec fse december3 san francisco ca usa codeupdatetask.one approachisbasedonusing themodels asa black box butwithcarefullydesignedprompts section .
.this promptingstrategyworksforbigllms suchas gpt3andcodex.
thesecondapproachisbasedon f ine tuningapretrainedlanguage model codet5in our case for our speci f ic associated code update task section .
.
.
pre trainedlanguage models there is now a large collection of pre trained language models.
these models are pre trained on data collected from millions of webpages and treat all data as a sequence of tokens which is the natural choice for representing natural language text .
the reason for the popularity of this class of models is that they exhibit ability to perform multiple different tasks with just some instructions and zero examples zero shot task transfer even though they are not explicitly trainedfor thesetasks.
it was observed that pre trained llms are not as effective when workingwithcodebecausecodehasstrictsyntacticandsemantic correctnessrequirements.differentrepresentationsforcodeand codeeditshavebeendevelopedandmodelshavebeentrainedto work with those representations .
however as the size of pretrainedlanguagemodelshasgrown theirzero shotperformance acrosstaskshasimproved.moreover thesemodelshavealsoshown theabilitytoperformanewtaskgivenjustafewdemonstrations intheprompt few shotlearning .usingtheirzero shotand few shot learning capabilities these models are now being used successfully on tasks that involve understanding manipulating or generating code while still viewing code just as text andnot as an abstract syntax tree for example .
.
prompting llms we experimented with a few different prompt designs and then f ixedoneforourexperiments.
theresultswerenotsigni f icantly different for other reasonable prompt designs.
before we describe thegraceprompt we f irst describe the completion insertion and editing variants ofthe codexfamily ofmodels .
thecodexfamily of models is available in the completion insertion and editing variants.
the completion model takes a prompt whichusually contains codebefore acursorlocation and predicts the code that will follow that prompt.
apart from the prompt the insertion model also takes a suffixprompt which usuallycontainsthepartofcodethatshouldcome afterthecodethe model predicts.
thus the insertion models perform the in f illing task predictthecodethatshouldcomeafterthepromptbutbefore the suffix.
finally the editing variant of the codex models has two different input prompts an inputthat is the string that needs to be edited and an instruction that tells the model how to edit the input.
we treat the associated code update task as an in f illing problem and hence use the codex insert family of models for our experiments.
the reasons for this choiceare as follows theinsertionmodelallowsustoincludecodethatisspatially after the target location inthe suffix.
the editing variant code davinci edit requires instruction onhowtoeditthegivenpieceofcode.ourexperimentswith providing the associated edits in this instruction prompt failed to generate good results.
this is possible because the editing1 currentedit prefix .
.
.
prefix before .
.
.
before after .
.
.
after suffix .
.
.
suffix currentedit ctxedits edit prefix .
.
.
prefix before .
.
.
before after .
.
.
after suffix .
.
.
suffix edit edit ... edit .
.
.
ctxedits figure graceprompt for the associated code update task.
model is better suited only for instructions given in natural language1.
figure2showsthe gracepromptweprovided codexmodels fortheassociatedcodeupdatetask.let u1d463 u1d45b 1bethecurrentversion of the f ile u1d43fbe thelocations where code needs to be updated and u1d6ff0 u1d6ff1 ... u1d6ff u1d45b u1d45b 1bethe u1d45b 1associatededits.weassumethat eachedit u1d6ff u1d456 u1d456canbepartitionedinfourparts prefix which contains the fragment of code in version u1d463 u1d456 1that is untouched bytheedit butoccursbeforetheeditedcode before which contains the fragment of code in version u1d463 u1d456 1at locations u1d43fthat is replaced by the edit after which contains the fragment of code in version u1d463 u1d456at locations u1d43fin place of beforein u1d463 u1d456 suffix which contains the fragment of code in version u1d463 u1d4561that is untouched by the edit but occurs after the edited code.
these four parts are included in the prompt for each edit as shown in figure .
the associated edits are all included within the ctxedits tag.
the editto be predictedisincludedinside the currentedit tag.
in this prompt format the current edit is written out f irst followed by the associated edits.
this style ensures that if the prompt gets bigger than what can f it in the input to the model the tokensfromtheassociatededitsarepruned.wealsoexperimented with variants where certain associated edits were placed before the current edit and some after depending on where they occurred spatially.mostsuchchangesdidnotcauseanysigni f icantchange inour experimental observations.
theinsertionmodelisexpectedtopredictthestringthatshould occur between after and after that occurs under currentedit .
the pre f ix of the prompt string up until after goes in the prompt andthesuffixofthepromptstringstartingfrom after isincluded inthe suffixprompt ofthe insertionmodel.
the prompt design above is reminiscent of few shot learning prompts where the prompt contains afewexamplesof the taskto beperformed.technicallyspeaking theabovepromptisnotafewshotpromptsincewearenotprovidingoneormoreexamplesofthe associatedcodeupdate task .however ifwe viewtheassociated code update problem as a means of providing few shot examples for the codeupdatetask thenanaturalquestioniswhetherassociated edit update task can just be viewed as a few shot prompting for code updatetask.we answer this questioninsection .
one of the central goals of the paper is to f ind how using associatededitscompareswithnotusingitwhenpredictingcodeupdates.
to enable this comparison we need a prompt for the case when 1wedo not include code davinci edit in ourexperiments 1487esec fse december3 san francisco ca usa p. gupta a.khare y. bajpai s.chakraborty s.gulwani a.kanade a.radhakrishna g.soares anda.tiwari currentedit prefix .
.
.
prefix before .
.
.
before after .
.
.
after suffix .
.
.
suffix currentedit figure promptwhenassociated edits are notused.
table the models used inour experiments.
name basemodel fine tuned on codex davinci code davinci codet5 u codet5 base un f iltered c3potrain codet5 uf codet5 u f ilteredc3potrain codet5 uo codet5 u overwatch train associatededitsareunavailable.here weusethepromptshownin figure3.
speci f ically we remove the ctxedits section in figure .
note that the current code context is still available to the model in the prefix and suffix tags within currentedit .
.
fine tuningllms wenowdescribehowwecreate f ine tunedmodelsforpredicting codeupdates withandwithout associatededits.westartedwiththe codet5 basemodel a pre trained encoder decoder transformermodel.thisbasemodelwastrainedoncodesearchnet thatcontainssourcecodein6commonprogramminglanguages extended with two additional c c datasets from bigquery .
we further f ine tuned several variants of this model on the task of predictingcodeedits seetable .there aretwoversions ofeach variant onethatis f ine tunedusingthegivenassociatededitsand onethatonlyusesthecurrentversionofthecode.thetwotypes of f ine tuning use the same dataset and base model weights the only difference being how the data was prepared.
the variants are discussedindetailinsection .
.
we prepare data for f ine tuning by turning each training example into the graceprompt as shown in figure2.
we adapt the codet5 tokenizer by adding specialtokens prefix prefix suffix suffix currentedit currentedit ctxedit ctxedit edit edit after after before before .
we formulate the training as a masked span prediction task where we replace the contents between after and after under currentedit with a sentineltoken andask the modelto predict the maskedspan.
when f ine tuning codet5to predict code update without using associatededits weusethepromptshowninfigure .again we formulatethetrainingasamaskedspanpredictiontaskreplacing the contents between after and after under currentedit with a sentinel token and asking the model to predict the masked span.
.
deployment we now discuss how the sub problems discussed in section .1can potentially be solved and integrated with our approach to create an ide basededitprediction tool setup as discussed in section .
the editing target could be thelinecorrespondingtotheuser scursorlocation.
overwatch can be used to extract temporal edits from patterns that match the target location andtheseeditscan serve as our associatededits.table the datasetsused for f ine tuningand testing.
dataset training eval test c3po f iltered .5k .4k5.9k c3poun f iltered .67m 180k210k overwatch 9k1k1k worklow consider a user editing code in an ide.
the tool will get triggered on the line where the user s cursor resides and the associated edits would be retrieved using overwatch .
our edit predictionpromptwillbegeneratedasdiscussedinsection .
.the promptwillthenbesentasaninputtoanllm say codex davinci andthepredictededit ortop kpredictededits willbesuggestedto the user.
we have designed an interactive tutorial to walk readers through this work f low using the various examples discussed in section2andsection .
see section 9for instructions .
experiments results .
experimentalsetup weusetwodatasetsfrompriorworkforourexperiments the c3po dataset andthe overwatch dataset see table .
c3podataset thec3podataset was created by scraping allcommitsin53mostpopularc githubrepositories.eachedit in a commit would create a single example and the edits if any onthe10linesaboveand10linesbelowtheeditwouldmakeup the associatededits.
the taskis to predict the code after an editis performed giventhecodebefore theeditand theassociatededits.
thus the c3podataset is an instance of the associated code update task where spatiallocality isusedtode f ineassociationsbetween edits.notethatthe c3popaperreferstotheseeditsas contextual editswhichtranslate to edits withspatial associations inour work.
thec3podataset was further f iltered by its creators into a f ilteredc3podatasetby removing simple benchmarks e.g.
those containingonlydeletionorrenaming .further theyremovedall benchmarkswherethetargeteditinvolvedinsertionof newcode as their approach cannot handle those.
the f iltered set was further partitioned into train validation and test benchmarks containing respectively39.5k .4k and5.9kbenchmarks seetable .weused the same partitionsinour evaluation.
overwatch dataset the dataset describedin was gathered from versions of source code f iles taken as they were being edited in an ide session over two separate periods.
in the f irst period .5k versions were collected over sessions.
in the second period .1k versions were collected over sessions.
theversionsinthe f irstperiodwereminedin togetasetof .9kedit sequences which are further used to learn a collection of symbolic rules representing commonly occurring edit sequence patterns.theselearnedrulesareusedtogeneratecodesuggestions inthesecondperiod andtheyarefoundtobecapableofproducing suggestions at f ileversions.for the purposeof thiswork we areconsidering90 ofthe .9keditsequencesfromthe f irstperiod astheoverwatch trainingset keepingother10 asthe overwatch evaluation set andthepointsofapplicationsasthe overwatch test set.we discuss thesedatasets further insection .
.
models we used two models as starting points.
the f irst is code davinci referred to as codex davinci in this text a 1488grace languagemodelsmeet code edits esec fse december3 san francisco ca usa decoder onlytransformermodelwhichisapartofthe openaigpt3.5series .thismodelispresentedwiththepromptsbased on either figure 2or figure 3depending if we want to use use associatededitsforeditprediction.thesecondmodelis codet5 an encoder decoder model introduced by salesforce in the baseandlargevariants.
we use the codet5 base variant which has220mparameterswith12transformerblocksintheencoderand decodereach.thismodelis f ine tunedontheun f iltered c3potraining datasetto create the model codet5 u .themodel codet5 u is further f ine tuned on the c3po f iltered train set and overwatch train set to create codet5 uf andcodet5 uo respectively see table2.
we used this two step f ine tuning process since the overwatchtraining data was limited.
the f ine tuning and inference setupsfor thesemodels are describedbelow.
setupfor codex davinci experiments weusedthe openai publicapitoperformtheinferenceexperimentswiththe codexdavincimodel.the insertmodeofthemodelwasusedandthe input was divided into promptandsuffixfollowing our prompting strategydiscussedinsection .
.temperaturesamplingwasused to generate n predictions and the temperature was set to .
afterevaluatingmultiplecandidatevalues.themaximumlength maximumnumberoftokenstogenerate wassetto256 stoptoken to after anddefaultvalueswere usedfor allotherparameters.
setup for codet5 experiments for our f ine tuning experiments weuseavirtualmachine with16amdmi200gpus each with64gibofvram 92cpucoresand1594gbofram.wesetthe inputtokenlengthto1024tokensandtruncateanylongerinputs fromtheend.there aretwosteps inour f ine tuningprocess f inetuning on the un f iltered c3podataset followed by dataset speci f ic f ine tuningonthe overwatch training and c3po f iltereddatasets.
fortheinitial f ine tuningwiththeun f iltered c3podataset weinitialize the model with the publicly released codet5 baseweights and train it for epochs with a batch size of per device.
the optimization is done using the adafactor optimizer with learningrateinitiallysetto u1d452 4andgraduallyupdatedusingalinear scheduler after a warmup of steps.
the best model weights are determinedusingtheperplexityscorebyevaluatingonthe c3po validationdatasetatevery1000steps.forfurther f ine tuningonthe overwatch trainingand c3po f iltereddatasets wesettheinitial learning rate to u1d452 the number of warmup steps to and train the model for epochs while evaluating it every steps.
during inference we use beam searchwithabeam widthof5.
metric in order to stay consistent with the metrics used by papers that curated the target datasets namely the c3poandoverwatchdatasets wede f ineametriccalledthe exactmatch .inthe experiments with the c3podataset a prediction is said to be an exact match if it syntactically matches the ground truth modulo whitespaces.
we use exactmatch to alsodenotethe percentage of caseswhereapredictionwasanexactmatch.moredetailsonthe overwatch datasetevaluationcanbefoundinsection .
.inall our results we report exactmatch for top 1predictions.
.2graceimprovesprediction a key question we set out to answer was whether associated edits helppredict future code changes.in otherwords rq1.
does availability of associated edits improve code update predictions?doestheanswerdependonthepredictionapproach?table associated edits improve codeprediction.
c3potestset overwatch testset modelwithout assoc.editswith assoc.edits grace without assoc.editswith assoc.edits grace codex davinci .
.
.
.
codet5 u .
.
.
.
codet5 uf .
.
.
.
toanswerthisquestion wetestedboth codex davinci andcodet5 onboththe c3poandoverwatch testsets oncewithassociated editsinthe prompt andoncewithoutthem.
results table4showstheexactmatch obtained when we use the different models on the different datasets with and without associated edits.
we see that codex davinci shows a absolute increaseinexactmatchwhenprovidedassociatededitsthanwhen not on the c3podataset and about absolute increase on the overwatch dataset.
the f ine tuned codet5models showed about a absolute increase in exact match on both datasets.
finally althoughtable 4reports thetrendfor2 modelsand oneprompting style we tried other models including other openaimodels from gpt3andgpt .5series anddifferentstylingoftheprompts for example using c comments rather than tags to delineate the before and after versions andin every case there was atleast a absolute increase in exact match often it was much higher.
result1 conditioningcodepredictiononassociatededitshelps acrossmodelsand test datasets.
.
relevanceofeditsmatters the associated code update problem conditions code prediction onsomeassociated edits.wehaveinformallymentionedthatthe associatededitsshouldbepickedbasedontheirrelevancetothe codethatisbeingupdated.ournextresearchquestionisconcerned withhowrelevanceimpacts prediction.
tomotivatethisresearchquestion we f irstmaketheconnection to few shotprompting .considerjustthe codeupdatetask predict the new version of the code given its old version.
the difference betweenthe codeupdatetask and associatedcodeupdatetask aretheassociatededits.now apromptcontaininganinstanceof the associatedcodeupdate taskbeginstolookalotsimilartoa few shot prompt for a code update task where the associated edits serve the purpose offew shotexamples of code update.
it may be tempting to say that the associated code update task just combines some few shot examples with a code update task.
however thisviewisnotbene f icialsinceassociatededitsaremore thanjust anyexamplesofcodeupdates .asdiscussedinsection the associated edits contain crucial information for performing the given code update.
to validate that associated edits are more than justcode updateexamples we turnto our nextresearch question rq2.areassociatededitsimportantforcodeupdateprediction or simply serveasfew shot examples for thecodeupdatetask?
inotherwords istheresomethingtobegainedbyusingassociated edits beyond what we gain by just adding some few shot examples ofcode updates that are not necessarily associated ?
results table5showsthe exactmatch wegetusingthe codexdavincimodelusingdifferentsetsofeditsasthe associatededits .
1489esec fse december3 san francisco ca usa p. gupta a.khare y. bajpai s.chakraborty s.gulwani a.kanade a.radhakrishna g.soares anda.tiwari table less relevant edits degrade prediction codexdavinci on f iltered c3potestsetfordifferentassociatededits.
choiceofassociated edits exact match association dataset repository spatial filtered same .
random filtered same .
random filtered other .
random un f iltered same .
random un f iltered other .
noassociated edits .
weusethe c3po f ilteredtestsetagainforevaluation.wesawbefore thatwegeta .
exactmatchonthistestset .thiscase corresponds to when the prompt includes spatially close f iltered editsfromthesame f ile thisisapropertyofthe c3pobenchmarks .
let us now randomly sample edits to include in the prompt.
there are twodimensions andtwobuckets ineachdimension touse for sampling the f iltereddatasetversustheun f iltereddataset andedits fromthesamerepositoryversuseditsfromdifferentrepositories.
randomlypicking f ilterededitsfromthesamerepodropsperformanceonlyslightly.however randomlypicking f ilterededitsfrom other repos drops performance more signi f icantly to .
.
when samplingfromun f ilterededits irrespectiveofwhethereditsare from the same or different repos the exact match remains consistentlyaround .werecallthatwhenweprovidenoassociated editsinthe prompt we had .
exactmatch .
the results show that going from f iltered to un f iltered edits reduces relevance of edits to the target f iltered edit .
this is because the f iltering stepin actually removes certain kinds of edits for example edits that are pure insertions or deletions or edits that result in unparseable code.
hence a randomly picked un f iltered editis morelikelytobe structurallydifferent fromourtarget edit whichwaspickedfrom the f ilteredtest set.
the results also show that pickingedits from repositories other than the repositoryof the target edit reduces relevance of the edit to the target edit.
this is because edits from the same repository could potentially be using common concepts classes methods programmingpractices andeven contain similar changes.
finally we note that using un f iltered edits from other repositories .
isstillbetterthannotusingthem .
.thisis possiblyduetothellmleveragingits few shotlearningcapabilities in that case.
the gain from around to around can thus be attributed to the associated edits.
we can therefore conclude that result2 associatededitsplayacrucialroleinpredictingatarget edit andtheexactmatchmetricdropsastherelevanceoftheedits to thetargeteditdrops.
.
pre trainedoutperforms custom when working with code and code edits llms such as codex andgpt3 andotherpre trainedmodels suchas codet5 usebytepairencodings bpe totokenizecodeandthenrepresentcodeas a sequence of tokens in the same way as natural language is represented.
in contrast some works have argued for the use of custom representations for code and code edits that partly capture theparsestructureand ortheprogramminglanguagesemantics.
thepaperthatintroducedthe c3podataset alsousedthespatialtable comparisonwith c3po.
exact match on model c3pooverwatch c3po .
.
codex davinci .
.
codet5 u .
.
codet5 uf codet5 uo .
.
editsusedbyourpre trainedllmsbuttheylearnedacustommodel employing code centric representations for code edits.
our next research question concerns comparing our approach based on pretrainedmodelswithpriorworkoncustomneuralapproaches.while both the approaches have access to the associated spatial edits we wantto understandhowpre trained modelswiththeir text based promptscompareagainstmodelswithcustomcoderepresentations.
rq3.
how does our llm based approach compare with the c3po approach based on a custom neural model on the associated code updatetask?
let us compare how the c3pocustom neural model performs in comparison to codex davinci and f ine tuned codet5.
we f irst compare these models on the f iltered c3potest set and the overwatchtestset.table 6showsthatboth codex davinci and f inetunedcodet5signi f icantly outperform the custom c3pomodel on both test datasets.
the c3pomodel was reported to give a .
accuracy on thec3potest set whereas both codex davinci and f ine tuned codet5give betterresults.the codet5 uf model gives81.
exact match which is signi f icantly higher than .
achieved by the c3pomodel.
similarly on the overwatch dataset the best possible con f iguration of c3powas reported to give .
exactmatch whereasallof codex davinci .
codet5 u andcodet5 uo .
perform signi f icantly better.
comparison on un f iltered c3potest set thec3pomodel doesnotreportresultsontheun f iltered c3podataset.thisispartly because it contains benchmarks that are out of scope for their technique.
two such notable benchmarks are a benchmarks that containalieninsertions wheretheinsertedcodecontainstokensthat do not occur in either the associated edits or the current version ofthetargetcodesnippet and b benchmarksthatcontaincode snippetsthatcannotbeparsedbyanunderlyingparser thisstep isimportant for c3poto generate theabstract syntax tree ast .
gracecan handle both these classes of benchmarks.
we evaluated codex davinci on a .9k random sample from this test set and obtaineda43.
exactmatch.these5.9ksamplesdidnotcontain anybenchmarksfromthe f ilteredset.
weusedasamplebecause ofthecostof fullun f iltered c3potestset codet5 u has57.
exactmatchusing graceand .
without.thesenumbersarelowerthanthoseforthe f iltered c3potest set.
this indicates that the un f iltered benchmarks are more challenging than the f iltered benchmarks which is at odds withthe informal assertions to the contrary in .
alieninsertionbenchmarks weextractedthesamplesfrom un f iltered c3potestset thatinvolved alieninsertions.onthatset codet5 u withgraceachieved17.
exactmatch butonly10.
without it.the codex davinci model achieved17.
exact match withgraceand .
without it.
this indicates that conditioning onassociatededitscan helpwithhardbenchmarks.
1490grace languagemodelsmeet code edits esec fse december3 san francisco ca usa table comparisonwith overwatch .
technique overwatchcodexdavincicodet5 u codet5 uo exact match .
.
.
.
admittedly the c3pomodel is much smaller 750k parameters comparedtoboth codex davinci 175b and codet5 220m .however these large models are pre trained and hence they can be quickly f ine tuned or prompt engineered for downstream tasks without need for excessive training data.
furthermore the pretrainedmodelsarenotlimitedinscope aswehavediscussedabove.
result3 pre trainedlanguagemodelscanbetunedtoyieldhigher exact match compared to the custom c3pomodel for associated code updateprediction.
.
temporaleditprediction the temporal edit prediction problem is an application that is wellsuited for using grace.
the state of the art in this application domain is overwatch .
fundamentally overwatch is solving adifferentproblemfromtheassociatededitspredictionproblem the input to overwatch are f ine grained ide version histories of the form u1d4630 ... u1d463 u1d45bwhere each u1d463 u1d456is a version of the source code f ile.
the edit histories are extremely f ine grained at the keystroke unlike source control histories.
for example if a developer types a variablename predicate eachintermediate f ileversioncontaining the pre f ixes p pr ...ispresent inthe edithistory.
theoverwatch technique takes the set of such ide version historiesas atraining set andproduces arankedsequenceofedit sequence patterns esps .
at inference or run time in an ide each espexaminesthecurrentversionhistory u1d4630 ... u1d463 u1d45band a identi f ies asequenceoftransitivecoarse grainededits u1d6ff u1d4560 u1d4561 u1d6ff u1d4561 u1d4562 ... u1d6ff u1d456 u1d458 u1d456 u1d458 i.e.
each u1d6ff u1d4560 u1d456 u1d458is the edit between the potentially non consecutive versions u1d463 u1d456 u1d457and u1d463 u1d456 u1d457 and b usestheseeditstopredictthenext edit to u1d463 u1d456 u1d458.
in short the esps are two tasks a identifying associated edits from f ine grained version histories and b using theseassociatededitsto predict the nextedit.
rq4.canourllmbasedapproachbeusedinconjunctionwith overwatch stemporalassociatededitidenti f ication?howdoesit comparewith overwatch ssymboliceditpredictioncomponent?
the second task above is exactly the prediction from associated editsproblemwearetacklinginthispaper.hence werun overwatchonitstestdataof 399versionhistorieswithover versions andgathertheassociatededitswherevertheespsareable toidentifythem.thisresultsinadatasetof1048casesasmentioned in section .
.
at training time overwatch identi f ies a set of .9k editsequencesfromtheolderdataof 682versionhistories however usingdifferenttechniques.theeditsequencesaresuchthateach ofthembelongtosomecommonlyoccurringeditsequencepattern acrossversionhistories theyarethesupportsforesps andthus each of them canbe treatedas aset of associatededits alledits in the sequence but the last and expected edit prediction last edit in thesequence .weusethissetof9.9kinstancestofurther f ine tune codet5 u to obtain codet5 uo see table .
table7summarizes the different models performance on the 1048test cases along with overwatch s predictionsasa baseline.1catch exception ex info.reportclienterror scheme is missing info.reportclienterror scheme is missing system.net.
httpstatuscode.badrequest 6default info.reportclienterror no such action info.reportclienterror no such action system.net.
httpstatuscode.notfound figure4 useradds badrequesterrorcodeonline3andmoves to line5 where we should predictinserting notfound.
exceptcodet5 u all of our models beat the prediction component ofoverwatch byaconsiderablemarginofroughly10 .
result4 ourllm basedtechniques inconjunctionwithsystems likeoverwatch createsneuro symbolicsolutionsthatarebetter at predicting next editcompared to purely symbolictechniques.
.
qualitativeanalysis our experiments support two major observations a llms can predicteditsthatexistingtechniquesfundamentallycannotsupport and b theadditionofassociatededitsimprovestheperformanceof llmsonthetaskofpredictingcodeedits.next weprovideinsights intowhy theseobservations holdtrue.
.
.1comparison with existing techniques.
in the following few paragraphs we discuss the salient features of llms and the gracepromptdesignthathelpourapproachoutperformexisting techniques i.e.
c3poandoverwatch oncertainkindsof edits.
generativecapabilitiesofllmsareusefulinpredicting alieninsertions asdiscussed in section thellmswe discuss in this paper can support most forms of insertions as they have accesstoawidenumberoftokensthroughtheirpre trainingand ourpromptingsetupdoesn trestrictthetokensthatthemodelscan generate.existingtechniquesarerestrictedinthisaspectbydesign c3pocannot insert tokens other than those found in the contextual editsandoverwatchmaylearnpatternswherethepredictiontemplate is incomplete due to unavailable mappings for holes in the temporal editpattern.
forinstance considerthescenarioinfigure 4wherea developeristryingtoaddhttperrorcodestoerrorreportingcalls.
here the developer f irst edits line by adding a badrequest error code to the reporting call.
they then move to line to make a similaredit.notethattheexpectederrorcodeonline5isdifferent fromtheoneonline3asitcorrespondstoa nosuchaction error message.moreover theexpectederrorcodehasatoken notfound whichisnotpresentanywhereintheexistingcontext.asc3po s pointer network can only pick paths to from existing nodes it cannotgeneratethisnewtoken.
codex davinci withgracecan correctlypredict this edit.
access to local spatial context in the prompt is useful overwatch learnspatternsandtemplatesfromobservededitsequencesandstrictlyreliesonthesepatternstomakepredictions.
there are cases however where the pattern learnt by overwatch istoogeneralandisapplicableirrespectiveofwhatisinthespatial vicinity of the target edit.
to better understand this limitation consider the scenario in figure 5from an active ide editing session.
theuser f irstreplaces exonline5with ex.outputandthenmoves toline3tomakethenextedit.overwatchgetstriggeredonline3 1491esec fse december3 san francisco ca usa p. gupta a.khare y. bajpai s.chakraborty s.gulwani a.kanade a.radhakrishna g.soares anda.tiwari 1foreach varexincurrentexamples console.writeline gettext ex diff.beforefile console.writeline gettext ex.input diff.beforefile console.writeline gettext ex diff.afterfile console.writeline gettext ex.output diff.afterfile 8varoutput run currentexamples .first .input 9assertequal currentexamples .first .output output figure5 userreplaces exonline5by ex.output movestoline3 where we should predictreplacing exbyex.input.
andpredictsthat exshouldbereplacedby ex.outputsinceitlearns the pattern repeat the same replacement which is an instance of a common pattern.
however this is incorrect since exshould be replaced by ex.inputhere.codex davinci withgrace correctly predictsthiseditbecause exonline3isfollowedby diff.beforefile andline8hasadditionalinformationabouttheproperty inputthat is associated with each entry in currentexamples .
our prompt design allows f lexible addition of this additional spatial context throughthe prefix and suffix tags.overwatch ontheother hand cannotaccessspatialcontextthatisnotalreadypresentin the learnedtemplate.
language based pre training is useful in identifying semanticediting pa t terns scenarios infigures 4and5alsohighlight the ability of llms to predict patterns based on semantics ofidenti f iersinthecontext.infigure codex davinci seemsto understand that the relationship between diff.afterfileanddiff.
beforefile wouldalsore f lectintheprecedingargument ex.output andex.input respectively .
in figure codex davinci uses the signalfrom the no such action errormessageto correctlypredict thattheerrorcodeshouldbe system.net.httpstatuscode .notfound.existingtechniquessuchas c3poandoverwatch relyoneditpath analogies and symbolic editing patterns respectively to understand theeditingintent.withouttheuseofalanguage basedpre training component itmaybedifficulttoobtainthesemanticunderstanding neededto perform the editsinfigures 4and5.
.
.2benefits of using associated edits.
we observed three key bene f itsofprovidingassociatededitsto llms associatededitshelpinclarifyingtheeditingintentofthe developer the illustrative example in section figure1 showed that associated edits provide strong signals about the next edit thatthedeveloperintendstoperform.infact withoutassociated edits codex davinci doesn tpredicttherightediteveninthetop results.
with associated edits the correct prediction is ranked at the top suggesting that associated edits help improve the top performance ofthe model.
associatededitsemphasizerelevantcodecontext while llms like codex davinci can support a large number of tokens in their prompts 4k in codex davinci s case it has been observed thatirrelevantinformationinthepromptaffectsthemodel sability to attend to the right set of tokens .
in the illustrative example in figure the target edit is lines away from the required spatialcontext.
codex davinci canpredicttherightimportwith only4 5linesinthespatialcontextandaccesstotheassociatededit.
the scenario without associated edits on the other hand requires providing250linesofmostlyirrelevantcodetothemodeltoinclude theexceptionthattherequiredimportprovides.
codex davinci fails to generate the right prediction in the top results even withall of this spatial context.
on a simpler version of this example where the relevantcodecontextis movedcloser to thetargetedit fromline250toline15 codex davinci withoutassociatededits predictstherightimportintop butitisnotthetop 1prediction.
associated edits contain information about edited code elements there may be key variables that are deleted or replaced by previous edits but referenced by the target codelocation.
withoutaccesstotheseassociatededits themodelhasnocontextabout thesevariables methodsorothercode elements.forexample ifa variable var1isreplacedby var2inapreviouseditandthedeveloper nowmovestoline var1 var1 themodelisexpectedtoreplace thislinewith var2 var2 .withoutaccesstothepreviousedit themodeldoesn tknowtherelationshipbetween var1andvar2and mayconsiderthemto be twodistinct variables.
.
additionalresults discussion weconductedadditionalexperimentstounderstandhow grace affectsrobustness andentropyduring prediction.
we also evaluated otherpromptingstylesandmodelcon f igurations.seethetechnical report for details andfurther discussion.
related work .
automaticcodeediting in recent years there has been a signi f icant boom in academic and industrial researchfor automating developers code editing activities.mostmodernides supportautomatedcodechanges like the addition of boilerplate code developer assisted refactoring etc.
while these developer assisted approaches tremendously help boostproductivity asigni f icantamountoffurtherresearchexists in automated code editing aimed at learning code edit patterns fromdeveloper spreviousedits .wedividetheseapproachesintotwoorthogonaldirections symbolic approaches symbolic approaches learn the code transformation patterns by representing the example edits with symbolicabstractions.givenasetofsuchsymbolicallyrepresented abstract edits these approaches generalize the edit patterns as a sequenceofeditoperations.forinstance refazer represents syntactic changes with domain speci f ic language and uses a deductive inference algorithm to generalize and synthesize common editpatterns.morerecently overwatch learnstogeneralize developercode editing behavior from asequence of code versions.
eacheditisrepresentedas preandpostprogramstates andgeneralized edit sequences are derived from an edit graph from these state pairs.whiletheearlierworksinsymbolicediting primarilyfocusedonsyntacticediting i.e.
refactoring similarto overwatch we also focus on semantic changes in code.
similartooverwatch we emphasizeonconditioningfuture edit w.r.t.
associatededits.however unlike overwatch gracedoesnotnecessarily need demonstrations of the speci f ic edit sequence pattern to learn to apply that pattern.
neural network based approaches recent advancements in machine learning and neural networks have catapulted the f ieldofcodeeditingwithneuralnetworksrelyingontheirnoise tolerance and generalization capabilities.
as such several approaches have been proposed over the years usingdifferenttypesofneuralnetworksforautomaticallygenerating edits.
notable among these are sequence to sequence neural 1492grace languagemodelsmeet code edits esec fse december3 san francisco ca usa machine translation based approaches tree to tree translationsapproach andgraphneuralnetworkbasedapproach .whilemostoftheseapproacheslearntogeneralize code edit patterns from seemingly unrelated example edits this workshowstheimportanceofrelatedassociatededits.nevertheless themostnotablefeatureofneuralcodeeditingapproaches is how the approach generates the edited code.
while some approaches generateascriptofeditoperations i.e.
insert delete update others generatetheeditedcodeapplying theeditpatternintheprocessoftranslation.similartothelatter approach we generate the edited code given the code before the edit.
.
deeplearning forsourcecode recentadvancementindeepneuralnetworks dnn hasdrawn focusontheapplicationofsuchindifferentsourcecodeunderstanding and generation tasks including bug detection code comprehension code search code generation code translation programrepair etc.thevastplethoraof dnnmodelsinsetasksrangesfromgeneral purposemodels inspiredbynaturallanguageprocessingtocustom builtmodelsfor source code modeling .
these models however require a large quantity of labeled data to optimize millions of parameters.
to overcomethisproblem researchershave proposedto pre train modelswithalargequantityofunlabelleddata andsubsequently re usesuchapre trainedmodelacrossdifferenttasks .there are a wide variety of pre trained models for source code proposed over the years some containing hundreds of billions ofparameters colloquiallyknownaslargelanguagemodels or llms.
llms showexcellent promise in autonomously learning programminglanguagepropertiesandadditionally haveshownthe abilityto learn deductive reasoninginherentinprogramming and naturallanguages .assuch thesellmsareleveraged in many industrial developer assistance tools such as github copilot amazoncodewhisperer intellicodecompose etc.
in this work we show an in depth investigation of harnessing the power ofthesellmsfor automatedcode editing.
limitations threats to validity limitations there are certain limitations of our approach that wewouldliketoaddressinfuturework.firstly asourapproach depends on other edit mining techniques it is restricted by the quality of the collected edits.
on rare occasions associated edits in the prompt can also mislead the model with some irrelevant information which in turn leads to incorrect predictions.
moreover our approach canalso failwhen thegroundtruthrequires knowledge ofcertaincontext methodsignatures forexample thatdoesnot appearintheassociatededits.secondly thellmsusedareprone toknownissuessuchashallucinations generationofuncompilable code etc.
despite being generative these models can still fail to predict editsthat involve generatingentirelynewcode.
threats to validity when using a pre trained model there is alwaysathreatoftestdataleakingtothetrainset .itispossible that the data used for pre training codex davinci contained some or all of the data in the c3potest set since the c3podataset was created from github repositories.
one wayto mitigate this threat istoperformevaluationonmultipletestsets.therefore wealsoperformed our evaluation on the overwatch dataset.
the overwatchtestsetwasnotpubliclyavailableandweobtaineditdirectly from the authors.
hence we believe our results are not in f lated because ofthe possibility of codex davinci having seenthe c3po test set.
we mitigated the threat further by performing the same experimentson f ine tuned codet5.allconclusionswemakeinthis workareinformedbyresultsfrombothmodelsonbothdatasets.
finally this potential data leak would affect all our experiment settings with the codex davinci model equally and any bene f it would also have been available to the model without associated edits.ourresultssuggestthatthemodelclearlybene f itsfromthe additionofassociatededitsthus entailingafair comparison.
thetestsetsareanothersourceofpossiblegapbetweenwhatwe observeinourexperimentsandwhatwemayseeiftheapproach were deployed in real world.
the c3podataset was created from commits.
it de f ined an edit at a certain level of granularity.
this de f initionmaynotmatchthenotionofeditsusedinsometarget application ofourcodepredictionmodels .again wemitigatethis threatbyalsotestingon overwatch datasetthatusesadifferent level of granularity for de f ining an edit.
our results appear to hold acrossthedifferentpossiblenotionsofan edit .infact bypresenting the associated edits to the model in the prompt and during f ine tuning weareabletoteachthenotionofanedittoit.even withthenotionofeditconveyed thedistributionofassociatededits in our test sets may not re f lect what we observe in practice.
the approach based on codex davinci is not immune to this threat but the f ine tuning approach can adapt if we have f ine tuning data.
conclusions futurework predictingcodeeditsisanimportantsoftware engineeringproblem.
in thispaper we leveragethe generative capability of llmstoaddressthisproblem.withouttheknowledgeofprioredits thellms fail topredict the requirededits butwhenwe combine themwith associatededits theirperformanceimprovesgreatly.thissimple strategy is quite effective and as shown in the experiments grace outperformsthecurrentstate of the artspecializedsymbolicand neuralmethodsontheirrespective datasets.
thegenerativecapabilityofllmshasopenedupmanyopportunities for addressing software engineering problems that have beenhardtodealwith.webelievethatcombiningthellmswith domain speci f ic insights such as our use of associated edits holds promise for hithertochallengingproblems.in thefuture we shall seektoexploitthisstrategyforothersoftwareengineeringproblems.ontheproblemofpredictingcodeedits weplantoexplore the problem of discovering associated edits and the application to large scale migrations refactorings andmaintenanceactivities.
data availabilitystatement thec3podataset is made publicly available by the authors of .
wesharethescripts prompts andinstructionstoaccessthe f inetunedmodelson c3poat .sincethe overwatch datasetisprivate wedonotholdtheauthoritytoredistributethedatasetoranymodelslearnedfromthatdataset.readers with access to overwatch data can reproduce the experiments using the shared scripts.
an interactive tutorial notebook discussing deployment of our approach in an ide based edit suggestions tool isalsoavailable at the same webpage.
1493esec fse december3 san francisco ca usa p. gupta a.khare y. bajpai s.chakraborty s.gulwani a.kanade a.radhakrishna g.soares anda.tiwari