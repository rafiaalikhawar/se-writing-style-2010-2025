the same only different on information modality for configuration performance analysis hongyuan liang1 yue huang1 tao chen2 1school of computer science and engineering university of electronic science and technology of china china 2ideas lab school of computer science university of birmingham united kingdom lianghy16 outlook.com huangyue16 outlook.com t.chen bham.ac.uk abstract configuration in software systems helps to ensure efficient operation and meet diverse user needs.
yet some if not all configuration options have profound implications for the system s performance.
configuration performance analysis wherein the key is to understand or infer the configuration options relations and their impacts on performance is crucial.
two major modalities exist that serve as the source information in the analysis either the manual or source code.
however it remains unclear what roles they play in configuration performance analysis.
much work that relies on manuals claims their benefits of information richness and naturalness while work that trusts the source code more prefers the structural information provided therein and criticizes the timeliness of manuals.
to fill such a gap in this paper we conduct an extensive empirical study over systems covering options words in the manual and lines of code for investigating the usefulness of manual and code in two important tasks of configuration performance analysis namely performancesensitive options identification and the associated dependencies extraction.
we reveal several new findings and insights such as it is beneficial to fuse the manual and code modalities for both tasks the current automated tools that rely on a single modality are far from being practically useful and generally remain incomparable to human analysis.
all those pave the way for further advancing configuration performance analysis.
index terms software configuration performance analysis manual source code analysis configuration dependency i. i ntroduction software systems are often designed with a daunting number of configuration options.
for example until now m ysql has more than configuration options that can be set by users .
although this certainly indicates the great flexibility of the software it also comes with a cost it has been reported that of the performance issues worldwide are caused by configuration .
therefore much work has been done to mitigate these issues mainly related to the modeling testing and tuning of configuration.
however all those research topics rely on or can benefit from two prerequisite tasks since not all the options are performance sensitive it is essential to preselect which are the ones that should be considered.
for example options that set a directory path or a port often do not impact performance.
in contrast options that control the code logic or some resource heavy operations are performance sensitive such as tao chen is the corresponding author.
hongyuan liang and yue huang are also supervised in the ideas lab.thewait timeout in m ysql and maxthreads in t omcat there might be a dependency between two options for example in n ginx the option tcp nopush can be used only when option sendfile is enabled.
as a result it is fundamental to know these dependencies beforehand.
those prerequisite tasks are the key steps in configuration performance analysis .
indeed much work has been done to investigate and propose tools for inferring performancesensitive options and their dependencies but they leverage a single modality either using manual e.g.
official guidelines or api docs or the source code.
the motivation for using either modality in configuration performance analysis is arguable on the one hand researchers who are in favor of manuals state that these artifacts provide strong and intuitive domain knowledge with rich information to be extracted .
on the other hand work that relies on source code claims that it offers comprehensive structure and state of the systems while being robust to the timeliness issue of the manual i.e.
what has been changed in the codebase might not have been timely reflected in the manual .
yet there has been a lack of understanding on which modality should be preferred or tends to be more useful in configuration performance analysis and why.
in this paper we seek to fulfill this gap.
to that end we conduct an extensive empirical study over systems that come with a total of configuration options words of manual and lines of code aiming to understand the role of manual and code for identifying performance sensitive options and extracting their dependencies in configuration performance analysis.
we do that in two phases in the first phase we carry on a human analysis involving all authors that independently parse and analyze the manual and code from each of which the results are compared.
in this second phase we examine the effectiveness of existing automated tools that leverage manual or code individually while comparing their outcomes against those from the human analyzers.
through those we aim to answer several important research questions rqs related to configuration performance analysis rq1 how can manual or code be relatively useful in identifying performance sensitive options?
rq2 comparatively to what extent does manual or code help to extract performance sensitive dependencies?arxiv .15475v2 feb rq3 how do existing automated tools in configuration performance analysis perform compared with human analysis using either manual or code information?
among others our key findings contributions are that to identify performance sensitive options the manual leads to fewer false negatives while the code is more false positive resistant but fusing both modalities can maximize the benefit.
they both cause false positives due to misleading information but incur different causes for false negatives the vague description in the manual and the non standard patterns of executions in code.
while code is often more helpful for extracting dependencies than manual the manual can still be useful for some edge cases.
we found no false positives for both modalities and the false negatives of finding dependencies from manual and code are mainly due to lack of information and excessive recursions of function calls respectively.
existing automated tools for both tasks are far from being practically useful and they are not comparable to human analysis except for identifying performancesensitive options from the manual.
deriving from these we articulate several insights for this direction of research such as the necessity of fusing both manual and code modality in identifying performancesensitive options and the extraction of dependencies therein should also consider performance insensitive options.
as part of our results we have also made the dataset code and tools used available at compared with existing datasets our labeled dataset is of a larger scale with more diverse systems complementing the research for automated configuration performance analysis.
the reminder of this paper is as follows section ii presents the background and motivation.
section iii delineates our empirical methodology.
section iv presents and analyzes the results followed by the insights learned in section v. sections vi and vii present the threats to validity and related work respectively.
section viii concludes the work.
ii.
b ackground and motivation a. analyzing configurable systems a configurable system can be configured by giving a vector of configuration values c x1 x2 ... x n where xndenotes the value of the nth configuration option which can be binary enumerate or numeric .
configuration performance analysis studies the correlation between the configuration options and the system s performance e.g.
runtime or throughput along with the interrelationships between the options.
the results are fundamental for many configuration related tasks such as configuration performance modeling configuration tuning testing and self adaptation .
b. performance sensitive options not all configuration options are performance sensitive therefore it is essential to identify which are performancesensitive in configuration performance analysis.
in essence option range optimizer max mem size partial description the range optimizer max mem size controls the limit on memory consumption for the range optimizer.
a value of means no limit.
if an execution plan considered by the optimizer uses the range access method but the optimizer estimates that the amount of memory needed for this method would exceed the limit it abandons the plan.
a manual from m ysqloption select into buffer size partial code snippet if init io cache cache file thd variables.
select into buffer size write cache 0l true myf my wme mysql file close file myf mysql file delete key select to file path myf return b code from m ysql fig.
examples of manual texts and source code for revealing performance sensitive options.
dependency sendfile andtcp nopush partial description in this configuration sendfile is called with the sf nodiskio flag which causes it not to block on disk i o but instead report back that the data are not in memory.
n ginx then initiates an asynchronous data load by reading one byte.
on the first read the freebsd kernel loads the first 128k bytes of a file into memory although next reads will only load data in 16k chunks.
this can be changed using the read ahead .
enables or disables the use of the tcp nopush socket option on freebsd or the tcp cork socket option on linux.
the options are enabled only when sendfile is used.
a manual from n ginxdependency dfs.replication anddfs.na menode.maintenance.replication.min partial code snippet final int minmaintenancer conf.getint dfsconfigkeys.
dfs namenode maintenance replication min key dfsconfigkeys.
dfs namenode maintenance replication min default ...... this.defaultreplication conf.getint dfsconfigkeys.dfs replication key dfsconfigkeys.
dfs replication default ...... if minmaintenancer defaultreplication throw new ioexception ...... b code from hdfs fig.
examples of manual texts and source code for revealing performance sensitive dependencies.
two common modalities exist for finding performancesensitive options the documentation manual and source code.
the former often provides clear information about the performance sensitivity.
as can be seen in figure 1a the description states that the option can control the allocated memory which would clearly be important for the performance.
the latter in contrast contains performance sensitive information with the code execution logic.
for example in figure 1b the call logic shows that select into buffer size affects the buffer size for initializing the cache a key performance indicator.
c. performance sensitive dependencies there can be a dependency between a pair of options.
for example in the h adoop implementation of m apreduce any value of the option mapreduce.jobhistory.loadedjobs.cache.size would be ignored if the option mapreduce.jobhistory .loaded.tasks.cache.size is set to a positive value.
failure to comply with the dependency might result in e.g.
ineffective configuration or runtime exceptions.
similar to the performance sensitive options extracting dependencies sensitive to the performance can also be achieved via analyzing the manual or code.
figure 2a shows an example of the manual in which we see that the texts directly state thattcp nopush depends on the value of sendfile .table i configurable software and the manuals studied.
software domain language words loc version used by httpd web server c .
.
lighttpd web server c .
nginx web server c .
.
redis in memory database c .
.
mysql sql database c c .
.
hdfs file system java .
.
mapreduce1distributed computing java .
.
hbase distributed nosql database java .
.
yarn resource management java .
.
tomcat application server java .
.
in contrast the source code also provides such information through some joint implication to the control flow between the two options.
for example in figure 2b the two options control two variables independently which would then be compared to determine if there is an exception hence a dependency.
however manually parsing analyzing dependencies from the software manuals and code is a labor intensive process regardless whether they are performance sensitive or not .
this is because the description in the manual can be lengthy and hard to be interpreted while the code of relevant options can involve complex call chains and code structure.
d. motivation why performance?
it is worth noting that even valid options might not be performance sensitive.
those nonperformance sensitive options overcomplicate tasks related to configuration performance e.g.
configuration tuning and debugging.
therefore a common way in those works would be to select either manually or automatically only the performance sensitive options to work on .
as a result it is crucial to specifically study performance sensitive options and understand how their dependencies can impact the mutation of those options in the tuning testing process which is the key motivation of this work.
why manual and code?
indeed much work relies on either manual or code individually.
those who use manuals claim that the manual contains a wide range of natural descriptions of the options and hence is more information rich .
conversely work that prefers code analysis states that the code is more structural and is less prone to outdated issues caused by rapid version updating .
despite the above it remains unclear which modalities are more useful particularly for identifying the performance sensitive options and their dependencies.
for example safetune is an automated tool that can be used for configuration performance analysis which only relies on manual texts while completely ignoring code information without giving clear justification.
this work seeks to bridge such a gap through an empirical study providing insights into the roles of different modalities in configuration performance analysis.
iii.
m ethodology a. software systems to collect data from widely used configurable systems we study those open sourced ones that also come with well1we use the h adoop implementation of m apreduce .table ii keywords for extracting performance sensitive options from the manual all are case insensitive .
terms blacklist file proxy forward path port address location updates version compatibility legacy address link restore plugin dir url host name precision descriptor principal key ui profile info catalogue password pwd whitelist debug optimize table cpu pct interleave block thread worker executor error time depth max logger range size min length timeout limit cache mode log documented manuals.
the list has been illustrated in table i. we can clearly see that the systems studied come from diverse domains with different languages and scales while being widely used by the community.
to ensure the validity of each system we take the latest stable version and analyze its code and the corresponding manual by the date of analysis i.e.
sep representing the most stable state of a system since the newest releases are likely to be error prone.
b. identifying performance sensitive options manual analysis two authors who are experienced software engineers independently analyzed the manual to identify performance sensitive options.
this follows a mix of blacklist and whitelist approaches.
the blacklist contains several keywords that if detected in the manual s description would cause the corresponding option to be eliminated immediately.
in contrast if a description contains any of the terms in the whitelist then the corresponding option is considered performance sensitive.
the procedure is as follows a screening as from table ii we conduct an initial screening by identifying the keywords for blacklist e.g.
names and paths and whitelist e.g.
resource and thread .
b blacklist exclusion we eliminate any options which have descriptions that contain the keywords in the blacklist.
the two authors exchanged their filtered list and any disagreement would be discussed or consulting the other author or external expert until a consensus can be made.
c whitelist inclusion in the remaining options the description of which contains any of the terms in the whitelist is said as performance sensitive.
extensive discussions were taken place to resolve disagreements.
d the two authors independently read the options collected and picked the performance sensitive ones as understood from the description.
a final list is shared and discussed until agreements are reached.
the same process is repeated for the manual of every system studied.
for example mapreduce.cshuffle.port is an option of m apreduce that was filtered out using the blacklist since it merely specifies the port for connecting the component.
conversely for m ysql the wait timeout is an option that is included under the whitelist since the waiting time would have a dramatic impact on the system s performance.
code analysis to understand the performance sensitive options from code we leverage a semi automated approachthat relies on taint analysis tools2.
the same two leading authors took the lead but all authors helped to resolve conflicts a operation categorization any operation function that belongs to the categories below are performance sensitive memory operations including array or stack allocations e.g.
the init mem in m ysql.
cache operations updates or synchronization e.g.
the ngx open cached file in n ginx .
i o operations e.g.
those accessing the persistent storage such as the bufferedreader for ja v a. multi threading process such as thread pooling e.g.
thenewcachedthreadpool in m ysql.
loop operations such as for while and do while loops.
network communication such as the http range header filter in h ttpd .
error control process code such as try catch assert my error .
those categories are discussed among all authors until an agreement has been reached.
b variable identification it is easy to understand what the configuration options are by reading the configuration file e.g.
yaml files.
yet since the configuration options in the code are codified in different ways depending on the systems we use different approaches below to establish the mappings between configuration options from the manual and the corresponding variables in the code unified analysis for m ysql l ighttpd nginx and r edis the variables for configuration options are centrally organized in a single file with identical names and hence the mappings can be found directly.
segmented analysis for h ttpd and t omcat the variables are written in separate source code files but they are still centrally involved in a main thread.
for those we analyze the control flow to find the relevant code files and the corresponding variables.
scattered analysis for hdfs m apreduce hbase and y arn the variables are distributed across the codebase and are only called on demand.
yet those variables are all accessible via the setter and getter function which can be easily allocated.
again the two lead authors conduct the analysis independently and exchange the results to reach a consensus.
c taint analysis to extract those performance sensitive options by analyzing the code we leverage taint analysis where we use variables identified serve as the sources and the performance sensitive operations are the sink.
for c c systems we use clang and for ja v a systems we use javaparser in two ways in the top down manner the taint analysis would procedure some taint flows that indicate which performance sensitive operations would interact with 2our taint analysis is technically similar to prior work but applied under different definitions of source and sink that fulfill our needs.the variables identified.
however it is unclear whether those variables can impact the behavior of the operation.
to confirm that we further extract the corresponding code fragments from the abstract syntax tree to investigate whether the path between the variables identified and performance sensitive operations involve logical operators e.g.
if else orwhile .
if it does we find a performance sensitive option variable.
we also follow a bottom up approach starting from each of the performance sensitive operations based on which we identify the related parameters.
we then manually traced back to the origin of those parameters to confirm their interactions with the variables.
if an interaction exists the corresponding variable is performance sensitive.
this helps to mitigate the false negative cases from the taint analysis.
system measurement ground truth to identify the ground truth we manually change each configuration option and examine whether such a change leads to significant performance deviation.
this is achieved by running and profiling each system under different benchmarks workloads setting according to the commonly used ones from the literature and practical applications3.
for example we use s ysbench and tpc for m ysql with different concurrent users test duration and table counts etc.
for each option we sample at least two possible values as the representatives for the numeric options we use the default maximum and minimum within the range documented.
if no range is provided we use the positive upper limit as the maximum value and as the minimum value.
for boolean options we simply flip their true andfalse .
for other enumerate options we compare all pairs of the possible values in the eligible range.
for all cases we compare the performance achieved by setting the two values of an option while keeping the other options with their default values.
we repeat the process five times to ensure the stability and reliability.
a numeric or boolean option is said to be indeed performance sensitive if under any workload the average performance of setting one value can have more than deviation compared with that of the setting the said option with the other value an enumerate option is performancesensitive if the average performance of any pair of its value has deviation greater than under any workload.
the above is a typical setting being acknowledged as a de facto standard in the field .
in total we run over 000performance tests resulting in more than 500cpu hours.
c. extracting performance sensitive dependencies next we extract the performance sensitive dependencies which involve at least one performance sensitive option.
dependencies by manual to do that by using manual two authors independently perform the following a the non performance sensitive options are filtered out.
3details main dataset perfsensitive groundtruth workloads.md b each author analyzed the description of all options remaining.
an option is said to have a dependency on the other if its description has mentioned the other option.
c in the end both lead authors combined the list of dependencies identified and discussed among all authors for cross checking on e.g.
the accuracy completeness and relationships between descriptions and related options.
d the above process was repeated by two rounds of rigorous review to mitigate bias.
the extracted dependencies represent the information that can be obtained by analyzing the manual.
those dependencies are then further verified by investigating if the two variables can indeed be involved in some logical operation in the relevant code.
however since the dependencies are initially extracted from the manual they are said to be manual related.
indeed as we will show manual analysis can reveal certain dependencies that are deeply hidden in purely code analysis.
dependencies by code to extract dependency information from the code we again focus on the performancesensitive options only.
we then apply taint analysis using the same aforementioned tools such that the variable of a performance sensitive option serves as the source while that of the other option is the sink.
within the results we check whether there is a call path that involves both variables.
if that is the case we investigate the code segment and examine if both variables are involved in some form of logical operation such as if else switch orwhile loop.
when such a logical relationship can be identified we say the code indicates that there exists a dependency between the variables options.
two authors independently execute the above process and the final results are discussed and agreed by all authors.
why not testing systems?
for dependencies in this work we regard the outcomes from both manual and code analysis above as the ground truth without system testing since the systems have mostly unclear responses when violating performance sensitive dependencies.
indeed chen et al.
have shown that for up to .
of the dependencies we checked that these include many performance sensitive ones there are no incomplete messages in the log from the system under their violations hence we cannot verify if there is a dependency even when testing on the actual systems.
why not extract all dependencies first?
we focus explicitly on the performance sensitive options identified and extract their dependencies as opposed to extracting all dependencies and then pruning dependencies not containing performance sensitive options for two reasons in our work extracting the dependencies only on the performance sensitive options under the scale of our study which constitutes merely of the options has already consumed several months of work per expert we have two lead authors merely for one round of analysis we have two rounds excluding the discussion time for resolving conflicts with all authors.
therefore extracting the dependencies for all options and further pruning them down would require significantly increased efforts in the first place since the number of initial dependencies pairs of options to be analyzed can increase considerably along the number of options considered.
further this would inevitably include many non performancesensitive dependencies which we are not interested.
since we are only interested in performance sensitive dependencies by which we mean those dependencies that involve at least one performance sensitive option which is known focusing on the performance sensitive options first and their dependencies would lead to similar results as to extracting all dependencies then prune them down.
this is because the known performance sensitive options are identical between the two processes.
iv.
e mpirical study results a. rq1 performance sensitive options operationalization in comparison to the ground truth obtained by actual system measurement we report on the true positive rate false positive rate and false negative rate of the results obtained by analyzing the manual and the code.
to study the discrepancy of correctly identifying performance sensitive options by using the manual and the code individually we use the following patterns observed manual only this refers to the true performancesensitive options against ground truth that are found by studying the manual only.
for example the description form manual for max execution time option specifies that it determines m ysql s execution waiting time for select statements.
however it does not involve any control flow related to performance sensitive operations in the code.
code only this refers to the true performance sensitive options against ground truth that are identified via analyzing the code but are not noticeable from the manual.
for example the server.defer accept is merely described in the manual as the listening socket for a tcp protocol tcp defer accept on or off .
yet in the code it is clear that server.defer accept directly controls the function setsockopt which affects various behaviors of l ighttpd e.g.
network traffic security errors and more.
to further understand why analyzing manual or code alone might lead to false positives we distinguish some patterns fp1 the configuration option is completely discarded.
however this might not be noticeable in the manual since it might not be mentioned or code due to complicated call graphs .
for example max length for sort data in m ysql is a typical example of fp1.
in the manual texts it clearly indicates that the option is performance sensitive but it has no impact on the system in the code.
tls session cache size from r edis change the default number of tls sessions cached which is invoked within the function tlsconfigure that has a clear impact on the system from the code.
however tlsconfigure is never triggered.table iii ground truth performance sensitive options identified by measuring the systems.
software all options true performance sensitive options httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat total fp2 the option has limited implication on the performance i.e.
the largest change is less than .
for example the limitinternalrecursion in h ttpd is identified as performance sensitive options from both the manual and the code analysis.
however in actual profiling it caused a maximal performance fluctuation of .
across the workloads.
fp3 the impact of the configuration option on performance is minimal.
that is to say there is indeed a performance fluctuation after option change but the maximal magnitude is less than .
for example the keepalive option in h ttpd was deemed to be performance sensitive from both manual and code but it merely has a maximum of .
variation in performance on all workloads.
similarly for false negative cases we draw on two patterns fn1 there is indeed a significant performance implication.
for example.
the optimizer prune level in mysql does not exhibit any control relationship with performance sensitive operations in code.
yet it significantly impacts the performance by in the profiling since its setting is passed to a third party algorithm.
fn2 the maximal performance implication of the options only marginally beyond e.g.
by less than and it is possibly due to randomness in the execution.
yet for the sake of completeness we alse classify these as performance sensitive options.
for example the mapreduce.job.am.node label expression in m apreduce determines whether range requests are permitted and it is clearly non performance sensitive from the texts in the manual.
however it has non trivial impacts on performance with a maximum of .
change across all workloads.
we also discuss the main causes of the above false results.
findings from table iii it is clear that the true performance sensitive options constitute a small proportion of all options merely in general across all the systems.
finding only a small set of options between and can non trivially impact the performance.
for identifying performance sensitive options from manual and code as shown in table iv we see that it is easier toidentify more options by manual than by code vs. which makes sense since the naturalness of manual could easily lead to more inclusion.
however analyzing the manual does not lead to more true positive samples than using code vs. meaning that the highly structural nature of code can be more beneficial.
taking the tcp nopush in nginx as an example this option enables or disables the use of the tcp nopush socket option on freebsdor of thetcp cork socket option.
it is difficult to obtain an accurate answer through manual texts but in the code this option is repeatedly invoked in recursive network operations which strongly implies its performance sensitivity.
on the other hand analyzing the manual can lead to higher false positives while analyzing code is more likely to be false negative prone.
however the benefit of identifying more performance sensitive options via manual is that it also allows us to find more ones that can only be detected therein i.e.
comparing the of manual only to the of code only.
this suggests the good side of the richer naturalness in the manual the more comprehensive summary of the interrelations between options can provide more understandable and intuitive information than analyzing code alone.
for example in the source code there is no performancesensitive flow for optimizer prune level but we can intuitively understand from the manual description that this option controls the third party heuristic algorithms applied during query optimization pruning less promising parts of the plan from the optimizer s search space in m ysql.
finding code is more helpful in finding the true performance sensitive options and with fewer false positives than manual.
yet the manual can still provide useful information for some that are difficult to identify in code and are less likely to be false negative prone.
as shown in table v clearly the patterns of false positive for extracting performance sensitive options from manual and code are of similar characteristics 3are due to the fact that the options have limited implications on the performance 3are due to noticeable but small implication a very limited proportion is related to discarded but have not been clearly marked options.
we found that the fundamental cause is also similar it is mainly due to both the manual and code giving misleading information about options performance sensitivity.
for example useasyncio in t omcat is a widely considered performance sensitive option since it enables asynchronous io while both the manual and code indicate such.
however upon the actual profiling it only has up to .
performance change over the workloads.
the patterns of false negatives when analyzing manual and code also differ marginally ignoring the options that can largely impact performance has only a little higher proportion for code than manual while those suspiciously performance sensitive options are merely a slightly more common pattern for manual than code .
yet we found that the predominated root causes can differ table iv identified performance sensitive options by manual and code against the ground truth along with the patterns tp fp and fn are the true positive rate false positive rate and false negative rate against the ground truth respectively .
softwaremanual code options tp fp fn manual only options tp fp fn code only httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat total table v patterns for false positives negatives on performance sensitive options found by analyzing manual and code.
softwaremanual code fp1 fp2 fp3 fn1 fn2 fp1 fp2 fp3 fn1 fn2 httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat total cause of false negatives in manual analysis the texts in the manual have rather vague information related to performance.
for instance from the l ighttpd manual the description of option server.range requests is defines whether range requests are allowed or not which is highly imprecise.
as such we did not include that as a performance sensitive option.
yet in the actual profiling it can considerably impact the performance.
cause of false negatives in code analysis the code involves non standard patterns of executions that affect the performance.
for example in m ysql max seeks for key is only called in the functionfind cost for ref .
within this function max seeks for key is compared with several similar options and the highest value is returned as the budget which does not involve our defined performance operations.
however m ysql uses the cost estimate provided by find cost for ref to choose the most efficient query execution plan.
finding the false positives for extracting performance sensitive options using manual and code are due to similar patterns and causes but the causes for false negative samples vary even though their patterns are similar.
b. rq2 performance sensitive dependencies operationalization we found that all the performancesensitive dependencies identified via manual or code areindeed present from which the unique dependencies for each system are reported hence there is no false positive.
however there are indeed false negative cases.
we observe the following discrepancy patterns when using manual or code manual only there exist performance sensitive dependencies that are only clearly observable in the manual but deeply hidden in the code.
for example in our taint analysis results there is no code segment where the taint flow between the options read buffer size and select into buffer size in m ysql.
however the manual provides detailed descriptions of the dependency relationship for them.
code only similarly certain performance sensitive dependencies have not been mentioned in the manual at all but clearly noticeable in the code.
for example in the source code of l ighttpd if the value of server.max connections is greater than half of the value for server.max fds then the system will log an error.
however such a dependency between the two said options is not mentioned at all in the the manual.
for a given dependency we found two possible patterns depending on the options type mixed options this means one of the options involved is not relevant to performance.
for example in mysql the option wait timeout is performancesensitive while the option interactive timeout is not but there is a dependency that the former s value should be smaller than that of the latter.
performance only options this refers totable vi performance sensitive dependencies identified.
c denotes the number of options involved in a dependency chain.
software all dependencies mixed options perf.
only options c httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat total the dependencies wherein both options are performance sensitive e.g.
the dependency between large client header buffers and connection pool size in n ginx .
in our analysis a dependency chain involving multiple options is simply a concatenation of different dependencies with two options providing that they have one option in common hence we also analyze the number of options involved in a dependency chain which is a direct reflection of the dependency of complexity for a system.
again we showcase how those complex dependency chains i.e.
the number of options involved greater than can be detected via manual and code analysis individually.
for example in r edis themaxmemory andmaxmemory policy options jointly determine when and which keys need to be deleted to free memory hence the latter depends on the former.
at the same time option lazyfree lazy eviction depends on the policy of maxmemory policy and it determines whether the deletion operation will block the processing of commands.
since some performance sensitive dependencies can only be identified in manual not code and vice versa we also study why certain dependencies have been missed.
findings table vi shows the general results and patterns for all the systems.
we see that the performancesensitive options can indeed involve dependencies.
more than half of those are only be relevant to performance sensitive options while the remaining half can involve non performancesensitive ones.
in particular there is a non trivial proportion of the chain that covers three or more options meaning that the complexity of dependency can be considerably high.
finding among the performance sensitive dependencies over half of these involve mixed options while there exist a non trivial proportion of the chain that involves more than options.
table vii summarizes the statistics of dependencies that can be identified from manual and code.
as can be seen the code identifies dependencies against the from the manual since no false positives are involved and there are dependencies in total this means that the information from the code only misses ones while that of the manual can cause misses.
further it is much more likely to find dependencies from code that cannot be found in the manual than theopposed way .
the key reason is that for systems like lighttpd there is almost no information about dependencies in the manual.
we believe that when the dependencies do not cause serious issues e.g.
crashes the developers tend to omit their descriptions from the manual.
the same can be reflected in identifying dependencies over the patterns of option types.
what we found is that for mixed options the manual is more likely to miss out on their dependencies.
for example in l ighttpd server.errorlog use syslog is performance sensitive while server.syslog facility is not and the manual has no information of their dependency.
yet they clearly have an enabling dependency in code.
code also helps to identify most complicated dependency chains while manual can only provide clear information on of those.
for example in h ttpd the keepalive option has a dependency on the timeout option which influences the data flow of the keepalivetimeout option.
this dependency chain with three options is not found in the manual but is clearly visible in the code.
finding code often offers more useful information than manual in identifying performance sensitive dependencies across the patterns and dependency length.
since we found no false positive we focus on the reason that causes false negatives using manual and code individually cause of false negatives in manual analysis we found that the lack of manual information has led to the false negative cases.
developers tend to omit option dependencies that do not have direct relationships or serious consequences when maintaining the manual to avoid causing too much trouble for the users.
cause of false negative in code analysis for the edge cases where the code analysis has resulted in false negatives the key reason is due to the depth limitations of taint analysis which prevent locating all dependencies within the code.
when configuration options are called through multiple recursions of function calls the taint analysis often fails to trace the code hence leading to most of the false negatives.
finding although code is generally more useful in identifying performance sensitive dependencies the manual can still help to resolve some edge cases.
c. rq3 usefulness of current automated tools operationalization we now seek to examine the effectiveness of existing automated tools for configuration performance analysis.
we choose state of the art tools which relies on the information modality of manual or code independently.
tools for performance sensitive options inference are safetune as a representative that relies on manual safetune identifies performance sensitive options by analyzing configuration documentation.table vii patterns between performance sensitive dependencies identified via manual and code.
same formats as table vi.
softwaremanual code dependencies manual only mixed options perf.
only options c dependencies code only mixed options perf.
only options c httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat total diagconfig diagconfig trains a random forest to predict whether the given configuration option and its associated code segments are performance sensitive.
as such it uses code as the main information modality.
since we did not find readily available tools that can directly extract performance sensitive dependencies we use gptuner andcdep which are designed to extract any dependencies gptuner gptuner leverages the understanding capabilities of large language model llm to infer dependencies by reading the texts from the manual .
cdep incdep the dependencies are predicted by using rule mining on code based on predefined patterns.
within the dependencies identified by those tools we then further extract the performance sensitive ones according to our ground truth of performance sensitive options i.e.
at least one option in the pair of options is performance sensitive .
in the evaluation we only consider those extracted samples against the performance sensitive dependencies derived from the process in section iii c. this mitigates the unfairness caused by the fact that those tools cover any dependency types.
since all tools are cross project we use all the manual code that exists in a target project as the testing data and pretrain them with the original dataset from their authors if possible.
notably none of our studied systems were studied in diagconfig hence we used all of their original datasets in training there are overlapping systems for safetune and hence we removed those systems and only used data from the remaining ones systems in their original datasets to train the model.
these prevent data leakage.
gptuner is a special one since it relies on llm gpt3.
as such we have no idea what data it has been using to pre train the llm.
cdep is a rule based tool that was designed using general understandings from the systems we studied and hence it is not subject to the data leakage issue in general machine learning based tools.
we use recall precision and f1 score as the metrics as they are robust to data imbalance .
note that diagconfig and cdep only work for ja v a based systems.
for all cases we follow the general rule of thumb that an f1 score of .
or higher means a practically useful configuration predictor .
we repeat runs of experiments for all stochastic tools.
findings for predicting performance sensitive options as from figures 3a and 3b we see that manual based tools likesafetune would likely have excellent recall but poor precision this matches with the human analysis of manual inrq1 where the false positive is more severe than false00.
.
.
.81httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat metric valuef1 score precision recall a safetune00.
.
.
.81hdfs mapreduce hbase yarn tomcat metric valuef1 score precision recall b diagconfig00.
.
.
.81httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat metric valuef1 score precision recall c gptuner00.
.
.
.81hdfs mapreduce hbase yarn tomcat metric valuef1 score precision recall d cdep fig.
effectiveness mean deviation of tools that predict performance sensitive options a and b and configuration dependencies c and d over runs.
safetune andgptuner are manual based diagconfig andcdep are code based.
.
.
.
.81tomcatyarnhbasemapreducehdfsmysqlredisnginxlighttpdhttpd metric valuef1 score precision recall a owith manual00.
.
.
.81httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat metric valuef1 score precision recall b owith code00.
.
.
.81httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat metric valuef1 score precision recall c dwith manual00.
.
.
.81httpd lighttpd nginx redis mysql hdfs mapreduce hbase yarn tomcat metric valuef1 score precision recall d dwith code fig.
human analysis for performance sensitive options i.e.
o a and b and their dependencies i.e.
d c and d .
negatives.
as for code based tools like diagconfig the precision is slightly better than the recall.
this deviates from the human results form rq1 because its data driven nature could parse code information that cannot be captured by human analysts.
however in both cases we see that the f1 score is still far away from the threshold of .
suggesting that those tools using either manual or code are still far from being practically useful in predicting performance sensitive options.
figures 3c and 3d illustrate the results for the tools that predict configuration dependencies using different modalities.
as can be seen there is generally poor precision but good recall regardless of the modality they rely on.
this means that false positives are a more serious issue therein which again deviates from human analysis.
as a result the final f1 score can hardly reach .
on any system.
one possible explanation is that those tools have not been explicitly tailored to cater to the characteristics of performance sensitivedependencies.
for example cdep is mainly designed based upon the understanding of some manually derived patterns of dependencies which are relevant to any dependencies type in general.
yet some of those patterns might not apply to most performance sensitive dependencies albeit they can be common in other dependency types.
for instance we found that the default value pattern which represents the dependency wherein the value of an option serves as the default of the other does not apply to any performance sensitive dependencies.
often this is relevant to path related options e.g.
the value of dfs.namenode.name.dir is the default fordfs.namenode.edits.dir in hdfs.
the above can eventually amplify their issue of a high number of false positives for identifying performance sensitive dependencies when correctly finding the dependencies of other types are no longer of concern leading to poor precision and negatively impacting their overall performance.
finding existing tools for predicting performancesensitive options and their dependencies tend to be considerably affected by false positives in general thus can hardly lead to practically useful results.
we also directly compare the results from tools with those of human analysis from rq1 and rq2 .
for identifying performance sensitive options in figure 3a and 4a we see that the f1 score of using tool safetune is comparable to that of human analysis when analyzing manual the latter merely slightly better since understanding natural language also shares similar difficulties as to machine interpretation.
however when comparing the results by analyzing code we note that human analysis is much superior to the tool i.e.
diagconfig figures 3b and 4b .
this is because the code is of complex interrelationships and the learned model in diagconfig has failed to retain the domain knowledge used in human analysis.
yet in all cases neither human analysis nor automated tools have reached the .
f1 score threshold.
for dependencies extraction clearly the human analysis figure 4c and 4d is significantly better than gptuner and cdep figures 3c and 3d for the ja v a systems with either modality.
notably the results of human analysis are mostly beyond the f1 score of .
for all systems.
finding existing tools are generally far from reaching the level of human analysis except for identifying performance sensitive options from manual.
v. a ctionable insights finding clearly suggests that there is only a relatively small proportion of the configuration options that can nontrivially impact the performance.
this means that insight it can be highly beneficial to extract performance sensitive options within or before tasks such as configuration testing and configuration tuning.finding andfinding imply that to better identify the performance sensitive options it is necessary to jointly investigate the information from both manual and code.
therefore insight when identifying performance sensitive options by hand or with tools it is necessary to combine the information from both modalities of manual and code.
yet the patterns causes of their false positives are similar but the causes for false negatives can differ insight when fusing the modality of code and manual for extracting performance sensitive options one should not believe in the information from the manual and code as it is possibly misleading.
in particular special attention needs to be paid to vague information for manual and code that does not follow standard performance relevant operations.
the above insight challenges the existing studies on either manual or code in which the proposed approach fully trusts the information from the modality implying the need for some potential confidence driven approach or some reinforced methods in automated configuration performance learning even when combining both manual and code .
the above also implies that key information might be missing in both modalities therefore configuration should be better catered to at the software design level.
this might include e.g.
standardizing the format between manual comment and code clearly annotating the property of configuration options designs or at the more fundamental level ensuring performance regression test is conducted and the results are declared when adding any new configuration options.
finding suggest that when finding performance sensitive dependencies of the cases involve mixed options.
this challenges the belief that dependencies used in e.g.
configuration tuning are mainly considered for performance sensitive options only .
that said insight performance sensitive dependencies identification should not be specific to performance sensitive options as mixed ones are highly possible.
further finding andfinding suggest that insight although it could be acceptable to only rely on the information from the code in identifying performance sensitive dependencies in general additionally incorporating manual information can still be useful for finding some edge cases.
for manual analysis the information regarding dependencies might be missing while for the source code the depth of nested invocations recursions is critical.
finally finding andfinding examine how far are we at automatic performance sensitive option identification and their dependencies extraction the results imply that insight relying on existing automated tools that leverage either manual or code individually is problematic and human analysis remains generally far more reliable.
hence it is promising to investigate interactive tools.
intuitively there will be a trade off between quality more human analysis and cost more tool supports which is highly case dependent.
our view is that the inferior accuracy of the tool will have an impact on the downstream tasks benefiting from the configuration performance analysis.
for example if we seek to identify the performance sensitive options and their dependencies for configuration tuning then more inaccuracies can lead to unnecessary testing of some options and frequently violated dependencies.
since the configuration tuning is expensive too in this case the extra overhead caused by the worse accuracies of a tool might exceed its relative cost saving.
for interactive tools one possible solution is to incorporate output explainability e.g.
lime or shap into the tool fused with manual and code based on which humans can then provide their inputs in various forms e.g.
a yes no answer some categories of confidence or even weights.
vi.
t hreats to validity the human analysis using manual and code might pose threats to construct validity.
to mitigate that we have followed a systematic protocol involving all authors with multiple rounds of reviews.
yet like any empirical study human errors oversights are always possible.
when testing the systems forrq1 a possible threat can come from the fact that the option is changed one at a time the standard way that has been followed in many prior works .
indeed due to the finite resources and a large number of options systems to test in our study fully covering the option interactions is infeasible.
there are also other forms of configuration beyond this study e.g.
alternative third party libraries and versions the understanding of which is promising for future research.
threats to internal validity might be related to the parameter setting of the automated tools and the methodology e.g.
the threshold for determining performance sensitivity.
in this work we set the same parameter values and procedures e.g.
the training data for diaconfig as used in their prior work.
while these serve as a convenient default setting we cannot guarantee that they are the best status.
to minimize the threats to external validity we ensure that our study covers a wide range of systems with diverse manuals and complexity of code.
we have also considered tools that leverage different modalities as well as using various workloads when testing the systems.
however studying more systems workloads or tools might be more fruitful.
vii.
r elated work performance sensitive options identification.
to identify performance sensitive options a common approach has been treating the configurable system as a black box and hence leveraging data driven methods in the prediction.
for example some studies have been using empirical observations e.g .
certain patterns of how the performance sensitive options influence the performance obtained via qualitative analysis .
recent work has followed a more whitebox approach where certain artifacts and modalities about configurable systems have been exploited.
among others he et al.
propose safetune a multi intent aware semisupervised method that analyzes the manuals aiming to infer the performance sensitive options.
diagconfig is an approach that also leverages machine learning random forest .
however it exploits the code invocation chains as part of the features for predicting performance sensitive options.
dependency extraction.
existing work on dependency extraction has relied on the systematic analysis of some modalities related to configurable systems.
gptuner is an approach that is based on a large language model to parse the manuals for extracting information about dependencies between options.
it proposes a prompt integration algorithm to unify the structured view of the refined knowledge.
apart from manual the other modality that is commonly used is the source code.
zhou et al.
introduce a new tool for multiconfiguration error diagnosis by analyzing the dependencies between options in the source code.
similarly cdep detects dependencies between configuration parameters manifested in the code through pattern matching and taint analysis.
performance bug detection.
there are approaches that rely on either manual or code to detect performance bugs.
among others pracextractor is a tool that uses natural language processing techniques to automatically extract best practice recommendations from manuals which serve as an oracle in performance bug detection.
ecstatic and taintmini are tools that use taint analysis to parse the flow graph of the code which detects the potential performance bugs.
yet these approaches have not covered configuration related performance.
related empirical studies.
empirical studies have been conducted on different aspects of configurable systems .
xu et al.
demonstrate the prevalence of configurability.
zhang et al.
investigate how configurations evolve among system versions.
however there has been no empirical study that compares configuration performance analysis using different modalities i.e.
manual or code.
viii.
c onclusion in this paper we conduct an extensive empirical study that assesses the usefulness of two modalities i.e.
manual and code for identifying performance sensitive options and extracting their dependencies.
through analyzing systems with options words in the manual and lines of code we reveal several insights that can impact the community of configuration performance analysis.
our observations pave the way for vast future research directions including but not limited to multiple modal configuration performance analysis and interactive tools thereof.
acknowledgement this work was supported by a nsfc grant and a ukri grant .