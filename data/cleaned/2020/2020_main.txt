hal id hal submitted on aug hal is a multi disciplinary open access archive for the deposit and dissemination of scientific research documents whether they are published or not.
the documents may come from teaching and research institutions in f rance or abroad or from public or private research centers.l archive ouverte pluridisciplinaire hal est destin e au d p t et la diffusion de documents scientifiques de niveau recherche publi s ou non manant des tablissements d enseignement et de recherche fran ais ou trangers des laboratoires publics ou priv s. distributed under a creative commons attribution .
international license hyperdiff computing source code diffs at scale quentin le dilavrec djamel eddine khelladi arnaud blouin jean marc j z quel t o cite this version quentin le dilavrec djamel eddine khelladi arnaud blouin jean marc j z quel.
hyperdiff computing source code diffs at scale.
esec fse 31st acm joint european software engineering conference and symposium on the f oundations of software engineering dec san f rancisco ca usa united states.
pp.
.
.
.
hal hyperdiff computing source code diffs at scale quentin le dilavrec univ rennes irisa inria france firstname.last name irisa.frdjamel eddine khelladi cnrs univ rennes irisa inria france first name.lastname irisa.frarnaud blouin insa univ rennes irisa inria france firstname.lastname irisa.frjean marc j z quel univ rennes irisa inria france firstname.lastname irisa.fr abstract with the advent of fast software evolution and multistage releases temporal code analysis is becoming useful for various purposes.
temporal code analyses can consist in analyzing multiple abstract syntax trees asts extracted from code evolutions e.g.
one ast for each commit or release.
core feature to temporal analysis is code differencing the computation of the so called diff or edit script between two given versions of the code.
however jointly analyzing and computing the difference on thousands versions of code faces scalability issues.
mainly because of the cost of parsing the original and evolved code in two source and target asts wasting resources by not reusing intermediate computation results that can be shared between versions.
this paper details a novel approach based on time oriented data structures that makes code differencing scale up to large software codebases.
in particular we leverage on the hyperast a novel representation of code histories to propose an incremental and memory efficient approach by lazifying the well known gumtree diffing algorithms a mainstream code differencing algorithm and tool.
we evaluated our approach on a curated list of large software projects and compared it to gumtree.
our approach outperforms it in scalability both in time and memory.
we observed an order of magnitude difference in cpu time from .2to .7for the total time of diff computation and up to 226in intermediate phases of the diff computation and in memory footprint of .5per ast node.
the approach produced .
of identical diffs with respect to gumtree.
ccs concepts software and its engineering software version control maintaining software .
keywords diff edit script code history mining temporal code analysis acm reference format quentin le dilavrec djamel eddine khelladi arnaud blouin and jeanmarc j z quel.
.
hyperdiff computing source code diffs at scale.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
esec fse december san francisco ca usa association for computing machinery.
this is the author s version of the work.
it is posted here for your personal use.
not for redistribution.
the definitive version of record was published in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa introduction code histories are increasingly used during software development as an important source of information for various software engineering tasks such as analyzing the origin of issues and bugs learning from code to build recommendation and bug prediction systems recovery of traceability links refactorings duplicate code bad smells and their origins mining of fixes for program repair or co evolution mining .
these kinds of analyses are examples of temporal code analyses as they rely on code history e.g.
git and their underlying data i.e.
commits tags etc.
.
temporal code analyses also consist in analyzing multiple abstract syntax trees asts extracted from code evolutions e.g.
one ast for each commit or release.
a core feature to temporal analysis is code differencing i.e.
the computation of the so called diff oredit script1between two given versions of the code from two commits or tags which is more complex than to diff two files that does not face scalability issue.
especially that identifying those files is the results of the diff itself and not known a priori in the commits.
however jointly analyzing and computing the difference on thousands versions of code faces scalability issues.
mainly because of the cost of parsing the original and evolved code to produce two asts and hence so on thousands of commits wasting resources by not reusing intermediate computation results that could be shared among versions unsuitable memory layout of compared trees by allocating nodes in the global heap i.e.
not contiguous indexed by generic pointers etc.
.
many existing works proposed to compute diffs but with scalability issues within a single evolution.
falleri et al.
later addressed the scalability issue within a single evolution by reducing the algorithmic complexity to o n2 compared to stat of the art techniques.
however they still suffer from a long computation time and hence do not scale at commit level on very large software projects and on code histories.
to the best of our knowledge we are the first to propose an approach that addresses the scalability issue of computing diffs on large code histories.
in this paper we propose a novel code differencing approach that enables the production of diffs edit scripts at scale on large software codebases.
we combine concepts of gumtree a mainstream code differencing algorithm and hyperast a novel representation of code histories to propose an incremental and memory efficient approach.
in particular rather that having an ast for each version to analyze the hyperast leverages code redundancy in space and time using a direct acyclic graph dag to provide a single temporal ast containing all versions at once.
on top of the hyperast we take on the challenge of proposing novel ast matching algorithms inspired by gumtree on this dag representation 1in the rest of the paper we only use the term diff.
1esec fse december san francisco ca usa q. le dilavrec d. e. khelladi a. blouin and j m. j z quel instead of the classical and inefficient analyses of a pair of full asts.
essentially we make the original greedy gumtree algorithms lazy.
our approach pre computes metadata and lazily decompresses the dag to decorrelates the cost of diffing from the size of the code base.
thus enabling code differencing at scale.
we evaluated our approach on a curated list of large software projects.
compared to gumtree as a baseline the proposed approach outperforms it in scalability both in time and memory.
we observed an order of magnitude difference in cpu time from .2to .7for the total time of diff computation from to 226for the top down and bottom up phases total time and from .2to 233for the top down phase total time.
we also observed an order of magnitude difference in memory footprint of .5per ast node.
finally we gain all the time while having a .
of identical diffs with respect to gumtree and .
of identical mappings and actions in the remaining .
diffs.
finally we also outperform gumtree spoon when including the parsing cost.
we are faster by .
times in median and when excluding extreme cases of gains we are faster on average by .
times.
this paper follows the engineering research method as we propose a novel scalable code differencing approach and evaluated through case studies supplemented with a replication package.
this paper makes the following contributions a novel approach for diffing commits that scales for the analyses of large code histories.
an evaluation of the proposal composed of benchmarking studies that demonstrates the ability of the approach to scale compared to a state of the art code differencing approach.
open science all the code of the approach and the artifacts of the evaluation are freely available as a replication package.
the rest of the paper is structured as follows.
section introduces background concepts for understanding the proposed approach.
section presents the novel approach.
section details the evaluation and threats to validity.
section discusses related work.
section concludes the paper and outlines future work.
background this section introduces the two main concepts we rely on computation of diffs and code history representation.
.
computing diffs as software evolves quickly one way to study its evolution is through computing source code diffs.
a diff is usually computed using a source and a target ast.
it is composed of actions i.e.
changes that represent the applied changes from an original version to an evolved version of the code.
actions are either atomic or composed.
an atomic action can be either an insert add delete remove or update of ast nodes.
a composed action is an action composed of other actions.
for example a move action is composed of a delete and an insert possibly also an update which moves a given node deletion to another place in the ast insertion .
a mainstream and efficient source code differencing approach is gumtree that uses two dedicated algorithms to map original and evolved asts as depicted by the left part of figure .
gumtree then produces diffs using the chawathe algorithm with the obtained mappings as input.
the first mapping phase is thetop down matching .
it is a top down traversal breadth first that consists in matching subtrees in rounds level by level starting from the root .
each round unmatched subtrees are opened i.e.
the root of the subtree is skipped and its children are made available to be matched on the next round.
the second mapping phase is thebottom up matching a bottom up traversal post order that matches previously unmatched tree nodes using an optimal matching algorithm and the previously matched subtrees.
in post order unmatched nodes are compared to candidates i.e.
other nodes that share mapped descendants in the other version.
then the most similar candidate is selected to apply an optimal matching algorithm such as rted or zs .
gumtree uses zs by default.
note that gumtree uses and computes the following metadata theheight of a subtree in number of nodes thesizeof a subtree in number of nodes or characters thestructural hash the hash of a subtree that depends on the type and order of nodes ignore labels thelabel hash the hash of a subtree that depends on the type label and order of nodes.
while the original gumtree focuses on diffing files we diff at commit level which is more complex as it has to scale.
gumtreespoon further integrates the spoon parser into gumtree in order to diff commits.
diffing commits is useful to analyze the applied changes so at scale can open new possibilities to track changes to a code block an attribute or a method on thousands of commits.
cregit improves further the tracking git blame but works at the token level and not at the ast level.
however such scalable tracking would be relevant for example to fully study the origin of fixed security issues in very large code base .
other scenarios can be about identification of distant co evolutions or step by step introduction of security breaches.
.
efficient representations of histories code is usually analyzed as a tree and augmented with additional edges such as referential relations.
however studying it on multiple versions may face scalability issue.
two approaches handling scalability representation of code histories emerged in the literature .
with lisa alexandru et al.
represent code as a graph where nodes of consecutive versions can be merged with a decimation algorithm.
with the hyperast le dilavrec et al.
represent code as a direct acyclic graph dag where subtrees are deduplicated.
it takes inspiration for the merkle dag of git but does not limit itself to files as leafs of the dag.
for each subtree thehyperast allows a simple and efficient reuse of computation that only depends on code in the subtree in contrast to lisa.
contribution considering a code history represented using the hyperast we introduce a lazy code differencing approach inspired by the gumtree approach with a faster and more efficient mapping of pairs of trees that characterize commits .
the contribution addresses the scalability issues as the following the use of the hyperast data structure overcomes the wasting of resources by not reusing intermediate computation results that could be shared among versions 2hyperdiff computing source code diffs at scale esec fse december san francisco ca usa figure gumtree pipeline left our pipeline with content of hyperast right the proposed lazy algorithm fixes the unadapted memory layout of compared trees by allocating nodes in the global heap combining the hyperast with the lazy diff algorithm reduces unneeded memory accesses to the asts during the diff algorithm.
the scaling capabilities of our approach benefit from the same hypothesis as the hyperast given a large code base the amount of changes i.e.
new subtrees brought by each commit is usually tiny compared to the size of the code base.
if both hyperast and gumtree handle subtrees hyperast de duplicates identical subtrees i.e.
stores unique subtrees only .
thus providing potential benefits if combined with specific algorithms for matching identical subtrees.
compared to the gumtree approach the one we propose leverages the structure of the hyperast to significantly reduce memory accesses and cache misses while speeding up the diff.
figure shows the main differences between the gumtree approach and ours the gumtree approach starts by parsing a pair of code files then process resulting trees to compute metadata and finally produce diffs.
it follows a top down and bottom up phases to compute the mappings between the original and evolved versions of the asts.
after that it computes the diff.
instead our approach relies on the hyperast to efficiently process a code history i.e.
a git repository.
for each version i.e.
commit our approach incrementally parses and computes metadata to persist.
in addition to persisting metadata the hyperast also precomputes the structural equality using a reference equality since identical subtree are de duplicated.
then pairs of trees from the hyperast are lazily decompressed and mapped also in the same two phases top down and bottom up and finally used to produce diffs.
thus the lazy decompression allows us to only focus on the changed parts of the code for diffing.
in this section we first detail how the approach leverages hyperast to provide structured code with metadata and then howthose structured data fit the requirements of each matching algorithm we propose.
we then present the two specific matching algorithm namely the lazy top down matching and the lazy bottomup matching.
our contribution herein is to adapt the original greedy algorithms of gumtree for the decompressed trees leveraging on the lazy decompression.
while optimizing the performances our approach produces the same results as the original algorithms.
.
gumtree to hyperast metadata to efficiently compare code elements gumtree uses several precomputed metadata see section .
as our approach leverages on thehyperast that needs to compute and expose these metadata.
the size and a hash similar to the label hash were already present in the hyperast .
thus we only extended the construction of the hyperast i.e.
parsing commits to compute the label and structural hashes.
in addition to these metadata gumtree needs to test whether two subtrees are identical i.e.
anisomorphic function.
however due to the dag nature of the hyperast identical subtrees are de duplicated.
thus we know that referentially identical subtrees are isomorphic without having to recursively compare their content as gumtree does in its implementation.
.
obtaining a tree from the hyperast we now present the compressed tree and its lazy decompression.
.
.
decompressed tree.
the hyperast is a dag where subtrees are unaware of their parents as such algorithms requiring global information on nodes i.e.
subtrees need additional structures.
global information refers to any information on parents of a node.
it can be the global position of a node its path declaring class file or offset in characters.
in the remainder of this paper as opposed to the compressed tree the dag in hyperast we call such a structure that holds global information on nodes a decompressed 3esec fse december san francisco ca usa q. le dilavrec d. e. khelladi a. blouin and j m. j z quel tree.
the process of extracting a decompressed tree from the hyperast is named a decompression .
to exploit spacial locality a decompressed tree is represented by a contiguous array using a post order layout.
actually the bottom up step mainly traverses trees in post order process subtrees descendants and other postorder properties such as contiguous descendants key roots and leftmost tree descendants.
it has almost no downsides other than having ao n access time to access the n th children precomputing leftmost tree descendants is mandatory to obtain this complexity .
a decompressed tree has the following structure inspired by the zs algorithm see section ids an array of ids that indexes subtrees in the hyperast llds an array of integers that indexes leftmost tree descendants parents an array of integers that indexes parents the decompressed tree is column oriented i.e.
is a struct of arrays2 while nodes are indexed by their position in post order.
in addition considering that a decompressed tree is contiguous we are able to replace the uses of hash sets by bit sets3.
indeed the original gumtree algorithm uses existential quantifier in various places and implemented by hash sets.
the downside of this decompressed tree would lie in the upfront cost of decompressing two entire versions before being able to compare them.
nonetheless it is countered by the fact that it is lazily decompressed.
.
.
lazy decompression.
the upfront decompression of the hyperast is actually not mandatory.
reducing and deferring the decompression effectively makes the decompression lazy.
indeed considering the original gumtree matching algorithms and depending on the amount of changes and where they are located entire subtrees might remain unchanged and will be matched early using metadata and referential equality without ever needing to access their descendants.
in those cases there is no need to decompress these subtrees.
in figure a subtree with compressed descendants is materialized by dotted cells fat red arrows means all descendants are matched uniformly .
to control the decompression process while computing the diff we provide three different methods to decompress a tree t decompress children t t decompresses in tthe children of the node located at position t. decompress to t t decompresses in tthe node located at positiont.
it also decompresses all its parents with the method decompress children .
decompress descendants t t decompresses in tthe descendants located at position t. it offers optimization opportunities when considering the layout of the decompressed tree e.g.
post order .
indeed for the post order layout it is possible to reduce the number of accesses to the hyperast with a stack that allows to defer the insertion of children when decompressing a node.
thus there is no need to access the size metadata in the hyperast of those children before actually decompressing them.
2structs of arrays soa reduce memory wasted by padding while helping with cache misses when only a subset of fields is needed.
3using a bit sets to implement a set is more memory efficient than a hash set a single bit per element considering modern virtual memory where zeroed pages are not physically allocated.each decompression method is incremental.
thus making the overall decompression incremental.
to check whether a node is decompressed we check if its parent is i.e.
its initial value .
this property always holds except for the edge case where t 1i.e.
the tree is a single node.
these three methods are used as follows in the proposed matching algorithms methodsdecompress children anddecompress descendants replace every call to the original non decompressing children accessor.
the method decompress descendants is specifically used when most or all descendants need to be decompressed.
the method decompress toshould be used when a specific node needs to be decompressed.
.
lazyfied top down mapping phase algorithm 14summarizes the top down phase that matches the largest isomorphic subtrees between the source t1and targett2.
the underlined expressions in algorithm represents our optimizations for lazifying the gumtree top down phase.
the following constructs are required to understand algorithm root t is the root node of t. s t returns the list of descendants in post order i.e.
from lld t tot see section .
.
bitset n is an array of bits of size n. plist t creates a height indexed priority tree list lcontaining the subtree t. peekmax l returns the greatest height of the list.
pop l takes the list of greatest height subtrees from l. open l t inserts all the children of tintol.
algorithm presents this function.
compared to gumtree we changed the access to children into a decompression of said children usingdecompress children .
then instead of accessing a precomputed height directly on the node as gumtree does we need to access the subtree corresponding to hyperast first recovering the identifier with original see section .
to the hyperast then accessing the height metadata with height see section sim t1 t2 computes a similarity distance between t1andt2.
it ranges from to where a value of indicates that the descendants of t1are the same as those of t2.
mm represents a structure containing multi mappings i.e.
mappings where nodes can be part of multiple mappings as opposed to mthat only contains distinct mappings.
allsrcs mm returns all mapped sources in mm .
srcs mm t2 returns all source nodes mapped to t2in mm .
dsts mm t1 returns all destination nodes mapped to t1in mm .
link mm t1 t2 mapst1andt2inmm .
algorithm follows three steps.
the first step lines calculates the multi mappings mm between the largest isomorphic subtrees.
it maps isomorphic subtrees lines while iteratively opening unmapped subtrees.
more specifically when the 4note that we use in algorithms to the same hyperparameters as gumtree namely minheight maxsize andmindice .
.
4hyperdiff computing source code diffs at scale esec fse december san francisco ca usa algorithm lazy subtree matching data a source tree t1and a destination tree t2 a multi mapmm an empty listaof candidate mappings and an empty set of mappings m the minimum height for a matched subtree minheight result the set of mappings m 1l1 plist root t1 2l2 plist root t2 3whilemin peekmax l1 peekmax l2 minheight do ifpeekmax l1 peekmax l2 then ifpeekmax l1 peekmax l2 then foreacht pop l1 doopen t l1 else foreacht pop l2 doopen t l2 else 10h1 pop l1 h2 pop l2 11b1 bitset h1 12b2 bitset h2 foreach i1 i2 .. h1 .. h2 do ifisomorphic h1 h2 then link mm h1 h2 b1 b2 foreachi1 .. h1 if b1 doopen h1 l1 foreachi2 .. h2 if b2 doopen h2 l2 20ignored bitset t1 21foreacht1 allsrcs mm do 22uniq if dsts mm t1 1then 24t2 dsts mm t1 if srcs mm t2 1then uniq add all pairs of isomorphic nodes of s t1 and s t2 tom ifignored uniq then continue foreacht1 srcs mm dsts mm t1 do 30ignored foreacht2 dsts mm t1 do add a t1 t2 33sort t1 t2 a usingsim t1 t2 m 34ignored src bitset t1 35ignored dst bitset t2 36foreach t1 t2 a do if ignored src ignored dst then add all pairs of isomorphic nodes of s t1 ands t2 tom 39ignored src ignored dst foreacht s t1 src foreacht s t2 dst algorithm open a subtree in priority list data a subtreet t tbeing layouted in post order and a height indexed priority list lcontaining subtrees of t result the range of descendants in post order 1foreacht decompress children t do 2h height original t ifh minheight then 4level maxheight h 5l t heights are not equals for the subtrees lines or when they are not mapped in line with bi lines .
the second step of algorithm lines moves multimappings stored in mm to a list of mappings a line while directly moving mono mappings i.e.
mappings between exactly nodes tom line .
it mainly serves as a preprocessing phase before sorting and filtering multi mappings.
it also removes monomappings and hence reduces the number of mapping to sort and filter.
it uses a bit set to mark nodes that are already mapped line .
then algorithm sorts the list mappings in ausing a similarity distance line such as the dicedistance used in gumtree .
the last step lines extracts mappings from atom line while filtering overlapping mappings with an already accepted one i.e.
sharing nodes.
nodes accepted in mare marked byignored srcandignored dst line that forbids them from being accepted later line .
.
lazyfied bottom up mapping phase the bottom up phase as shown in algorithm complements the top down phase leveraging the previously mapped subtrees in m to further map the remaining nodes.
the underlined expressions represent our optimizations for lazifying the gumtree bottom up phase.
using the subtrees mapped in previous phase algorithm is able to map slightly different nodes i.e.
not isomorphic nodes .
the matcher first compares the number of shared descendants to then match subtrees smaller than minheight leveraging existing optimal mapping algorithms .
with the bottom up phase in post order we aim to map remaining unmapped source nodes line to destination nodes.
we only decompress the unmapped source nodes line before skipping the nodes with no matched children line .
then the auxiliary candidate function is used to find a candidate destination node t2 line most similar to t1.
using the dice distance compared to mindice gumtree parameter if t1andt2are similar enough line they are matched in m line .
ift1andt2have a small number of descendants compared to maxsize gumtree parameter their descendants are then decompressed and provided to opt lines .optis an optimal matching algorithm such as zs that matches nodes ofu1andu2while minimizing the edit distance.
finally only mappings with unmatched nodes and same types are kept and added inm.
to generate the diff we use the same algorithm of chawathe et al.
without lazifying it in this paper.
5esec fse december san francisco ca usa q. le dilavrec d. e. khelladi a. blouin and j m. j z quel algorithm lazy bottom up matching data a source tree t1and a destination tree t2 and an empty set of mappings m a threshold mindice and a maximum tree size maxsize result the set of mappings m 1foreachs1 t1 s1is not matched in post order do 2t1 decompress to t1 s1 ift1has no matched children then continue 4t2 candidate t1 m ift2 null dice t1 t2 m mindice then 6m m t1 t2 if s t1 maxsize s t2 maxsize then u1 decompress descendants t1 u2 decompress descendants t2 10r opt u1 u2 foreach ta tb r do ifta tbnot already mapped type ta type tb then m m ta tb evaluation this section presents the evaluation of our code differencing approach.
first we present the research questions.
we then present the data set and evaluation process.
after that we present our evaluation protocol and the obtained results.
we finally discuss the threats to validity limitations and the scope of the approach.
all the material of this section and a replication package are available on our companion web page5.
we ran the implementation of our approach on the following hardware configuration x intel r xeon r gold cpu .10ghz 187gb ram t ssd running ubuntu .
.
.
.
research questions we formulate the research questions as follows rq1 to what extent can our approach produce identical results as the state of the art technique?
this aims to investigate the soundness of the produced mappings and diff compared to a ground truth namely gumtree.
rq2 to what extent does our approach perform and scale on the memory footprint of computing diffs compared to a state of the art approach?
this aims to position the scalability performance on memory consumption of our diffing algorithm over a long evolution history with an established state of the art solution.
in particular we measure the memory heap allocated per node.
rq3 to what extent does our approach perform and scale on the time performance of computing diffs compared to a state of the art approach?
this aims to position the scalability performance on execution time of our diffing algorithm over a long evolution history with an established data set characteristics from .
projects loc files commits contributors stars apache hadoop .63m .2k 12k apache flink .5m .2k .8k quarkus 614k .5k .7k google guava 509k .16k 44k netty 317k .78k 29k apache dubbo 197k .81k 37k alibaba fastjson 188k .12k 24k apache log4j2 183k .32k .8k jenkins 181k .69k 19k javaparser 179k .67k .1k inria spoon 154k .06k .3k aws toolkit eclipse .9k .08k 27k apache maven .5k .05k .1k apache spark .6k .06k 33k apache skywalking .7k .58k .9k jackson core .3k alibaba arthas .2k 29k google gson .8k 21k slf4j .5k .9k state of the art solution.
in particular we measure the total time to compute a diff and the time for the two phases of top down and bottom up of the mappings.
rq4 to what extent does our approach perform compared to a state of the art approach on a practical use case of parsing and diffing commits?
the previous rqs evaluated the performance of our commit diffing algorithm.
thus ignoring the ast preparation extraction and construction from a history.
rq4 measures the cost of parsing the commits along with the cost of computing the diffs.
it works on a concrete use case as experienced by a developer that computes commit diffs on a code history.
.
dataset table details the final list of software projects we used in the following evaluation.
the evaluation re used the dataset employed in the hyperast paper .
it contains large open source realworld java projects with a large number of commits per history.
thus serving as a representative code histories in our evaluation.
.
evaluation protocol we now present the experimental protocol we followed for the evaluation.
as gumtree is the most advanced state of the art differencing tool we select it as a baseline.
the evaluation protocol is divided into three parts one protocol for rq1 another one for rq2 and rq3 and one specific protocol for rq4.
these four rqs use several of the following five objects gumtree .
the original version of gumtree in java without its java parser.
used in rq1 to rq3.
lazy .
the current proposed approach in rust relying on the hyperast version with lazy top down and bottom up phases.
used in rq1 to rq4.
not lazy .
the closest equivalent of gumtree java but in rust and relying on the hyperast .
this object is useful to mitigate comparison between java and rust program executions.
this version still benefits from the hyperast but no lazy phases.
used in rq3.
6hyperdiff computing source code diffs at scale esec fse december san francisco ca usa partial lazy .
similarly to not lazy but lazy on the top down phase.
useful to measure the effect of not lazifying during the bottom up phase.
used in rq3.
gumtree spoon6.
the original version of gumtree in java backed with its official java parser spoon .
used in rq4.
we now detail the protocol for each rq rq1.
it compares the results of our approach object lazy to the baseline i.e.
object gumtree .
to do so we check whether each difflazy and gumtree produced are identical.
the independent variables are the mappings found in a diff and the actions a diff contains.
thus we compare the mappings and the diff s actions for our approach against gumtree.
rq2.
it focuses on the following two objects gumtree lazy .
partial lazy and not lazy are not discussed here as they only differ from lazy about when memory is used not the total amount consumed.
the independent variables for rq2 are the measured memory heap allocated using the objects lazy andgumtree the number of nodes used in both objects both lazy andgumtree use the same number of nodes .
we then divided the measured heap size by the number of nodes to obtain a result in byte per node.
rq3.
it studies the four following objects gumtree lazy not lazy partial lazy .
the independent variable in rq3 is the execution time first the total time spent top down bottom up and computing the diff actions from the mappings with chawathe algorithm then only the two matching phases top down and bottom up finally only the top down phase.
rq1 .
regarding rq1 to rq3 the first step of the protocol consists in parsing the various commits to provide to gumtree and to construct the hyperast .
this is a preprocessing step before computing the diffs.
then we provide the resulting parsed trees to our approach and to gumtree.
to precisely evaluate the commit diffing algorithm we provide hyperast and gumtree with the same asts for these three rqs.
only after that we start measuring the performance of the different phases and algorithms for computing the diffs.
so the parsing time for gumtree and construction time of the hyperast are not considered in rq1 rq2 and rq3.
thus ensuring a more controlled measurements of the algorithmic performances and an unbiased comparison.
rq4.
it studies the two following objects gumtree spoon and lazy .
the independent variable in rq4 is the execution time.
we compare the spent time at computing diffs of the latest 100commits for each project while including the asts parsing time for gumtree spoon and construction time of the hyperast .
indeed the combination of spoon with gumtree allows us to feed entire commits as asts to the gumtree algorithm with the spoon parser.
.
results we now present and discuss the observed results.
.
.
rq1.
to answer rq1 we compare the mappings and diffs produced by our approach to the gumtree baseline.
in total we calculated diffs implying mappings7.
.
of these diffs were identical by matching the gumtree results identically.
.
of the mappings of the diffs are also identical.
7distribution of change size per diff is given in our companion web page in size plot.png varying from small changes to very large changes several commits.we manually scrutinized and checked the other .
diff and found out the following edge cases.
for cases an error occurred during the diff generation and hence not computing fully the final diff actions.
however when comparing their mappings they were identical.
for cases our diff had more actions than the gumtree diff.
for the other cases our diff had less actions than the gumtree diff.
this last cases are explained by the fact that we could calculate more mappings between the ast nodes than what gumtree did.
in fact these cases could highlight better diffs since they do not contain additional add and delete actions due to unmapped ast nodes.
overall even in these diffs .
of mappings and .
of the diff actions are identical.
only few mappings and diff actions cause comparison issues.
therefore we consider those marginal cases as outliers due to implementation issues in our prototype or our execution environment in comparison to the more than of correct diffs and mappings.
rq1insights the results show that .
of the diffs and .
of the mappings our approach produced are identical to the gumtree outputs.
120diffs where not identical to gumtree.
still they remain similar at .
of diff actions and at .
of mappings.
thus our approach produces identical results that gumtree on the involved data set.
.
.
rq2.
to answer rq2 we measure the memory heap allocated given the same number of nodes for our approach and for gumtree.
on average gumtree needs .4bytes per node.
our implementation needs .278byte per node in the dag over commits and .13additional bytes per decompressed node.
our approach decompresses nodes lazily.
considering the allocation of a zeroed and contiguous piece of memory on modern operating systems such an allocation is deferred at the granularity of a virtual memory page.
thus with our approach physical memory is only allocated when a node is decompressed turned from zeros to ones .
the peak transient memory footprint of our approach occurs during the diff generation at the exact same stage as the original gumtree implementation.
indeed we did not make the chawathe et al.
algorithm lazy as it is not a focus of the core contribution on computing the mappings both in our approach and gumtree.
moreover the chawathe algorithm diff generation uses a third tree that starts as a copy of the source tree and is mutated for each change until it structurally becomes the destination tree.
rq2insights results show out low memory footprint compared to gumtree with on average .
.13bytes per node in our approach versus .4bytes per node in gumtree.
representing an order of magnitude difference of .
.
.
.
rq3.
herein we measure time at the three different stages of the gumtree algorithm namely the total time spent the two matching phases top down and bottom up phases and the topdown phase only.
figures to shows respectively the results for the three aforementioned stages.
each figure uses the same plotting scheme the time taken is displayed as vertical box plots on a logarithmic scale where the mean is a thick horizontal black line and the median is a thin colored horizontal line.
the box delimits the first and third quantile.
the vertical bar delimits the confidence interval.
extreme points are displayed outside this interval.
7esec fse december san francisco ca usa q. le dilavrec d. e. khelladi a. blouin and j m. j z quel .. changes 10m1100time s .
p .
.
p .
.48s .26s .48s .26s .
p .
.
p .
.05s .4s .05s .4s .
p .
.
p .
.5s .3s .5s .3s .
p .
.
p .
.32s .39s .32s .39s .
p .
.
p .
.45s .86s .45s .86s .
p .
.
p .
.97s .36s .97s .36s .
p .
.
p .
.61s .5s .61s .5s .
p .
.
p .
.0s .0s .
p .
.
p .
.5s .0s .5s .0s .. 10m1100 .
p .
.
p .
.19s .52s .19s .52s .
p .
.
p .
.81s .04s .81s .04s .
p .
.
p .
.91s .3s .91s .3s .
p .
.
p .
.32s .39s .32s .39s .
p .
.
p .
.45s .86s .45s .86s .
p .
.
p .
.97s .36s .97s .36s .
p .
.
p .
.61s .5s .61s .5s .
p .
.
p .
.0s .0s .
p .
.
p .
.5s .0s .5s .0s .. ..305k 307k..667k .00m.. .97m .00m.. .35m .00m.. .1m .0m.. .0m nodes 10m1 .
p .
.
p .
703ms .84s 703ms .84s .
p .
.
p .
.68s .86s .68s .86s .
p .
.
p .
.49s .0s .49s .0s .
p .
.
p .
.32s .39s .32s .39s .
p .
.
p .
.45s .86s .45s .86s .
p .
.
p .
.97s .36s .97s .36s .
p .
.
p .
.61s .5s .61s .5s .
p .
.
p .
.0s .0s .
p .
.
p .
.5s .0s .5s .0s figure comparing overall diff time facet legend in black relative gain in p value mann whitney avg.
for our approach avg.
for gumtree facet legend in red relative gain in p value mann whitney avg.
for our lazy approach avg.
for our non lazy variant color legend lazy blue partial lazy orange not lazy red gumtree green .. changes 10m1100time s .
p .
.
p .
.26s .30s .26s .30s .
p .
.
p .
.49s .4s .49s .4s .
p .
.
p .
.4s .7s .4s .7s .
p .
.
p .
.72s .2s .72s .2s .
p .
.
p .
.5s .5s .
p .
.
p .
.5s .5s .
p .
.
p .
.20s .29s .20s .29s .
p .
.
p .
.81s .31s .81s .31s .
p .
.
p .
.09s .91s .09s .91s .
p .
.
p .
.99s .73s .99s .73s .
p .
.
p .
.2s .8s .2s .8s .
p .
.
p .
.58s .6s .58s .6s .. 10m1100 .
p .
.
p .
.99s .65s .99s .65s .
p .
.
p .
.15s .77s .15s .77s .
p .
.
p .
.23s .5s .23s .5s .
p .
.
p .
.77s .31s .77s .31s .
p .
.
p .
.47s .7s .47s .7s .
p .
.
p .
.66s .66s .
p .
.
p .
.20s .29s .20s .29s .
p .
.
p .
.81s .31s .81s .31s .
p .
.
p .
.09s .91s .09s .91s .
p .
.
p .
.99s .73s .99s .73s .
p .
.
p .
.2s .8s .2s .8s .
p .
.
p .
.58s .6s .58s .6s .. ..305k 307k..667k .00m.. .97m .00m.. .35m .00m.. .1m .0m.. .0m nodes 10m1100 .
p .
.
p .
532ms .10s 532ms .10s .
p .
.
p .
.07s .92s .07s .92s .
p .
.
p .
.03s .34s .03s .34s .
p .
.
p .
542ms .93s 542ms .93s .
p .
.
p .
.35s .0s .35s .0s .
p .
.
p .
.50s .7s .50s .7s .
p .
.
p .
.20s .29s .20s .29s .
p .
.
p .
.81s .31s .81s .31s .
p .
.
p .
.09s .91s .09s .91s .
p .
.
p .
.99s .73s .99s .73s .
p .
.
p .
.2s .8s .2s .8s .
p .
.
p .
.58s .6s .58s .6s figure comparing the top down and bottom up phases time facet legend relative gain in p value mann whitney avg.
for our approach avg.
for gumtree facet legend in red relative gain in p value mann whitney avg.
for our lazy approach avg.
for our non lazy variant color legend lazy blue partial lazy orange not lazy red gumtree green figures to shows groups of four box plots corresponding to the four objects we compare see section .
respectively lazy our approach in blue partial lazy not lazy in red and gumtree in green .
we faceted i.e.
grouped the figure in both axis due to the correlation of computation time both with the size of the output i.e.
the diff horizontally with three groups rows to to and to changes.
the number of changes is the size of the diff in terms of number of modifications needed to transform one ast version to the other.
the size of each version vertically in terms of number of nodes from hundred thousands tomillions in six groups columns .
on top of each facet i.e.
groups of box plots in black we put the relative gain in the p value and the average time for our approach and gumtree.
in red we display the same information for our lazy and the non lazy variant.
figure presents the total time taken to compute the diff including the top down and bottom up mapping phases with the diff generation.
we can first observe that our approach outperforms the original gumtree each time gaining between avg.
.
s vs8.
s and avg.
.
svs22 s of reduced overall computation time.
the gains are more important with a lower number of 8hyperdiff computing source code diffs at scale esec fse december san francisco ca usa .. changes 10m1100time s .
p .
.
p .
.1ms 510ms .1ms 510ms .
p .
.
p .
120ms .06s 120ms .06s .
p .
.
p .
367ms .09s 367ms .09s .
p .
.
p .
251ms .58s 251ms .58s .
p .
.
p .
.04s .8s .04s .8s .
p .
.
p .
.29s .5s .29s .5s .
p .
.
p .
.3ms 129ms .3ms 129ms .
p .
.
p .
767ms .31s 767ms .31s .
p .
.
p .
500ms .42s 500ms .42s .
p .
.
p .
400ms .18s 400ms .18s .
p .
.
p .
.35s .8s .35s .8s .
p .
.
p .
.33s .0s .33s .0s .. 10m1 .
p .
.
p .
.5ms 320ms .5ms 320ms .
p .
.
p .
.1ms 629ms .1ms 629ms .
p .
.
p .
133ms .28s 133ms .28s .
p .
.
p .
213ms .24s 213ms .24s .
p .
.
p .
464ms .5s 464ms .5s .
p .
.
p .
.08s .7s .08s .7s .
p .
.
p .
.3ms 129ms .3ms 129ms .
p .
.
p .
767ms .31s 767ms .31s .
p .
.
p .
500ms .42s 500ms .42s .
p .
.
p .
400ms .18s 400ms .18s .
p .
.
p .
.35s .8s .35s .8s .
p .
.
p .
.33s .0s .33s .0s .. ..305k 307k..667k .00m.. .97m .00m.. .35m .00m.. .1m .0m.. .0m nodes 100 10m1 .
p .
.
p .
.38ms 235ms .38ms 235ms .
p .
.
p .
.6ms 558ms .6ms 558ms .
p .
.
p .
.4ms .77s .4ms .77s .
p .
.
p .
177ms .00s 177ms .00s .
p .
.
p .
449ms .3s 449ms .3s .
p .
.
p .
.03s .4s .03s .4s .
p .
.
p .
.3ms 129ms .3ms 129ms .
p .
.
p .
767ms .31s 767ms .31s .
p .
.
p .
500ms .42s 500ms .42s .
p .
.
p .
400ms .18s 400ms .18s .
p .
.
p .
.35s .8s .35s .8s .
p .
.
p .
.33s .0s .33s .0s figure comparing top down phase time facet legend relative gain in p value mann whitney avg.
for our approach avg.
for gumtree facet legend in red relative gain in p value mann whitney avg.
for our lazy approach avg.
for our non lazy variant color legend lazy blue partial lazy orange not lazy red gumtree green changes relative to the size of the codebase.
in particular between and changes our gains increase up to of execution time compared to the baseline.
then between and changes our gains increase up to .
finally for more than changes our gains improve up to .
we observe that our approach with the lazy implementation also outperforms the non lazy variant all the time regardless of the size of the code and the size of the diff.
the gain varies from avg.
svs60 s to24 .
svs55 s .
moreover for the four largest projects with more than 300kloc and 3m nodes we disabled the generation of the diff for the gumtree implementation.
indeed we observed extreme slowdowns of gumtree when enabled leading almost every time to out of memory errors.
as we did not attempt to lazify the generation of the diff we further measured the time performance without the generation i.e.
the top down and bottom up phases only and the time performance of the top down phase only.
figure presents the time taken to compute the mappings during the top down and bottom up matching phases.
herein our approach outperforms the original gumtree top down and bottomup phases each time gaining between avg.
.
svs7.
s and avg.
.
svs40.
s of reduced overall computation time of the mappings.
between and changes our gains increase from to94 .
then between and changes our gains increase from to93 .
finally for more than changes our gains improve from to90 .
similarly our lazy variant outperforms the non lazy variant all the time.
the gain varies from to80 .
figure presents the time taken to compute only the mappings with the top down matching phase.
herein our lazy top down phase outperforms the original gumtree top down each time gaining between avg.
.1msvs510ms and avg.
.38ms vs235ms of reduced overall computation time of the top down mappings.
between and changes our gains increase from to98 .
between and changes they increase from to96 .
finally for more than changes they increase from to .
similarly our lazy variant largely outperforms the non lazy variant all the time.
the gain varies from to94 .
these three figures highlight our lazy approach significantly gains during the top down phase in percentage and the bottomup phase in absolute time .
while our diff generation is faster than the gumtree one and scales up to thousands of actions on large software projects e.g.
hadoop further gain could be achieved.
indeed we did not lazify the diff generation meaning that a full decompression of the trees is mandatory before generating the diff.
lazifying the generation is future work.
it is worth noting that for the results of the lazy variant a full decompression is done before computing the diff taking .
of the generation time while only .
for the non lazy variant.
furthermore in figure we observe that the bigger the projects the better our lazy approach performs compared to the two variants partial lazy andnot lazy .
in figure we also observe the same gain compared to the not lazy variant.
rq3insights results show systematic and significant gains of on average and up to of our lazy approach compared to gumtree in all phases from the top down and bottom up matching phases to the generation of the diff.
representing an order of magnitude difference in total time from .
to .7for diff computation from 1to 226for the topdown and bottom up phases and from .2to 233for the top down phase.
.
.
rq4.
to answer this rq we measured the execution time for parsing and diffing 100commits for each project.
hence we can compare the overall performance in a practical use case similarly as a developer would go through to compute diffs on given number of commits.
to do so we use the gumtree spoon version that parses a commit and then calls the gumtree diffing algorithm.
9esec fse december san francisco ca usa q. le dilavrec d. e. khelladi a. blouin and j m. j z quel figure depicts the time of our approach including the construction of the hyperast in orange versus the computation time for gumtree spoon including the parsing of the commits.
we observe that our approach outperforms gumtree spoon on all the projects.
for our largest projects hadoop andflink gumtree spoon crashed in most of the time during the generation of the diffs due to using more than 32gbof memory heap.
we also had other cases of crashes in jenkins and in gson .
those cases are in red rather than orange in our results.
in skywalker gumtree spoon could not parse completely most of the commits due to an issue with the multi modules of java.
gumtree spoon could only parse some modules sometimes one module only.
this explains why in many commits it was faster than our approach that did diff all modules.
overall when computing diff successfully our approach was on .
times faster than gumtree spoon in half of the cases median .
we had extreme cases of improvement in the six last small projects from jackson toslf4j where we were thousands and hundred thousands times faster than gumtree spoon .
excluding these projects we reach an average of .
times where we are faster.
we also investigated the case of diffing only two commits by looking at the first two commits in our projects since we must construct the hyperast entirely in the first commit and incrementally update it in the second commit before computing the diff.
we see that we could compute the diff faster than gumtree as shown by t in figure note that it is in two formats min sec or .millisec .
for example in maximum in hadoop andflink our approach took respectively 1min sand1min swhile gumtree took 21min sand 24min s. in minimum in slf4j we took 300mswhile gumtree took s. in other medium sized projects as netty dubbo log4j jenkins javaparser spoon maven andspark we respectively took from sto27 s while gumtree took from sto3min s. on the smallest four projects our approach varied from 300ms .
s to675ms whereas gumtree spoon varied from sto36 s. therefore even on two commits where we must build the hyperast from scratch a developer benefits of our lazified approach based to diff two commits or more.
we could reach a gain up to and an order of magnitude of 122when considering the two first commits only.
rq4insights our lazy approach outperforms gumtree spoon .
we are faster by .
times in half of the cases median and when excluding extreme cases of gains we are faster on average by .
times.
we also outperform gumtree spoon on the basic use case of diffing two commits only with a gain up to and an order of magnitude of .
.
threats to validity this sections discusses threats to validity w.r.t.
.
internal validity.
considering the computation of diffs we first had to evaluate its ability to produce identical outputs mappings and diffs as gumtree.
to make sure we have unbiased measurements we used the hyperast to construct the code history and we retrieved the trees of each commit from the hyperast .
thus we had a uniform representation of the node elements i.e.
same parser and same grammar for the asts for comparing our approach and gumtree.
moreover our implementation and the hyperast areimplemented in rust while gumtree is developed in java.
to mitigate this difference of language while comparing execution time and memory usage we provide and compared our approach with two other objects developed in rust using the hyperast partial lazy andnot lazy .not lazy is the closest rust version of gumtree while still benefiting of the hyperast .
compared to these two variant objects our evaluation still shows significant benefits for our approach with all lazy phases.
external validity.
the evaluation implied projects.
we carefully follow a clear protocol to select relevant and significant java projects.
our curated list of projects represents real world complex software systems with very large histories.
we implemented our approach on top of the hyperast by lazifying the gumtree algorithms.
we then and evaluated our approach for java with maven build system.
our conclusions in theory could generalize to other programming languages with similar features as java e.g.
strong static nominal typing .
nonetheless further experimentation remains necessary on other languages to generalize our results.
we also cannot generalize the diffing results to other diffing algorithms such as .
note that as the hyperast supports only java so far we could only evaluated and compared to gumtree on java projects.
however the goal of this paper was not to support multiple languages but to show the scalability of computing diffs on large code history.
besides gumtree performance are not language dependent .
construct validity.
our evaluation shows that our approach scales the computation of diffs on large code history and large projects representing real world complex software with very large histories.
compared to gumtree we outperformed it by an orderof magnitude difference in cpu time from .2to .7for the total time of diff computation and up to 226in intermediate phases of the diff computation and an order of magnitude difference in memory footprint of .5per ast node.
further evaluation remains necessary for more insights and statistical evidence.
related work this section focuses on approaches computing diffs of source code.
a first category of approaches compute the diffs on the textual format of the code.
asaduzzaman et al.
canfora et al.
and reiss et al.
proposed a language independent techniques for diffing.
however as they work on the textual representation of the code they are unable to compute fine grained changes such as a change in a parameter or a condition guard.
this thus hinders any automatic code analysis based on them.
a second category of approaches compute the diffs on the structured tree representation of the code.
pawlik et al.
compute the diff but without the move action.
chawathe et al.
are the first to compute a diff including move actions thus shortening the diff.
duley et al.
generate a diff for verilog hdl files.
hashimoto et al.
are able to work on raw asts by producing a diff.
nguyen et al.
also propose to compute a diff but they focus more on finding clones rather than a complete diff.
fluri et al.
propose changedistiller a tool inspired by for computing diffs.
falleri et al.
also proposed gumtree a tool inspired by and also inspired from the algorithm of cobena et al.
for compression purposes.
matsumoto et al.
propose an 10hyperdiff computing source code diffs at scale esec fse december san francisco ca usa hadoop flink netty dubbo logging log4j2 jenkins javaparser spoon maven spark skywalking jackson core arthas jacoco junit4 gson slf4j0.
.
.
.
time seconds commits .
.
.
.
.
.
.
.
x16x16 .
.
x20x20 .
.
x4x4 .
.
x22x22 .
.
x17x17 .
.
x3x3 .
.
x14x14 .
.
x8x8 .
.
x7x7 .
.
x12x12 .
.
x1x1 .
.
x1x1 .
.
x20x20 .
.
x46x46 .
.
x14x14 .
.
x17x17 .
.
x122x122 .
.
figure comparing overall execution time parsing diff on commits facet legend time to compute first diff between first and second commit lazy gumtree format min sec or .millisec color legend lazy orange times lazy while gumtree spoon failed red times gumtree spoon blue plus extension of gumtree by incorporating information of line differences in addition to the ast to propose a diff easier to understand for developers.
higo et al.
also propose to integrate a new action of copy and paste to make the diff easier to understand too.
all these existing approaches focus on efficiently computing a diff between two files.
our approach focuses on a different yet complementary concern diffing commits picked from a source code history git .
since a git commit can contain numerous files diffing commits directly faces the scalability issue we overcome in this paper.
other approaches aim at improving the diffs that gumtree produces .
this is out of our scope however they can still process our diff output similarly as they would do with gumtree diffs.
tsantalis et al.
propose an approach for detecting refactorings.
refactorings are high level changes that are not the goal of this paper and of the approaches of computing diffs.
moreover refactoringminer mostly focuses on java while some other recent extensions try to make it work on python and kotlin it seems like a complex process.
to the best of our knowledge our approach is the unique one targeting scaling up the computation of diffs to thousands of commits and in large software projects.
conclusion this paper proposes a novel code differencing approach that scales on large code histories.
we leverage on the hyperast novel representation of code histories and lazify the gumtree algorithms to scale on large histories.
the evaluation shows that our approach outperforms the mainstream code diffing approach.
in particular we observed an order of magnitude difference in cpu time from .2to .7for the total time of diff computation and up to 226in intermediate phases of the diff computation.
we also observed an order of magnitude difference in memory footprint of .5per ast node.
finally we gain all the time while having .
of identical diffs with respect to gumtree and99.
of identical mappings in the remaining .
diffs.
when including the parsing cost along with the diff we still outperform gumtree spoon .
we are faster by .
times in half of the cases median and when excluding extreme cases of gains we are faster on average by .
times.
in the future we would like to further apply the lazification to the diff generator i.e.
the chawathe algorithm .
this may imply functional changes in the produced diffs and on how it should be reapplied thus possibly exposing structurally independent changes.
our approach also opens new research opportunities to build temporal code analyses on top of our scalable differing approach.
for example it should be easier to compute more sophisticated metrics on subtrees to be then used in the top down phase of the gumtree algorithm.
second considering the first part of the topdown phase of gumtree it should also be possible to devise levels of cloning extraction detection.
third for the industry of software forges given the efficiency of the hyperast at persisting versions as a fine grained dag it should be possible to efficiently cache mappings and diffs.
this would permit software forges to serve related services at very low amortized latency.
finally multi mappings are currently intermediate and internal data not exported as output in the diffs.
our approach may export such data to better track cloned duplicated or merged code nodes.
acknowledgment the research leading to these results has received funding from the anr agency under grant anr jcjc mc evo2204687 .
11esec fse december san francisco ca usa q. le dilavrec d. e. khelladi a. blouin and j m. j z quel