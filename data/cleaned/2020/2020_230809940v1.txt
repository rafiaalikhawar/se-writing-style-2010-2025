evaluating transfer learning for simplifying github readmes haoyu gao the university of melbourne melbourne victoria australia haoyug1 student.unimelb.edu.auchristoph treude the university of melbourne melbourne victoria australia christoph.treude unimelb.edu.aumansooreh zahedi the university of melbourne melbourne victoria australia mansooreh.zahedi unimelb.edu.au abstract software documentation captures detailed knowledge about a software product e.g.
code technologies and design.
it plays an important role in the coordination of development teams and in conveying ideas to various stakeholders.
however software documentation can be hard to comprehend if it is written with jargon and complicated sentence structure.
in this study we explored the potential of text simplification techniques in the domain of software engineering to automatically simplify github readme files.
we collected software related pairs of github readme files consisting of entries aligned difficult sentences with their simplified counterparts and trained a transformer based model to automatically simplify difficult versions.
to mitigate the sparse and noisy nature of the software related simplification dataset we applied general text simplification knowledge to this field.
since many generaldomain difficult to simple wikipedia document pairs are already publicly available we explored the potential of transfer learning by first training the model on the wikipedia data and then fine tuning it on the readme data.
using automated bleu scores and human evaluation we compared the performance of different transfer learning schemes and the baseline models without transfer learning.
the transfer learning model using the best checkpoint trained on a general topic corpus achieved the best performance of .
bleu score and statistically significantly higher human annotation scores compared to the rest of the schemes and baselines.
we conclude that using transfer learning is a promising direction to circumvent the lack of data and drift style problem in software readme files simplification and achieved a better trade off between simplification and preservation of meaning.
ccs concepts software and its engineering documentation computing methodologies neural networks applied computing text editing .
keywords software documentation github text simplification transfer learning permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november san francisco usa association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
reference format haoyu gao christoph treude and mansooreh zahedi.
.
evaluating transfer learning for simplifying github readmes.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse .
acm new york ny usa pages.
introduction software documents describe key information about software products such as technologies code structure system design and architecture.
these documents are an integral part of the software development process as they can be used to describe the application requirements design decisions architecture as well as deployment and installation.
in particular readme files shape developers first impression about a software repository and document the software project that the repository hosts .
however readme documents often contain jargon abbreviations and code blocks making the text challenging to comprehend for non specialists and people from other language backgrounds.
in fact readability issues and complicated documents are important issues that practitioners frequently encounter .
a simple search on github for complicated readme yields over issues and pull requests1.
therefore simplification of readme files is needed to improve the efficiency of communication between members of development teams and to propagate new technologies to broader fields.
significant advancement in natural language processing nlp has been witnessed over the last decade thanks to the development of artificial neural networks.
text simplification ts is an nlp task that focuses on rewriting texts into simpler versions while preserving the original meaning to the extent possible.
the simplification of text in the general domain has been studied extensively and its data sources include mainly wikipedia and its simple wikipedia counterpart as well as newsela articles written for specific age groups .
these data sources especially their simplified versions are written by professionals with the intention of catering to people with different levels of reading ability.
the simplification of sentences in general domain text can be implemented by three main operations including splitting deletion and paraphrasing .
these simplification operations are implicitly encoded in text simplification datasets of the general domain and neural simplification models trained on these datasets memorise the rules in their parameters and achieve competitive performance .
the significant disparity between the simplification of general domain documents and domain specific software documentation prohibits the simple reuse of text simplification systems designed for the general domain.
first the text style of software documentation includes many code blocks and external links such as urls which do not resemble the style for general domain texts.
in addition the aug 2023esec fse november san francisco usa haoyu gao christoph treude and mansooreh zahedi simplification rules for software documents differ from the generaldomain text by performing fewer deletions and more elaborations.
indeed in this paper we show that a state of the art approach trained on general domain documents from wikipedia is not able to produce semantically identical and or grammatically correct content in around of the cases when applied to readme files cf.
section .
a simplification system trained directly on general domain text cannot simplify software documentation satisfactorily.
to the best of our knowledge there is no previous research focusing on simplifying software documentation.
to address this gap we collected a software specific text simplification dataset and trained a simplification system on the data.
we experimented with transferring general domain text simplification rules to software documentation and evaluated the system through automatic metrics and manual analysis.
we found that by applying transfer learning the model was able to generate the most satisfactory simplifications.
specifically the best performing model achieved a .
bleu score in the test set and exceeded the baseline models in terms of semantic similarity grammar and simplicity based on human annotation.
the contributions of our research include the following a readme files simplification dataset.
a pipeline to collect such a dataset.
an exploration of the application of transfer learning to the problem of simplification of readme files with promising results.
related work .
documentation issues and solutions previous studies provide rich empirical evidence for software documentation issues.
steinmacher et al.
discovered several barriers to participating in open source software oss projects one related to documentation problems .
after that aghajani et al.
conducted empirical studies to investigate software documentation issues and practitioners perspectives.
software documentation issues could generally be categorised into what information is contained and how the information is presented.
to address the problems the automatic generation of software documentation could potentially mitigate correctness completeness up to dateness and various other issues.
automatic software documentation generation can be applied to various software artifacts including source code bug reports and pull requests .
in terms of source code sridhara et al.
used algorithms to generate comments for java methods while mcburney and mcmillan improved it by adding surrounding contexts.
moreno et al.
summarised java classes using stereotype rules and manually defined templates.
hu et al.
proposed using a sequence to sequence model and formulated code summarisation as a translation task.
in terms of bug reports rastkar et al.
trained a classifier to identify important sentences from bug reports and used them as summaries.
regarding api documentation treude and robillard augmented api documents using insight sentences from stack overflow.
pull request descriptions can also be generated considering commits and code comments .
source code changes are also used to generate commit messages .automatic documentation generation could help developers identify components that are prone to be overlooked and improve development efficiency.
among the issues readability is an important issue that practitioners frequently encounter .
one of the practitioners in aghajani et al.
s survey stated a developer in our team created confusing and overly complicated documentation for customers of our solution .
to the best of our knowledge no previous studies focused on simplifying software documentation to improve people s comprehension.
text simplification is an nlp technique that could bridge this gap and enhance developers understanding of software documentation.
.
text simplification multiple data sources have been proposed for the task of simplification of text.
zhu et al.
first used wikipedia and simple wikipedia as a source which was later expanded by zhang and lapta to the wikilarge dataset.
however xu et al.
argued that this wikipedia based simplification dataset is suboptimal and difficult to generalise to other genres of text.
they proposed a new dataset called newsela which contains different levels of simplification.
moreover there are also simplification corpora of languages other than english .
due to easier access to a large corpus of wikipedia data we performed part of our experiment on wikipedia datasets.
meanwhile the success of a text simplification system is highly dependent on the quality and quantity of complex simple sentence pairs in the training corpus .
zhu et al.
first used sentencelevel tf idf term frequency inverse document frequency similarity to construct the alignment between a simple wikipedia corpus and its regular counterpart.
later more sophisticated sentence alignment techniques were proposed that consider sentence orders and word level similarity increasing the alignment quality and the dataset size.
recently jiang et al.
proposed a neuralbased conditional random field crf aligner which decomposes the potential function into semantic similarity approximated by the bert classifier and alignment label transition approximated by the feedforward network .
their model automatically aligns 604k non identical aligned and partially aligned sentence pairs.
this powerful tool is able to achieve more than .
f1 score on the previous wikipedia corpus alignment task thus making their auto aligned dataset of higher quality.
sentence alignment is the first procedure in the pipeline of text simplification.
in our research we borrow their idea to align software document pairs as our first step in building a software documentation simplification system.
recent work began to see text simplification as a monolingual translation task.
specia first applied statistical machine translation to text simplification.
kauchak incorporated regular and simple sentences to train an n gram language model to perform text simplification tasks.
nisioi et al.
began to see text simplification as a task similar to machine translation mt and trained a standard sequence to sequence model based on lstm that surpasses the performance of previous statistical mt models.
different network designs were also developed for the model to learn a more effective simplification.
zhang and lapta used reinforcement learning for simplification with a reward that approximates simplicity relevance and fluency.
zhang et al.
combineevaluating transfer learning for simplifying github readmes esec fse november san francisco usa lexical simplification with sentence level simplification by first performing lexical substitution and then feeding the sentence into a constrained sequence to sequence model.
nishihara et al.
proposed a controllable simplification system by adding a level token and modifying the loss function while mallinson and lapata did it employing syntactic and lexical constraints.
current text simplification systems use transformer architecture and can achieve state of the art performance.
in our work we primarily used the transformer model and explored the simplification rule gap between software and general domain documentation.
when there is a disparity between the data distribution such as the text styles for software documentation and documents of the general domain the performance of the model can be degraded in which case transfer learning is needed.
transfer learning improves the performance of a learner in one domain by transferring information from a related domain .
it is widely used in many areas including image processing and natural language processing and has achieved significant success.
in our work we experimented with various transfer learning techniques for the task of software documentation simplification.
we applied the knowledge learnt from general domain document simplification to mitigate the noisy and sparse attributes of software related texts.
data collection in order to obtain enough software related documents to train our model we collected readme files from github using its restful api.
we implemented a program using github access tokens to iterate from the very first repository ordered by github id and check for candidates for the simplification dataset.
we only considered repositories not forking other repositories and with at least ten stars to filter out toy projects.
one reason we collected older repositories is that we believe readme files in older repositories need more simplification as different techniques were used back then and more old repositories have gone through simplification updates compared to recent repositories.
as we needed to get updates in the readmes longer commit histories will be more likely to contain candidate data.
therefore only projects with at least commits are investigated.
the left part of figure describes the procedures for collecting the data.
specifically for each repository we iterated through its entire commit history.
we collected a list of keywords that can be a hint for simplification.
we identified those commits that contain at least one of those keywords and only modify the readme file as document simplification instances.
the previous readme file is marked as the difficult version and the newly committed file is marked as the simplified version.
to encourage more prominent simplifications and avoid training data duplication we only preserved the first commit and the last commit with simplifications on the readme file for each repository.
we collected document level regularto simple instances in total.
regarding keyword selection we initially chose three keywords i.e.
simplify clarify and explain .
their definitions were searched in wordnet and their synonyms were further added to the set of keywords.
after that we expanded the keyword set by adding different forms including nouns verbs and adjectives.
the keywords along with their distribution in the final collected corpus table list of keywords and their distribution in data keywords count sum sample commit message simplification simplify intro paragraphsimplify simple simplicity reduction 20314change link text to reduce confusion reduce clarification clarifying readme a bitclarify clear clarity elucidation 2elucidate what we do with errorcode.elucidate elucidative elucidatory explanation 419update the documentation to explain how this worksexplain explanatory comprehension more comprehensible comprehend comprehensible ease 044rewrote readme.md to make it easier to follow easy are listed in table .
looking at the table the keywords clarify simplify explain and ease along with their derivations are the most frequently used keywords in the collected documents.
the more complicated words like elucidate and comprehend were rarely used.
to provide readers with more information on the effect of these keywords we also added sample commit messages with the most common word sets and listed them in table .
although keywords in commit message histories convey information about the modified contents using a unigram of occurrence can be ambiguous.
for example a simple negation term could render the semantics of simplification to the opposite meaning.
also sometimes the hint word for simplification might not refer to the harvested readme file but to structures in the code blocks.
furthermore even if it refers to the simplification of the readme file only a few sentences might be simplified with most parts remaining unchanged.
therefore further filtering and preprocessing steps are required which are illustrated in section .
in terms of implementation detail we used the authors access tokens and use pydriller to mitigate the impact of the github restful api rate limit as much as possible.
the collected documents are in json format with fields including difficult document simple document commit message language used and project fork counts.
we collected of these document pairs in total which are used to construct our software document simplification dataset.
instead of focusing on certain programming languages we want to investigate the overall simplification of readme files through the commit history and thus did not filter on the programmingesec fse november san francisco usa haoyu gao christoph treude and mansooreh zahedi language field.
figure lists the top ten languages used by the repositories that we collected.
figure repository languages data preprocessing after harvesting github readme files the difficult to simple correspondence is at the document level.
this is too long to build an effective translation model directly as most sentences in two versions of documents are duplicates which creates difficulty for the model to learn simplification.
also considering that it is not reliable to only depend on heuristic keywords as an indicative sign of simplification we need to further filter the harvested dataset and perform the sentence alignment task in order to give a higher confidence dataset in a sentence level correspondence.
figure depicts the overall procedure with the left hand side describing the dataset construction process.
each component will be discussed in detail in this section.
figure overall procedure .
data cleansing and masking the collected readme files are written in several formats including recommended markdown style2 plain text and html syntax which makes them noisy.
for example some people follow the conventions and for constructing tables while others choose to use html syntax.
for data cleaning and preprocessing we first semantic components and assigned tokens component type token inline code block code small chunks of code code large path of file or directory file table table hyperlink url removed emojis and different spacing characters including t and n .
another critical characteristic of our data source is that it contains a large number of semantic components that depend on the document and its context.
these components include urls code blocks tables etc.
these components are essential for software documentation as they usually include instructions specifications and external links that elaborate on projects.
but it can be challenging for the translation model to implicitly learn their attributes as different documents typically contain components that are barely the same.
therefore we used python package markdown2 3to identify and convert these special components into different individual tokens that are distinguishable for their usage.
the translation model is explicitly told where the special components are and can generate more cohesive sentences.
we manually inspect the data and categorise the tokens into five types.
the special tokens are listed in table .
version requirements and plain text code without using the markdown syntax are also important semantic information in the sentences.
however detecting these elements would require using regular expressions which is noisy and not the major goal of this paper.
therefore we leave these elements as in their original form.
.
sentence alignment sentence alignment methods were extensively studied in previous research.
jiang et al.
recently developed a neural based crf sentence alignment method that achieves an f1 score over .
on wikipedia data.
they decompose the potential function as follows ai ai s c sim si cai t ai ai where s denotes simple sentences c denotes complex sentences and aidenotes the index of the aligned sentence.
a fine tuned bert model is used to approximatesim si cai and a simple multi layer perceptron is used to approximate t ai ai .
they finally used a viterbi algorithm for decoding the optimal alignment arrangements.
for wikipedia data since original and simple documents are not composed concurrently the positions of difficult sentences and their simplified correspondence could differ a lot.
however the aligned sentences tend to be in a relatively similar order in terms of our harvested github readme files due to the incremental development nature of many software projects.
therefore the calculation oft ai ai in our software documents will not benefit much and will only increase training and decoding time.
therefore we discarded other components and only borrowed the fine tuned bert classifier to perform our alignment task.
transfer learning for simplifying github readmes esec fse november san francisco usa the specific alignment task is performed as follows.
for each pair of regular simple documents the sentences of the simplified document will be fed into the bert classifier with the regular sentences one by one.
for those that are classified as aligned we would temporarily mark them as aligning candidates.
to avoid o n2 time complexity when performing the alignment task we exploited the fact that many github readme files tend to grow incrementally.
unless a complete refactor of the documents aligning sentences should appear at the a closer section compared to non aligning ones.
each sentence in the simplified document will only be compared with the regular ones that have the sentence position within a window size of to the simplified sentence.
this window size is able to cover most of the readme file sentences except for excessively long ones thus reducing the processing time while providing good coverage for the majority of sentence pairs.
however the drifted sentence style for software documentation and the large amount of potential matches for each simple sentence make the false positive rate relatively high.
to accommodate this situation multiple filtering methods rules are used.
first we filtered the candidates using the tf idf based cosine distance.
tf idf is a commonly used statistic in natural language processing which computes weights for each occurring word by taking into account the frequency of the word as well as the frequency of documents containing it.
in this case it does not give great weight to frequently occurring but meaningless words such as the and a .
we trained our tf idf model on the corpus of all readme files.
using the tf idf vectorizer each sentence is represented as a vector and we are able to compute the cosine distances between simple and complicated sentences.
to filter out false positive candidate pairs using the cosine distance based on tf idf we manually selected simple to complicated sentence pairs and labelled them with the ground truth alignments with pairs labelled aligned and the other as not aligned .
we experimented with different filtering thresholds for cosine distances and the result is shown in figure .
figure performance of different thresholds as seen in the figure the f1 score and recall increase until the cosine distance threshold of .
.
after that the recall flattens while both the precision and f1 score start to decrease.
as in the real collected alignment candidate pairs more of them tend to be false positive instead of only taking half the proportion we choose the threshold of .
to prevent a further drop in the accuracy score.
allcandidate pairs with tf idf cosine distances greater than .
are categorised as false positives and discarded in this step.
furthermore the bleu score is a widely used metric for machine translation tasks that computes the n gram overlaps between the target and reference sentences and provides an intuitive score for the level of similarity between sentences.
a bleu score greater than .
typically indicates merely a copy of the source sentence while a bleu score below .
means overlap only in some name entities .
therefore we discarded sentence pairs with bleu scores greater than .
or less than .
.
the size of the dataset after applying the tf idf distance and bleu score filter is .
.
dataset anomaly filter after performing the previous alignment steps we have constructed a dataset of regular to simple software documents.
to obtain a highquality dataset we collected statistics on the number of alignments of regular sentences for each simple sentence and eliminated those that appeared to be outliers.
the average alignment number for the simplified sentence is .
with a maximum number of and a variance of .
.
we then removed data that are outside the range of3 .
as a result only sentences that are aligned with no more than three sentences were preserved.
we also discarded excessively long sentences.
sentences with more than alphabetic words were eliminated.
this procedure further reduces the size of the dataset to .
after a closer look at these eliminated sentence pairs that were initially categorised as aligned by our sentence aligner most of these outliers either appear to be too short or repeat instructions that only change a few words or urls.
for example an original document of url url which is a markdown syntax to show some urls is matched with three other texts of url url in the simplified document.
these masked sentences are short but similar only in markdown syntax structure instead of semantic meanings and should be considered as noise.
using this method we further cleaned up our proposed dataset.
.
dataset comparison for the simplicity of elaboration we refer to the dataset constructed using the wikipedia and simple wikipedia source as wiki data and refer to the dataset we constructed in the previous steps as sw data .
in this section we briefly discuss the attributes of both datasets.
table lists statistics for wiki data and sw data .
table statistics for sw data simple regular simple regular ratio sw data statistics average length .
.
.
vocabulary size .
exclusive vocab size .
wiki data statistics average length .
.
.
vocabulary size .
exclusive vocab size .
esec fse november san francisco usa haoyu gao christoph treude and mansooreh zahedi as seen from the table these two datasets differ significantly.
specifically sentences in wiki data tend to have a shorter length.
an average length of over .
and .
for simple and regular sentences in the sw data indicates that sentences harvested from github can be more complex and wordy.
moreover the vocabulary size in wiki data significantly surpasses that of the sw data.
this can happen because software documentation only focuses on specific topics while wiki data covers a much wider range of topics.
also the simple to regular ratio statistics in wiki data and swdata indicate that the simplification in wiki data is more aggressive.
in contrast the simplification of sw data makes less apparent changes.
this could happen because simple wikipedia articles are written with the intention of letting non native speakers feel confident in reading.
at the same time the simplification in github files includes different operations such as rewrite exemplify and clarify and some of the detail changes are minor.
as the sw data dataset contains relatively less apparent simplification the model tends to memorise the original sentence and barely performs simplification.
the simplification rule gap between wiki data and sw data plus the noise in the sw data motivates us to explore transfer learning as discussed in the next section.
to further illustrate our points we picked two representative simplification examples one from wiki data and the other from sw data as can be seen in table .
in this example the wiki data simplification performs aggressively and ignores some details of the evolution of the presidency armies.
however the author who simplified the sw data document only changes a few words at the end of the sentence making the argument clearer by giving a specific instruction.
as we are going to use both datasets to train our software documentation simplification system we split both datasets into train validation and test sets.
for the sw data we have a train set of size a validation set of size and the rest of the data forms the test set.
for the wiki data we have a train set of size as well as a validation set and a test set both of size .
the training of the model and the transfer learning will be conducted on the train set and the performance of cross entropy loss will be evaluated on the validation set.
we will finally generate new texts on the test set for more detailed evaluation.
model training and transfer learning in this section we elaborate on how we trained our model using transfer learning and discuss the output of the model based on bleu score evaluation.
.
model tokeniser before feeding sentences into our model we need to tokenise sentences into a list of tokens so that the model can learn their representations in the embedding layer.
the tokens can be whole words or subwords.
a tokeniser off the shelf is able to perform well on general domain tasks like simplifying wiki data.
however software documentation has a lower lexical complexity and contains components that the model does not want to reduce.
to better fit our study a custom tokeniser is needed.
therefore we trained our own tokeniser using all sentences in the sw data and wiki data trainingset using the wordpiece tokenisation algorithm .
wordpiece is a subword tokenisation algorithm developed by google which is widely used in various models .
similar to byte pair encoding bpe it learns how to merge characters into words when provided with a large corpus.
during tokenising a sub word with a symbol indicates it is the continuation of the previous subword and is later concatenated.
in section .
we used regular expressions to mask these special components to prevent key components from automatic simplification to different tokens.
these special tokens are listed in table .
however to ensure that our tokeniser does not further split these tokens we specify those as special tokens during our training of the tokeniser along with sos eos and unk indicating the start of a sentence the end of a sentence and unknown words respectively.
in this case the tokeniser can directly tokenise these components as a whole.
as a result the downstream model will directly know the meaning of these tokens making it easier for the model to learn how to manipulate them.
we also specified the vocabulary size of the tokeniser at .
.
model architecture and hyperparamters the text simplification task can be considered a translation task in which sequence to sequence models are widely used.
transformer is a multi headed self attention sequence to sequence model that achieves competitive performance in neural translation tasks.
this architecture has become an essential building block in many models in the deep learning area.
as this work focuses more on the simplification rules of software documentation and mitigating the drawbacks in the currently collected sw data we adopted the vanilla version of transformer in the original paper with only some minor changes in the tokeniser and a reduction of trainable parameters to save training time.
with limited access to gpu computing resources plus the long training time of our model we did not try to tune the hyperparameters extensively to reach the best performance.
instead we experimented with only a few sets of hyperparameters close to the setting from and used one set of them that performs the best on the task of sw data.
this set of hyperparameters was later used on every model we trained including the wiki data and the transfer learning.
specifically the model configuration and hyperparameter choices are listed in table .
.
training on wiki data and sw data as a starting point models with the given architecture were trained solely on the wiki data sets and the sw data set.
the cross entropy loss curve of the entropy of the model trained with wiki data is shown in figure .
and figure respectively.
we trained our models on wiki data for epochs and sw data for epochs.
their learning curves exhibit similar patterns representing the model gradually overfitted on the training set.
in the final epochs as the loss on the validation set is not decreasing we stop the training process and preserve checkpoints with the lowest validation error as final models.
.
transfer learning because of the limitations mentioned in the dataset comparison section it is difficult for the wiki data trained model to adapt to theevaluating transfer learning for simplifying github readmes esec fse november san francisco usa table simplification examples regular simple sw data limitations due to the nature of irssi s readline it is not possible to add formatting directly in the input line hence the need for the extra window kludge.
limitations due to the nature of irssi s readline it is not possible to add formatting directly in the input line so an extra line is output to the screen instead.
wiki data the presidency armies were the armies of the three presidencies of the east india company s rule in india later the forces of the british crown in india composed primarily of indian sepoys.the presidency armies were the armies of the three presidencies of british india .
table model configuration and hyperparameters multi head numbers learning rate 1e encoder layers batch size decoder layers optimiser adam embedding dimension alpha 2e feed forward dimension dropout rate .
figure wiki data loss curve change in text style and in simplification rule.
meanwhile it is difficult for the sw data trained model to generate ideal simplifications as the dataset contains different styles of simplification and many simplifications with only a few swaps of words.
these two models are used as baselines to compare our later transferred learning models.
for simplicity of argument we denoted these two models as baseline wiki and baseline sw. a combination of both styles in wiki data and sw data namely practical simplification and technical precision is desired for software documentation simplification.
by applying transfer learning we intend to share general domain text simplification knowledge with the software documentation simplification task.
figure shows three vertical red lines corresponding to the model checkpoint after training for epochs epochs and epochs.
for simplicity of elaboration we call them checkpoint early checkpoint mid and checkpoint best .
in terms of the checkpoint early the model is still under fitted after only seeing the dataset a few times.
some high level knowledge for general domain text simplification has been learnt but not enough to perform well.
with respect to the checkpoint mid the model has established a firm understanding of the text simplification task in the general domain.
also it is at the elbow point for the validation loss curve meaning that the learning speed decreased significantly after this point.
in terms of the checkpoint best the model has overfitted the training set but its performance on the validation set is the best.
we also incorporated the optimiser into the checkpoint for a smoother optimising process.
we adopted different transfer learning paradigms to explore the effect of transfer learning in the software documentation simplification task.
specifically we started from the checkpoint early mid and best and used these pre trained checkpoints to train our models on the sw data.
figure contains the validation loss curves for all three different transfer learning paradigms and their comparison to the performance of the baseline sw. figure transfer learning loss curves as seen in the figure the cross entropy loss curves for the three transfer learning strategies all have lower starting points.
moreover the loss drops faster than the model trained solely on the sw data.
epochs were trained on the three models and their loss in the validation set has reached the minimum.
in terms of cross entropy loss for the three transfer learning models the checkpoint best is the lowest while the checkpoint early is the highest.
however loss in the validation set is merely an indicator of model performance.
this metric suggests the uncertainty level of the model when decoding encoded sentences into their simplifications.
the lower this metric the more confident the model will be.
however as our sw data dataset contains different writing styles and mask tokens including urls and code blocks the model can find it difficult to generate more fluent sentences.
therefore better performance in terms of the cross entropy loss could happen just because the model learnt how to generate more fluent sentences from the checkpoint of the wiki data.
in this case we need to look at the model performance in more detail.esec fse november san francisco usa haoyu gao christoph treude and mansooreh zahedi .
bleu score evaluation sequence to sequence models have an exposure bias problem .
therefore we use the beam search method which keeps track of the top k most probable candidate words during the data generation part.
we choose the beam size k to be .
epochs were trained for the three transfer learning models.
we also took snapshots of the models after every four epochs of training.
for example for the checkpoint early model we took the checkpoints after it was trained on sw data for epochs of and .
we generate simplification text on two baselines plus all the transfer learning model checkpoints on the test set.
the generated texts will be investigated more thoroughly in later sections.
we used the bleu score to evaluate the quality of the texts generated in the last section.
bleu score measures the similarity of the generated text with its