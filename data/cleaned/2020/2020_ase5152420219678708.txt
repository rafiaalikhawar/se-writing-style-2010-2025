groot an event graph based approach for root cause analysis in industrial settings hanzhang wang zhengkai wu huai jiang yichao huang jiamu wang selcuk kopru tao xie ebay university of illinois at urbana champaign peking university email hanzwang huajiang yichhuang jiamuwang skopru ebay.com zw3 illinois.edu taoxie pku.edu.cn abstract for large scale distributed systems it is crucial to efficiently diagnose the root causes of incidents to maintain high system availability.
the recent development of microservicearchitecture brings three major challenges i.e.
complexities ofoperation system scale and monitoring to root cause analysis rca in industrial settings.
to tackle these challenges in thispaper we present g root an event graph based approach for rca.
g root constructs a real time causality graph based on events that summarize various types of metrics logs andactivities in the system under analysis.
moreover to incorporatedomain knowledge from site reliability engineering sre engi neers g root can be customized with user defined events and domain specific rules.
currently g root supports rca among real production services and is actively used by the sre teams in ebay a global e commerce system serving more than159 million active buyers per year.
over months we collect a data set containing labeled root causes of real productionincidents for evaluation.
the evaluation results show that g root is able to achieve top accuracy and top accuracy.
toshare our experience in deploying and adopting rca in industrialsettings we conduct a survey to show that users of g root find it helpful and easy to use.
we also share the lessons learnedfrom deploying and adopting g root to solve rca problems in production environments.
index t erms microservices root cause analysis aiops observability i. i ntroduction since the emergence of microservice architecture it has been quickly adopted by many large companies such as amazon google and microsoft.
microservice architecture aims toimprove the scalability development agility and reusability ofthese companies business systems.
despite these undeniablebenefits different levels of components in such a systemcan go wrong due to the fast evolving and large scale natureof microservices architecture .
even if there are minimalhuman induced faults in code the system might still be at riskdue to anomalies in hardware configurations etc.
therefore itis critical to detect anomalies and then efficiently analyze theroot causes of the associated incidents subsequently helpingthe system reliability engineering sre team take furtheractions to bring the system back to normal.
in the process of recovering a system it is critical to conduct accurate and efficient root cause analysis rca thesecond one of a three step process.
in the first step anomalies tao xie is also affiliated with key laboratory of high confidence software technologies peking university ministry of education china.
hanzhang wang is the corresponding author.are detected with alerting mechanisms based on monitoring data such as logs metrics key performanceindicators kpis or a combination thereof .
in the second step when the alerts are triggered rcais performed to analyze the root cause of these alerts andadditional events and to propose recovery actions from theassociated incident .
rca needs to considermultiple possible interpretations of potential causes for theincident and these different interpretations could lead todifferent mitigation actions to be performed.
in the last step the sre teams perform those mitigation actions and recoverthe system.
based on our industrial sre experiences we find that rca is difficult in industrial practice due to three complexities particularly under microservice settings operational complexity.
for large scale systems thereare typically centered aka infrastructure sre and do main aka embedded sre engineers .
their com munication is often ineffective or limited under the mi croservice scenarios due to a more diversified tech stack granular services and shorter life cycles than traditionalsystems.
the knowledge gap between the centered sreteam and the domain sre team gets further enlargedand makes rca much more challenging.
centered sreengineers have to learn from domain sre engineerson how the new domain changes work to update thecentralized rca tools.
thus adaptive and customizablerca is required instead of one size fits all solutions.
scale complexity.
there could be thousands of ser vices simultaneously running in a large microservicesystem resulting in a very high number of monitoringsignals.
a real incident could cause numerous alerts tobe triggered across services.
the inter dependencies andincident triaging between the services are proportionallymore complicated than a traditional system .
to detectroot causes that may be distributed and many steps awayfrom an initially observed anomalous service the rcaapproach must be scalable and very efficient to digesthigh volume signals.
monitoring complexity.
a high quantity of observabilitydata types metrics logs and activities need to be mon itored stored and processed such as intra service andinter service metrics.
different services in a system may 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee produce different types of logs or metrics with different patterns.
there are also various kinds of activities suchas code deployment or configuration changes.
the rcatools must be able to consume such highly diversified andunstructured data and make inferences.
to overcome the limited effectiveness of existing approaches as mentioned insection ii in industrial settings due to the aforementionedcomplexities we propose g root an event graph based rca approach.
in particular g root constructs an event causality graph whose basic nodes are monitoring events such asperformance metric deviation events status change events and developer activity events.
these events carry detailedinformation to enable accurate rca.
the events and thecausalities between them are constructed using specified rulesand heuristics reflecting domain knowledge .
in contrast tothe existing fully learning based approaches g root provides better transparency and interpretability.
such interpretability is critical in our industrial settings becausea graph based approach can offer visualized reasoning withcausality links to the root cause and details of every eventinstead of just listing the results.
besides our approach canenable effective tracking of cases and targeted detailed im provements e.g.
by enhancing the rules and heuristics usedto construct the graph.
g root has two salient advantages over existing graphbased approaches fine granularity events as basic nodes .
first unlike existing graph based approaches which directly use ser vices or hosts vms as basic nodes g root constructs the causality graph by using monitoring eventsas basic nodes.
graphs based on events from the ser vices can provide more accurate results to address themonitoring complexity.
second for the scale complexity g root can dynamically create hidden events or additional dependencies based on the context such as addingdependencies to the external service providers and theirissues.
third to construct the causality graph g root takes the detailed contextual information of each eventinto consideration for analysis with more depth.
also helps g root incorporate sre insights with the context details of each event to address the operationalcomplexity.
high diversity a wide range of event types supported .
first the causality graph in g root supports various event types such as performance metrics status logs anddeveloper activities to address the monitoring complexity.this multi scenario graph schema can directly boost therca coverage and precision.
for example g root is able to detect a specific configuration change on a serviceas the root cause instead of performance anomaly symp toms thus reducing triaging efforts and time to recovery ttr .
second g root allows the sre engineers to introduce different event types that are powered by differentdetection strategies or from different sources.
for therules that decide causality between events we designa grammar that allows easy and fast implementationsof domain specific rules narrowing the knowledge gapof the operational complexity.
third g root provides a robust and transparent ranking algorithm that can digestdiverse events improve accuracy and produce resultsinterpretable by visualization.
to demonstrate the flexibility and effectiveness of g root we evaluate it on ebay s production system that serves morethan million active users and features more than services deployed over three data centers.
we conduct experi ments on a labeled and validated data set to show that g root achieves top accuracy and top accuracy for 952real production incidents collected over months.
further more g root is deployed in production for real time rca and is used daily by both centered and domain sre teams with the achievement of top accuracy in action.
finally the end to end execution time of g root for each incident in our experiments is less than seconds demonstrating the highefficiency of g root .
we report our experiences and lessons learned when using g root to perform rca in the industrial e commerce system.
we survey among the sre users and developers of groot who find g root easy to use and helpful during the triage stage.
meanwhile the developers also find the g root design to be desirable to make changes and facilitate newrequirements.
we also share the lessons learned from adopting g root in production for sre in terms of technology transfer and adoption.
in summary this paper makes four main contributions an event graph based approach named g root for root cause analysis tackling challenges in industrial settings.
implementation of g root in an rca framework for allowing the sre teams to instill domain knowledge.
evaluation performed in ebay s production environ ment with more than services for demonstrating g root s effectiveness and efficiency.
experiences and lessons learned when deploying andapplying g root in production.
ii.
r ela ted work anomaly detection.
anomaly detection aims to detect potential issues in the system.
anomaly detection approachesusing time series data can generally be categorized into threetypes batch processing and historical analysis such assurus machine learning based such as donut usage of adaptive concept drift such as stepwise .
g root currently uses a combination of manually written thresholds statistical models and machine learning ml algorithms to detect anomalies.
since our approach is event driven as long as fairly accurate alerts are generated g root is able to incorporate them.
root cause analysis.
traditional rca approaches e.g.
adtributor and hotspot find the multi dimensionalcombination of attribute values that would lead to certainquality of service qos anomalies.
these approaches are 420effective at discrete static data.
once there are continuous data introduced by time series information these approaches wouldbe much less effective.
to tackle these difficulties there are two categories of approaches based on ml and graph respectively.
ml based rca.
some ml based approaches use features such as time series information and features ex tracted using textual and temporal information .
some otherapproaches conduct deep learning by first constructingthe dependency graph of the system and then representingthe graph in a neural network.
however these ml basedapproaches face the challenge of lacking training data.
gan etal.
proposed seer to make use of historical tracking data.although seer also focuses on the microservice scenario itis designed to detect qos violations while lacking supportfor other kinds of errors.
there is also an effort to useunsupervised learning such as gan but it is generallyhard to simulate large complicated distributed systems to givemeaningful data.
graph based rca.
a recent survey on rca approaches categorizes more than rca algorithms by more than10 theoretical models to represent the relationships betweencomponents in a microservice system.
nguyen et al.
proposed fchain which introduces time series informationinto the graph but they still use server vm as nodes in thegraph.
chen et al.
proposed causeinfer which constructsa two layered hierarchical causality graph.
it applies metricsas nodes that indicate service level dependency.
schoenfischet al.
proposed to use markov logic network to expressconditional dependencies in the first order logic but still builddependency on the service level.
lin et al.
proposedmicroscope which targets the microservice scenario.
it buildsthe graph only on service level metrics so it cannot get fulluse of other information and lacks customization.
brandon etal.
proposed to build the system graph using metrics logs and anomalies and then use pattern matching against alibrary to identify the root cause.
however it is difficult toupdate the system to facilitate the changing requirements.
wuet al.
proposed microrca which models both servicesand machines in the graph and tracks the propagation amongthem.
it would be hard to extend the graph from machines tothe concept of other resources such as databases in our paper.
as mentioned in section i by using the event graph g root mainly overcomes the limitations of existing graphbased approaches in two aspects build a more accurateand precise causality graph use the event graph based model allow adaptive customization of link construction rules toincorporate domain knowledge in order to facilitate the rapidrequirement changes in the microservice scenario.
our g root approach uses a customized page rank algorithm in the event ranking and can also be seen as an unsu pervised ml approach.
therefore g root is complementary to other ml approaches as long as they can accept our eventcausality graph as a feature.
settings and scale.
the challenges of operational scale and monitoring complexities are observed especially beingt able i the scale of experiments in existing rca ap proaches evaluations qps queries per second approach year scale validated on real incidents?
fchain vms no causeinfer 20services on servers no microscope 36services qps no apg services on vms no seer services on servers partially microrca 13services qps no rca graph services on vms no causality rca services no substantial in the industrial settings.
hence we believe thatthe target rca approach should be validated at the enterprisescale and against actual incidents for effectiveness.
table i lists the experimental settings and scale in existing rca approaches evaluations.
all the listed existingapproaches are evaluated in a relatively small scenario.
incontrast our experiments are performed upon a system con taining production services on hundreds of thousands ofvms.
on average the sub dependency graph constructed insection iv a of our service based data set is already .5services more than the total number in any of the listedevaluations.
moreover out of the listed approaches areevaluated under simulative fault injection on top of existingbenchmarks such as rubis which cannot represent real worldincidents seer collects only the real world results withno validations.
our data set contains actual incidentscollected from real world settings.
iii.
m otiv a ting examples in this section we demonstrate the effectiveness of eventbased graph and adaptive customization strategies with twomotivating examples.
figure shows an abstracted real incident example with the dependency graph and the corresponding causality graph con structed by g root .
the checkout service of our e commerce system suddenly gets an additional latency spike due to a codedeployment on the service e. the service monitor is reporting api call timeout detected by the ml based anomaly detection system.
the simplified sub dependency graph consisting of 6services is shown in figure 1a.
the initial alert is triggeredon the checkout entrance service.
the other nodes service are the internal services that the checkout service directly or indirectly depends on.
the color of the nodes in figure 1aindicates the severity count of anomalies alerts reported oneach service.
we can see that service b is the most severe one as there are two related alerts on it.
the traditional graph based approach usually takes into account only thegraph between services in addition to the severity informationon each service.
if the traditional approach got applied onfigure 1a either service b service d o rservice e could be a potential root cause and service b would have the highest possibility since it has two related alerts.
such results are notuseful to the sre teams.
g root constructs the event based causality graph as shown in figure 1b.
the events in each service are used as the nodeshere.
we can see that the api call timeout issue in checkout is possibly caused by api call timeout inservice a which is a dependency graph b causality graph fig.
motivating example of event causality graph fig.
example of event type addition further caused by latency spike indatacenter a ofservicec.g root further tracks back to find that it is likely caused by latency spike inservice e which happens in the same data center.
finally g root figures out that the most probable root cause is a recent code deployment event in service e. the sre teams then could quickly locate the root cause and roll back this code deployment followed by further investigations.
there are no casual links between events in service b and service a since no causal rules are matched.
the api call timeout event is less likely to depend on the event type high cpu and high gc.
therefore the inference can eliminate service b from possible root causes.
this elimination shows the benefit of the event based graph.
note that there is anotherevent latency spike inservice d but not connected to latency spike inservice c in the causality graph.
the reason is that the latency spike event in service c happens in datacentera not datacenter b. figures and show how sre engineers can easily change g root to adapt to new requirements by updating the events and rules.
in figure sre engineers want to add a newtype of deployment activity ml model deployment.
usually the sre engineers first need to select the anomaly detectionmodel or set their own alerts and provide alert activity datasources for the stored events.
in this example the event can bedirectly fetched from the ml model management system.
then g root also requires related properties e.g.
the detection time range to be set for the new event type.
lastly thesre engineers add the rules for building the causal linksbetween the new event type and existing ones.
the bluebox in figure shows the rule which denotes the edge fig.
example of event and rule update fig.
workflow of g root direction target event and target service self upstream anddownstream dependency .
figure shows a real world example of how g root is able to incorporate sre insights and knowledge.
more specifically sre engineers would like to change the rules to allow g root to distinguish the latency spikes from different data centers.as an example in figure 1b latency spike events propagate only within the same data center.
during g root development sre engineers could easily add new property datacenter to the latency spike event.
then they add the corresponding conditional rules to be differentiated with the basic rulesin figure .
in conditional rules links are constructed onlywhen the specified conditions are satisfied.
422iv .
a pproach figure shows the overall workflow of g root .
the triggers for using g root are usually alert s from automated anomaly detection or sometimes an sre engineer s suspicion.
there are three major steps constructing the servicedependency graph constructing the event causality graph androot cause ranking.
the outputs are the root causes ranked bythe likelihood.
to support fast human investigation experience we build an interactive ui as shown in figure the servicedependency events with causal links and additional detailssuch as raw metrics or the developer contact of a codedeployment event are presented to the user for next steps.as an offline part of human investigation we label collect adata set perform validation and summarize the knowledge forfurther improvement on all incidents on a daily basis.
a. constructing service dependency graph the construction of the service dependency graph starts with the initial alerted or suspicious service s denoted as i. for example in figure 1a i checkout .
ican contain multiple services based on the range of the trigger alerts or suspicions.
we maintain domain service lists where domain level alerts can be triggered because there is no clear service level indication.
at the back end g root maintains a global service dependency graph gglobal via distributed tracing and log analysis.
the directed edge from nodes atob two services or system components in the dependency graph indicates a serviceinvocation or other forms of dependency.
in figure 1a theblack arrows indicate such edges.
bi directional edges andcycles between the services can be possible and exist.
in thiswork the global dependency graph is updated daily.
the service dependency sub graph gis constructed using g global andi.
an extended service list lis first constructed by traversing each service in iovergglobal for a radius range r. each service u lcan be traversed by at least one servicev iwithinrsteps l u v i dist u v r or dist v u r .
then the service dependency subgraph gis constructed by the nodes in land the edges between them ingglobal .
in our current implementation ris set to since this dependency graph may be dynamically extended inthe next steps based on events detail for longer issue chainsor additional dependencies.
b. constructing event causality graph in the second step g root collects all supported events for each service in gand constructs the causal links between events.
collecting events table ii presents some example event types and detection techniques for g root s production implementation.
for detection techniques de facto indicates that the event can be directly collected via a specific api orstorage.
the detection either runs passively in the back endto reduce delay and improve accuracy or runs actively foronly the services within the dependency graph range to saveresources.t able ii list of example event types used in g root type event type detection technique performance metricshigh gc overhead rule based high cpu usage rule based latenc y spike statistical model tps spike statistical model database anomaly ml model business metric anomaly ml model status logswebapi error statistical model internal error statistical model serviceclient error statistical model bad host ml model developer activitiescode deployment de facto configuration change de facto execute url de facto there are three major categories of events performance metrics status logs and developer activities performance metrics represent an anomaly of monitored time series metrics.
for example high cpu usage in dicates that the service is causing high cpu usage ona certain machine.
in this category most events arecontinuously and passively detected and stored.
status logs are caused by abnormal system status such as spike of http error code metrics while accessingother services endpoints.
different types of error metricsare important and supported in g root including thirdparty apis.
for example bad host indicates abnormalpatterns on some machines running the service and canbe detected by a clustering based ml approach.
developer activities are the events generated when a certain activity of developers is triggered such as codedeployment and config change.
in groot there are more than a dozen event types such aslatency spike as listed in the column of table ii.
each event type is characterized by three aspects name indicates the name of this event type lookbackperiod indicates the time range to look back from the time whenthe use of g root is triggered for collecting events of this event type propertytype indicates the types of the properties that an event of this event type should hold.properttype is characterized by a vector of pairs each of which indicates the string type for a property s name andthe primitive type for the property s value such as string integer and float.
formally an event type is defined as a tuple et name lookbackperiod propertytype where propertytype string type ... string typen n is the number of properties that an event of this event typeholds .
each event of a certain event type et is characterized by four aspects service indicates the service name that the event belongs to type indicates et sname starttime indicates the time when the event happens properties indicates the properties that the event holds.
formally an event is definedas a tuple e service type starttime properties whereproperties is an instantiation of et spropertytype.
for example in figure the generated event for latency spike in datacenter a inservice c would be service c prime prime latency spike prime prime datacenter prime prime dc prime prime ... .
423fig.
example of dynamic rule constructing causal link after collecting all events on all services in g in this step causal links between these events are constructed for rca ranking.
the causal links red arrows in figure 1b are such examples.
a causal link represents thatthe source event can possibly be caused by the target event.sre knowledge is engineered into rules and used to createcausal links between the pairs of events.
a rule for constructing a causal link is defined as a tuple rule target type source events target events direction target service condition condition can be optionally specified .
target type indicates the type of the rule being eitherstatic ordynamic explained further later .
source events indicates the type of the causal link s source event source events are listed in the names of the rules shown in figures and .
target events indicates the type of the causal link s target event.
direction indicates the direction of the casual link between the target event andsource event.
target service indicates the service that the target event should belong to.
note that target service in static rules can be self which indicates that the target event would be within the same service as the source event oroutgoing incoming which indicates that the target event would belong to the downstream upstream services of theservice that the source event belongs to in g. there are two categories of special rules.
the first category is dynamic rules i.e.
rules whose target type is set todynamic to support dynamic dependencies.
here target service does not indicate any of the three possible options listed earlier but indicates the name of the targetservice that g root would need to create.
for example live db dependencies are not available due to different tech stacksand high volume.
in figure a db issue db markdown isshown in service a. based on the listed dynamic rule g root creates a new service db ing a new event issues that belongs to db and a causal link between the two events.
in practice the sre teams use dynamic rules to cover alot of third party services and database issues since the livedependencies are not easy to maintain.
the second category of special rules is conditional rules.
conditional rules are used when some prerequisite conditions should be satisfied before a certain causal link is created.
inthese rules condition is specified with a boolean predicate.
as shown in figure the sre teams believe latency spike events from different services are related only when bothevents happen within the same data center.
based on thisobservation g root would first evaluate the predicate incondition and build only the causal link when the predicate is true.
a conditional rule overwrites the basic rule on thesame source target event pair.
when constructing causal links g root first applies the dynamic rules so that dynamic dependencies and events are first created at once.
then for every event in the initial services denoted as i if the rule conditions are satisfied one or many causal links are created from this event to other events from thesame or upstream downstream services.
when a causal link iscreated the step is repeated recursively for the target event asa new origin to create new causal links.
after no new causallinks are created the construction of the event causality graphis finished.
c. root cause ranking finally g root ranks and recommends the most probable root causes from the event causality graph.
similar to how search engines infer the importance of pages by page links we customize the pagerank algorithm to calculate theroot cause ranking the customized algorithm is named asgrootrank.
the input is the event causality graph from theprevious step.
each edge is associated with a weighted scorefor weighted propagation.
the default value is set as and is set lower for alerts with high false positive rates.
based on the observation that dangling nodes are more likely to be the root cause we customize the personalizationvector as p n fnorpd wherepdis the personalization score for dangling nodes and pnis for the remaining nodes andfnis a value smaller than to enhance the propagation between dangling nodes.
in our work the parameter setting isf n .
.
max iter which are parameters for the pagerank algorithm .
figure illustrates an example.
thegrey circles are the events collected from three services andone database.
the grey arrows are the dependency links andthe red ones are the causal links with the weight of .
both of the pagerank and grootrank algorithms detect event db issue as the root cause which is expected and correct.however the pagerank algorithm ranks event 4higher than event .
butevent 3ofservice c is more likely to be the second most possible root cause besides event because the scores on dangling nodes are propagated to all others equallyin each iteration.
we can see that event 3is correctly ranked as second using the grootrank algorithm.
the second step of grootrank is to break the tied results from the previous step.
the tied results are due to the fact thatthe event graph can contain multiple disconnected sub graphswith the same shape.
we design two techniques to untie theranking for each joint event the access distance sum is calculated from the initial anomaly service s to the servicewhere the event belongs to.
if any access is notreachable the distance is set as d m wheredmis the maximum possible distance.
the one with shorteraccess distance sum would be ranked higher and viceversa.
figure presents an example where service a and service b are both initial anomaly services.
since 424fig.
example of personalization vector customization fig.
example of using access distance to untie the ranking results groot suspects that event 2is caused by either event orevent 1with the same weight.
the scores of event andevent 1are tied.
then event 3has a score of i.e.
andevent 1has a score of i.e.
since it is not reachable by service b .
therefore event 3is ranked first and logical.
for the remaining joint results with the same access distances g root continues to untie by using the historical root cause frequency of the event types under the sametrigger conditions e.g.
checkout domain alerts .
thisfrequency information is generated from the manuallylabeled dataset.
a more frequently occurred root causetype is ranked higher.
d. rule customization management while g root users create or update the rules there could be overlaps inconsistencies or even conflicts being introducedsuch as the example in figure .
g root uses two graphs to manage the rule relationships and avoid conflicts for users.one graph is to represent the link rules between events inthe same service same graph while the other is to representlinks between different services diff graph .
the nodes inthese two graphs are the event types defined in section iv b.there are three statuses between each directional pair ofevent types no rule only basic rule and conditionalrule since it overwrites the basic rule .
in same graph g root does not allow self loop as it does not build links between an event and itself.
when rule change happens existing rules are enumerated to build edges in same graph and diff graph based on target events andtarget service .
based on the users operation of remove a rule g root removes the corresponding edge on the graphs add update a rule g root checks whether there are existing edges between the givenevent types and then warns the users for possible overwrites.
ifthere are no conflicts g root just adds updates edges between the event types.
after all changes g root extracts the rules from the graphs by converting each edge to a single rule.
these rules areautomatically implemented and then tested against our labeleddata set.
the g root users need to review the changes with validation reports before the changes go online.
v. e v alua tion we evaluate g root in two aspects effectiveness accuracy which assesses how accurate g root is in detecting and ranking root causes and efficiency which assesses how long it takes for g root to derive root causes and conduct endto end analysis in action.
particularly we intend to address thefollowing research questions rq1.
what are the accuracy and efficiency of g root when applied on the collected dataset?
rq2.
how does g root compare with baseline approaches in terms of accuracy?
rq3.
what are the accuracy and efficiency of g root in an end to end scenario?
a. evaluation setup to evaluate g root in a real world scenario we deploy and apply g root in ebay s e commerce system that serves more than million active buyers.
in particular we apply g root upon a microservice ecosystem that contains over ser vices on three data centers.
these services are built on differenttech stacks with different programming languages includingjava python node.js etc.
furthermore these services interactwith each other by using different types of service protocols including http grpc and message queue.
the distributedtracing of the ecosystem generates 147b traces on average perday.
data set the sre teams at ebay help collect a labeled data set containing incidents over months jan apr .
each incident data contains the input required by g root e.g.
dependency snapshot and events with details and the root cause manually labeled by the sre teams.
theseincidents are grouped into two categories business domain incidents.
these incidents are detected mainly due to their business impact.
for example endusers encounter failed interactions and business or cus tomer experience is impacted similar to the example infigure .
service based incidents.
these incidents are detected mainly due to their impact on the service level similarto the example in figure .
an internal incident may get detected early and then likely get categorized as a service based incident or evensolved directly by owners without records.
on the otherhand infrastructure level issues or issues of external serviceproviders e.g.
checkout and shipping services may not getdetected until business impact is caused.
there are business domain incidents and servicebased incidents in the data set.
for each incident the root cause 425t able iii accuracy of rca by g root and baselines groot nai ve non adapti ve top top top top top top service based business domain combined is manually labeled validated and collected by the sre teams who handle the site incidents everyday.
for a case with mul tiple interacting causes only the most actionable influentialevent is labelled as the root cause for the case.
these actualroot causes and incident contexts serve as the ground truth inour evaluation.
g root setup the g root production system is deployed as three microservices and federated in three datacenters with nine core cpus 20gb ram pods each onkubernetes.
baseline approaches in order to compare g root with other related approaches we design and implement two base line approaches for the evaluation naive approach.
this approach directly uses the constructed service dependency graph section iv a .
theevents are assigned a score by the severeness of theassociated anomaly.
then a normalized score for eachservice is calculated summarizing all the events relatedto the service.
lastly the pagerank algorithm is used tocalculate the root cause ranking.
non adaptive approach.
this approach is not contextaware.
it replaces all special rules i.e.
conditional anddynamic ones with their basic rule versions.
its otherparts are identical to g root .
the non adaptive approach can be seen as a baseline forreflecting a group of graph based approaches e.g.
cau seinfer and microscope .
these approaches alsospecify certain service level metrics but lack the context awarecapabilities of g root .
because the tools for these approaches are not publicly available we implement the non adaptiveapproach to approximate these approaches.
b. evaluation results rq1 table iii shows the results of applying g root on the collected data set.
we measure both top and top accuracy.
the top and top accuracy is calculated as thepercentage of cases where their ground truth root cause isranked within top and top respectively in g root s results.
groot achieves high accuracy on both incident categories.
for example for business domain incidents g root achieves top accuracy.
the unsuccessful cases that g root ranks the root cause after top are mostly caused by missing event s .
more thanone third of these unsuccessful cases have been addressed byadding necessary events and corresponding rules over time.for example initially we had only an event type of generalerror spike which mixes different categories of errors and thuscauses high false positive rate.
we then have designed differentevent types for each category of the error metrics includingvarious internal and client api errors .
in many cases thatt able iv comparison of g root results on the dataset and end to end scenario service based business domain dataset end to end dataset end to end top accuracy top accuracy average runtime cost .06s .16s .98s .98s maximum runtime cost .69s .56s .14s .61s groot ranks the root cause after top the labeled root cause is just one of the multiple co existing root causes.
but forfairness the sre teams label only a single root cause in eachcase.
according to the feedback from the sre teams g root still facilitates the rca process for these cases.
our results show that the runtime cost of applying g root is relatively low.
for a service based incident the averageruntime cost of g root is .06s while the maximum is .69s.
for a business domain incident the average runtime cost is0.98s while the maximum is .14s.
rq2 we additionally apply the baseline approaches on the data set.
table iii also shows the evaluation results.the results show that the accuracy of g root is substantially higher than that of the baseline approaches.
in terms of thetop accuracy g root achieves compared with and of the naive and non adaptive approaches respectively.
interms of the top accuracy g root achieves compared with and of the naive and non adaptive approaches respectively.
the naive approach performs worst in all settings because it blindly propagates the score at service levels.
the accuracyof the non adaptive approach is much worse for businessdomain incidents.
the reason is that for a business domainincident it often takes a longer propagation path since theincident is triggered by a group of services and new dynamicdependencies may be introduced during the event collection causing more inaccuracy for the non adaptive approach.
therecan be many non critical or irrelevant error events in an actualproduction scenario aka soft errors.
we suspect that thesenon critical or irrelevant events may be ranked higher by thenon adaptive approach since they are similar to injected faultsand hard to be distinguished from the actual ones.
g root uses dynamic and conditional rules to discover the actualcausal links building fewer links related to such non criticalor irrelevant events for leading to higher accuracy.
rq3 to evaluate g root under an end to end scenario we apply g root upon actual incidents in action.
table iv shows the results.
the accuracy has a decrease of up to9 percentage points in the end to end scenario with somefailures caused by production issues such as missing dataand service storage failures.
in addition the runtime cost isincreased by up to nearly seconds due to the time spent onfetching data from different data sources e.g.
querying theevents for a certain time period.
vi.
e xperience groot currently supports daily sre work.
figure shows al i v eg root s bird s eye view ui on an actual simple checkout incident.
service chas the root cause errorspike 426fig.
g root ui in production and belongs to an external provider.
although the domain serviceaalso carries an error spike and gets impacted g root correctly ignores the irrelevant deployment event which hasno critical impact.
the events on care virtually created based on the dynamic rule.
note that all causal links yellow in theui indicate is cause of being the opposite of is caused by as described in section iv b to provide more intuitive ui forusers to navigate through.
g root visualizes the dependency and event causality graph with extra information such as anerror message.
the sre teams can quickly comprehend theincident context and derived root cause to investigate g root further.
a mouseover can trigger event enrichment based onthe event type to present details such as raw metrics and otheradditional information.
we next share two major kinds of experience feedback from g root users and developers reflecting the general experience of two groups domain sreteams who use g root to find the root cause and a centered sre team who maintains g root to facilitate new requirements.
lessons learned representing the lessons learned fromdeploying and adopting g root in production for the realworld rca process.
a. feedback from groot users and developers we invite the sre members who use g root for rca in their daily work to the user survey.
we call them users in thissection.
we also invite different sre members responsible formaintaining g root to the developer survey.
we call them developers in this section.
in total there are users and 6developers 1who respond to the surveys.
for the user survey we ask users the following questions questions have the same choices as question1 question .
when g root correctly locates the root cause how does it help with your triaging experience?answer choices helpful somewhat helpful nothelpful misleading .
question .
when g root correctly locates the root cause how does it save extend your or the team s triaging 1the g root researchers and developers who are authors of this paper are excluded.01234question 5question 4question 3question 2question a from g root users01234question 5question 4question 3question 2question b from g root developers fig.
survey results time?
detection and remediation time not included answer choices lots of time saved some timesaved no time saved waste time instead .
question .
based on your estimation how much triage time g root would save on average when it correctly locates the root cause?
detection and remediation timenot included answer choices more than n a .
question .
when g root correctly locates the root cause do you find that the result graph provided by groot helps you understand how and why the incident happens?
question .
when g root does not correctly locate the root cause does the result graph make it easier for yourinvestigation of the root cause?
figure 9a shows the results of the user survey.
we can see that most users find g root very useful to locate the root cause.
the average score for question is .
and outof participants find g root very helpful.
as for question g root saves the triage time by .
even in cases that g root cannot correctly locate the root cause it is still helpful to provide information for further investigation withan average score of .
in question .
for the developer survey we ask the developers the following questions questions have the same choicesas question question .
overall how convenient is it to change and customize events rules domains while using g root ?
answer choices convenient somewhat convenient not convenient difficult .
question .
how convenient is it to change customize event models while using g root ?
question .
how convenient is it to add new domains while using g root ?
question .
how convenient is it to change customize causality rules while using g root ?
question .
how convenient is it to change customize groot compared to other sre tools?
figure 9b shows the results of the developer survey.
overall most developers find it convenient to make changes on andcustomize events rules domains in g root .
b. lessons learned in this section we share the lessons learned in terms of technology transfer and adoption on using g root in production environments.
427embedded in practice.
to build a successful rca tool in practice it is important to embed the r d efforts in the live environment with sre experts and users.
we have a minuteroutine meeting daily with an sre team to manually test andreview every site incident.
in addition we actively reach outto the end users for feedback.
for example the users foundour initial ui hard to understand.
based on their suggestions we have introduced alert enrichment with the detailed contextof most events raw metrics and links to other tools for thenext steps.
we also make the ui interactive and build userguides training videos and sections.
as a result g root has become increasingly practical and well adopted in practice.
webelieve that r d work on observability should be incubatedand grown within daily sre environments.
it is also vitalto bring developers with rich rca experience into the r dteam.
v ertical enhancements.
high confidence and automated vertical enhancements can empower great experiences.
g root is enhanced and specialized in critical scenarios such as groupedrelated alerts across services or critical business domain issues and large scale scenarios such as infrastructure changes ordatabase issues.
furthermore the end to end automation isalso built for integration and efficiency with anomaly detec tion rca and notification.
for notification domain businessanomalies and diagnostic results are sent through communi cation apps e.g.
slack and email for better reachability andexperience.
within months of r d g root now supports business domains and sub domains of the company.
onaverage g root ui supports more than active internal users and the service sends thousands of results every month.most of these usages are around the vertical enhancements.
data and tool reliability.
reliability is critical to g root itself and requires a lot of attention and effort.
for example ifa critical event is missing g root may infer a totally different root cause which would mislead users.
we estimate the alertaccuracy to be greater than .
in order to be useful.
recall iseven more important since g root can effectively eliminate false positive alerts based on the casual ranking.
since thereare hundreds of different metrics supported in g root w e spend time to ensure a robust back end by adding partialand dynamic retry logic and high efficiency cache.
g root s unsuccessful cases can be caused by imperfect data flawedalgorithms or simply code defects.
to better trace the reasonbehind each unsuccessful case we add a tracing component.every g root request can be traced back to atomic actions such as retrieving data data cleaning and anomaly detectionvia algorithms.
trade off among models.
the accuracy and scalability tradeoff among anomaly detection models should be carefullyconsidered and tested.
in general some algorithms such asdeep learning based or ensemble models are more adaptiveand accurate than typical ones such as traditional ml orstatistical models.
however the former requires more com putation resources operational efforts and additional systemcomplexities such as training or model fine tuning.
due to theactual complexities and fast evolving nature of our context it is not possible to scale each model e.g.
deep learning based models nor have it deeply customized for every metricat every level.
therefore while selecting models we mustmake careful trade off in aspects such as accuracy scalability efficiency effort and robustness.
in general we first setdifferent acceptance levels by analyzing each event s impactand frequency and then test different models in staging andpick the one that is good enough.
for example a few alertssuch as high thread usage are defined by thresholds and workjust fine even without a model.
some alerts such as serviceclient error are more stochastic and require coverage on everymetric of every service and thus we select fast and robuststatistical models and actively conduct detection on the fly.
phased incorporation of ml.
in the current industrial settings ml powered rca products still require effectiveknowledge engineering.
due to the higher complexity andlower signal to noise ratio of real production incidents manyexisting approaches cannot be applied in practice.
we be lieve that the knowledge engineering capabilities can facilitateadoption of technologies such as aiops.
therefore g root is designed to be highly customizable and easy to infuse sreknowledge and to achieve high effectiveness and efficiency.moreover a multi scenario rca tool requires various andinterpretable events from different detection strategies.
auto ml based anomaly detection or unsupervised rca for largeservice ecosystems is not yet ready in such context.
as forthe path of supervised learning the training data is tricky tolabel and vulnerable to potential cognitive bias.
lastly theend users often require complete understanding to fully adoptnew solutions because there is no guarantee of correctness.many recent ml algorithms e.g.
ensemble and deep learning lack interpretability.
via the knowledge engineering and graphcapabilities g root is able to explain diversity and causality between ml model driven and other types of events.
movingforward we are building a white box deep learning approachwith causal graph algorithms where the causal link weightsare parameters and derivable.
vii.
c onclusion in this paper we have presented our work around root cause analysis rca in industrial settings.
to tackle threemajor rca challenges complexities of operation systemscale and monitoring we have proposed a novel event graph based approach named g root that constructs a real time causality graph for allowing adaptive customization.
g root can handle diversified anomalies and activities from the systemunder analysis and is extensible to different approaches ofanomaly detection or rca.
we have integrated g root into ebay s large scale distributed system containing more than5 microservices.
our evaluation of g root on a data set consisting of real production incidents shows that groot achieves high accuracy and efficiency across different scenarios and also largely outperforms baseline graph basedapproaches.
we also share the lessons learned from deployingand adopting g root in production environments.
428references a. balalaie a. heydarnoori and p .
jamshidi microservices architecture enables devops migration to a cloud native architecture ieee software vol.
no.
pp.
.
m. sol v .
munt s mulero a. i. rana and g. estrada survey on models and techniques for root cause analysis arxiv preprint arxiv .
.
n. zhao p .
jin l. wang x. y ang r. liu w .
zhang k. sui and d. pei automatically and adaptively identifying severe alerts for online service systems in proceedings of ieee conference on computer communications.
ieee pp.
.
j. xu y .
wang p .
chen and p .
wang lightweight and adaptive service api performance monitoring in highly dynamic cloud environment inproceedings of ieee international conference on services computing.
ieee pp.
.
l. tang t. li f. pinel l. shwartz and g. grabarnik optimizing system monitoring configurations for non actionable alerts in proceedings of ieee network operations and management symposium.
ieee pp.
.
m. k. aguilera j. c. mogul j. l. wiener p .
reynolds and a. muthitacharoen performance debugging for distributed systems of blackboxes acm sigops operating systems review vol.
no.
pp.
.
h. zawawy k. kontogiannis and j. mylopoulos log filtering and interpretation for root cause analysis in proceedings of ieee international conference on software maintenance.
ieee pp.
.
v .
nair a. raul s. khanduja v .
bahirwani q. shao s. sellamanickam s. keerthi s. herbert and s. dhulipalla learning a hierarchicalmonitoring system for detecting and diagnosing service issues inproceedings of the 21th acm sigkdd international conference onknowledge discovery and data mining.
acm pp.
.
s. lu b. rao x. wei b. tak l. wang and l. wang logbased abnormal task detection and root cause analysis for spark inproceedings of ieee international conference on web services .
ieee pp.
.
y .
gan y .
zhang k. hu d. cheng y .
he m. pancholi and c. delimitrou seer leveraging big data to navigate the complexity of perfor mance debugging in cloud microservices in proceedings of the 24th international conference on architectural support for programminglanguages and operating systems.
acm pp.
.
j. mace r. roelke and r. fonseca pivot tracing dynamic causal monitoring for distributed systems in proceedings of the 25th acm symposium on operating systems principles.
acm pp.
.
h. xu w .
chen n. zhao z. li j. bu z. li y .
liu y .
zhao d. pei y .
feng et al.
unsupervised anomaly detection via variational autoencoder for seasonal kpis in web applications in proceedings of the world wide web conference.
acm pp.
.
m. ma w .
lin d. pan and p .
wang ms rank multi metric and self adaptive root cause diagnosis for microservice applications inproceedings of ieee international conference on web services .
ieee pp.
.
y .
meng s. zhang y .
sun r. zhang z. hu y .
zhang c. jia z. wang and d. pei localizing failure root causes in a microservicethrough causality inference in proceedings of ieee acm 28th international symposium on quality of service.
ieee pp.
.
l. wu j. tordsson e. elmroth and o. kao microrca root cause localization of performance issues in microservices in proceedings of ieee ifip network operations and management symposium .
ieee pp.
.
m. kim r. sumbaly and s. shah root cause detection in a serviceoriented architecture acm sigmetrics performance evaluation review vol.
no.
pp.
.
h. wang p .
nguyen j. li s. kopru g. zhang s. katariya and s. ben romdhane grano interactive graph based root cause analysisfor cloud native distributed data platform proceedings of the v ery large data base endowment vol.
no.
pp.
.
h. baek a. srivastava and j. v an der merwe cloudsight a tenantoriented transparency framework for cross layer cloud troubleshooting inproceedings of 17th ieee acm international symposium on cluster cloud and grid computing.
ieee pp.
.
g. da cunha rodrigues r. n. calheiros v .
t. guimaraes g. l. d. santos m. b. de carvalho l. z. granville l. m. r. tarouco andr.
buyya monitoring of cloud computing environments concepts solutions trends and future directions in proceedings of the 31st annual acm symposium on applied computing.
acm pp.
.
how sre teams are organized and how to get started ac cessed h. nguyen z. shen y .
tan and x. gu fchain toward black box online fault localization for cloud systems in proceedings of ieee 33rd international conference on distributed computing systems.ieee pp.
.
p .
chen y .
qi p .
zheng and d. hou causeinfer automatic and distributed performance diagnosis with hierarchical causality graph inlarge distributed systems in proceedings of ieee conference on computer communications.
ieee pp.
.
m. ma z. yin s. zhang s. wang c. zheng x. jiang h. hu c. luo y .
li n. qiu et al.
diagnosing root causes of intermittent slow queries in cloud databases proceedings of the v ery large data base endowment vol.
no.
pp.
.
j. schoenfisch c. meilicke j. von st lpnagel j. ortmann and h. stuckenschmidt root cause analysis in it infrastructures using ontologies andabduction in markov logic networks information systems vol.
pp.
.
.
brand n m. sol a. hu lamo d. solans m. s. p rez and v .
munt s mulero graph based root cause analysis for service orientedand microservice architectures journal of systems and software vol.
p. .
d. y .
y oon n. niu and b. mozafari dbsherlock a performance diagnostic tool for transactional databases in proceedings of the international conference on management of data.
acm pp.
.
v .
jeyakumar o. madani a. parandeh a. kulshreshtha w .
zeng and n. y adav explainit!
a declarative root cause analysis engine for timeseries data in proceedings of the international conference on management of data.
acm pp.
.
h. jayathilaka c. krintz and r. wolski performance monitoring and root cause analysis for cloud hosted web applications in proceedings of the 26th international conference on world wide web.
acm pp.
.
m. a. marvasti a. v .
poghosyan a. n. harutyunyan and n. m. grigoryan an anomaly event correlation engine identifying rootcauses bottlenecks and black swans in it environments vmware technical journal vol.
no.
pp.
.
j. weng j. h. wang j. y ang and y .
y ang root cause analysis of anomalies of multitier services in public clouds ieee acm transactions on networking vol.
no.
pp.
.
j. qiu q. du k. yin s. l. zhang and c. qian a causality mining and knowledge graph based method of root cause diagnosis for performanceanomaly in cloud applications applied sciences vol.
no.
p. .
surus accessed m. ma s. zhang d. pei x. huang and h. dai robust and rapid adaption for concept drift in software system anomaly detection inproceedings of ieee 29th international symposium on softwarereliability engineering.
ieee pp.
.
r. bhagwan r. kumar r. ramjee g. v arghese s. mohapatra h. manoharan and p .
shah adtributor revenue debugging in advertis ing systems in proceedings of 11th usenix symposium on networked systems design and implementation.
usenix pp.
.
y .
sun y .
zhao y .
su d. liu x. nie y .
meng s. cheng d. pei s. zhang x. qu et al.
hotspot anomaly localization for additive kpis with multi dimensional attributes ieee access vol.
pp.
.
j. lin p .
chen and z. zheng microscope pinpoint performance issues with causal graphs in micro service environments in proceedings of international conference on service oriented computing.
springer pp.
.
c. manning p .
raghavan and h. sch tze introduction to information retrieval natural language engineering vol.
no.
pp.
.