benchmarking robustness of ai enabled multi sensor fusion systems challenges and opportunities xinyu gao state key laboratory for novel software technology nanjing university chinazhijie wang university of alberta edmonton canadayang feng state key laboratory for novel software technology nanjing university china lei ma the university of tokyo japan university of alberta canadazhenyu chen state key laboratory for novel software technology nanjing university chinabaowen xu state key laboratory for novel software technology nanjing university china abstract multi sensor fusion msf based perception systems have been the foundation in supporting many industrial applications and domains such as self driving cars robotic arms and unmanned aerial vehicles.
over the past few years the fast progress in datadriven artificial intelligence ai has brought a fast increasing trend to empower msf systems by deep learning techniques to further improve performance especially on intelligent systems and their perception systems.
although quite a few ai enabled msf perception systems and techniques have been proposed up to the present limited benchmarks that focus on msf perception are publicly available.
given that many intelligent systems such as self driving cars are operated in safety critical contexts where perception systems play an important role there comes an urgent need for a more in depth understanding of the performance and reliability of these msf systems.
to bridge this gap we initiate an early step in this direction and construct a public benchmark of ai enabled msf based perception systems including three commonly adopted tasks i.e.
object detection object tracking and depth completion .
based on this to comprehensively understand msf systems robustness and reliability we design common and realistic corruption patterns to synthesize large scale corrupted datasets.
we further perform a systematic evaluation of these systems through our large scale evaluation and identify the following key findings existing ai enabled msf systems are not robust enough against corrupted sensor signals small synchronization and calibration errors can lead to a crash of ai enabled msf systems existing ai enabled msf systems are usually tightly coupled in which bugs errors from an individual sensor could result in a system crash the robustness of msf systems can be enhanced by improving fusion mechanisms.
our results yang feng and lei ma are the corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
the vulnerability of the current ai enabled msf perception systems calling for researchers and practitioners to take robustness and reliability into account when designing ai enabled msf.
our benchmark code and detailed evaluation results are publicly available at ccs concepts software and its engineering software defect analysis general and reference empirical studies .
keywords multi sensor fusion benchmarks ai systems perception systems acm reference format xinyu gao zhijie wang yang feng lei ma zhenyu chen and baowen xu.
.
benchmarking robustness of ai enabled multi sensor fusion systems challenges and opportunities.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction multi sensor fusion msf refers to the technique that combines data from multiple sources of sensors to achieve specific tasks which has been widely adopted in many real world complex systems.
the integration of information from different sensors avoids the inherent perception limitations of individual sensors and improves the system s overall performance.
over the past years msfbased perception systems have been widely used in various industrial domains and safety critical applications such as self driving cars unmanned aerial vehicles and robotic systems .
with recent advances in data driven artificial intelligence ai there comes an increasing trend in proposing deep learning dl techniques to further enable more advanced heterogeneous data processing from different sensors in order to achieve more accurate perception and prediction.
given the advantage of deep neural networks dnns in processing and extracting complex semantic information from sensors data e.g.
image point cloud ai enabled msf has been increasingly adopted in the perception systems of autonomous driving .
the rapid development of ai enabled msf systems also brings challenges and concerns.
one of the biggest concerns is the lack of a deep understanding of the current ai enabled msf s reliability.arxiv .03454v2 aug 2023esec fse december san francisco ca usa xinyu gao zhijie wang yang feng lei ma zhenyu chen and baowen xu in practice an ai enabled msf system could behave incorrectly and lead to severe accidents in safety critical contexts especially in autonomous driving .
thus it is highly desirable to enable testing analysis and systematic assessment of such intelligent systems beforehand comprehensively.
one common practice to enable such quality assurance activities in ai se communities is to establish a benchmark that enables both researchers and practitioners to perform systematic studies and develop novel techniques to better fulfill important quality requirements.
however to the best of our knowledge up to the present few benchmarks specifically designed for ai enabled msf are yet available.
it is unclear whether and to what extent the potential quality issues and risks can be how they are brought from each sensing unit and their impacts on the integration and the state of the art information fusion processes.
to bridge this gap in this paper we initiate an early step to present a benchmark and perform an empirical study of ai enabled msf perception systems.
fig.
summarizes the high level design and workflow of our benchmark construction and our empirical study in which we mainly investigate the following research questions aiming to identify the potential challenges and opportunities rq1.
how do ai enabled msf based perception systems perform against common corrupted signals?
this rq aims to investigate the potential risks of ai enabled msf systems against corrupted signals that commonly occur in the operational environments.
through a large scale evaluation on eleven types of corrupted sensor signals we find that the current ai enabled msf systems are not robust enough especially against weather condition changes.
rq2.
how sensitive is ai enabled msf when facing spatial and temporal misalignment of sensors?
in the practical open and wild environment it is almost impossible to always maintain perfect calibration or precise time synchronization of the system across sensors.
rq2 aims to investigate the sensitivity of ai enabled msf to spatial and temporal misalignment.
our experiment results reveal that even small calibration or synchronization issues could lead to abnormal behaviors of the system.
rq3.
to what extent are existing sensing components coupled of an ai enabled msf system?
a robust and reliable msf should not completely fail when one or a part of the whole sensing modules lose the source signal.
rq3 aims to investigate how ai enabled msf systems can be impacted when one source of the signal is partially completely lost.
overall we find that the tightly coupled architecture of ai enabled msf systems exhibits less robustness against signal loss.
rq4.
what is the weakness of different ai enabled msf mechanisms and is it possible to repair them?
rq4 aims to investigate the unique advantages of each fusion mechanism and potential opportunities for improving the robustness of aienabled msf systems.
our results demonstrate that deep fusion is more robust in some cases however weak and late fusion can be easier to be repaired in terms of robustness against corruption patterns.
to sum up this work makes the following contributions benchmark .
we initiate to create an early public benchmark of ai enabled msf based perception systems.
this provides acommon ground for the study and analysis of ai enabled msf systems robustness and enables future quality assurance research in this direction.
empirical study .
based on the benchmark we perform a largescale empirical study of ai enabled msf systems to investigate their current status regarding robustness.
discussion .
we further make discussions about existing aienabled msf systems and future directions including the unique advantages of different fusion mechanisms as well as the opportunities of their robustness enhancement.
to the best of our knowledge this paper is among the very early research to benchmark and investigate the msf system which is a common and representative ai system composed of multiple sensing channels and corresponding models.
on one hand at present it is not clear how much and to what extent each sensing unit could impact the integrated sensing results of an msf it is not clear how the issues of different sensing units and channels are involved and propagate to the final results of different msf designs either.
creating a benchmark at the current stage enables to investigate these important questions quantitatively which also enables further relevant quality assurance research along this direction.
on the other hand in general msf based perception systems play a key role to enable autonomous and intelligent systems which potentially has a big impact on many applications and domains.
with the recent fast pace in transforming into the data driven intelligent era we believe an early stage benchmark and investigation of the current msf systems empowered by deep learning would also benefit the practitioners in understanding the limitation and proposing better msf engineering techniques paving the path towards designing safe and reliable autonomous intelligent systems.
background .
perception systems in intelligent systems an intelligent system e.g.
a self driving car a robotic an unmanned aerial vehicle is usually a complex system composed of various subsystems.
these subsystems are cooperated to ensure safe and reliable operations of the intelligent system.
the perception system is one of the key components in an intelligent system which is in charge of sensing and processing environmental information through sensors to perform crucial tasks e.g.
object detection and object tracking.
the prediction results from perception systems are then propagated to other components in the intelligent system such as planning and control systems.
the perception systems lay the foundation of the intelligent system s workflow in perceiving and understanding the environment which also significantly impact the quality and reliability of the whole system.
most industrial level systems leverage multi sensor fusion msf strategy to avoid inherent perception limitations of individual sensors and thus sense the environment more reliably .
for instance the camera and lidar are usually fused in self driving cars since camera is more effective in capturing semantic information and lidar could provide more accurate geographic information .
as shown in fig.
right part each sensor in a camera lidar fusion first senses the surrounding environment individually.
then signals from different sensors are transformed into the same coordinate system and matched across the timestamps based on thebenchmarking robustness of ai enabled multi sensor fusion systems challenges and opportunities esec fse december san francisco ca usa figure workflow summary of ai enabled msf benchmark construction and high level empirical study design.
temporal and spatial calibration among sensors.
finally the fusion module receives calibrated and synchronized signals from different sensors and fuses them to make predictions for downstream tasks.
.
ai enabled multi sensor fusion different from traditional msf that only fuses the data or output ai enabled msf also has the possibility to fuse the deep semantic features learned by dnns.
we take the fusion of camera and lidar as an example right part of fig.
in the following sections.
each branch that processes signals in ai enabled msf can be represented as a composite function chain eq.
that maps the input datamto the output result fl.
fl f l f l f m whereldenotes the depth of a branch.
the medium output in the chain i.e.
f j j ... l represents the output from jth hidden layer in a dnn.
based on the stage where the fusion is made see fig.
ai enabled msf can be categorized into four different mechanisms at a high level early fusion late fusion deep fusion and weak fusion .
since early fusion is not commonly used in ai enabled msf we focus on the other three fusion mechanisms in the rest of this paper.
for allayer deep neural network we denote miandmjas two different modalities and define as a fusion operation.
now we briefly introduce each msf mechanism.
late fusion directly combines the output results of each branch which can be formulated as fl f l f l i f i mi f l f l j f j mj each branch in late fusion process data from sensors independently and does not depend on specific network architecture.
compared with other fusion mechanisms late fusion is highly flexible.
for instance late fusion can easily combine image based object detectors and lidar based ones.
late fusion does not involve hidden feature interaction which also leads to higher efficiency.
deep fusion involves frequent interactions among hidden features from different branches to gain rich semantic information.
suppose that the depth of branch iis greater than that of branch j when only one feature fusion is performed the deep fusion can be figure different ai enabled msf mechanisms.
formulated as fl f l f l i f l i f mi f l j f mj wherel i l jdenotes that the fusion starts from ith andjth hidden layers respectively.
weak fusion does not fuse the hidden features nor fuse the output results.
instead weak fusion adopts rule based methods to transform data from one branch to guide the process of data in another branch.
the process of weak fusion can be described as fl f l f l f g mi mj where g is the function that extracts the guidance from branch i. one typical example of weak fusion is extracting the frustums in the point cloud data using the 2d detection bounding boxes from the image as guidance .
benchmark construction .
benchmark collection to collect as many appropriate ai enabled msf perception systems as possible for our study we mainly focus on two sources the leaderboard of kitti benchmark and existing msf related literature.
kitti is a public autonomous driving benchmark that involves several different perception tasks.
for msf related literature we collect papers published in relevant top tier conferences andesec fse december san francisco ca usa xinyu gao zhijie wang yang feng lei ma zhenyu chen and baowen xu table the collected msf systems.
performance of each system is evaluated by task specific metrics detailed in sec.
.
.
system task fusion year modality performance epnet object detection deep c l .
fconv object detection weak c l .
clocs object detection late c l .
jmodt object tracking deep c l .
dfmot object tracking late c l .
twise depth completion deep c l .
mdanet depth completion deep c l .
journals during the last four years covering software engineering robotics computer vision etc.
we refer readers to our supplementary website for a complete list of selected venues.
eventually we selected state of the art msf systems from these two sources based on the following criteria multi sensors .
an msf system should involve two or more types of different sensors.
open source .
an msf system should be open source so that we can conduct experimental evaluations and enable further replication studies.
data available .
an msf system should have open source data for training and evaluation.
representative task .
an msf system should be designed for representative perception tasks with real world applications e.g.
object detection.
table summarizes the seven msf systems selected in our benchmark.
these seven systems cover three different tasks and three different fusion mechanisms.
due to the page limit we refer audiences to our supplementary website for details of each msf system.
.
corruption patterns operational environments of many msf systems are usually open with unexpected condition changes compared with environments during the design phase.
such environment changes are more critical to ai enabled msf systems due to the data driven nature of ml and dl.
for instance an autonomous driving system s object detector might be trained with data only collected from sunny days.
while the autonomous driving system is expected to be safe and reliable during rainy days however it is hard to determine to what extent the system can handle such a weather change.
that is the weather change in the open environment could result in corrupted sensor signals leading to potential distribution changes of data that affect an msf system s performance.
to evaluate an msf system s performance against such operational environments changes collecting and labeling real world data is ideal but not feasible.
to address these we collect and design thirteen corruption patterns to synthesize corrupted signals for msf systems which can be grouped into three categories weather corruption sensor corruption and sensor misalignment.
weather corruptions represent the external environment changes of an msf system e.g.
rainy foggy days and bright dark light conditions for a self driving car a uav etc.
sensor corruptions reflect the internal environment changes of an msf system such as transmission noise.table corruption patterns used in this study.
category corruption modality rain rn camera lidar fog fg camera lidar brightness br cameraweather corruption darkness dk camera distortion dt camera motion blur mb camera defocus blur db camera image gaussian noise gn camera point cloud gaussian noise gn lidar image impulse noise in camerasensor corruption point cloud impulse noise in lidar spatial misalignment sm camera lidar sensor misalignment temporal misalignment tm camera lidar sensor misalignment is specifically designed for msf systems given that the fusion of different signals requires accurate temporal and spatial calibration.
now we briefly introduce each category.
.
.
weather corruption.
weather conditions are an important factor that can inevitably affect the sensor s perception in the open environment resulting in the performance degradation of msf systems.
for example normal cameras could hardly perceive the surroundings at night.
in this work we leverage weather corruption patterns from two perspectives light conditions change and adverse weather conditions.
lighting conditions .
the camera is sensitive to lighting conditions variations in daylight and road illumination can easily affect the image quality while lighting conditions effects on lidar are limited .
therefore we mainly focus on adjusting the brightness br anddarkness dk of the image pixels.
weather conditions .
adverse weather can cause asymmetric measurement distortion of sensors which poses a significant challenge for msf perception systems that rely on redundant information.
for example on rainy days raindrops could lead to pixel attenuation and rain streaks on the image meanwhile the droplets will make the laser scattering and absorption resulting in a lower intensity of points and perceived quality of lidar.
in our benchmark we choose the domain specific physical model to simulate the properties of two representative adverse weather i.e.
rain rn andfog fg .
specifically we adopt rain model described in and fog model described in for camera and rain fog model described in for lidar.
another critical problem when designing rain or fog corruptions is ensuring different sensors are sensing identical environments e.g.
the camera and lidar are both sensing a rain of 10mm h. to address this we control the environmental parameters in lidar and camera model to ensure the consistency of the rain s volumes or fog s maximum visibility.
realisticness validation of rain and fog corruption .
to validate the naturalness of rain and fog corruptions we train deep fusion based classifiers to distinguish real rain fog scenes from clean scenes using datasets collected from real rainy foggybenchmarking robustness of ai enabled multi sensor fusion systems challenges and opportunities esec fse december san francisco ca usa a fog b rain figure feature visualization of simulated rain fog by t snes.
weather .
then we use these trained classifiers to make predictions on simulated data to measure the similarity between simulated and real data.
we retrain each classifier five times and take the averaged accuracy.
the average classification accuracy of these trained weather classifiers is .
and .
on simulated fog rain data.
these results confirm that the simulated fog rain data are highly similar compared to the real data.
we further analyze the similarity between the semantic features distribution of real and simulated data.
specifically we extract the high level semantic features from the trained classifier.
then we utilize t sne to reduce the dimensionality of acquired features to and visualize these 2d features.
as shown in figure the distributions of the real and simulated corruptions are similar.
.
.
sensor corruption.
sensor corruptions reflect internal environment changes that lead to corrupted sensor signals e.g.
noises during transmission and sensor artifacts that lead to blurry images.
in this benchmark we consider sensor corruption from two perspectives noise pattern and sensor artifacts.
noise pattern .
noise typically exists in both camera and lidar .
there are two main sources of noise one is from the sensor itself such as sensor vibration random reflections and the low ranging accuracy of lidar lasers .
the other is due to the digital signal in its transmission recording process .
we leverage two of the most common noise for each sensor i.e.
gaussian noise gn andimpulse noise in .
specifically gaussian noise applies gaussian distributed noise to each point s coordinate in a point cloud or each pixel s value in an image.
impulse noise applies deterministic perturbations to a subset of points or randomly changes the value of image pixels.
sensor artifacts .
sensor corruption could also result in artifacts of sensing results.
for instance defocus blur db occurs when a camera is out of focus motion blur mb appears when a camera is shaking or moving quickly .distortion dt is one of the common basic optical aberrations caused by the optical design of lenses .
note that as an early attempt we only consider artifacts of camera sensors.
we leave artifacts of lidar sensors e.g.
one of lidar s beams is broken as the future work.
.
.
sensor misalignment.
well calibrated and synchronized sensors are a prerequisite for msf based perception systems.
however it is not easy to guarantee the perfect alignment of sensors in thereal world .
therefore we design two corruption patterns spatial misalignment sm andtemporal misalignment tm to simulate the misalignment between the camera and lidar.
spatial misalignment .
msf system requires an external calibration of each sensor during the assembly process to ensure that the position measured in different coordinate systems can be converted to each other.
however even with well calibrated sensors the position of the sensors can inevitably deviate due to mechanical vibrations e.g.
when a self driving car rides on a bumpy road and thermal fluctuations .
suppose a 3d point in the lidar coordinate is pliand a corresponding point in the camera coordinate is pcam.
the transformation from the lidar coordinate to the camera one can be expressed as pcam tcam velopli where tcam velois a rigid body transformation matrix.
in our experiments we add a minor rotation within to each rotation angle i.e.
roll yaw pitch to simulate spatial misalignment between the camera and lidar.
temporal misalignment .
msf system requires synchronization of sensors to ensure the output from each individual branch is sensed at the same time.
in practical scenarios sensor or transmission failure may cause a delay in one branch resulting in a temporal misalignment .
to simulate temporal misalignment for a timestamp to we replace the data mi to with themi to t .
this could represent a signal delay of tsecond on branch i. .
evaluation metrics our benchmarks provide specific quantitative performance evaluation metrics for each perception task including object detection object tracking and depth completion.
then we define robustness evaluation metrics based on these metrics.
below we describe each perception task and the corresponding evaluation metrics.
object detection aims to locate classify and estimate oriented bounding boxes in the 3d space.
note that in this benchmark we mainly evaluate the detection of carobjects with moderate difficulty.
the accuracy of object detection can be measured by iou intersection over union and ap average precision .
iou measures the overlap area between a ground truth 3d bounding box bgand a predicted 3d bounding box bpover theiresec fse december san francisco ca usa xinyu gao zhijie wang yang feng lei ma zhenyu chen and baowen xu union .
the computation of iou can be represented as iou area bp bg area bp bg in our experiments evaluation we define a successful detection as an iou larger than .
ap is used to measure the performance of the overall detection performance which approximates the shape of the precision recall curve as ap r r r r interp r we apply forty equally spaced recall levels i.e.
r40 ... .
the interpolation function is defined as interp r maxr r r r where r gives the precision at r. multiple object tracking aims to maintain objects identities and track their location across data frames over time.
the accuracy is measured by mota multiple object tracking accuracy mota t fnt fpt idswt tgtt where fnt fpt and idswtare the number of misses of false positives and of mismatches respectively during a period t. the mota can be regarded as a measurement of three different types of errors.
depth completion aims to up sample sparse irregular depth to dense regular depth.
the depth completion tasks focus on predicting the distance for every pixel in the image from the viewer given lidar point cloud and image data.
we use the root mean squared error rmse mm to measure the distance between the predicted depth and ground truth value rmse vt mm i dip dig wheredip digare the predicted depth and ground truth of the ith position mis the total number of ground truth.
to further evaluate the robustness of different fusion mechanisms across different msf systems and tasks we define the robustness of msf on a corruption pattern c cwith severity s sas its performance pscrelative topclean performance on clean data rbs c ps c pclean wherepis measured by one of the evaluation metrics for the corresponding msf task i.e.
ap mota or rmse1andclean represents the clean data.
a larger rbcmeans that the system s performance against a specific corruption pattern is closer to the normal performance.
then we estimate the robustness of an msf system by averaging over all of the corruption patterns cwith severity s i.e.
mrb s s s1 c c crbs c a lowermrb means a higher risk of performance degradation when the msf system is deployed in the open operational environment.
note that both rbcandmrb can generalize to different msf systems tasks and corruption patterns.
in this way we expect our benchmark and evaluation metrics to be flexible and extensible.
1note that we normalize the metric of each task into .
.
dataset kitti is one of the most popular autonomous driving datasets which adopts four high resolution cameras a velodyne hdl 64e lidar and an advanced positioning system to collect data from different real world driving scenarios.
kitti supports diverse perception tasks including 3d object detection 3d object tracking depth completion etc.
during the paper collection process we also found that more than two thirds of the msf perception systems are evaluated on kitti.
to this end we use the kitti as our base dataset to construct kitti c to benchmark ai enabled msf systems performance and robustness.
note that corruption patterns used in this study can also generalize to other datasets such as waymo and nuscenes .
empirical study design in this section we introduce our research questions and experimental setup.
we first investigate the robustness of existing ai enabled msf systems from three perspectives against corrupted signals rq1 against spatial temporal misalignments rq2 and against partial complete signal loss rq3 .
then we investigate the potential of repairing these msf systems robustness rq4 .
.
research questions rq1.
how do ai enabled msf based perception systems perform against common corrupted signals?
though a few aienabled msf perception systems have been proposed and used there is no systematic study on the robustness of these systems.
in this rq we focus on corrupted signals due to weather sensor and noise corruptions .
for each corruption pattern we adopt three different levels of severity.
specifically for rain and fog three severity levels represent 10mm h 25mm h and 50mm h of rainfall and 104m 80m and 51m of visibility respectively.
to sum up we conduct experiments with different configurations corruptions levels msf systems to investigate this rq.
rq2.
how sensitive is ai enabled msf when facing spatial and temporal misalignment of sensors?
rq2 aims to evaluate the ai enabled msf system s sensitivity to calibration errors.
to simulate the spatial misalignment we rotate the lidar sensor around the x y and z axes by .
and respectively.
to simulate temporal misalignment we create five levels of lidar and camera signal delay i.e.
.1s .2s ... .5s respectively.
we only investigate temporal misalignment s effects on object tracking systems as the other two tasks are not time sensitive.
rq3.
to what extent are existing sensing components coupled of an ai enabled msf system?
this rq aims to investigate how existing ai enabled msf systems are coupled and if they are robust enough against signal loss of one source of signals.
to investigate this rq we simulate the signal loss with five different levels of each branch.
for the camera branch we reshape the image into a one dimensional array and randomly drop pixels.
for the lidar branch we randomly remove points with different percentages.
rq4.
what is the weakness of different ai enabled msf mechanisms and is it possible to repair them?
rq4 aims to investigate the properties of different fusion mechanisms and analyzebenchmarking robustness of ai enabled multi sensor fusion systems challenges and opportunities esec fse december san francisco ca usa a epnet b clocs c fconv d jmodt e dfmot f twise g mdanet figure robustness performance of seven msf systems against different corruption patterns.
table average robustness performance of msf systems against different corruption patterns across three severity levels.
taskweather sensor noiserbs1rbs2rbs3mrb rn fg br dk dt mb db gn c gn l in c in l objectepnet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fconv .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
clocs .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trackingjmodt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dfmot .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
depthtwise .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mdanet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the weakness or potential threats of each based on the experiment results from rq1 .
we first divide msf systems into three categories according to their fusion mechanisms.
to further investigate the possibility of repairing msf systems we make an early attempt on enhancing msf systems robustness by improving the fusion mechanism of late and weak fusion.
.
experimental setup in experiments we use second as the lidar branch for clocs and dfmot cascade rcnn as the camera branch for clocs dfmot and fconv.
we implement all msf systems with pytorch .
and python .
.
for each system we use default configurations to ensure a consistent runtime environment.
table shows the performance of each reproduced system.
the detailed settings of each system can be found in supplementary website .
all experiments are conducted on a server with an intel i7 10700k cpu .
ghz gb ram and an nvidia rtx gpu gb vram .
experimental results .
rq1.
ai enabled msf is not robust against corrupted signals.
fig.
summarizes the robustness benchmark results for seven aienabled msf perception systems against eleven corruption patterns via radar charts.
each axis in the figure represents the robustness scorerbscagainst corruption cwith severity level s. these results reveal that all the selected ai enabled msf systems have robustness issues against corrupted signals while their robustness properties could be varied.
for instance all the selected systems perform poorly against fog fg corruption.
however for the blur effects mb db some systems perform relatively robust e.g.
epnet twise jmodt while some face severe robustness issues e.g.
clocs fconv dfmot .
to further analyze how different msf systems perform against different categories of corrupted signals we interpret the detailed robustness performance in table by presenting the average performance against each corruption pattern across three severity levels.esec fse december san francisco ca usa xinyu gao zhijie wang yang feng lei ma zhenyu chen and baowen xu weather corruption .
as shown in table weather corruptions pose significant robustness issues for msf systems where the average robustness score against rain rn and fog fg are .
and .
respectively.
we also find that the depth completion systems i.e.
twise mdanet hardly work on foggy days.
specifically the highest robustness score among depth completion systems is only .
.
besides decreasing brightness affects msf systems more significantly compared with increasing brightness where the average robustness scores are .
and .
respectively.
sensor artifact .
while all the msf systems are relatively robust against distortion robustness score higher than .
some of them i.e.
fconv clocs dfmot particularly have significant performance degradation against blur effects mb db .
we further qualitatively check the image signals corrupted by distortion dt and find that only the edges of images are distorted.
this could be one possible reason that the effects of dt are limited.
noise corruption .
as shown in table camera signals corrupted by noise patterns are usually more vulnerable in msf systems where the robustness score against noises in cameras .
gn .
in are lower than those in lidar .
gn .
in .
based on these observations adding appropriate filters for image signals could be important for designing robust ai enabled msf.
answer to rq1 existing ai enabled msf systems are not robust enough against common corruption patterns.
moreover among the common corruptions adverse weather causes the most severe robustness degradation.
.
rq2.
ai enabled msf is sensitive to sensor misalignment.
through our investigation of rq2 we find that ai enabled msf systems are sensitive to both spatial and temporal misalignment.
spatial misalignment .
table shows the experimental results of spatial misalignment where each cell represents the robustness score.
according to the average robustness score across different rotation axes and angles last row of table we can find that spatial misalignment significantly affects msf s robustness.
specifically the highest average robustness score among the seven systems is lower than .
.
we also find that msf systems of different tasks could have different sensitivity regarding spatial misalignment.
for instance the robustness scores of object detection systems i.e.
epnet fconv clocs are relatively lower than object tracking and depth completion systems.
in addition we also find that msf systems are more sensitive to rotation around y axis.
when the rotation angle around y axis is increased to highlighted in table five out of seven systems crash robustness score is while the other two also have poor performance.
by contrast there is no such dramatic decrease for rotations around x and z axes.
we qualitatively compare the effects of2 rotation around different axes by projecting the point cloud onto the image in fig.
.
a rotation around y axis results in a significant malposition between the image and point cloud which possibly leads to the system crash.
temporal misalignment .
fig.
shows the effects of temporal misalignment on ai enabled msf systems for object tracking i.e.
table robustness performance of msf systems against spatial misalignment.
axis epnet fconv clocs jmodt dfmot twise mdanet x0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
y0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
z0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg .
.
.
.
.
.
.
a clean b x axis c y axis d z axis figure an example of a rotation error in calibration around x y and z axes.
figure robustness performance of msf systems against temporal misalignment.
jmodt dfmot .
as we can observe from fig.
both the camera and lidar branch are sensitive to the delay.
when the delay increases the robustness score of the msf system decreases.
in particular we find that lidar is more sensitive solid lines in fig.
to the delay.
when the delay of lidar increases to .
seconds the robustness score of jmodt and dfmot drops nearly from .
to .
.
in contrast the same level delay of the camera only drops their robustness performance by .
answer to rq2 ai enabled msf perception systems are sensitive to both temporal and spatial misalignment especially for lidar.
even small synchronization .
seconds and calibration errors can lead to a crash of ai enabled msf systems.benchmarking robustness of ai enabled multi sensor fusion systems challenges and opportunities esec fse december san francisco ca usa a object detection b object tracking c depth completion figure msf systems performance when partially losing one source of the signals.
.
rq3.
tightly coupled ai enabled msf could be less robust.
when deploying an ai enabled msf system developers might expect it to be reliable even if one of the signals is lost.
however our experiments demonstrate that ai enabled msf systems are less robust as they crash when they partially or completely lose a source of signals.
fig.
shows the robustness of different msf systems with different severity levels of signal loss.
these results suggest that partially losing either camera or lidar signal could affect the msf system s performance while losing the camera signal could be more critical dashed line in fig.
.
specifically we find that losing the camera signal significantly affects out of systems except epnet compared with losing the lidar signal.
when losing of the camera signal out of selected systems have a low robustness performance mrb smaller than .
.
these results also suggest that existing msf systems heavily depend on camera signals.
to further investigate ai enabled msf systems robustness against signal loss table shows the robustness performance of these systems when completely losing one source of the signal.
we can find that when losing lidar signals all of the systems crash.
when losing camera signals out of systems also crash and systems have poor performance e.g.
epnet jmodt .
surprisingly we find that mdanet does not crash when completely losing the camera signal however it crashes when losing partial signals see fig.
7c .
one possible explanation is that due to the sparsity of objects in the image data discarding or pixels could have dropped all the valuable information e.g.
pixels including objects .
the remaining pixels instead could bring interference to the msf system and thus lead to the system crash.
answer to rq3 ai enabled msf systems could be vulnerable when partially or completely losing one source of signals even if the other source is working properly.
in particular partially losing camera signals could be more critical for ai enabled msf systems.
we also find that though tightly coupled ai enabled msf systems have promising performance they could be less robust when completely losing either camera or lidar signals.
.
rq4.
fusion mechanisms could affect ai enabled msf s robustness and reliability.
while there is no systematic evidence indicating that one specific fusion mechanism is the most robust and reliable we particularlytable msf systems performance when completely losing one source of the signals.
modality epnet fconv clocs jmodt dfmot twise mdanet c .
.
.
.
l figure improved ai enabled msf mechanisms.
find that different fusion mechanisms may have unique advantages and potential threats due to their inherent properties.
according to our findings from rq1 three deep fusion msf systems i.e.
epnet jmodt twise are more robust against blur images mb db and noise patterns in c in l than others .
according to our finding from rq3 these systems also perform robustly when partially losing camera signals .
two late fusion msf systems i.e.
clocs dfmot show similar trends against corrupted signal rq1 and signal loss rq3 .
to further investigate the effect of the fusion mechanism on the robustness we try to repair the badly performed late and weak fusion msf system based on the inherent properties of different fusion mechanisms.
to improve the late fusion we leverage a shortcut between the lidar branch and the fusion layer to enhance the msf robustness left part of fig.
.
specifically we design a matching method to aggregate high confidence and unique results from an individual branch to the fusion results.
this is motivated by our findings in rq1 and rq3 where the camera is more susceptible to external environmental interference.
weak fusion uses a cascade architecture to connect two modules in series.
its robustness performance bottleneck is due to inaccurate missing guidance signals.
therefore for weak fusion we leverage a neural network to extract extra guidance from anotheresec fse december san francisco ca usa xinyu gao zhijie wang yang feng lei ma zhenyu chen and baowen xu a clocs b fconv figure performance of the original and enhanced msf.
table improved performance of clocs rb and fconv rb against partial or complete signal loss.
systems modality avg clocs rbc .
.
.
.
.
.
l .
.
.
.
.
fconv rbc .
.
.
.
.
.
l .
.
.
.
.
modality and connect it to the downstream module as an additional guidance branch right part of fig.
.
specifically we first train a 2d detector by projecting the point cloud to 2d front view images.
then we use the detecting results from the 2d front view as an extra guidance input.
to evaluate the effectiveness of improved fusion mechanisms we choose clocs and fconv as late and weak fusion systems and conduct the same experiments in rq1 and rq3.
fig.
shows the performance against corruptions of original msf and enhanced msf.
we find that the enhanced msf systems are significantly more robust against common corruption patterns.
furthermore table shows the improved performance rb rb where rband rbare robustness score with without improved fusion mechanisms respectively against signal loss.
we find that enhanced clocs clocs rb and fconv fconv rb show promising robustness performance against partial and even complete image signal loss.
for instance when the camera signal is completely lost in table the proposed robustness enhancement strategy almost fully recovers the msf systems performance highlighted in red in table .
answer to rq4 msf systems with the same type of fusion mechanisms may have similar robustness issues due to their inherent properties.
deep fusion performs better against some of the corruption patterns.
however weak fusion and late fusion are easier to be repaired when facing specific robustness issues.
discussion discussions.
according to our findings from rq1 existing aienabled msf systems are not robust enough.
first corrupted signals could result in significant performance degradation of ai enabledmsf systems.
the data driven nature makes it challenging to train a robust msf system that satisfies safety and reliability requirements under all conditions.
therefore more research on the continuous enhancement of ai enabled msf is needed such as debugging and repair.
our findings from rq2 also reveal that ai enabled msf systems are sensitive to calibration and synchronization errors.
in the real world these two types of errors commonly exist.
even well calibrated sensors can still be misaligned due to the changes in external environments.
to deploy a reliable ai enabled msf system developers must address the calibration issues carefully.
modular redundancy is a critical way to improve system quality and reliability .
by coupling multiple sensors ai enabled msf systems are expected to be robust against signal loss from one specific sensor.
however our experimental results suggest that existing work usually ignores taking this into account when designing ai enabled msf resulting in a lack of robustness.
thus future work should consider designing ai enabled msf systems that can still be reliable with one or more sources of signal loss.
though existing ai enabled msf systems are not robust enough we also find it possible to repair them with fusion mechanisms improvements.
in sec.
.
we propose a potential repairing strategy to repair weak and late fusion mechanisms.
the experimental results demonstrate their effectiveness showing that improving fusion mechanisms could be a promising research direction.
future directions.
based on these insights we summarize the following future directions in this work we focus on ai enabled msf perception systems.
however msf can also be used in systems beyond perception and autonomous driving.
therefore more comprehensive benchmarks and more fine grained robustness evaluation metrics for ai enabled msf systems can be considered in the future.
there is an urgent need for robustness enhancement techniques to continuously improve the reliability of ai enabled msf systems.
based on our investigation results improving fusion mechanisms to repair msf systems could be a promising research direction.
different fusion mechanism based msf systems show different robustness issues.
therefore practical software and system engineering approaches e.g.
testing debugging formal analysis and repairing would be needed for different msf systems.
threats to validity.
in terms of construct validity ideally it would be highly desirable to expose to diverse and as many corruption datasets as possible to better approximate the robustness performance of msf systems.
besides randomness could also affect the process of synthesizing corrupted data.
therefore we try our best and adopt a large scale systematic corrupted dataset across thirteen corruption patterns and multiple severity levels to comprehensively measure and analyze the robustness and reliability of msf in our benchmark.
even though the robustness results might still not generalize to cases of more diverse types of corruption patterns that are not evaluated in this paper.
in terms of internal validity one potential threat is that the leveraged weather corruption may differ from real world weather.
to mitigate this threat we choose the domain specific physical model to simulate the properties of adverse weather for different sensors.
further we ensure that different sensors are sensing identical environments by controllingbenchmarking robustness of ai enabled multi sensor fusion systems challenges and opportunities esec fse december san francisco ca usa the hyperparameters in the physical model.
in terms of external validity one potential threat is that our analysis results may not be generalized to other msf systems.
to mitigate this threat we try our best to collect a diverse set of msf systems with different perception tasks model structures and fusion mechanisms.
related works multi sensor fusion.
a pioneering work of ai enabled msf is mv3d .
mv3d takes multi view representations i.e.
front view and bird s eye view of 3d point clouds and images as input and uses a deep fusion mechanism to combine region wise features from multiple views.
to avoid information loss in generating view through perspective projections epnet proposes a lidar guided image fusion li fusion module that enables the interaction between the hidden features of the point cloud and image data to improve system performance.
clocs is another representative work of late fusion which leverages geometric and semantic consistencies of 2d and 3d output candidates to produce more accurate final detection results.
one of the early works of weak fusion is f pointnets which uses 2d bounding boxes as guidance to extract frustum in the point cloud and then estimate 3d bounding boxes.
fconv extends the f pointnets by proposing a sliding frustums method to aggregate local point features into frustumlevel feature vectors to achieve end to end prediction.
however few benchmarks are available to measure the robustness and reliability of these well designed msf systems in open environments with corrupted misaligned sensor signals.
robustness benchmarks.
several specific robustness benchmarks designed for one data modality have been proposed.
imagenetc evaluates the robustness of image specific recognition models against several corruptions.
cityscapes c extends this imagenet c to 2d object detection.
however the weather corruption in imagenet c and cityscapes c is not guaranteed to respect the underlying physics of weather conditions.
moreover mirza et al.
evaluate the performance of autonomous driving systems under image data collected in real weather conditions.
however they do not provide a benchmark of lidar based sensing modules against adverse weather conditions.
inspired by imagenet c modelnet40 c measures the performance of 3d point cloud recognition models.
however these corruptions can only be applied to object level point clouds instead of open scenes.
none of these existing works has focused on benchmarking msf systems with corrupted data from multiple different modalities.
our benchmark is thus proposed to address this.
msf testing and attack.
zhong et al.
propose an evolutionary based search framework to detect fusion errors for advanced driver assistance systems.
our work is parallel to them which is to establish a general benchmark rather than testing a specific system.
in addition some recent work has investigated how to attack ai enabled msf systems .
cao et al.
and tu et al.
attack all branches of msf systems by inserting adversarial objects.
abdelfattah et al.
and liu et al.
investigate attacks on weak fusion and deep fusion systems respectively.
in contrast our benchmark aims to evaluate the robustness of the msf systems against common real world corruptions instead of artificial adversarial objects or perturbations.
conclusion in this paper we present an early public robustness benchmark of ai enabled msf systems which can further be used as a fundamental evaluation and testing framework for understanding msf systems limitations and potential risks.
we further perform largescale robustness evaluation on seven msf systems against different corruption patterns including corrupted signals sensor misalignment and signal loss .
our findings reveal that existing ai enabled msf are usually tightly coupled and not robust enough.
thus we make an early attempt to enhance the msf system s robustness by improving fusion mechanisms.
finally we present discussions and highlight several possible future directions in order to build robust and reliable msf systems with the emergence of ai.
data availability our benchmark replication packages and detailed evaluation results are publicly available at .
acknowledgement we would like to thank anonymous reviewers for their constructive comments.
this project was partially funded by the national natural science foundation of china under grant no.
no.
and no.
.
this work was also supported in part by canada cifar ai chairs program the natural sciences and engineering research council of canada nserc no.rgpin no.rgpas no.dgecr as well as jst mirai program grant no.jpmjmi20b8 jsps kakenhi grant no.jp20h04168 no.jp21h04877.