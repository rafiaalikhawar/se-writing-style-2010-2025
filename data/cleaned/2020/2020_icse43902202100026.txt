code prediction by feeding trees to transformers seohyun kim facebook inc. u.s.a. skim131 fb.comjinman zhao university of wisconsin madison u.s.a. jz cs.wisc.eduyuchi tian columbia university u.s.a. yuchi.tian columbia.edusatish chandra facebook inc. u.s.a. schandra acm.org abstract code prediction more specifically autocomplete has become an essential feature in modern ides.
autocomplete is more effective when the desired next token is at or close to the top of the list of potential completions offered by the ide at cursor position.
this is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play.
we advance the state of the art in the accuracy of code prediction next token prediction used in autocomplete systems.
our work uses transformers as the base neural architecture.
we show that by making the transformer architecture aware of the syntactic structure of code we increase the margin by which a transformer based system outperforms previous systems.
with this it outperforms the accuracy of several state of the art next token prediction systems by margins ranging from to .
we present in the paper several ways of communicating the code structure to the transformer which is fundamentally built for processing sequence data.
we provide a comprehensive experimental evaluation of our proposal along with alternative design choices on a standard python dataset as well as on a company internal python corpus.
our code and data preparation pipeline will be available in open source.
index terms code embedding code prediction autocomplete i. i ntroduction a. code prediction the idea of code prediction in general is to predict some code element given code surrounding the point of prediction.
code prediction is commonly used in an ide for autocomplete where based on the code already written up to the developer s cursor position the ide offers the most likely next tokens perhaps as a drop down list to choose from as shown in ide views in fig .
other forms of code prediction could predict missing tokens at arbitrary code locations or predict larger units of code in this paper we will concern ourselves with prediction of the immediate next token at the cursor position.
consider the python code fragment shown in fig .
suppose a developer has written code up to string following by a dot.
at this point it will be helpful for the ide to prompt the developer with attribute names that are likely to follow preferably with atoi ranked at the top because in this case that is the correct next token.
developers have come to rely on autocomplete in their ides for multiple reasons.
first and most obviously it saves the effort of typing in the next token s in the ide.
for this reason alone most modern ides come with at least both authors contributed equally to this research.
a type based alphabetical b seqrnn c travtrans fig.
screenshots of ranked autocomplete predictions from three different models type based alphabetical seqrnn and travtrans as would appear in an ide.
a type based autocomplete tool such as jedi that sorts choices alphabetically ranks atoi low.
seqrnn a rnn based model predicts it as the second result.
travtrans a transformer based model predicts it as the first result.
fewer keystrokes are needed to choose the correct answer as we go from left to right.
some autocomplete facility for the languages they support.
notice that a top ranked suggestion is often selectable by hitting a tab as would be the case in fig c whereas the lower ranked suggestions have to be selected by scrolling fig a b which is more effort.
thus providing the right completion at the top of the list or if not among the top few is important.
second autocomplete is also a powerful code discovery mechanism.
for instance a developer might not know the name of an api call they need off the top of their head but is ieee acm 43rd international conference on software engineering icse .
ieee able to choose among the choices shown by an autocomplete tool.
without assistance from ide the developer might need to change their mental context go to stack overflow or some other web site and come back to the ide.
however the code discovery assistance from autocomplete works only when a contextually appropriate code suggestion is offered among the topchoices in the list because developers do not have the time to go through a comprehensive list of completions.
b. machine learning for code prediction it is clear that effective autocomplete requires the intended next token to be predicted at the top of the list or as close to the top as possible.
type based autocomplete e.g.
eclipse1and jedi2 returns a list of type compatible names but does naive ranking alphabetically or with simple countbased statistics.
for instance the autocomplete model used in the ide jedi which ranks type compatible suggestions alphabetically shown in figure a ranks atoi fairly low.
the example shows why one of the earliest attempts of code prediction using type based methods is not very effective.
additionally for dynamic languages it is extremely difficult to gather an accurate type compatible list of tokens that could occur in a context.
these limitations have motivated the use of machine learning for code prediction as machine learning methods are able to base their predictions on the naturalness of code.
early approaches adapted n gram language models on linearized source code tokens .
more recently deep neural networks have been applied to code prediction surpassing ngram models.
the most common neural technique for code prediction is recurrent neural networks rnns and their variants where the code represented as a linear sequence is fed as input to the model.
fig b shows how an rnn model does better than a non ml alphabetical ranking fig a in showing the expected item closer to the top.
researchers have also investigated using the syntactic structure of code for prediction as opposed to seeing code as text both using probabilistic graphical models probabilistic context free grammars and probabilistic higher order grammars as well as using deep learning .
c. is this a solved problem?
although predictive models cannot be expected to be perfect the accuracy of the current state of the art methods leaves substantial margin to be improved.
for instance a typical rnn based method provides less than mean reciprocal rank this equates to the correct answer being in the top 7results sec iv c on the py150 benchmark.
improving this metric is exactly the goal of this paper.
we report that our techniques are able to suggest the correct next tokens at ranks better showing correct result to the developer by .
to ranks higher than those achieved by previous methods mrr increase of to .
with code assist.htm ip socket.gethostbyname host map string.atoi sys.argv chain build request chain num requests host request size ... fig.
running example of python code.
the code snippet3is from the py150 dataset .
as a concrete example table i shows the ranks of the various non punctuation tokens to be predicted for the code in figure using various recent methods as well as for our work.
specifically the rank of atoi is predicted at rank only by the new methods we propose in this paper.
d. feeding trees to transformers to improve the accuracy of next token predicted a number of alternatives come to mind.
first we could strengthen the neural architecture alone.
for instance researchers have suggested adding attention to rnns to compensate for loss of signal on long range dependence.
without longrange dependence a model would be making a potentially suboptimal decision only on the most recent tokens.
transformers handle long range dependencies better.
in the nlp community transformers have achieved state of the art results outperforming rnns for a variety of nlp tasks such as language modeling question answering and sentence entailment.
for us since training transformers did not take much more resources that training rnns table iv we decided to proceed with transformers.
an orthogonal way to improve the accuracy is to enable the machine learning system to see more code structure.
raychev et al.
had found that for the code prediction problem a non neural but ast aware engine could outperform rnns.
in the same spirit alon et al.
had found for code summarization problem though notfor code prediction in their paper that embedding the ast structure of code vastly outperformed purely sequence based methods.
inspired by the above we explore how to leverage code structure while using transformers on code.
this is not obvious we cannot simply jam an ast into the transformer which is a sequence processing model.
we explore two models that represent two ways to capture the partial structure of an ast one based on decomposing the tree into paths pathtrans and the other based on a tree traversal order travtrans .
we also investigated a variant of travtrans that takes into account even more tree structure.
e. key results tab ii compares the new models we introduce in this paper in bold with three state of the art models from previous work sec ii a discusses these models.
we report results based on training and evaluating on the py150 dataset and 3data jeremygrosser supervisor src supervisor medusa test test .py 151token value ip socket get name host map string atoi sys argv chain previous seqrnn work deep3 code2seq seqtrans our pathtrans work travtrans table i ranks for the predictions for the leaf nodes listed in fig .
means the model did not get the right answer in the top results.
model sequence ast transformer seqtrans .
travtrans .
pathtrans .
attention code2seq .
rnn seqrnn .
decision tree deep3 .
table ii overview of the models considered in this paper.
models in bold font are models from this work seqtrans feeds source tokens in linear order to the transformer.
the numbers in parenthesis denote accuracy see sec v. it is clear that the accuracy increases as models uses information from the ast columns and as more sophisticated neural architectures are used rows .
for each we report accuracy in mean reciprocal rank mrr as a percentage see sec iv .
our best model travtrans which communicates the tree structure to the transformer significantly outperforms all previous models for code prediction with improvements in reciprocal ranks from .
to .
when comparing a non neural tree based model deep3 vs. travtrans from .
to .
when comparing code2seq vs. travtrans from .
to .
when comparing an rnn implementation seqrnn vs. travtrans4 we also evaluated our trained model on a dataset selected from a python code repository internal to a facebook and found the relative benefits of the transformer models to be similar to those on the py150 dataset.
this indicates that the relative advantage of transformer models carries over to other datasets.
sec v table viii deep learning models can be rather opaque as to their working.
to better understand whether travtrans is taking advantage of the tree structure we employed saliency maps to examine where the model focuses its attention when making a prediction.
we found that indeed the model tends to focus on the most relevant parts of the tree starting from the parent node sec vi .
f .
contributions we move the state of the art forward in accurate next token prediction a capability increasingly expected in modern ides.
4comparing travtrans against seqrnn is slightly different than comparing it against deep3 or code2seq hence the difference in mrr.
we discuss in detail in sec v. we describe ways of using transformers for the task of next token prediction especially ways that profitably communicate the syntactic structure of code.
sec ii .
although there have been previous work in applying transformers in the context of code code summarization code correction and code translation this paper is the among the first to explore and evaluate transformers for code next token prediction.
we present a systematic comparison of our proposed models with the most effective models from prior work5 that are applicable to next token prediction on a widely available python dataset py150 .
the findings clearly indicate a to gain in accuracy with our best model relative to prior state of the art.
we provide a preliminary attribution study in an attempt to understand the prediction given by our best performing model travtrans.
this study sec vi indicates that travtrans indeed conditions its predictions on the most pertinent tokens in the context.
to our knowledge this kind of model interpretability analysis for autocomplete is also a first.
our overall conclusion is that transformer based models over asts provide the best prediction power for autocomplete.
g. outline sec ii provides background on the previous models that we use in our evaluation along with a primer on transformers.
sec iii explains the transformer based models of our own creation.
sec iv describes our datasets and implementation.
sec v presents our quantitative results.
sec vi takes a closer look into why our models worked well or did not .
sec vii lists some threats to validity.
sec viii discusses prior related work in the field of code prediction and transformers.
we conclude the paper with our future work in sec ix.
ii.
b ackground in this section we define the code prediction task we examine in this work followed by details of previous stateof the art methods of code prediction we use for comparison.
we end the section with a brief introduction to the original transformer model.
we will refer to the nodes of the ast for fig as shown in fig .
5incidentally this paper is also the first to compare the three prior works on a common dataset.
152fig.
part of the ast for the example in fig .
the leaf terminal nodes have values and the interior non terminal nodes have types.
a. code prediction task code prediction task studied in this work is to predict the next code unit given the partial program up to the point of prediction.
let p unitjctx be the empirical distribution of code unit given the partial program context ctx.
our task is to learn to approximate p using a machine learning model m. in our proposals mwill be some transformer trans.
the learned distribution can be viewed as p unitjctx m ctx where represents the trainable parameters of the model.
we train the models by minimizing the kl divergence between p andp or equivalently minimizing the cross entropy loss l over all code prediction locations.
we explore several ways of representing partial program and predicting different kinds of code units.
when using source code token as code unit and representing partial program as sequence of source code tokens seqtrans the problem aligns with the traditional notion of language modeling predicting the next token in a sequence given all previous tokens p tij t1 t i where tiis the i th token.
more interestingly we explore various representations of a partial program to better utilize its ast information.
the intuition is that the more we can utilize the syntactic information provided by the ast the better we can predict the next token.
the next section discusses three models used in previous work.
b.seqrnn for next token prediction a popular method is to feed the source sequence tokens into an rnn or lstm .
an rnn embeds the input tokens into a vector xt emb wt where wtis the source token seen at the t th time step.
the hidden state ht 1at the t th time fig.
fragment of a tgen program encoding a decision tree on the left bold words are the steps that comprise a path with the corresponding paths shown on the ast on the right.
step is computed as ht rnn xt ht where rnn is a trainable rnn unit.
the last hidden state is then fed through a classification layer.
the pertinent point to note is that the hidden state ht encodes the knowledge of not just the current token but of last several and theoretically all previous tokens via the propagation of information in previous hidden states.
in our experiments we feed the source code tokens into an lstm and call this model seqrnn.
c.deep3 raychev et al.
presented a system deep3 based on a learned decision tree combined with count based probabilities at the leaves of the decision tree.
fig shows part of a learned decision tree written in the form of program in a specialized language called tgen.
given an ast tand a starting node n a tgen program walks certain paths in tstarting from n. for example up writevalue line goes to the parent of nand records the label.
at the end of a tgen program is a probability distribution for the possible values of the starting node.
for example starting with node the tgen program predicts atoi with split with etc.
a tgen program is learned on a specific corpus by a genetic search procedure that simultaneously selects paths and grows the decision tree from the training data with an entropy minimization objective.
in this paper we use their pretrained model as well as their python dataset for our experiments.
d.code2seq code2seq is a model by alon et al.
that embeds code snippets by embedding ast paths in a neural network.
at a high level given an ast code2seq creates path representations for all leaf to leaf paths.
for example fig shows three leaf to leaf paths for nodes from the full ast fig .
for each path a path representation is created with .
the starting leaf value tokenized by snake and camel case .
the path itself and .
the ending leaf value also tokenized.
and are embedded using lstms and is embedded using bi directional lstms.
these three embeddings 153fig.
example of an input for code2seq which consists of leaf to leaf path representations given a partial ast.
a path representation is made of tokenized starting tokens path and tokenized ending tokens.
if the path ends with the target node in this example atoi the value is replaced by placehholder .
are concatenated and then fed through a feed forward network.
finally all of the path represention embeddings in the ast are combined using a simple attention mechanism.
in code2seq a decoder is then used to solve a code summarization task given a method body how well can code2seq generate the correct method name?
the training proposed in is not well suited for next token prediction.
in code summarization a set of leaf to leaf paths needs to be created one time for a method.
by contrast in code prediction a new set of leaf to leaf paths has to be created for each point of prediction.
for example to predict atoi node in fig we must first create an representative embedding for the partially completed ast up to node using all leaf to leaf paths available up to node .
paths that end in atoi are also used withatoi replaced with a placeholder token to prevent information leak e.g.
paths and in fig .
the representative embedding is then fed through a classification layer to generate predictions.
by treating each point of prediction as a separate data point compared to a language model where one sequence is considered one data point the number of training data points along with the effort to create them makes code2seq computationally very expensive.
e. a primer on transformers here we present a brief introduction of transformers.
readers familiar with transformers can skip ahead to section iii.
transformers belong to a class of deep neural networks that are designed for sequence processing.
in transformers information from any previous location of the sequence can directly affect the encoding of the next token through a mechanism called self attention which helps greatly improve the connectivity in long sequences.
to be precise a transformer is a stack of attention blocks attnblk preceded by an input embedding layer emb and 6note that this is different than the generative decoder that code2seq uses.
fig.
schematic of a gpt2 transformer.
the self attention layer is able to consider all tokens in the input up to the point of prediction.
here the self attention box depicts the information flow when predicting next token after the .
see table iii for where the numbers come from.
followed by a classification layer clsfr where attnblk is repeated nblock times.
trans ctx clsfr attnblk attnblk emb ctx see fig for a schematic of a transformer with embedding layer a stack of here attention blocks and finally a classification layer.
the self attention layer which constitutes the main part of an attention block is the crux of the model.
the intuition here is to attend to the elements in the input sequence in proportion of their relevance to the location being predicted.
for example take an example input token sequence map string .
and the target next token being atoi.
it is first fed through the initial embedding layer to give e .
then we feed eto three fullyconnected networks w q wk wv to create query key and value embeddings q ew q k ew k v ew v fig depicts the vectors qcomprised of its elements qmap q qstring q. in green and likewise for kandv.
the self attention then works by querying keys kusing queries qand then using the result to summarize values v through the attention function attn q k v softmax qk pdk!
v where dkis the dimension of key vectors.
here qk is used to determine which token relationships are the most important resulting in a matrix of size n n where nis the length of the input sequence.
each row is then normalized and passed through a softmax layer.
table iii shows an example of the self attention weights.
looking at the last row we can see that most of the attention is given to .
meaning it has a greater ... map string .
map .
.
.
string .
.
.
.
.
.
.
.
table iii example matrix for the numerical self attention scores after taking the softmax over the normalized values ofqk .
for example the entry against string map is obtained by multiplying qstring withkmap after softmax and normalization.
note that the rows listed here do not sum up to exactly since there are previous tokens in the input sequence that are not shown in this matrix.
factor in predicting the next token atoi .
note how the matrix is a lower triangular matrix this is because self attention cannot be applied to tokens that have not been seen before.
after multiplying this matrix with the value vector in our example attn q k v .
transformers also uses multiple heads of these self attention blocks called multi headed attention which enables the model to simultaneously consider different ways of attending to previous information within one block and also across other blocks.
in our implementation we omit positional encoding see sec iv b. for more details please refer to and .
the next sections discuss various ways of feeding code fragments into this transformer architecture.
iii.
o ur work transformer based models the question that interests us is can transformer based models also benefit from syntactic structure and if so how can we communicate the syntactic structure to transformer?
in this section we first begin with a model seqtrans that uses a transformer to take source code tokens as input.
then we introduce two other models pathtrans and travtrans that use more syntactic information obtained from the ast.
seqtrans our first model is to apply a transformer over source token sequences which can be easily obtained by applying a tokenizer.
here the input is the partial program represented as source token sequences and the output is a source code token.
this is a straightforward application of the original transformer design and functions as a baseline for our later attempts that take on more ast information.
the model can be written as o trans et t2srcseq whereois a distribution over all possible tokens and etis the embedding for source token tfor every tin the source token sequence.
as we show in the experiments seqtrans turns out to be an already strong model as a direct comparison to the baseline rnn model seqrnn.
since transformers are originally designed as a sequential model the challenge becomes finding ways to convey ast information to transformers.
in the next subsection we will vary the inputs and the outputs to the transformer but the principles of operation will remain the same as in seqtrans.
fig.
example of an input to the pathtrans model.
it takes all leaf nodes along with its path to root to make the next leaf token prediction.
the root paths are embedded using an lstm and the leaf tokens are embedded using an embedding layer.
these two embeddings are added together to create a path representation embedding which is then fed to the transformer as shown in fig .
the classification layer of the transformer outputs leaf tokens.
pathtrans pathtrans enhances seqtrans by exposing tree structure to the transformer via root paths.
a root path is the path from the leaf node tto the root of the ast by traversing up its ancestors recording all the nodes along with it and thus a sequence of internal ast nodes.
fig shows an example of an input datapoint for predicting node of fig .
the root paths are first fed into a an lstm7added with the embedding of the leaf node and is fed through a transformer o trans et pt t2leafseq whereois a distribution over all possible leaf nodes andetis the embedding for ast node tandpt lstm eu u2rootpath t is the summarized representation of root path from tfor every leaf node tin the leaf sequence leaf seq.8the hope here is that the root paths captures the local syntactical information and thus can help the prediction.
since the points of prediction are the leaf nodes of the ast the loss is taken over only the leaf ast nodes and it predicts only leaf tokens.
travtrans as a transformer naturally only takes a sequence as input we provide the ast nodes as a sequence in pre order traversal or a depth first search dfs order.
for fig for node the previous nodes in dfs order would be ... call nameload map attributeload nameload string attr .
the travtrans model can be written as o trans et t2ast seq 7we could have used a transformer to embed the path sequences in lieu of an lstm but since the path sequences are short capped at tokens enough for lstms to perform adequately well we decided to use an lstm.
see sec iv b for details.
is used here following the convention of transformer computations and to keep the embedding dimension the same for every component.
155whereois a distribution over all possible tokens and etis the embedding for ast token tfor every tin the partial program represented as a ast token sequence ast seqin dfs order.
capturing even more ast structure?
travtrans presents the tree nodes in a pre determined order but still does not retain detailed structural relationship between nodes.
for example consider the sequence of nodes in fig .
this would be represented as the three nodes appearing consecutively in dfs order.
looking at the ast we can see that the relations between nameload string and string attr are actually quite different nameload is one node up from string while string is two nodes up and one node down from attr .
we create a new model travtrans that enhances travtrans by capturing these richer path based relations.
similarly to hellendoorn et al.
we enhance the self attention block of the transformer with a matrix rthat captures the unique path needed to reach from atob.
this path is represented abstractly only in terms of up and down moves udpath a b uidj where i and jare the number of up and down nodes respectively node ahas to travel to reach node b. for example udpath u2d2for node and in fig .
ris introduced to the transformer by replacing the attn function with the following attn treerel function.
attn treerel q k v r softmax r qk pdk!
v where is element wise product.
this provides a way for the self attention to consider the previous tokens taking into account the ast relationship between pairs of nodes as well.
iv.
i mplementation and datasets a. dataset we train our models using the py150 dataset used in .
the dataset consists of 150k python source code files from github repositories along with their parsed asts split into 100k for training and 50k for evaluation.
in this work we slightly modify the ast to ensure that the internal nodes only carry syntactic types and the leaf nodes only carry token values.
to incorporate large trees greater than nodes which is the limit we chose for transformer window we deploy a technique adopted by which slices a large tree into shorter segments with a sliding window to maintain part of the previous context.
we evaluate our models on two evaluation datasets py150 we use the evaluation dataset used in which consists of 50k python asts.
after the modifications mentioned above there are leaf nodes.
internal we also created an evaluation dataset consisting of python files from a code repository internal to facebook.
with this dataset we can evaluate how our trained model can generalize to a different dataset even if the code comes from disjoint projects.
after the modifications there are leaf nodes.b.
implementation a transformers for the models that use transformers seqtrans pathtrans travtrans travtrans we adapt the pytorch implementation9of gpt small .
we use six transformer blocks six heads in each block nctx andembedding dim .
we borrow other hyperparameters from .
we limit the token vocabulary size to 100k which covers over of the tokens used in the training dataset.
for pathtrans we limit the maximum length of the path from leaf node to root to be which covers over of the nodes.
for any path longer than we keep the nodes closest to the leaf and truncate the nodes near the root.
in our implementation we did not use positional encoding or positional embedding to provide extra positional information over elements since our early trials with leafseq suggested positional embedding is rather hurting than helping.
this is also supported by the claims in that positional encoding does not help for language modeling.
recently tried to introduce tree structures to transformer models via positional encoding.
however their relative improvement is small compared to what we see with tree relational prior in section v. b rnn for seqrnn we adapt the pytorch lstm example implementation10.
we use embedding dimension dmodel with dropout 5andnlayers .
we maintain the same vocabulary size at 100k.
c code2seq for code2seq we used a pytorch adaptation of the publicly released model11 using the same hyperparameters except changing the vocab size to 100k.
for selecting max number of paths paths per ast we first picked paths that ended with the target to maximize the amount of local context .
since for each prediction point in the ast a new set of leaf to leaf paths have to be generated the data processing for code2seq takes a substantial amount of time magnitude of days worth of time .
we trained all models on nvidia tesla v100 using gpus at a time until the loss converged with all of the parameters randomly initialized.
we used the adam optimizer with the learning rate set to 1e .
implementation details regarding number of epochs until convergence training time minutes per epoch inference time to evaluate over the py150 dataset and model size are listed in table iv.
d deep3 for the deep3 model since the authors have shared only the model and not the training algorithm we used the model pretrained on py150.
c. evaluation metric we evaluate the models on next token prediction for the leaf tokens.
we report numbers for all leaf token predictions as well as breaking down into more interesting categories attribute access numeric constant name variable module and function parameter name.
language model 156prior work our work seqrnn deep3 code2seq seqtrans pathtrans travtrans num epochs n a training time min epoch n a inference time min model size mb n a table iv implementation details for all the models number of epochs until convergence training time minutes per epoch inference time to evaluate over the py150 dataset and model size state dict of pytorch the learnable parameters of the model .
note that some information about deep3 is not available since the authors have shared only the model.
to measure performance on these tasks we use mean reciprocal rank mrr .
the rank is defined as mrr nnx i rank i where nis the number of predicting locations and rank iis the rank of the correct label given by the model for the ith data point.
we present mrr as a percentage in keeping with prior work .
while acc only gives score when the correct label is ranked at the top mrr also give scores when the true label is not ranked as the top but among top few prediction.
comparing to the hit or miss style metric acc this is closer to the realistic scenario when completion suggestions are presented to developers.
with this practical perspective and for ease of computation we only consider rank i 10for each location i allrank i 10will have a score of .
we share our data processing scripts and model implementations at https github.com facebookresearch code prediction transformer.
v. e valuation rq1 given a source token sequence does a transformer work better than an rnn as in previous work?
comparing seqrnn and travtrans we find that a transformer does work better than an rnn.
for the py150 dataset we can see a significant improvement in mrr for predicting all leaf tokens in table v from .
to .
for the seqrnn and seqtrans models respectively.
the same holds for comparing on the internal dataset as shown in table vi .
vs .
.
consistent improvements can be seen for specific types of leaf tokens.
prior work our work applications seqrnn seqtranstravtrans type value attribute access .
.
.
numeric constant .
.
.
name variable module .
.
.
function parameter name .
.
.
all leaf tokens .
.
.
table v mrr of various types of next token predictions for py150.
to fairly compare these models travtrans makes two predictions one for the leaf node and then one for its parent internal node see rq3 for details .
rq2 do the transformer models on tree outperform previous work on ast based prediction?prior work our work applications seqrnn seqtranstravtrans type value attribute access .
.
.
numeric constant .
.
.
name variable module .
.
.
function parameter name .
.
.
all leaf tokens .
.
.
table vi mrr of various types of next token predictions for internal dataset.
travtrans makes two predictions one for the leaf node and one for its parent internal node see rq3 for details .
we compare deep3 and code2seq against pathtrans and travtrans table vii .
overall we found that both transformer based models achieve better scores than both deep3 and code2seq for all leaf tokens as well as for specific types of leaf tokens.
our best performing model travtrans improves deep3 s mrr by .
from .
to .
and code2seq s mrr by .
from .
to .
.
similar results can be seen for the internal dataset table viii .
we also compared the accuracy on ast internal predictions comparison deep3 and travtrans as they are the only models with this capability.
in table ix we see that travtrans improves accuracy over deep3 across the board.
table ix show non terminal value prediction for both py150 and internal dataset respectively.
we see that travtrans is outperforms deep3 for all internal node types as well.
prior work our work applications deep3 code2seq pathtrans travtrans attribute access .
.
.
.
numeric constant .
.
.
.
name variable module .
.
.
.
function parameter name .
.
.
.
all leaf nodes .
.
.
.
table vii mrr of various types of next token predictions for py150.
rq3 does adding syntactic structure help?
comparing seqtrans and travtrans we confirm that adding syntactic structure does help.
comparing seqtrans against travtrans on leaf nodes fairly is a bit tricky.
a source code token we are looking at here 12we do not include code2seq comparison for non terminal node predictions due to the overhead required to prepare and process the dataset.
since the main part of the paper was on leaf token prediction and we have shown that travtrans performs significantly better than code2seq we did not deem it essential to include the results on non terminal value predictions.
157prior work our work applications deep3 code2seq pathtrans travtrans attribute access .
.
.
.
numeric constant .
.
.
.
name variable module .
.
.
.
function parameter name .
.
.
.
all leaf nodes .
.
.
.
table viii mrr of various types of next token value prediction for internal dataset.
dataset py150 internal applications deep3 travtrans deep3 travtrans function call .
.
.
.
assignment .
.
.
.
return .
.
.
.
list .
.
.
.
dictionary .
.
.
.
raise .
.
.
.
all types .
.
.
.
table ix mrr of various type predictions for py150 and internal dataset.
contains both syntactic type and lexical value which translates to two nodes in the ast.
for example the source code token is equivalent to an internal node with value num and its child leaf node .
thus for a fair comparison travtrans has to make two predictions one for the leaf node and one for its parent internal node.
for example given the sequence y and the next prediction should be travtrans must predict both the internal num node as well as the leaf node.
for evaluation we implemented a local beam search to choose the pair with the maximum joint probability.
table v shows that travtrans still performs better than seqrnn and seqtrans except for predicting function parameter name for predicting the leaf tokens.
it is important to note that even with this correction it is tricky to completely fairly compare the accuracy of seqtrans against travtrans model on leaf nodes.
the main issue here is that the contexts used to predict a leaf value is different in the two models because of different order of seeing information.
consider the case of a code fragment x y z and let us say we need to predict what comes after the .
in the case of source token based prediction the predictor has seen x and would be expected to produce the next token y .
in an ast one would first construct an assign internal node with the right child to be filled next and the next prediction would be actually an interior node binopplus .
the y would be predicted as the left child of binopplus.
in a context in which applications travtrans travtrans attribute access.
.
.
numeric constant .
.
name variable module .
.
function parameter name .
.
all leaf nodes .
.
all internal nodes .
.
table x mrr travtrans compared against its variant travtrans that incorporates more tree structure.the ast has been augmented with the binopplus node the prediction of y has more information compared to what a source token model did immediately after the .
this makes a direct comparison of token predictions difficult.
one way to achieve better parity between the models would be to use an in order traversal instead of a pre order traversal we chose the latter because we want to do top down generation which will be useful in future ast generative models.
we also compare travtrans with its variant travtrans that extends the model to incorporate even more tree structure as mentioned in sec iii.
table x shows a slight increase in mrr .
vs .
for all leaf nodes and .
to .
for internal nodes.
due to mixed benefit on leaf node predictions we continue to treat travtrans as our flagship model.
but this does hint at the existence of more powerful models if we incorporate more tree structure.
vi.
m odel inspection despite their wide adoption in many areas of computing deep learning models have been repeatedly shown susceptible to adversarial examples .
interpretability studies that provide human understandable justifications have thus become an important property in building safe and trustworthy ai systems.
with the growing adoption of neural models for software engineering tasks the same concern raises.
recent discoveries of unexpected behaviours of deep learning models of code calls attention for such studies.
besides of helping understand the behaviour of deep learning models interpretability methods have also been used to directly improve the process of software engineering.
for example used lime a model agnostic explainability method to pinpoint the defective lines of code from defect detection models.
as a crucial first step towards interpretability in this section we reveal what travtrans has learned that leads to its good predicative power.
we found that travtrans has learned to attribute the prediction to relevant previous tokens.
while this is not a principled study we hope it sheds light on a possible approach to perform a complete model inspection study.
although our transformer based models heavily relies on attentions direct visualizations of weights in individual attention heads did not yield clear insights as the attentions are stacked across layers and multiple attention heads are in effect in each layer see sec ii e .
we thus turn to gradientbased saliency maps for a more comprehensive account of the influence of each input token.
following the influence of each input token is computed by taking the partial derivative of the loss function with respect to the embedding vector of that token.
fig is the saliency map we got for travtrans which visualizes the magnitudes of the gradients fall at each input token x axis when the model predicts a particular output y axis .
intuitively the larger the value for a particular token the more sensitive the output is to the variations at that input token.
we first observe that the parent ast node the internal node right above the leaf is generally important in pre158fig.
influence of previous nodes in value prediction of the example in fig by travtrans.
x axis is labeled with the input values.
y axis is labeled with the values to be predicted.
color indicates the model s prediction is correct orwrong.
dicting the next leaf node value the last token usually has a high influence.
more generally we found that travtrans tends to attribute more towards the internal nodes that are directly followed by relevant leaf nodes.
for example we can see the prediction of atoi is influenced by the locations previous to string map andnum requests.
it is not shown in the figure but the predictions of sys andargv before 2are influenced by the previous occurrence of the same values followed by 0and1.
the prediction of host gethostbyname are influenced by the location previous to ip.
for the position of gethostbyname travtrans predicts socket with probability .
with the correct answer ranked second with slightly lower probability .
.
all above suggest that travtrans is capable of picking up information from relevant previous code locations.
the reason travtrans tends to focus more on the parent nodes may be that all relevant information for the next token are already summarized in the hidden representation at the location previous to the token aka parent nodes for the leaf nodes .
on an orthogonal note we also observe that for many predicting locations the magnitude of gradients are very small suggesting the robustness of the model in the sense that it is less sensitive to minor perturbations of the input sequence.
vii.
t hreats to validity a out of vocabulary oov handling one of the challenges in next token prediction is predicting tokens that are rare or unseen in the training corpus.
when trained on py150 and of prediction locations observe oov tokens for the py150 and the internal test set respectively.
none of the models in our main evaluation handle oov predictions and thus are unable to correctly predict for such locations.
handling of oov tokens while important is orthogonal to core model design and is outside the scope of our cur rent work.
adding oov handling to a model should further improve its performance as suggested by previous works .
moreover a quick comparison see sec viii against pointermixture from found that travtrans already has an edge even without oov handling for the py150 dataset.
we thus do not expect oov handling to reverse our findings.
b training corpus while larger python corpora have appeared py150 is still sizable at 500mb we do not expect the larger corpora to reverse our findings.
c python specificity we have only carried out evaluations on python and have not demonstrated that our results would carry over in trends to other languages.
the deep3 paper did find their results in trends to roughly carry over from python to javascript.
d model comparison to bound the scope of this paper we limit our investigation to techniques that do not require any compiler support beyond constructing the ast thus we exclude ways to communicate program analysis results such as def use information as in .
we also limit our study to the transformer architecture and purposely exclude the graph neural networks gnn because there is already an active debate between gnn and transformer for nlp applications that transcends code prediction.
viii.
r elated work we compared our models extensively with three baselines seqrnn deep3 and code2seq both qualitatively in sec ii and quantitatively in sec v. before proceeding to a broader discussion we briefly compare our work with the attentive pointer mixture model pointermixture from li at al.
as another plausible baseline.
we are particularly interested in this model because it works with serialized ast node sequences uses attention mechanism on top of lstm and has reportedly achieved better accuracy than deep3 s results.
the similarities and differences in relation to our work are as follows.
for the preorder traversal of the ast they used is similar to ours.
they did not separate types and values into different nodes as we do which makes their sequences more compact than ours.
for their attention is applied within a fixed sized window on top of lstm outputs whereas our transformer based models use stacked layers of attentions as the key mechanism for computing throughout the network.
also used parent attention which is based on the parentchild relation in the ast.
in comparison our pathtrans and travtrans extend far beyond the direct parent child relation by making use of paths.
for the accuracy reported in included the predictions of null values for internal nodes whereas our numbers only consider leaf nodes with non empty values.
to fairly compare the predicative power between our proposal and theirs we implemented pointermixture in our setting.13note that pointermixture uses a pointer network which decides for each 13to maintain consistency the hidden size and the embedding sizes were set to and the vocab size was increased to 100k.
the model was trained for epochs.
159dataset py150 internal applications pointermixture travtrans oov rate pointermixture travtrans oov rate attribute access .
.
.
.
.
.
numeric constant .
.
.
.
.
.
name variable module .
.
.
.
.
.
function parameter name .
.
.
.
.
.
all leaf nodes .
.
.
.
.
.
table xi mrr of pointermixture compared against travtrans for various types of next token prediction for py150 and internal dataset.
the out of vocabulary oov rates for internal dataset is much higher compared to py150 dataset.
point of prediction whether to use the lstm output or to copy from a previous location whereas our travtrans has no means for handling out of vocabulary oov tokens i.e.
all predictions requiring an oov token are incorrect.
we found on the basis of py150 dataset that pointermixture outperforms seqrnn as well as the other baselines of deep3 and code2seq.
however travtrans outperforms pointermixture even though all the oov predictions count as wrong for travtrans!
for the internal dataset pointermixture model does better than travtrans for some prediction types.
this is expected we trained the model for illustration on a different dataset py150 than the one for evaluation internal causing a significantly higher percentage of oov predictions.
in actual usage we would retrain our models on the internal dataset.
the results are shown in table xi focusing on pointermixture vs. travtrans.
since we have already introduced a brief history of autocomplete in the paper this section will focus on other explorations in the transformer and code prediction space.
a code prediction in this paper we viewed the context of prediction as code that appears strictly before the cursor .
among other flavors of code prediction are the ones where code after the prediction location when available is taken into account e.g.
when completing a hole in a program or correcting a misused local variable instance.
another dimension of work considers prediction at varying granularities of predictions e.g.
from characters to subtokens to ast fragments e.g.
sub asts .
in the context of autocomplete we believe that subtokens or bpe are indeed a promising future direction as we mentioned in sec ix.
there have also been work discussing the practical implications of applying a code prediction tool into production.
state that synthetic benchmarks are not representative of real world data and accuracy of the models drops when evaluated on real world data.
discusses approaches to make models more lightweight to allow for faster computations and less memory usage in an ide.
a user evaluation of an autocomplete tool with stronger ml models is outside the scope of this paper.
we would also like to point out that advantages in idealistic settings often transfer to advantages in more practical settings as confirmed by the results reported in the above two works table ii and iii in and table in .
b transformers other than next token prediction transformers have been used recently for code summariza tion .
furthermore there has been a surge of interest since in extending transformer models to handle beyond sequential structures for nlp .
it has been shown that taking tree structure into account helped code correction and code translation .
there is practical interest outside of academic literature in the topic of code prediction using transformers.
galois is an open source project that uses gpt for code prediction.
tabninetm published a blog post in july mentioning the use of gpt in their code prediction but revealed no technical detail.
recently a team from microsoft also published their ongoing efforts on applying transformers for code autocomplete.
we believe we are among the first to systematically evaluate transformers for code prediction and compare them to previous models.
c deep learning techniques over code beyond code prediction there are many other uses of deep learning techniques for code beyond code prediction.
these include techniques for code summarization bug finding repair and many others.
an interesting aspect of this body of work is in the different ways in which they represent a program as an input to a neural architecture.
these representations have ranged from linear token sequence as for code prediction to paths in an ast and sometimes even ways to convey static analysis information to the neural network .
ix.
c onclusion and future work in this paper we presented ways to using the transformer for code prediction.
we showed that the transformer outperforms existing models for code prediction and when supplied with code s structural information we are able to get even better predictive power.
attribution study show that our best model tends to focus on relevant code locations for prediction.
in the future one avenue we wish to continue working on is handling out of vocabulary words better.
source code presents a difficulty shared with nlp in handling large vocabularies and rare words.
the token word to be predicted in test data may not appear in the training data.
this is even more challenging when predicting identifiers such as method names variable names as developers can come up with arbitrary identifier names.
possible mitigation includes copying mechanism and open vocabulary models .
this paper focused on predicting the next token as it is already a challenging task.
in future we also want to explore predicting multiple tokens at a time i.e.
autocompleting entire expressions.
160references a. hindle e. t. barr m. gabel z. su and p. devanbu on the naturalness of software communications of the acm vol.
no.
pp.
.
m. allamanis e. t. barr p. devanbu and c. sutton a survey of machine learning for big code and naturalness acm computing surveys csur vol.
no.
pp.
.
t. t. nguyen a. t. nguyen h. a. nguyen and t. n. nguyen a statistical semantic language model for source code in proceedings of the 9th joint meeting on foundations of software engineering ser.
esec fse .
new york ny usa association for computing machinery p. .
.
available c. liu x. wang r. shin j. e. gonzalez and d. song neural code completion .
.
available id rjbpbt9lg j. li y .
wang m. r. lyu and i. king code completion with neural attention and pointer networks in proceedings of the 27th international joint conference on artificial intelligence ser.
ijcai .
aaai press p. .
f. liu l. zhang and z. jin modeling programs hierarchically with stack augmented lstm journal of systems and software p. .
.
available r. m. karampatsis h. babii r. robbes c. sutton and a. janes big code !
big vocabulary open vocabulary models for source code in international conference on software engineering icse .
a. svyatkovskiy y .
zhao s. fu and n. sundaresan pythia ai assisted code completion system proceedings of the 25th acm sigkdd international conference on knowledge discovery data mining jul .
.
available v .
raychev m. vechev and e. yahav code completion with statistical language models in proceedings of the 35th acm sigplan conference on programming language design and implementation ser.
pldi .
new york ny usa association for computing machinery p. .
.
available .
g. a. aye and g. e. kaiser sequence model design for code completion in the modern ide arxiv preprint arxiv .
.
m. allamanis and c. sutton mining idioms from source code in proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering pp.
.
p. bielik v .
raychev and m. vechev phog probabilistic model for code in international conference on machine learning pp.
.
v .
raychev p. bielik m. vechev and a. krause learning programs from noisy data in proceedings of the 43rd annual acm sigplan sigact symposium on principles of programming languages ser.
popl .
new york ny usa association for computing machinery p. .
.
available v .
raychev p. bielik and m. vechev probabilistic model for code with decision trees in proceedings of the acm sigplan international conference on object oriented programming systems languages and applications ser.
oopsla .
new york ny usa association for computing machinery p. .
.
available u. alon o. levy and e. yahav code2seq generating sequences from structured representations of code in international conference on learning representations .
.
available https openreview.net forum?id h1gkyo09tx 150k python dataset .
.
available io py150 s. iyer i. konstas a. cheung and l. zettlemoyer summarizing source code using a neural attention model in proceedings of the 54th annual meeting of the association for computational linguistics volume long papers .
berlin germany association for computational linguistics aug. pp.
.
.
available j. devlin m. w. chang k. lee and k. toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint arxiv .
.
l. dong n. yang w. wang f. wei x. liu y .
wang j. gao m. zhou and h. w. hon unified language model pre trainingfor natural language understanding and generation in advances in neural information processing systems pp.
.
.
available a. radford j. wu r. child d. luan d. amodei and i. sutskever language models are unsupervised multitask learners in openai blog .
.
available u. alon m. zilberstein o. levy and e. yahav code2vec learning distributed representations of code proceedings of the acm on programming languages vol.
no.
popl pp.
.
.
available v .
j. hellendoorn and p. devanbu are deep neural networks the best choice for modeling source code?
in proceedings of the 11th joint meeting on foundations of software engineering pp.
.
k. simonyan a. vedaldi and a. zisserman deep inside convolutional networks visualising image classification models and saliency maps arxiv preprint arxiv .
.
v .
j. hellendoorn c. sutton r. singh and p. maniatis global relational models of source code in international conference on learning representations .
.
available https openreview.net forum?id b1lnbrntwr j. harer c. reale and p. chin tree transformer a transformerbased method for correction of tree structured data arxiv preprint arxiv .
.
v .
shiv and c. quirk novel positional encodings to enable tree based transformers in advances in neural information processing systems pp.
.
.
available pretrained probabilistic models for code .
.
available a. vaswani n. shazeer n. parmar j. uszkoreit l. jones a. n. gomez u. kaiser and i. polosukhin attention is all you need in proceedings of the 31st international conference on neural information processing systems ser.
nips .
red hook ny usa curran associates inc. p. .
v .
j. hellendoorn c. sutton r. singh p. maniatis and d. bieber global relational models of source code in international conference on learning representations .
.
available https openreview.net forum?id b1lnbrntwr r. al rfou d. choe n. constant m. guo and l. jones characterlevel language modeling with deeper self attention in proceedings of the aaai conference on artificial intelligence vol.
no.
pp.
.
k. irie a. zeyer r. schl uter and h. ney language modeling with deep transformers interspeech sep .
.
available v .
j. hellendoorn and p. t. devanbu are deep neural networks the best choice for modeling source code?
in proceedings of the 11th joint meeting on foundations of software engineering .
n. akhtar and a. mian threat of adversarial attacks on deep learning in computer vision a survey ieee access vol.
pp.
.
a. chakraborty m. alam v .
dey a. chattopadhyay and d. mukhopadhyay adversarial attacks and defences a survey arxiv preprint arxiv .
.
w. e. zhang q. z. sheng a. alhazmi and c. li adversarial attacks on deep learning models in natural language processing a survey acm transactions on intelligent systems and technology tist vol.
no.
pp.
.
s. chakraborty r. tomsett r. raghavendra d. harborne m. alzantot f. cerutti m. srivastava a. preece s. julier r. m. rao et al.
interpretability of deep learning models a survey of results in ieee smartworld ubiquitous intelligence computing advanced trusted computed scalable computing communications cloud big data computing internet of people and smart city innovation smartworld scalcom uic atc cbdcom iop sci .
ieee pp.
.
q. s. zhang and s. c. zhu visual interpretability for deep learning a survey frontiers of information technology electronic engineering vol.
no.
pp.
.
d. v .
carvalho e. m. pereira and j. s. cardoso machine learning interpretability a survey on methods and metrics electronics vol.
no.
p. .
x. huang d. kroening w. ruan j. sharp y .
sun e. thamo m. wu and x. yi a survey of safety and trustworthiness of deep neural networks verification testing adversarial attack and defence and interpretability computer science review vol.
p. .
k. wang and m. christodorescu coset a benchmark for evaluating neural program embeddings arxiv preprint arxiv .
.
n. yefet u. alon and e. yahav adversarial examples for models of code proceedings of the acm on programming languages vol.
no.
oopsla nov. .
.
available g. ramakrishnan j. henkel z. wang a. albarghouthi s. jha and t. reps semantic robustness of models of source code arxiv preprint arxiv .
.
j. jiarpakdee c. tantithamthavorn h. k. dam and j. grundy an empirical study of model agnostic techniques for defect prediction models ieee transactions on software engineering pp.
.
s. wattanakriengkrai p. thongtanunam c. tantithamthavorn h. hata and k. matsumoto predicting defective lines using a model agnostic technique ieee transactions on software engineering no.
pp.
sep .
m. t. ribeiro s. singh and c. guestrin why should i trust you?
explaining the predictions of any classifier in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining ser.
kdd .
new york ny usa association for computing machinery p. .
.
available d. smilkov n. thorat b. kim f. vi egas and m. wattenberg smoothgrad removing noise by adding noise arxiv preprint arxiv .
.
m. sundararajan a. taly and q. yan axiomatic attribution for deep networks in proceedings of the 34th international conference on machine learning volume ser.
icml .
jmlr.org p. .
m. allamanis h. peng and c. sutton a convolutional attention network for extreme summarization of source code in proceedings of the 33rd international conference on machine learning ser.
proceedings of machine learning research m. f. balcan and k. q. weinberger eds.
vol.
.
new york new york usa pmlr jun pp.
.
.
available m. allamanis m. brockschmidt and m. khademi learning to represent programs with graphs in international conference on learning representations .
.
available https openreview.net forum?id bjofetxr o. vinyals m. fortunato and n. jaitly pointer networks arxiv preprint arxiv .
.
m. brockschmidt m. allamanis a. l. gaunt and o. polozov generative code modeling with graphs in international conference on learning representations .
.
available https openreview.net forum?id bke4ksa5fx u. alon r. sadaka o. levy and e. yahav structural language models of code in proceedings of the 37th international conference on machine learning ser.
proceedings of machine learning research h. d. iii and a. singh eds.
vol.
.
pmlr jul pp.
.
.
available p. bielik v .
raychev and m. vechev program synthesis for character level language modeling in international conference on learning representations .
.
available https openreview.net forum?id ry sjfqgx v .
j. hellendoorn s. proksch h. c. gall and a. bacchelli when code completion fails a case study on real world completions in proceedings of the 41st international conference on software engineering p. .
g. a. aye s. kim and h. li learning autocompletion from realworld datasets in proceedings of the acm ieee 43rd international conference on software engineering software engineering in practice .
.
available a. svyatkovskiy s. lee a. hadjitofi m. riechert j. franco and m. allamanis fast and memory efficient neural code completion .
y .
wang h. y .
lee and y .
n. chen tree transformer integrating tree structures into self attention in proceedings of the conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp ijcnlp pp.
.
.
available https m. ahmed m. r. samee and r. e. mercer you only need attention to traverse trees in proceedings of the 57th annual meeting of the association for computational linguistics pp.
.
.
available x. p. nguyen s. joty s. hoi and r. socher tree structured attention with hierarchical accumulation in international conference on learning representations .
.
available https openreview.net forum?id hjxk5peyvr galois autocompleter tabnine autocompletion with deep learning tabnine blog jul .
.
available a. svyatkovskiy s. k. deng s. fu and n. sundaresan intellicode compose code generation using transformer .
new york ny usa association for computing machinery p. .
m. vasic a. kanade p. maniatis d. bieber and r. singh neural program repair by jointly learning to localize and repair arxiv preprint arxiv .
.
y .
yang and c. xiang improve language modelling for code completion through learning general token repetition of source code in 31st international conference software engineering and knowledge engineering pp.
.
p. fernandes m. allamanis and m. brockschmidt structured neural summarization in international conference on learning representations .
.
available forum?id h1ersorqtm m. cvitkovic b. singh and a. anandkumar open vocabulary learning on source code with a graph structured cache in proceedings of the 36th international conference on machine learning ser.
proceedings of machine learning research k. chaudhuri and r. salakhutdinov eds.
vol.
.
long beach california usa pmlr jun pp.
.
.
available