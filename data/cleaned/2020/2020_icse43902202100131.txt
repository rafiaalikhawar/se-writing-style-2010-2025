deeplv suggesting log levels using ordinal based neural networks zhenhao li heng liy tse hsun peter chen and weiyi shang department of computer science and software engineering concordia university montreal canada flzhenha peterc shang g encs.concordia.ca ydepartment of computer engineering and software engineering polytechnique montr eal montreal canada heng.li polymtl.ca abstract developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems.
log level is an important component of a logging statement which enables developers to control the information to be generated at system runtime.
however due to the complexity of software systems and their runtime behaviors deciding a proper log level for a logging statement is a challenging task.
for example choosing a higher level e.g.
error for a trivial event may confuse end users and increase system maintenance overhead while choosing a lower level e.g.
trace for a critical event may prevent the important execution information to be conveyed opportunely.
in this paper we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels.
we find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels and log levels that are further apart in order e.g.
trace and error tend to have more differences in their characteristics.
based on this we then propose a deep learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels by using the syntactic context and message features of the logging statements extracted from the source code.
through an evaluation on nine large scale open source projects we find that our approach outperforms the state of the art baseline approaches we can further improve the performance of our approach by enlarging the training data obtained from other systems our approach also achieves promising results on cross system suggestions that are even better than the baseline approaches on within system suggestions.
our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.
index terms logs deep learning log level empirical study i. i ntroduction software logs have been widely used in practice for various maintenance activities such as testing failure diagnosis and program comprehension .
developers insert logging statements in the source code with different verbosity levels e.g.
trace debug info warn error and fatal to record system execution information and values of dynamic variables.
for example in the logging statement log.info stopping server servername the static text message is stopping server and the dynamic message is the value of the variable servername .
the logging statement is at theinfo level which is the level for recording informational messages that highlight the progress of the application at a coarse grained level .
log levels enable developers to only print important log messages e.g.
error or warning information at runtime whilesuppressing less important messages e.g.
debug messages .
it is important for developers to choose the right log levels for their logging statements.
on one hand choosing a lower log level e.g.
debug for a critical event can hide important runtime information and make it difficult to diagnose runtime failures .
on the other hand choosing a higher level e.g.
warn for a trivial event can confuse end users and increase the overhead of log management and analysis .
however it is usually challenging for developers to choose a proper log level for the logging statements .
prior studies shows that developers may not have sufficient understanding of the runtime behaviors of their systems and the purposes of different log levels leading to suboptimial choices of log levels.
in particular prior work observes that developers spend significant efforts in modifying the levels of existing logging statements as it is challenging for them to make the right decisions in the first place.
in this paper we conduct a study to help developers make informed decisions on deciding proper log levels.
through a preliminary manual study on the logging statements from nine open source systems we find that the decisions of log levels might be related to the locations of the logging statements and the messages to be recorded and log levels that are further apart in order e.g.
trace anderror tend to have more differences in their characteristics of locations and messages.
we then extract syntactic context features to represent the location information of the logging statements as well as their log messages and propose a deep learning based approach to automatically suggest log levels.
unlike other multi class classification tasks which consider the classes as independent log levels have an ordinal nature i.e.
the levels preserve an order among each other.
therefore we ordinally encode the log levels to capture their ordinal nature.
we evaluate our approach on nine large scale open source systems and compare the results with two baseline approaches a state of the art ordinal regression approach from a prior study and a deep learning based approach with standard one hot encoding.
we find that our models trained using the syntactic context feature achieve an average auc of .
outperforming our models trained using the log message feature i.e.
with an average auc of .
in suggesting log levels.
combining both features in our approach would lead to the best performance i.e.
with an average auc of .
.
ieee acm 43rd international conference on software engineering icse .
ieee trained from either the syntactic context feature i.e.
without log message feature or the combined feature i.e.
with log message feature our approach outperforms both baseline approaches in all the studied systems.
by further studying the results of our approach we find that the syntactic context and combined features have a similar capability of distinguishing different log levels while the log message feature may only be useful for specific levels such as error andwarn .
finally we evaluate the benefit and applicability of using data from other systems to enlarge the training data.
we find that by carefully choosing the training dataset from other systems the results of our approach can be further improved.
in addition our approach can achieve encouraging results on cross system suggestions e.g.
on average .
of the accuracy of within system suggestions which still outperform the baseline approaches on within system suggestions.
the contributions of this paper are as follows we propose an automated deep learning based approach that leverages the ordinal nature of log levels to make suggestions on choosing log levels1.
our approach outperforms the existing state of the art approaches in suggesting log levels.
our approach have encouraging cross system suggestion results which can benefit the systems without long development histories.
our manual study results can be leveraged as guidelines in future research on suggesting and improving log levels.
in short our findings highlight the potentials of leveraging the characteristics of logging statements in suggesting log levels that can help developers make informed logging decisions.
our results also reveal the challenges and future research directions in assisting developers with logging.
paper organization.
section ii discusses the setup and results of manually studying the characteristics of log levels.
section iii describes our deep learning approach on suggesting log levels.
section iv presents the evaluation results of our approach by answering three research questions.
section v discusses the threats to the validity of our study.
section vi summarizes the related work.
section vii concludes the paper.
ii.
p reliminary study on log levels a. an overview of the studied systems studied systems.
we conduct the study on nine large scale open source java systems.
table i shows an overview of the systems.
the studied systems are in various sizes loc from 97k to .5m and nol from .4k to .5k have high quality logging code are commonly used in prior log related studies and cover various domains e.g.
database systems and search engines .
log level distribution.
table i shows the distribution of the log levels in the studied systems.
we find that many logging statements are used to show potential issues during system 1we share the data of this paper in the repository spear se icse2021 log level data.table i an overview of the studied systems and their log level distributions system version loc nol trace debug info warn error fatal cassandra .
.
432k .3k .
.
.
.
.
.
elasticsearch .
.
.50m .5k .
.
.
.
.
.
flink .
.
177k .5k .
.
.
.
.
.
hbase .
.
.26m .5k .
.
.
.
.
.
jmeter .
.
143k .9k .
.
.
.
.
.
kafka .
.
267k .5k .
.
.
.
.
.
karaf .
.
133k .8k .
.
.
.
.
.
wicket .
.
216k .4k .
.
.
.
.
.
zookeeper .
.
97k .2k .
.
.
.
.
.
average 469k .0k .
.
.
.
.
.
note loc refers to the lines of code nol refers to the number of logging statements.
execution i.e.
on average .
are at the warn level and .
are at the error level .
note that in modern logging frameworks such as slf4j fatal level is removed due to its redundancy with other log levels such as error .
as we found in the studied systems only karaf contains some logging statements with fatal level and the number is very small only .
.
therefore we focus our study on the other log levels.
we find that there is also a large proportion of the logging statements that are used for debugging i.e.
.
for debug .
as mentioned by the instruction of slf4j the trace level is not recommended since it has a high overlap with thedebug level .
hence it may be the reason that some systems have noticeably fewer logging statements at the trace level.
in general there are fewer logging statements that show the general system execution i.e.
.
are at the infolevel .
our preliminary findings show that the studied systems have a different distribution of log levels and the levels are not evenly distributed.
therefore suggesting log levels accurately either within the same or cross systems is a challenging task.
b. investigating log level related issues we collect the most recently resolved issue reports from jan. to jul.
in the bug tracking systems of our studied systems and identify the log related issue reports by examining if there are changes or patches on logging statements issue reports in total .
we then manually examine the changes and the discussions in those issue reports.
we find that a large portion .
of the issue reports have changes or discussions on log levels.
specifically for .
of the log level related issue reports developers suggested changes of log levels on existing logging statements.
for .
of the log level related issue reports developers suggested adding new logging statements and mentioned the reasons of the log levels of those newly added logging statements based on their execution point and the messages.
in short the proper choice of log levels is important and is actively considered by developers in both processes of improving existing logging statements and composing new ones.
both the locations and the messages of the logging statements might be important for deciding log levels.
c. manually studying the characteristics of log levels prior work found that developers spend significant efforts modifying the levels of existing logging 1462statements that were inserted previously and they tend to evaluate the impact of their logging statements and adjust their log levels over time .
motivated by the prior studies and our investigation on log level related issue reports we conduct a manual study to investigate the characteristics of different log levels in order to better provide supports for developers on deciding log levels.
in particular we study the message and location of a logging statement to investigate if a log level is implicitly or explicitly related to the context information or the log message of the logging statement.
manual study process.
to prepare the data for our manual study we first extract the logging statements from the source code using static analysis.
we identify the method invocation statements that invoke common logging libraries e.g.
log4j and slf4j .
then for each identified logging statement we extract its log message including static message and dynamic variables verbosity level and the method that contains the logging statement.
in total we extract .6k logging statements from the nine studied systems.
then we randomly sample out of .6k logging statements based on a confidence level and a confidence interval .
we apply stratified sampling to ensure the distribution of logging statements from different systems and their log levels in the sampled data is the same as the complete data .
our manual study contains the following three phases phase i we leverage the categories of logging locations and messages that were derived in prior studies .
two authors of this paper i.e.
a1 and a2 use the categories to categorize randomly sampled logging statements collaboratively.
during this phase the categories of logging locations and log messages are revised and refined.
in the end we reused and revised three categories of logging locations and three categories of log messages.
we also derived two new categories of logging locations in this phase.
phase ii a1 and a2 independently categorized the rest of the sampled logging statements logging statements by using the categories derived in phase i. phase iii a1 and a2 compared the results from phase ii.
any disagreement of the categorization was discussed until reaching a consensus.
no new categories were introduced during the discussion.
the results in this phase have a substantiallevel of agreement for both of the categorizations of logging location and log message cohen s kappa of .
and .
for logging location and log message respectively .
manual study results.
table ii shows the distribution of the categories of logging locations and log messages for different log levels.
each row represents the number of logging statements that belong to each category and each column represents the number of logging statements with each log level.
the percentage in each cell shows the ratio between the logging statements out of the total sampled logging statements with the corresponding log level.
below we discuss the results by each category.table ii the distribution of the categories of logging locations and log messages for each log level category trace debug info warn error locationct .
.
.
.
.
lb .
.
.
.
.
lp .
.
.
.
.
mt .
.
.
.
.
op .
.
.
.
.
messageod .
.
.
.
.
vd .
.
.
.
.
nd .
.
.
.
.
categories of logging locations v.s.
log levels location catch clause ct .
catch clause is used for capturing the exceptions raised during the execution.
as shown in the code snippet below developers often log the exception information e.g.
the context information of the execution point in catch clauses .
in our manual study we find that a large portion of the sampled warn .
and error .
logging statements are in this category.
however there are still a non negligible number of logging statements that have different log levels.
the percentage for the other three levels ranges from .
at the debug level to .
at the info level .
location category catch block ct catch exception ex log.error failed to stop infoserver ex location logic branch lb .
logic branch is the code statement that leads to different system execution paths e.g.
if else and switch .
developers may insert logging statements in the logic branches to help identify the execution path or record the information in some critical branches.
as shown in the code snippet below developers added a warn logging statement to record an unexpected branch execution.
we find that the distribution of the five log levels for the logging statements in lb are similar.
each log level has many logging statements in this category the percentage ranges from .
at the error level to .
at the warn level .
location category logic branch lb if logfilereader null log.warn nothing to split in wal logpath return true else location looping block lp .
logging statements in looping blocks e.g.
forand while may record the execution state during iterating e.g.
recording the ith execution inside aforblock or recording variable values as shown in the code snippet below.
we do not find any logging statements at the warn orerror level that belong to this category.
the logging statements that belong to this category generally have three log levels .
at the trace level .
at the debug level and .
at the info level.
location category looping block lp while active logger.trace checking jobs clock.instant .atzone zoneoffset.utc checkjobs location method start or end mt .
logging statements might reside at the beginning or the end of a method mostly for recording the program execution state or debugging purposes.
for example the code snippet below logs the event 1463execution time whenever the method is executed.
we find that .
of the logging statements are at the trace level .
are at the debug level and .
are at the info level.
however logging statements with warn and error level only have a small portion .
and .
respectively.
location category method start or end mt public void oneventtime long timertimestamp logger.trace oneventtime timertimestamp location observation point op .
we categorize the rest logging locations that do not belong to any of the abovementioned categories as observation point .
logging statements in this category may have various characteristics of logging locations such as locating before the entry point or after the exit point of a code block to record the execution status as shown in the code snippet below .
we find that a large portion of logging statements that belong to this category from .
to .
is at the trace debug and info level while only .
and .
are at the warn and error level respectively.
location category observation point op final binaryinmemorysortbuffer buffer currwritebuffer.buffer log.debug retrieved empty read buffer currwritebuffer.id .
long occupancy buffer.getoccupancy if !buffer.write current categories of log messages v.s.
log levels message operation description od .
log messages in this category summarize the actions or intentions of its surrounding code .
logging statements with this kind of log message could be placed before inside or after the execution point to record the status of an upcoming ongoing or a completed operation.
as shown in the example below an info logging statement logs the closing of a connection.
we find that most of the infologging statements .
are in this category.
there are also a large portion of trace .
and debug .
logging statements in this category.
for warn anderror level .
and .
of the logging statements belong to this category.
message category operation descripion od connectiontracker.closeall logger.info stop listening for cql clients message variable description vd .
variable description records the value of a variable during execution .
as shown in the example below a trace logging statement is placed after defining the variable parametermap to record its value.
we find that many logging statements at the trace .
anddebug .
level belong to this category.
for other levels the percentage is noticeably smaller from .
at the error level to .
at the info level .
message category variable description vd map string list string parametermap request.getparametermap log.trace parametermap parametermap if parametermap !
null message negative execution behavior description nd .
during the system runtime some unexpected execution behaviors may happen e.g.
an exception or a failure .
logging statements are often inserted into these unexpectedexecution points to record the related information.
hence developers can then be aware of the problem and fix the issue.
we consider log messages as this category if they describe an unsuccessful attempt or an unexpected situation with some specific negative words e.g.
fail exception unable .
we find that most of the error .
and a large number of warn .
logging statements are in this category.
forinfo anddebug level there are .
and .
of the logging statements in this category respectively.
we do not find logging statements at the trace level that belong to this category.
message category negative execution behavior description nd if tokensindex.isavailable false logger.warn failed to get access token tokenid listener.onresponse null else summary of the manual study findings as we found in our manual study the information of logging locations and log messages may be related to the decision of log levels.
for example we find that developers are more likely to set the log level to warn orerror if the logging statement resides in catch blocks category ct .
moreover if the logging statements reside at the beginning or end of a method category mt the log levels are more likely to betrace ordebug .
similarly logging statements with certain types of log messages such as the category vd variable description are more often set to trace ordebug level.
log levels that are further apart in order e.g.
trace and error tend to have more different characteristics to distinguish.
our findings shed light on the relationship between log levels and the categories of logging location and log messages as well as the ordinal nature of log levels that may be further leveraged to assist developers in determining log levels.
we find that log levels that are further apart in order tend to have more different characteristics of logging locations and log messages.
locations and messages of logging statements as well as the ordinal nature of log levels might be leveraged to help decide log levels.
iii.automaticallysuggestingloglevels inspired by our manual study findings in this section we propose an approach that automatically suggests log levels.
w e formulate the process of suggesting log levels as a multi class classification problem.
given the information of an existing or a potential ne w logging statement i.e.
the structural information or the log message or both we apply deep learning models to suggest which le velto use.
below we discuss ho w we extract the fe atures and the frame work of our deep learning approach for suggesting log levels.
a. f eature extraction for each logging statement we extract three types of features syntactic context features simplified assyn in the rest of paper log message features msg and combined features of syntactic conte xt and log messages comb .
1464public void notifycheckpointcomplete long cp boolean success false if isrunning log.debug notification of complete check point cp for streamoperator operator opchain.getoperators process operator ... scope of basic block syntactic context feature scope of logging statement b0b1b2 syntactic methoddeclar vardeclar booleanliteral ifstm logstm methodinvoc message combined methoddeclar vardeclar booleanliteral ifstm logstm msgstart notif complet check point msgend methodinvoc note the number following syntactic token is only for indicating the line number in this example it does not appear in the actual featurenote msgstart and msgend are the tokens inserted into combined features in order to help the model distinguish the message tokens from syntactic tokens getstatus cp 8fig.
.
an example of the syntactic log message and combined features we extracted for each logging statement syntactic context features.
we extract the syntactic context feature that represents the location information of a logging statement.
specifically we parse the abstract syntax tree ast of the source code and extract the ast nodes that are related to the control flow of the code to capture the structural information e.g.
ifstatement and catchclause .
we exclude the ast nodes that do not contain structural information of the code such as simplename i.e.
identifier name and simpletype i.e.
identifier type .
we also exclude ast nodes that are related to log guards e.g.
if istraceenabled .
for each logging statement we count the occurrence of each ast node from the start of the method to the end of the basic block in which the logging statement resides.
we analyze the ast nodes from the beginning of a method since the nodes represent the syntactic context of the logging statement e.g.
the logical flow of the method .
as we found in section ii such syntactic context have a certain relationships with log levels.
we choose to extract the features based on basic blocks since they represent a sequence of code statements where there is no branching in between i.e.
no other structural information that can affect the decision of the level of a logging statement in the block .
finally we obtain a set of tokens i.e.
ast nodes for each logging statement that represents the syntactic feature of the logging statement.
figure shows an example of the syntactic context feature that we extract for the logging statement on line .
log message features.
we extract the log message features from the textual information inside the logging statements.
we exclude the dynamic variables since many variable names in logging statements are not composed of natural language words e.g.
variable cpin the logging statement in figure which is abbreviated from check point .
for the static message in each logging statement we first split the words using space and camel cases.
we then follow common text pre processing techniques remove the punctuation convert the words into lower case filter the common english words and apply stemming on the filtered words .
at the end of this process we obtain a set of log sour ce code ... featur e v ectors interger representation embedding layer...... rnn layer bi lstm output layer ordinal representation layer unit rnn cell ......fig.
.
overall framework of our approach message tokens which represents its log message feature for each logging statement.
combined features.
as we found in section ii both the logging locations and log messages may have a certain relationship with the log levels as they capture different aspects of a logging statement.
therefore we combine both the syntactic information and the log message by following an approach that is similar to prior studies .
for each logging statement we add the log message feature to the syntactic feature and preserve their actual order in the source code i.e.
the log message feature is added to the place that the logging statement appears in the source code .
we then add a special token at the beginning and the end of the log message feature to help the model distinguish it with syntactic information.
finally we obtain a set of tokens for each logging statement that represents the combined feature of the logging statement.
figure shows an example of how do we combine the syntactic context feature and log message feature for the logging statement in line .
ordinally encoding log levels.
one hot encoding is widely used by prior studies for multi class classification problems .
however log levels by nature have an ordinal relationship.
for example if the system is configured to run and record debug logs the system would also enable logging statements that are at the info warn and error levels and record logs in those levels.
therefore we ordinally encode the log levels to preserve such ordinal relationship when suggesting log levels.
table iii shows the comparison between the vectors of each log level that are ordinally encoded and encoded by standard one hot encoding.
our encoding preserves the ordinal characteristics of log levels where when a system is configured to record a certain log level e.g.
info the system would also record all logs that have a higher log level e.g.
warn anderror .
b. deep learning framework and implementation overall architecture.
figure shows the overall architecture of our approach.
the deep learning framework contains an embedding layer an rnn layer and an output layer.
given the syntactic features log message features or combined features of logging statements the embedding layer learns the relationship among the input vectors and transform each vector to a distributed representation based on probability.
we 1465then use a recurrent neural network rnn layer to learn the relationship between the log level and the embedded vectors returned from the embedding layer.
after that the output layer gives an ordinal vector as the suggestion result.
finally we map the ordinal vector returned from the output layer to a real log level as the final result.
below we discuss the details of each component of our approach.
embedding layer.
our extracted features i.e.
syn msg andcomb are represented in the form of vectors.
each dimension represents the unique tokens of the corresponding feature e.g.
the types of ast node in syn and each element represents the number of occurrences of the token for each logging statement.
we then feed the feature vectors into the embedding layer.
the embedding layer captures the linear relationships among the tokens in the feature vectors and outputs the probabilistic representations of the vectors i.e.
word embeddings .
in other words word embeddings learn the similarities among the tokens to create a more concise representation of the features .
rnn layer.
we model the source code and log message as sequential data i.e.
the order of the tokens that appear in the source code is preserved by following prior studies .
we employ a layer of bidirectional long short term memory bi lstm in the deep learning model which is widely used by prior studies to process source code and natural language .
bi lstm is a variant of rnn that concatenates the outputs of two rnns one processing the sequence of input vector from the beginning to the end the other one from the end to the beginning.
each rnn is composed of recurrent units including a memory cell and gate mechanisms to preserve long term dependencies of the given input.
while training the model we encode the log level of a logging statement into its ordinal representation as discussed in subsection iii a .
output layer.
we then use a five dimension dense layer as the output layer.
specifically the output layer takes the highdimensional output vectors from the previous layer i.e.
the rnn layer to the five neurons in this layer.
each of the five neuron represents one number in our ordinally encoded vector of log level.
then each neuron gives the result of the corresponding number i.e.
the probability of this digit to be in the vector.
after that we accept the output vector and map the vector into an actual log level as the final suggestion result.
for example if the returned vector from the output layer is we check each probability value from the start to the end of the vector.
if a probability is larger than .
the number is mapped to .
if a probability that is smaller than .
is encountered the rest numbers will be mapped to .
in the above mentioned example the output vector will be mapped to as discussed in subsection iii a which is an info level.
implementation and training we use keras to implement our deep learning framework.
we use skip gram from word2vec in the embedding layer and set the dimension to by following prior work .
we obtain the wordtable iii acomparison between the vectors of log levels that are ordinally encoded and one hot encoded ordinally encoded one hot encoded trace debug info warn error embeddings for each type of features i.e.
syntactic context log message and combined features separately.
for the rnn layer we set the number of units i.e.
the dimension of hidden states as and attach a dropout layer with a .
dropout rate in order to reduce the potential impact of overfitting on the trained system .
for each training process we set the number of epochs as and the batch size as .
since the model learns and predicts on each digit of the ordinally encoded vector we use sigmoid as the activation function and use binary cross entropy as the loss function.
note that the distribution of log levels is noticeably different e.g.
on average only .
of the logging statements are intrace while .
of the logging statement are in error level as discussed in section ii.
hence we apply stratified random sampling while splitting the training validation and testing data to ensure the sampled data set has the same distribution of log levels as the original data.
iv.
e valuation a. evaluation metrics we use accuracy and area under the curve auc which are widely used by prior multi class classification studies to evaluate our approach .
according to the ordinal nature of log levels we also propose a new metric average ordinal distance score aod to measure the average distance between the actual level and the suggested level.
accuracy.
similar to the usage in prior classification studies accuracy in our study is the percentage of correctly suggested log levels out of all the suggestion results.
a higher accuracy means a model can correctly suggest the log levels for more logging statements.
as a reference the accuracy of a category random guess is around .
area under the curve auc .
auc is the area under the roc receiver operating characteristic curve that plots the true positive rate against the false positive rate which evaluates the ability of a model in discriminating different classes.
auc ranges between and a high value for the auc indicates a high discriminative ability of a model an auc lower than .
indicates a performance that is not better than random guessing.
following prior work we use a multiple class version of the auc defined by hand et al.
.
the auc gives us the insight about how well the model can discriminate different log levels e.g.
how likely a model is able to predict an actual info level as info i.e.
true positive rather than predict an actual debug level as info i.e.
false positive .
average ordinal distance score aod .
the prior two metrics consider different log levels as independent classes i.e.
1466table iv the results of suggesting logging levels using syntactic context syn log message msg and a combination of both comb compared with ordinal regression or and one hot encoding neurual network oen accuracy auc aod systems syn msg comb or oen syn msg comb or oen syn msg comb or oen cassandra .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
elasticsearch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
flink .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hbase .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jmeter .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kafka .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
karaf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wicket .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
zookeeper .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
note the number that is higher than both of the baselines is marked in bold the best result is marked in italic bold .
the ordinal nature of log levels as discussed in section iii is not considered .
hence we propose average ordinal distance score aod which measures the average distance between the actual log level and the suggested log level for each logging statement.
it is computed as aod pn i dis ai si maxdis ai n where nis the total number of logging statements in the results.
for each logging statement and its suggested log level dis a s is the distance between the actual log level aiand thesuggested log level si e.g.
the distance between error andinfo is .
maxdis a is the maximum possible distance of the actual log level ai.
for example the maximum possible distance for trace is i.e.
from trace toerror for info is i.e.
from info totrace orerror .
a higher aod indicates suggested log levels are closer to their actual log levels.
b. case study results rq1 how effective is our approach in suggesting log levels?
motivation.
as we found in the manual study the decision of log level may be related to the syntactic information in the code and the log message.
in this rq we want to evaluate the performance of our deep learning models trained using each of the three features i.e.
syntactic context log message and combined as described in section iii a .
approach.
we first apply stratified random sampling to split the input data into training set validation set testing set and ensure each of the sampled datasets has the same distribution of log levels as the original data.
we compare our approach with two baselines described below.
we then train our deep learning framework and the two baselines on the training data using each of the three features.
below we describe the two baselines in details.
baseline ordinal regression or model.
we use ordinal regression or models to suggest log levels by following a prior study .
or considers the orders of the log levels e.g.
error is more severe than info when training the model and predicting the log level given a new logging statement.
in this work we migrate the or approach to our problem context suggesting the log level of each logging statement in the static code.
we consider all the metrics used in the prior work except those related to code changes as thecode change related metrics are irrelevant in our context we suggest the log level of each logging statement in the static code.
besides prior work finds that the influence of the metrics related to the code changes is negligible .
baseline one hot encoding rnn oen .
as discussed in section iii the standard one hot encoding treats all the classes as independent classes without considering the ordinal relation among them.
in order to understand the effectiveness of our encoding on log levels we would like to compare the performance of the models using our ordinally encoded log level vectors with the models using standard one hot encoded log level vectors.
different from our approach that uses sigmoid as the activation function and binary cross entropy as the loss function to predict the value of each number in the ordinally encoded vector as discussed in section iii to adopt standard one hot encoding we change the activation function to softmax and the loss function to categorical cross entropy .
hence the goal of the baseline model is to predict the one hot encoded vector as shown in table iii which can be mapped back to a log level.
similar to our approach we train the baseline on the syn sem and comb features respectively.
results and discussions.
our approach can effectively suggest log levels for the studied systems.
our best models i.e.
using the combined feature achieve an average auc of .
.
table iv presents the results of our models trained using the syntactic feature syn the log message feature msg and the combined feature comb .
table iv shows that the models trained using thesyn feature perform better than the models using the msg feature in terms of all the three evaluation metrics.
specifically the average accuracy auc and aod of the models trained using the syn feature are .
.
and .
respectively while the average accuracy auc and aod of the models trained using the msg feature are only .
.
and .
.
more importantly for all the three evaluation metrics the models trained using the comb feature have better results than the models trained only using syn ormsg .
specifically on average the accuracy auc and aod of the models trained using comb are .
.
and .
respectively.
our results show that the syn andmsg features both provide valuable information that can complement each other in our models.
our approach outperforms the two baseline approaches 1467table v the distribution of incorrectly suggested log levels for each actual log level the first column syntactic context log message combined trace debug info warn error trace debug info warn error trace debug info warn error trace .
.
.
.
.
.
.
.
.
.
.
.
debug .
.
.
.
.
.
.
.
.
.
.
.
info .
.
.
.
.
.
.
.
.
.
.
.
warn .
.
.
.
.
.
.
.
.
.
.
.
error .
.
.
.
.
.
.
.
.
.
.
.
note for each feature and each actual log level the highest percentage of incorrectly suggested log level is marked in bold .
or and oen .
for the baselines due to the limitation of space we only discuss the results of the models trained using thecomb feature which lead to the best results among the three features.
for the the results of two baselines the average accuracy are .
for or and .
for oen the average auc are .
for or and .
for oen and the average aod are .
and .
respectively.
for every system our models using syn orcomb features always outperform the two baselines in the three evaluation metrics as shown in table iv .
the results demonstrate the higher capability of our neural networks with ordinally encoded log levels than the ordinal regression and the standard one hot encoding in suggesting log levels.
our approach outperforms the two baseline approaches in suggesting log levels.
in particular our approach achieves the best performance when both the syntactic and log message features are considered.
rq2 what is the perf ormance of our appr oach on different log le vels?
motiv ation.
in rq1 we find that our approach can effecti vely suggest the log le velof a logging statement and that the models trained using the syntactic message and combined information sho w dif ferent performa nce.
ho wever for the logging statements with dif ferent log levels choosing an inappropriate log levelmay ha vedifferent costs.
f or example choosing the error level for aninfo message may be w orse i.e.
cause user confusion than choosing the infolevel for adebug message.
besides different stakeholders may be more interested in certain log levels.
f or example operators may be most interested in thewarn anderror levels which need their immediate actions while developers debugging activities may be most interested in the debug level.
therefore in this rq we further in vestig ate the performance of our approach in providing suggestion for each log level.
appr oach.
wefirst analyze the o verall performance of our models for each log level.
w e group the logging statements in our test datasets by their actual log le vel.then for each group of actual log le vel we measure the accurac y of our approach for suggesting the log le vels.
wethen in vestig ate how our models mis classify each log level by computing the distribution of the incorrectly suggested log le vels.
in this rq we train the models and analyze the results of the three features i.e.
syn msg orcomb respecti vely.
results and discussions.
the syntactic and combined features show more consistent performance than the log message feature among suggesttrace debug info warn error01020304050607080accuracy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
syntactic message combinedfig.
.
the accurac y of our approach on each log le vel ing differ ent log levels.foreach log level and each feature we present the results by sho wing the average accurac y of the models trained us ing dif ferent system s. figure sho ws the accurac y of the trained models using syntactic conte xt feature red bar log message feature blue bar and combined features purple bar for each log level.
ov erall the syntactic context and combined features ha vea similar trend on the results for different levels while log message features ha vea notable dif ference.
the log message feature has a relati vely high accurac y on suggesting the warn and error levels b ut has a very lowaccuracy on other levels ranges from .
to .
.
the potential reason might be that warn anderror level might contain some specific w ords that can be used to distinguish them from other levels.
as we found in section ii .
and .
of the log messages atwarn anderror level describe ne gativeexecution beha viors.
ho wever syntactic and combined feature also achi eve relati vely good results on these two levels range from .
to .
forwarn level and from .
to .
forerror level .
both the syntactic and combined features also ha vereasonable results on suggesting debug andinfo levels range from .
to .
accuracy .
most of the incorr ectly suggested log le vels provided by our approach are close to their actual log le vels.
table v presents the distrib ution of incorrectly suggested log levels for each actual log le vel the first column mark ed in bold .
all the numbers are the percentage of an incorrectly suggested log le velover all the incorrect suggestions for each actual log le vel.
overall there is only a small portion of logging s tatements that areincorrectly suggested astrace level range from .
to .
across all the three features .
in comparison most of the incorrect suggestions onerror logging statements are suggested as warn level o ver70 for all the three features .
reversely many warn logging statements are incorrectly suggested aserror level which isalso the most common incorrectly suggested log le vel .
wefind that for each feature and each actual log le vel the most common incorrect suggestions are one of their neighbouring log levels i.e.
the closest log levels .
in particular some log levels e.g.
warn and error levels might be hard to distinguis h. future studies could conduct in depth in vestig ations on more characteristics of different log le vels and help provide a more accurate suggestion correspondingly.
1468table vi the results of comparing enlarging training data rq3 a on syntactic s enlarge.
and combined feature c enlarge.
and cross project prediction rq3 b on syntactic s cross.
and combined feature c cross.
with the within project prediction in rq1 accuracy auc aod systems s enlar ge c enlarge s cross c cross s enlar ge c enlarge s cross c cross s enlar ge c enlarge s cross c cross cassandra .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
elasticsearch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
flink .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hbase .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jmeter .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kafka .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
karaf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wicket .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
zookeeper .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
note the number after each data in the columns of b enlarge andc enlarge indicates the improve or decrease compared with within system prediction in rq1.
the percentage in the columns of b cross andc cross represents the ratio against the results of within system prediction in rq1.
the syntactic context and combined features show more consistent capability in making suggestion among different log levels while the log message feature may only provide helpful suggestion on specific levels e.g.
warn anderror .
many of the incorrectly suggested log levels are close to their actual log levels.
future work could investigate opportunities that leverage the characteristics of different log levels to distinguish log levels that are close in order.
rq3 can our approach benefit fr om transfer learning?
motiv ation.
the success of deep neural netw orks often requires a large dataset in order to provide sufficient information for training .
howe ver aspresented in section ii the amount of logging statements in the studied systems ranges from .4k to .5k i.e.
small datasets compared to other areas such as computer vision where these deep neural networks are extensi vely leveraged .
moreov er different from mature systems with a long period of development and maintenance history new software systems may not ha veenough e xisting logging statements to train a deep neural netw ork.
t ransfer learning techniques areoften used to address the challenge of limited dataset .
in particular one may use data from other projects to complement the e xisting dataset to train a better model.
in this rq we in vestig ate whether using transfer learning among dif ferent studied systems can benefit our approach.
in particular we study tw o sub rqs rq3 a can we improv e the performance of our approach by including more training data from other studied systems?
rq3 b how accurate is our approach in cross system suggestions?
appr oach.
wechoose to study the use of transfer learning with syn andcomb features of our approach since both outperform the baselines as discussed in rq1.
below we describe the approach of each sub rq.
rq3 a weenlar gethedataset bycombing the data from all the studied systems.
foreach system we follo w stratified sampling to split the data into training data v alidation data and testing data .
wethen mer ge the training data from all studied systems and train a deep learning model while using the validation data set combined from e very studied system to validate the model during the training process.
finally we apply the model trained using the enlargeddataset separately on the testing data of each studied system.
rq3 b foreach tar get system we combi ne the data from the remaining eight systems together and apply stratified sampling to split of the data combined from the eight systems as training data and as validation data.
w e then use the complete data of the target system as the testing data and apply the model trained using the combined data from the other eight studied systems.
results and discussions.
rq3 a our appr oach can benefit fr om the enlarged training data from other systems.
table vi sho ws the results of enlar ging the training set using the syntactic conte xt features s enlarge and the combined features c enlarge .
the number after each data indicates the improv e ordecrease compared to within system suggestion in rq1.
overall for both of the tw o features the performance is impro vedin eight of the studied systems on all of the evaluation metrics.
specifically for syntactic context features i.e.
s enlarge in table vi the impro vement of accurac y ranges from .
in kafka to .
in cassandra.
the average a uc and aod also impro ve by .
and .
respectiv ely.forcombined features i.e.
cenlarge in table vi the impro vement of accurac y ranges from .
in flink and zook eeper to .
in hbase.
the a verage auc and a od also improv e by .
and .
respecti vely.
on the other hand the performance in elasticsearch is decreased after enlar ging the training data from other systems accurac y decreases by .
on s enlarge and by .
onc enlarge .
as sho wn in table i elasticsearch has considerably different log le veldistrib ution compared to other systems.
in particular there e xist considerably more trace level logging statements than other systems .
versus .
while much fe wer error level logging statements .
v ersus .
hence the data from other systems may not be able to complement the data from elasticsearch in the model training.
our finding shows that while enlarging the training data may improv e suggestion performance practitioners should carefully and tactically choose the data when enlar ging the training set.
rq3 b our appr oach achie ves encouraging r esults in cross system log le vel suggestions.
table vi sho ws the results of cross system suggestions using the syntactic conte xt features s cross and the combi ned features c cross .
the percentage follo wing each number represents the ratio of the 1469corresponding evaluation metric against the results of withinsystem prediction in rq1.
for example the accuracy of c cross in cassandra is .
.
compared with the original within system accuracy of comb in cassandra i.e.
.
the accuracy ratio of c cross against comb in cassandra is .
.
.
.
overall the cross system suggestions achieve .
accuracy on average for s cross compared to syn in rq1 and achieve .
accuracy on average for c cross compared to comb in rq1.
we also find that the results of cross system suggestions on combined features are still higher than the results of the two baselines in rq1.
in other words even with cross system suggestions our approach can still outperform the two baseline approaches that are trained and tested with data from the same system.
our approach can benefit from transfer learning.
by enlarging the training set the performance of our approach can be improved in eight out of nine studied systems.
our approach also has an encouraging performance for crosssystem log level suggestions which still outperforms the within system suggestions by the baseline approaches.
v. threatstovalidity construct v alidity.
tomitigate the fluctuation caused by different testing data set we follow prior studies to split the training validation and testing data and apply stratified random sampling to ensure each randomly sampled data set has the same distrib ution of log levels as the original data.
our approach presumes that the training data has high quali ty source code and follows good logging practice.
howe ver there is no golden rule for ho w to write logging statements which may affect the stability of logging statements .
t o mitig ate this threat we choose nine well maintained large scale systems across various domains with different sizes to conduct our study .
they are commonly used i n prior log related studies and are considered as following good logging practice .
internal validity .different hyper parameters used in the neural netw orks might affect the ef fectiv eness of the trained models.
w e follo w the advanced practices from prior studies to set the h yper parameters for our deep learning framew ork.
w e conduct manual studies toinvestigate whether log le velis implicitly or explicitly related to log message or the structural information of the logging statement.
t o avoid biases tw o of the authors examine the data independently.
formost of the cases the authors reach an agreement.
an y disagreement is discussed until a consensus is reached with a substantial le velagreement cohen s kappa of .
and .
for logging location and log message respecti vely .
involving third party logging e xperts to verify our manual study results may further mitigate this threat.
external validity .our studied systems are all implement ed in java the results and models may not be transferable to systems in other programming languages.
weconducted our studyon nine large scale open source systems only .
ho wever we selected the studied systems thatare across various domains different sizes and different amount of logging statements in order to impro vethe representati veness of our studied systems.
future studies should validate the generalizability of our findings and the transferability of our models in systems that are implemented in other programming languages.
vi.
r elatedwork studies on logging practices.
chen et al.
and y uan et al.
conducted quantitativ e studies on logging statements in large scale open source c c and java systems respectiv ely.they found that logs are essential for debugging and maintenance purposes.
fu et al.
studied the logging practices in microsoft soft ware systems.
the y investig ated what cate gories of code blocks e.g.
catch blocks are logged.
li et al.
summarized the benefits and costs of logging through a qualitativ e study .
zhi et al.
studied how logging configurations are used in practice with respect to logging management storage and formatting.
in this paper we focus on studying the characteristics of log levels specifically their explicit or implicit relationship with the syntactic context or message of a logging statement.
the findings of our study could complement prior studies in pro viding more comprehensiv e logging supports to developers.
impro ving logging practices.
given the importance of logging some studies try to help developers impro velogging practices.
y uan et al.
propos ed an approach that can automatically insert additional v ariab lesinto logging statements to enhance the error diagnostic information.
zhu et al.
proposed an automated tool for suggesting logging locations.
li et al.
proposed a deep learning frame work for suggesting logging locations at the code block le vel.
liu et al.
proposed a deep learning framew ork to suggest the variables that should be recorded in logging statements.
chen et al.
found that de veloper s commonly make some mistakes when writing logging statements e.g.
logging objects whose v alues may be null and concluded five cate gories of logging anti patterns from code changes.
li et al.
unco vered potential problems with logging statements that have the same te xt message and de veloped an automated tool to detect the probl ems.
hassani etal.
identified se venrootcauses of the log related issues from log related bug reports and found that inappropriate log messages and missing log statements are the most commo n issues.
different from prior studies we focus on suggesting log levels by using features extracted from the source code.
weconduct a manual study on the characteristics of log le velsand propose a deep learning based approach to pro vide automated suggestions.
vii.conclusion deciding proper log le vels for logging statements is a challenging task.
in this paper we tackle the challenges in tw o steps.
first we conduct a manual study on the characteristics of log levels.
w e find that the syntactic conte xt of logging statements and their messages as well as the ordinal nature 1470of log levels might be leveraged to help determine proper log levels.
we then propose a deep learning based approach to automatically suggest log levels for logging statements.
our approach ordinally encodes log levels and leverages the syntactic context information and the log message information of each logging statement to provide log level suggestions.
our approach outperforms the baseline approaches and are effective at suggesting log levels in both within system and crosssystem scenarios.
our results also highlight future research opportunities on improving logging decisions for example by leveraging the characteristics of different log levels to help distinguish similar log levels.
practitioners may also benefit from our findings to make better logging decisions.