modeling team dynamics for the characterization and prediction of delays in user stories elvan kula delft university of technology delft the netherlands e.kula tudelft.nlarie van deursen delft university of technology delft the netherlands arie.vandeursen tudelft.nlgeorgios gousios delft university of technology delft the netherlands g.gousios tudelft.nl abstract in agile software development proper team structures and effort estimates are crucial to ensure the on time delivery of software projects.
delivery performance can vary due tothe influence of changes in teams resulting in team dynamics thatremain largely unexplored.
in this paper we explore the effects ofvarious aspects of teamwork on delays in software deliveries.
weconducted a case study at ing and analyzed historical log datafrom user stories and teams to identify team factorscharacterizing delayed user stories.
based on these factors webuilt models to predict the likelihood and duration of delaysin user stories.
the evaluation results show that the use ofteam related features leads to a significant improvement in thepredictions of delay achieving on average precision recall and f measure.
moreover our resultsshow that team related features can help improve the predictionof delay likelihood while delay duration can be explained exclu sively using them.
finally training on recent user stories usinga sliding window setting improves the predictive performance our predictive models perform significantly better for teams thathave been stable.
overall our results indicate that planningin agile development settings can be significantly improved byincorporating team related information and incremental learningmethods into analysis predictive models.
i. i ntroduction the overall perceived success of a software project depends heavily on the timeliness of its delivery .
reducing delays is therefore a critical goal for software companies.over the past two decades software organizations have in creasingly embraced agile development methods to managesoftware projects .
agile gained popularity in the soft ware industry because in comparison to traditional waterfall like approaches it uses an iterative approach to softwaredevelopment aimed at reducing development time managingchanging priorities and inherently reducing risk .
however on time delivery remains a challenge in agile software devel opment.
prior work has found that around half of the agileprojects run into effort overruns of or more.
in agile settings software is incrementally developed through short iterations to enable a fast response to changingmarkets and customer demands.
each iteration requires thecompletion of a number of user stories which are a common way for agile teams to express user requirements.
agileteams are responsible for determining the next iteration sworkload together and then breaking these into user storiesthat can be implemented tested and shipped in one iteration.agile teams are characterized by self organization and intensecollaboration .
several studies have shownthe importance of teamwork for the success of agile projects.various aspects of teamwork such as team orientation teamcoordination and work division can affect software deliveryperformance .
moreover delivery performance canvary due to the influence of changes in teams resulting inteam dynamics that remain largely unexplored.
hence thereis a need to better understand the effects of teamwork and teamdynamics on delays which can benefit the effective applicationof agile methods in software development.
today s agile projects require different approaches to planning due to their iterative and team oriented nature .central to the planning is the ability to predict at any phaseof the project if a team can deliver the planned softwarefeatures on time.
agile teams would therefore benefit fromteam specific actionable information about the current exis tence of delay risks at the fine grained level of user stories allowing them to take measures to reduce the chance ofdelays.
recent approaches have leveraged machine learningtechniques for evaluating risk factors in software projects e.g.
estimating effort for issue reports e.g.
and predicting delays in bugs or issues e.g.
.these approaches focus on the technical aspects of softwaredeliveries and do not adequately take into account team related factors.
studies of software teams have developed theoretical concepts and detailed performancemodels that articulate relationships between various aspectsof teamwork quality and the extent to which a team is ableto meet time and cost objectives in software projects.
thesestudies point out that various aspects of teamwork need to beconsidered when planning software deliveries.
therefore thepredictive power of existing effort prediction models might beenhanced by incorporating such factors.
in this paper we explore the effects of various aspects of teamwork on delays in software deliveries.
project delays arecommon in the software industry which makes itimportant to study this phenomenon in more detail.
there is aneed to understand and predict especially during early projectphases which projects will be delayed.
this would allowteams to better manage and possibly prevent delays.
to do so we conduct a case study at ing a large dutch internationallyoperating bank with more than developers.
teams ating develop software using an agile development process.
36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee ing offers a great opportunity to study delays in agile projects as around one quarter of its user stories are delayed.
weanalyze historical log data from teams and userstories at ing to identify team factors characterizing delayeduser stories.
based on these factors we build models that caneffectively predict the likelihood and duration of delays in userstories.
our models learn from a team s past delivery perfor mance to predict delay risks in new user stories.
to determinewhether the use of team features has a positive impact onthe predictive performance we compare the results of modelslearned using different sets of features story features textfeatures and team features.
we also evaluate the models with asliding window setting to explore incremental learning and theimpact of team churn on the models performance.
the slidingwindow works as a forgetting mechanism the model learnsfrom a team s recent delivery performance in the window andforgets older irrelevant data to follow team changes over time.
our results show that the use of team features leads to a significant improvement in the predictions of delay achievingon average precision recall f measure and auc.
team features can help improvethe prediction of delay likelihood while delay duration canbe predicted exclusively using them.
moreover developerworkload team experience team stability and past effortestimates are the most important team features for predictingdelay.
finally training on recent user stories using a slidingwindow improves the predictive performance our predictivemodels perform significantly better for teams that have beenstable.
ii.
c ontext in agile software development a project has a number of iterations e.g.
sprints in scrum .
an iteration is usually a short weeks period in which the developmentteam designs implements tests and delivers a distinct productincrement.
each iteration requires the completion of a numberof user stories.
agile teams work with a product backlogto keep track of the status and priority of user stories .figure shows an example of a user story from the backlogmanagement tool used by teams at ing.
a user story has atitle textual description and a few standard fields to record itspriority type status and dependencies on other stories.
planning is done before an iteration starts and focuses on selecting and estimating the user stories to be delivered in thatiteration.
to plan the iteration the team discusses each storyand breaks it down into tasks to facilitate estimation .multiple developers can work on various sub tasks of a storybut only one developer is assigned to the story and responsiblefor its implementation.
agile teams heavily rely on experts subjective assessment to estimate the effort of completing auser story .
story points are a commonly used unit of measure that reflect the relative amount of effort complexityand risks involved in implementing the user story .
agileteams usually estimate story points together in a dedicatedplanning session e.g.
using planning poker .a.
usage scenarios at the end of an iteration a number of user stories are completed and there may also be a number of incomplete unre solved user stories delayed to future iterations.
our predictionmodels enable teams to identify these user stories beforethe start of an iteration.
there are two scenarios in whichpredictions are being made before and after an effort estimate has been made for a user story.
the availability of an estimatemight affect the accuracy and usefulness of our predictions.
itis likely that in the latter scenario our predictions get moreaccurate since we have information about the estimated sizeof a user story but the less useful it is since the team hasalready spent a considerable amount of time on estimating thestory .
in both scenarios our predictive models can be used as a decision support system to generate proactive feedback andmake informed decisions on the planning and feasibility ofa user story.
foreseeing delay risks allows teams to identifyproblematic user stories and take corrective actions such asstory splicing i.e.
splitting large stories into smaller ones orresolving inter story dependencies.
our models learn from thepast delivery performance of the specific team which they aredeployed to assist.
hence the predictions our models makeare team specific.
this helps teams improve their schedule es timates and gain an increased awareness of their own behaviorpatterns.
b. t eams and user stories at ing in recent years ing has reinvented its organisational structure moving from traditional functional departments to a completely agile organisational structure based on spotify s squads tribes and chapters model .
the main purposeof this model is to be able to control agile with hundredsof development teams.
all development teams at ing usescrum as agile methodology.
they work with sprints ofone to four weeks.
the teams consist of to members including a scrum master and product owner.
the productowner is responsible for prioritizing the product backlog.
inconsultation with the product owner the teams divide up the fig.
an example of a user story 992work to be done into user stories.
story points are assigned using planning poker in a structured group meeting calledsprint planning before the start of the sprint.
iii.
s tudy design in this paper we propose a team driven approach to determine early on the impact and probability of a delay riskoccurring in a user story.
to do so we extract risk factorsrepresenting technical and team related aspects of a user story.based on these factors we build models that can effectivelypredict the likelihood and duration of delays in user stories.as discussed in section ii predictions are made for two usagescenarios before and after an effort estimate has been madefor the user story.
for convenience in the remainder of thispaper we denote the scenarios as sc1 and sc2 respectively.
throughout our study the following research questions guide our work rq1.
benefits of team features does the use of team features have a positive impact on our predictive per formance?
rq1.
how effective is our approach whenteam features are used exclusively?
rq1.
to answer these questions we compare the performance of modelslearned using combinations of different sets of features story features text features and team features.
rq2.
feature importance which team features are most important for predicting delays in user stories?
for this question we train models using the extracted 24features and determine the relative importance of teamfeatures in terms of predictive power.
rq3.
benefits of sliding window does the use of a sliding window provide more accurate and robust estimates?as teams change over time and these changes might affecttheir delivery performance we want to analyze whether itis beneficial for our predictive model to learn from recentuser stories and forget older data.
to do so we comparethe performance of models learned using all features in asliding window setting versus expanding window setting.
rq4.
factor of change how does team churn affect story delays and our predictive performance?
changes in team composition due to either a member leavingor joining the team can cause teams to become lesspredictable at delivering software.
we employ the slidingwindow setting and determine for each user story thenumber of consecutive windows a team has been stablefor.
we perform a statistical analysis to assess the impactof the number of consecutive stable windows on storydelays and our models performance.
we can split our approach into four main steps data collection and pre processing we collect and preprocess backlog management data past user stories from571 development teams at ing.
risk factor extraction and analysis we extract risk factors representing technical and team related aspectsof user stories and then perform correlation analysis todetermine whether the factors affect delays in user stories.
t ext feature extraction we use roberta a state ofthe art language representation model to produce vectorrepresentations i.e.
embeddings for the textual descrip tions of user stories.
to adapt the model to our predictiontask we update it with additional training on our corpusof unlabeled user stories.
model building we use the selected risk factors and text embeddings to build models that predict delays in userstories.
model evaluation we evaluate our models using various sets of features in different experimental settings toanswer the research questions.
a. data collection and pre processing we extracted log data from servicenow a backlog management tool used by a majority of teams at ing .
the datasetconsists of user stories delivered by teams at ing betweenjanuary and january .
the user storieshave significant variety in terms of the products developed the size and application domain banking applications cloudsoftware software tools .
the dataset contains the followingfields for user stories identification number creation date sprint identification number planned start date actual start date planned delivery date actual delivery date story points and the textual title and description fields.
the planned start date coincides with the start date of the sprint that the story was originally assigned to.
we acknowledge that theplanned start date of a user story might change before thesprint is started.
therefore we consider only the planned startdate as scheduled on the day that the development phase of asprint is started.
for each user story the dataset contains theentire history of changes.
this enabled us to track the numberof sprints a user story was delayed for.
we acknowledge thata team might decide to temporarily move a story back tothe product backlog after a delay.
therefore we calculate thedelay duration based on the number of sprints a story hasactually been part of.
to eliminate noise and missing values we removed user stories with a status other than completed .
we also filteredout user stories with empty planned delivery date actual delivery date story points and description fields.
moreover we deleted user stories that have not been assigned to adeveloper.
we also removed stories that were added to a sprintduring the development phase because they are likely to beunstable and not accurately represent delay.
we found a fewuser stories that had been delayed for an unusually long periodof time e.g.
in some cases over sprints .
we removedsuch outliers that exceed two standard deviations from themean delay duration of all user stories.
the original datasetcontained user stories.
after removing outliers andpre processing the data the final dataset decreased to 200user stories from teams.
this dataset consists of delayed and non delayed user stories.
993table i the extracted risk factors representing the characteristics of a user story and development team.
correlation coefficients are based on spearman s correlation they measure the strength of the relationship between factors and therisk classes.
statistical significance with holm correction is indicated with p value .
and p value .
.
category factor name description typecorrelation coefficient spearman s interpretation story factorsdev type the development type .
new feature .
bug fix or .
improvement of a story categorical .
weak priority does a story have a major priority to the customer?
binary .
weak security whether a story is associated with a security critical system binary .
moderate out degree number of outgoing dependencies of a story on other stories continuous .
moderate sprint duration planned duration of the sprint that a story was originally assigned to continuous .
weak planned stories total number of user stories in the sprint that a story was originally assigned to continuous .
weak planned points total number of story points in the sprint that a story was originally assigned to continuous .
weak initial points number of story points initially estimated for a user story continuous .
moderate team factorsteam size number of team members continuous .
weak avg story size average number of story points that the team assigned to past stories continuous .
moderate team existence number of years the team has existed for continuous .
weak team stability ratio of team members that did not change in the last six months continuous .
moderate po stability did the product owner of the team stay the same in the last six months?
binary .
weak team capacity stories total number of user stories that have been completed by the team so far continuous .
weak team capacity points total number of story points that have been completed by the team so far continuous .
weak global distance the global distance metric measured across teams members continuous .
weak dev seniority the seniority rank of a developer at ing categorical .
weak dev age team number of years spent by a developer in the current team continuous .
weak dev age project number of years spent by a developer in the current project continuous .
weak dev age abc number of years spent by a developer at ing continuous .
moderate dev workload stories number of user stories assigned to a developer in the current sprint continuous .
moderate dev workload points number of story points assigned to a developer in the current sprint continuous .
moderate dev capacity stories total number of user stories that have been completed by a developer so far continuous .
moderate dev capacity points total number of story points that have been completed by a developer so far continuous .
moderate risk classes.
the delayed stories in our dataset consist of stories that were delayed for a single sprint stories that were delayed for two sprints stories that were delayed for three sprints and11 stories that were delayed for more than threesprints.
for our predictions we choose to use four risk classesthat reflect the degree of delay non delayed minor delay delay of one sprint medium delay delay of two sprints and major delay delay of three sprints or more .
since a small fraction of the user stories in our data were delayed for morethan three sprints we decided to merge this group with theuser stories that were delayed for three sprints.
b. risk factor extraction and analysis we extracted risk factors from the collected log data to explore which factors characterize delayed user stories.
table i provides an overview and correlation analysis of thesefactors.
the factors are divided in two groups story factors and team factors.
the story factors represent the inherent characteristics of user stories such as its size type and priority.the team related factors represent characteristics of individualteam members and the group as a whole.
we now explain therisk factors in detail.
the factor names are underlined.
story factors.
several story factors are extracted directly from the story s primitive attributes which include dev type and priority .
each story will be assigned a type and priority which indicate the nature and urgency of the task associatedwith implementing the story.
both factors have been shown toaffect the delivery of a story in related work .
we extractsecurity to determine whether a user story needs to go through a mandatory resource intensive security testing procedure ating that might lead to delay.
we extract the outgoing degree out degree of dependencies of a story this has been shown topredict delay in related work .
the remaining story factorsare used to extract the size of a story and the sprint.
team factors.
previous work e.g.
has found that member turnover can lead to tacit knowledge loss andthus may negatively affect team productivity.
therefore wecompute the stability of a team team stability and that of its product owner po stability .
we also measure team existence to quantify the familiarity and maturity of a team bothhave been shown to lead to better team interactions andproject performance in related work e.g.
.
previousstudies e.g.
have shown that the interactionsamong team members are less effective in distributed teams.to quantify the distance between team members we calculateglobal distance based on the global distance metric proposed in related work .
we calculate the metric for pair wisecombinations of team members and take the maximum value.
developers capabilities and experience can influence their contributions to projects e.g.
.
thus we com pute dev capacity stories and dev capacity points to quantify the software delivery experience of the developer that a userstory is assigned to.
similarly we use team capacity stories and team capacity points to quantify the team s overall experience with software deliveries.
moreover we extract thedevelopers seniority the intuition here is that senior developers might more often be assigned to complex user storiesthat have a higher delay risk.
ing employs the five stagedreyfus model to assess the expertise of developers basedon their experience in the software industry.
we also extractdev age team dev age project and dev age abc to measure how long team members have been working in their specificteams projects and at ing.
related work has identified an inappropriate division 994of work as an important barrier to achieving team effectiveness .
hence we calculate dev workload stories and dev workload points .
finally we extract team size and avg story size as larger projects are associated with greater risk in literature e.g.
.
c. t ext feature extraction the title and description of a user story can provide good features since they explain the nature and complexity of a story.
to extract features from text we combined the titleand description of a user story into a single text documentwhere the title is followed by the description.
our approachcomputes vector representations for these documents that arethen used as features to predict delays in user stories.
we trieddifferent methods for text feature extraction the traditionalbag of words with tf idf and okapi bm25 the neural network based doc2v ec and the state of theart transformer based language model robert a .
in case of tf idf and bm25 we pre processed the texts by lowercasing the words and removing punctuation and stop words.we compared and evaluated the methods on our predictiontask and corpus of user stories.
we found that robertaoutperforms the other methods on average by inprecision recall and f measure.
therefore we decided to use roberta as part of our experimental setupfor answering the research questions.
roberta is an optimization based on google s bert which is able to learn bidirectional word embeddings fromtexts.
to adapt the model to the domain specific vocabularyof user stories we updated it with additional training on ourcorpus of unlabeled user stories.
in this updating procedure we implemented the same masked language modeling strategyas in the pre training procedure of the original model with aset of newly designated hyperparameters training steps 60k batch size optimizer adam learning rate .
d. model building our objective is to predict the probability of a delay occurring i.e.
delay likelihood and a probability distribution over the aforementioned risk classes i.e.
delay duration in termsof the number of sprints overrun .
therefore our predictivemodels should be able to provide probability estimates.
weemploy binary classification for predicting the likelihood of delay.
we reduce the aforementioned risk classes into twobinary classes delayed and non delayed.
the delayed class covers the user stories that belong to the aforementioned minor delay medium delay and major delay classes.
for predicting the delay duration of delayed stories we employ multi class classification for the minor delay medium delay and major delay classes.
we compared and evaluated four different classifiers that are able to provide class probabilities and that have beenshown to be effective classifiers in risk prediction randomforests adaboost multi layer perceptron andnaive bayes .
a comparison on both prediction tasksshowed that random forests outperforms the other classifiers fig.
our pipeline for predicting delays in user stories on average by in precision in recall and in terms of f measure.
therefore we chose to employrandom forests rf as part of our experimental setup.
figure shows the design of our pipeline of predicting delays in user stories i extract numerical story features ii extract numerical team features iii produce document rep resentations of user stories using roberta iv concatenatefeatures and iv classification using random forests.
e. model evaluation evaluation setup.
we performed experiments on the user stories in our dataset.
we built team specific predictive models meaning that our models are trained andtested on a dataset containing the past user stories from onespecific team.
hence we built two models for each team inthe dataset one for predicting delay likelihood and one forpredicting delay duration.
to mimic a real prediction scenario where the delay of a given user story is estimated based on knowledge fromprevious stories we sorted the stories based on their startdate.
then for training and evaluation in rq1 and rq2 weused time based fold cross validation.
cross validation isa well known technique to prevent the classifier from over fitting.
the time based variant of cross validation ensures thatin the kth split the stories in the first k folds training set arecreated before the stories in the k th fold test set .
unlikestandard cross validation successive training sets are supersetsof previous ones also known as an expanding window .
for rq1 we evaluated the performance of the models learned using different sets of features.
we ran all experimentsfor the two types of usage scenarios sc1 excluding the initial points feature and sc2 including the initial points feature .
for rq2 rq4 we ran the experiments using allfeatures including initial points .
our predictive models are able to estimate class probabilities for the delay likelihood and delay duration of user stories.during the testing phase of our models we chose the classwith the highest probability as the predicted class.
a evaluation results obtained for predicting delay likelihood the story features baseline achieved .
.
precision .
.
recall .
.
f1 and .
.
auc for sc1 sc2.
b evaluation results obtained for predicting delay duration the story features baseline achieved .
.
precision .
.
recall .
.
f1 .
.
auc minor delay .
.
auc medium delay and .
.
auc major delay for sc1 sc2.
fig.
evaluation results for predicting the likelihood and duration of delay in usage scenario sc1 before effort estimation and sc2 after effort estimation using story features a combination of story and text features s t and all features story text and team features rq1.
.
the results are averaged across the teams in the dataset.
the predictions of the story features model are used as a baseline and visualized as a dashed line.
results are also given for when team features are used exclusively rq1.
.
sliding window setting.
for rq3 we evaluated our predictive models using two different experimental settings the expanding window and the sliding window.
in both settings the user stories are first sorted based on their start date and thendivided into multiple time based windows.
for each windowk iin the expanding window setting we use the stories from the previous windows k0...ki 1to train a model.
in the sliding window setting however we train the model only on the lastwindow k i .
the sliding window allows us to train the model on a team s recent delivery performance while the expandingwindow uses all observations available.
to analyze the impact of team churn on delays and our models performance rq4 we employed the sliding windowsetting and determined for each window whether a team hadbeen stable or not.
we marked a team as stable during awindow if no team members left or joined the team during thatwindow i.e.
if the team composition did not change duringthat window .
then for each story we determined the numberof consecutive windows a team had been stable for.
if theteam composition had changed in the previous window thenthis number was considered to be zero.
finally we performeda statistical comparison of delays and our models evaluationresults between stable and unstable teams.
performance measures.
we computed the widely used precision recall and f1 score to evaluate the performance ofour predictive models.
to account for class imbalance wecalculated the weighted averages of these measures i.e thescore of each class is weighted by the number of samplesfrom that class .
we also used area under the curve auc of receiver operator characteristics roc in classifyingthe outcome of a user story.
to compare the performance of predictive models we tested the statistical significance of their evaluation results using thewilcoxon signed rank test .
this is a non parametric testthat makes no assumptions about underlying data distributions.we employed the non parametric effect size measure thevargha and delaney s a 12statistic .
this measure is commonly used for evaluation in effort estimation .
iv .
r esults in this section we report the results in answering research questions rqs .
rq1 benefits of t eam features for this research question we compared the performance of models learned using story features a combination of story and text features and all features story text and teamfeatures .
we used the wilcoxon test and a 12effect size to investigate whether the improvements achieved by the additionof team features are statistically significant.
a b fig.
feature importance for predicting delays in user stories team features are highlighted in blue story features in gray.
rq1.
does the use of team features have a positive impact on our predictive performance?
figure 3a presents the evaluation results for predicting delay likelihood in usage scenarios sc1 and sc2 described in section ii a .
in bothscenarios the models learned using all features outperform themodels learned using a subset of features in terms of precision recall f1 and auc.
on average the all features modelsimprove the story and story text models by precision recall f1 score and auc .
statisticaltests show that the improvements achieved by the all featuresmodels are significant p .
and the effect sizes are large ranging between .
and .
.
this demonstrates that the addition of team features leads to a significant improvementin the predictions of delay likelihood.
figure 3b presents the evaluation results for predicting delay duration.
similarly in both scenarios the models learned usingall features outperform the models learned using a subset offeatures in terms of precision recall f1 and auc.
on average the all features models improve the story and story text models by precision recall f1 score and auc .
these improvements are significant p .
and the effect sizes are at least medium ranging between .
and0.
.
this indicates that the addition of team features leads toa significant improvement in the predictions of delay duration.
rq1.
how effective is our approach when team features are used exclusively?
as shown in figure 3a the models learned using team features only for predicting delaylikelihood perform better than the story models and slightly worse than the story text models.
the improvements achieved by the all features models over the team models are significant and the effect sizes are at least medium ranging between0.
and .
.
this indicates that the three feature sets have statistically significant contributions to the predictions of delaylikelihood.
figure 3b shows that the models learned using team features only for predicting delay duration achieve similar results asthe all features models.
the statistical tests show that the differences between both models in terms of precision recall f1 and auc are significant but the effect sizes are negligible.this indicates that we can effectively predict delay durationusing team features exclusively.
rq2 feature importance using the feature importance evaluation built in random forests we obtained the top most important features and their normalized weights from models learned using story and fig.
evaluation results for predicting delay likelihood across different window sizes in a sliding versus expanding window setting.
a sliding window b expanding window fig.
evaluation results obtained over time in the sliding window and expanding window settings using a month window team features including initial points .
the text features were not included as it is not possible to reduce the vectors produced by roberta to one single feature.
the models learnedusing story and team features achieve on average .
.76precision .
.
recall .
.
f1 and .
.
aucfor predicting delay likelihood duration.
figures 4a and 4bprovide a ranking of the features by order of importance forpredicting delay likelihood and delay duration.
we averagedthe importance values of the features across the teams in thedataset to produce an overall ranking.
features that have animportance value lower than .
are not shown.
figure 4a shows that features from table i contribute significantly to the predictions of delay likelihood.
dev work load stories team capacity stories planned stories avg story size and out degree are the top most important features.
their importance values range from to .
figure 4bshows that a partially overlapping set of features is ef fective in predicting delay duration.
even though the topmost important features in figure 4b are similar to those fordelay likelihood there are a few ranking differences.
overall the team features have greater explanatory power for delayduration than for delay likelihood.
this corresponds to ourresults for rq1.
dev capacity stories and team stability play a significantly larger role in the predictions of delay duration.
rq3 benefits of sliding window figure presents the evaluation results obtained for predicting the likelihood of delay using an expanding versus sliding window.
the results are averaged across windows andthe teams in the dataset.
as shown in figure the slidingwindow consistently outperforms the expanding window interms of precision recall f1 and auc across all windowsizes.
the wilcoxon test shows that the improvements aresignificant p .
and the effect sizes are between .
and .
small to medium .
comparing the results acrosswindow sizes we observe that both the expanding windowand the sliding window achieve the best performance for awindow of six months.
figures 6a and 6b visualize the evaluation results obtained over time in the sliding and expanding window settings using a6 month window.
the first window is not included as it is usedfor training only.
figure 6a shows lower variance over timein the prediction results for the sliding window.
both windowsettings start off with the same precision recall f1 and aucscores for the second window and then their performancedeclines during the third and fourth windows.
further analysisof the data shows that a majority of teams have greatervariance in their story delays during the initial windows.
thismight explain why the performance of both approaches declineat the start.
we observe that the performance of the expandingwindow drops drastically during the initial windows whilethe performance of the sliding window remains more stable.this suggests that the sliding window is better able to adaptto changes in teams delivery performance.
rq4 factor of change figure presents a percentage distribution of different levels of team stability based on the percentage of stories that were delayed.
we observe that user stories that are delivered bystable teams are less likely to be delayed.
of the storiesthat have been delivered after a team change i.e.
zero stablewindows are delayed of which are hindered by a majordelay.
the percentages of delayed user stories decrease forteams that have been stable for a longer period of time.
of the stories that have been delivered after one stable windoware delayed of which has a major delay.
only of the stories that have been delivered after more than onestable window are delayed.
fig.
delay percentage distribution across different stability levels 998fig.
evaluation results obtained for different levels of stability figure presents the evaluation results obtained for predicting the likelihood of delay for stories delivered by teams of varying stability.
we observe that our predictive modelsachieve better precision recall f1 and auc scores for teamsthat have been stable.
on average the models achieve higher precision higher recall higher f1 and higher auc for stories that are delivered after at least onestable window.
statistical tests show that these improvementsare significant p .
and the effect sizes are between .
and .
small to medium .
figure also shows a greatervariance in the results for stories delivered after a team change.
v. d iscussion a. recommendations for practitioners our study provides practitioners with an extensive list of risk factors.
by collecting and analyzing these factors softwarecompanies can identify delay risks and derive useful modelsto predict delays in deliveries.
our models are effective inpredicting the likelihood and duration of delays in user stories achieving on average precision recall f measure and auc.
our models enabledevelopment teams to foresee either before or after effortestimation if a user story is at risk of being moved to a futureiteration.
this allows teams to identify problematic user storiesand take corrective actions to reduce the chance of delays.
the evaluation results of our predictive models show that the use of team features leads to a significant improvement inthe predictions of delay.
moreover our results show that teamfeatures can help improve the prediction of delay likelihood while delay duration can be explained exclusively using them.for delay likelihood the feature sets are complementaryto each other.
this means that the probability of a delayoccurring depends on a combination of technical and team related aspects while the impact of the occurred delay i.e.
how fast it is resolved mainly depends on the characteristics ofthe team.
we therefore recommend organizations to encourageand facilitate teams to improve their work allocation effortestimates knowledge sharing and accountability in order toreduce the impact of delays.
companies must also have astable ecosystem in place to ensure that teams are able tooperate effectively.
b. implications for researchers feedback mechanisms on team behaviors.
our predictive models can be used by teams as awareness or feedback mechanisms on their behavior patterns during planning.
thesupport provided by our models can help teams to increase theawareness of their own behavioral habits e.g.
to reduce thebias towards over optimistic estimates .
this is likely to fosterproductive behavior change and improve schedule estimates.to better realize the benefits of such mechanisms there isa need for better tool support that can support agile teamsin tracking their team behavior and improving the manage ment of agile projects.
current agile project managementtools lack advanced analytical methods that are capable ofderiving actionable insights from project data for planning.
anextension of existing tools with actionable information aboutteam dynamics and the current existence of risks in a sprintwould be beneficial.
initial work in this direction has beencarried out by kortum et al.
.
team dynamics monitoring.
one of the key novelties in our approach is deriving new team features for a user storyby aggregating the features either at team or individual level.we derived the features by using a range of statistics overthe past stories delivered by a specific team or developer.our experimental results demonstrate the effectiveness of thisapproach.
previous research has shown that team related information is often difficult to capture or not availabledue to lacking information sources.
as a consequence thesefactors are often not monitored.
our results indicate the signifi cant benefits of incorporating such data into analysis predictivemodels for effort estimation and planning in agile projects.further research on team monitoring approaches is neededto address this gap and to gain a better understanding ofwhat information and metrics can be collected across softwareorganizations.
impact of social driven factors.
the set of team related factors identified in this paper are by no means comprehensiveto encompass all aspects of teamwork.
we were limited tothe repository data available at ing.
it is an interestingopportunity for future work to analyze the effects of social driven factors related to the collaborative nature of softwaredevelopment work.
previous studies have re ported on the impact of social driven factors such as trust team leadership team cohesion and communication.
thesefactors would be a good starting point for future work.
applicability of effort prediction models.
our evaluation results show that our predictive models perform significantly 999better for teams that have been stable for a longer period of time.
in our analyses we did not make a distinction betweensomeone leaving or joining the team nor did we take intoaccount the number of people leaving or joining.
it is aninteresting opportunity for future work to analyze the effects ofdifferent types of team changes on the models performance.this could also include external changes such as organiza tional restructuring or changes in senior management.
suchchanges have been identified as risk factors in literature .
future research should also explore the impact of otherteam characteristics e.g.
team experience and seniority onthe performance of effort prediction models.
vi.
t hreats to validity construct validity.
we consider data variables as constructs to meaningfully measure delay and risk factors.
this intro duces possible threats to construct validity .
we mitigatedthese threats by collecting real world data from user storiesand teams and all the relevant historical information available.the ground truth i.e.
the delay in terms of the number ofsprints overrun is based on the number of sprints a story hasbeen part of.
however it might happen that teams close theirstories too early or too late.
we cannot account for the impactof poor record keeping on our results.
even though all teams ating are encouraged to deliver on time there is a possibilitythat some teams treat their delivery deadlines less seriouslythan others.
these teams might add stories to sprints withoutthe commitment to deliver on time.
internal validity.
our dataset has the class imbalance problem.
this has implications to a classifier s ability to learnto identify delayed stories.
to overcome this issue we usedthe weighted variants of performance measures and employedauc which is insensitive to class imbalance.
we appliedstatistical tests to verify our assumptions and followedbest practices in evaluating and comparing predictive modelsfor effort estimation .
however we acknowledgethat more advanced techniques could also be used such asstatistical over sampling .
another threat to our studyis that the patterns in the training data may not reflect thesituation in the test data.
to mitigate this threat we used time based cross validation to mimic a real prediction scenario.
external validity.
as with any single case empirical study external threats are concerned with our ability to generalizeour results.
we have considered user stories from571 teams which differ significantly in size composition products developed and application domain.
although wecontrol for variations using a large number of projects andteams we acknowledge that our data may not be representativeof software projects in other organizations and open sourcesettings.
agile teams in other settings might have a differentteam structure and may work at a different pace or createstories differently.
further research is required to confirm ourfindings in different development settings.vii.
r elated work teamwork in agile development.
previous research has analyzed the nature of agile teams in software development their characteristics how they collaborate and the challengesthey face in geographically and culturally diverse environ ments .
a survey of success factors of agile projectsidentified team capability as a critical factor .
other stud ies have used performance models to investigate theimpact of teamwork quality on project success.
moe et al.
showed that problems with team orientation coordination work division and conflict between team and individual auton omy are important barriers for achieving team effectiveness.melo et al.
found that team structure work allocation andmember turnover are the most influential factors in achievingproductivity in agile projects.
our study complements priorwork by exploring the effects of various aspects of teamworkin the context of predicting software delays.
effort estimation and planning.
a great body of research has been published on the study of effort estimation meth ods .
estimation methods that rely on expert s subjectivejudgement are most commonly used in agile projects .project factors and personnel factors are the top mentionedeffort drivers in agile projects .
recent efforts have leveraged machine learning techniques to support effort estimation and planning in software projects.they have achieved promising results in estimating effortinvolved in resolving issues predicting the elapsedtime for bug fixing or resolving an issue e.g.
.
previous research has also been donein predicting the delay risk of resolving an issue in traditionaldevelopment.
some studies have been dedicated to effortestimation for agile development at the level of issues and iterations .
our work specifically focuseson predicting delays in user stories and complements priorwork by introducing team features and incremental learningto follow team changes over time.
viii.
c onclusion modern agile development settings require different approaches to planning due to their iterative and team orientednature.
in this paper we explored the effects of various aspectsof teamwork on delays in software deliveries.
we extracted aset of technical and team related risk factors that characterizedelayed user stories.
based on these factors we built modelsthat can effectively predict the likelihood and duration ofdelays in user stories.
the evaluation results demonstrate that the use of team features leads to a significant improvement in the predictions of delay likelihood while delayduration can be predicted exclusively using them.
developer workload team experience team stability and past effort estimates are the most important predictors fordelays in user stories.
training on recent user stories leads to more accurate and robust predictions of delay.
our predictive models perform significantly better and more consistently for teams that have been stable.
1000overall our results indicate that planning in agile software development can be significantly improved by incorporating team related information and incremental learning methodsinto analysis predictive models.
we identified several promis ing research directions related to team dynamics monitoring designing feedback and awareness mechanisms on team be haviors and the applicability of effort prediction models inagile projects.
progress in these areas is crucial in order tobetter realize the benefits of agile development methods.
r eferences t. chow and d. b. cao a survey study of critical success factors in agile software projects journal of systems and software vol.
no.
pp.
.
versionone 14th state of agile survey com ufh i 14th annual state of agile report accessed .
a. cockburn and j. highsmith agile software development the people factor computer vol.
no.
pp.
.
m. usman e. mendes f. weidt and r. britto effort estimation in agile software development a systematic literature review in proceedings of the 10th international conference on predictive models in softwareengineering.
acm pp.
.
h. sharp and h. robinson three c s of agile practice collaboration co ordination and communication in agile software development.
springer pp.
.
s. c. misra v .
kumar and u. kumar identifying some important success factors in adopting agile software development practices journal of systems and software vol.
no.
pp.
.
m. lindvall v .
basili b. boehm p. costa k. dangle f. shull r. tesoriero l. williams and m. zelkowitz empirical findings in agilemethods in conference on extreme programming and agile methods.
springer pp.
.
n. b. moe t. dings yr and t. dyb a overcoming barriers to selfmanagement in software teams ieee software vol.
no.
pp.
.
n. b. moe t. dings yr and t. dyb a a teamwork model for understanding an agile team a case study of a scrum project information and software t echnology vol.
no.
pp.
.
m. hoegl and h. g. gemuenden teamwork quality and the success of innovative projects a theoretical concept and empirical evidence organization science vol.
no.
pp.
.
m. cohn agile estimating and planning.
pearson education .
e. letier d. stefan and e. t. barr uncertainty risk and information value in software requirements and architecture in proceedings of the 36th international conference on software engineering pp.
.
h. r. joseph poster software development risk management using machine learning for generating risk prompts in ieee acm 37th ieee international conference on software engineering vol.
.
ieee pp.
.
m. choetkiertikul h. k. dam t. tran t. pham a. ghose and t. menzies a deep learning model for estimating story points ieee transactions on software engineering vol.
no.
pp.
.
s. porru a. murgia s. demeyer m. marchesi and r. tonelli estimating story points from issue reports in proceedings of the the 12th international conference on predictive models and data analytics insoftware engineering pp.
.
e. scott and d. pfahl using developers features to estimate story points in proceedings of the international conference on software and system process pp.
.
m. choetkiertikul h. k. dam t. tran and a. ghose predicting delays in software projects using networked classification t in 30th ieee acm international conference on automated software engineer ing ase .
ieee pp.
.
m. choetkiertikul h. k. dam t. tran and a. ghose predicting the delay of issues with due dates in software projects empirical software engineering vol.
no.
pp.
.
h. zhang l. gong and s. versteeg predicting bug fixing time an empirical study of commercial software projects in 35th international conference on software engineering icse .
ieee pp.
.
t. l. dickinson and r. m. mcintyre a conceptual framework for teamwork measurement t eam performance assessment and measurement pp.
.
y .
lindsj rn d. i. sj berg t. dings yr g. r. bergersen and t. dyb a teamwork quality and project success in software development asurvey of agile development teams journal of systems and software vol.
pp.
.
m. bloch s. blumberg and j. laartz delivering largescale it projects on time on budget and on value .
h. f. cervone understanding agile project management methods using scrum oclc systems services international digital library perspectives .
k. schwaber and m. beedle agile software development with scrum .
prentice hall upper saddle river vol.
.
servicenow.
servicenow workflows for the modern enterprise.
.
available m. cohn user stories applied f or agile software development .
addison wesley professional .
m. usman e. mendes and j. b orstler effort estimation in agile software development a survey on the state of the practice in proceedings of the 19th international conference on evaluation and assessment insoftware engineering pp.
.
j. grenning planning poker or how to avoid analysis paralysis while release planning hawthorn w oods renaissance software consulting vol.
pp.
.
h. kniberg and a. ivarsson scaling agile spotify online ucvof ucvox.
files.
wordpress.
com scaling agile spotify .
pdf .
y .
liu m. ott n. goyal j. du m. joshi d. chen o. levy m. lewis l. zettlemoyer and v .
stoyanov roberta a robustly optimized bertpretraining approach arxiv preprint arxiv .
.
j. noll and s. beecham measuring global distance a survey of distance factors and interventions in international conference on software process improvement and capability determination.
springer pp.
.
w. w. daniel et al.
applied nonparametric statistics .
s. holm a simple sequentially rejective multiple test procedure scandinavian journal of statistics pp.
.
m. choetkiertikul h. k. dam t. tran and a. ghose characterization and prediction of issue related risks in software projects in ieee acm 12th w orking conference on mining software repositories.ieee pp.
.
b. boehm and r. turner using risk to balance agile and plan driven methods computer vol.
no.
pp.
.
c. d. o. melo d. s. cruzes f. kon and r. conradi interpretative case studies on agile team productivity and management information and software t echnology vol.
no.
pp.
.
v .
lalsing s. kishnah and s. pudaruth people factors in agile software development and project management international journal of software engineering applications vol.
no.
p. .
r. popli and n. chauhan agile estimation using people and project related factors in international conference on computing for sustainable global development indiacom .
ieee pp.
.
s. dorairaj j. noble and p. malik understanding team dynamics in distributed agile software development in international conference on agile software development.
springer pp.
.
j. a. espinosa s. a. slaughter r. e. kraut and j. d. herbsleb familiarity complexity and team performance in geographically distributedsoftware development organization science vol.
no.
pp.
.
t. tan q. li b. boehm y .
yang m. he and r. moazeni productivity trends in incremental and iterative software development in 3rd international symposium on empirical software engineering andmeasurement.
ieee pp.
.
k. d. maxwell and p. forselius benchmarking software development productivity ieee software vol.
no.
pp.
.
s. e. dreyfus and h. l. dreyfus a five stage model of the mental activities involved in directed skill acquisition california univ berkeleyoperations research center tech.
rep. .
r. w. zmud management of large software development efforts mis quarterly pp.
.
l. wallace m. keil and a. rai understanding software project risk a cluster analysis information management vol.
no.
pp.
.
j. ramos et al.
using tf idf to determine word relevance in document queries in proceedings of the first instructional conference on machine learning vol.
no.
.
citeseer pp.
.
s. e. robertson s. walker s. jones m. m. hancock beaulieu m. gatford et al.
okapi at trec nist special publication sp vol.
p. .
q. le and t. mikolov distributed representations of sentences and documents in international conference on machine learning.
pmlr pp.
.
j. devlin m. w. chang k. lee and k. toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint arxiv .
.
l. breiman random forests machine learning vol.
no.
pp.
.
y .
freund and r. e. schapire a decision theoretic generalization of on line learning and an application to boosting journal of computer and system sciences vol.
no.
pp.
.
d. e. rumelhart g. e. hinton and r. j. williams learning internal representations by error propagation california univ san diego la jolla inst for cognitive science tech.
rep. .
p. domingos and m. pazzani on the optimality of the simple bayesian classifier under zero one loss machine learning vol.
no.
pp.
.
j. huang and c. x. ling using auc and accuracy in evaluating learning algorithms ieee transactions on knowledge and data engineering vol.
no.
pp.
.
a. arcuri and l. briand a hitchhiker s guide to statistical tests for assessing randomized algorithms in software engineering software t esting v erification and reliability vol.
no.
pp.
.
f. sarro a. petrozziello and m. harman multi objective software effort estimation in ieee acm 38th international conference on software engineering icse .
ieee pp.
.
r. genuer j. m. poggi and c. tuleau malot variable selection using random forests pattern recognition letters vol.
no.
pp.
.
f. kortum j. kl under and k. schneider behavior driven dynamics in agile development the effect of fast feedback on teams in ieee acm international conference on software and system processes icssp .
ieee pp.
.
v .
r. basili and r. w. reiter jr an investigation of human factors in software development.
ieee computer vol.
no.
pp.
.
f. kortum j. kl under and k. schneider don t underestimate the human factors!
exploring team communication effects in international conference on product focused software process improvement.
springer pp.
.
r. s. huckman b. r. staats and d. m. upton team familiarity role experience and performance evidence from indian software services management science vol.
no.
pp.
.
r. schmidt k. lyytinen m. keil and p. cule identifying software project risks an international delphi study journal of management information systems vol.
no.
pp.
.
k. ewusi mensah software development failures .
mit press .
s. l. jarvenpaa and b. ives executive involvement and participationin the management of information technology mis quarterly pp.
.
p. ralph and e. tempero construct validity in software engineering research and software metrics in proceedings of the 22nd international conference on evaluation and assessment in software engineering2018 pp.
.
a. arcuri and l. briand a practical guide for using statistical tests to assess randomized algorithms in software engineering in 33rd international conference on software engineering icse .
ieee pp.
.
e. kocaguneli t. menzies and j. w. keung on the value of ensemble effort estimation ieee transactions on software engineering vol.
no.
pp.
.
t. menzies z. chen j. hihn and k. lum selecting best practices for effort estimation ieee transactions on software engineering vol.
no.
pp.
.
n. v .
chawla k. w. bowyer l. o. hall and w. p. kegelmeyer smote synthetic minority over sampling technique journal of artificial intelligence research vol.
pp.
.
t. dyb a and t. dings yr empirical studies of agile software development a systematic review information and software technology vol.
no.
pp.
.
t. dings yr s. nerur v .
balijepally and n. b. moe a decade of agile methodologies towards explaining agile software development .
m. j rgensen a review of studies on expert estimation of software development effort journal of systems and software vol.
no.
pp.
.
l. d. panjer predicting eclipse bug lifetimes in f ourth international w orkshop on mining software repositories msr icse w orkshops2007 .
ieee pp.
.
p. bhattacharya and i. neamtiu bug fix time prediction models can we do better?
in proceedings of the 8th w orking conference on mining software repositories pp.
.
e. giger m. pinzger and h. gall predicting the fix time of bugs inproceedings of the 2nd international w orkshop on recommendation systems for software engineering pp.
.
m. choetkiertikul h. k. dam t. tran a. ghose and j. grundy predicting delivery capability in iterative software development ieee transactions on software engineering vol.
no.
pp.
.
p. abrahamsson r. moser w. pedrycz a. sillitti and g. succi effort prediction in iterative software development processes incrementalversus global prediction models in first international symposium on empirical software engineering and measurement esem .ieee pp.
.