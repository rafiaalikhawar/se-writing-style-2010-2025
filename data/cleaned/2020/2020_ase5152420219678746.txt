graph based incident aggregation for large scale online service systems zhuangbin chen jinyang liu yuxin su hongyu zhang xuemin wen xiao ling yongqiang yang michael r. lyu the chinese university of hong kong hong kong china zbchen jyliu yxsu lyu cse.cuhk.edu.hk the university of newcastle nsw australia hongyu.zhang newcastle.edu.au huawei china wenxuemin lingxiao1 yangyongqiang huawei.com abstract as online service systems continue to grow in terms of complexity and volume how service incidents are managed will significantly impact company revenue and user trust.
due to thecascading effect cloud failures often come with an overwhelmingnumber of incidents from dependent services and devices.
topursue efficient incident management related incidents shouldbe quickly aggregated to narrow down the problem scope.
to thisend in this paper we propose grlia an incident aggregationframework based on graph representation learning over the cas cading graph of cloud failures.
a representation vector is learnedfor each unique type of incident in an unsupervised and unifiedmanner which is able to simultaneously encode the topologicaland temporal correlations among incidents.
thus it can be easilyemployed for online incident aggregation.
in particular to learnthe correlations more accurately we try to recover the completescope of failures cascading impact by leveraging fine grainedsystem monitoring data i.e.
key performance indicators kpis .the proposed framework is evaluated with real world incidentdata collected from a large scale online service system of huaweicloud.
the experimental results demonstrate that grlia iseffective and outperforms existing methods.
furthermore ourframework has been successfully deployed in industrial practice.
index t erms cloud computing online service systems incident management graph representation learning i. i ntroduction in recent years it enterprises started to deploy their applications as online services on cloud such as microsoft azure amazon web services and google cloud platform.
thesecloud computing platforms have benefited many enterprisesby taking over the maintenance of it and infrastructure andallowing them to improve their core business competence.however for large scale online service systems failures areinevitable which may lead to performance degradation orservice unavailability.
whether or not the service failures areproperly managed will have a great impact on the company srevenue and users trust.
for example an hour episode ofdowntime in amazon led to a loss of million dollars .
when a failure happens system monitors will render a large number of incidents to capture different failure symptoms which can help engineers quickly obtain a big picture ofthe failure and pinpoint the root cause.
for example specialinstance cannot be migrated is a critical network failure invirtual private cloud vpc service and the incident tunnel corresponding author.bearing network pack loss is a signal for this network failure which is caused by the breakdown of a physical network cardon the tunnel path.
due to the large scale and complexity ofonline service systems the number of incidents is overwhelm ing the existing incident management systems .when a service failure occurs aggregating related incidentscan greatly reduce the number of incidents that need to beinvestigated.
for example linking incidents that are caused bya hardware issue can provide engineers with a clear pictureof the failure e.g.
the type of the hardware error or eventhe specific malfunctioning components.
without automatedincident aggregation engineers may need to go through eachincident to discover the existence of such a problem and collectall related incidents to understand it.
moreover incident aggre gation can also facilitate failure diagnosis.
in cloud systems some trivial incidents are being generated continuously andmultiple independent failures can happen at the same time.identifying correlated incidents can therefore accelerate theprocess of root cause localization.
to aggregate related incidents one straightforward way is to measure the text similarity between two incident reports .
for example incidents that share similar titles are likelyto be related.
besides textual similarity system topology e.g.
service dependency network ip routing is also an importantfeature to resort to.
due to the dependencies among onlineservices failures often have a cascading effect on other inter dependent services.
a service dependency graph can help trackrelated incidents caused by such an effect.
however as cloudsystems often possess certain ability of fault tolerance someservices may not report incidents impeding the tracking offailures impact to be explained in section ii .
this issueis ubiquitous in production systems which has not yet beenproperly addressed in existing work.
moreover the patterns ofincidents are collectively influenced by different factors suchas their topological and temporal locality.
existing work combine them by a simple weighted sum which may notbe able to reveal the latent correlations among incidents.
in this work we propose grlia stands for graph representation learning based incident aggregation which is anincident aggregation framework to assist engineers in failureunderstanding and diagnosis.
different from the existing workof alert storm handling and linked incident identifica4302021 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee tion we do not rely on incidents textual similarity.
moreover we learn incidents topological and temporal correlationsin a unified manner instead of by a weighted combination .traditional applications of graph representation learning oftenlearn the semantics of a fixed graph.
unlike them we proposeto learn a feature representation for each unique type ofincident which can appear in multiple places of the graph.the representation encodes the historical co occurrence ofincidents and their topological structure.
thus they can benaturally used for incident aggregation in online scenarios.
totrack the impact graph of a failure i.e.
the incidents triggeredby the failure we exploit more fine grained system signals i.e.
kpis as a piece of auxiliary information to discover thescope of its cascading effect.
kpis profile the impact of failuresin a more sophisticated way.
therefore if two services exhibitsimilar abnormal behaviors characterized by incidents andkpis they should be suffering from the same problems even ifno incidents have been reported.
finally we apply communitydetection algorithms to find the scope of different failures.
to sum up this work makes the following major contributions we propose to identify service failures impact graph which consists of the incidents that originate from thesame failures.
such an impact graph helps us obtain acomplete picture of failures cascading effect.
to this end we combine incidents with kpis to measure the behav ioral similarity between services.
community detectionalgorithms are then applied to determine the failure impact graph of different failures automatically.
we propose grlia an incident aggregation frameworkbased on graph representation learning.
the embeddingvector for each unique type of incident is learned inan unsupervised and unified fashion which encodes itsinteractions with other incidents in temporal and topo logical dimensions.
online incident aggregation can thenbe naturally performed by calculating their distance.
theimplementation of grlia is available on github .
we conduct experiments with real world incidents col lected from huawei cloud which is a large scale cloudservice provider.
the results demonstrate the effective ness of the proposed framework.
furthermore our frame work has been successfully incorporated into the incidentmanagement system of huawei cloud.
feedback fromon site engineers confirms its practical usefulness.
the remainder of this paper is organized as follows.
section ii introduces the background and problem statement ofthis paper.
section iii describes the proposed framework.section iv shows the experiments and experimental results.section v presents our success story and lessons learnedfrom practice.
section vi discusses the related work.
finally section vii concludes this work.
ii.
b ackground and problem statement a. topology of large scale online service systems cloud vendors provide a variety of online services to customers worldwide.
in general there are three main models fig.
.
an illustration of service failures cascading effect.
the irregular circle in the third subfigure shows the failure impact graph.
fig.
.
an example of incomplete failure impact graph of cloud based services namely software as a service saas platform as a service paas and infrastructure as a service iaas .
online service systems often possess a hierarchicaltopology i.e.
the stack of application platform and infrastruc ture layers.
each service embodies the integration of code anddata required to execute a complete and discrete functionality.for example in the application layer the services provided tocustomers can be a user application a microservice or even afunction in the platform layer the services can be a containeror a database in the infrastructure layer the services can bea virtual machine or storage.
different services communicatethrough virtual networks using protocols such as hypertexttransfer protocol http and remote procedure call rpc .such communications among services constitute the complextopology of large scale online service systems.
b. cascading effect of service failures with such a topology a failure occurring to one service tends to have a cascading effect across the entire system.
representative service failures include slow response requesttimeout service unavailability etc.
which could be caused bycapacity issues configuration errors software bugs hardwarefaults etc.
to quickly understand failure symptoms a largenumber of monitors are configured to monitor the states ofdifferent services in a cloud system .
a monitor will renderan incident when certain predefined conditions e.g.
cpuutilization rate exceeds are met.
typical configurationsof monitors include setting thresholds for specific metrics e.g.
rpc latency error counter checking service deviceavailability or status etc.
when a failure happens the monitorsoften render a large number of incidents.
these incidents aretriggered by the common root cause and describe the failurefrom different aspects.
thus they can be aggregated to helpengineers understand and diagnose the failure.
in this paper we model the set of incidents triggered by a failure as its impact graph or failure impact graph as illus431table i examples of incident aggregation 1r qflghqw 7lwoh 7lph 3rg 6hyhulw qflghqw jurxs 9luwxdo pdfklqh lv lq deqrupdo vwdwh srg rz 9luwxdo qhwzrun lqwhuidfh uhfhlyh orvw udwlr ryhu srg ljk 7udiilf exuvw vhhq lq 1jlq qrgh srg rz 7udiilf exuvw vhhq lq lqx 9luwxdo 6huyhu qrgh srg 0hglxp 2shq 6kruwhvw 3dwk luvw surwrfro vwdwh fkdqjh srg 0hglxp qflghqw jurxs fhvvlyh ghod ri vwrudjh glvn srg 0hglxp rpsrqhqw idloxuh srg ljk dug glvn idloxuh srg 0hglxp dwdedvh dffrxqw orjlq huuru srg 0hglxp 0rqlwru ghwhfwhg fxvwrphu lpsdfwlqj lqflghqwv iru 6wrudjh lq srg 0hglxp trated by fig.
.
specifically service aencounters a failure and the impact propagates to other services along the system topology.
the circled area indicates the impact graph of thefailure where irrelevant incidents in blue in service dandg are excluded.
in general the system topology can have manydifferent forms such as the dependencies of services theconfigured ip routing of a cloud network etc.
intuitively it might seem that the impact graph can be easily constructedby tracing incidents along the system topology.
however ourindustrial practices reveal that they are usually incomplete.
anexample is given in fig.
where service boccasionally fails to report any incident during the failure.
existing approachesmay perceive it as two separate failures which is undesirable.we have summarized the following two main reasons for themissing incidents system monitors that report incidents are configured withrules predefined by engineers.
due to the diversity ofcloud services and user behaviors the impact of a failuremay not meet the rules of some monitors.
for example if an application triggers an incident when its cpu usagerate exceeds then any value below the threshold willbe unqualified.
as a consequence the monitors will notreport any incident and thus the tracking of the failure simpact is blocked.
to ensure the continuity of online services cloud systemsare designed to have a certain fault tolerance capability.
inthis case service systems can bear some abnormal condi tions and thus no incidents will be reported.
therefore the impact of a failure may not manifest itself completelyover the system topology.
recent studies on cloud incident management have demonstrated the incompleteness and imperfection of monitordesign and distribution in online service systems.
thus alongthe service dependency chain some services in the middlemay remain silent i.e.
report no incident which impedesthe tracking of failure s cascading effect.
therefore althoughonline service systems generate many incidents they are oftenscattered.c.
problem statement this work aims to assist engineers in failure understanding and diagnosis with online incident aggregation which is toaggregate incidents caused by the common failure.
whenservices encounter failures incidents that capture differentfailure symptoms constitute an essential source for engineersto conduct a diagnosis.
however it is time consuming andtedious for engineers to manually examine each incident forfailure investigation when faced with such an overwhelmingnumber of incidents.
online incident aggregation is to clusterrelevant incidents when they come in a streaming manner i.e.
continuously reported by the system .
examples are presentedin table i where items in blue and gray belong to two groupsof aggregated incidents.
particularly the first group shows avirtual network failure.
note that only the no.
and no.4incidents share some words in common while the others donot.
meanwhile the second group describes a hardware failure and more specifically a storage disk error.
engineers canbenefit from such incident aggregation as the problem scopeis narrowed down to each incident cluster.
however accurately aggregating incidents for online service systems is challenging.
we have identified three main reasons.
background noise.
although related incidents are indeed generated around the same time many other cloud componentsare also constantly rendering incidents.
these incidents aremostly trivial issues and therefore become background noise.incident aggregation based on temporal similarity would sufferfrom a high rate of false positives.
dissimilar textual description.
text e.g.
incident title and summary similarity is an essential metric for incidentcorrelation which has been widely used in existing work .
however in reality related incidents especially the criticalones do not necessarily have similar titles.
failing to correlatesuch critical incidents greatly hinders root cause diagnosis.
unclear failure impact graph .
to correlate incidents accurately we need to estimate the impact graph of servicefailures.
as discussed in section ii b this task is challenging.incidents alone are insufficient to completely reflect the impact fig.
.
the overall framework of grlia of failures on the entire system.
therefore we need to utilize more fine grained information of the failures.
iii.
m ethodology a. overview in cloud systems a large number of monitors are configured to continuously monitor the states of its services fromdifferent aspects.
many incidents rendered by the monitorstend to co occur due to their underlying dependencies.
forexample some failure symptoms often appear together andsome incidents may develop causal relationship.
our main ideais to capture the co occurrences among incidents by learningfrom historical failures.
in online scenarios such correlationscan be leveraged to distinguish correlated incidents that aregenerated in streams.
the overall framework of grlia is illustrated in fig.
which consists of four phases i.e.
service failure detection failure impact graph completion graph representation learning and online incident aggregation.
the first phase tries to identify the occurrence of service failures and retrievesdifferent types of monitoring data including incidents kpitime series and service system topology.
in the second phase we try to identify the incidents that are triggered by eachindividual failure detected above.
more often than not itis hard to precisely identify the impact scope of failures as discussed in section ii b which hinders the learningof incidents correlations.
therefore we utilize the trendsobserved in kpi curves to auto complete the failure impactgraphs.
after obtaining the set of incidents associated witheach failure in the third phase an embedding vector is learnedfor different types of incidents by leveraging existing graphrepresentation learning models .
such representationencodes not only the temporal locality of incidents but alsotheir topological relationship.
in the final phase the learnedincident representation will be employed for online incidentaggregation by considering their cosine similarity and topo logical distance.
in particular we do not explicitly considerthe dynamic change of a system topology because the changesoften happen to a small area of the topology e.g.
containercreation or kill.
grlia essentially learns the correlationsamong incidents which are also applicable to the changedportion of the topology.
nevertheless when the system topol ogy goes through a significant alteration our framework isefficient enough to support quick model retraining.
b. service failure detection due to the cascading effect when service failures occur a large number of incidents are often reported in a short period of time.
thus setting a fixed threshold for the averagenumber of reported incidents e.g.
incidents min could be a reasonable criterion to detect failures.
however such adesign suffers from a trade off between false positives andfalse negatives due to online service systems complex andever changing nature .
for example different services havedistinct sensitivity to the number of incidents and continuoussystem evolution feature upgrades could change the threshold.thus a self adaptive algorithm is more desirable.
for time series data anomalies often manifest themselves as having a large magnitude of upward downward changes.
ex treme value theory evt is a popular statistical tool toidentify data points with extreme deviations from the medianof a probability distribution.
it has been applied to predictingunusual events e.g.
severe floods and tornado outbreaks by finding the law of extreme values that usually reside atthe tail of a distribution.
moreover it requires no hand setthresholds and makes no assumptions on data distribution.
inthis work we follow to detect bursts in time series ofthe number of incidents per minute.
as a typical time seriesanomaly detection problem other approaches e.g.
in this field are also applicable.
the bursts are regarded as theoccurrence of service failures.
this algorithm can automati cally learn the normality of the data in a dynamic environmentand adapt the detection method accordingly.
fig.
phaseone presents an example of service failure detection whereall abnormal spikes are successfully found by the decisionboundary the orange dashed line .
for consecutive bins thatare marked as anomalies we regard them as one failurebecause failures may last for more than one minute.
thenext phase will distinguish multiple independent failures thathappen simultaneously.
particularly the detection algorithm isonly required to have a high recall and the precision is of lessimportance.
it is because the goal of the follow up two phasesis to find the correlations between incidents.
such correlationrules will not be violated even incidents are not appearingtogether during actual cloud failures.
433c.
failure impact graph identification in the first phase the number of incidents per minute is calculated and incident bursts are regarded as the occurrence of service failures.
for each failure the incidents collectedfrom the entire system are not necessarily related to it.
thisis because while some services are suffering from thefailure others may continuously report incidents could betrivial and unrelated issues and multiple service failurescould happen simultaneously.
therefore we need to identifythe set of incidents for each individual failure that is generateddue to the cascading effect.
to this end the concept of community detection is exploited.
community detection algorithms aim to group thevertices of a graph into distinct sets or communities suchthat there exist dense connections within a community andsparse connections between communities.
each communityrepresents a collection of incidents rendered by the commonservice failure in which the correlations among incidents canbe explored.
a comparative review of different communitydetection algorithms is available in .
in this work we em ploy the well known louvain algorithm which is based upon modularity maximization.
the modularity of a graphpartition measures the density of links inside communitiescompared to links between communities.
for weighted graphs the modularity can be calculated as follows m 2m summationdisplay i j wi j kikj 2m ci cj wherewijis the weight of the link between node iandj ki summationtext jwijsums the weights of the links associated with nodei ciis the community to which node iis assigned to m summationtext ijwij and the u v ifu vand otherwise.
to better understand the identification of failure impact graph using community detection an illustrating example isdepicted in fig.
phase two .
in this case except for nodesbandf other nodes all report incidents.
by conducting community detection we obtain two communities a b c and c e f g which are regarded as the complete impact graph of their respective failure.
the weight between nodes isprovided with their link.
we can see that intra community linksall have a relatively large weight.
such partition can achievethe best modularity score for this example.
particularly nodehis excluded from the second community due to the small weight of its connection to node f. to apply community detection the weight between two nodes should be defined.
inspired by we combine kpiswith incidents to calculate the behavioral similarity betweentwo nodes and use the similarity value as the weight.
specif ically the weight is composed of two parts i.e.
incidentsimilarity and kpi trend similarity.
incident similarity incident similarity is to compare the incidents reported by two nodes.
typically if two nodesencounter similar errors they will render similar types of in cidents.
jaccard index is employed to quantify such similarity fig.
.
cpu usage curve of four servers which is defined as the size of the intersection divided by thesize of the union of two incident sets jaccard i j inc i inc j inc i inc j whereinc i is the incidents reported by node i. in particular we allow duplicate types of incidents in each set by assigningthem a unique number.
this is because the distribution ofincident types also characterizes the failure symptoms.
kpi trend similarity as discussed in section ii some services may remain silent when failures happen hinderingthe tracking of related incidents.
to bridge this gap we resortto kpis which are more sophisticated monitoring signals.intuitively the kpi trend similarity measures the underlyingconsistency of cloud components abnormal behaviors whichcannot be captured by incidents alone.
an example is shownin fig.
which records the cpu utilization of four servers.clearly the curve of the first three servers exhibits a highlysimilar trend while such a trend cannot be observed in serverfour.
the implication is that the first three servers are likelyto be suffering from the same issue and thus should belongto the same community.
we adopt dynamic time warping dtw to measure the similarity between two temporalsequences with varying speeds.
we observe the issue oftemporal drift between two time series which is common asdifferent cloud components may not be affected by a failuresimultaneously during its propagation.
therefore dtw fitsour scenario.
the remaining problem is which kpis should be utilized for similarity evaluation.
normal kpis which record the system snormal status should be excluded as they provide trivial andnoisy information.
therefore evt introduced in phase oneis utilized again to detect anomalies for each kpi.
only theabnormal kpis shared by two connected cloud componentswill be compared.
particularly when there exists more thanone type of abnormal kpis we use the average similarity scorecalculated as follows dtw i j kk summationdisplay k 1dtw ti k tj k wherekis the number of kpis to compare for node iand j ti kis thekthkpi of node i anddtw u v measures the 434dtw similarity between two kpi time series uandv which is normalized for path length.
the weight wijbetween node iandjis computed by taking the weighted sum of the two types of similarities as follows wij jaccard i j dtw i j where the balance weight is a hyper parameter.
in our experiments if two nodes both report incidents we set it as .
otherwise it is set to be i.e.
only the kpi trendsimilarity is considered.
finally for each discovered community the incidents inside it form the complete impact graph of the service failure.note that in online scenarios we cannot directly adopt thetechniques introduced in this phase for incident aggregation.this is because they involve a comparison between differentkpis which are not complete until the failures fully mani fest themselves.
thus the comparison is often delayed andinefficient.
moreover they can be error prone without fullyconsidering the historical cases.
d. graph based incident representation learning after obtaining the impact graph for each service failure i.e.
the actual incidents triggered by it we can learn the correlations among incidents.
such correlations describe thesets of incidents that tend to appear together.
fp growthproposed by han et al.
is a standard algorithm to minesuch frequent item sets.
however our analysis reveals thefollowing drawbacks it possesses for our problem it is vulnerable to background noise.
in production en vironments some simple incidents are constantly beingreported e.g.
high cpu utilization rate .
these inci dents will appear in many transactions a collection ofitems that appear together for fp growth.
as a result unrelated incidents might be put into the same frequentitem set due to sharing such incidents.
these simpleincidents cannot be trivially removed as they providenecessary information about a system and a burst of suchincidents can also indicate serious problems.
it cannot handle incidents with a low frequency.
fp growth has a parameter called support which describes how frequently an item set is in the dataset.
incident setswith a low support value will be excluded to guaranteethe statistical significance of the results.
however moreoften than not such incident sets are more important as they report some critical failures that do not happenfrequently.
in online service systems different resources e.g.
microservices and devices are naturally structured in graphicalforms such as service dependency and network ip routing.therefore graph representation learning can be an idealsolution to deal with the above issues.
graph representationlearning is an essential and ubiquitous task with applicationsranging from drug design to friendship recommendation insocial networks.
it aims to find a representation for graphstructure that preserves the semantics of the graph.
a typicalgraph representation learning algorithm learns an embeddingvector for all nodes of a graph.
for example chen et al.
employed node2vec to learn a feature representation for cloud components.
different from them we propose to learna representation for each unique type of incident which canappear in multiple places of the graph.
in our framework weemploy deepwalk because of its simplicity and superior performance.
deepwalk belongs to the class of shallow em bedding approaches that learn the node embeddings based onrandom walk statistics.
the basic idea is to learn an embedding ifor node viin graph gsuch that emb i j definese i j summationtext vk ve i k pg t vj vi wherevis the set of nodes in the graph and pg t vj vi is the probability of visiting vjwithinthops of distance starting at vi.
the loss function to maximize such probability is l summationdisplay vi vj d log emb i j wheredis the training data generated by sampling random walks starting from each node.
readers are referred to theoriginal paper for more details.
for each failure impact graph incident sequences are generated through random walk starting from every node inside.
inreality each node usually generates more than one incidentwhen failures happen.
our tailored random walk strategytherefore contains two hierarchical steps.
in the first step anode is chosen by performing random walks on node level in the second step an incident will be randomly selectedfrom those reported by the chosen node.
duplicate types ofincidents in a node will be kept because frequency is alsoan important feature of incidents it impacts the probabilityof being selected .
following the original setting of weset the walk length as i.e.
each incident sequence willcontain samples.
finally the incident sequences will be fedinto a word2vec model for embedding vector learning.the word2vec model has two important hyper parameters the window size and the dimension of the embedding vector.we set the window size as ten by following and set thedimension as .
in particular by considering the topologicaldistance between incidents we can alleviate the problem ofbackground noise.
this is because as the distance increases the impact of noisy incidents gradually weakens while in fp growth all incidents play an equivalent role in a transaction.
e. online incident aggregation with the learned incident representation from the last phase we can conduct incident aggregation in production environments where the incidents come in a streaming manner.
eachgroup of aggregated incidents represents a specific type ofservice issue such as hardware issue network traffic issue network interface down etc.
the evt based method also playsa role in this phase by continuously monitoring the number ofincidents per minute.
if it alerts a failure the online incident 435aggregation will be triggered.
when two incidents say iandj appear consecutively grlia measures their similarity.
if the similarity score is greater than a predefined threshold they willbe grouped together immediately.
in particular the similarityscore consists of two parts i.e.
historical closeness hc and topological rescaling tr which are defined as follows hc i j i j bardbl i bardbl bardbl j bardbl tr i j max d i j t where iand jare the embedding vectors of incident iand j as described in section iii d respectively d i j is the topological distance between iandj which is the number of hops along their shortest path in the system topology and t is the threshold for considering the penalty of long distance.that is the topological rescaling becomes effective i.e.
only if their distance is larger than t. in our experiments t is set as four.
incorrect correlations will be learned if tis too large while important correlations will be missed if tis too small.
our experiments show similar results when tis in .
cosine similarity is adopted to calculate the historical closeness which is related to their co occurrences in the past.finally the similarity between iandjcan be obtained by taking the product of tr i j andhc i j sim i j tr i j hc i j max d i j t i j bardbl i bardbl bardbl j bardbl we set an aggregation threshold forsim i j to consider whether or not two incidents are correlated cor i j braceleftbigg i f s i m i j o t h e r w i s e in our experiments is empirically set as .
.
in particular the distance of an incident to a group of incidents is defined asthe largest value obtained through element wise comparison.
iv .
e xperiments in this section we evaluate our framework using realworld incidents collected from industry.
particularly we aimat answering the following research questions.
rq1 how effective is the service failure detection module of grlia?
rq2 how effective is grlia in incident aggregation?
rq3 can the failure impact graph help incident aggregation?
a. experiment setting dataset incident aggregation is a typical problem across different online service systems.
in this experiment we select a representative large scale system i.e.
the networking serviceof huawei cloud to evaluate the proposed framework.
besidesoffering traditional services such as virtual network vpngateway it also features intelligent ip networks and othernext generation network solutions.
in particular the servicesystem comprises a large and complex topological structure.in the layer of infrastructure platform and software ithas multiple instances of virtual machines containers andapplications respectively.
in each layer their dependenciesform a topology graph.
the cross layer topology is mainlyconstructed by their placement relationships i.e.
the mappingsbetween applications containers and virtual machines.
likeother cloud enterprises huawei cloud s resources are hostedin multiple regions and endpoints worldwide.
each regionis composed of several availability zones isolated locationswithin regions from which public online services originateand operate for service reliability assurance.
the incidentmanagement of the networking service is also conducted insuch a multi region way with each region having relativelyisolated issues.
in this paper we collect incidents generatedbetween may and november during which thenetworking service reported a large number of incidents.although we conduct the evaluation on a single online servicesystem we believe grlia can be easily applied to otheronline service systems and bring them benefits.
to evaluate the effectiveness of grlia experienced domain engineers manually labeled related incidents.
thanksto the well designed incident management system with user friendly interfaces the engineers can quickly perform thelabeling.
note that the manual labels are only required forevaluating the effectiveness of our framework which is unsu pervised.
to calculate the kpi trend similarity we adopt thefollowing kpis which are suggested by the engineers cpu utilization refers to the amount of processing resources used.
round trip delay records the amount of time it takes to send a data packet plus the time it takes to receive anacknowledgement of that data packet.
port in bound out bound traffic rate refers to the average amount of data coming in to going out of a port.
in bound packet error rate calculates the error rate of the packet that a network interface receives.
out bound packet loss rate calculates the loss rate of the packet that a network interface sends.
these kpis are representative that characterize the basic states of the networking service system.
in particular cpuutilization is monitored for different containers and virtualmachines while the remaining kpis are monitored for thevirtual interfaces of each network device.
each kpi is calcu lated or sampled every minute.
we collect two hours of datato measure the kpi trend similarity.
note that the set of kpiscan be tailored for different systems.
for example a databaseservice may also care about the number of failed databaseconnection attempts the number of sql queries etc.
in this paper we select the largest ten availability zones for experiments each of which contains a large system topology.six months of production incidents are collected from thenetworking service of huawei cloud.
the number of distinct 436table ii dataset statistics dwdvhw 7udlqlqj shulrg ydoxdwlrq prqwk qflghqwv dloxuhv dwdvhw 0d xo xj a n a n dwdvhw 0d xj 6hsw a n a n dwdvhw 0d 6hsw 2fw a n a n incident types is more than .
similar to we conduct three groups of experiments using incidents reportedin the first four months the first five months and all months respectively.
in all periods incident aggregation is appliedto the failures that happened in the last month based onthe incident representations learned from previous months.table ii summarizes the dataset.
for column incidents resp.
failures the first figure calculates the incidents resp.
fail ures captured during the training period while the secondfigure shows that of the evaluation month.
particularly somefailures are of small scale and can be quickly mitigated whileothers are cross region and become an expensive drain onthe company s revenue.
we can see each failure is associatedwith roughly incidents demonstrating a strong need forincident aggregation.
evaluation metrics for rq1 which is a binary classification problem we employ precision recall and f1 score for evaluation.
specifically precision measures the percentageof incident bursts that are successfully identified as servicefailures over all the incident bursts that are predicted asfailures precision tp tp fp.
recall calculates the portion of service failures that are successfully identified by grliaover all the actual service failures recall tp tp fn.
finally f1 score is the harmonic mean of precision and recall f1score precision recall precision recall.tpis the number of service failures that are correctly discovered by grlia fp is the number of trivial incident bursts i.e.
no failure is actuallyhappening that are wrongly predicted as service failures bygrlia fn is the number of service failures that grlia fails to discover.
for rq2 and rq3 we choose normalized mutual information nmi which is a widely used metric forevaluating the quality of clustering algorithms.
the value ofnmi ranges from to with indicating the worst result no mutual information and the best perfect correlation nmi c i c h h c where is the set of clusters c is the set of classes h is the entropy and i c calculates and mutual information between andc.
implementation our framework is implemented in python.
we parallelize our experiments by assigning availability zones to different processors.
the output of each processoris a list of incident sequences generated through random walk which we merge and feed to a word2vec model implementedwith gensim an open source library for topic modelingand natural language processing.
we run our experiments ona machine with intel r xeon r gold cpu .60ghz and 256gb of ram.
the results show that eachphase of our framework takes only a few seconds.
the lastphase can even produce results in a real time manner as itonly involves simple vector calculation.
thus our frameworkcan quickly respond in online scenarios.
this demonstratesthat grlia is of high efficiency.
b. comparative methods the following methods are selected for comparative evaluation of grlia.
fp growth .
fp growth is a widely used algorithm for association pattern mining.
it is utilized as an analytical process that finds a set of items that frequently co occur in datasets.
in our experiments each impact graphis regarded as a transaction for this algorithm.
given a setof impact graphs it searches incidents that often appeartogether regardless of their distance.
uhas .
this approach is proposed by zhao et al.
aiming at handling alert storms for online service systems.similar to incident bursts alert storms also serve as asignal for service failures.
particularly uhas employsdbscan for alert clustering based on their textual andtopological similarity.
the textual similarity between twoalerts is measured by jaccard distance.
the topologicalsimilarity considers two types of topologies i.e.
softwaretopology service and hardware topology server .
thetopological distance is computed by the shortest pathlength between two nodes.
finally a weighted combi nation of the two types of similarities yields the finalsimilarity score.
lidar .
lidar is a supervised method proposedby chen et al.
to identify linked incidents in large scale online service systems.
specifically lidar iscomposed of two modules i.e.
textual encoding moduleand component embedding module.
the first moduleproduces similar representations for the text descriptionof linked incidents which are labeled by engineers.
inthe evaluation stage the textual similarity between twoincidents is measured by the cosine distance of their rep resentations.
the second module learns a representationfor the system topology instead of incidents .
the finalsimilarity is calculated by taking a weighted sum of bothparts.
as lidar is supervised it would be unfair tocompare it with other unsupervised methods.
consideringthe success of word2vec model in identifyingsemantically similar words in an unsupervised manner we alter lidar to be unsupervised to fit our scenario byrepresenting the text of incidents with off the shelf wordvectors .
c. experimental results rq1 the effectiveness of grlia s service failure detection to answer this research question we compare grlia with the fixed thresholding method on three datasets andreport precision recall and f1 score.
thresholding remains aneffective way for anomaly detection in production systems andserves as a baseline in many existing work.
since both methodsrequire no parameter training we use them to detect failures 437table iii experimental results of service failure detection dwdvhw 0hwulf 7kuhvkroglqj 3uhflvlrq dwdvhw 5hfdoo 6fruh 3uhflvlrq dwdvhw 5hfdoo 6fruh 3uhflvlrq dwdvhw 5hfdoo 6fruh for both the training data and evaluation data.
particularly the threshold of the baseline method is incidents min which is recommended by field engineers.
moreover the ground truthis obtained directly from the historical failure tickets whichare stored in the incident management system.
the results are shown in table iii where grlia outperforms the fixed thresholding in all datasets and metrics.
inparticular grlia achieves an f1 score of more than .93in different datasets demonstrating its effectiveness in servicefailure detection.
indeed we observe that some failures maynot always incur a large number of incidents at the beginning.however if ignored they could become worse and end upyielding more severe impacts across multiple services.
fixedthresholding does not possess the merit of threshold adaptationbased on the context and thus produces many false positives.grlia outperforms it for being able to adjust the thresholdautomatically.
rq2 the effectiveness of grlia in incident aggregation we compare the performance of grlia against a series of baseline methods for incident aggregation.
table ivshows the nmi values of different experiments.
from dataset1 to grlia achieves an nmi score of .
.
and0.
respectively while the best results from the baselinemethods are .
.
and .
all attained by li dar.
lidar outperforms uhas by explicitly consideringthe entire system topology.
except for uhas all approachesachieve better performance with more training data available.this is because uhas directly works on alert storms whenfailures are detected.
without learning from the history itcannot handle complicated scenarios.
recall that both uhasand lidar rely on the textual similarity between incidents.however in our system related incidents do not necessarilypossess similar text descriptions.
for example there is a clearcorrelation between the incident traffic drops sharply invrouter and os network ping abnormal in vpc service which tends to be missed by them.
moreover monitors thatrender incidents are configured by multiple service teams which further damages the credit of textual similarity.
thisis particularly true for some critical incidents because theyare often tailored for special system errors which may not beshared across different services.
on the other hand althoughtable iv experimental results of incident aggregation 0hwkrg dwdvhw dwdvhw dwdvhw urzwk l grlia does not explicitly leverage incident s textual features our experiments show that it is capable of correlating incidentsthat share some common words e.g.
vpc service tomcatport does not exist and vpc service tomcat status is dead .this is because such a relationship is reflected in their temporaland topological locality which can be precisely captured byincidents representation vectors.
another observation is that fp growth does not fit the task of incident aggregation whose best nmi score is .
.as discussed in section iii d this method is not robustagainst background noise.
indeed in the system some trivialincidents e.g.
virtual machine is in abnormal state arecontinuously being reported which may connect incidentsfrom distinct groups.
furthermore many essential incidentsare excluded by this method due to low frequency whichis undesirable.
this problem can be effectively alleviatedby leveraging the topological relationship between incidentsas done by other approaches.
according to eq.
the im pact of background noise weakens with distance.
however in fp growth each incident co occurrence will be countedequally towards the final association rules.
uhas considersthe topological similarity by simply calculating the distance.lidar employs a more expressive machine learning model i.e.
node2vec an algorithmic framework for learning a continuous representation for a network s nodes.
however they both ignore the problem of incomplete failure impactgraph which is a common issue in online service systemsaccording to our study.
the necessity of completing the impactgraph will be demonstrated in rq3.
moreover different fromthe traditional applications of graph representation learning we learn a representation for each unique type of incident which compactly encodes its relationship with others.
rq3 the necessity of the failure impact graph for incident aggregation we demonstrate the importance of impact graphs by creating a variant of grlia without thephase of failure impact graph completion i.e.
phase two infig.
denoted as grlia prime.
we follow lidar to remove this feature which considers two incidents as related onlywhen they are directly connected in the system topology.the experimental results are presented in table v where wecan see a noticeable drop in the nmi score for all datasets.due to the high complexity and large scale of online servicesystems monitors are often configured in an ad hoc manner.these monitors may not be able to accommodate to the ever changing systems and environments.
thus some incidents arenot successfully captured by them.
system engineers may 438table v experimental results of incident aggregation using grlia w and w o failure impact graph completion 0hwkrg dwdvhw dwdvhw dwdvhw incorrectly perceive the service as healthy which is a typical situation of gray failures .
without completing the impactgraph of failures the true correlations among incidents cannotbe fully recovered.
d. threats to v alidity during our study we have identified the following major threats to the validity.
labeling noise.
our experiments are conducted based on six months of real world incidents collected from huawei cloud.
the evaluation requires engineers to inspect and labelthe incidents manually.
label noises false positives falsenegatives may be introduced during the manual labelingprocess.
however the engineers we invite are cloud systemprofessionals and have years of system troubleshooting expe rience.
moreover the labeling work can be done quickly andconfidently thanks to the incident management system whichhas user friendly interfaces.
therefore we believe the amountof noise is small if it exists .
selection of study subjects.
in our experiments we only collect incidents from one online service of huawei cloud i.e.
the networking service.
this is a large scale service thatsupports many upper layer services such as web application virtual machine.
sufficient data can be collected from thisservice system.
another benefit we can enjoy is that thetopology of the networking service system is readily availableand accurate.
although we use the networking service as thesubject in this paper our proposed framework is generalizable as this service is a typical representative online service.
thus we believe grlia can be applied to other services and cloudcomputing platforms and bring them benefits.
the second type of subject that could threaten the validity is the kpi.
in production systems there is a large amountof kpis available to gauge the similarity between two nodes.although we only select six representative kpis as presentedin section iv a1 they record the basic and critical states ofa service component.
thus we believe they are able to profilethe service system comprehensively.
implementation and parameter setting.
the implementation and parameter setting are two critical internal threatsto the validity.
to reduce the threat of implementation weemploy peer code review.
specifically the authors are invitedto carefully check others code for mistakes.
in terms ofparameter setting we conduct many groups of comparativeexperiments with different parameters.
we choose the param eters by following the original work or empirically based onthe best experimental results.
in particular we found grliais not very sensitive to the parameter setting.v.
d iscussion a. success story grlia has been successfully incorporated into the incident management system of huawei cloud.
based on thepositive feedback we have received on site engineers oses highly appreciated the novelty of our approach and benefitedfrom it during their daily system maintenance.
specifically oses confirmed the difficulty of the auto detection of ser vice failures in the existing monitoring system.
this is be cause simple detection techniques e.g.
fixed thresholding are widely adopted.
grlia introduces more intelligence andautomation by leveraging evt based incident burst detection.interestingly oses found problems for some monitors bycomparing their configurations with the aggregated incidents including wrong names missing information etc.
meanwhile during failure diagnosis incident aggregation assists osesin reducing their investigation scope.
before the deploymentof grlia they would have to examine a large number ofincidents to locate the failures.
to quantify the practical benefits conveyed to the networking service system we further collect failure tickets generatedduring november .
in total failures are recorded.we calculate the average failure handling time in novemberand compare it with that in august september and october.results show that the time reduction rate is .
.
and18.
respectively demonstrating the effectiveness of grliain accelerating the incident management of huawei cloud.
b. lessons learned optimizing monitor configurations.
today popular online services are serving tens of millions of customers.
during daily operations they can produce terabytes and even petabytes oftelemetry data such as kpis logs and incidents.
however themajority of these data does not contain much valuable infor mation for service failure analysis.
for example a significantportion of kpis only record plain system runtime states mostof the incidents are trivial and likely to mitigate automaticallywith time.
the configuration of system monitors should beoptimized to report more important yet fewer incidents.
inthe meantime monitor configurations show different stylesacross different service teams making the monitoring dataheterogeneous.
standards should be established for monitorconfigurations so that high quality incidents can be created tofacilitate the follow up system analysis e.g.
fault localization.
building data collection pipeline .
in online service systems it operations play a critical role in system mainte nance.
since it is data driven by nature modern cloud ser vice providers should build a complete and efficient pipelinefor monitoring data collection.
common data quality issuesinclude extremely imbalanced data small quantity of data poor signal to noise ratio etc.
in general we are facing thefollowing three challenges what data should be collected?
we need to identify what metrics and events that are mostrepresentative for cloud resource health.
not everything thatcan be measured needs to be monitored.
how to collect 439and label data?
labeling incidents e.g.
incident linkages culprit incidents requires oses to have a decent knowledge about the cloud systems.
since they often devote themselvesto emerging issue mitigation and resolution tools should bedeveloped to facilitate the labeling process such as labelrecommendation and friendly interfaces.
how to store and query data?
today s cloud monitoring data are challenging the conventional database systems.
to save space domain specificcompression techniques should be developed for example logcompression .
vi.
r elated work a. problem identification to provide high quality online services many researchers have conducted a series of investigations including problemidentification and incident diagnosis from runtime log dataand alerts .
for example to identify problemsfrom a large volume of log data lin et al.
proposedlogcluster to cluster log sequences and pick the center ofeach cluster.
rosenberg et al.
extended logcluster byincorporating dimension reduction techniques to solve thehigh dimension challenge of log sequence vectors.
inspired bylogcluster zhao et al.
clustered online service alertsto identify the representative alerts to engineers.
different fromthe clustering techniques jiang et al.
proposed an alertprioritization approach by ranking the importance of alertsbased on the kpis in alert data.
the top ranked alerts are morevaluable to identifying problems.
however this approach hasa limited scope of application because it is only practical tokpi alerts generated from manually defined threshold rules.to conduct problem identification more aggressively chen etal.
proposed an incident diagnosis framework to predictgeneral incidents by analyzing their relationships with differentalerting signals.
zhao et al.
considered a more practicalscenario where there are plenty of noisy alerts in online servicesystems.
they proposed ewarn to filter out the noisy alerts andgenerate interpretable results for incident prediction.
b. incident management in recent years cloud computing has gained unprecedented popularity and incidents are almost inevitable.
thus incident management becomes a hotspot topic in both academia andindustry.
massive amount of effort has been devoted to incidentdetection and incident triage .for example lim et al.
utilized hidden markov randomfield for performance issue clustering to identify representa tive issues.
chen et al.
proposed deepct a deep learning based approach that is able to accumulate knowledge fromincidents discussions and automate incident triage.
however due to high manual examination costs these methods cannothandle the overwhelming number of incidents.
many existingwork address this problem by reducing the duplicatedor correlated alerts.
for example zhao et al.
aimed torecommend the severe alerts to engineers.
lin et al.
proposed an alert correlation method to cluster semi structuredalert texts to gain insights from the clustering results.similar to our method zhao et al.
conducted alert reduction by calculating their textual and topological simi larity.
the centroid alert of each cluster is then selected asthe representative incident to engineers.
specifically they firstleveraged conventional methods to detect alert storms and theassociated anomalous alerts and then adopted dbscan to cluster alerts based on their textual and topological sim ilarity.
another similar work is lidar proposed by chenet al.
which links relevant incidents by incorporating therepresentation of cloud components.
their framework consistsof two modules a textual encoding module and a componentembedding module.
the first module learns a representationvector for incident s description in a supervised manner.
thetextual similarity between two incidents is measured by thecosine distance of their representation vectors.
similarly thesecond module learns a vector for system components.
thefinal similarity is calculated by leveraging two parts of infor mation.
however these methods employ a simple weightedsum to combine the information from different sources andstill hardly capture the relationship between incidents.
differ ently our method utilizes sophisticated graph representationlearning to obtain the semantic relationship of incidents fromdiverse sources including temporal locality topological struc ture and kpi metric data.
moreover many existing incidentmanagement methods rely on supervised machine learningtechniques to detect anomalies or conduct incident triage.more intelligent approaches with weak supervision or evenunsupervised frameworks are still largely unexplored.
vii.
c onclusion in this paper we propose grlia an incident aggregation framework based on graph representation learning.
therepresentation for different types of incidents is learned inan unsupervised and unified fashion which encodes the in teractions among incidents in both temporal and topologicaldimensions.
online incident aggregation can be efficientlyperformed by calculating their distance.
we have conductedexperiments with real world incidents collected from huaweicloud.
compared with fixed thresholding grlia achievesbetter performance for being able to adjust the thresholdautomatically.
in terms of online incident aggregation grliaalso outperforms existing methods by a noticeable margin confirming its effectiveness.
furthermore our framework hasbeen successfully incorporated into the incident managementsystem of huawei cloud.
feedback from on site engineersconfirms its practical usefulness.
we believe our proposedincident aggregation framework can assist engineers in failureunderstanding and diagnosis.
a cknowledgement the work was supported by key area research and development program of guangdong province no.2020b010165002 the research grants council of thehong kong sar china cuhk and australianresearch council arc discovery project dp200102940 .
440references s. wolfe amazon s one hour of downtime on prime day may have cost it up to million in lost sales .
.
available z. chen y .
kang l. li x. zhang h. zhang h. xu y .
zhou l. yang j. sun z. xu et al.
towards intelligent incident management why we need it and how we make it in proceedings of the 28th acm joint meeting on european software engineering conference and symposiumon the f oundations of software engineering pp.
.
y .
chen x. yang h. dong x. he h. zhang q. lin j. chen p. zhao y .
kang f. gao et al.
identifying linked incidents in large scale online service systems in proceedings of the 28th acm joint meeting on european software engineering conference and symposium on thef oundations of software engineering pp.
.
n. zhao j. chen x. peng h. wang x. wu y .
zhang z. chen x. zheng x. nie g. wang et al.
understanding and handling alert storm for online service systems in proceedings of the acm ieee 42nd international conference on software engineering software en gineering in practice pp.
.
z. chen y .
kang f. gao l. yang j. sun z. xu p. zhao b. qiao l. li x. zhang et al.
aiops innovations of incident management for cloud services .
o. team grlia graph based incident aggregation for largescale online service systems .
.
available m. j. kavis architecting the cloud design decisions for cloud computing service models saas paas and iaas .
john wiley sons .
s. p. ma c. y .
fan y .
chuang w. t. lee s. j. lee and n. l. hsueh using service dependency graph to analyze and test microservices in2018 ieee 42nd annual computer software and applications confer ence compsac vol.
.
ieee pp.
.
a. natarajan p. ning y .
liu s. jajodia and s. e. hutchinson nsdminer automated discovery of network service dependencies.
ieee .
p. huang c. guo l. zhou j. r. lorch y .
dang m. chintalapati and r. yao gray failure the achilles heel of cloud scale systems inproceedings of the 16th workshop on hot topics in operating systems pp.
.
w. l. hamilton r. ying and j. leskovec representation learning on graphs methods and applications arxiv preprint arxiv .
.
j. zhou g. cui z. zhang c. yang z. liu l. wang c. li and m. sun graph neural networks a review of methods and applications arxiv preprint arxiv .
.
a. siffer p. a. fouque a. termier and c. largouet anomaly detection in streams with extreme value theory in proceedings of the 23rd acm sigkdd international conference on knowledge discovery anddata mining pp.
.
l. de haan and a. ferreira extreme value theory an introduction.
springer science business media .
q. lin k. hsieh y .
dang h. zhang k. sui y .
xu j. g. lou c. li y .w u r .y a oet al.
predicting node failure in cloud service systems inproceedings of the 26th acm joint meeting on european software engineering conference and symposium on the f oundations of softwareengineering esec fse .
acm pp.
.
k. hundman v .
constantinou c. laporte i. colwell and t. soderstrom detecting spacecraft anomalies using lstms and nonparametricdynamic thresholding in proceedings of the 24th acm sigkdd international conference on knowledge discovery data mining pp.
.
z. yang r. algesheimer and c. j. tessone a comparative analysis of community detection algorithms on artificial networks scientific reports vol.
p. .
v .
d. blondel j. l. guillaume r. lambiotte and e. lefebvre fast unfolding of communities in large networks journal of statistical mechanics theory and experiment vol.
no.
p. p10008 .
p. liu y .
chen x. nie j. zhu s. zhang k. sui m. zhang and d. pei fluxrank a widely deployable framework to automatically localizingroot cause machines for software service failure mitigation in ieee 30th international symposium on software reliability engineering issre .
ieee pp.
.
e. keogh and c. a. ratanamahatana exact indexing of dynamic time warping knowledge and information systems vol.
no.
pp.
.
j. han j. pei and y .
yin mining frequent patterns without candidate generation acm sigmod record vol.
no.
pp.
.
a. grover and j. leskovec node2vec scalable feature learning for networks in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining pp.
.
b. perozzi r. al rfou and s. skiena deepwalk online learning of social representations in proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining pp.
.
t. mikolov i. sutskever k. chen g. s. corrado and j. dean distributed representations of words and phrases and their composi tionality in advances in neural information processing systems pp.
.
s. he q. lin j. g. lou h. zhang m. r. lyu and d. zhang identifying impactful service system problems via log analysis inproceedings of the 26th acm joint meeting on european softwareengineering conference and symposium on the f oundations of softwareengineering esec fse .
acm press pp.
.
stanford evaluation of clustering online accessed november r. reh u rek gensim topic modelling for humans .
.
available t. mikolov e. grave p. bojanowski c. puhrsch and a. joulin advances in pre training distributed word representations arxiv preprint arxiv .
.
a. joulin e. grave p. bojanowski m. douze h. j egou and t. mikolov fasttext.
zip compressing text classification models arxiv preprint arxiv .
.
j. liu j. zhu s. he p. he z. zheng and m. r. lyu logzip extracting hidden structures via iterative clustering for log compression in 34th ieee acm international conference on automated softwareengineering ase .
ieee pp.
.
r. christensen and f. li adaptive log compression for massive log data.
in sigmod conference pp.
.
p. he z. chen s. he and m. r. lyu characterizing the natural language descriptions in software logging statements in 33rd ieee acm international conference on automated software engineer ing ase .
ieee pp.
.
q. lin h. zhang j. g. lou y .
zhang and x. chen log clustering based problem identification for online service systems in proceedings of the 38th international conference on software engineering icse2016 austin tx usa may companion v olume.
acm pp.
.
g. jiang h. chen k. yoshihira and a. saxena ranking the importance of alerts for problem determination in large computer systems vol.
no.
pp.
.
c. m. rosenberg and l. moonen improving problem identification via automated log clustering using dimensionality reduction in proceedings of the 12th acm ieee international symposium on empiricalsoftware engineering and measurement.
acm pp.
.
y .
chen x. yang q. lin h. zhang f. gao z. xu y .
dang d. zhang h. dong y .
xu et al.
outage prediction and diagnosis for cloud service systems in proceedings of the international conference on world wide web www pp.
.
n. zhao j. chen z. wang x. peng g. wang y .
wu f. zhou z. feng x.nie w. zhang et al.
real time incident prediction for online service systems in proceedings of the 28th acm joint meeting on european software engineering conference and symposium on the f oundationsof software engineering pp.
.
j. gu c. luo s. qin b. qiao q. lin h. zhang z. li y .
dang s. cai w. wu et al.
efficient incident identification from multi dimensional issue reports via meta heuristic search in proceedings of the 28th acm joint meeting on european software engineering conference andsymposium on the f oundations of software engineering pp.
.
d. lin r. raghu v .
ramamurthy j. yu r. radhakrishnan and j. fernandez unveiling clusters of events for alert and incident managementin large scale enterprise it in the 20th acm sigkdd international conference on knowledge discovery and data mining kdd new 441york ny usa august s. a. macskassy c. perlich j. leskovec w. wang and r. ghani eds.
acm pp.
.
j. gao n. yaseen r. macdavid f. v .
frujeri v .
liu r. bianchini r. aditya x. wang h. lee d. maltz et al.
scouts improving the diagnosis process through domain customized incident routing inproceedings of the annual conference of the acm special interest groupon data communication on the applications technologies architectures and protocols for computer communication pp.
.
j. chen x. he q. lin y .
xu h. zhang d. hao f. gao z. xu y .
dang and d. zhang an empirical investigation of incident triagefor online service systems in proceedings of the 41st international conference on software engineering software engineering in practice icse seip .
ieee press pp.
.
j. chen x. he q. lin h. zhang d. hao f. gao z. xu y .
dang and d. zhang continuous incident triage for large scale online service sys tems in proceedings of the 34th ieee acm international conference on automated software engineering ase .
ieee pp.
.
m. h. lim j. g. lou h. zhang q. fu a. b. j. teoh q. lin r. ding and d. zhang identifying recurrent and unknown performance issues in2014 ieee international conference on data mining icdm shenzhen china december r. kumar h. toivonen j. pei j. z. huang and x. wu eds.
ieee computer society pp.
.
j. xu y .
wang p. chen and p. wang lightweight and adaptive service api performance monitoring in highly dynamic cloud environment in2017 ieee international conference on services computing scc honolulu hi usa june x. f. liu and u. bellur eds.ieee computer society pp.
.
n. zhao p. jin l. wang x. yang r. liu w. zhang k. sui and d. pei automatically and adaptively identifying severe alerts for online servicesystems in 39th ieee conference on computer communications infocom toronto on canada july .
ieee pp.
.
m. ester h. p. kriegel j. sander and x. xu a density based algorithm for discovering clusters in large spatial databases with noise in proceedings of the second international conference on knowledge discov ery and data mining kdd portland oregon usa e. simoudis j. han and u. m. fayyad eds.
aaai press pp.
.